[{"id": "1904.00103", "submitter": "Milo\\v{s} Simi\\'c", "authors": "Milo\\v{s} Simi\\'c (University of Belgrade, Belgrade, Serbia)", "title": "How to Estimate the Ability of a Metaheuristic Algorithm to Guide\n  Heuristics During Optimization", "comments": "24 pages, 3 figures, submitted to Journal of Heuristics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metaheuristics are general methods that guide application of concrete\nheuristic(s) to problems that are too hard to solve using exact algorithms.\nHowever, even though a growing body of literature has been devoted to their\nstatistical evaluation, the approaches proposed so far are able to assess only\ncoupled effects of metaheuristics and heuristics. They do not reveal us\nanything about how efficient the examined metaheuristic is at guiding its\nsubordinate heuristic(s), nor do they provide us information about how much the\nheuristic component of the combined algorithm contributes to the overall\nperformance. In this paper, we propose a simple yet effective methodology of\ndoing so by deriving a naive, placebo metaheuristic from the one being studied\nand comparing the distributions of chosen performance metrics for the two\nmethods. We propose three measures of difference between the two distributions.\nThose measures, which we call BER values (benefit, equivalence, risk) are based\non a preselected threshold of practical significance which represents the\nminimal difference between two performance scores required for them to be\nconsidered practically different. We illustrate usefulness of our methodology\non the example of Simulated Annealing, Boolean Satisfiability Problem, and the\nFlip heuristic.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 22:06:40 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Simi\u0107", "Milo\u0161", "", "University of Belgrade, Belgrade, Serbia"]]}, {"id": "1904.00143", "submitter": "Zhi-Xiu Ye", "authors": "Zhi-Xiu Ye, Zhen-Hua Ling", "title": "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag\n  Attentions", "comments": "accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural relation extraction method to deal with the\nnoisy training data generated by distant supervision. Previous studies mainly\nfocus on sentence-level de-noising by designing neural networks with intra-bag\nattentions. In this paper, both intra-bag and inter-bag attentions are\nconsidered in order to deal with the noise at sentence-level and bag-level\nrespectively. First, relation-aware bag representations are calculated by\nweighting sentence embeddings using intra-bag attentions. Here, each possible\nrelation is utilized as the query for attention calculation instead of only\nusing the target relation in conventional methods. Furthermore, the\nrepresentation of a group of bags in the training set which share the same\nrelation label is calculated by weighting bag representations using a\nsimilarity-based inter-bag attention module. Finally, a bag group is utilized\nas a training sample when building our relation extractor. Experimental results\non the New York Times dataset demonstrate the effectiveness of our proposed\nintra-bag and inter-bag attention modules. Our method also achieves better\nrelation extraction accuracy than state-of-the-art methods on this dataset.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 03:55:20 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ye", "Zhi-Xiu", ""], ["Ling", "Zhen-Hua", ""]]}, {"id": "1904.00231", "submitter": "Junjie Wang", "authors": "Junjie Wang, Qichao Zhang, Dongbin Zhao, Yaran Chen", "title": "Lane Change Decision-making through Deep Reinforcement Learning with\n  Rule-based Constraints", "comments": "6 pages, 5 figures, accepted at 2019 International Joint Conference\n  on Neural Networks(IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving decision-making is a great challenge due to the complexity\nand uncertainty of the traffic environment. Combined with the rule-based\nconstraints, a Deep Q-Network (DQN) based method is applied for autonomous\ndriving lane change decision-making task in this study. Through the combination\nof high-level lateral decision-making and low-level rule-based trajectory\nmodification, a safe and efficient lane change behavior can be achieved. With\nthe setting of our state representation and reward function, the trained agent\nis able to take appropriate actions in a real-world-like simulator. The\ngenerated policy is evaluated on the simulator for 10 times, and the results\ndemonstrate that the proposed rule-based DQN method outperforms the rule-based\napproach and the DQN method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 15:16:39 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:26:22 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Junjie", ""], ["Zhang", "Qichao", ""], ["Zhao", "Dongbin", ""], ["Chen", "Yaran", ""]]}, {"id": "1904.00296", "submitter": "Julian Estevez Dr", "authors": "Julian Estevez, Gorka Garate, JM Lopez Guede and Manuel Gra\\~na", "title": "Using Scratch to Teach Undergraduate Students' Skills on Artificial\n  Intelligence", "comments": "6 pages, 7 figures, workshop presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a educational workshop in Scratch that is proposed for\nthe active participation of undergraduate students in contexts of Artificial\nIntelligence. The main objective of the activity is to demystify the complexity\nof Artificial Intelligence and its algorithms. For this purpose, students must\nrealize simple exercises of clustering and two neural networks, in Scratch. The\ndetailed methodology to get that is presented in the article.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 21:37:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Estevez", "Julian", ""], ["Garate", "Gorka", ""], ["Guede", "JM Lopez", ""], ["Gra\u00f1a", "Manuel", ""]]}, {"id": "1904.00317", "submitter": "Patrick Rodler", "authors": "Patrick Rodler and Michael Eichholzer", "title": "A New Expert Questioning Approach to More Efficient Fault Localization\n  in Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When ontologies reach a certain size and complexity, faults such as\ninconsistencies, unsatisfiable classes or wrong entailments are hardly\navoidable. Locating the incorrect axioms that cause these faults is a hard and\ntime-consuming task. Addressing this issue, several techniques for\nsemi-automatic fault localization in ontologies have been proposed. Often,\nthese approaches involve a human expert who provides answers to\nsystem-generated questions about the intended (correct) ontology in order to\nreduce the possible fault locations. To suggest as informative questions as\npossible, existing methods draw on various algorithmic optimizations as well as\nheuristics. However, these computations are often based on certain assumptions\nabout the interacting user.\n  In this work, we characterize and discuss different user types and show that\nexisting approaches do not achieve optimal efficiency for all of them. As a\nremedy, we suggest a new type of expert question which aims at fitting the\nanswering behavior of all analyzed experts. Moreover, we present an algorithm\nto optimize this new query type which is fully compatible with the (tried and\ntested) heuristics used in the field. Experiments on faulty real-world\nontologies show the potential of the new querying method for minimizing the\nexpert consultation time, independent of the expert type. Besides, the gained\ninsights can inform the design of interactive debugging tools towards better\nmeeting their users' needs.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 01:25:52 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Rodler", "Patrick", ""], ["Eichholzer", "Michael", ""]]}, {"id": "1904.00325", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "title": "ImageGCN: Multi-Relational Image Graph Convolutional Networks for\n  Disease Identification with Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representation is a fundamental task in computer vision. However, most\nof the existing approaches for image representation ignore the relations\nbetween images and consider each input image independently. Intuitively,\nrelations between images can help to understand the images and maintain model\nconsistency over related images. In this paper, we consider modeling the\nimage-level relations to generate more informative image representations, and\npropose ImageGCN, an end-to-end graph convolutional network framework for\nmulti-relational image modeling. We also apply ImageGCN to chest X-ray (CXR)\nimages where rich relational information is available for disease\nidentification. Unlike previous image representation models, ImageGCN learns\nthe representation of an image using both its original pixel features and the\nfeatures of related images. Besides learning informative representations for\nimages, ImageGCN can also be used for object detection in a weakly supervised\nmanner. The Experimental results on ChestX-ray14 dataset demonstrate that\nImageGCN can outperform respective baselines in both disease identification and\nlocalization tasks and can achieve comparable and often better results than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 02:42:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mao", "Chengsheng", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1904.00326", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Liang Yao, Yuan Luo", "title": "MedGCN: Graph Convolutional Networks for Multiple Medical Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laboratory testing and medication prescription are two of the most important\nroutines in daily clinical practice. Developing an artificial intelligence\nsystem that can automatically make lab test imputations and medication\nrecommendations can save cost on potentially redundant lab tests and inform\nphysicians in more effective prescription. We present an intelligent model that\ncan automatically recommend the patients' medications based on their incomplete\nlab tests, and can even accurately estimate the lab values that have not been\ntaken. We model the complex relations between multiple types of medical\nentities with their inherent features in a heterogeneous graph. Then we learn a\ndistributed representation for each entity in the graph based on graph\nconvolutional networks to make the representations integrate information from\nmultiple types of entities. Since the entity representations incorporate\nmultiple types of medical information, they can be used for multiple medical\ntasks. In our experiments, we construct a graph to associate patients,\nencounters, lab tests and medications, and conduct the two tasks: medication\nrecommendation and lab test imputation. The experimental results demonstrate\nthat our model can outperform the state-of-the-art models in both tasks.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 02:48:50 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Mao", "Chengsheng", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1904.00346", "submitter": "Xijun Wang", "authors": "Xijun Wang, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Fully Learnable Group Convolution for Acceleration of Deep Neural\n  Networks", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefitted from its great success on many tasks, deep learning is\nincreasingly used on low-computational-cost devices, e.g. smartphone, embedded\ndevices, etc. To reduce the high computational and memory cost, in this work,\nwe propose a fully learnable group convolution module (FLGC for short) which is\nquite efficient and can be embedded into any deep neural networks for\nacceleration. Specifically, our proposed method automatically learns the group\nstructure in the training stage in a fully end-to-end manner, leading to a\nbetter structure than the existing pre-defined, two-steps, or iterative\nstrategies. Moreover, our method can be further combined with depthwise\nseparable convolution, resulting in 5 times acceleration than the vanilla\nResnet50 on single CPU. An additional advantage is that in our FLGC the number\nof groups can be set as any value, but not necessarily 2^k as in most existing\nmethods, meaning better tradeoff between accuracy and speed. As evaluated in\nour experiments, our method achieves better performance than existing learnable\ngroup convolution and standard group convolution when using the same number of\ngroups.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 06:24:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Wang", "Xijun", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1904.00441", "submitter": "Uk Jo", "authors": "Uk Jo, Taehyun Jo, Wanjun Kim, Iljoo Yoon, Dongseok Lee, Seungho Lee", "title": "Cooperative Multi-Agent Reinforcement Learning Framework for Scalping\n  Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore deep Reinforcement Learning(RL) algorithms for scalping trading\nand knew that there is no appropriate trading gym and agent examples. Thus we\npropose gym and agent like Open AI gym in finance. Not only that, we introduce\nnew RL framework based on our hybrid algorithm which leverages between\nsupervised learning and RL algorithm and uses meaningful observations such\norder book and settlement data from experience watching scalpers trading. That\nis very crucial information for traders behavior to be decided. To feed these\ndata into our model, we use spatio-temporal convolution layer, called Conv3D\nfor order book data and temporal CNN, called Conv1D for settlement data. Those\nare preprocessed by episode filter we developed. Agent consists of four sub\nagents divided to clarify their own goal to make best decision. Also, we\nadopted value and policy based algorithm to our framework. With these features,\nwe could make agent mimic scalpers as much as possible. In many fields, RL\nalgorithm has already begun to transcend human capabilities in many domains.\nThis approach could be a starting point to beat human in the financial stock\nmarket, too and be a good reference for anyone who wants to design RL algorithm\nin real world domain. Finally, weexperiment our framework and gave you\nexperiment progress.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 16:15:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Jo", "Uk", ""], ["Jo", "Taehyun", ""], ["Kim", "Wanjun", ""], ["Yoon", "Iljoo", ""], ["Lee", "Dongseok", ""], ["Lee", "Seungho", ""]]}, {"id": "1904.00511", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Daniel Seita, Yang Gao, John Canny", "title": "Risk Averse Robust Adversarial Reinforcement Learning", "comments": "ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has recently made significant progress in solving\ncomputer games and robotic control tasks. A known problem, though, is that\npolicies overfit to the training environment and may not avoid rare,\ncatastrophic events such as automotive accidents. A classical technique for\nimproving the robustness of reinforcement learning algorithms is to train on a\nset of randomized environments, but this approach only guards against common\nsituations. Recently, robust adversarial reinforcement learning (RARL) was\ndeveloped, which allows efficient applications of random and systematic\nperturbations by a trained adversary. A limitation of RARL is that only the\nexpected control objective is optimized; there is no explicit modeling or\noptimization of risk. Thus the agents do not consider the probability of\ncatastrophic events (i.e., those inducing abnormally large negative reward),\nexcept through their effect on the expected objective. In this paper we\nintroduce risk-averse robust adversarial reinforcement learning (RARARL), using\na risk-averse protagonist and a risk-seeking adversary. We test our approach on\na self-driving vehicle controller. We use an ensemble of policy networks to\nmodel risk as the variance of value functions. We show through experiments that\na risk-averse agent is better equipped to handle a risk-seeking adversary, and\nexperiences substantially fewer crashes compared to agents trained without an\nadversary.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 23:46:26 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Pan", "Xinlei", ""], ["Seita", "Daniel", ""], ["Gao", "Yang", ""], ["Canny", "John", ""]]}, {"id": "1904.00512", "submitter": "Yi Wang", "authors": "Yi Wang, Joohyung Lee", "title": "Elaboration Tolerant Representation of Markov Decision Process via\n  Decision-Theoretic Extension of Probabilistic Action Language pBC+", "comments": "31 pages, 3 figures; Under consideration in Theory and Practice of\n  Logic Programming (TPLP). arXiv admin note: text overlap with\n  arXiv:1805.00634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend probabilistic action language pBC+ with the notion of utility as in\ndecision theory. The semantics of the extended pBC+ can be defined as a\nshorthand notation for a decision-theoretic extension of the probabilistic\nanswer set programming language LPMLN. Alternatively, the semantics of pBC+ can\nalso be defined in terms of Markov Decision Process (MDP), which in turn allows\nfor representing MDP in a succinct and elaboration tolerant way as well as to\nleverage an MDP solver to compute pBC+. The idea led to the design of the\nsystem pbcplus2mdp, which can find an optimal policy of a pBC+ action\ndescription using an MDP solver. This paper is under consideration in Theory\nand Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 00:14:01 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 17:15:20 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Wang", "Yi", ""], ["Lee", "Joohyung", ""]]}, {"id": "1904.00601", "submitter": "Mohit Sharma", "authors": "Mohit K.Sharma, Alessio Zappone, Mohamad Assaad, Merouane Debbah,\n  Spyridon Vassilaras", "title": "Distributed Power Control for Large Energy Harvesting Networks: A\n  Multi-Agent Deep Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a multi-agent reinforcement learning (MARL)\nframework to obtain online power control policies for a large energy harvesting\n(EH) multiple access channel, when only causal information about the EH process\nand wireless channel is available. In the proposed framework, we model the\nonline power control problem as a discrete-time mean-field game (MFG), and\nanalytically show that the MFG has a unique stationary solution. Next, we\nleverage the fictitious play property of the mean-field games, and the deep\nreinforcement learning technique to learn the stationary solution of the game,\nin a completely distributed fashion. We analytically show that the proposed\nprocedure converges to the unique stationary solution of the MFG. This, in\nturn, ensures that the optimal policies can be learned in a completely\ndistributed fashion. In order to benchmark the performance of the distributed\npolicies, we also develop a deep neural network (DNN) based centralized as well\nas distributed online power control schemes. Our simulation results show the\nefficacy of the proposed power control policies. In particular, the DNN based\ncentralized power control policies provide a very good performance for large EH\nnetworks for which the design of optimal policies is intractable using the\nconventional methods such as Markov decision processes. Further, performance of\nboth the distributed policies is close to the throughput achieved by the\ncentralized policies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:16:51 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:00:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sharma", "Mohit K.", ""], ["Zappone", "Alessio", ""], ["Assaad", "Mohamad", ""], ["Debbah", "Merouane", ""], ["Vassilaras", "Spyridon", ""]]}, {"id": "1904.00619", "submitter": "EPTCS", "authors": "Nuno Baeta (University of Coimbra), Pedro Quaresma (University of\n  Coimbra)", "title": "Towards Ranking Geometric Automated Theorem Provers", "comments": "In Proceedings ThEdu'18, arXiv:1903.12402", "journal-ref": "EPTCS 290, 2019, pp. 30-37", "doi": "10.4204/EPTCS.290.3", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of geometric automated theorem provers has a long and rich history,\nfrom the early AI approaches of the 1960s, synthetic provers, to today\nalgebraic and synthetic provers.\n  The geometry automated deduction area differs from other areas by the strong\nconnection between the axiomatic theories and its standard models. In many\ncases the geometric constructions are used to establish the theorems'\nstatements, geometric constructions are, in some provers, used to conduct the\nproof, used as counter-examples to close some branches of the automatic proof.\nSynthetic geometry proofs are done using geometric properties, proofs that can\nhave a visual counterpart in the supporting geometric construction.\n  With the growing use of geometry automatic deduction tools as applications in\nother areas, e.g. in education, the need to evaluate them, using different\ncriteria, is felt. Establishing a ranking among geometric automated theorem\nprovers will be useful for the improvement of the current\nmethods/implementations. Improvements could concern wider scope, better\nefficiency, proof readability and proof reliability.\n  To achieve the goal of being able to compare geometric automated theorem\nprovers a common test bench is needed: a common language to describe the\ngeometric problems; a comprehensive repository of geometric problems and a set\nof quality measures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:53:09 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Baeta", "Nuno", "", "University of Coimbra"], ["Quaresma", "Pedro", "", "University of\n  Coimbra"]]}, {"id": "1904.00623", "submitter": "Yu-Jung Heo", "authors": "Yu-Jung Heo, Kyoung-Woon On, Seongho Choi, Jaeseo Lim, Jinah Kim,\n  Jeh-Kwang Ryu, Byung-Chull Bae and Byoung-Tak Zhang", "title": "Constructing Hierarchical Q&A Datasets for Video Story Understanding", "comments": "Accepted to AAAI 2019 Spring Symposium Series : Story-Enabled\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is emerging as a new paradigm for studying human-like AI.\nQuestion-and-Answering (Q&A) is used as a general benchmark to measure the\nlevel of intelligence for video understanding. While several previous studies\nhave suggested datasets for video Q&A tasks, they did not really incorporate\nstory-level understanding, resulting in highly-biased and lack of variance in\ndegree of question difficulty. In this paper, we propose a hierarchical method\nfor building Q&A datasets, i.e. hierarchical difficulty levels. We introduce\nthree criteria for video story understanding, i.e. memory capacity, logical\ncomplexity, and DIKW (Data-Information-Knowledge-Wisdom) pyramid. We discuss\nhow three-dimensional map constructed from these criteria can be used as a\nmetric for evaluating the levels of intelligence relating to video story\nunderstanding.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 08:05:19 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Heo", "Yu-Jung", ""], ["On", "Kyoung-Woon", ""], ["Choi", "Seongho", ""], ["Lim", "Jaeseo", ""], ["Kim", "Jinah", ""], ["Ryu", "Jeh-Kwang", ""], ["Bae", "Byung-Chull", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1904.00781", "submitter": "Dawei Li", "authors": "Dawei Li, Serafettin Tasci, Shalini Ghosh, Jingwen Zhu, Junting Zhang,\n  Larry Heck", "title": "RILOD: Near Real-Time Incremental Learning for Object Detection at the\n  Edge", "comments": "Camera-ready for ACM/IEEE SEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection models shipped with camera-equipped edge devices cannot\ncover the objects of interest for every user. Therefore, the incremental\nlearning capability is a critical feature for a robust and personalized object\ndetection system that many applications would rely on. In this paper, we\npresent an efficient yet practical system, RILOD, to incrementally train an\nexisting object detection model such that it can detect new object classes\nwithout losing its capability to detect old classes. The key component of RILOD\nis a novel incremental learning algorithm that trains end-to-end for one-stage\ndeep object detection models only using training data of new object classes.\nSpecifically to avoid catastrophic forgetting, the algorithm distills three\ntypes of knowledge from the old model to mimic the old model's behavior on\nobject classification, bounding box regression and feature extraction. In\naddition, since the training data for the new classes may not be available, a\nreal-time dataset construction pipeline is designed to collect training images\non-the-fly and automatically label the images with both category and bounding\nbox annotations. We have implemented RILOD under both edge-cloud and edge-only\nsetups. Experiment results show that the proposed system can learn to detect a\nnew object class in just a few minutes, including both dataset construction and\nmodel training. In comparison, traditional fine-tuning based method may take a\nfew hours for training, and in most cases would also need a tedious and costly\nmanual dataset labeling step.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:22:01 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:37:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Dawei", ""], ["Tasci", "Serafettin", ""], ["Ghosh", "Shalini", ""], ["Zhu", "Jingwen", ""], ["Zhang", "Junting", ""], ["Heck", "Larry", ""]]}, {"id": "1904.00838", "submitter": "Changhee Han", "authors": "Changhee Han, Kohei Murao, Shin'ichi Satoh, Hideki Nakayama", "title": "Learning More with Less: GAN-based Medical Image Augmentation", "comments": "6 pages, 2 figures, to appear in MEDICAL IMAGING TECHNOLOGY Special\n  Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN)-based accurate prediction typically\nrequires large-scale annotated training data. In Medical Imaging, however, both\nobtaining medical data and annotating them by expert physicians are\nchallenging; to overcome this lack of data, Data Augmentation (DA) using\nGenerative Adversarial Networks (GANs) is essential, since they can synthesize\nadditional annotated training data to handle small and fragmented medical\nimages from various scanners--those generated images, realistic but completely\nnovel, can further fill the real image distribution uncovered by the original\ndataset. As a tutorial, this paper introduces GAN-based Medical Image\nAugmentation, along with tricks to boost classification/object\ndetection/segmentation performance using them, based on our experience and\nrelated work. Moreover, we show our first GAN-based DA work using automatic\nbounding box annotation, for robust CNN-based brain metastases detection on 256\nx 256 MR images; GAN-based DA can boost 10% sensitivity in diagnosis with a\nclinically acceptable number of additional False Positives, even with\nhighly-rough and inconsistent bounding boxes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:41:28 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 15:21:40 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 13:08:32 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Han", "Changhee", ""], ["Murao", "Kohei", ""], ["Satoh", "Shin'ichi", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1904.00839", "submitter": "David Dov", "authors": "David Dov, Shahar Kovalsky, Jonathan Cohen, Danielle Range, Ricardo\n  Henao, and Lawrence Carin", "title": "Thyroid Cancer Malignancy Prediction From Whole Slide Cytopathology\n  Images", "comments": null, "journal-ref": "Proceedings of Machine Learning Research, 2019, Vol. 106", "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider preoperative prediction of thyroid cancer based on\nultra-high-resolution whole-slide cytopathology images. Inspired by how human\nexperts perform diagnosis, our approach first identifies and classifies\ndiagnostic image regions containing informative thyroid cells, which only\ncomprise a tiny fraction of the entire image. These local estimates are then\naggregated into a single prediction of thyroid malignancy. Several unique\ncharacteristics of thyroid cytopathology guide our deep-learning-based\napproach. While our method is closely related to multiple-instance learning, it\ndeviates from these methods by using a supervised procedure to extract\ndiagnostically relevant regions. Moreover, we propose to simultaneously predict\nthyroid malignancy, as well as a diagnostic score assigned by a human expert,\nwhich further allows us to devise an improved training strategy. Experimental\nresults show that the proposed algorithm achieves performance comparable to\nhuman experts, and demonstrate the potential of using the algorithm for\nscreening and as an assistive tool for the improved diagnosis of indeterminate\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 17:47:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Dov", "David", ""], ["Kovalsky", "Shahar", ""], ["Cohen", "Jonathan", ""], ["Range", "Danielle", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1904.00842", "submitter": "Daniel Bauer", "authors": "Daniel Bauer, Lars Kuhnert, Lutz Eckstein", "title": "Deep, spatially coherent Inverse Sensor Models with Uncertainty\n  Incorporation using the evidential Framework", "comments": "Submitted for Intelligent Vehicle Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform high speed tasks, sensors of autonomous cars have to provide as\nmuch information in as few time steps as possible. However, radars, one of the\nsensor modalities autonomous cars heavily rely on, often only provide sparse,\nnoisy detections. These have to be accumulated over time to reach a high enough\nconfidence about the static parts of the environment. For radars, the state is\ntypically estimated by accumulating inverse detection models (IDMs). We employ\nthe recently proposed evidential convolutional neural networks which, in\ncontrast to IDMs, compute dense, spatially coherent inference of the\nenvironment state. Moreover, these networks are able to incorporate sensor\nnoise in a principled way which we further extend to also incorporate model\nuncertainty. We present experimental results that show This makes it possible\nto obtain a denser environment perception in fewer time steps.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 11:50:13 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bauer", "Daniel", ""], ["Kuhnert", "Lars", ""], ["Eckstein", "Lutz", ""]]}, {"id": "1904.00876", "submitter": "Yawei Luo", "authors": "Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang", "title": "Significance-aware Information Bottleneck for Domain Adaptive Semantic\n  Segmentation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For unsupervised domain adaptation problems, the strategy of aligning the two\ndomains in latent feature space through adversarial learning has achieved much\nprogress in image classification, but usually fails in semantic segmentation\ntasks in which the latent representations are overcomplex. In this work, we\nequip the adversarial network with a \"significance-aware information bottleneck\n(SIB)\", to address the above problem. The new network structure, called SIBAN,\nenables a significance-aware feature purification before the adversarial\nadaptation, which eases the feature alignment and stabilizes the adversarial\ntraining course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and\nSYNTHIA -> Cityscapes, we validate that the proposed method can yield leading\nresults compared with other feature-space alternatives. Moreover, SIBAN can\neven match the state-of-the-art output-space methods in segmentation accuracy,\nwhile the latter are often considered to be better choices for domain adaptive\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:19:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Luo", "Yawei", ""], ["Liu", "Ping", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Yang", "Yi", ""]]}, {"id": "1904.00956", "submitter": "Russell Mendonca", "authors": "Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey\n  Levine, Chelsea Finn", "title": "Guided Meta-Policy Search", "comments": "Published at Neurips 2019. Website :\n  https://sites.google.com/berkeley.edu/guided-metapolicy-search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) algorithms have demonstrated promising results on\ncomplex tasks, yet often require impractical numbers of samples since they\nlearn from scratch. Meta-RL aims to address this challenge by leveraging\nexperience from previous tasks so as to more quickly solve new tasks. However,\nin practice, these algorithms generally also require large amounts of on-policy\nexperience during the meta-training process, making them impractical for use in\nmany problems. To this end, we propose to learn a reinforcement learning\nprocedure in a federated way, where individual off-policy learners can solve\nthe individual meta-training tasks, and then consolidate these solutions into a\nsingle meta-learner. Since the central meta-learner learns by imitating the\nsolutions to the individual tasks, it can accommodate either the standard\nmeta-RL problem setting or a hybrid setting where some or all tasks are\nprovided with example demonstrations. The former results in an approach that\ncan leverage policies learned for previous tasks without significant amounts of\non-policy data during meta-training, whereas the latter is particularly useful\nin cases where demonstrations are easy for a person to provide. Across a number\nof continuous control meta-RL problems, we demonstrate significant improvements\nin meta-RL sample efficiency in comparison to prior work as well as the ability\nto scale to domains with visual observations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:47:28 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 09:27:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Mendonca", "Russell", ""], ["Gupta", "Abhishek", ""], ["Kralev", "Rosen", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1904.00962", "submitter": "Yang You", "authors": "Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv\n  Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt\n  Keutzer and Cho-Jui Hsieh", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 16:53:35 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 06:20:00 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 17:09:47 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2019 16:07:11 GMT"}, {"version": "v5", "created": "Fri, 3 Jan 2020 06:53:00 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["You", "Yang", ""], ["Li", "Jing", ""], ["Reddi", "Sashank", ""], ["Hseu", "Jonathan", ""], ["Kumar", "Sanjiv", ""], ["Bhojanapalli", "Srinadh", ""], ["Song", "Xiaodan", ""], ["Demmel", "James", ""], ["Keutzer", "Kurt", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1904.01068", "submitter": "Erdem B{\\i}y{\\i}k", "authors": "Erdem B{\\i}y{\\i}k, Jonathan Margoliash, Shahrouz Ryan Alimo, Dorsa\n  Sadigh", "title": "Efficient and Safe Exploration in Deterministic Markov Decision\n  Processes with Unknown Transition Models", "comments": "Proceedings of the American Control Conference (ACC), July 2019. The\n  first two authors have equal contribution", "journal-ref": null, "doi": "10.23919/ACC.2019.8815276", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a safe exploration algorithm for deterministic Markov Decision\nProcesses with unknown transition models. Our algorithm guarantees safety by\nleveraging Lipschitz-continuity to ensure that no unsafe states are visited\nduring exploration. Unlike many other existing techniques, the provided safety\nguarantee is deterministic. Our algorithm is optimized to reduce the number of\nactions needed for exploring the safe space. We demonstrate the performance of\nour algorithm in comparison with baseline methods in simulation on navigation\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 19:09:08 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["B\u0131y\u0131k", "Erdem", ""], ["Margoliash", "Jonathan", ""], ["Alimo", "Shahrouz Ryan", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "1904.01126", "submitter": "Rakshit Agrawal", "authors": "Jack W. Stokes, Rakshit Agrawal, Geoff McDonald, Matthew Hausknecht", "title": "ScriptNet: Neural Static Analysis for Malicious JavaScript Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious scripts are an important computer infection threat vector in the\nwild. For web-scale processing, static analysis offers substantial computing\nefficiencies. We propose the ScriptNet system for neural malicious JavaScript\ndetection which is based on static analysis. We use the Convoluted Partitioning\nof Long Sequences (CPoLS) model, which processes Javascript files as byte\nsequences. Lower layers capture the sequential nature of these byte sequences\nwhile higher layers classify the resulting embedding as malicious or benign.\nUnlike previously proposed solutions, our model variants are trained in an\nend-to-end fashion allowing discriminative training even for the sequential\nprocessing layers. Evaluating this model on a large corpus of 212,408\nJavaScript files indicates that the best performing CPoLS model offers a 97.20%\ntrue positive rate (TPR) for the first 60K byte subsequence at a false positive\nrate (FPR) of 0.50%. The best performing CPoLS model significantly outperform\nseveral baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 22:00:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Stokes", "Jack W.", ""], ["Agrawal", "Rakshit", ""], ["McDonald", "Geoff", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "1904.01138", "submitter": "Lifu Tu", "authors": "Lifu Tu, Kevin Gimpel", "title": "Benchmarking Approximate Inference Methods for Neural Structured\n  Prediction", "comments": "NAACL2019 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exact structured inference with neural network scoring functions is\ncomputationally challenging but several methods have been proposed for\napproximating inference. One approach is to perform gradient descent with\nrespect to the output structure directly (Belanger and McCallum, 2016). Another\napproach, proposed recently, is to train a neural network (an \"inference\nnetwork\") to perform inference (Tu and Gimpel, 2018). In this paper, we compare\nthese two families of inference methods on three sequence labeling datasets. We\nchoose sequence labeling because it permits us to use exact inference as a\nbenchmark in terms of speed, accuracy, and search error. Across datasets, we\ndemonstrate that inference networks achieve a better speed/accuracy/search\nerror trade-off than gradient descent, while also being faster than exact\ninference at similar accuracy levels. We find further benefit by combining\ninference networks and gradient descent, using the former to provide a warm\nstart for the latter.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 23:08:05 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 23:05:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Tu", "Lifu", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1904.01191", "submitter": "Zaheer Abbas", "authors": "Yi Wan, Zaheer Abbas, Adam White, Martha White and Richard S. Sutton", "title": "Planning with Expectation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution and sample models are two popular model choices in model-based\nreinforcement learning (MBRL). However, learning these models can be\nintractable, particularly when the state and action spaces are large.\nExpectation models, on the other hand, are relatively easier to learn due to\ntheir compactness and have also been widely used for deterministic\nenvironments. For stochastic environments, it is not obvious how expectation\nmodels can be used for planning as they only partially characterize a\ndistribution. In this paper, we propose a sound way of using approximate\nexpectation models for MBRL. In particular, we 1) show that planning with an\nexpectation model is equivalent to planning with a distribution model if the\nstate value function is linear in state features, 2) analyze two common\nparametrization choices for approximating the expectation: linear and\nnon-linear expectation models, 3) propose a sound model-based policy evaluation\nalgorithm and present its convergence results, and 4) empirically demonstrate\nthe effectiveness of the proposed planning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:25:25 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 04:48:48 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 04:33:31 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 22:40:04 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wan", "Yi", ""], ["Abbas", "Zaheer", ""], ["White", "Adam", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1904.01201", "submitter": "Manolis Savva", "authors": "Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik\n  Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra\n  Malik, Devi Parikh, Dhruv Batra", "title": "Habitat: A Platform for Embodied AI Research", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:52:27 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 01:39:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Savva", "Manolis", ""], ["Kadian", "Abhishek", ""], ["Maksymets", "Oleksandr", ""], ["Zhao", "Yili", ""], ["Wijmans", "Erik", ""], ["Jain", "Bhavana", ""], ["Straub", "Julian", ""], ["Liu", "Jia", ""], ["Koltun", "Vladlen", ""], ["Malik", "Jitendra", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.01241", "submitter": "Walid Abdullah Al", "authors": "Walid Abdullah Al, Il Dong Yun, Eun Ju Chun", "title": "Centerline Depth World Reinforcement Learning-based Left Atrial\n  Appendage Orifice Localization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Left atrial appendage (LAA) closure (LAAC) is a minimally invasive\nimplant-based method to prevent cardiovascular stroke in patients with\nnon-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT\nangiography plays a crucial role in choosing an appropriate LAAC implant size\nand a proper C-arm angulation. However, accurate orifice localization is hard\nbecause of the high anatomic variation of LAA, and unclear position and\norientation of the orifice in available CT views. Deep localization models also\nyield high error in localizing the orifice in CT image because of the tiny\nstructure of orifice compared to the vastness of CT image. In this paper, we\npropose a centerline depth-based reinforcement learning (RL) world for\neffective orifice localization in a small search space. In our scheme, an RL\nagent observes the centerline-to-surface distance and navigates through the LAA\ncenterline to localize the orifice. Thus, the search space is significantly\nreduced facilitating improved localization. The proposed formulation could\nresult in high localization accuracy comparing to the expert-annotations in 98\nCT images. Moreover, the localization process takes about 8 seconds which is 18\ntimes more efficient than the existing method. Therefore, this can be a useful\naid to physicians during the preprocedural planning of LAAC.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 06:56:11 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 01:28:42 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Al", "Walid Abdullah", ""], ["Yun", "Il Dong", ""], ["Chun", "Eun Ju", ""]]}, {"id": "1904.01390", "submitter": "Shiv Ram Dubey", "authors": "Sai Prasanna Teja Reddy, Surya Teja Karri, Shiv Ram Dubey, Snehasis\n  Mukherjee", "title": "Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal\n  Convolutional Neural Networks", "comments": "Accepted in 2019 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition in videos is an active area of research in\ncomputer vision. However, fake facial expressions are difficult to be\nrecognized even by humans. On the other hand, facial micro-expressions\ngenerally represent the actual emotion of a person, as it is a spontaneous\nreaction expressed through human face. Despite of a few attempts made for\nrecognizing micro-expressions, still the problem is far from being a solved\nproblem, which is depicted by the poor rate of accuracy shown by the\nstate-of-the-art methods. A few CNN based approaches are found in the\nliterature to recognize micro-facial expressions from still images. Whereas, a\nspontaneous micro-expression video contains multiple frames that have to be\nprocessed together to encode both spatial and temporal information. This paper\nproposes two 3D-CNN methods: MicroExpSTCNN and MicroExpFuseNet, for spontaneous\nfacial micro-expression recognition by exploiting the spatiotemporal\ninformation in CNN framework. The MicroExpSTCNN considers the full spatial\ninformation, whereas the MicroExpFuseNet is based on the 3D-CNN feature fusion\nof the eyes and mouth regions. The experiments are performed over CAS(ME)^2 and\nSMIC micro-expression databases. The proposed MicroExpSTCNN model outperforms\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:45:49 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Reddy", "Sai Prasanna Teja", ""], ["Karri", "Surya Teja", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1904.01484", "submitter": "Patrick Rodler", "authors": "Patrick Rodler, Dietmar Jannach, Konstantin Schekotihin, Philipp\n  Fleiss", "title": "Are Query-Based Ontology Debuggers Really Helping Knowledge Engineers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world semantic or knowledge-based systems, e.g., in the biomedical\ndomain, can become large and complex. Tool support for the localization and\nrepair of faults within knowledge bases of such systems can therefore be\nessential for their practical success. Correspondingly, a number of knowledge\nbase debugging approaches, in particular for ontology-based systems, were\nproposed throughout recent years. Query-based debugging is a comparably recent\ninteractive approach that localizes the true cause of an observed problem by\nasking knowledge engineers a series of questions. Concrete implementations of\nthis approach exist, such as the OntoDebug plug-in for the ontology editor\nProt\\'eg\\'e.\n  To validate that a newly proposed method is favorable over an existing one,\nresearchers often rely on simulation-based comparisons. Such an evaluation\napproach however has certain limitations and often cannot fully inform us about\na method's true usefulness. We therefore conducted different user studies to\nassess the practical value of query-based ontology debugging. One main insight\nfrom the studies is that the considered interactive approach is indeed more\nefficient than an alternative algorithmic debugging based on test cases. We\nalso observed that users frequently made errors in the process, which\nhighlights the importance of a careful design of the queries that users need to\nanswer.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:17:56 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Rodler", "Patrick", ""], ["Jannach", "Dietmar", ""], ["Schekotihin", "Konstantin", ""], ["Fleiss", "Philipp", ""]]}, {"id": "1904.01497", "submitter": "Srushti Rath", "authors": "Srushti Rath, Joseph Y.J. Chow", "title": "Air Taxi Skyport Location Problem for Airport Access", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider design of skyport locations for air taxis accessing airports and\nadopt a novel use of the classic hub location problem to properly make\ntrade-offs on access distances for travelers to skyports from other zones,\nwhich is shown to reduce costs relative to a clustering approach from the\nliterature. Extensive experiments on data from New York City show the method\noutperforms the benchmark clustering method by more than 7.4% here. Results\nsuggest that six skyports located between Manhattan and Brooklyn can adequately\nserve the airport access travel needs and are sufficiently stable against\ntravel time or transfer time increases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 01:00:49 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 01:12:50 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 03:21:53 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Rath", "Srushti", ""], ["Chow", "Joseph Y. J.", ""]]}, {"id": "1904.01540", "submitter": "Nadisha-Marie Aliman", "authors": "Nadisha-Marie Aliman and Leon Kester", "title": "Augmented Utilitarianism for AGI Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the light of ongoing progresses of research on artificial intelligent\nsystems exhibiting a steadily increasing problem-solving ability, the\nidentification of practicable solutions to the value alignment problem in AGI\nSafety is becoming a matter of urgency. In this context, one preeminent\nchallenge that has been addressed by multiple researchers is the adequate\nformulation of utility functions or equivalents reliably capturing human\nethical conceptions. However, the specification of suitable utility functions\nharbors the risk of \"perverse instantiation\" for which no final consensus on\nresponsible proactive countermeasures has been achieved so far. Amidst this\nbackground, we propose a novel socio-technological ethical framework denoted\nAugmented Utilitarianism which directly alleviates the perverse instantiation\nproblem. We elaborate on how augmented by AI and more generally science and\ntechnology, it might allow a society to craft and update ethical utility\nfunctions while jointly undergoing a dynamical ethical enhancement. Further, we\nelucidate the need to consider embodied simulations in the design of utility\nfunctions for AGIs aligned with human values. Finally, we discuss future\nprospects regarding the usage of the presented scientifically grounded ethical\nframework and mention possible challenges.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:54:38 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Aliman", "Nadisha-Marie", ""], ["Kester", "Leon", ""]]}, {"id": "1904.01554", "submitter": "Ali Payani", "authors": "Ali Payani and Faramarz Fekri", "title": "Learning Algorithms via Neural Logic Networks", "comments": "Under Review in ICLM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning paradigm for Deep Neural Networks (DNN) by using\nBoolean logic algebra. We first present the basic differentiable operators of a\nBoolean system such as conjunction, disjunction and exclusive-OR and show how\nthese elementary operators can be combined in a simple and meaningful way to\nform Neural Logic Networks (NLNs). We examine the effectiveness of the proposed\nNLN framework in learning Boolean functions and discrete-algorithmic tasks. We\ndemonstrate that, in contrast to the implicit learning in MLP approach, the\nproposed neural logic networks can learn the logical functions explicitly that\ncan be verified and interpreted by human. In particular, we propose a new\nframework for learning the inductive logic programming (ILP) problems by\nexploiting the explicit representational power of NLN. We show the proposed\nneural ILP solver is capable of feats such as predicate invention and recursion\nand can outperform the current state of the art neural ILP solvers using a\nvariety of benchmark tasks such as decimal addition and multiplication, and\nsorting on ordered list.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 17:17:02 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Payani", "Ali", ""], ["Fekri", "Faramarz", ""]]}, {"id": "1904.01638", "submitter": "Li Yao", "authors": "Li Yao, Jordan Prosky, Ben Covington, Kevin Lyman", "title": "A Strong Baseline for Domain Adaptation and Generalization in Medical\n  Imaging", "comments": "Extended abstract of a journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a strong baseline for the problem of multi-source\nmulti-target domain adaptation and generalization in medical imaging. Using a\ndiverse collection of ten chest X-ray datasets, we empirically demonstrate the\nbenefits of training medical imaging deep learning models on varied patient\npopulations for generalization to out-of-sample domains.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 19:38:34 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Yao", "Li", ""], ["Prosky", "Jordan", ""], ["Covington", "Ben", ""], ["Lyman", "Kevin", ""]]}, {"id": "1904.01650", "submitter": "Rosario Scalise", "authors": "Rosario Scalise, Jesse Thomason, Yonatan Bisk and Siddhartha Srinivasa", "title": "Improving Robot Success Detection using Static Object Data", "comments": "IROS 2019 + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use static object data to improve success detection for stacking objects\non and nesting objects in one another. Such actions are necessary for certain\nrobotics tasks, e.g., clearing a dining table or packing a warehouse bin.\nHowever, using an RGB-D camera to detect success can be insufficient:\nsame-colored objects can be difficult to differentiate, and reflective\nsilverware cause noisy depth camera perception. We show that adding static data\nabout the objects themselves improves the performance of an end-to-end pipeline\nfor classifying action outcomes. Images of the objects, and language\nexpressions describing them, encode prior geometry, shape, and size information\nthat refine classification accuracy. We collect over 13 hours of egocentric\nmanipulation data for training a model to reason about whether a robot\nsuccessfully placed unseen objects in or on one another. The model achieves up\nto a 57% absolute gain over the task baseline on pairs of previously unseen\nobjects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:18:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 18:18:01 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Scalise", "Rosario", ""], ["Thomason", "Jesse", ""], ["Bisk", "Yonatan", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1904.01664", "submitter": "Katherine Metcalf", "authors": "Katherine Metcalf, Barry-John Theobald, Garrett Weinberg, Robert Lee,\n  Ing-Marie Jonsson, Russ Webb, Nicholas Apostoloff", "title": "Mirroring to Build Trust in Digital Assistants", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe experiments towards building a conversational digital assistant\nthat considers the preferred conversational style of the user. In particular,\nthese experiments are designed to measure whether users prefer and trust an\nassistant whose conversational style matches their own. To this end we\nconducted a user study where subjects interacted with a digital assistant that\nresponded in a way that either matched their conversational style, or did not.\nUsing self-reported personality attributes and subjects' feedback on the\ninteractions, we built models that can reliably predict a user's preferred\nconversational style.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:51:27 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Metcalf", "Katherine", ""], ["Theobald", "Barry-John", ""], ["Weinberg", "Garrett", ""], ["Lee", "Robert", ""], ["Jonsson", "Ing-Marie", ""], ["Webb", "Russ", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "1904.01677", "submitter": "Josef Urban", "authors": "Jan Jakub\\r{u}v and Josef Urban", "title": "Hammering Mizar by Learning Clause Guidance", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.03182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a very large improvement of existing hammer-style proof\nautomation over large ITP libraries by combining learning and theorem proving.\nIn particular, we have integrated state-of-the-art machine learners into the E\nautomated theorem prover, and developed methods that allow learning and\nefficient internal guidance of E over the whole Mizar library. The resulting\ntrained system improves the real-time performance of E on the Mizar library by\n70% in a single-strategy setting.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:36:40 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Jakub\u016fv", "Jan", ""], ["Urban", "Josef", ""]]}, {"id": "1904.01739", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Guo-sen Xie, Yang Li, Sheng Li, Zi Huang", "title": "SADIH: Semantic-Aware DIscrete Hashing", "comments": "Accepted by The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, hashing has been recognized\nto accomplish similarity search in large-scale multimedia retrieval\napplications. Particularly supervised hashing has recently received\nconsiderable research attention by leveraging the label information to preserve\nthe pairwise similarities of data points in the Hamming space. However, there\nstill remain two crucial bottlenecks: 1) the learning process of the full\npairwise similarity preservation is computationally unaffordable and unscalable\nto deal with big data; 2) the available category information of data are not\nwell-explored to learn discriminative hash functions. To overcome these\nchallenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH)\nframework, which aims to directly embed the transformed semantic information\ninto the asymmetric similarity approximation and discriminative hashing\nfunction learning. Specifically, a semantic-aware latent embedding is\nintroduced to asymmetrically preserve the full pairwise similarities while\nskillfully handle the cumbersome n times n pairwise similarity matrix.\nMeanwhile, a semantic-aware autoencoder is developed to jointly preserve the\ndata structures in the discriminative latent semantic space and perform data\nreconstruction. Moreover, an efficient alternating optimization algorithm is\nproposed to solve the resulting discrete optimization problem. Extensive\nexperimental results on multiple large-scale datasets demonstrate that our\nSADIH can clearly outperform the state-of-the-art baselines with the additional\nbenefit of lower computational costs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:45:05 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 04:51:18 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhang", "Zheng", ""], ["Xie", "Guo-sen", ""], ["Li", "Yang", ""], ["Li", "Sheng", ""], ["Huang", "Zi", ""]]}, {"id": "1904.01747", "submitter": "Dacheng Tao", "authors": "Ya Li, Xinmei Tian, Tongliang Liu, Dacheng Tao", "title": "On Better Exploring and Exploiting Task Relationships in Multi-Task\n  Learning: Joint Model and Feature Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2017.2690683", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning (MTL) aims to learn multiple tasks simultaneously through\nthe interdependence between different tasks. The way to measure the relatedness\nbetween tasks is always a popular issue. There are mainly two ways to measure\nrelatedness between tasks: common parameters sharing and common features\nsharing across different tasks. However, these two types of relatedness are\nmainly learned independently, leading to a loss of information. In this paper,\nwe propose a new strategy to measure the relatedness that jointly learns shared\nparameters and shared feature representations. The objective of our proposed\nmethod is to transform the features from different tasks into a common feature\nspace in which the tasks are closely related and the shared parameters can be\nbetter optimized. We give a detailed introduction to our proposed multitask\nlearning method. Additionally, an alternating algorithm is introduced to\noptimize the nonconvex objection. A theoretical bound is given to demonstrate\nthat the relatedness between tasks can be better measured by our proposed\nmultitask learning algorithm. We conduct various experiments to verify the\nsuperiority of the proposed joint model and feature a multitask learning\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 03:14:20 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Li", "Ya", ""], ["Tian", "Xinmei", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.01766", "submitter": "Chen Sun", "authors": "Chen Sun and Austin Myers and Carl Vondrick and Kevin Murphy and\n  Cordelia Schmid", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning", "comments": "ICCV 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has become increasingly important to leverage the\nabundance of unlabeled data available on platforms like YouTube. Whereas most\nexisting approaches learn low-level representations, we propose a joint\nvisual-linguistic model to learn high-level features without any explicit\nsupervision. In particular, inspired by its recent success in language\nmodeling, we build upon the BERT model to learn bidirectional joint\ndistributions over sequences of visual and linguistic tokens, derived from\nvector quantization of video data and off-the-shelf speech recognition outputs,\nrespectively. We use VideoBERT in numerous tasks, including action\nclassification and video captioning. We show that it can be applied directly to\nopen-vocabulary classification, and confirm that large amounts of training data\nand cross-modal information are critical to performance. Furthermore, we\noutperform the state-of-the-art on video captioning, and quantitative results\nverify that the model learns high-level semantic features.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 04:40:16 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 19:52:54 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Sun", "Chen", ""], ["Myers", "Austin", ""], ["Vondrick", "Carl", ""], ["Murphy", "Kevin", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1904.01777", "submitter": "Heng Wang", "authors": "Heng Wang, Mingzhi Mao", "title": "Defeats GAN: A Simpler Model Outperforms in Knowledge Representation\n  Learning", "comments": "5 pages, 1 figure, has been accepted as a conference paper of the 3rd\n  IEEE International Conference on Computational Intelligence and Applications\n  (ICCIA 2018)", "journal-ref": "Proceedings of the 3rd IEEE International Conference on\n  Computational Intelligence and Applications (ICCIA 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of knowledge representation learning is to embed entities and\nrelations into a low-dimensional, continuous vector space. How to push a model\nto its limit and obtain better results is of great significance in knowledge\ngraph's applications. We propose a simple and elegant method, Trans-DLR, whose\nmain idea is dynamic learning rate control during training. Our method achieves\nremarkable improvement, compared with recent GAN-based method. Moreover, we\nintroduce a new negative sampling trick which corrupts not only entities, but\nalso relations, in different probabilities. We also develop an efficient way,\nwhich fully utilizes multiprocessing and parallel computing, to speed up\nevaluation of the model in link prediction tasks. Experiments show that our\nmethod is effective.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:42:14 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Heng", ""], ["Mao", "Mingzhi", ""]]}, {"id": "1904.01778", "submitter": "Ramanathan Subramanian", "authors": "Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Mohan\n  Kankanhalli, Stefan Winkler, Ramanathan Subramanian", "title": "Recognition of Advertisement Emotions with Application to Computational\n  Advertising", "comments": "Under consideration for publication in IEEE Trans. Affective\n  Computing. arXiv admin note: text overlap with arXiv:1709.01684", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertisements (ads) often contain strong affective content to capture viewer\nattention and convey an effective message to the audience. However, most\ncomputational affect recognition (AR) approaches examine ads via the text\nmodality, and only limited work has been devoted to decoding ad emotions from\naudiovisual or user cues. This work (1) compiles an affective ad dataset\ncapable of evoking coherent emotions across users; (2) explores the efficacy of\ncontent-centric convolutional neural network (CNN) features for AR vis-\\~a-vis\nhandcrafted audio-visual descriptors; (3) examines user-centric ad AR from\nElectroencephalogram (EEG) responses acquired during ad-viewing, and (4)\ndemonstrates how better affect predictions facilitate effective computational\nadvertising as determined by a study involving 18 users. Experiments reveal\nthat (a) CNN features outperform audiovisual descriptors for content-centric\nAR; (b) EEG features are able to encode ad-induced emotions better than\ncontent-based features; (c) Multi-task learning performs best among a slew of\nclassification algorithms to achieve optimal AR, and (d) Pursuant to (b), EEG\nfeatures also enable optimized ad insertion onto streamed video, as compared to\ncontent-based or manual insertion techniques in terms of ad memorability and\noverall user experience.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:42:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shukla", "Abhinav", ""], ["Gullapuram", "Shruti Shriya", ""], ["Katti", "Harish", ""], ["Kankanhalli", "Mohan", ""], ["Winkler", "Stefan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1904.01790", "submitter": "Daichi Nishio", "authors": "Daichi Nishio and Satoshi Yamane", "title": "Random Projection in Neural Episodic Control", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep reinforcement learning has enabled agents to learn with\nlittle preprocessing by humans. However, it is still difficult to learn stably\nand efficiently because the learning method usually uses a nonlinear function\napproximation. Neural Episodic Control (NEC), which has been proposed in order\nto improve sample efficiency, is able to learn stably by estimating action\nvalues using a non-parametric method. In this paper, we propose an architecture\nthat incorporates random projection into NEC to train with more stability. In\naddition, we verify the effectiveness of our architecture by Atari's five\ngames. The main idea is to reduce the number of parameters that have to learn\nby replacing neural networks with random projection in order to reduce\ndimensions while keeping the learning end-to-end.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 06:17:33 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 10:05:35 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Nishio", "Daichi", ""], ["Yamane", "Satoshi", ""]]}, {"id": "1904.01863", "submitter": "Xixi Lu", "authors": "Seyed Amin Tabatabaei, Xixi Lu, Mark Hoogendoorn, and Hajo A. Reijers", "title": "Identifying Patient Groups based on Frequent Patterns of Patient Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping patients meaningfully can give insights about the different types of\npatients, their needs, and the priorities. Finding groups that are meaningful\nis however very challenging as background knowledge is often required to\ndetermine what a useful grouping is. In this paper we propose an approach that\nis able to find groups of patients based on a small sample of positive examples\ngiven by a domain expert. Because of that, the approach relies on very limited\nefforts by the domain experts. The approach groups based on the activities and\ndiagnostic/billing codes within health pathways of patients. To define such a\ngrouping based on the sample of patients efficiently, frequent patterns of\nactivities are discovered and used to measure the similarity between the care\npathways of other patients to the patients in the sample group. This approach\nresults in an insightful definition of the group. The proposed approach is\nevaluated using several datasets obtained from a large university medical\ncenter. The evaluation shows F1-scores of around 0.7 for grouping kidney injury\nand around 0.6 for diabetes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:10:29 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Tabatabaei", "Seyed Amin", ""], ["Lu", "Xixi", ""], ["Hoogendoorn", "Mark", ""], ["Reijers", "Hajo A.", ""]]}, {"id": "1904.01883", "submitter": "Ivan Bravi", "authors": "Ivan Bravi and Simon Lucas and Diego Perez-Liebana and Jialin Liu", "title": "Rinascimento: Optimising Statistical Forward Planning Agents for Playing\n  Splendor", "comments": "Submitted to IEEE Conference on Games 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-based benchmarks have been playing an essential role in the development\nof Artificial Intelligence (AI) techniques. Providing diverse challenges is\ncrucial to push research toward innovation and understanding in modern\ntechniques. Rinascimento provides a parameterised partially-observable\nmultiplayer card-based board game, these parameters can easily modify the\nrules, objectives and items in the game. We describe the framework in all its\nfeatures and the game-playing challenge providing baseline game-playing AIs and\nanalysis of their skills. We reserve to agents' hyper-parameter tuning a\ncentral role in the experiments highlighting how it can heavily influence the\nperformance. The base-line agents contain several additional contribution to\nStatistical Forward Planning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:53:10 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bravi", "Ivan", ""], ["Lucas", "Simon", ""], ["Perez-Liebana", "Diego", ""], ["Liu", "Jialin", ""]]}, {"id": "1904.01933", "submitter": "Jinze Bai", "authors": "Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li,\n  Jun Gao", "title": "Personalized Bundle List Recommendation", "comments": "WWW2019, 11 pages", "journal-ref": null, "doi": "10.1145/3308558.3313568", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product bundling, offering a combination of items to customers, is one of the\nmarketing strategies commonly used in online e-commerce and offline retailers.\nA high-quality bundle generalizes frequent items of interest, and diversity\nacross bundles boosts the user-experience and eventually increases transaction\nvolume. In this paper, we formalize the personalized bundle list recommendation\nas a structured prediction problem and propose a bundle generation network\n(BGN), which decomposes the problem into quality/diversity parts by the\ndeterminantal point processes (DPPs). BGN uses a typical encoder-decoder\nframework with a proposed feature-aware softmax to alleviate the inadequate\nrepresentation of traditional softmax, and integrates the masked beam search\nand DPP selection to produce high-quality and diversified bundle list with an\nappropriate bundle size. We conduct extensive experiments on three public\ndatasets and one industrial dataset, including two generated from co-purchase\nrecords and the other two extracted from real-world online bundle services. BGN\nsignificantly outperforms the state-of-the-art methods in terms of quality,\ndiversity and response time over all datasets. In particular, BGN improves the\nprecision of the best competitors by 16\\% on average while maintaining the\nhighest diversity on four datasets, and yields a 3.85x improvement of response\ntime over the best competitors in the bundle list recommendation problem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:51:43 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bai", "Jinze", ""], ["Zhou", "Chang", ""], ["Song", "Junshuai", ""], ["Qu", "Xiaoru", ""], ["An", "Weiting", ""], ["Li", "Zhao", ""], ["Gao", "Jun", ""]]}, {"id": "1904.01957", "submitter": "Paul Todorov", "authors": "Paul Todorov", "title": "A Game of Dice: Machine Learning and the Question Concerning Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some practical and philosophical questions raised by the use of\nmachine learning in creative practice. Beyond the obvious problems regarding\nplagiarism and authorship, we argue that the novelty in AI Art relies mostly on\na narrow machine learning contribution : manifold approximation. Nevertheless,\nthis contribution creates a radical shift in the way we have to consider this\nmovement. Is this omnipotent tool a blessing or a curse for the artists?\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 09:37:44 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Todorov", "Paul", ""]]}, {"id": "1904.02057", "submitter": "Kaidi Xu", "authors": "Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan,\n  Chuang Gan, Xue Lin", "title": "Interpreting Adversarial Examples by Activation Promotion and\n  Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that convolutional neural networks (CNNs) are vulnerable\nto adversarial examples: images with imperceptible perturbations crafted to\nfool classifiers. However, interpretability of these perturbations is less\nexplored in the literature. This work aims to better understand the roles of\nadversarial perturbations and provide visual explanations from pixel, image and\nnetwork perspectives. We show that adversaries have a promotion-suppression\neffect (PSE) on neurons' activations and can be primarily categorized into\nthree types: i) suppression-dominated perturbations that mainly reduce the\nclassification score of the true label, ii) promotion-dominated perturbations\nthat focus on boosting the confidence of the target label, and iii) balanced\nperturbations that play a dual role in suppression and promotion. We also\nprovide image-level interpretability of adversarial examples. This links PSE of\npixel-level perturbations to class-specific discriminative image regions\nlocalized by class activation mapping (Zhou et al. 2016). Further, we examine\nthe adversarial effect through network dissection (Bau et al. 2017), which\noffers concept-level interpretability of hidden units. We show that there\nexists a tight connection between the units' sensitivity to adversarial attacks\nand their interpretability on semantic concepts. Lastly, we provide some new\ninsights from our interpretation to improve the adversarial robustness of\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:25:21 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:32:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Zhang", "Gaoyuan", ""], ["Sun", "Mengshu", ""], ["Zhao", "Pu", ""], ["Fan", "Quanfu", ""], ["Gan", "Chuang", ""], ["Lin", "Xue", ""]]}, {"id": "1904.02063", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch, Jack Jewson, Theodoros Damoulas", "title": "Generalized Variational Inference: Three arguments for deriving new\n  Posteriors", "comments": "103 pages, 23 figures (comprehensive revision of previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate an optimization-centric view on and introduce a novel\ngeneralization of Bayesian inference. Our inspiration is the representation of\nBayes' rule as infinite-dimensional optimization problem (Csiszar, 1975;\nDonsker and Varadhan; 1975, Zellner; 1988). First, we use it to prove an\noptimality result of standard Variational Inference (VI): Under the proposed\nview, the standard Evidence Lower Bound (ELBO) maximizing VI posterior is\npreferable to alternative approximations of the Bayesian posterior. Next, we\nargue for generalizing standard Bayesian inference. The need for this arises in\nsituations of severe misalignment between reality and three assumptions\nunderlying standard Bayesian inference: (1) Well-specified priors, (2)\nwell-specified likelihoods, (3) the availability of infinite computing power.\nOur generalization addresses these shortcomings with three arguments and is\ncalled the Rule of Three (RoT). We derive it axiomatically and recover existing\nposteriors as special cases, including the Bayesian posterior and its\napproximation by standard VI. In contrast, approximations based on alternative\nELBO-like objectives violate the axioms. Finally, we study a special case of\nthe RoT that we call Generalized Variational Inference (GVI). GVI posteriors\nare a large and tractable family of belief distributions specified by three\narguments: A loss, a divergence and a variational family. GVI posteriors have\nappealing properties, including consistency and an interpretation as\napproximate ELBO. The last part of the paper explores some attractive\napplications of GVI in popular machine learning models, including robustness\nand more appropriate marginals. After deriving black box inference schemes for\nGVI posteriors, their predictive performance is investigated on Bayesian Neural\nNetworks and Deep Gaussian Processes, where GVI can comprehensively improve\nupon existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:31:46 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 10:48:21 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 01:53:32 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 15:02:50 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Knoblauch", "Jeremias", ""], ["Jewson", "Jack", ""], ["Damoulas", "Theodoros", ""]]}, {"id": "1904.02235", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Christian Kroer, Adam Lerer", "title": "Robust Multi-agent Counterfactual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using logged data to make predictions about what\nwould happen if we changed the `rules of the game' in a multi-agent system.\nThis task is difficult because in many cases we observe actions individuals\ntake but not their private information or their full reward functions. In\naddition, agents are strategic, so when the rules change, they will also change\ntheir actions. Existing methods (e.g. structural estimation, inverse\nreinforcement learning) make counterfactual predictions by constructing a model\nof the game, adding the assumption that agents' behavior comes from optimizing\ngiven some goals, and then inverting observed actions to learn agent's\nunderlying utility function (a.k.a. type). Once the agent types are known,\nmaking counterfactual predictions amounts to solving for the equilibrium of the\ncounterfactual environment. This approach imposes heavy assumptions such as\nrationality of the agents being observed, correctness of the analyst's model of\nthe environment/parametric form of the agents' utility functions, and various\nother conditions to make point identification possible. We propose a method for\nanalyzing the sensitivity of counterfactual conclusions to violations of these\nassumptions. We refer to this method as robust multi-agent counterfactual\nprediction (RMAC). We apply our technique to investigating the robustness of\ncounterfactual claims for classic environments in market design: auctions,\nschool choice, and social choice. Importantly, we show RMAC can be used in\nregimes where point identification is impossible (e.g. those which have\nmultiple equilibria or non-injective maps from type distributions to outcomes).\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 20:49:37 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Kroer", "Christian", ""], ["Lerer", "Adam", ""]]}, {"id": "1904.02266", "submitter": "Maani Ghaffari Jadidi", "authors": "Maani Ghaffari, William Clark, Anthony Bloch, Ryan M. Eustice, Jessy\n  W. Grizzle", "title": "Continuous Direct Sparse Visual Odometry from RGB-D Images", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a novel formulation and evaluation of visual odometry\nfrom RGB-D images. Assuming a static scene, the developed theoretical framework\ngeneralizes the widely used direct energy formulation (photometric error\nminimization) technique for obtaining a rigid body transformation that aligns\ntwo overlapping RGB-D images to a continuous formulation. The continuity is\nachieved through functional treatment of the problem and representing the\nprocess models over RGB-D images in a reproducing kernel Hilbert space;\nconsequently, the registration is not limited to the specific image resolution\nand the framework is fully analytical with a closed-form derivation of the\ngradient. We solve the problem by maximizing the inner product between two\nfunctions defined over RGB-D images, while the continuous action of the rigid\nbody motion Lie group is captured through the integration of the flow in the\ncorresponding Lie algebra. Energy-based approaches have been extremely\nsuccessful and the developed framework in this paper shares many of their\ndesired properties such as the parallel structure on both CPUs and GPUs,\nsparsity, semi-dense tracking, avoiding explicit data association which is\ncomputationally expensive, and possible extensions to the simultaneous\nlocalization and mapping frameworks. The evaluations on experimental data and\ncomparison with the equivalent energy-based formulation of the problem confirm\nthe effectiveness of the proposed technique, especially, when the lack of\nstructure and texture in the environment is evident.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 23:25:01 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 03:08:07 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 23:21:18 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ghaffari", "Maani", ""], ["Clark", "William", ""], ["Bloch", "Anthony", ""], ["Eustice", "Ryan M.", ""], ["Grizzle", "Jessy W.", ""]]}, {"id": "1904.02293", "submitter": "Akshay Budhkar", "authors": "Akshay Budhkar, Krishnapriya Vishnubhotla, Safwan Hossain, Frank\n  Rudzicz", "title": "Generative Adversarial Networks for text using word2vec intermediaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown considerable success,\nespecially in the realistic generation of images. In this work, we apply\nsimilar techniques for the generation of text. We propose a novel approach to\nhandle the discrete nature of text, during training, using word embeddings. Our\nmethod is agnostic to vocabulary size and achieves competitive results relative\nto methods with various discrete gradient estimators.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:17:29 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Budhkar", "Akshay", ""], ["Vishnubhotla", "Krishnapriya", ""], ["Hossain", "Safwan", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1904.02303", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch", "title": "Robust Deep Gaussian Processes", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an in-depth overview over the implications and novelty\nGeneralized Variational Inference (GVI) (Knoblauch et al., 2019) brings to Deep\nGaussian Processes (DGPs) (Damianou & Lawrence, 2013). Specifically, robustness\nto model misspecification as well as principled alternatives for uncertainty\nquantification are motivated with an information-geometric view. These\nmodifications have clear interpretations and can be implemented in less than\n100 lines of Python code. Most importantly, the corresponding empirical results\nshow that DGPs can greatly benefit from the presented enhancements.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:37:54 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 02:05:44 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Knoblauch", "Jeremias", ""]]}, {"id": "1904.02341", "submitter": "Xin Huang", "authors": "Xin Huang, Sungkweon Hong, Andreas Hofmann, Brian C. Williams", "title": "Online Risk-Bounded Motion Planning for Autonomous Vehicles in Dynamic\n  Environments", "comments": "Accepted at ICAPS'19. 10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial challenge to efficient and robust motion planning for autonomous\nvehicles is understanding the intentions of the surrounding agents. Ignoring\nthe intentions of the other agents in dynamic environments can lead to risky or\nover-conservative plans. In this work, we model the motion planning problem as\na partially observable Markov decision process (POMDP) and propose an online\nsystem that combines an intent recognition algorithm and a POMDP solver to\ngenerate risk-bounded plans for the ego vehicle navigating with a number of\ndynamic agent vehicles. The intent recognition algorithm predicts the\nprobabilistic hybrid motion states of each agent vehicle over a finite horizon\nusing Bayesian filtering and a library of pre-learned maneuver motion models.\nWe update the POMDP model with the intent recognition results in real time and\nsolve it using a heuristic search algorithm which produces policies with\nupper-bound guarantees on the probability of near colliding with other dynamic\nagents. We demonstrate that our system is able to generate better motion plans\nin terms of efficiency and safety in a number of challenging environments\nincluding unprotected intersection left turns and lane changes as compared to\nthe baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 04:19:38 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Huang", "Xin", ""], ["Hong", "Sungkweon", ""], ["Hofmann", "Andreas", ""], ["Williams", "Brian C.", ""]]}, {"id": "1904.02390", "submitter": "Jiachen Li", "authors": "Jiachen Li and Hengbo Ma and Masayoshi Tomizuka", "title": "Interaction-aware Multi-agent Tracking and Probabilistic Behavior\n  Prediction via Adversarial Learning", "comments": "Accepted by 2019 International Conference on Robotics and Automation\n  (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to enable high-quality decision making and motion planning of\nintelligent systems such as robotics and autonomous vehicles, accurate\nprobabilistic predictions for surrounding interactive objects is a crucial\nprerequisite. Although many research studies have been devoted to making\npredictions on a single entity, it remains an open challenge to forecast future\nbehaviors for multiple interactive agents simultaneously. In this work, we take\nadvantage of the Generative Adversarial Network (GAN) due to its capability of\ndistribution learning and propose a generic multi-agent probabilistic\nprediction and tracking framework which takes the interactions among multiple\nentities into account, in which all the entities are treated as a whole.\nHowever, since GAN is very hard to train, we make an empirical research and\npresent the relationship between training performance and hyperparameter values\nwith a numerical case study. The results imply that the proposed model can\ncapture both the mean, variance and multi-modalities of the groundtruth\ndistribution. Moreover, we apply the proposed approach to a real-world task of\nvehicle behavior prediction to demonstrate its effectiveness and accuracy. The\nresults illustrate that the proposed model trained by adversarial learning can\nachieve a better prediction performance than other state-of-the-art models\ntrained by traditional supervised learning which maximizes the data likelihood.\nThe well-trained model can also be utilized as an implicit proposal\ndistribution for particle filtered based Bayesian state estimation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 07:41:07 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1904.02418", "submitter": "Jing Qian", "authors": "Jing Qian, Mai ElSherief, Elizabeth Belding, William Yang Wang", "title": "Learning to Decipher Hate Symbols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing computational models to understand hate speech typically frame the\nproblem as a simple classification task, bypassing the understanding of hate\nsymbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we\npropose a novel task of deciphering hate symbols. To do this, we leverage the\nUrban Dictionary and collected a new, symbol-rich Twitter corpus of hate\nspeech. We investigate neural network latent context models for deciphering\nhate symbols. More specifically, we study Sequence-to-Sequence models and show\nhow they are able to crack the ciphers based on context. Furthermore, we\npropose a novel Variational Decipher and show how it can generalize better to\nunseen hate symbols in a more challenging testing setting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:11:24 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Qian", "Jing", ""], ["ElSherief", "Mai", ""], ["Belding", "Elizabeth", ""], ["Wang", "William Yang", ""]]}, {"id": "1904.02435", "submitter": "Kai Olav Ellefsen", "authors": "Kai Olav Ellefsen and Jim Torresen", "title": "Self-Adapting Goals Allow Transfer of Predictive Models to New Tasks", "comments": "Accepted for publication in the proceedings of the 2019 Symposium of\n  the Norwegian AI Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing challenge in Reinforcement Learning is enabling agents to\nlearn a model of their environment which can be transferred to solve other\nproblems in a world with the same underlying rules. One reason this is\ndifficult is the challenge of learning accurate models of an environment. If\nsuch a model is inaccurate, the agent's plans and actions will likely be\nsub-optimal, and likely lead to the wrong outcomes. Recent progress in\nmodel-based reinforcement learning has improved the ability for agents to learn\nand use predictive models. In this paper, we extend a recent deep learning\narchitecture which learns a predictive model of the environment that aims to\npredict only the value of a few key measurements, which are be indicative of an\nagent's performance. Predicting only a few measurements rather than the entire\nfuture state of an environment makes it more feasible to learn a valuable\npredictive model. We extend this predictive model with a small, evolving neural\nnetwork that suggests the best goals to pursue in the current state. We\ndemonstrate that this allows the predictive model to transfer to new scenarios\nwhere goals are different, and that the adaptive goals can even adjust agent\nbehavior on-line, changing its strategy to fit the current context.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:52:18 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 11:36:20 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Ellefsen", "Kai Olav", ""], ["Torresen", "Jim", ""]]}, {"id": "1904.02495", "submitter": "Joani Mitro", "authors": "John Mitros and Brian Mac Namee", "title": "A Categorisation of Post-hoc Explanations for Predictive Models", "comments": "5 pages, 3 figures, AAAI 2019 Spring Symposia (#SSS19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of machine learning based predictive models in modern society\nnaturally leads people to ask how trustworthy those models are? In predictive\nmodeling, it is quite common to induce a trade-off between accuracy and\ninterpretability. For instance, doctors would like to know how effective some\ntreatment will be for a patient or why the model suggested a particular\nmedication for a patient exhibiting those symptoms? We acknowledge that the\nnecessity for interpretability is a consequence of an incomplete formalisation\nof the problem, or more precisely of multiple meanings adhered to a particular\nconcept. For certain problems, it is not enough to get the answer (what), the\nmodel also has to provide an explanation of how it came to that conclusion\n(why), because a correct prediction, only partially solves the original\nproblem. In this article we extend existing categorisation of techniques to aid\nmodel interpretability and test this categorisation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:34:05 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Mitros", "John", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1904.02523", "submitter": "Sandro Sozzo", "authors": "Sandro Sozzo", "title": "Explaining versus Describing Human Decisions. Hilbert Space Structures\n  in Decision Theory", "comments": "16 pages, 1 figure, LateX", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive success of quantum structures to model long-standing\nhuman judgement and decision puzzles, the {\\it quantum cognition research\nprogramme} still faces challenges about its explanatory power. Indeed, quantum\nmodels introduce new parameters, which may fit empirical data without\nnecessarily explaining them. Also, one wonders whether more general\nnon-classical structures are better equipped to model cognitive phenomena. In\nthis paper, we provide a {\\it realistic-operational foundation of decision\nprocesses} using a known decision-making puzzle, the {\\it Ellsberg paradox}, as\na case study. Then, we elaborate a novel representation of the Ellsberg\ndecision situation applying standard quantum correspondence rules which map\nrealistic-operational entities into quantum mathematical terms. This result\nopens the way towards an independent, foundational rather than\nphenomenological, motivation for a general use of quantum Hilbert space\nstructures in human cognition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:53:35 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Sozzo", "Sandro", ""]]}, {"id": "1904.02547", "submitter": "Lisa Beinborn", "authors": "Lisa Beinborn, Samira Abnar, Rochelle Choenni", "title": "Robust Evaluation of Language-Brain Encoding Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Language-brain encoding experiments evaluate the ability of language models\nto predict brain responses elicited by language stimuli. The evaluation\nscenarios for this task have not yet been standardized which makes it difficult\nto compare and interpret results. We perform a series of evaluation experiments\nwith a consistent encoding setup and compute the results for multiple fMRI\ndatasets. In addition, we test the sensitivity of the evaluation measures to\nrandomized data and analyze the effect of voxel selection methods. Our\nexperimental framework is publicly available to make modelling decisions more\ntransparent and support reproducibility for future comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:34:18 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Beinborn", "Lisa", ""], ["Abnar", "Samira", ""], ["Choenni", "Rochelle", ""]]}, {"id": "1904.02579", "submitter": "Rogerio Bonatti", "authors": "Mirko Gschwindt, Efe Camci, Rogerio Bonatti, Wenshan Wang, Erdal\n  Kayacan, Sebastian Scherer", "title": "Can a Robot Become a Movie Director? Learning Artistic Principles for\n  Aerial Cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial filming is constantly gaining importance due to the recent advances in\ndrone technology. It invites many intriguing, unsolved problems at the\nintersection of aesthetical and scientific challenges. In this work, we propose\na deep reinforcement learning agent which supervises motion planning of a\nfilming drone by making desirable shot mode selections based on aesthetical\nvalues of video shots. Unlike most of the current state-of-the-art approaches\nthat require explicit guidance by a human expert, our drone learns how to make\nfavorable viewpoint selections by experience. We propose a learning scheme that\nexploits aesthetical features of retrospective shots in order to extract a\ndesirable policy for better prospective shots. We train our agent in realistic\nAirSim simulations using both a hand-crafted reward function as well as reward\nfrom direct human input. We then deploy the same agent on a real DJI M210 drone\nin order to test the generalization capability of our approach to real world\nconditions. To evaluate the success of our approach in the end, we conduct a\ncomprehensive user study in which participants rate the shot quality of our\nmethods. Videos of the system in action can be seen at\nhttps://youtu.be/qmVw6mfyEmw.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:30:09 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 15:45:57 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Gschwindt", "Mirko", ""], ["Camci", "Efe", ""], ["Bonatti", "Rogerio", ""], ["Wang", "Wenshan", ""], ["Kayacan", "Erdal", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1904.02628", "submitter": "Silvio Olivastri", "authors": "Silvio Olivastri, Gurkirt Singh, Fabio Cuzzolin", "title": "End-to-End Video Captioning", "comments": "Accepted at Large Scale Holistic Video Understanding, ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building correspondences across different modalities, such as video and\nlanguage, has recently become critical in many visual recognition applications,\nsuch as video captioning. Inspired by machine translation, recent models tackle\nthis task using an encoder-decoder strategy. The (video) encoder is\ntraditionally a Convolutional Neural Network (CNN), while the decoding (for\nlanguage generation) is done using a Recurrent Neural Network (RNN). Current\nstate-of-the-art methods, however, train encoder and decoder separately. CNNs\nare pretrained on object and/or action recognition tasks and used to encode\nvideo-level features. The decoder is then optimised on such static features to\ngenerate the video's description. This disjoint setup is arguably sub-optimal\nfor input (video) to output (description) mapping. In this work, we propose to\noptimise both encoder and decoder simultaneously in an end-to-end fashion. In a\ntwo-stage training setting, we first initialise our architecture using\npre-trained encoders and decoders -- then, the entire network is trained\nend-to-end in a fine-tuning stage to learn the most relevant features for video\ncaption generation. In our experiments, we use GoogLeNet and\nInception-ResNet-v2 as encoders and an original Soft-Attention (SA-) LSTM as a\ndecoder. Analogously to gains observed in other computer vision problems, we\nshow that end-to-end training significantly improves over the traditional,\ndisjoint training process. We evaluate our End-to-End (EtENet) Networks on the\nMicrosoft Research Video Description (MSVD) and the MSR Video to Text (MSR-VTT)\nbenchmark datasets, showing how EtENet achieves state-of-the-art performance\nacross the board.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:57:23 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:28:48 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Olivastri", "Silvio", ""], ["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1904.02642", "submitter": "Michael Volpp", "authors": "Michael Volpp, Lukas P. Fr\\\"ohlich, Kirsten Fischer, Andreas Doerr,\n  Stefan Falkner, Frank Hutter, Christian Daniel", "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across tasks to improve data-efficiency is one of the\nopen key challenges in the field of global black-box optimization. Readily\navailable algorithms are typically designed to be universal optimizers and,\ntherefore, often suboptimal for specific tasks. We propose a novel transfer\nlearning method to obtain customized optimizers within the well-established\nframework of Bayesian optimization, allowing our algorithm to utilize the\nproven generalization capabilities of Gaussian processes. Using reinforcement\nlearning to meta-train an acquisition function (AF) on a set of related tasks,\nthe proposed method learns to extract implicit structural information and to\nexploit it for improved data-efficiency. We present experiments on a\nsimulation-to-real transfer task as well as on several synthetic functions and\non two hyperparameter search problems. The results show that our algorithm (1)\nautomatically identifies structural properties of objective functions from\navailable source tasks or simulations, (2) performs favourably in settings with\nboth scarse and abundant source data, and (3) falls back to the performance\nlevel of general AFs if no particular structure is present.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 16:27:06 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 15:29:46 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 13:05:14 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2019 09:58:29 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2020 13:24:57 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Volpp", "Michael", ""], ["Fr\u00f6hlich", "Lukas P.", ""], ["Fischer", "Kirsten", ""], ["Doerr", "Andreas", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""], ["Daniel", "Christian", ""]]}, {"id": "1904.02688", "submitter": "Ralph Abboud", "authors": "Ralph Abboud, Ismail Ilkan Ceylan, Thomas Lukasiewicz", "title": "Learning to Reason: Leveraging Neural Networks for Approximate DNF\n  Counting", "comments": "To appear in Proceedings of the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20). Code and data available at:\n  https://github.com/ralphabb/NeuralDNF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted model counting (WMC) has emerged as a prevalent approach for\nprobabilistic inference. In its most general form, WMC is #P-hard. Weighted DNF\ncounting (weighted #DNF) is a special case, where approximations with\nprobabilistic guarantees are obtained in O(nm), where n denotes the number of\nvariables, and m the number of clauses of the input DNF, but this is not\nscalable in practice. In this paper, we propose a neural model counting\napproach for weighted #DNF that combines approximate model counting with deep\nlearning, and accurately approximates model counts in linear time when width is\nbounded. We conduct experiments to validate our method, and show that our model\nlearns and generalizes very well to large-scale #DNF instances.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:45:45 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 17:27:08 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 20:27:13 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 18:03:23 GMT"}, {"version": "v5", "created": "Wed, 29 Jan 2020 21:39:41 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Abboud", "Ralph", ""], ["Ceylan", "Ismail Ilkan", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "1904.02697", "submitter": "Lucio Vismari", "authors": "A. M. Nascimento, L. F. Vismari, C. B. S. T. Molina, P.S. Cugnasca,\n  J.B. Camargo Jr., J.R. de Almeida Jr., R. Inam, E. Fersman, M. V. Marquezini,\n  A. Y. Hata", "title": "A Systematic Literature Review about the impact of Artificial\n  Intelligence on Autonomous Vehicle Safety", "comments": "32 pages, 5 figures, 9 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Vehicles (AV) are expected to bring considerable benefits to\nsociety, such as traffic optimization and accidents reduction. They rely\nheavily on advances in many Artificial Intelligence (AI) approaches and\ntechniques. However, while some researchers in this field believe AI is the\ncore element to enhance safety, others believe AI imposes new challenges to\nassure the safety of these new AI-based systems and applications. In this\nnon-convergent context, this paper presents a systematic literature review to\npaint a clear picture of the state of the art of the literature in AI on AV\nsafety. Based on an initial sample of 4870 retrieved papers, 59 studies were\nselected as the result of the selection criteria detailed in the paper. The\nshortlisted studies were then mapped into six categories to answer the proposed\nresearch questions. An AV system model was proposed and applied to orient the\ndiscussions about the SLR findings. As a main result, we have reinforced our\npreliminary observation about the necessity of considering a serious safety\nagenda for the future studies on AI-based AV systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:55:36 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Nascimento", "A. M.", ""], ["Vismari", "L. F.", ""], ["Molina", "C. B. S. T.", ""], ["Cugnasca", "P. S.", ""], ["Camargo", "J. B.", "Jr."], ["Almeida", "J. R. de", "Jr."], ["Inam", "R.", ""], ["Fersman", "E.", ""], ["Marquezini", "M. V.", ""], ["Hata", "A. Y.", ""]]}, {"id": "1904.02698", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic", "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order\n  Tensor", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent findings indicate that over-parametrization, while crucial for\nsuccessfully training deep neural networks, also introduces large amounts of\nredundancy. Tensor methods have the potential to efficiently parametrize\nover-complete representations by leveraging this redundancy. In this paper, we\npropose to fully parametrize Convolutional Neural Networks (CNNs) with a single\nhigh-order, low-rank tensor. Previous works on network tensorization have\nfocused on parametrizing individual layers (convolutional or fully connected)\nonly, and perform the tensorization layer-by-layer separately. In contrast, we\npropose to jointly capture the full structure of a neural network by\nparametrizing it with a single high-order tensor, the modes of which represent\neach of the architectural design parameters of the network (e.g. number of\nconvolutional blocks, depth, number of stacks, input features, etc). This\nparametrization allows to regularize the whole network and drastically reduce\nthe number of parameters. Our model is end-to-end trainable and the low-rank\nstructure imposed on the weight tensor acts as an implicit regularization. We\nstudy the case of networks with rich structure, namely Fully Convolutional\nNetworks (FCNs), which we propose to parametrize with a single 8th-order\ntensor. We show that our approach can achieve superior performance with small\ncompression rates, and attain high compression rates with negligible drop in\naccuracy for the challenging task of human pose estimation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:55:37 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kossaifi", "Jean", ""], ["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.02792", "submitter": "Hugh Zhang", "authors": "Tatsunori B. Hashimoto, Hugh Zhang, Percy Liang", "title": "Unifying Human and Statistical Evaluation for Natural Language\n  Generation", "comments": "NAACL Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we measure whether a natural language generation system produces both\nhigh quality and diverse outputs? Human evaluation captures quality but not\ndiversity, as it does not catch models that simply plagiarize from the training\nset. On the other hand, statistical evaluation (i.e., perplexity) captures\ndiversity but not quality, as models that occasionally emit low quality samples\nwould be insufficiently penalized. In this paper, we propose a unified\nframework which evaluates both diversity and quality, based on the optimal\nerror rate of predicting whether a sentence is human- or machine-generated. We\ndemonstrate that this error rate can be efficiently estimated by combining\nhuman and statistical evaluation, using an evaluation metric which we call\nHUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects\ndiversity defects which fool pure human evaluation and that (ii) techniques\nsuch as annealing for improving quality actually decrease HUSE due to decreased\ndiversity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:03:34 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Zhang", "Hugh", ""], ["Liang", "Percy", ""]]}, {"id": "1904.02793", "submitter": "Ashutosh Modi", "authors": "Pierre Colombo and Wojciech Witon and Ashutosh Modi and James Kennedy\n  and Mubbasir Kapadia", "title": "Affect-Driven Dialog Generation", "comments": "8+2 Pages, Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of current systems for end-to-end dialog generation focus on\nresponse quality without an explicit control over the affective content of the\nresponses. In this paper, we present an affect-driven dialog system, which\ngenerates emotional responses in a controlled manner using a continuous\nrepresentation of emotions. The system achieves this by modeling emotions at a\nword and sequence level using: (1) a vector representation of the desired\nemotion, (2) an affect regularizer, which penalizes neutral words, and (3) an\naffect sampling method, which forces the neural network to generate diverse\nwords that are emotionally relevant. During inference, we use a reranking\nprocedure that aims to extract the most emotionally relevant responses using a\nhuman-in-the-loop optimization process. We study the performance of our system\nin terms of both quantitative (BLEU score and response diversity), and\nqualitative (emotional appropriateness) measures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:05:13 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Colombo", "Pierre", ""], ["Witon", "Wojciech", ""], ["Modi", "Ashutosh", ""], ["Kennedy", "James", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.02811", "submitter": "Du Tran", "authors": "Du Tran and Heng Wang and Lorenzo Torresani and Matt Feiszli", "title": "Video Classification with Channel-Separated Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group convolution has been shown to offer great computational savings in\nvarious 2D convolutional architectures for image classification. It is natural\nto ask: 1) if group convolution can help to alleviate the high computational\ncost of video classification networks; 2) what factors matter the most in 3D\ngroup convolutional networks; and 3) what are good computation/accuracy\ntrade-offs with 3D group convolutional networks.\n  This paper studies the effects of different design choices in 3D group\nconvolutional networks for video classification. We empirically demonstrate\nthat the amount of channel interactions plays an important role in the accuracy\nof 3D group convolutional networks. Our experiments suggest two main findings.\nFirst, it is a good practice to factorize 3D convolutions by separating channel\ninteractions and spatiotemporal interactions as this leads to improved accuracy\nand lower computational cost. Second, 3D channel-separated convolutions provide\na form of regularization, yielding lower training accuracy but higher test\naccuracy compared to 3D convolutions. These two empirical findings lead us to\ndesign an architecture -- Channel-Separated Convolutional Network (CSN) --\nwhich is simple, efficient, yet accurate. On Sports1M, Kinetics, and\nSomething-Something, our CSNs are comparable with or better than the\nstate-of-the-art while being 2-3 times more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:28:24 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 00:15:33 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 17:04:52 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 22:30:49 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Tran", "Du", ""], ["Wang", "Heng", ""], ["Torresani", "Lorenzo", ""], ["Feiszli", "Matt", ""]]}, {"id": "1904.02815", "submitter": "Ashutosh Modi", "authors": "Pooja Chitkara, Ashutosh Modi, Pravalika Avvaru, Sepehr Janghorbani,\n  Mubbasir Kapadia", "title": "Topic Spotting using Hierarchical Networks with Self Attention", "comments": "5+2 Pages, Accepted at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success of deep learning techniques have renewed the interest in development\nof dialogue systems. However, current systems struggle to have consistent long\nterm conversations with the users and fail to build rapport. Topic spotting,\nthe task of automatically inferring the topic of a conversation, has been shown\nto be helpful in making a dialog system more engaging and efficient. We propose\na hierarchical model with self attention for topic spotting. Experiments on the\nSwitchboard corpus show the superior performance of our model over previously\nproposed techniques for topic spotting and deep models for text classification.\nAdditionally, in contrast to offline processing of dialog, we also analyze the\nperformance of our model in a more realistic setting i.e. in an online setting\nwhere the topic is identified in real time as the dialog progresses. Results\nshow that our model is able to generalize even with limited information in the\nonline setting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:54:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chitkara", "Pooja", ""], ["Modi", "Ashutosh", ""], ["Avvaru", "Pravalika", ""], ["Janghorbani", "Sepehr", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.02830", "submitter": "Konstantinos Krommydas", "authors": "Ruchira Sasanka, Konstantinos Krommydas", "title": "An Evolutionary Framework for Automatic and Guided Discovery of\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Automatic Algorithm Discoverer (AAD), an evolutionary\nframework for synthesizing programs of high complexity. To guide evolution,\nprior evolutionary algorithms have depended on fitness (objective) functions,\nwhich are challenging to design. To make evolutionary progress, instead, AAD\nemploys Problem Guided Evolution (PGE), which requires introduction of a group\nof problems together. With PGE, solutions discovered for simpler problems are\nused to solve more complex problems in the same group. PGE also enables several\nnew evolutionary strategies, and naturally yields to High-Performance Computing\n(HPC) techniques.\n  We find that PGE and related evolutionary strategies enable AAD to discover\nalgorithms of similar or higher complexity relative to the state-of-the-art.\nSpecifically, AAD produces Python code for 29 array/vector problems ranging\nfrom min, max, reverse, to more challenging problems like sorting and\nmatrix-vector multiplication. Additionally, we find that AAD shows adaptability\nto constrained environments/inputs and demonstrates outside-of-the-box problem\nsolving abilities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 00:03:23 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Sasanka", "Ruchira", ""], ["Krommydas", "Konstantinos", ""]]}, {"id": "1904.02851", "submitter": "Aamodh Suresh", "authors": "Aamodh Suresh and Sonia Martinez", "title": "Planning under non-rational perception of uncertain spatial costs", "comments": "12 pages and 10 figures. This revision adds more explanation and\n  clearer figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the design of risk-perception-aware motion-planning\nstrategies that incorporate non-rational perception of risks associated with\nuncertain spatial costs. Our proposed method employs the Cumulative Prospect\nTheory (CPT) to generate a perceived risk map over a given environment.\nCPT-like perceived risks and path-length metrics are then combined to define a\ncost function that is compliant with the requirements of asymptotic optimality\nof sampling-based motion planners (RRT*). The modeling power of CPT is\nillustrated in theory and in simulation, along with a comparison to other risk\nperception models like Conditional Value at Risk (CVaR). Theoretically, we\ndefine a notion of expressiveness for a risk perception model and show that\nCPT's is higher than that of CVaR and expected risk. We then show that this\nexpressiveness translates to our path planning setting, where we observe that a\nplanner equipped with CPT together with a simultaneous perturbation stochastic\napproximation (SPSA) method can better approximate arbitrary paths in an\nenvironment. Additionally, we show in simulation that our planner captures a\nrich set of meaningful paths, representative of different risk perceptions in a\ncustom environment. We then compare the performance of our planner with T-RRT*\n(a planner for continuous cost spaces) and Risk-RRT* (a risk-aware planner for\ndynamic human obstacles) through simulations in cluttered and dynamic\nenvironments respectively, showing the advantage of our proposed planner.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 02:42:24 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 03:14:50 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 03:00:21 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 20:32:34 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Suresh", "Aamodh", ""], ["Martinez", "Sonia", ""]]}, {"id": "1904.02856", "submitter": "Takuma Ebisu", "authors": "Takuma Ebisu and Ryutaro Ichise", "title": "Graph Pattern Entity Ranking Model for Knowledge Graph Completion", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have evolved rapidly in recent years and their usefulness\nhas been demonstrated in many artificial intelligence tasks. However, knowledge\ngraphs often have lots of missing facts. To solve this problem, many knowledge\ngraph embedding models have been developed to populate knowledge graphs and\nthese have shown outstanding performance. However, knowledge graph embedding\nmodels are so-called black boxes, and the user does not know how the\ninformation in a knowledge graph is processed and the models can be difficult\nto interpret. In this paper, we utilize graph patterns in a knowledge graph to\novercome such problems. Our proposed model, the {\\it graph pattern entity\nranking model} (GRank), constructs an entity ranking system for each graph\npattern and evaluates them using a ranking measure. By doing so, we can find\ngraph patterns which are useful for predicting facts. Then, we perform link\nprediction tasks on standard datasets to evaluate our GRank method. We show\nthat our approach outperforms other state-of-the-art approaches such as ComplEx\nand TorusE for standard metrics such as HITS@{\\it n} and MRR. Moreover, our\nmodel is easily interpretable because the output facts are described by graph\npatterns.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 03:17:00 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ebisu", "Takuma", ""], ["Ichise", "Ryutaro", ""]]}, {"id": "1904.02868", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani and James Zou", "title": "Data Shapley: Equitable Valuation of Data for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data becomes the fuel driving technological and economic growth, a\nfundamental challenge is how to quantify the value of data in algorithmic\npredictions and decisions. For example, in healthcare and consumer markets, it\nhas been suggested that individuals should be compensated for the data that\nthey generate, but it is not clear what is an equitable valuation for\nindividual data. In this work, we develop a principled framework to address\ndata valuation in the context of supervised machine learning. Given a learning\nalgorithm trained on $n$ data points to produce a predictor, we propose data\nShapley as a metric to quantify the value of each training datum to the\npredictor performance. Data Shapley value uniquely satisfies several natural\nproperties of equitable data valuation. We develop Monte Carlo and\ngradient-based methods to efficiently estimate data Shapley values in practical\nsettings where complex learning algorithms, including neural networks, are\ntrained on large datasets. In addition to being equitable, extensive\nexperiments across biomedical, image and synthetic data demonstrate that data\nShapley has several other benefits: 1) it is more powerful than the popular\nleave-one-out or leverage score in providing insight on what data is more\nvaluable for a given learning task; 2) low Shapley value data effectively\ncapture outliers and corruptions; 3) high Shapley value data inform what type\nof new data to acquire to improve the predictor.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 04:54:10 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 08:10:40 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Zou", "James", ""]]}, {"id": "1904.02873", "submitter": "Ga Wu", "authors": "Ga Wu, Buser Say and Scott Sanner", "title": "Scalable Planning with Deep Neural Network Learned Transition Models", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world planning problems with factored, mixed discrete and\ncontinuous state and action spaces such as Reservoir Control, Heating\nVentilation, and Air Conditioning, and Navigation domains, it is difficult to\nobtain a model of the complex nonlinear dynamics that govern state evolution.\nHowever, the ubiquity of modern sensors allows us to collect large quantities\nof data from each of these complex systems and build accurate, nonlinear deep\nneural network models of their state transitions. But there remains one major\nproblem for the task of control -- how can we plan with deep network learned\ntransition models without resorting to Monte Carlo Tree Search and other\nblack-box transition model techniques that ignore model structure and do not\neasily extend to mixed discrete and continuous domains? In this paper, we\nintroduce two types of nonlinear planning methods that can leverage deep neural\nnetwork learned transition models: Hybrid Deep MILP Planner (HD-MILP-Plan) and\nTensorflow Planner (TF-Plan). In HD-MILP-Plan, we make the critical observation\nthat the Rectified Linear Unit transfer function for deep networks not only\nallows faster convergence of model learning, but also permits a direct\ncompilation of the deep network transition model to a Mixed-Integer Linear\nProgram encoding. Further, we identify deep network specific optimizations for\nHD-MILP-Plan that improve performance over a base encoding and show that we can\nplan optimally with respect to the learned deep networks. In TF-Plan, we take\nadvantage of the efficiency of auto-differentiation tools and GPU-based\ncomputation where we encode a subclass of purely continuous planning problems\nas Recurrent Neural Networks and directly optimize the actions through\nbackpropagation. We compare both planners and show that TF-Plan is able to\napproximate the optimal plans found by HD-MILP-Plan in less computation time...\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:21:46 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:02:55 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 16:18:49 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 05:24:47 GMT"}, {"version": "v5", "created": "Wed, 15 Jul 2020 02:21:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wu", "Ga", ""], ["Say", "Buser", ""], ["Sanner", "Scott", ""]]}, {"id": "1904.03008", "submitter": "Yunlong Liu", "authors": "Yunlong Liu, Jianyang Zheng", "title": "Combining Offline Models and Online Monte-Carlo Tree Search for Planning\n  from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning in stochastic and partially observable environments is a central\nissue in artificial intelligence. One commonly used technique for solving such\na problem is by constructing an accurate model firstly. Although some recent\napproaches have been proposed for learning optimal behaviour under model\nuncertainty, prior knowledge about the environment is still needed to guarantee\nthe performance of the proposed algorithms. With the benefits of the Predictive\nState Representations~(PSRs) approach for state representation and model\nprediction, in this paper, we introduce an approach for planning from scratch,\nwhere an offline PSR model is firstly learned and then combined with online\nMonte-Carlo tree search for planning with model uncertainty. By comparing with\nthe state-of-the-art approach of planning with model uncertainty, we\ndemonstrated the effectiveness of the proposed approaches along with the proof\nof their convergence. The effectiveness and scalability of our proposed\napproach are also tested on the RockSample problem, which are infeasible for\nthe state-of-the-art BA-POMDP based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 11:57:41 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liu", "Yunlong", ""], ["Zheng", "Jianyang", ""]]}, {"id": "1904.03084", "submitter": "Lukas Schmelzeisen", "authors": "Ipek Baris and Lukas Schmelzeisen and Steffen Staab", "title": "CLEARumor at SemEval-2019 Task 7: ConvoLving ELMo Against Rumors", "comments": "5 pages, 2 figures, 3 tables. Accepted for publication at\n  SemEval@NAACL-HLT 2019", "journal-ref": "SemEval@NAACL-HLT (2019) 1105-1109", "doi": "10.18653/v1/S19-2193", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to SemEval-2019 Task 7: RumourEval:\nDetermining Rumor Veracity and Support for Rumors. We participated in both\nsubtasks. The goal of subtask A is to classify the type of interaction between\na rumorous social media post and a reply post as support, query, deny, or\ncomment. The goal of subtask B is to predict the veracity of a given rumor. For\nsubtask A, we implement a CNN-based neural architecture using ELMo embeddings\nof post text combined with auxiliary features and achieve a F1-score of 44.6%.\nFor subtask B, we employ a MLP neural network leveraging our estimates for\nsubtask A and achieve a F1-score of 30.1% (second place in the competition). We\nprovide results and analysis of our system performance and present ablation\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:25:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Baris", "Ipek", ""], ["Schmelzeisen", "Lukas", ""], ["Staab", "Steffen", ""]]}, {"id": "1904.03092", "submitter": "Zhaopeng Tu", "authors": "Jie Hao and Xing Wang and Baosong Yang and Longyue Wang and Jinfeng\n  Zhang and Zhaopeng Tu", "title": "Modeling Recurrence for Transformer", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Transformer model that is based solely on attention mechanisms,\nhas advanced the state-of-the-art on various machine translation tasks.\nHowever, recent studies reveal that the lack of recurrence hinders its further\nimprovement of translation capacity. In response to this problem, we propose to\ndirectly model recurrence for Transformer with an additional recurrence\nencoder. In addition to the standard recurrent neural network, we introduce a\nnovel attentive recurrent network to leverage the strengths of both attention\nand recurrent networks. Experimental results on the widely-used WMT14\nEnglish-German and WMT17 Chinese-English translation tasks demonstrate the\neffectiveness of the proposed approach. Our studies also reveal that the\nproposed model benefits from a short-cut that bridges the source and target\nsequences with a single recurrent layer, which outperforms its deep\ncounterpart.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:40:22 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hao", "Jie", ""], ["Wang", "Xing", ""], ["Yang", "Baosong", ""], ["Wang", "Longyue", ""], ["Zhang", "Jinfeng", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03100", "submitter": "Zhaopeng Tu", "authors": "Jian Li and Baosong Yang and Zi-Yi Dou and Xing Wang and Michael R.\n  Lyu and Zhaopeng Tu", "title": "Information Aggregation for Multi-Head Attention with\n  Routing-by-Agreement", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attention is appealing for its ability to jointly extract\ndifferent types of information from multiple representation subspaces.\nConcerning the information aggregation, a common practice is to use a\nconcatenation followed by a linear transformation, which may not fully exploit\nthe expressiveness of multi-head attention. In this work, we propose to improve\nthe information aggregation for multi-head attention with a more powerful\nrouting-by-agreement algorithm. Specifically, the routing algorithm iteratively\nupdates the proportion of how much a part (i.e. the distinct information\nlearned from a specific subspace) should be assigned to a whole (i.e. the final\noutput representation), based on the agreement between parts and wholes.\nExperimental results on linguistic probing tasks and machine translation tasks\nprove the superiority of the advanced information aggregation over the standard\nlinear transformation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:52:28 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Li", "Jian", ""], ["Yang", "Baosong", ""], ["Dou", "Zi-Yi", ""], ["Wang", "Xing", ""], ["Lyu", "Michael R.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03107", "submitter": "Zhaopeng Tu", "authors": "Baosong Yang and Longyue Wang and Derek Wong and Lidia S. Chao and\n  Zhaopeng Tu", "title": "Convolutional Self-Attention Networks", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention networks (SANs) have drawn increasing interest due to their\nhigh parallelization in computation and flexibility in modeling dependencies.\nSANs can be further enhanced with multi-head attention by allowing the model to\nattend to information from different representation subspaces. In this work, we\npropose novel convolutional self-attention networks, which offer SANs the\nabilities to 1) strengthen dependencies among neighboring elements, and 2)\nmodel the interaction between features extracted by multiple attention heads.\nExperimental results of machine translation on different language pairs and\nmodel settings show that our approach outperforms both the strong Transformer\nbaseline and other existing models on enhancing the locality of SANs. Comparing\nwith prior studies, the proposed model is parameter free in terms of\nintroducing no more parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 15:02:26 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Yang", "Baosong", ""], ["Wang", "Longyue", ""], ["Wong", "Derek", ""], ["Chao", "Lidia S.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "1904.03177", "submitter": "Jessica Hamrick", "authors": "Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L.\n  Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, Jessica B. Hamrick", "title": "Structured agents for physical construction", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical construction---the ability to compose objects, subject to physical\ndynamics, to serve some function---is fundamental to human intelligence. We\nintroduce a suite of challenging physical construction tasks inspired by how\nchildren play with blocks, such as matching a target configuration, stacking\nblocks to connect objects together, and creating shelter-like structures over\ntarget objects. We examine how a range of deep reinforcement learning agents\nfare on these challenges, and introduce several new approaches which provide\nsuperior performance. Our results show that agents which use structured\nrepresentations (e.g., objects and scene graphs) and structured policies (e.g.,\nobject-centric actions) outperform those which use less structured\nrepresentations, and generalize better beyond their training when asked to\nreason about larger scenes. Model-based agents which use Monte-Carlo Tree\nSearch also outperform strictly model-free agents in our most challenging\nconstruction problems. We conclude that approaches which combine structured\nrepresentations and reasoning with powerful learning are a key path toward\nagents that possess rich intuitive physics, scene understanding, and planning.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:52:35 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 11:51:04 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Bapst", "Victor", ""], ["Sanchez-Gonzalez", "Alvaro", ""], ["Doersch", "Carl", ""], ["Stachenfeld", "Kimberly L.", ""], ["Kohli", "Pushmeet", ""], ["Battaglia", "Peter W.", ""], ["Hamrick", "Jessica B.", ""]]}, {"id": "1904.03178", "submitter": "Joseph Early", "authors": "Joseph Early", "title": "Reducing catastrophic forgetting when evolving neural networks", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key stepping stone in the development of an artificial general intelligence\n(a machine that can perform any task), is the production of agents that can\nperform multiple tasks at once instead of just one. Unfortunately, canonical\nmethods are very prone to catastrophic forgetting (CF) - the act of overwriting\nprevious knowledge about a task when learning a new task. Recent efforts have\ndeveloped techniques for overcoming CF in learning systems, but no attempt has\nbeen made to apply these new techniques to evolutionary systems. This research\npresents a novel technique, weight protection, for reducing CF in evolutionary\nsystems by adapting a method from learning systems. It is used in conjunction\nwith other evolutionary approaches for overcoming CF and is shown to be\neffective at alleviating CF when applied to a suite of reinforcement learning\ntasks. It is speculated that this work could indicate the potential for a wider\napplication of existing learning-based approaches to evolutionary systems and\nthat evolutionary techniques may be competitive with or better than learning\nsystems when it comes to reducing CF.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:57:29 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Early", "Joseph", ""]]}, {"id": "1904.03241", "submitter": "Markus N Rabe", "authors": "Kshitij Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy, and\n  Stewart Wilcox", "title": "HOList: An Environment for Machine Learning of Higher-Order Theorem\n  Proving", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an environment, benchmark, and deep learning driven automated\ntheorem prover for higher-order logic. Higher-order interactive theorem provers\nenable the formalization of arbitrary mathematical theories and thereby present\nan interesting, open-ended challenge for deep learning. We provide an\nopen-source framework based on the HOL Light theorem prover that can be used as\na reinforcement learning environment. HOL Light comes with a broad coverage of\nbasic mathematical theorems on calculus and the formal proof of the Kepler\nconjecture, from which we derive a challenging benchmark for automated\nreasoning. We also present a deep reinforcement learning driven automated\ntheorem prover, DeepHOL, with strong initial results on this benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:04:33 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:08:20 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 20:16:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bansal", "Kshitij", ""], ["Loos", "Sarah M.", ""], ["Rabe", "Markus N.", ""], ["Szegedy", "Christian", ""], ["Wilcox", "Stewart", ""]]}, {"id": "1904.03244", "submitter": "Sarthak Jain", "authors": "Sarthak Jain, Ramin Mohammadi, Byron C. Wallace", "title": "An Analysis of Attention over Clinical Notes for Predictive Tasks", "comments": "Accepted at The 2nd Clinical Natural Language Processing Workshop (At\n  NAACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift to electronic medical records (EMRs) has engendered research into\nmachine learning and natural language technologies to analyze patient records,\nand to predict from these clinical outcomes of interest. Two observations\nmotivate our aims here. First, unstructured notes contained within EMR often\ncontain key information, and hence should be exploited by models. Second, while\nstrong predictive performance is important, interpretability of models is\nperhaps equally so for applications in this domain. Together, these points\nsuggest that neural models for EMR may benefit from incorporation of attention\nover notes, which one may hope will both yield performance gains and afford\ntransparency in predictions. In this work we perform experiments to explore\nthis question using two EMR corpora and four different predictive tasks, that:\n(i) inclusion of attention mechanisms is critical for neural encoder modules\nthat operate over notes fields in order to yield competitive performance, but,\n(ii) unfortunately, while these boost predictive performance, it is decidedly\nless clear whether they provide meaningful support for predictions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:22:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jain", "Sarthak", ""], ["Mohammadi", "Ramin", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1904.03259", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo", "title": "Is 'Unsupervised Learning' a Misconceived Term?", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is all of machine learning supervised to some degree? The field of machine\nlearning has traditionally been categorized pedagogically into\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\nreferred to learning from labeled data, while unsupervised learning has\ntypically referred to learning from unlabeled data. In this paper, we assert\nthat all machine learning is in fact supervised to some degree, and that the\nscope of supervision is necessarily commensurate to the scope of learning\npotential. In particular, we argue that clustering algorithms such as k-means,\nand dimensionality reduction algorithms such as principal component analysis,\nvariational autoencoders, and deep belief networks are each internally\nsupervised by the data itself to learn their respective representations of its\nfeatures. Furthermore, these algorithms are not capable of external inference\nuntil their respective outputs (clusters, principal components, or\nrepresentation codes) have been identified and externally labeled in effect. As\nsuch, they do not suffice as examples of unsupervised learning. We propose that\nthe categorization `supervised vs unsupervised learning' be dispensed with, and\ninstead, learning algorithms be categorized as either\n$internally~or~externally~supervised$ (or both). We believe this change in\nperspective will yield new fundamental insights into the structure and\ncharacter of data and of learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:05:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Odaibo", "Stephen G.", ""]]}, {"id": "1904.03266", "submitter": "Ashutosh Modi", "authors": "Sepehr Janghorbani and Ashutosh Modi and Jakob Buhmann and Mubbasir\n  Kapadia", "title": "Domain Authoring Assistant for Intelligent Virtual Agents", "comments": "8+1 pages, Accepted at 18th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing intelligent virtual characters has attracted a lot of attention in\nthe recent years. The process of creating such characters often involves a team\nof creative authors who describe different aspects of the characters in natural\nlanguage, and planning experts that translate this description into a planning\ndomain. This can be quite challenging as the team of creative authors should\ndiligently define every aspect of the character especially if it contains\ncomplex human-like behavior. Also a team of engineers has to manually translate\nthe natural language description of a character's personality into the planning\ndomain knowledge. This can be extremely time and resource demanding and can be\nan obstacle to author's creativity. The goal of this paper is to introduce an\nauthoring assistant tool to automate the process of domain generation from\nnatural language description of virtual characters, thus bridging between the\ncreative authoring team and the planning domain experts. Moreover, the proposed\ntool also identifies possible missing information in the domain description and\niteratively makes suggestions to the author.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:27:26 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Janghorbani", "Sepehr", ""], ["Modi", "Ashutosh", ""], ["Buhmann", "Jakob", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1904.03295", "submitter": "Ishan Durugkar", "authors": "Ishan Durugkar, Matthew Hausknecht, Adith Swaminathan, Patrick\n  MacAlpine", "title": "Multi-Preference Actor Critic", "comments": "NeurIPS Workshop on Deep RL, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient algorithms typically combine discounted future rewards with\nan estimated value function, to compute the direction and magnitude of\nparameter updates. However, for most Reinforcement Learning tasks, humans can\nprovide additional insight to constrain the policy learning. We introduce a\ngeneral method to incorporate multiple different feedback channels into a\nsingle policy gradient loss. In our formulation, the Multi-Preference Actor\nCritic (M-PAC), these different types of feedback are implemented as\nconstraints on the policy. We use a Lagrangian relaxation to satisfy these\nconstraints using gradient descent while learning a policy that maximizes\nrewards. Experiments in Atari and Pendulum verify that constraints are being\nrespected and can accelerate the learning process.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:50:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Durugkar", "Ishan", ""], ["Hausknecht", "Matthew", ""], ["Swaminathan", "Adith", ""], ["MacAlpine", "Patrick", ""]]}, {"id": "1904.03396", "submitter": "Amit Moryossef", "authors": "Amit Moryossef, Yoav Goldberg and Ido Dagan", "title": "Step-by-Step: Separating Planning from Realization in Neural\n  Data-to-Text Generation", "comments": "9 main pages, 10 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-to-text generation can be conceptually divided into two parts: ordering\nand structuring the information (planning), and generating fluent language\ndescribing the information (realization). Modern neural generation systems\nconflate these two steps into a single end-to-end differentiable system. We\npropose to split the generation process into a symbolic text-planning stage\nthat is faithful to the input, followed by a neural generation stage that\nfocuses only on realization. For training a plan-to-text generator, we present\na method for matching reference texts to their corresponding text plans. For\ninference time, we describe a method for selecting high-quality text plans for\nnew inputs. We implement and evaluate our approach on the WebNLG benchmark. Our\nresults demonstrate that decoupling text planning from neural realization\nindeed improves the system's reliability and adequacy while maintaining fluent\noutput. We observe improvements both in BLEU scores and in manual evaluations.\nAnother benefit of our approach is the ability to output diverse realizations\nof the same input, paving the way to explicit control over the generated text\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:25:32 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 20:58:28 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Moryossef", "Amit", ""], ["Goldberg", "Yoav", ""], ["Dagan", "Ido", ""]]}, {"id": "1904.03438", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Negar Rostamzadeh, Yoshua Bengio, Sungjin Ahn, Pedro O.\n  Pinheiro", "title": "Reinforced Imitation in Heterogeneous Action Space", "comments": "The extended version of the work \"Reinforced Imitation Learning from\n  Observations\" presented on the NeurIPS workshop \"Imitation Learning and its\n  Challenges in Robotics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is an effective alternative approach to learn a policy\nwhen the reward function is sparse. In this paper, we consider a challenging\nsetting where an agent and an expert use different actions from each other. We\nassume that the agent has access to a sparse reward function and state-only\nexpert observations. We propose a method which gradually balances between the\nimitation learning cost and the reinforcement learning objective. In addition,\nthis method adapts the agent's policy based on either mimicking expert behavior\nor maximizing sparse reward. We show, through navigation scenarios, that (i) an\nagent is able to efficiently leverage sparse rewards to outperform standard\nstate-only imitation learning, (ii) it can learn a policy even when its actions\nare different from the expert, and (iii) the performance of the agent is not\nbounded by that of the expert, due to the optimized usage of sparse rewards.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 13:07:12 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 15:26:06 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zolna", "Konrad", ""], ["Rostamzadeh", "Negar", ""], ["Bengio", "Yoshua", ""], ["Ahn", "Sungjin", ""], ["Pinheiro", "Pedro O.", ""]]}, {"id": "1904.03461", "submitter": "Erik Wijmans", "authors": "Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia\n  Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra", "title": "Embodied Question Answering in Photorealistic Environments with Point\n  Cloud Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help bridge the gap between internet vision-style problems and the goal of\nvision for embodied perception we instantiate a large-scale navigation task --\nEmbodied Question Answering [1] in photo-realistic environments (Matterport\n3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB\nimages, or their combination. Our analysis of these models reveals several key\nfindings. We find that two seemingly naive navigation baselines, forward-only\nand random, are strong navigators and challenging to outperform, due to the\nspecific choice of the evaluation setting presented by [1]. We find a novel\nloss-weighting scheme we call Inflection Weighting to be important when\ntraining recurrent models for navigation with behavior cloning and are able to\nout perform the baselines with this technique. We find that point clouds\nprovide a richer signal than RGB images for learning obstacle avoidance,\nmotivating the use (and continued study) of 3D deep learning models for\nembodied navigation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 14:50:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wijmans", "Erik", ""], ["Datta", "Samyak", ""], ["Maksymets", "Oleksandr", ""], ["Das", "Abhishek", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Essa", "Irfan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.03534", "submitter": "Ali Sadeghian", "authors": "Ali Sadeghian, Deoksu Lim, Johan Karlsson, Jian Li", "title": "Automatic Target Recognition Using Discrimination Based on Optimal\n  Transport", "comments": null, "journal-ref": "2015 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of distances based on optimal transportation has recently shown\npromise for discrimination of power spectra. In particular, spectral estimation\nmethods based on l1 regularization as well as covariance based methods can be\nshown to be robust with respect to such distances. These transportation\ndistances provide a geometric framework where geodesics corresponds to smooth\ntransition of spectral mass, and have been useful for tracking. In this paper,\nwe investigate the use of these distances for automatic target recognition. We\nstudy the use of the Monge-Kantorovich distance compared to the standard l2\ndistance for classifying civilian vehicles based on SAR images. We use a\nversion of the Monge-Kantorovich distance that applies also for the case where\nthe spectra may have different total mass, and we formulate the optimization\nproblem as a minimum flow problem that can be computed using efficient\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:43:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sadeghian", "Ali", ""], ["Lim", "Deoksu", ""], ["Karlsson", "Johan", ""], ["Li", "Jian", ""]]}, {"id": "1904.03535", "submitter": "Nikolaos Tziortziotis", "authors": "Nikolaos Tziortziotis, Christos Dimitrakakis, Michalis Vazirgiannis", "title": "Randomised Bayesian Least-Squares Policy Iteration", "comments": "European Workshop on Reinforcement Learning 14, October 2018, Lille,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy,\nmodel-free, policy iteration algorithm that uses the Bayesian least-squares\ntemporal-difference (BLSTD) learning algorithm to evaluate policies. An online\nvariant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that\nimproves its policy based on an incomplete policy evaluation step. In online\nsetting, the exploration-exploitation dilemma should be addressed as we try to\ndiscover the optimal policy by using samples collected by ourselves. RBLSPI\nexploits the advantage of BLSTD to quantify our uncertainty about the value\nfunction. Inspired by Thompson sampling, RBLSPI first samples a value function\nfrom a posterior distribution over value functions, and then selects actions\nbased on the sampled value function. The effectiveness and the exploration\nabilities of RBLSPI are demonstrated experimentally in several environments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:50:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tziortziotis", "Nikolaos", ""], ["Dimitrakakis", "Christos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.03606", "submitter": "Mohannad Babli", "authors": "Mohannad Babli, Eva Onaindia, Eliseo Marzal", "title": "Extending planning knowledge using ontologies for goal opportunities", "comments": "10 pages, 8 Figures, 31st\n  International-Business-Information-Management-Association Conference, Milan\n  ITALY, date: APR 25-26, 2018", "journal-ref": "31st IBIMA Conference (2018), INNOVATION MANAGEMENT AND EDUCATION\n  EXCELLENCE THROUGH VISION 2020, VOLS IV-VI (3199-3208)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to goal-directed behaviour including online planning and\nopportunistic planning tackle a change in the environment by generating\nalternative goals to avoid failures or seize opportunities. However, current\napproaches only address unanticipated changes related to objects or object\ntypes already defined in the planning task that is being solved. This article\ndescribes a domain-independent approach that advances the state of the art by\nextending the knowledge of a planning task with relevant objects of new types.\nThe approach draws upon the use of ontologies, semantic measures, and ontology\nalignment to accommodate newly acquired data that trigger the formulation of\ngoal opportunities inducing a better-valued plan.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 08:39:10 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Babli", "Mohannad", ""], ["Onaindia", "Eva", ""], ["Marzal", "Eliseo", ""]]}, {"id": "1904.03736", "submitter": "Weiyan Shi", "authors": "Weiyan Shi, Tiancheng Zhao, Zhou Yu", "title": "Unsupervised Dialog Structure Learning", "comments": "Long paper accepted by NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a shared dialog structure from a set of task-oriented dialogs is an\nimportant challenge in computational linguistics. The learned dialog structure\ncan shed light on how to analyze human dialogs, and more importantly contribute\nto the design and evaluation of dialog systems. We propose to extract dialog\nstructures using a modified VRNN model with discrete latent vectors. Different\nfrom existing HMM-based models, our model is based on variational-autoencoder\n(VAE). Such model is able to capture more dynamics in dialogs beyond the\nsurface forms of the language. We find that qualitatively, our method extracts\nmeaningful dialog structure, and quantitatively, outperforms previous models on\nthe ability to predict unseen data. We further evaluate the model's\neffectiveness in a downstream task, the dialog system building task.\nExperiments show that, by integrating the learned dialog structure into the\nreward function design, the model converges faster and to a better outcome in a\nreinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:28:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 01:54:46 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Shi", "Weiyan", ""], ["Zhao", "Tiancheng", ""], ["Yu", "Zhou", ""]]}, {"id": "1904.03816", "submitter": "Sungjoo Ha", "authors": "Seokjun Seo, Seungwoo Choi, Martin Kersner, Beomjun Shin, Hyungsuk\n  Yoon, Hyeongmin Byun, Sungjoo Ha", "title": "Towards Real-Time Automatic Portrait Matting on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of automatic portrait matting on mobile devices. The\nproposed model is aimed at attaining real-time inference on mobile devices with\nminimal degradation of model performance. Our model MMNet, based on\nmulti-branch dilated convolution with linear bottleneck blocks, outperforms the\nstate-of-the-art model and is orders of magnitude faster. The model can be\naccelerated four times to attain 30 FPS on Xiaomi Mi 5 device with moderate\nincrease in the gradient error. Under the same conditions, our model has an\norder of magnitude less number of parameters and is faster than Mobile\nDeepLabv3 while maintaining comparable performance. The accompanied\nimplementation can be found at \\url{https://github.com/hyperconnect/MMNet}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:21:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Seo", "Seokjun", ""], ["Choi", "Seungwoo", ""], ["Kersner", "Martin", ""], ["Shin", "Beomjun", ""], ["Yoon", "Hyungsuk", ""], ["Byun", "Hyeongmin", ""], ["Ha", "Sungjoo", ""]]}, {"id": "1904.03821", "submitter": "Inseok Oh", "authors": "Inseok Oh, Seungeun Rho, Sangbin Moon, Seongho Son, Hyoil Lee, and\n  Jinyun Chung", "title": "Creating Pro-Level AI for a Real-Time Fighting Game Using Deep\n  Reinforcement Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning combined with deep neural networks has performed\nremarkably well in many genres of games recently. It has surpassed human-level\nperformance in fixed game environments and turn-based two player board games.\nHowever, to the best of our knowledge, current research has yet to produce a\nresult that has surpassed human-level performance in modern complex fighting\ngames. This is due to the inherent difficulties with real-time fighting games,\nincluding: vast action spaces, action dependencies, and imperfect information.\nWe overcame these challenges and made 1v1 battle AI agents for the commercial\ngame \"Blade & Soul\". The trained agents competed against five professional\ngamers and achieved a win rate of 62%. This paper presents a practical\nreinforcement learning method that includes a novel self-play curriculum and\ndata skipping techniques. Through the curriculum, three different styles of\nagents were created by reward shaping and were trained against each other.\nAdditionally, this paper suggests data skipping techniques that could increase\ndata efficiency and facilitate explorations in vast spaces. Since our method\ncan be generally applied to all two-player competitive games with vast action\nspaces, we anticipate its application to game development including level\ndesign and automated balancing.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:46:24 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 02:01:56 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 08:21:56 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Oh", "Inseok", ""], ["Rho", "Seungeun", ""], ["Moon", "Sangbin", ""], ["Son", "Seongho", ""], ["Lee", "Hyoil", ""], ["Chung", "Jinyun", ""]]}, {"id": "1904.03845", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang Lai, Zhengtao\n  Yu, and Liang Lin", "title": "Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A\n  New Benchmark", "comments": "Accepted by TNNLS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) benefits greatly from the accurate\nannotations of existing datasets (e.g., CUHK03 [1] and Market-1501 [2]), which\nare quite expensive because each image in these datasets has to be assigned\nwith a proper label. In this work, we ease the annotation of Re-ID by replacing\nthe accurate annotation with inaccurate annotation, i.e., we group the images\ninto bags in terms of time and assign a bag-level label for each bag. This\ngreatly reduces the annotation effort and leads to the creation of a\nlarge-scale Re-ID benchmark called SYSU-30$k$. The new benchmark contains $30k$\nindividuals, which is about $20$ times larger than CUHK03 ($1.3k$ individuals)\nand Market-1501 ($1.5k$ individuals), and $30$ times larger than ImageNet ($1k$\ncategories). It sums up to 29,606,918 images. Learning a Re-ID model with\nbag-level annotation is called the weakly supervised Re-ID problem. To solve\nthis problem, we introduce a differentiable graphical model to capture the\ndependencies from all images in a bag and generate a reliable pseudo label for\neach person image. The pseudo label is further used to supervise the learning\nof the Re-ID model. When compared with the fully supervised Re-ID models, our\nmethod achieves state-of-the-art performance on SYSU-30$k$ and other datasets.\nThe code, dataset, and pretrained model will be available at\n\\url{https://github.com/wanggrun/SYSU-30k}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 05:27:53 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 16:01:41 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 08:16:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Guangcong", ""], ["Zhang", "Xujie", ""], ["Lai", "Jianhuang", ""], ["Yu", "Zhengtao", ""], ["Lin", "Liang", ""]]}, {"id": "1904.03897", "submitter": "Nguyen Van Huynh", "authors": "Nguyen Van Huynh, Diep N. Nguyen, Dinh Thai Hoang, and Eryk Dutkiewicz", "title": "\"Jam Me If You Can'': Defeating Jammer with Deep Dueling Neural Network\n  Architecture and Ambient Backscattering Augmented Communications", "comments": "30 pages, 13 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With conventional anti-jamming solutions like frequency hopping or spread\nspectrum, legitimate transceivers often tend to \"escape\" or \"hide\" themselves\nfrom jammers. These reactive anti-jamming approaches are constrained by the\nlack of timely knowledge of jamming attacks. Bringing together the latest\nadvances in neural network architectures and ambient backscattering\ncommunications, this work allows wireless nodes to effectively \"face\" the\njammer by first learning its jamming strategy, then adapting the rate or\ntransmitting information right on the jamming signal. Specifically, to deal\nwith unknown jamming attacks, existing work often relies on reinforcement\nlearning algorithms, e.g., Q-learning. However, the Q-learning algorithm is\nnotorious for its slow convergence to the optimal policy, especially when the\nsystem state and action spaces are large. This makes the Q-learning algorithm\npragmatically inapplicable. To overcome this problem, we design a novel deep\nreinforcement learning algorithm using the recent dueling neural network\narchitecture. Our proposed algorithm allows the transmitter to effectively\nlearn about the jammer and attain the optimal countermeasures thousand times\nfaster than that of the conventional Q-learning algorithm. Through extensive\nsimulation results, we show that our design (using ambient backscattering and\nthe deep dueling neural network architecture) can improve the average\nthroughput by up to 426% and reduce the packet loss by 24%. By augmenting the\nambient backscattering capability on devices and using our algorithm, it is\ninteresting to observe that the (successful) transmission rate increases with\nthe jamming power. Our proposed solution can find its applications in both\ncivil (e.g., ultra-reliable and low-latency communications or URLLC) and\nmilitary scenarios (to combat both inadvertent and deliberate jamming).\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:06:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Van Huynh", "Nguyen", ""], ["Nguyen", "Diep N.", ""], ["Hoang", "Dinh Thai", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "1904.03898", "submitter": "Jue Wang", "authors": "Jue Wang, Ke Chen, Lidan Shou, Sai Wu and Sharad Mehrotra", "title": "Semi-Supervised Few-Shot Learning for Dual Question-Answer Extraction", "comments": "7 pages, 5 figures, submission to IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of key phrase extraction from sentences.\nExisting state-of-the-art supervised methods require large amounts of annotated\ndata to achieve good performance and generalization. Collecting labeled data\nis, however, often expensive. In this paper, we redefine the problem as\nquestion-answer extraction, and present SAMIE: Self-Asking Model for\nInformation Ixtraction, a semi-supervised model which dually learns to ask and\nto answer questions by itself. Briefly, given a sentence $s$ and an answer $a$,\nthe model needs to choose the most appropriate question $\\hat q$; meanwhile,\nfor the given sentence $s$ and same question $\\hat q$ selected in the previous\nstep, the model will predict an answer $\\hat a$. The model can support few-shot\nlearning with very limited supervision. It can also be used to perform\nclustering analysis when no supervision is provided. Experimental results show\nthat the proposed method outperforms typical supervised methods especially when\ngiven little labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:07:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Jue", ""], ["Chen", "Ke", ""], ["Shou", "Lidan", ""], ["Wu", "Sai", ""], ["Mehrotra", "Sharad", ""]]}, {"id": "1904.03964", "submitter": "Sebastian Gottwald", "authors": "Sebastian Gottwald, Daniel A. Braun", "title": "Bounded rational decision-making from elementary computations that\n  reduce uncertainty", "comments": null, "journal-ref": "Entropy 2019, 21, 375", "doi": "10.3390/e21040375", "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In its most basic form, decision-making can be viewed as a computational\nprocess that progressively eliminates alternatives, thereby reducing\nuncertainty. Such processes are generally costly, meaning that the amount of\nuncertainty that can be reduced is limited by the amount of available\ncomputational resources. Here, we introduce the notion of elementary\ncomputation based on a fundamental principle for probability transfers that\nreduce uncertainty. Elementary computations can be considered as the inverse of\nPigou-Dalton transfers applied to probability distributions, closely related to\nthe concepts of majorization, T-transforms, and generalized entropies that\ninduce a preorder on the space of probability distributions. As a consequence\nwe can define resource cost functions that are order-preserving and therefore\nmonotonic with respect to the uncertainty reduction. This leads to a\ncomprehensive notion of decision-making processes with limited resources. Along\nthe way, we prove several new results on majorization theory, as well as on\nentropy and divergence measures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 11:28:02 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gottwald", "Sebastian", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1904.04035", "submitter": "Simon Streicher", "authors": "Simon Streicher, Carl Sandrock", "title": "Plant-wide fault and disturbance screening using combined transfer\n  entropy and eigenvector centrality analysis", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the source of a disturbance or fault in complex systems such as\nindustrial chemical processing plants can be a difficult task and consume a\nsignificant number of engineering hours. In many cases, a systematic\nelimination procedure is considered to be the only feasible approach but can\ncause undesired process upsets. Practitioners desire robust alternative\napproaches.\n  This paper presents an unsupervised, data-driven method for ranking process\nelements according to the magnitude and novelty of their influence. Partial\nbivariate transfer entropy estimation is used to infer a weighted directed\ngraph of process elements. Eigenvector centrality is applied to rank network\nnodes according to their overall effect. As the ranking of process elements\nrely on emerging properties that depend on the aggregate of many connections,\nthe results are robust to errors in the estimation of individual edge\nproperties and the inclusion of indirect connections that do not represent the\ntrue causal structure of the process.\n  A monitoring chart of continuously calculated process element importance\nscores over multiple overlapping time regions can assist with incipient fault\ndetection. Ranking results combined with visual inspection of information\ntransfer networks is also useful for root cause analysis of known faults and\ndisturbances. A software implementation of the proposed method is available.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:02:28 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Streicher", "Simon", ""], ["Sandrock", "Carl", ""]]}, {"id": "1904.04211", "submitter": "Michele Piana", "authors": "Sabrina Guastavino, Michele Piana, Anna Maria Massone, Richard\n  Schwartz, Federico Benvenuto", "title": "Desaturating EUV observations of solar flaring storms", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4357/ab35d8", "report-no": null, "categories": "astro-ph.SR cs.AI math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image saturation has been an issue for several instruments in solar\nastronomy, mainly at EUV wavelengths. However, with the launch of the\nAtmospheric Imaging Assembly (AIA) as part of the payload of the Solar Dynamic\nObservatory (SDO) image saturation has become a big data issue, involving\naround 10^$ frames of the impressive dataset this beautiful telescope has been\nproviding every year since February 2010. This paper introduces a novel\ndesaturation method, which is able to recover the signal in the saturated\nregion of any AIA image by exploiting no other information but the one\ncontained in the image itself. This peculiar methodological property, jointly\nwith the unprecedented statistical reliability of the desaturated images, could\nmake this algorithm the perfect tool for the realization of a reconstruction\npipeline for AIA data, able to work properly even in the case of long-lasting,\nvery energetic flaring events.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:33:19 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Guastavino", "Sabrina", ""], ["Piana", "Michele", ""], ["Massone", "Anna Maria", ""], ["Schwartz", "Richard", ""], ["Benvenuto", "Federico", ""]]}, {"id": "1904.04281", "submitter": "Tolga Birdal", "authors": "Haowen Deng and Tolga Birdal and Slobodan Ilic", "title": "3D Local Features for Direct Pairwise Registration", "comments": "To appear in CVPR 2019. 16 pages, identical to the camera ready\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, data driven approach for solving the problem of\nregistration of two point cloud scans. Our approach is direct in the sense that\na single pair of corresponding local patches already provides the necessary\ntransformation cue for the global registration. To achieve that, we first endow\nthe state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling,\nwhere the discrepancy between the two leads to pose-specific descriptors. Based\nupon this, we introduce RelativeNet, a relative pose estimation network to\nassign correspondence-specific orientations to the keypoints, eliminating any\nlocal reference frame computations. Finally, we devise a simple yet effective\nhypothesize-and-verify algorithm to quickly use the predictions and align two\npoint sets. Our extensive quantitative and qualitative experiments suggests\nthat our approach outperforms the state of the art in challenging real datasets\nof pairwise registration and that augmenting the keypoints with local pose\ninformation leads to better generalization and a dramatic speed-up.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:17:36 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Deng", "Haowen", ""], ["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1904.04360", "submitter": "Andras Hajdu Dr.", "authors": "Attila Tiba, Andras Hajdu, Gyorgy Terdik, Henrietta Toman", "title": "Optimizing Majority Voting Based Systems Under a Resource Constraint for\n  Multiclass Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble-based approaches are very effective in various fields in raising the\naccuracy of its individual members, when some voting rule is applied for\naggregating the individual decisions. In this paper, we investigate how to find\nand characterize the ensembles having the highest accuracy if the total cost of\nthe ensemble members is bounded. This question leads to Knapsack problem with\nnon-linear and non-separable objective function in binary and multiclass\nclassification if the majority voting is chosen for the aggregation. As the\nconventional solving methods cannot be applied for this task, a novel\nstochastic approach was introduced in the binary case where the energy function\nis discussed as the joint probability function of the member accuracy. We show\nsome theoretical results with respect to the expected ensemble accuracy and its\nvariance in the multiclass classification problem which can help us to solve\nthe Knapsack problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:16:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tiba", "Attila", ""], ["Hajdu", "Andras", ""], ["Terdik", "Gyorgy", ""], ["Toman", "Henrietta", ""]]}, {"id": "1904.04388", "submitter": "Vicky Zayats", "authors": "Vicky Zayats and Mari Ostendorf", "title": "Giving Attention to the Unexpected: Using Prosody Innovations in\n  Disfluency Detection", "comments": "Accepted at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disfluencies in spontaneous speech are known to be associated with prosodic\ndisruptions. However, most algorithms for disfluency detection use only word\ntranscripts. Integrating prosodic cues has proved difficult because of the many\nsources of variability affecting the acoustic correlates. This paper introduces\na new approach to extracting acoustic-prosodic cues using text-based\ndistributional prediction of acoustic cues to derive vector z-score features\n(innovations). We explore both early and late fusion techniques for integrating\ntext and prosody, showing gains over a high-accuracy text-only model.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 22:47:37 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zayats", "Vicky", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1904.04404", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David Crandall, Devi\n  Parikh, Dhruv Batra", "title": "Embodied Visual Recognition", "comments": "14 pages, 13 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive visual systems typically fail to recognize objects in the amodal\nsetting where they are heavily occluded. In contrast, humans and other embodied\nagents have the ability to move in the environment, and actively control the\nviewing angle to better understand object shapes and semantics. In this work,\nwe introduce the task of Embodied Visual Recognition (EVR): An agent is\ninstantiated in a 3D environment close to an occluded target object, and is\nfree to move in the environment to perform object classification, amodal object\nlocalization, and amodal object segmentation. To address this, we develop a new\nmodel called Embodied Mask R-CNN, for agents to learn to move strategically to\nimprove their visual recognition abilities. We conduct experiments using the\nHouse3D environment. Experimental results show that: 1) agents with embodiment\n(movement) achieve better visual recognition performance than passive ones; 2)\nin order to improve visual recognition abilities, agents can learn strategical\nmoving paths that are different from shortest paths.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 00:33:17 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yang", "Jianwei", ""], ["Ren", "Zhile", ""], ["Xu", "Mingze", ""], ["Chen", "Xinlei", ""], ["Crandall", "David", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.04458", "submitter": "Angli Liu", "authors": "Angli Liu, Jingfei Du, Veselin Stoyanov", "title": "Knowledge-Augmented Language Model and its Application to Unsupervised\n  Named-Entity Recognition", "comments": "NAACL 2019; updated to cite Zhou et al. (2018) EMNLP as a piece of\n  related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional language models are unable to efficiently model entity names\nobserved in text. All but the most popular named entities appear infrequently\nin text providing insufficient context. Recent efforts have recognized that\ncontext can be generalized between entity names that share the same type (e.g.,\n\\emph{person} or \\emph{location}) and have equipped language models with access\nto an external knowledge base (KB). Our Knowledge-Augmented Language Model\n(KALM) continues this line of work by augmenting a traditional model with a KB.\nUnlike previous methods, however, we train with an end-to-end predictive\nobjective optimizing the perplexity of text. We do not require any additional\ninformation such as named entity tags. In addition to improving language\nmodeling performance, KALM learns to recognize named entities in an entirely\nunsupervised way by using entity type information latent in the model. On a\nNamed Entity Recognition (NER) task, KALM achieves performance comparable with\nstate-of-the-art supervised models. Our work demonstrates that named entities\n(and possibly other types of world knowledge) can be modeled successfully using\npredictive learning and training on large corpora of text without any\nadditional information.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:09:45 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 07:48:21 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Liu", "Angli", ""], ["Du", "Jingfei", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1904.04475", "submitter": "Xianrui Meng", "authors": "Xianrui Meng, Dimitrios Papadopoulos, Alina Oprea, Nikos Triandopoulos", "title": "Private Two-Party Cluster Analysis Made Formal & Scalable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is widely used for predictive tasks in numerous\nimportant applications---most successfully, in the context of collaborative\nlearning, where a plurality of entities contribute their own datasets to\njointly deduce global ML models. Despite its efficacy, this new learning\nparadigm fails to encompass critical application domains, such as healthcare\nand security analytics, that involve learning over highly sensitive data,\nwherein privacy risks limit entities to individually deduce local models using\nsolely their own datasets.\n  In this work, we present the first comprehensive study for privacy-preserving\ncollaborative hierarchical clustering, overall featuring scalable cryptographic\nprotocols that allow two parties to safely perform cluster analysis over their\ncombined sensitive datasets. For this problem at hand, we introduce a formal\nsecurity notion that achieves the required balance between intended accuracy\nand privacy and presents a class of two-party hierarchical clustering protocols\nthat guarantee strong privacy protection, provable in our new security model.\nCrucially, our solution employs modular design and judicious use of\ncryptography to achieve high degrees of efficiency and extensibility.\nSpecifically, we extend our core protocol to obtain two secure variants that\nsignificantly improve performance, an optimized variant for single-linkage\nclustering and a scalable approximate variant. Finally, we provide a prototype\nimplementation of our approach and experimentally evaluate its feasibility and\nefficiency on synthetic and real datasets, obtaining encouraging results. For\nexample, end-to-end execution of our secure approximate protocol, over 1M\n10-dimensional records, completes in 35 sec, transferring only 896KB and\nachieving 97.09% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 05:58:28 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 04:09:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Meng", "Xianrui", ""], ["Papadopoulos", "Dimitrios", ""], ["Oprea", "Alina", ""], ["Triandopoulos", "Nikos", ""]]}, {"id": "1904.04579", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A Concept-Value Network as a Brain Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a statistical framework for describing the relations\nbetween the physical and conceptual entities of a brain-like model. In\nparticular, features and concept instances are put into context. This may help\nwith understanding or implementing a similar model. The paper suggests that\nfeatures are in fact the wiring. With this idea, the actual length of the\nconnection is important, because it is related to firing rates and neuron\nsynchronization. The paper then suggests that concepts are neuron groups that\nlink features and concept instances are the signals from those groups.\nTherefore, features become the static framework of the interconnected neural\nsystem and concepts are combinations of these, as determined by the external\nstimulus and the neural synaptic strengths. Along with this statistical model,\nit is possible to propose a simplified design for a neuron, based on an action\npotential and variable output signal. A strong comparison with Hebbian theory\nis then proposed, with some test results to support the theory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 10:30:23 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 10:15:52 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1904.04595", "submitter": "Carlos Mastalli", "authors": "Bernardo Aceituno-Cabezas, Carlos Mastalli, Hongkai Dai, Michele\n  Focchi, Andreea Radulescu, Darwin G. Caldwell, Jose Cappelletto, Juan C.\n  Grieco, Gerardo Fernandez-Lopez and Claudio Semini", "title": "Simultaneous Contact, Gait and Motion Planning for Robust Multi-Legged\n  Locomotion via Mixed-Integer Convex Optimization", "comments": "8 pages, IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2017.2779821", "report-no": null, "categories": "cs.RO cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional motion planning approaches for multi-legged locomotion divide the\nproblem into several stages, such as contact search and trajectory generation.\nHowever, reasoning about contacts and motions simultaneously is crucial for the\ngeneration of complex whole-body behaviors. Currently, coupling theses problems\nhas required either the assumption of a fixed gait sequence and flat terrain\ncondition, or non-convex optimization with intractable computation time. In\nthis paper, we propose a mixed-integer convex formulation to plan\nsimultaneously contact locations, gait transitions and motion, in a\ncomputationally efficient fashion. In contrast to previous works, our approach\nis not limited to flat terrain nor to a pre-specified gait sequence. Instead,\nwe incorporate the friction cone stability margin, approximate the robot's\ntorque limits, and plan the gait using mixed-integer convex constraints. We\nexperimentally validated our approach on the HyQ robot by traversing different\nchallenging terrains, where non-convexity and flat terrain assumptions might\nlead to sub-optimal or unstable plans. Our method increases the motion\ngenerality while keeping a low computation time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 11:20:25 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Aceituno-Cabezas", "Bernardo", ""], ["Mastalli", "Carlos", ""], ["Dai", "Hongkai", ""], ["Focchi", "Michele", ""], ["Radulescu", "Andreea", ""], ["Caldwell", "Darwin G.", ""], ["Cappelletto", "Jose", ""], ["Grieco", "Juan C.", ""], ["Fernandez-Lopez", "Gerardo", ""], ["Semini", "Claudio", ""]]}, {"id": "1904.04691", "submitter": "Muhammad Usman Ghani", "authors": "Muhammad Usman Ghani, W. Clem Karl", "title": "Fast Enhanced CT Metal Artifact Reduction using Data Domain Deep\n  Learning", "comments": "Accepted for publication in IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtered back projection (FBP) is the most widely used method for image\nreconstruction in X-ray computed tomography (CT) scanners. The presence of\nhyper-dense materials in a scene, such as metals, can strongly attenuate\nX-rays, producing severe streaking artifacts in the reconstruction. These metal\nartifacts can greatly limit subsequent object delineation and information\nextraction from the images, restricting their diagnostic value. This problem is\nparticularly acute in the security domain, where there is great heterogeneity\nin the objects that can appear in a scene, highly accurate decisions must be\nmade quickly. The standard practical approaches to reducing metal artifacts in\nCT imagery are either simplistic non-adaptive interpolation-based projection\ndata completion methods or direct image post-processing methods. These standard\napproaches have had limited success. Motivated primarily by security\napplications, we present a new deep-learning-based metal artifact reduction\n(MAR) approach that tackles the problem in the projection data domain. We treat\nthe projection data corresponding to metal objects as missing data and train an\nadversarial deep network to complete the missing data in the projection domain.\nThe subsequent complete projection data is then used with FBP to reconstruct\nimage intended to be free of artifacts. This new approach results in an\nend-to-end MAR algorithm that is computationally efficient so practical and\nfits well into existing CT workflows allowing easy adoption in existing\nscanners. Training deep networks can be challenging, and another contribution\nof our work is to demonstrate that training data generated using an accurate\nX-ray simulation can be used to successfully train the deep network when\ncombined with transfer learning using limited real data sets. We demonstrate\nthe effectiveness and potential of our algorithm on simulated and real\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:13:41 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 01:11:12 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 16:16:35 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ghani", "Muhammad Usman", ""], ["Karl", "W. Clem", ""]]}, {"id": "1904.04697", "submitter": "Xipeng Qiu", "authors": "Hang Yan, Xipeng Qiu, Xuanjing Huang", "title": "A Graph-based Model for Joint Chinese Word Segmentation and Dependency\n  Parsing", "comments": "Accepted at Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese word segmentation and dependency parsing are two fundamental tasks\nfor Chinese natural language processing. The dependency parsing is defined on\nword-level. Therefore word segmentation is the precondition of dependency\nparsing, which makes dependency parsing suffer from error propagation and\nunable to directly make use of the character-level pre-trained language model\n(such as BERT). In this paper, we propose a graph-based model to integrate\nChinese word segmentation and dependency parsing. Different from previous\ntransition-based joint models, our proposed model is more concise, which\nresults in fewer efforts of feature engineering. Our graph-based joint model\nachieves better performance than previous joint models and state-of-the-art\nresults in both Chinese word segmentation and dependency parsing. Besides, when\nBERT is combined, our model can substantially reduce the performance gap of\ndependency parsing between joint models and gold-segmented word-based models.\nOur code is publicly available at https://github.com/fastnlp/JointCwsParser.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 14:25:17 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:07:33 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Yan", "Hang", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1904.04762", "submitter": "Bhairav Mehta", "authors": "Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J. Pal, Liam\n  Paull", "title": "Active Domain Randomization", "comments": "Code available at\n  https://github.com/montrealrobotics/active-domainrand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain randomization is a popular technique for improving domain transfer,\noften used in a zero-shot setting when the target domain is unknown or cannot\neasily be used for training. In this work, we empirically examine the effects\nof domain randomization on agent generalization. Our experiments show that\ndomain randomization may lead to suboptimal, high-variance policies, which we\nattribute to the uniform sampling of environment parameters. We propose Active\nDomain Randomization, a novel algorithm that learns a parameter sampling\nstrategy. Our method looks for the most informative environment variations\nwithin the given randomization ranges by leveraging the discrepancies of policy\nrollouts in randomized and reference environment instances. We find that\ntraining more frequently on these instances leads to better overall agent\ngeneralization. Our experiments across various physics-based simulated and\nreal-robot tasks show that this enhancement leads to more robust, consistent\npolicies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:15:39 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 20:20:10 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mehta", "Bhairav", ""], ["Diaz", "Manfred", ""], ["Golemo", "Florian", ""], ["Pal", "Christopher J.", ""], ["Paull", "Liam", ""]]}, {"id": "1904.04764", "submitter": "Haohan Guo", "authors": "Haohan Guo, Frank K. Soong, Lei He, Lei Xie", "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS", "comments": "Submitted to Interspeech 2019, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The end-to-end TTS, which can predict speech directly from a given sequence\nof graphemes or phonemes, has shown improved performance over the conventional\nTTS. However, its predicting capability is still limited by the\nacoustic/phonetic coverage of the training data, usually constrained by the\ntraining set size. To further improve the TTS quality in pronunciation, prosody\nand perceived naturalness, we propose to exploit the information embedded in a\nsyntactically parsed tree where the inter-phrase/word information of a sentence\nis organized in a multilevel tree structure. Specifically, two key features:\nphrase structure and relations between adjacent words are investigated.\nExperimental results in subjective listening, measured on three test sets, show\nthat the proposed approach is effective to improve the pronunciation clarity,\nprosody and naturalness of the synthesized speech of the baseline system.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:20:52 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Guo", "Haohan", ""], ["Soong", "Frank K.", ""], ["He", "Lei", ""], ["Xie", "Lei", ""]]}, {"id": "1904.04805", "submitter": "Jacques Kaiser", "authors": "Jacques Kaiser, Alexander Friedrich, J. Camilo Vasquez Tieck, Daniel\n  Reichard, Arne Roennau, Emre Neftci, R\\\"udiger Dillmann", "title": "Embodied Neuromorphic Vision with Event-Driven Random Backpropagation", "comments": "v2: title update, better plots and wordings. 8 pages, 9 figures, 1\n  table, video: https://neurorobotics-files.net/index.php/s/sBQzWFrBPoH9Dx7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-based communication between biological neurons is sparse and\nunreliable. This enables the brain to process visual information from the eyes\nefficiently. Taking inspiration from biology, artificial spiking neural\nnetworks coupled with silicon retinas attempt to model these computations.\nRecent findings in machine learning allowed the derivation of a family of\npowerful synaptic plasticity rules approximating backpropagation for spiking\nnetworks. Are these rules capable of processing real-world visual sensory data?\nIn this paper, we evaluate the performance of Event-Driven Random\nBack-Propagation (eRBP) at learning representations from event streams provided\nby a Dynamic Vision Sensor (DVS). First, we show that eRBP matches\nstate-of-the-art performance on the DvsGesture dataset with the addition of a\nsimple covert attention mechanism. By remapping visual receptive fields\nrelatively to the center of the motion, this attention mechanism provides\ntranslation invariance at low computational cost compared to convolutions.\nSecond, we successfully integrate eRBP in a real robotic setup, where a robotic\narm grasps objects according to detected visual affordances. In this setup,\nvisual information is actively sensed by a DVS mounted on a robotic head\nperforming microsaccadic eye movements. We show that our method classifies\naffordances within 100ms after microsaccade onset, which is comparable to human\nperformance reported in behavioral study. Our results suggest that advances in\nneuromorphic technology and plasticity rules enable the development of\nautonomous robots operating at high speed and low energy consumption.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:35:24 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 12:56:48 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kaiser", "Jacques", ""], ["Friedrich", "Alexander", ""], ["Tieck", "J. Camilo Vasquez", ""], ["Reichard", "Daniel", ""], ["Roennau", "Arne", ""], ["Neftci", "Emre", ""], ["Dillmann", "R\u00fcdiger", ""]]}, {"id": "1904.04862", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Bita Darvish Rouhani, Farinaz Koushanfar", "title": "SWNet: Small-World Neural Networks and Rapid Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large and highly accurate deep learning (DL) models is\ncomputationally costly. This cost is in great part due to the excessive number\nof trained parameters, which are well-known to be redundant and compressible\nfor the execution phase. This paper proposes a novel transformation which\nchanges the topology of the DL architecture such that it reaches an optimal\ncross-layer connectivity. This transformation leverages our important\nobservation that for a set level of accuracy, convergence is fastest when\nnetwork topology reaches the boundary of a Small-World Network. Small-world\ngraphs are known to possess a specific connectivity structure that enables\nenhanced signal propagation among nodes. Our small-world models, called SWNets,\nprovide several intriguing benefits: they facilitate data (gradient) flow\nwithin the network, enable feature-map reuse by adding long-range connections\nand accommodate various network architectures/datasets. Compared to densely\nconnected networks (e.g., DenseNets), SWNets require a substantially fewer\nnumber of training parameters while maintaining a similar level of\nclassification accuracy. We evaluate our networks on various DL model\narchitectures and image classification datasets, namely, CIFAR10, CIFAR100, and\nILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement\nin convergence speed to the desired accuracy\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:41:26 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Rouhani", "Bita Darvish", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1904.04937", "submitter": "Henok Yared Mr", "authors": "Henok Yared Agizew (Mettu University, Ethiopia)", "title": "Adaptive Learning Expert System for Diagnosis and Management of Viral\n  Hepatitis", "comments": "14 pages", "journal-ref": "International journal of artificial intelligence and applications,\n  2019, Volume 10, Number 2", "doi": "10.5121/ijaia.2019.10204", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Viral hepatitis is the regularly found health problem throughout the world\namong other easily transmitted diseases, such as tuberculosis, human immune\nvirus, malaria and so on. Among all hepatitis viruses, the uppermost numbers of\ndeaths are result from the long-lasting hepatitis C infection or long-lasting\nhepatitis B. In order to develop this system, the knowledge is acquired using\nboth structured and semi-structured interviews from internists of St.Paul\nHospital. Once the knowledge is acquired, it is modeled and represented using\nrule based reasoning techniques. Both forward and backward chaining is used to\ninfer the rules and provide appropriate advices in the developed expert system.\nFor the purpose of developing the prototype expert system SWI-prolog editor\nalso used. The proposed system has the ability to adapt with dynamic knowledge\nby generalizing rules and discover new rules through learning the newly arrived\nknowledge from domain experts adaptively without any help from the knowledge\nengineer.\n  Keywords: Expert System, Diagnosis and Management of Viral Hepatitis,\nAdaptive Learning, Discovery and Generalization Mechanism\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 22:29:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Agizew", "Henok Yared", "", "Mettu University, Ethiopia"]]}, {"id": "1904.04969", "submitter": "Yu Cao", "authors": "Yu Cao, Meng Fang, Dacheng Tao", "title": "BAG: Bi-directional Attention Entity Graph Convolutional Network for\n  Multi-hop Reasoning Question Answering", "comments": "5 pages, 1 figure, accepted short paper on NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reasoning question answering requires deep comprehension of\nrelationships between various documents and queries. We propose a\nBi-directional Attention Entity Graph Convolutional Network (BAG), leveraging\nrelationships between nodes in an entity graph and attention information\nbetween a query and the entity graph, to solve this task. Graph convolutional\nnetworks are used to obtain a relation-aware representation of nodes for entity\ngraphs built from documents with multi-level features. Bidirectional attention\nis then applied on graphs and queries to generate a query-aware nodes\nrepresentation, which will be used for the final prediction. Experimental\nevaluation shows BAG achieves state-of-the-art accuracy performance on the\nQAngaroo WIKIHOP dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:40:08 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cao", "Yu", ""], ["Fang", "Meng", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.04971", "submitter": "Jiquan Ngiam", "authors": "Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam", "title": "CondConv: Conditionally Parameterized Convolutions for Efficient\n  Inference", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional layers are one of the basic building blocks of modern deep\nneural networks. One fundamental assumption is that convolutional kernels\nshould be shared for all examples in a dataset. We propose conditionally\nparameterized convolutions (CondConv), which learn specialized convolutional\nkernels for each example. Replacing normal convolutions with CondConv enables\nus to increase the size and capacity of a network, while maintaining efficient\ninference. We demonstrate that scaling networks with CondConv improves the\nperformance and inference cost trade-off of several existing convolutional\nneural network architectures on both classification and detection tasks. On\nImageNet classification, our CondConv approach applied to EfficientNet-B0\nachieves state-of-the-art performance of 78.3% accuracy with only 413M\nmultiply-adds. Code and checkpoints for the CondConv Tensorflow layer and\nCondConv-EfficientNet models are available at:\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:46:48 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 17:58:56 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 00:53:44 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Yang", "Brandon", ""], ["Bender", "Gabriel", ""], ["Le", "Quoc V.", ""], ["Ngiam", "Jiquan", ""]]}, {"id": "1904.04973", "submitter": "Yoshiharu Sato", "authors": "Yoshiharu Sato", "title": "Model-Free Reinforcement Learning for Financial Portfolios: A Brief\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial portfolio management is one of the problems that are most\nfrequently encountered in the investment industry. Nevertheless, it is not\nwidely recognized that both Kelly Criterion and Risk Parity collapse into Mean\nVariance under some conditions, which implies that a universal solution to the\nportfolio optimization problem could potentially exist. In fact, the process of\nsequential computation of optimal component weights that maximize the\nportfolio's expected return subject to a certain risk budget can be\nreformulated as a discrete-time Markov Decision Process (MDP) and hence as a\nstochastic optimal control, where the system being controlled is a portfolio\nconsisting of multiple investment components, and the control is its component\nweights. Consequently, the problem could be solved using model-free\nReinforcement Learning (RL) without knowing specific component dynamics. By\nexamining existing methods of both value-based and policy-based model-free RL\nfor the portfolio optimization problem, we identify some of the key unresolved\nquestions and difficulties facing today's portfolio managers of applying\nmodel-free RL to their investment portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:48:52 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 09:29:20 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sato", "Yoshiharu", ""]]}, {"id": "1904.05033", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Matteo Pagliardini and Martin Jaggi", "title": "Better Word Embeddings by Disentangling Contextual n-Gram Information", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained word vectors are ubiquitous in Natural Language Processing\napplications. In this paper, we show how training word embeddings jointly with\nbigram and even trigram embeddings, results in improved unigram embeddings. We\nclaim that training word embeddings along with higher n-gram embeddings helps\nin the removal of the contextual information from the unigrams, resulting in\nbetter stand-alone word embeddings. We empirically show the validity of our\nhypothesis by outperforming other competing word representation models by a\nsignificant margin on a wide variety of tasks. We make our models publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:44:06 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Gupta", "Prakhar", ""], ["Pagliardini", "Matteo", ""], ["Jaggi", "Martin", ""]]}, {"id": "1904.05046", "submitter": "Quanming Yao", "authors": "Yaqing Wang and Quanming Yao and James Kwok and Lionel M. Ni", "title": "Generalizing from a Few Examples: A Survey on Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been highly successful in data-intensive applications\nbut is often hampered when the data set is small. Recently, Few-Shot Learning\n(FSL) is proposed to tackle this problem. Using prior knowledge, FSL can\nrapidly generalize to new tasks containing only a few samples with supervised\ninformation. In this paper, we conduct a thorough survey to fully understand\nFSL. Starting from a formal definition of FSL, we distinguish FSL from several\nrelevant machine learning problems. We then point out that the core issue in\nFSL is that the empirical risk minimized is unreliable. Based on how prior\nknowledge can be used to handle this core issue, we categorize FSL methods from\nthree perspectives: (i) data, which uses prior knowledge to augment the\nsupervised experience; (ii) model, which uses prior knowledge to reduce the\nsize of the hypothesis space; and (iii) algorithm, which uses prior knowledge\nto alter the search for the best hypothesis in the given hypothesis space. With\nthis taxonomy, we review and discuss the pros and cons of each category.\nPromising directions, in the aspects of the FSL problem setups, techniques,\napplications and theories, are also proposed to provide insights for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:05:48 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 16:13:24 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 16:47:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wang", "Yaqing", ""], ["Yao", "Quanming", ""], ["Kwok", "James", ""], ["Ni", "Lionel M.", ""]]}, {"id": "1904.05072", "submitter": "Carlos Mastalli", "authors": "Rohan Budhiraja, Justin Carpentier, Carlos Mastalli and Nicolas\n  Mansard", "title": "Differential Dynamic Programming for Multi-Phase Rigid Contact Dynamics", "comments": "6 pages, IEEE RAS International Conference on Humanoid Robots", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2018.8624925", "report-no": null, "categories": "cs.RO cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy today to generate efficient locomotion movements is to\nsplit the problem into two consecutive steps: the first one generates the\ncontact sequence together with the centroidal trajectory, while the second one\ncomputes the whole-body trajectory that follows the centroidal pattern. Yet the\nsecond step is generally handled by a simple program such as an inverse\nkinematics solver. In contrast, we propose to compute the whole-body trajectory\nby using a local optimal control solver, namely Differential Dynamic\nProgramming (DDP). Our method produces more efficient motions, with lower\nforces and smaller impacts, by exploiting the Angular Momentum (AM). With this\naim, we propose an original DDP formulation exploiting the Karush-Kuhn-Tucker\nconstraint of the rigid contact model. We experimentally show the importance of\nthis approach by executing large steps walking on the real HRP-2 robot, and by\nsolving the problem of attitude control under the absence of external forces.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 09:01:20 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Budhiraja", "Rohan", ""], ["Carpentier", "Justin", ""], ["Mastalli", "Carlos", ""], ["Mansard", "Nicolas", ""]]}, {"id": "1904.05124", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila", "title": "Predicting Novel Views Using Generative Adversarial Query Network", "comments": "12 pages, 4 figures, accepted for presentation at the Scandinavian\n  Conference on Image Analysis 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of predicting a novel view of the scene using an arbitrary number\nof observations is a challenging problem for computers as well as for humans.\nThis paper introduces the Generative Adversarial Query Network (GAQN), a\ngeneral learning framework for novel view synthesis that combines Generative\nQuery Network (GQN) and Generative Adversarial Networks (GANs). The\nconventional GQN encodes input views into a latent representation that is used\nto generate a new view through a recurrent variational decoder. The proposed\nGAQN builds on this work by adding two novel aspects: First, we extend the\ncurrent GQN architecture with an adversarial loss function for improving the\nvisual quality and convergence speed. Second, we introduce a feature-matching\nloss function for stabilizing the training procedure. The experiments\ndemonstrate that GAQN is able to produce high-quality results and faster\nconvergence compared to the conventional approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:49:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nguyen-Ha", "Phong", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "1904.05168", "submitter": "Xiaobo Qu", "authors": "Xiaobo Qu, Yihui Huang, Hengfa Lu, Tianyu Qiu, Di Guo, Tatiana Agback,\n  Vladislav Orekhov, Zhong Chen", "title": "Accelerated Nuclear Magnetic Resonance Spectroscopy with Deep Learning", "comments": "23 pages, 23 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.LG math.SP physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear magnetic resonance (NMR) spectroscopy serves as an indispensable tool\nin chemistry and biology but often suffers from long experimental time. We\npresent a proof-of-concept of application of deep learning and neural network\nfor high-quality, reliable, and very fast NMR spectra reconstruction from\nlimited experimental data. We show that the neural network training can be\nachieved using solely synthetic NMR signal, which lifts the prohibiting demand\nfor a large volume of realistic training data usually required in the deep\nlearning approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:10:34 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 04:49:32 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Qu", "Xiaobo", ""], ["Huang", "Yihui", ""], ["Lu", "Hengfa", ""], ["Qiu", "Tianyu", ""], ["Guo", "Di", ""], ["Agback", "Tatiana", ""], ["Orekhov", "Vladislav", ""], ["Chen", "Zhong", ""]]}, {"id": "1904.05248", "submitter": "Reshmashree Bangalore Kantharaju", "authors": "Reshmashree B. Kantharaju, Dominic De Franco, Alison Pease, Catherine\n  Pelachaud", "title": "Is Two Better than One? Effects of Multiple Agents on User Persuasion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual humans need to be persuasive in order to promote behaviour change in\nhuman users. While several studies have focused on understanding the numerous\naspects that influence the degree of persuasion, most of them are limited to\ndyadic interactions. In this paper, we present an evaluation study focused on\nunderstanding the effects of multiple agents on user's persuasion. Along with\ngender and status (authoritative & peer), we also look at type of focus\nemployed by the agent i.e., user-directed where the agent aims to persuade by\naddressing the user directly and vicarious where the agent aims to persuade the\nuser, who is an observer, indirectly by engaging another agent in the\ndiscussion. Participants were randomly assigned to one of the 12 conditions and\npresented with a persuasive message by one or several virtual agents. A\nquestionnaire was used to measure perceived interpersonal attitude, credibility\nand persuasion. Results indicate that credibility positively affects\npersuasion. In general, multiple agent setting, irrespective of the focus, was\nmore persuasive than single agent setting. Although, participants favored\nuser-directed setting and reported it to be persuasive and had an increased\nlevel of trust in the agents, the actual change in persuasion score reflects\nthat vicarious setting was the most effective in inducing behaviour change. In\naddition to this, the study also revealed that authoritative agents were the\nmost persuasive.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:38:30 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kantharaju", "Reshmashree B.", ""], ["De Franco", "Dominic", ""], ["Pease", "Alison", ""], ["Pelachaud", "Catherine", ""]]}, {"id": "1904.05250", "submitter": "Antonino Furnari", "authors": "Antonino Furnari, Sebastiano Battiato, Kristen Grauman, Giovanni Maria\n  Farinella", "title": "Next-Active-Object prediction from Egocentric Videos", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation, Volume\n  49, 2017, Pages 401-411, ISSN 1047-3203", "doi": "10.1016/j.jvcir.2017.10.004", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although First Person Vision systems can sense the environment from the\nuser's perspective, they are generally unable to predict his intentions and\ngoals. Since human activities can be decomposed in terms of atomic actions and\ninteractions with objects, intelligent wearable systems would benefit from the\nability to anticipate user-object interactions. Even if this task is not\ntrivial, the First Person Vision paradigm can provide important cues to address\nthis challenge. We propose to exploit the dynamics of the scene to recognize\nnext-active-objects before an object interaction begins. We train a classifier\nto discriminate trajectories leading to an object activation from all others\nand forecast next-active-objects by analyzing fixed-length trajectory segments\nwithin a temporal sliding window. The proposed method compares favorably with\nrespect to several baselines on the Activity of Daily Living (ADL) egocentric\ndataset comprising 10 hours of videos acquired by 20 subjects while performing\nunconstrained interactions with several objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:39:19 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Furnari", "Antonino", ""], ["Battiato", "Sebastiano", ""], ["Grauman", "Kristen", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "1904.05265", "submitter": "Peng Jiang Dr.", "authors": "Bin Liu, Qian Guo, Shucai Li, Benchao Liu, Yuxiao Ren, Yonghao Pang,\n  Xu Guo, Lanbo Liu, Peng Jiang", "title": "Deep Learning Inversion of Electrical Resistivity Data", "comments": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "journal-ref": null, "doi": "10.1109/TGRS.2020.2969040", "report-no": null, "categories": "cs.CV cs.AI physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse problem of electrical resistivity surveys (ERSs) is difficult\nbecause of its nonlinear and ill-posed nature. For this task, traditional\nlinear inversion methods still face challenges such as suboptimal approximation\nand initial model selection. Inspired by the remarkable nonlinear mapping\nability of deep learning approaches, in this article, we propose to build the\nmapping from apparent resistivity data (input) to resistivity model (output)\ndirectly by convolutional neural networks (CNNs). However, the vertically\nvarying characteristic of patterns in the apparent resistivity data may cause\nambiguity when using CNNs with the weight sharing and effective receptive field\nproperties. To address the potential issue, we supply an additional tier\nfeature map to CNNs to help those aware of the relationship between input and\noutput. Based on the prevalent U-Net architecture, we design our network\n(ERSInvNet) that can be trained end-to-end and can reach a very fast inference\nspeed during testing. We further introduce a depth weighting function and a\nsmooth constraint into loss function to improve inversion accuracy for the deep\nregion and suppress false anomalies. Six groups of experiments are considered\nto demonstrate the feasibility and efficiency of the proposed methods.\nAccording to the comprehensive qualitative analysis and quantitative\ncomparison, ERSInvNet with tier feature map, smooth constraints, and depth\nweighting function together achieve the best performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:06:29 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:14:16 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "Bin", ""], ["Guo", "Qian", ""], ["Li", "Shucai", ""], ["Liu", "Benchao", ""], ["Ren", "Yuxiao", ""], ["Pang", "Yonghao", ""], ["Guo", "Xu", ""], ["Liu", "Lanbo", ""], ["Jiang", "Peng", ""]]}, {"id": "1904.05276", "submitter": "Anupiya Nugaliyadde Mr", "authors": "K.S.D. Ishwari, A.K.R.R.Aneeze, S.Sudheesan, H.J.D.A. Karunaratne, A.\n  Nugaliyadde, Y. Mallawarrachchi", "title": "Advances in Natural Language Question Answering: A Review", "comments": "arXiv admin note: text overlap with arXiv:1609.04667 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering has recently received high attention from artificial\nintelligence communities due to the advancements in learning technologies.\nEarly question answering models used rule-based approaches and moved to the\nstatistical approach to address the vastly available information. However,\nstatistical approaches are shown to underperform in handling the dynamic nature\nand the variation of language. Therefore, learning models have shown the\ncapability of handling the dynamic nature and variations in language. Many deep\nlearning methods have been introduced to question answering. Most of the deep\nlearning approaches have shown to achieve higher results compared to machine\nlearning and statistical methods. The dynamic nature of language has profited\nfrom the nonlinear learning in deep learning. This has created prominent\nsuccess and a spike in work on question answering. This paper discusses the\nsuccesses and challenges in question answering question answering systems and\ntechniques that are used in these challenges.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:26:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ishwari", "K. S. D.", ""], ["Aneeze", "A. K. R. R.", ""], ["Sudheesan", "S.", ""], ["Karunaratne", "H. J. D. A.", ""], ["Nugaliyadde", "A.", ""], ["Mallawarrachchi", "Y.", ""]]}, {"id": "1904.05381", "submitter": "Xudong Sun", "authors": "Xudong Sun, Jiali Lin, Bernd Bischl", "title": "ReinBo: Machine Learning pipeline search and configuration with Bayesian\n  Optimization embedded Reinforcement Learning", "comments": null, "journal-ref": "ECML PKDD 2019: Machine Learning and Knowledge Discovery in\n  Databases pp 68-84", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning pipeline potentially consists of several stages of\noperations like data preprocessing, feature engineering and machine learning\nmodel training. Each operation has a set of hyper-parameters, which can become\nirrelevant for the pipeline when the operation is not selected. This gives rise\nto a hierarchical conditional hyper-parameter space. To optimize this mixed\ncontinuous and discrete conditional hierarchical hyper-parameter space, we\npropose an efficient pipeline search and configuration algorithm which combines\nthe power of Reinforcement Learning and Bayesian Optimization. Empirical\nresults show that our method performs favorably compared to state of the art\nmethods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:26:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sun", "Xudong", ""], ["Lin", "Jiali", ""], ["Bischl", "Bernd", ""]]}, {"id": "1904.05405", "submitter": "Cogan Shimizu", "authors": "Cogan Shimizu and Quinn Hirt and Pascal Hitzler", "title": "MODL: A Modular Ontology Design Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-based, modular ontologies have several beneficial properties that\nlend themselves to FAIR data practices, especially as it pertains to\nInteroperability and Reusability. However, developing such ontologies has a\nhigh upfront cost, e.g. reusing a pattern is predicated upon being aware of its\nexistence in the first place. Thus, to help overcome these barriers, we have\ndeveloped MODL: a modular ontology design library. MODL is a curated collection\nof well-documented ontology design patterns, drawn from a wide variety of\ninterdisciplinary use-cases. In this paper we present MODL as a resource,\ndiscuss its use, and provide some examples of its contents.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:36:36 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Shimizu", "Cogan", ""], ["Hirt", "Quinn", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1904.05426", "submitter": "Ronald Cardenas Acosta", "authors": "Ronald Cardenas, Ying Lin, Heng Ji, Jonathan May", "title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource\n  Languages", "comments": "NAACL-HLT 2019, 12 pages, code available at\n  https://github.com/isi-nlp/universal-cipher-pos-tagging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised part of speech (POS) tagging is often framed as a clustering\nproblem, but practical taggers need to \\textit{ground} their clusters as well.\nGrounding generally requires reference labeled data, a luxury a low-resource\nlanguage might not have. In this work, we describe an approach for low-resource\nunsupervised POS tagging that yields fully grounded output and requires no\nlabeled training data. We find the classic method of Brown et al. (1992)\nclusters well in our use case and employ a decipherment-based approach to\ngrounding. This approach presumes a sequence of cluster IDs is a `ciphertext'\nand seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We\nshow intrinsically that, despite the difficulty of the task, we obtain\nreasonable performance across a variety of languages. We also show\nextrinsically that incorporating our POS tagger into a name tagger leads to\nstate-of-the-art tagging performance in Sinhalese and Kinyarwanda, two\nlanguages with nearly no labeled POS data available. We further demonstrate our\ntagger's utility by incorporating it into a true `zero-resource' variant of the\nMalopa (Ammar et al., 2016) dependency parser model that removes the current\nreliance on multilingual resources and gold POS tags for new languages.\nExperiments show that including our tagger makes up much of the accuracy lost\nwhen gold POS tags are unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:22:31 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cardenas", "Ronald", ""], ["Lin", "Ying", ""], ["Ji", "Heng", ""], ["May", "Jonathan", ""]]}, {"id": "1904.05440", "submitter": "Ashutosh Modi", "authors": "Yeyao Zhang, Eleftheria Tsipidi, Sasha Schriber, Mubbasir Kapadia,\n  Markus Gross, Ashutosh Modi", "title": "Generating Animations from Screenplays", "comments": "9+1+6 Pages, Accepted at StarSEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating animation from natural language text finds\napplication in a number of areas e.g. movie script writing, instructional\nvideos, and public safety. However, translating natural language text into\nanimation is a challenging task. Existing text-to-animation systems can handle\nonly very simple sentences, which limits their applications. In this paper, we\ndevelop a text-to-animation system which is capable of handling complex\nsentences. We achieve this by introducing a text simplification step into the\nprocess. Building on an existing animation generation system for screenwriting,\nwe create a robust NLP pipeline to extract information from screenplays and map\nthem to the system's knowledge base. We develop a set of linguistic\ntransformation rules that simplify complex sentences. Information extracted\nfrom the simplified sentences is used to generate a rough storyboard and video\ndepicting the text. Our sentence simplification module outperforms existing\nsystems in terms of BLEU and SARI metrics.We further evaluated our system via a\nuser study: 68 % participants believe that our system generates reasonable\nanimation from input screenplays.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:04:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zhang", "Yeyao", ""], ["Tsipidi", "Eleftheria", ""], ["Schriber", "Sasha", ""], ["Kapadia", "Mubbasir", ""], ["Gross", "Markus", ""], ["Modi", "Ashutosh", ""]]}, {"id": "1904.05493", "submitter": "Juan Liu", "authors": "Juan Liu and Kevin M. Koch", "title": "Non-locally Encoder-Decoder Convolutional Network for Whole Brain QSM\n  Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative Susceptibility Mapping (QSM) reconstruction is a challenging\ninverse problem driven by ill conditioning of its field-to -susceptibility\ntransformation. State-of-art QSM reconstruction methods either suffer from\nimage artifacts or long computation times, which limits QSM clinical\ntranslation efforts. To overcome these limitations, a non-locally\nencoder-decoder gated convolutional neural network is trained to infer whole\nbrain susceptibility map, using the local field and brain mask as the inputs.\nThe performance of the proposed method is evaluated relative to synthetic data,\na publicly available challenge dataset, and clinical datasets. The proposed\napproach can outperform existing methods on quantitative metrics and visual\nassessment of image sharpness and streaking artifacts. The estimated\nsusceptibility maps can preserve conspicuity of fine features and suppress\nstreaking artifacts. The demonstrated methods have potential value in advancing\nQSM clinical research and aiding in the translation of QSM to clinical\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 01:20:05 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liu", "Juan", ""], ["Koch", "Kevin M.", ""]]}, {"id": "1904.05530", "submitter": "Xiang Ren", "authors": "Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren", "title": "Recurrent Event Network: Autoregressive Structure Inference over\n  Temporal Knowledge Graphs", "comments": "15 pages, 8 figures, accepted at as full paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph reasoning is a critical task in natural language processing.\nThe task becomes more challenging on temporal knowledge graphs, where each fact\nis associated with a timestamp. Most existing methods focus on reasoning at\npast timestamps and they are not able to predict facts happening in the future.\nThis paper proposes Recurrent Event Network (RE-NET), a novel autoregressive\narchitecture for predicting future interactions. The occurrence of a fact\n(event) is modeled as a probability distribution conditioned on temporal\nsequences of past knowledge graphs. Specifically, our RE-NET employs a\nrecurrent event encoder to encode past facts and uses a neighborhood aggregator\nto model the connection of facts at the same timestamp. Future facts can then\nbe inferred in a sequential manner based on the two modules. We evaluate our\nproposed method via link prediction at future times on five public datasets.\nThrough extensive experiments, we demonstrate the strength of RENET, especially\non multi-step inference over future timestamps, and achieve state-of-the-art\nperformance on all five datasets. Code and data can be found at\nhttps://github.com/INK-USC/RE-Net.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 04:45:42 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 19:06:37 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 03:32:40 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 18:40:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jin", "Woojeong", ""], ["Qu", "Meng", ""], ["Jin", "Xisen", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.05538", "submitter": "Annie Xie", "authors": "Annie Xie, Frederik Ebert, Sergey Levine, Chelsea Finn", "title": "Improvisation through Physical Understanding: Using Novel Objects as\n  Tools with Visual Foresight", "comments": "Videos available at https://sites.google.com/view/gvf-tool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have enabled robots to learn narrow, yet complex\ntasks and also perform broad, yet simple skills with a wide variety of objects.\nHowever, learning a model that can both perform complex tasks and generalize to\npreviously unseen objects and goals remains a significant challenge. We study\nthis challenge in the context of \"improvisational\" tool use: a robot is\npresented with novel objects and a user-specified goal (e.g., sweep some\nclutter into the dustpan), and must figure out, using only raw image\nobservations, how to accomplish the goal using the available objects as tools.\nWe approach this problem by training a model with both a visual and physical\nunderstanding of multi-object interactions, and develop a sampling-based\noptimizer that can leverage these interactions to accomplish tasks. We do so by\ncombining diverse demonstration data with self-supervised interaction data,\naiming to leverage the interaction data to build generalizable models and the\ndemonstration data to guide the model-based RL planner to solve complex tasks.\nOur experiments show that our approach can solve a variety of complex tool use\ntasks from raw pixel inputs, outperforming both imitation learning and\nself-supervised learning individually. Furthermore, we show that the robot can\nperceive and use novel objects as tools, including objects that are not\nconventional tools, while also choosing dynamically to use or not use tools\ndepending on whether or not they are required.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 05:20:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Xie", "Annie", ""], ["Ebert", "Frederik", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1904.05548", "submitter": "Zilong Zheng", "authors": "Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu", "title": "Reasoning Visual Dialogs with Structural and Partial Observations", "comments": "CVPR 2019 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model to address the task of Visual Dialog which exhibits\ncomplex dialog structures. To obtain a reasonable answer based on the current\nquestion and the dialog history, the underlying semantic dependencies between\ndialog entities are essential. In this paper, we explicitly formalize this task\nas inference in a graphical model with partially observed nodes and unknown\ngraph structures (relations in dialog). The given dialog entities are viewed as\nthe observed nodes. The answer to a given question is represented by a node\nwith missing value. We first introduce an Expectation Maximization algorithm to\ninfer both the underlying dialog structures and the missing node values\n(desired answers). Based on this, we proceed to propose a differentiable graph\nneural network (GNN) solution that approximates this process. Experiment\nresults on the VisDial and VisDial-Q datasets show that our model outperforms\ncomparative methods. It is also observed that our method can infer the\nunderlying dialog structure for better dialog reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:46:15 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 23:40:33 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zheng", "Zilong", ""], ["Wang", "Wenguan", ""], ["Qi", "Siyuan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1904.05596", "submitter": "Olivier Corby", "authors": "Guidedi Kaladzavi (UMa), Papa Fary Diallo (WIMMICS), Cedric B\\'er\\'e,\n  Olivier Corby (WIMMICS), Isabelle Mirbel (WIMMICS), Moussa Lo (UGB), Dina\n  Taiwe Kolyang (LaRI)", "title": "Ontologies-based Architecture for Sociocultural Knowledge\n  Co-Construction Systems", "comments": null, "journal-ref": "Online Journal of Applied Knowledge Management, 2018, 6:1", "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the evolution of the semantic wiki engine based platforms, two\nmain approaches could be distinguished: Ontologies for Wikis (OfW) and Wikis\nfor Ontologies (WfO). OfW vision requires existing ontologies to be imported.\nMost of them use the RDF-based (Resource Description Framework) systems in\nconjunction with the standard SQL (Structured Query Language) database to\nmanage and query semantic data. But, relational database is not an ideal type\nof storage for semantic data. A more natural data model for SMW (Semantic\nMediaWiki) is RDF, a data format that organizes information in graphs rather\nthan in fixed database tables. This paper presents an ontology based\narchitecture, which aims to implement this idea. The architecture mainly\nincludes three layered functional architectures: Web User Interface Layer,\nSemantic Layer and Persistence Layer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 09:23:49 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kaladzavi", "Guidedi", "", "UMa"], ["Diallo", "Papa Fary", "", "WIMMICS"], ["B\u00e9r\u00e9", "Cedric", "", "WIMMICS"], ["Corby", "Olivier", "", "WIMMICS"], ["Mirbel", "Isabelle", "", "WIMMICS"], ["Lo", "Moussa", "", "UGB"], ["Kolyang", "Dina Taiwe", "", "LaRI"]]}, {"id": "1904.05606", "submitter": "Pavel Kral", "authors": "Ji\\v{r}\\'i Mart\\'inek, Pavel Kr\\'al, Ladislav Lenc, Christophe\n  Cerisara", "title": "Multi-lingual Dialogue Act Recognition with Deep Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with multi-lingual dialogue act (DA) recognition. The\nproposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed for\nthis task. The first approach uses one general model trained on the embeddings\nfrom all available languages. The second method trains the model on a single\npivot language and a linear transformation method is used to project other\nlanguages onto the pivot language. The popular convolutional neural network and\nLSTM architectures with different set-ups are used as classifiers. To the best\nof our knowledge this is the first attempt at multi-lingual DA recognition\nusing neural networks. The multi-lingual models are validated experimentally on\ntwo languages from the Verbmobil corpus.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 09:55:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Mart\u00ednek", "Ji\u0159\u00ed", ""], ["Kr\u00e1l", "Pavel", ""], ["Lenc", "Ladislav", ""], ["Cerisara", "Christophe", ""]]}, {"id": "1904.05673", "submitter": "Thomas Boulay", "authors": "Thomas Boulay, Said El-Hachimi, Mani Kumar Surisetti, Pullarao Maddu,\n  Saranya Kandan", "title": "YUVMultiNet: Real-time YUV multi-task CNN for autonomous driving", "comments": "This paper is accepted for CVPR workshop demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-task convolutional neural network (CNN)\narchitecture optimized for a low power automotive grade SoC. We introduce a\nnetwork based on a unified architecture where the encoder is shared among the\ntwo tasks namely detection and segmentation. The pro-posed network runs at\n25FPS for 1280x800 resolution. We briefly discuss the methods used to optimize\nthe network architecture such as using native YUV image directly, optimization\nof layers & feature maps and applying quantization. We also focus on memory\nbandwidth in our design as convolutions are data intensives and most SOCs are\nbandwidth bottlenecked. We then demonstrate the efficiency of our proposed\nnetwork for a dedicated CNN accelerators presenting the key performance\nindicators (KPI) for the detection and segmentation tasks obtained from the\nhardware execution and the corresponding run-time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:08:05 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Boulay", "Thomas", ""], ["El-Hachimi", "Said", ""], ["Surisetti", "Mani Kumar", ""], ["Maddu", "Pullarao", ""], ["Kandan", "Saranya", ""]]}, {"id": "1904.05759", "submitter": "Pablo Hernandez-Leal", "authors": "Bilal Kartal, Pablo Hernandez-Leal, Chao Gao, and Matthew E. Taylor", "title": "Safer Deep RL with Shallow MCTS: A Case Study in Pommerman", "comments": "Adaptive Learning Agents (ALA) Workshop at AAMAS 2019. arXiv admin\n  note: substantial text overlap with arXiv:1812.00045", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe reinforcement learning has many variants and it is still an open\nresearch problem. Here, we focus on how to use action guidance by means of a\nnon-expert demonstrator to avoid catastrophic events in a domain with sparse,\ndelayed, and deceptive rewards: the recently-proposed multi-agent benchmark of\nPommerman. This domain is very challenging for reinforcement learning (RL) ---\npast work has shown that model-free RL algorithms fail to achieve significant\nlearning. In this paper, we shed light into the reasons behind this failure by\nexemplifying and analyzing the high rate of catastrophic events (i.e.,\nsuicides) that happen under random exploration in this domain. While model-free\nrandom exploration is typically futile, we propose a new framework where even a\nnon-expert simulated demonstrator, e.g., planning algorithms such as Monte\nCarlo tree search with small number of rollouts, can be integrated to\nasynchronous distributed deep reinforcement learning methods. Compared to\nvanilla deep RL algorithms, our proposed methods both learn faster and converge\nto better policies on a two-player mini version of the Pommerman game.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:34:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kartal", "Bilal", ""], ["Hernandez-Leal", "Pablo", ""], ["Gao", "Chao", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "1904.05775", "submitter": "Hugo Gilbert", "authors": "Bruno Escoffier, Hugo Gilbert, Ad\\`ele Pass-Lanneau", "title": "The Convergence of Iterative Delegations in Liquid Democracy in a Social\n  Network", "comments": "arXiv admin note: text overlap with arXiv:1809.04362. Changes w.r.t.\n  previous version: added Theorem 9 on page 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid democracy is a collective decision making paradigm which lies between\ndirect and representative democracy. One of its main features is that voters\ncan delegate their votes in a transitive manner such that: A delegates to B and\nB delegates to C leads to A indirectly delegates to C. These delegations can be\neffectively empowered by implementing liquid democracy in a social network, so\nthat voters can delegate their votes to any of their neighbors in the network.\nHowever, it is uncertain that such a delegation process will lead to a stable\nstate where all voters are satisfied with the people representing them. We\nstudy the stability (w.r.t. voters preferences) of the delegation process in\nliquid democracy and model it as a game in which the players are the voters and\nthe strategies are their possible delegations. We answer several questions on\nthe equilibria of this process in any social network or in social networks that\ncorrespond to restricted types of graphs.\n  We show that a Nash-equilibrium may not exist, and that it is even\nNP-complete to decide whether one exists or not. This holds even if the social\nnetwork is a complete graph or a bounded degree graph. We further show that\nthis existence problem is W[1]-hard w.r.t. the treewidth of the social network.\nBesides these hardness results, we demonstrate that an equilibrium always\nexists whatever the preferences of the voters iff the social network is a tree.\nWe design a dynamic programming procedure to determine some desirable\nequilibria (e.g., minimizing the dissatisfaction of the voters) in polynomial\ntime for tree social networks. Lastly, we study the convergence of delegation\ndynamics. Unfortunately, when an equilibrium exists, we show that a best\nresponse dynamics may not converge, even if the social network is a path or a\ncomplete graph.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:03:15 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 11:47:59 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Escoffier", "Bruno", ""], ["Gilbert", "Hugo", ""], ["Pass-Lanneau", "Ad\u00e8le", ""]]}, {"id": "1904.05811", "submitter": "Dan Busbridge", "authors": "Dan Busbridge, Dane Sherburn, Pietro Cavallo and Nils Y. Hammerla", "title": "Relational Graph Attention Networks", "comments": "10 pages + 8 pages of appendices. Layer implementation available at\n  https://github.com/Babylonpartners/rgat/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Relational Graph Attention Networks, a class of models that\nextends non-relational graph attention mechanisms to incorporate relational\ninformation, opening up these methods to a wider variety of problems. A\nthorough evaluation of these models is performed, and comparisons are made\nagainst established benchmarks. To provide a meaningful comparison, we retrain\nRelational Graph Convolutional Networks, the spectral counterpart of Relational\nGraph Attention Networks, and evaluate them under the same conditions. We find\nthat Relational Graph Attention Networks perform worse than anticipated,\nalthough some configurations are marginally beneficial for modelling molecular\nproperties. We provide insights as to why this may be, and suggest both\nmodifications to evaluation strategies, as well as directions to investigate\nfor future work.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:11:36 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Busbridge", "Dan", ""], ["Sherburn", "Dane", ""], ["Cavallo", "Pietro", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1904.05835", "submitter": "Zhenwen Dai", "authors": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen\n  Dai", "title": "Variational Information Distillation for Knowledge Transfer", "comments": "To appear at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from a teacher neural network pretrained on the same\nor a similar task to a student neural network can significantly improve the\nperformance of the student neural network. Existing knowledge transfer\napproaches match the activations or the corresponding hand-crafted features of\nthe teacher and the student networks. We propose an information-theoretic\nframework for knowledge transfer which formulates knowledge transfer as\nmaximizing the mutual information between the teacher and the student networks.\nWe compare our method with existing knowledge transfer methods on both\nknowledge distillation and transfer learning tasks and show that our method\nconsistently outperforms existing methods. We further demonstrate the strength\nof our method on knowledge transfer across heterogeneous network architectures\nby transferring knowledge from a convolutional neural network (CNN) to a\nmulti-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly\noutperforms the-state-of-the-art methods and it achieves similar performance to\nthe CNN with a single convolutional layer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:39:19 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Ahn", "Sungsoo", ""], ["Hu", "Shell Xu", ""], ["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Dai", "Zhenwen", ""]]}, {"id": "1904.05876", "submitter": "Idan Schwartz", "authors": "Idan Schwartz, Alexander Schwing and Tamir Hazan", "title": "A Simple Baseline for Audio-Visual Scene-Aware Dialog", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed audio-visual scene-aware dialog task paves the way to a\nmore data-driven way of learning virtual assistants, smart speakers and car\nnavigation systems. However, very little is known to date about how to\neffectively extract meaningful information from a plethora of sensors that\npound the computational engine of those devices. Therefore, in this paper, we\nprovide and carefully analyze a simple baseline for audio-visual scene-aware\ndialog which is trained end-to-end. Our method differentiates in a data-driven\nmanner useful signals from distracting ones using an attention mechanism. We\nevaluate the proposed approach on the recently introduced and challenging\naudio-visual scene-aware dataset, and demonstrate the key features that permit\nto outperform the current state-of-the-art by more than 20\\% on CIDEr.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:51 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Schwartz", "Idan", ""], ["Schwing", "Alexander", ""], ["Hazan", "Tamir", ""]]}, {"id": "1904.05879", "submitter": "Unnat Jain", "authors": "Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana\n  Lazebnik, Ali Farhadi, Alexander Schwing and Aniruddha Kembhavi", "title": "Two Body Problem: Collaborative Visual Task Completion", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration is a necessary skill to perform tasks that are beyond one\nagent's capabilities. Addressed extensively in both conventional and modern AI,\nmulti-agent collaboration has often been studied in the context of simple grid\nworlds. We argue that there are inherently visual aspects to collaboration\nwhich should be studied in visually rich environments. A key element in\ncollaboration is communication that can be either explicit, through messages,\nor implicit, through perception of the other agents and the visual world.\nLearning to collaborate in a visual environment entails learning (1) to perform\nthe task, (2) when and what to communicate, and (3) how to act based on these\ncommunications and the perception of the visual world. In this paper we study\nthe problem of learning to collaborate directly from pixels in AI2-THOR and\ndemonstrate the benefits of explicit and implicit modes of communication to\nperform visual tasks. Refer to our project page for more details:\nhttps://prior.allenai.org/projects/two-body-problem\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:57 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Jain", "Unnat", ""], ["Weihs", "Luca", ""], ["Kolve", "Eric", ""], ["Rastegari", "Mohammad", ""], ["Lazebnik", "Svetlana", ""], ["Farhadi", "Ali", ""], ["Schwing", "Alexander", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1904.05880", "submitter": "Idan Schwartz", "authors": "Idan Schwartz and Seunghak Yu and Tamir Hazan and Alexander Schwing", "title": "Factor Graph Attention", "comments": "Accepted to CVPR 2019; revised version includes bottom-up features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:58 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 20:05:12 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 23:35:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Schwartz", "Idan", ""], ["Yu", "Seunghak", ""], ["Hazan", "Tamir", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05902", "submitter": "Dmitry Yudin", "authors": "Adriano Macarone Palmieri, Egor Kovlakov, Federico Bianchi, Dmitry\n  Yudin, Stanislav Straupe, Jacob Biamonte, Sergei Kulik", "title": "Experimental neural network enhanced quantum tomography", "comments": "11 pages, 3+6 figures; All data and source code are available online;\n  RevTeX", "journal-ref": "npj Quantum Information 6:20 (2020)", "doi": "10.1038/s41534-020-0248-6", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum tomography is currently ubiquitous for testing any implementation of\na quantum information processing device. Various sophisticated procedures for\nstate and process reconstruction from measured data are well developed and\nbenefit from precise knowledge of the model describing state preparation and\nthe measurement apparatus. However, physical models suffer from intrinsic\nlimitations as actual measurement operators and trial states cannot be known\nprecisely. This scenario inevitably leads to state-preparation-and-measurement\n(SPAM) errors degrading reconstruction performance. Here we develop and\nexperimentally implement a machine learning based protocol reducing SPAM\nerrors. We trained a supervised neural network to filter the experimental data\nand hence uncovered salient patterns that characterize the measurement\nprobabilities for the original state and the ideal experimental apparatus free\nfrom SPAM errors. We compared the neural network state reconstruction protocol\nwith a protocol treating SPAM errors by process tomography, as well as to a\nSPAM-agnostic protocol with idealized measurements. The average reconstruction\nfidelity is shown to be enhanced by 10\\% and 27\\%, respectively. The presented\nmethods apply to the vast range of quantum experiments which rely on\ntomography.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:00:13 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:47:48 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Palmieri", "Adriano Macarone", ""], ["Kovlakov", "Egor", ""], ["Bianchi", "Federico", ""], ["Yudin", "Dmitry", ""], ["Straupe", "Stanislav", ""], ["Biamonte", "Jacob", ""], ["Kulik", "Sergei", ""]]}, {"id": "1904.05967", "submitter": "Xin Wang", "authors": "Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, Joseph E. Gonzalez", "title": "TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning good feature embeddings for images often requires substantial\ntraining data. As a consequence, in settings where training data is limited\n(e.g., few-shot and zero-shot learning), we are typically forced to use a\ngeneric feature embedding across various tasks. Ideally, we want to construct\nfeature embeddings that are tuned for the given task. In this work, we propose\nTask-Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt the\nimage representation to a new task in a meta learning fashion. Our network is\ncomposed of a meta learner and a prediction network. Based on a task input, the\nmeta learner generates parameters for the feature layers in the prediction\nnetwork so that the feature embedding can be accurately adjusted for that task.\nWe show that TAFE-Net is highly effective in generalizing to new tasks or\nconcepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot and\nfew-shot learning. Our model matches or exceeds the state-of-the-art on all\ntasks. In particular, our approach improves the prediction accuracy of unseen\nattribute-object pairs by 4 to 15 points on the challenging visual\nattribute-object composition task.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 22:14:45 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Wang", "Ruth", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1904.06025", "submitter": "Yeping Hu", "authors": "Yeping Hu, Alireza Nakhaei, Masayoshi Tomizuka, and Kikuo Fujimura", "title": "Interaction-aware Decision Making with Adaptive Strategies under Merging\n  Scenarios", "comments": "Best Paper Finalist of IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to drive safely and efficiently under merging scenarios, autonomous\nvehicles should be aware of their surroundings and make decisions by\ninteracting with other road participants. Moreover, different strategies should\nbe made when the autonomous vehicle is interacting with drivers having\ndifferent level of cooperativeness. Whether the vehicle is on the merge-lane or\nmain-lane will also influence the driving maneuvers since drivers will behave\ndifferently when they have the right-of-way than otherwise. Many traditional\nmethods have been proposed to solve decision making problems under merging\nscenarios. However, these works either are incapable of modeling complicated\ninteractions or require implementing hand-designed rules which cannot properly\nhandle the uncertainties in real-world scenarios. In this paper, we proposed an\ninteraction-aware decision making with adaptive strategies (IDAS) approach that\ncan let the autonomous vehicle negotiate the road with other drivers by\nleveraging their cooperativeness under merging scenarios. A single policy is\nlearned under the multi-agent reinforcement learning (MARL) setting via the\ncurriculum learning strategy, which enables the agent to automatically infer\nother drivers' various behaviors and make decisions strategically. A masking\nmechanism is also proposed to prevent the agent from exploring states that\nviolate common sense of human judgment and increase the learning efficiency. An\nexemplar merging scenario was used to implement and examine the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 04:01:18 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 18:00:23 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Hu", "Yeping", ""], ["Nakhaei", "Alireza", ""], ["Tomizuka", "Masayoshi", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1904.06100", "submitter": "Ismini Lourentzou", "authors": "Ismini Lourentzou, Kabir Manghnani, ChengXiang Zhai", "title": "Adapting Sequence to Sequence models for Text Normalization in Social\n  Media", "comments": "Accepted at the 13th International AAAI Conference on Web and Social\n  Media (ICWSM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media offer an abundant source of valuable raw data, however informal\nwriting can quickly become a bottleneck for many natural language processing\n(NLP) tasks. Off-the-shelf tools are usually trained on formal text and cannot\nexplicitly handle noise found in short online posts. Moreover, the variety of\nfrequently occurring linguistic variations presents several challenges, even\nfor humans who might not be able to comprehend the meaning of such posts,\nespecially when they contain slang and abbreviations. Text Normalization aims\nto transform online user-generated text to a canonical form. Current text\nnormalization systems rely on string or phonetic similarity and classification\nmodels that work on a local fashion. We argue that processing contextual\ninformation is crucial for this task and introduce a social media text\nnormalization hybrid word-character attention-based encoder-decoder model that\ncan serve as a pre-processing step for NLP applications to adapt to noisy text\nin social media. Our character-based component is trained on synthetic\nadversarial examples that are designed to capture errors commonly found in\nonline user-generated text. Experiments show that our model surpasses neural\narchitectures designed for text normalization and achieves comparable\nperformance with state-of-the-art related work.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 08:45:43 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lourentzou", "Ismini", ""], ["Manghnani", "Kabir", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "1904.06110", "submitter": "Stefano Nichele", "authors": "Joachim Berg, Nils Gustav Andreas Berggren, Sivert Allergodt\n  Borgeteien, Christian Ruben Alexander Jahren, Arqam Sajid, Stefano Nichele", "title": "Evolved Art with Transparent, Overlapping, and Geometric Shapes", "comments": "Proceedings of the Norwegian AI Symposium 2019 (NAIS 2019),\n  Trondheim, Norway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, an evolutionary art project is presented where images are\napproximated by transparent, overlapping and geometric shapes of different\ntypes, e.g., polygons, circles, lines. Genotypes representing features and\norder of the geometric shapes are evolved with a fitness function that has the\ncorresponding pixels of an input image as a target goal. A\ngenotype-to-phenotype mapping is therefore applied to render images, as the\nchosen genetic representation is indirect, i.e., genotypes do not include\npixels but a combination of shapes with their properties. Different\ncombinations of shapes, quantity of shapes, mutation types and populations are\ntested. The goal of the work herein is twofold: (1) to approximate images as\nprecisely as possible with evolved indirect encodings, (2) to produce visually\nappealing results and novel artistic styles.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:09:56 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 13:44:43 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 11:57:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Berg", "Joachim", ""], ["Berggren", "Nils Gustav Andreas", ""], ["Borgeteien", "Sivert Allergodt", ""], ["Jahren", "Christian Ruben Alexander", ""], ["Sajid", "Arqam", ""], ["Nichele", "Stefano", ""]]}, {"id": "1904.06156", "submitter": "Ke Zuo", "authors": "Peizhen Xie, Ke Zuo, Yu Zhang, Fangfang Li, Mingzhu Yin, Kai Lu", "title": "Interpretable Classification from Skin Cancer Histology Slides Using\n  Deep Learning: A Retrospective Multicenter Study", "comments": "6 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For diagnosing melanoma, hematoxylin and eosin (H&E) stained tissue slides\nremains the gold standard. These images contain quantitative information in\ndifferent magnifications. In the present study, we investigated whether deep\nconvolutional neural networks can extract structural features of complex\ntissues directly from these massive size images in a patched way. In order to\nface the challenge arise from morphological diversity in histopathological\nslides, we built a multicenter database of 2241 digital whole-slide images from\n1321 patients from 2008 to 2018. We trained both ResNet50 and Vgg19 using over\n9.95 million patches by transferring learning, and test performance with two\nkinds of critical classifications: malignant melanomas versus benign nevi in\nseparate and mixed magnification; and distinguish among nevi in maximum\nmagnification. The CNNs achieves superior performance across both tasks,\ndemonstrating an AI capable of classifying skin cancer in the analysis from\nhistopathological images. For making the classifications reasonable, the\nvisualization of CNN representations is furthermore used to identify cells\nbetween melanoma and nevi. Regions of interest (ROI) are also located which are\nsignificantly helpful, giving pathologists more support of correctly diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 11:17:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Xie", "Peizhen", ""], ["Zuo", "Ke", ""], ["Zhang", "Yu", ""], ["Li", "Fangfang", ""], ["Yin", "Mingzhu", ""], ["Lu", "Kai", ""]]}, {"id": "1904.06250", "submitter": "Jiaqi Guan", "authors": "Jiaqi Guan, Ye Yuan, Kris M. Kitani, Nicholas Rhinehart", "title": "Generative Hybrid Representations for Activity Forecasting with\n  No-Regret Learning", "comments": "Oral presentation at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically reasoning about future human behaviors is a difficult problem\nbut has significant practical applications to assistive systems. Part of this\ndifficulty stems from learning systems' inability to represent all kinds of\nbehaviors. Some behaviors, such as motion, are best described with continuous\nrepresentations, whereas others, such as picking up a cup, are best described\nwith discrete representations. Furthermore, human behavior is generally not\nfixed: people can change their habits and routines. This suggests these systems\nmust be able to learn and adapt continuously. In this work, we develop an\nefficient deep generative model to jointly forecast a person's future discrete\nactions and continuous motions. On a large-scale egocentric dataset,\nEPIC-KITCHENS, we observe our method generates high-quality and diverse samples\nwhile exhibiting better generalization than related generative models. Finally,\nwe propose a variant to continually learn our model from streaming data,\nobserve its practical effectiveness, and theoretically justify its learning\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:22:37 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 18:27:39 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Guan", "Jiaqi", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris M.", ""], ["Rhinehart", "Nicholas", ""]]}, {"id": "1904.06260", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "Similarities between policy gradient methods (PGM) in Reinforcement\n  learning (RL) and supervised learning (SL)", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is about sequential decision making and is\ntraditionally opposed to supervised learning (SL) and unsupervised learning\n(USL). In RL, given the current state, the agent makes a decision that may\ninfluence the next state as opposed to SL (and USL) where, the next state\nremains the same, regardless of the decisions taken, either in batch or online\nlearning. Although this difference is fundamental between SL and RL, there are\nconnections that have been overlooked. In particular, we prove in this paper\nthat gradient policy method can be cast as a supervised learning problem where\ntrue label are replaced with discounted rewards. We provide a new proof of\npolicy gradient methods (PGM) that emphasizes the tight link with the cross\nentropy and supervised learning. We provide a simple experiment where we\ninterchange label and pseudo rewards. We conclude that other relationships with\nSL could be made if we modify the reward functions wisely.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:49:28 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 07:39:36 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 17:44:44 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1904.06302", "submitter": "Mingde Zhao", "authors": "Mingde Zhao and Hongwei Ge and Kai Zhang and Yaqing Hou", "title": "A Reference Vector based Many-Objective Evolutionary Algorithm with\n  Feasibility-aware Adaptation", "comments": "Revision 1 submitted to Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infeasible parts of the objective space in difficult many-objective\noptimization problems cause trouble for evolutionary algorithms. This paper\nproposes a reference vector based algorithm which uses two interacting engines\nto adapt the reference vectors and to evolve the population towards the true\nPareto Front (PF) s.t. the reference vectors are always evenly distributed\nwithin the current PF to provide appropriate guidance for selection. The\ncurrent PF is tracked by maintaining an archive of undominated individuals, and\nadaptation of reference vectors is conducted with the help of another archive\nthat contains layers of reference vectors corresponding to different density.\nExperimental results show the expected characteristics and competitive\nperformance of the proposed algorithm TEEA.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:12:46 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhao", "Mingde", ""], ["Ge", "Hongwei", ""], ["Zhang", "Kai", ""], ["Hou", "Yaqing", ""]]}, {"id": "1904.06312", "submitter": "Kaleigh Clary", "authors": "Kaleigh Clary, Emma Tosch, John Foley, and David Jensen", "title": "Let's Play Again: Variability of Deep Reinforcement Learning Agents in\n  Atari Environments", "comments": "NeurIPS 2018 Critiquing and Correcting Trends Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility in reinforcement learning is challenging: uncontrolled\nstochasticity from many sources, such as the learning algorithm, the learned\npolicy, and the environment itself have led researchers to report the\nperformance of learned agents using aggregate metrics of performance over\nmultiple random seeds for a single environment. Unfortunately, there are still\npernicious sources of variability in reinforcement learning agents that make\nreporting common summary statistics an unsound metric for performance. Our\nexperiments demonstrate the variability of common agents used in the popular\nOpenAI Baselines repository. We make the case for reporting post-training agent\nperformance as a distribution, rather than a point estimate.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:37:52 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Clary", "Kaleigh", ""], ["Tosch", "Emma", ""], ["Foley", "John", ""], ["Jensen", "David", ""]]}, {"id": "1904.06317", "submitter": "Tom Silver", "authors": "Tom Silver, Kelsey R. Allen, Alex K. Lew, Leslie Pack Kaelbling, Josh\n  Tenenbaum", "title": "Few-Shot Bayesian Imitation Learning with Logical Program Policies", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn many novel tasks from a very small number (1--5) of\ndemonstrations, in stark contrast to the data requirements of nearly tabula\nrasa deep learning methods. We propose an expressive class of policies, a\nstrong but general prior, and a learning algorithm that, together, can learn\ninteresting policies from very few examples. We represent policies as logical\ncombinations of programs drawn from a domain-specific language (DSL), define a\nprior over policies with a probabilistic grammar, and derive an approximate\nBayesian inference algorithm to learn policies from demonstrations. In\nexperiments, we study five strategy games played on a 2D grid with one shared\nDSL. After a few demonstrations of each game, the inferred policies generalize\nto new game instances that differ substantially from the demonstrations. Our\npolicy learning is 20--1,000x more data efficient than convolutional and fully\nconvolutional policy learning and many orders of magnitude more computationally\nefficient than vanilla program induction. We argue that the proposed method is\nan apt choice for tasks that have scarce training data and feature significant,\nstructured variation between task instances.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:51:01 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 15:34:48 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Silver", "Tom", ""], ["Allen", "Kelsey R.", ""], ["Lew", "Alex K.", ""], ["Kaelbling", "Leslie Pack", ""], ["Tenenbaum", "Josh", ""]]}, {"id": "1904.06345", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Jean Kossaifi and Georgios Tzimiropoulos and Maja\n  Pantic", "title": "Incremental multi-domain learning with network latent tensor\n  factorization", "comments": "AAAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prominence of deep learning, large amount of annotated data and\nincreasingly powerful hardware made it possible to reach remarkable performance\nfor supervised classification tasks, in many cases saturating the training\nsets. However the resulting models are specialized to a single very specific\ntask and domain. Adapting the learned classification to new domains is a hard\nproblem due to at least three reasons: (1) the new domains and the tasks might\nbe drastically different; (2) there might be very limited amount of annotated\ndata on the new domain and (3) full training of a new model for each new task\nis prohibitive in terms of computation and memory, due to the sheer number of\nparameters of deep CNNs. In this paper, we present a method to learn\nnew-domains and tasks incrementally, building on prior knowledge from already\nlearned tasks and without catastrophic forgetting. We do so by jointly\nparametrizing weights across layers using low-rank Tucker structure. The core\nis task agnostic while a set of task specific factors are learnt on each new\ndomain. We show that leveraging tensor structure enables better performance\nthan simply using matrix operations. Joint tensor modelling also naturally\nleverages correlations across different layers. Compared with previous methods\nwhich have focused on adapting each layer separately, our approach results in\nmore compact representations for each new task/domain. We apply the proposed\nmethod to the 10 datasets of the Visual Decathlon Challenge and show that our\nmethod offers on average about 7.5x reduction in number of parameters and\ncompetitive performance in terms of both classification accuracy and Decathlon\nscore.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:57:05 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 14:04:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bulat", "Adrian", ""], ["Kossaifi", "Jean", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.06449", "submitter": "Ryan Rossi", "authors": "John Boaz Lee, Giang Nguyen, Ryan A. Rossi, Nesreen K. Ahmed, Eunyee\n  Koh, and Sungchul Kim", "title": "Dynamic Node Embeddings from Edge Streams", "comments": "IEEE Transactions on Emerging Topics in Computational Intelligence\n  (TETIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks evolve continuously over time with the addition, deletion, and\nchanging of links and nodes. Such temporal networks (or edge streams) consist\nof a sequence of timestamped edges and are seemingly ubiquitous. Despite the\nimportance of accurately modeling the temporal information, most embedding\nmethods ignore it entirely or approximate the temporal network using a sequence\nof static snapshot graphs. In this work, we propose using the notion of\ntemporal walks for learning dynamic embeddings from temporal networks. Temporal\nwalks capture the temporally valid interactions (e.g., flow of information,\nspread of disease) in the dynamic network in a lossless fashion. Based on the\nnotion of temporal walks, we describe a general class of embeddings called\ncontinuous-time dynamic network embeddings (CTDNEs) that completely avoid the\nissues and problems that arise when approximating the temporal network as a\nsequence of static snapshot graphs. Unlike previous work, CTDNEs learn dynamic\nnode embeddings directly from the temporal network at the finest temporal\ngranularity and thus use only temporally valid information. As such CTDNEs\nnaturally support online learning of the node embeddings in a streaming\nreal-time fashion. Finally, the experiments demonstrate the effectiveness of\nthis class of embedding methods that leverage temporal walks as it achieves an\naverage gain in AUC of 11.9% across all methods and graphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 23:44:25 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:44:02 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lee", "John Boaz", ""], ["Nguyen", "Giang", ""], ["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""]]}, {"id": "1904.06475", "submitter": "Bo Chen", "authors": "Bo Chen, Xiaotao Gu, Yufeng Hu, Siliang Tang, Guoping Hu, Yueting\n  Zhuang, Xiang Ren", "title": "Improving Distantly-supervised Entity Typing with Compact Latent Space\n  Clustering", "comments": "accepted by NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distant supervision has gained great success on Fine-grained Entity\nTyping (FET). Despite its efficiency in reducing manual labeling efforts, it\nalso brings the challenge of dealing with false entity type labels, as distant\nsupervision assigns labels in a context agnostic manner. Existing works\nalleviated this issue with partial-label loss, but usually suffer from\nconfirmation bias, which means the classifier fit a pseudo data distribution\ngiven by itself. In this work, we propose to regularize distantly supervised\nmodels with Compact Latent Space Clustering (CLSC) to bypass this problem and\neffectively utilize noisy data yet. Our proposed method first dynamically\nconstructs a similarity graph of different entity mentions; infer the labels of\nnoisy instances via label propagation. Based on the inferred labels, mention\nembeddings are updated accordingly to encourage entity mentions with close\nsemantics to form a compact cluster in the embedding space,thus leading to\nbetter classification performance. Extensive experiments on standard benchmarks\nshow that our CLSC model consistently outperforms state-of-the-art distantly\nsupervised entity typing systems by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 03:52:56 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Chen", "Bo", ""], ["Gu", "Xiaotao", ""], ["Hu", "Yufeng", ""], ["Tang", "Siliang", ""], ["Hu", "Guoping", ""], ["Zhuang", "Yueting", ""], ["Ren", "Xiang", ""]]}, {"id": "1904.06539", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen,\n  Ze Ma, Shiyi Wang, Hao-Shu Fang, Cewu Lu", "title": "HAKE: Human Activity Knowledge Engine", "comments": "Work in progress. Project website: http://hake-mvig.cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity understanding is crucial for building automatic intelligent\nsystem. With the help of deep learning, activity understanding has made huge\nprogress recently. But some challenges such as imbalanced data distribution,\naction ambiguity, complex visual patterns still remain. To address these and\npromote the activity understanding, we build a large-scale Human Activity\nKnowledge Engine (HAKE) based on the human body part states. Upon existing\nactivity datasets, we annotate the part states of all the active persons in all\nimages, thus establish the relationship between instance activity and body part\nstates. Furthermore, we propose a HAKE based part state recognition model with\na knowledge extractor named Activity2Vec and a corresponding part state based\nreasoning network. With HAKE, our method can alleviate the learning difficulty\nbrought by the long-tail data distribution, and bring in interpretability. Now\nour HAKE has more than 7 M+ part state annotations and is still under\nconstruction. We first validate our approach on a part of HAKE in this\npreliminary paper, where we show 7.2 mAP performance improvement on\nHuman-Object Interaction recognition, and 12.38 mAP improvement on the one-shot\nsubsets.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 12:56:17 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 17:18:11 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 16:00:12 GMT"}, {"version": "v4", "created": "Sat, 3 Aug 2019 07:47:43 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 12:47:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Li", "Yong-Lu", ""], ["Xu", "Liang", ""], ["Liu", "Xinpeng", ""], ["Huang", "Xijie", ""], ["Xu", "Yue", ""], ["Chen", "Mingyang", ""], ["Ma", "Ze", ""], ["Wang", "Shiyi", ""], ["Fang", "Hao-Shu", ""], ["Lu", "Cewu", ""]]}, {"id": "1904.06611", "submitter": "John Collomosse", "authors": "John Collomosse, Tu Bui, Hailin Jin", "title": "LiveSketch: Query Perturbations for Guided Sketch-based Visual Search", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiveSketch is a novel algorithm for searching large image collections using\nhand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch\nsearch by creating visual suggestions that augment the query as it is drawn,\nmaking query specification an iterative rather than one-shot process that helps\ndisambiguate users' search intent. Our technical contributions are: a triplet\nconvnet architecture that incorporates an RNN based variational autoencoder to\nsearch for images using vector (stroke-based) queries; real-time clustering to\nidentify likely search intents (and so, targets within the search embedding);\nand the use of backpropagation from those targets to perturb the input stroke\nsequence, so suggesting alterations to the query in order to guide the search.\nWe show improvements in accuracy and time-to-task over contemporary baselines\nusing a 67M image corpus.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 00:33:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Collomosse", "John", ""], ["Bui", "Tu", ""], ["Jin", "Hailin", ""]]}, {"id": "1904.06703", "submitter": "Benjamin Beyret", "authors": "Benjamin Beyret, Ali Shafti, A. Aldo Faisal", "title": "Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic\n  Manipulation", "comments": null, "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968488", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic systems are ever more capable of automation and fulfilment of complex\ntasks, particularly with reliance on recent advances in intelligent systems,\ndeep learning and artificial intelligence. However, as robots and humans come\ncloser in their interactions, the matter of interpretability, or explainability\nof robot decision-making processes for the human grows in importance. A\nsuccessful interaction and collaboration will only take place through mutual\nunderstanding of underlying representations of the environment and the task at\nhand. This is currently a challenge in deep learning systems. We present a\nhierarchical deep reinforcement learning system, consisting of a low-level\nagent handling the large actions/states space of a robotic system efficiently,\nby following the directives of a high-level agent which is learning the\nhigh-level dynamics of the environment and task. This high-level agent forms a\nrepresentation of the world and task at hand that is interpretable for a human\noperator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based\nmodel of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its\nperformance. Results show efficient learning of complex actions/states spaces\nby the low-level agent, and an interpretable representation of the task and\ndecision-making process learned by the high-level agent.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 14:54:14 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 06:37:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Beyret", "Benjamin", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1904.06725", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and George Karypis", "title": "Distributed representation of multi-sense words: A loss-driven approach", "comments": "PAKDD 2018 Best paper award runner-up", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2018.\n  Lecture Notes in Computer Science, vol 10938. Springer, Cham", "doi": "10.1007/978-3-319-93037-4_27", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2Vec's Skip Gram model is the current state-of-the-art approach for\nestimating the distributed representation of words. However, it assumes a\nsingle vector per word, which is not well-suited for representing words that\nhave multiple senses. This work presents LDMI, a new model for estimating\ndistributional representations of words. LDMI relies on the idea that, if a\nword carries multiple senses, then having a different representation for each\nof its senses should lead to a lower loss associated with predicting its\nco-occurring words, as opposed to the case when a single vector representation\nis used for all the senses. After identifying the multi-sense words, LDMI\nclusters the occurrences of these words to assign a sense to each occurrence.\nExperiments on the contextual word similarity task show that LDMI leads to\nbetter performance than competing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 17:01:26 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manchanda", "Saurav", ""], ["Karypis", "George", ""]]}, {"id": "1904.06730", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and George Karypis", "title": "Text segmentation on multilabel documents: A distant-supervised approach", "comments": "Accepted in 2018 IEEE International Conference on Data Mining (ICDM)", "journal-ref": "2018 IEEE International Conference on Data Mining (ICDM),\n  1170-1175", "doi": "10.1109/ICDM.2018.00154", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting text into semantically coherent segments is an important task with\napplications in information retrieval and text summarization. Developing\naccurate topical segmentation requires the availability of training data with\nground truth information at the segment level. However, generating such labeled\ndatasets, especially for applications in which the meaning of the labels is\nuser-defined, is expensive and time-consuming. In this paper, we develop an\napproach that instead of using segment-level ground truth information, it\ninstead uses the set of labels that are associated with a document and are\neasier to obtain as the training data essentially corresponds to a multilabel\ndataset. Our method, which can be thought of as an instance of distant\nsupervision, improves upon the previous approaches by exploiting the fact that\nconsecutive sentences in a document tend to talk about the same topic, and\nhence, probably belong to the same class. Experiments on the text segmentation\ntask on a variety of datasets show that the segmentation produced by our method\nbeats the competing approaches on four out of five datasets and performs at par\non the fifth dataset. On the multilabel text classification task, our method\nperforms at par with the competing approaches, while requiring significantly\nless time to estimate than the competing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 17:32:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manchanda", "Saurav", ""], ["Karypis", "George", ""]]}, {"id": "1904.06736", "submitter": "Dhruv Ramani", "authors": "Dhruv Ramani", "title": "A Short Survey On Memory Based Reinforcement Learning", "comments": "arXiv admin note: text overlap with arXiv:1803.10760,\n  arXiv:1803.01846, arXiv:1702.08360, arXiv:1805.12375, arXiv:1507.06527,\n  arXiv:1810.02274, arXiv:1711.06677 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is a branch of machine learning which is employed\nto solve various sequential decision making problems without proper\nsupervision. Due to the recent advancement of deep learning, the newly proposed\nDeep-RL algorithms have been able to perform extremely well in sophisticated\nhigh-dimensional environments. However, even after successes in many domains,\none of the major challenge in these approaches is the high magnitude of\ninteractions with the environment required for efficient decision making.\nSeeking inspiration from the brain, this problem can be solved by incorporating\ninstance based learning by biasing the decision making on the memories of high\nrewarding experiences. This paper reviews various recent reinforcement learning\nmethods which incorporate external memory to solve decision making and a survey\nof them is presented. We provide an overview of the different methods - along\nwith their advantages and disadvantages, applications and the standard\nexperimentation settings used for memory based models. This review hopes to be\na helpful resource to provide key insight of the recent advances in the field\nand provide help in further future development of it.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 18:18:45 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Ramani", "Dhruv", ""]]}, {"id": "1904.06786", "submitter": "Sarah Bechtle", "authors": "Sarah Bechtle, Yixin Lin, Akshara Rai, Ludovic Righetti, Franziska\n  Meier", "title": "Curious iLQR: Resolving Uncertainty in Model-based RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curiosity as a means to explore during reinforcement learning problems has\nrecently become very popular. However, very little progress has been made in\nutilizing curiosity for learning control. In this work, we propose a\nmodel-based reinforcement learning (MBRL) framework that combines Bayesian\nmodeling of the system dynamics with curious iLQR, an iterative LQR approach\nthat considers model uncertainty. During trajectory optimization the curious\niLQR attempts to minimize both the task-dependent cost and the uncertainty in\nthe dynamics model. We demonstrate the approach on reaching tasks with 7-DoF\nmanipulators in simulation and on a real robot. Our experiments show that MBRL\nwith curious iLQR reaches desired end-effector targets more reliably and with\nless system rollouts when learning a new task from scratch, and that the\nlearned model generalizes better to new reaching tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 00:02:27 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 00:34:22 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Bechtle", "Sarah", ""], ["Lin", "Yixin", ""], ["Rai", "Akshara", ""], ["Righetti", "Ludovic", ""], ["Meier", "Franziska", ""]]}, {"id": "1904.06807", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan", "title": "Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance\n  for Cross-View Image Translation", "comments": "20 pages, 16 figures, accepted to CVPR 2019 as an oral paper", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image translation is challenging because it involves images with\ndrastically different views and severe deformation. In this paper, we propose a\nnovel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that\nmakes it possible to generate images of natural scenes in arbitrary viewpoints,\nbased on an image of the scene and a novel semantic map. The proposed\nSelectionGAN explicitly utilizes the semantic information and consists of two\nstages. In the first stage, the condition image and the target semantic map are\nfed into a cycled semantic-guided generation network to produce initial coarse\nresults. In the second stage, we refine the initial results by using a\nmulti-channel attention selection mechanism. Moreover, uncertainty maps\nautomatically learned from attentions are used to guide the pixel loss for\nbetter network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top\ndatasets show that our model is able to generate significantly better results\nthan the state-of-the-art methods. The source code, data and trained models are\navailable at https://github.com/Ha0Tang/SelectionGAN.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:04:15 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:36:07 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Wang", "Yanzhi", ""], ["Corso", "Jason J.", ""], ["Yan", "Yan", ""]]}, {"id": "1904.06809", "submitter": "Ao Liu", "authors": "Ao Liu, Lirong Xia, Andrew Duchowski, Reynold Bailey, Kenneth\n  Holmqvist, Eakta Jain", "title": "Differential Privacy for Eye-Tracking Data", "comments": "10 pages including appendix", "journal-ref": null, "doi": "10.1145/3314111.3319823", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large eye-tracking datasets are created, data privacy is a pressing\nconcern for the eye-tracking community. De-identifying data does not guarantee\nprivacy because multiple datasets can be linked for inferences. A common belief\nis that aggregating individuals' data into composite representations such as\nheatmaps protects the individual. However, we analytically examine the privacy\nof (noise-free) heatmaps and show that they do not guarantee privacy. We\nfurther propose two noise mechanisms that guarantee privacy and analyze their\nprivacy-utility tradeoff. Analysis reveals that our Gaussian noise mechanism is\nan elegant solution to preserve privacy for heatmaps. Our results have\nimplications for interdisciplinary research to create differentially private\nmechanisms for eye tracking.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:23:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Ao", ""], ["Xia", "Lirong", ""], ["Duchowski", "Andrew", ""], ["Bailey", "Reynold", ""], ["Holmqvist", "Kenneth", ""], ["Jain", "Eakta", ""]]}, {"id": "1904.06813", "submitter": "Changhua Pei", "authors": "Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao\n  Sun, Jian Wu, Peng Jiang and Wenwu Ou", "title": "Personalized Re-ranking for Recommendation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a core task in recommender systems, which aims at providing an\nordered list of items to users. Typically, a ranking function is learned from\nthe labeled dataset to optimize the global performance, which produces a\nranking score for each individual item. However, it may be sub-optimal because\nthe scoring function applies to each item individually and does not explicitly\nconsider the mutual influence between items, as well as the differences of\nusers' preferences or intents. Therefore, we propose a personalized re-ranking\nmodel for recommender systems. The proposed re-ranking model can be easily\ndeployed as a follow-up modular after any ranking algorithm, by directly using\nthe existing ranking feature vectors. It directly optimizes the whole\nrecommendation list by employing a transformer structure to efficiently encode\nthe information of all items in the list. Specifically, the Transformer applies\na self-attention mechanism that directly models the global relationships\nbetween any pair of items in the whole list. We confirm that the performance\ncan be further improved by introducing pre-trained embedding to learn\npersonalized encoding functions for different users. Experimental results on\nboth offline benchmarks and real-world online e-commerce systems demonstrate\nthe significant improvements of the proposed re-ranking model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:47:40 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:02:13 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 03:00:56 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Pei", "Changhua", ""], ["Zhang", "Yi", ""], ["Zhang", "Yongfeng", ""], ["Sun", "Fei", ""], ["Lin", "Xiao", ""], ["Sun", "Hanxiao", ""], ["Wu", "Jian", ""], ["Jiang", "Peng", ""], ["Ou", "Wenwu", ""]]}, {"id": "1904.06836", "submitter": "Peihua Li", "authors": "Qilong Wang and Jiangtao Xie and Wangmeng Zuo and Lei Zhang and Peihua\n  Li", "title": "Deep CNNs Meet Global Covariance Pooling: Better Representation and\n  Generalization", "comments": "Accepted to IEEE TPAMI. Code is at http://peihuali.org/MPN-COV/", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2974833", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with global average pooling in existing deep convolutional neural\nnetworks (CNNs), global covariance pooling can capture richer statistics of\ndeep features, having potential for improving representation and generalization\nabilities of deep CNNs. However, integration of global covariance pooling into\ndeep CNNs brings two challenges: (1) robust covariance estimation given deep\nfeatures of high dimension and small sample size; (2) appropriate usage of\ngeometry of covariances. To address these challenges, we propose a global\nMatrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a\nrobust covariance estimator, very suitable for scenario of high dimension and\nsmall sample size. It can also be regarded as Power-Euclidean metric between\ncovariances, effectively exploiting their geometry. Furthermore, a global\nGaussian embedding network is proposed to incorporate first-order statistics\ninto MPN-COV. For fast training of MPN-COV networks, we implement an iterative\nmatrix square root normalization, avoiding GPU unfriendly eigen-decomposition\ninherent in MPN-COV. Additionally, progressive 1x1 convolutions and group\nconvolution are introduced to compress covariance representations. The proposed\nmethods are highly modular, readily plugged into existing deep CNNs. Extensive\nexperiments are conducted on large-scale object classification, scene\ncategorization, fine-grained visual recognition and texture classification,\nshowing our methods outperform the counterparts and obtain state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 04:30:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:49:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Qilong", ""], ["Xie", "Jiangtao", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Li", "Peihua", ""]]}, {"id": "1904.06866", "submitter": "Ori Plonsky", "authors": "Ori Plonsky, Reut Apel, Eyal Ert, Moshe Tennenholtz, David Bourgin,\n  Joshua C. Peterson, Daniel Reichman, Thomas L. Griffiths, Stuart J. Russell,\n  Evan C. Carter, James F. Cavanagh, Ido Erev", "title": "Predicting human decisions with behavioral theories and machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Behavioral decision theories aim to explain human behavior. Can they help\npredict it? An open tournament for prediction of human choices in fundamental\neconomic decision tasks is presented. The results suggest that integration of\ncertain behavioral theories as features in machine learning systems provides\nthe best predictions. Surprisingly, the most useful theories for prediction\nbuild on basic properties of human and animal learning and are very different\nfrom mainstream decision theories that focus on deviations from rational\nchoice. Moreover, we find that theoretical features should be based not only on\nqualitative behavioral insights (e.g. loss aversion), but also on quantitative\nbehavioral foresights generated by functional descriptive models (e.g. Prospect\nTheory). Our analysis prescribes a recipe for derivation of explainable, useful\npredictions of human decisions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 06:12:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Plonsky", "Ori", ""], ["Apel", "Reut", ""], ["Ert", "Eyal", ""], ["Tennenholtz", "Moshe", ""], ["Bourgin", "David", ""], ["Peterson", "Joshua C.", ""], ["Reichman", "Daniel", ""], ["Griffiths", "Thomas L.", ""], ["Russell", "Stuart J.", ""], ["Carter", "Evan C.", ""], ["Cavanagh", "James F.", ""], ["Erev", "Ido", ""]]}, {"id": "1904.06879", "submitter": "Francisco Cruz", "authors": "Francisco Cruz, Sven Magg, Yukie Nagai, Stefan Wermter", "title": "Improving interactive reinforcement learning: What makes a good teacher?", "comments": "21 pages, 12 figures", "journal-ref": "Connection Science, Vol. 30, Nr. 3, 2018", "doi": "10.1080/09540091.2018.1443318", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive reinforcement learning has become an important apprenticeship\napproach to speed up convergence in classic reinforcement learning problems. In\nthis regard, a variant of interactive reinforcement learning is policy shaping\nwhich uses a parent-like trainer to propose the next action to be performed and\nby doing so reduces the search space by advice. On some occasions, the trainer\nmay be another artificial agent which in turn was trained using reinforcement\nlearning methods to afterward becoming an advisor for other learner-agents. In\nthis work, we analyze internal representations and characteristics of\nartificial agents to determine which agent may outperform others to become a\nbetter trainer-agent. Using a polymath agent, as compared to a specialist\nagent, an advisor leads to a larger reward and faster convergence of the reward\nsignal and also to a more stable behavior in terms of the state visit frequency\nof the learner-agents. Moreover, we analyze system interaction parameters in\norder to determine how influential they are in the apprenticeship process,\nwhere the consistency of feedback is much more relevant when dealing with\ndifferent learner obedience parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 07:17:20 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Cruz", "Francisco", ""], ["Magg", "Sven", ""], ["Nagai", "Yukie", ""], ["Wermter", "Stefan", ""]]}, {"id": "1904.06903", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Muchen Li, Wenxiu Sun", "title": "Learning Deformable Kernels for Image and Video Denoising", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the classical denoising methods restore clear results by selecting\nand averaging pixels in the noisy input. Instead of relying on hand-crafted\nselecting and averaging strategies, we propose to explicitly learn this process\nwith deep neural networks. Specifically, we propose deformable 2D kernels for\nimage denoising where the sampling locations and kernel weights are both\nlearned. The proposed kernel naturally adapts to image structures and could\neffectively reduce the oversmoothing artifacts. Furthermore, we develop 3D\ndeformable kernels for video denoising to more efficiently sample pixels across\nthe spatial-temporal space. Our method is able to solve the misalignment issues\nof large motion from dynamic scenes. For better training our video denoising\nmodel, we introduce the trilinear sampler and a new regularization term. We\ndemonstrate that the proposed method performs favorably against the\nstate-of-the-art image and video denoising approaches on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 08:15:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Xiangyu", ""], ["Li", "Muchen", ""], ["Sun", "Wenxiu", ""]]}, {"id": "1904.06950", "submitter": "Mayukh Das", "authors": "Mayukh Das, Yang Yu, Devendra Singh Dhami, Gautam Kunapuli, Sriraam\n  Natarajan", "title": "Human-Guided Learning of Column Networks: Augmenting Deep Learning with\n  Advice", "comments": "Under Review at 'Machine Learning Journal' (MLJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep models have been successfully applied in several applications,\nespecially with low-level representations. However, sparse, noisy samples and\nstructured domains (with multiple objects and interactions) are some of the\nopen challenges in most deep models. Column Networks, a deep architecture, can\nsuccinctly capture such domain structure and interactions, but may still be\nprone to sub-optimal learning from sparse and noisy samples. Inspired by the\nsuccess of human-advice guided learning in AI, especially in data-scarce\ndomains, we propose Knowledge-augmented Column Networks that leverage human\nadvice/knowledge for better learning with noisy/sparse samples. Our experiments\ndemonstrate that our approach leads to either superior overall performance or\nfaster convergence (i.e., both effective and efficient).\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 10:23:10 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Das", "Mayukh", ""], ["Yu", "Yang", ""], ["Dhami", "Devendra Singh", ""], ["Kunapuli", "Gautam", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1904.07073", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Felix Zhou, Adam Bailey, Barbara Braden, James East, Xin\n  Lu and Jens Rittscher", "title": "A deep learning framework for quality assessment and restoration in\n  video endoscopy", "comments": "14 pages", "journal-ref": "Medical Image Analysis, 101900(2020)", "doi": "10.1016/j.media.2020.101900", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Endoscopy is a routine imaging technique used for both diagnosis and\nminimally invasive surgical treatment. Artifacts such as motion blur, bubbles,\nspecular reflections, floating objects and pixel saturation impede the visual\ninterpretation and the automated analysis of endoscopy videos. Given the\nwidespread use of endoscopy in different clinical applications, we contend that\nthe robust and reliable identification of such artifacts and the automated\nrestoration of corrupted video frames is a fundamental medical imaging problem.\nExisting state-of-the-art methods only deal with the detection and restoration\nof selected artifacts. However, typically endoscopy videos contain numerous\nartifacts which motivates to establish a comprehensive solution.\n  We propose a fully automatic framework that can: 1) detect and classify six\ndifferent primary artifacts, 2) provide a quality score for each frame and 3)\nrestore mildly corrupted frames. To detect different artifacts our framework\nexploits fast multi-scale, single stage convolutional neural network detector.\nWe introduce a quality metric to assess frame quality and predict image\nrestoration success. Generative adversarial networks with carefully chosen\nregularization are finally used to restore corrupted frames.\n  Our detector yields the highest mean average precision (mAP at 5% threshold)\nof 49.0 and the lowest computational time of 88 ms allowing for accurate\nreal-time processing. Our restoration models for blind deblurring, saturation\ncorrection and inpainting demonstrate significant improvements over previous\nmethods. On a set of 10 test videos we show that our approach preserves an\naverage of 68.7% which is 25% more frames than that retained from the raw\nvideos.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:26:38 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ali", "Sharib", ""], ["Zhou", "Felix", ""], ["Bailey", "Adam", ""], ["Braden", "Barbara", ""], ["East", "James", ""], ["Lu", "Xin", ""], ["Rittscher", "Jens", ""]]}, {"id": "1904.07091", "submitter": "Vicen\\c{c} G\\'omez Cerd\\`a", "authors": "Miquel Junyent, Anders Jonsson, Vicen\\c{c} G\\'omez", "title": "Deep Policies for Width-Based Planning in Pixel Domains", "comments": "In Proceedings of the 29th International Conference on Automated\n  Planning and Scheduling (ICAPS 2019). arXiv admin note: text overlap with\n  arXiv:1806.05898", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Width-based planning has demonstrated great success in recent years due to\nits ability to scale independently of the size of the state space. For example,\nBandres et al. (2018) introduced a rollout version of the Iterated Width\nalgorithm whose performance compares well with humans and learning methods in\nthe pixel setting of the Atari games suite. In this setting, planning is done\non-line using the \"screen\" states and selecting actions by looking ahead into\nthe future. However, this algorithm is purely exploratory and does not leverage\npast reward information. Furthermore, it requires the state to be factored into\nfeatures that need to be pre-defined for the particular task, e.g., the B-PROST\npixel features. In this work, we extend width-based planning by incorporating\nan explicit policy in the action selection mechanism. Our method, called\n$\\pi$-IW, interleaves width-based planning and policy learning using the\nstate-actions visited by the planner. The policy estimate takes the form of a\nneural network and is in turn used to guide the planning step, thus reinforcing\npromising paths. Surprisingly, we observe that the representation learned by\nthe neural network can be used as a feature space for the width-based planner\nwithout degrading its performance, thus removing the requirement of pre-defined\nfeatures for the planner. We compare $\\pi$-IW with previous width-based methods\nand with AlphaZero, a method that also interleaves planning and learning, in\nsimple environments, and show that $\\pi$-IW has superior performance. We also\nshow that $\\pi$-IW algorithm outperforms previous width-based methods in the\npixel setting of Atari games suite.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 10:50:12 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 09:32:49 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Junyent", "Miquel", ""], ["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1904.07099", "submitter": "Alasdair Newson", "authors": "Alasdair Newson, Andr\\'es Almansa, Yann Gousseau, Sa\\\"id Ladjal", "title": "Processsing Simple Geometric Attributes with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis is a core problem in modern deep learning, and many recent\narchitectures such as autoencoders and Generative Adversarial networks produce\nspectacular results on highly complex data, such as images of faces or\nlandscapes. While these results open up a wide range of new, advanced synthesis\napplications, there is also a severe lack of theoretical understanding of how\nthese networks work. This results in a wide range of practical problems, such\nas difficulties in training, the tendency to sample images with little or no\nvariability, and generalisation problems. In this paper, we propose to analyse\nthe ability of the simplest generative network, the autoencoder, to encode and\ndecode two simple geometric attributes : size and position. We believe that, in\norder to understand more complicated tasks, it is necessary to first understand\nhow these networks process simple attributes. For the first property, we\nanalyse the case of images of centred disks with variable radii. We explain how\nthe autoencoder projects these images to and from a latent space of smallest\npossible dimension, a scalar. In particular, we describe a closed-form solution\nto the decoding training problem in a network without biases, and show that\nduring training, the network indeed finds this solution. We then investigate\nthe best regularisation approaches which yield networks that generalise well.\nFor the second property, position, we look at the encoding and decoding of\nDirac delta functions, also known as `one-hot' vectors. We describe a\nhand-crafted filter that achieves encoding perfectly, and show that the network\nnaturally finds this filter during training. We also show experimentally that\nthe decoding can be achieved if the dataset is sampled in an appropriate\nmanner.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:01:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Newson", "Alasdair", ""], ["Almansa", "Andr\u00e9s", ""], ["Gousseau", "Yann", ""], ["Ladjal", "Sa\u00efd", ""]]}, {"id": "1904.07105", "submitter": "Juan Liu", "authors": "Juan Liu and Kevin M. Koch", "title": "MRI Tissue Magnetism Quantification through Total Field Inversion with\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quantitative susceptibility mapping (QSM) utilizes MRI signal phase to infer\nestimates of local tissue magnetism (magnetic susceptibility), which has been\nshown useful to provide novel image contrast and as biomarkers of abnormal\ntissue. QSM requires addressing a challenging post-processing problem:\nfiltering of image phase estimates and inversion of the phase to susceptibility\nrelationship. A wide variety of quantification errors, robustness limitations,\nand artifacts plague QSM algorithms. To overcome these limitations, a robust\ndeep-learning-based single-step QSM reconstruction approach is proposed and\ndemonstrated. This neural network was trained using magnetostatic physics\nsimulations based on in-vivo data sources. Random perturbations were added to\nthe physics simulations to provide sufficient input-label pairs for the\ntraining purposes. The network was quantitatively tested using gold-standard\nin-silico labeled datasets against established QSM total field inversion\napproaches. In addition, the algorithm was applied to susceptibility-weighted\nimaging (SWI) data collected on a cohort of clinical subjects with brain\nhemmhorage. When quantitatively compared against gold-standard in-silico\nlabels, the proposed algorithm outperformed the existing comparable approaches.\nHigh quality QSM were consistently estimated from clinical\nsusceptibility-weighted data on 100 subjects without any noticeable inversion\nfailures. The proposed approach was able to robustly generate high quality QSM\nwith improved accuracy in in-silico gold-standard experiments. QSM produced by\nthe proposed method can be generated in real-time on existing MRI scanner\nplatforms and provide enhanced visualization and quantification of\nmagnetism-based tissue contrasts.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 20:18:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Juan", ""], ["Koch", "Kevin M.", ""]]}, {"id": "1904.07180", "submitter": "Qinbing Fu", "authors": "Qinbing Fu, Cheng Hu, Pengcheng Liu, Shigang Yue", "title": "Synthetic Neural Vision System Design for Motion Pattern Recognition in\n  Dynamic Robot Scenes", "comments": "8 pages, IEEE format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insects have tiny brains but complicated visual systems for motion\nperception. A handful of insect visual neurons have been computationally\nmodeled and successfully applied for robotics. How different neurons\ncollaborate on motion perception, is an open question to date. In this paper,\nwe propose a novel embedded vision system in autonomous micro-robots, to\nrecognize motion patterns in dynamic robot scenes. Here, the basic motion\npatterns are categorized into movements of looming (proximity), recession,\ntranslation, and other irrelevant ones. The presented system is a synthetic\nneural network, which comprises two complementary sub-systems with four spiking\nneurons -- the lobula giant movement detectors (LGMD1 and LGMD2) in locusts for\nsensing looming and recession, and the direction selective neurons (DSN-R and\nDSN-L) in flies for translational motion extraction. Images are transformed to\nspikes via spatiotemporal computations towards a switch function and decision\nmaking mechanisms, in order to invoke proper robot behaviors amongst collision\navoidance, tracking and wandering, in dynamic robot scenes. Our robot\nexperiments demonstrated two main contributions: (1) This neural vision system\nis effective to recognize the basic motion patterns corresponding to timely and\nproper robot behaviors in dynamic scenes. (2) The arena tests with multi-robots\ndemonstrated the effectiveness in recognizing more abundant motion features for\ncollision detection, which is a great improvement compared with former studies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:53:36 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fu", "Qinbing", ""], ["Hu", "Cheng", ""], ["Liu", "Pengcheng", ""], ["Yue", "Shigang", ""]]}, {"id": "1904.07189", "submitter": "Maxime Bouton", "authors": "Maxime Bouton, Jesper Karlsson, Alireza Nakhaei, Kikuo Fujimura, Mykel\n  J. Kochenderfer, Jana Tumova", "title": "Reinforcement Learning with Probabilistic Guarantees for Autonomous\n  Driving", "comments": null, "journal-ref": "Workshop on Safety Risk and Uncertainty in Reinforcement Learning,\n  Conference on Uncertainty in Artificial Intelligence (UAI), 2018", "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing reliable decision strategies for autonomous urban driving is\nchallenging. Reinforcement learning (RL) has been used to automatically derive\nsuitable behavior in uncertain environments, but it does not provide any\nguarantee on the performance of the resulting policy. We propose a generic\napproach to enforce probabilistic guarantees on an RL agent. An exploration\nstrategy is derived prior to training that constrains the agent to choose among\nactions that satisfy a desired probabilistic specification expressed with\nlinear temporal logic (LTL). Reducing the search space to policies satisfying\nthe LTL formula helps training and simplifies reward design. This paper\noutlines a case study of an intersection scenario involving multiple traffic\nparticipants. The resulting policy outperforms a rule-based heuristic approach\nin terms of efficiency while exhibiting strong guarantees on safety.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:03:35 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 23:20:14 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bouton", "Maxime", ""], ["Karlsson", "Jesper", ""], ["Nakhaei", "Alireza", ""], ["Fujimura", "Kikuo", ""], ["Kochenderfer", "Mykel J.", ""], ["Tumova", "Jana", ""]]}, {"id": "1904.07204", "submitter": "Adarsh Subbaswamy", "authors": "Suchi Saria and Adarsh Subbaswamy", "title": "Tutorial: Safe and Reliable Machine Learning", "comments": "Overview of the \"Safe and Reliable Machine Learning\" tutorial given\n  at the 2019 ACM Conference on Fairness, Accountability, and Transparency\n  (FAT* 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document serves as a brief overview of the \"Safe and Reliable Machine\nLearning\" tutorial given at the 2019 ACM Conference on Fairness,\nAccountability, and Transparency (FAT* 2019). The talk slides can be found\nhere: https://bit.ly/2Gfsukp, while a video of the talk is available here:\nhttps://youtu.be/FGLOCkC4KmE, and a complete list of references for the\ntutorial here: https://bit.ly/2GdLPme.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:28:50 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Saria", "Suchi", ""], ["Subbaswamy", "Adarsh", ""]]}, {"id": "1904.07272", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins", "title": "Introduction to Multi-Armed Bandits", "comments": "Published with Foundations and Trends(R) in Machine Learning,\n  November 2019. The present version is a revision of the \"Foundations and\n  Trends\" publication. It contains numerous edits for presentation and accuracy\n  (based in part on readers' feedback), updated and expanded literature\n  reviews, and some new exercises", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n  The book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n  The chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:17:01 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 20:45:01 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 14:39:03 GMT"}, {"version": "v4", "created": "Sun, 15 Sep 2019 02:06:22 GMT"}, {"version": "v5", "created": "Mon, 30 Sep 2019 00:15:42 GMT"}, {"version": "v6", "created": "Sat, 26 Jun 2021 20:15:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Slivkins", "Aleksandrs", ""]]}, {"id": "1904.07302", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Fran\\c{c}ois\n  Petitjean, Lhassane Idoumghar, Pierre-Alain Muller", "title": "Automatic alignment of surgical videos using kinematic data", "comments": "Accepted at AIME 2019", "journal-ref": null, "doi": "10.1007/978-3-030-21642-9_14", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past one hundred years, the classic teaching methodology of \"see\none, do one, teach one\" has governed the surgical education systems worldwide.\nWith the advent of Operation Room 2.0, recording video, kinematic and many\nother types of data during the surgery became an easy task, thus allowing\nartificial intelligence systems to be deployed and used in surgical and medical\npractice. Recently, surgical videos has been shown to provide a structure for\npeer coaching enabling novice trainees to learn from experienced surgeons by\nreplaying those videos. However, the high inter-operator variability in\nsurgical gesture duration and execution renders learning from comparing novice\nto expert surgical videos a very difficult task. In this paper, we propose a\nnovel technique to align multiple videos based on the alignment of their\ncorresponding kinematic multivariate time series data. By leveraging the\nDynamic Time Warping measure, our algorithm synchronizes a set of videos in\norder to show the same gesture being performed at different speed. We believe\nthat the proposed approach is a valuable addition to the existing learning\ntools for surgery.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:46:08 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 12:27:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Petitjean", "Fran\u00e7ois", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1904.07346", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier", "title": "Efficient Supervision for Robot Learning via Imitation, Simulation, and\n  Adaptation", "comments": "Dissertation Summary", "journal-ref": null, "doi": "10.1007/s13218-019-00587-0", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in machine learning have led to a shift in the design of\nautonomous systems, improving performance on existing tasks and rendering new\napplications possible. Data-focused approaches gain relevance across diverse,\nintricate applications when developing data collection and curation pipelines\nbecomes more effective than manual behaviour design. The following work aims at\nincreasing the efficiency of this pipeline in two principal ways: by utilising\nmore powerful sources of informative data and by extracting additional\ninformation from existing data. In particular, we target three orthogonal\nfronts: imitation learning, domain adaptation, and transfer from simulation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 22:19:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wulfmeier", "Markus", ""]]}, {"id": "1904.07366", "submitter": "Jingkai Chen", "authors": "Jingkai Chen, Cheng Fang, David Wang, Andrew Wang, Brian Williams", "title": "Efficiently Exploring Ordering Problems through Conflict-directed Search", "comments": "Accepted at ICAPS2019. 9 pages, 4 figures, 2 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In planning and scheduling, solving problems with both state and temporal\nconstraints is hard since these constraints may be highly coupled. Judicious\norderings of events enable solvers to efficiently make decisions over sequences\nof actions to satisfy complex hybrid specifications. The ordering problem is\nthus fundamental to planning. Promising recent works have explored the ordering\nproblem as search, incorporating a special tree structure for efficiency.\nHowever, such approaches only reason over partial order specifications. Having\nobserved that an ordering is inconsistent with respect to underlying\nconstraints, prior works do not exploit the tree structure to efficiently\ngenerate orderings that resolve the inconsistency. In this paper, we present\nConflict-directed Incremental Total Ordering (CDITO), a conflict-directed\nsearch method to incrementally and systematically generate event total orders\ngiven ordering relations and conflicts returned by sub-solvers. Due to its\nability to reason over conflicts, CDITO is much more efficient than Incremental\nTotal Ordering. We demonstrate this by benchmarking on temporal network\nconfiguration problems that involve routing network flows and allocating\nbandwidth resources over time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 23:47:21 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Chen", "Jingkai", ""], ["Fang", "Cheng", ""], ["Wang", "David", ""], ["Wang", "Andrew", ""], ["Williams", "Brian", ""]]}, {"id": "1904.07374", "submitter": "Glenn Fink", "authors": "Glenn A. Fink and Penny McKenzie", "title": "Helping IT and OT Defenders Collaborate", "comments": "7 pages, 6 figures, 1 table, In proceedings of The Third IEEE\n  International Workshop on Security and Privacy for Internet of Things and\n  Cyber-Physical Systems (IoT/CPS-Security 2018), Seattle, WA, USA, October 22,\n  2018", "journal-ref": null, "doi": null, "report-no": "PNNL-SA-138585", "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems, especially in critical infrastructures, have become\nprimary hacking targets in international conflicts and diplomacy. However,\ncyber-physical systems present unique challenges to defenders, starting with an\ninability to communicate. This paper outlines the results of our interviews\nwith information technology (IT) defenders and operational technology (OT)\noperators and seeks to address lessons learned from them in the structure of\nour notional solutions. We present two problems in this paper: (1) the\ndifficulty of coordinating detection and response between defenders who work on\nthe cyber/IT and physical/OT sides of cyber-physical infrastructures, and (2)\nthe difficulty of estimating the safety state of a cyber-physical system while\nan intrusion is underway but before damage can be effected by the attacker. To\nmeet these challenges, we propose two solutions: (1) a visualization that will\nenable communication between IT defenders and OT operators, and (2) a\nmachine-learning approach that will estimate the distance from normal the\nphysical system is operating and send information to the visualization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:05:35 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Fink", "Glenn A.", ""], ["McKenzie", "Penny", ""]]}, {"id": "1904.07380", "submitter": "Alireza Shamsoshoara", "authors": "Alireza Shamsoshoara, Mehrdad Khaledi, Fatemeh Afghah, Abolfazl Razi,\n  Jonathan Ashdown, Kurt Turck", "title": "A Solution for Dynamic Spectrum Management in Mission-Critical UAV\n  Networks", "comments": "10 Pages, 5 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of spectrum scarcity in a network of\nunmanned aerial vehicles (UAVs) during mission-critical applications such as\ndisaster monitoring and public safety missions, where the pre-allocated\nspectrum is not sufficient to offer a high data transmission rate for real-time\nvideo-streaming. In such scenarios, the UAV network can lease part of the\nspectrum of a terrestrial licensed network in exchange for providing relaying\nservice. In order to optimize the performance of the UAV network and prolong\nits lifetime, some of the UAVs will function as a relay for the primary network\nwhile the rest of the UAVs carry out their sensing tasks. Here, we propose a\nteam reinforcement learning algorithm performed by the UAV's controller unit to\ndetermine the optimum allocation of sensing and relaying tasks among the UAVs\nas well as their relocation strategy at each time. We analyze the convergence\nof our algorithm and present simulation results to evaluate the system\nthroughput in different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:28:01 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Shamsoshoara", "Alireza", ""], ["Khaledi", "Mehrdad", ""], ["Afghah", "Fatemeh", ""], ["Razi", "Abolfazl", ""], ["Ashdown", "Jonathan", ""], ["Turck", "Kurt", ""]]}, {"id": "1904.07451", "submitter": "Yash Goyal", "authors": "Yash Goyal and Ziyan Wu and Jan Ernst and Dhruv Batra and Devi Parikh\n  and Stefan Lee", "title": "Counterfactual Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a technique to produce counterfactual visual\nexplanations. Given a 'query' image $I$ for which a vision system predicts\nclass $c$, a counterfactual visual explanation identifies how $I$ could change\nsuch that the system would output a different specified class $c'$. To do this,\nwe select a 'distractor' image $I'$ that the system predicts as class $c'$ and\nidentify spatial regions in $I$ and $I'$ such that replacing the identified\nregion in $I$ with the identified region in $I'$ would push the system towards\nclassifying $I$ as $c'$. We apply our approach to multiple image classification\ndatasets generating qualitative results showcasing the interpretability and\ndiscriminativeness of our counterfactual explanations. To explore the\neffectiveness of our explanations in teaching humans, we present machine\nteaching experiments for the task of fine-grained bird classification. We find\nthat users trained to distinguish bird species fare better when given access to\ncounterfactual explanations in addition to training examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:16:11 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 16:49:55 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Goyal", "Yash", ""], ["Wu", "Ziyan", ""], ["Ernst", "Jan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1904.07461", "submitter": "Siyu Chen", "authors": "Jingzhou Chen, Siyu Chen, Peilin Zhou, Yuntao Qian", "title": "Deep Neural Network Based Hyperspectral Pixel Classification With\n  Factorized Spectral-Spatial Feature Representation", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely used for hyperspectral pixel classification due\nto its ability of generating deep feature representation. However, how to\nconstruct an efficient and powerful network suitable for hyperspectral data is\nstill under exploration. In this paper, a novel neural network model is\ndesigned for taking full advantage of the spectral-spatial structure of\nhyperspectral data. Firstly, we extract pixel-based intrinsic features from\nrich yet redundant spectral bands by a subnetwork with supervised pre-training\nscheme. Secondly, in order to utilize the local spatial correlation among\npixels, we share the previous subnetwork as a spectral feature extractor for\neach pixel in a patch of image, after which the spectral features of all pixels\nin a patch are combined and feeded into the subsequent classification\nsubnetwork. Finally, the whole network is further fine-tuned to improve its\nclassification performance. Specially, the spectral-spatial factorization\nscheme is applied in our model architecture, making the network size and the\nnumber of parameters great less than the existing spectral-spatial deep\nnetworks for hyperspectral image classification. Experiments on the\nhyperspectral data sets show that, compared with some state-of-art deep\nlearning methods, our method achieves better classification results while\nhaving smaller network size and less parameters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:52:19 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Chen", "Jingzhou", ""], ["Chen", "Siyu", ""], ["Zhou", "Peilin", ""], ["Qian", "Yuntao", ""]]}, {"id": "1904.07469", "submitter": "Bin Wen", "authors": "Bin Wen, Jianhou Gan, Juan L.G. Guirao, and Wei Gao", "title": "An extended description logic system with knowledge element based on ALC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of knowledge management and knowledge economy, the knowledge\nelements that directly link and embody the knowledge system have become the\nresearch focus and hotspot in certain areas. The existing knowledge element\nrepresentation methods are limited in functions to deal with the formality,\nlogic and reasoning. Based on description logic ALC and the common knowledge\nelement model, in order to describe the knowledge element, the description\nlogic ALC is expanded. The concept is extended to two diferent ones (that is,\nthe object knowledge element concept and the attribute knowledge element\nconcept). The relationship is extended to three (that is, relationship between\nobject knowledge element concept and attribute knowledge element concept,\nrelationship among object knowledge element concepts, relationship among\nattribute knowledge element concepts), and the inverse relationship constructor\nis added to propose a description logic KEDL system. By demonstrating, the\nrelevant properties, such as completeness, reliability,of the described logic\nsystem KEDL are obtained. Finally, it is verified by the example that the\ndescription logic KEDL system has strong knowledge element description ability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:24:14 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wen", "Bin", ""], ["Gan", "Jianhou", ""], ["Guirao", "Juan L. G.", ""], ["Gao", "Wei", ""]]}, {"id": "1904.07482", "submitter": "Guangxiang Zhu", "authors": "Guangxiang Zhu, Jianhao Wang, Zhizhou Ren, Zichuan Lin, and Chongjie\n  Zhang", "title": "Object-Oriented Dynamics Learning through Multi-Level Abstraction", "comments": "Accepted to the Thirthy-Fourth AAAI Conference On Artificial\n  Intelligence (AAAI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-based approaches for learning action-conditioned dynamics has\ndemonstrated promise for generalization and interpretability. However, existing\napproaches suffer from structural limitations and optimization difficulties for\ncommon environments with multiple dynamic objects. In this paper, we present a\nnovel self-supervised learning framework, called Multi-level Abstraction\nObject-oriented Predictor (MAOP), which employs a three-level learning\narchitecture that enables efficient object-based dynamics learning from raw\nvisual observations. We also design a spatial-temporal relational reasoning\nmechanism for MAOP to support instance-level dynamics learning and handle\npartial observability. Our results show that MAOP significantly outperforms\nprevious methods in terms of sample efficiency and generalization over novel\nenvironments for learning environment models. We also demonstrate that learned\ndynamics models enable efficient planning in unseen environments, comparable to\ntrue environment models. In addition, MAOP learns semantically and visually\ninterpretable disentangled representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:01:17 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 03:46:55 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 10:29:10 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 06:05:28 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhu", "Guangxiang", ""], ["Wang", "Jianhao", ""], ["Ren", "Zhizhou", ""], ["Lin", "Zichuan", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1904.07491", "submitter": "Moyuru Kurita", "authors": "Moyuru Kurita, Kunihito Hoki", "title": "Method for Constructing Artificial Intelligence Player with Abstraction\n  to Markov Decision Processes in Multiplayer Game of Mahjong", "comments": "Copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for constructing artificial intelligence (AI) of mahjong,\nwhich is a multiplayer imperfect information game. Since the size of the game\ntree is huge, constructing an expert-level AI player of mahjong is challenging.\nWe define multiple Markov decision processes (MDPs) as abstractions of mahjong\nto construct effective search trees. We also introduce two methods of inferring\nstate values of the original mahjong using these MDPs. We evaluated the\neffectiveness of our method using gameplays vis-\\`{a}-vis the current strongest\nAI player.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 06:43:05 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kurita", "Moyuru", ""], ["Hoki", "Kunihito", ""]]}, {"id": "1904.07528", "submitter": "Yikang Li", "authors": "Yikang Li, Chris Twigg, Yuting Ye, Lingling Tao, Xiaogang Wang", "title": "Disentangling Pose from Appearance in Monochrome Hand Images", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from the monocular 2D image is challenging due to the\nvariation in lighting, appearance, and background. While some success has been\nachieved using deep neural networks, they typically require collecting a large\ndataset that adequately samples all the axes of variation of hand images. It\nwould, therefore, be useful to find a representation of hand pose which is\nindependent of the image appearance~(like hand texture, lighting, background),\nso that we can synthesize unseen images by mixing pose-appearance combinations.\nIn this paper, we present a novel technique that disentangles the\nrepresentation of pose from a complementary appearance factor in 2D monochrome\nimages. We supervise this disentanglement process using a network that learns\nto generate images of hand using specified pose+appearance features. Unlike\nprevious work, we do not require image pairs with a matching pose; instead, we\nuse the pose annotations already available and introduce a novel use of cycle\nconsistency to ensure orthogonality between the factors. Experimental results\nshow that our self-disentanglement scheme successfully decomposes the hand\nimage into the pose and its complementary appearance features of comparable\nquality as the method using paired data. Additionally, training the model with\nextra synthesized images with unseen hand-appearance combinations by re-mixing\npose and appearance factors from different images can improve the 2D pose\nestimation performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:15:26 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Li", "Yikang", ""], ["Twigg", "Chris", ""], ["Ye", "Yuting", ""], ["Tao", "Lingling", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1904.07559", "submitter": "Giovanni Casini", "authors": "Katarina Britz, Giovanni Casini, Thomas Meyer, Kody Moodley, Uli\n  Sattler, Ivan Varzinczak", "title": "Theoretical Foundations of Defeasible Description Logics", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend description logics (DLs) with non-monotonic reasoning features. We\nstart by investigating a notion of defeasible subsumption in the spirit of\ndefeasible conditionals as studied by Kraus, Lehmann and Magidor in the\npropositional case. In particular, we consider a natural and intuitive\nsemantics for defeasible subsumption, and investigate KLM-style syntactic\nproperties for both preferential and rational subsumption. Our contribution\nincludes two representation results linking our semantic constructions to the\nset of preferential and rational properties considered. Besides showing that\nour semantics is appropriate, these results pave the way for more effective\ndecision procedures for defeasible reasoning in DLs. Indeed, we also analyse\nthe problem of non-monotonic reasoning in DLs at the level of entailment and\npresent an algorithm for the computation of rational closure of a defeasible\nontology. Importantly, our algorithm relies completely on classical entailment\nand shows that the computational complexity of reasoning over defeasible\nontologies is no worse than that of reasoning in the underlying classical DL\nALC.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 09:44:56 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Britz", "Katarina", ""], ["Casini", "Giovanni", ""], ["Meyer", "Thomas", ""], ["Moodley", "Kody", ""], ["Sattler", "Uli", ""], ["Varzinczak", "Ivan", ""]]}, {"id": "1904.07601", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Shiming Xiang and Chunhong Pan", "title": "Relation-Shape Convolutional Neural Network for Point Cloud Analysis", "comments": "Accepted to CVPR 2019 as an oral presentation. Project page at\n  https://yochengliu.github.io/Relation-Shape-CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is very challenging, as the shape implied in irregular\npoints is difficult to capture. In this paper, we propose RS-CNN, namely,\nRelation-Shape Convolutional Neural Network, which extends regular grid CNN to\nirregular configuration for point cloud analysis. The key to RS-CNN is learning\nfrom relation, i.e., the geometric topology constraint among points.\nSpecifically, the convolutional weight for local point set is forced to learn a\nhigh-level relation expression from predefined geometric priors, between a\nsampled point from this point set and the others. In this way, an inductive\nlocal representation with explicit reasoning about the spatial layout of points\ncan be obtained, which leads to much shape awareness and robustness. With this\nconvolution as a basic operator, RS-CNN, a hierarchical architecture can be\ndeveloped to achieve contextual shape-aware learning for point cloud analysis.\nExtensive experiments on challenging benchmarks across three tasks verify\nRS-CNN achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 11:28:51 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 06:56:46 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 03:55:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1904.07656", "submitter": "Syed Qureshi", "authors": "Syed Arbaaz Qureshi, Mohammed Hasanuzzaman, Sriparna Saha, Ga\\\"el Dias", "title": "The Verbal and Non Verbal Signals of Depression -- Combining Acoustics,\n  Text and Visuals for Estimating Depression Level", "comments": "10 pages including references, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is a serious medical condition that is suffered by a large number\nof people around the world. It significantly affects the way one feels, causing\na persistent lowering of mood. In this paper, we propose a novel\nattention-based deep neural network which facilitates the fusion of various\nmodalities. We use this network to regress the depression level. Acoustic, text\nand visual modalities have been used to train our proposed network. Various\nexperiments have been carried out on the benchmark dataset, namely, Distress\nAnalysis Interview Corpus - a Wizard of Oz (DAIC-WOZ). From the results, we\nempirically justify that the fusion of all three modalities helps in giving the\nmost accurate estimation of depression level. Our proposed approach outperforms\nthe state-of-the-art by 7.17% on root mean squared error (RMSE) and 8.08% on\nmean absolute error (MAE).\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:18:00 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Qureshi", "Syed Arbaaz", ""], ["Hasanuzzaman", "Mohammed", ""], ["Saha", "Sriparna", ""], ["Dias", "Ga\u00ebl", ""]]}, {"id": "1904.07705", "submitter": "Rafael Vasquez", "authors": "Rafael Vasquez, Bilal Farooq", "title": "Multi-Objective Autonomous Braking System using Naturalistic Dataset", "comments": "Accepted in the proceedings of IEEE ITSC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep reinforcement learning based multi-objective autonomous braking system\nis presented. The design of the system is formulated in a continuous action\nspace and seeks to maximize both pedestrian safety and perception as well as\npassenger comfort. The vehicle agent is trained against a large naturalistic\ndataset containing pedestrian road-crossing trials in which respondents walked\nacross a road under various traffic conditions within an interactive virtual\nreality environment. The policy for brake control is learned through computer\nsimulation using two reinforcement learning methods i.e. Proximal Policy\nOptimization and Deep Deterministic Policy Gradient and the efficiency of each\nare compared. Results show that the system is able to reduce the negative\ninfluence on passenger comfort by half while maintaining safe braking\noperation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:07:45 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 15:27:28 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Vasquez", "Rafael", ""], ["Farooq", "Bilal", ""]]}, {"id": "1904.07714", "submitter": "Yung-Hsiang Lu", "authors": "Sergei Alyamkin, Matthew Ardi, Alexander C. Berg, Achille Brighton, Bo\n  Chen, Yiran Chen, Hsin-Pai Cheng, Zichen Fan, Chen Feng, Bo Fu, Kent Gauen,\n  Abhinav Goel, Alexander Goncharenko, Xuyang Guo, Soonhoi Ha, Andrew Howard,\n  Xiao Hu, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Jong Gook Ko, Alexander\n  Kondratyev, Junhyeok Lee, Seungjae Lee, Suwoong Lee, Zichao Li, Zhiyu Liang,\n  Juzheng Liu, Xin Liu, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Hong Hanh\n  Nguyen, Eunbyung Park, Denis Repin, Liang Shen, Tao Sheng, Fei Sun, David\n  Svitov, George K. Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang,\n  Shaojie Zhuo", "title": "Low-Power Computer Vision: Status, Challenges, Opportunities", "comments": "Preprint, Accepted by IEEE Journal on Emerging and Selected Topics in\n  Circuits and Systems. arXiv admin note: substantial text overlap with\n  arXiv:1810.01732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has achieved impressive progress in recent years. Meanwhile,\nmobile phones have become the primary computing platforms for millions of\npeople. In addition to mobile phones, many autonomous systems rely on visual\ndata for making decisions and some of these systems have limited energy (such\nas unmanned aerial vehicles also called drones and mobile robots). These\nsystems rely on batteries and energy efficiency is critical. This article\nserves two main purposes: (1) Examine the state-of-the-art for low-power\nsolutions to detect objects in images. Since 2015, the IEEE Annual\nInternational Low-Power Image Recognition Challenge (LPIRC) has been held to\nidentify the most energy-efficient computer vision solutions. This article\nsummarizes 2018 winners' solutions. (2) Suggest directions for research as well\nas opportunities for low-power computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 17:48:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alyamkin", "Sergei", ""], ["Ardi", "Matthew", ""], ["Berg", "Alexander C.", ""], ["Brighton", "Achille", ""], ["Chen", "Bo", ""], ["Chen", "Yiran", ""], ["Cheng", "Hsin-Pai", ""], ["Fan", "Zichen", ""], ["Feng", "Chen", ""], ["Fu", "Bo", ""], ["Gauen", "Kent", ""], ["Goel", "Abhinav", ""], ["Goncharenko", "Alexander", ""], ["Guo", "Xuyang", ""], ["Ha", "Soonhoi", ""], ["Howard", "Andrew", ""], ["Hu", "Xiao", ""], ["Huang", "Yuanjun", ""], ["Kang", "Donghyun", ""], ["Kim", "Jaeyoun", ""], ["Ko", "Jong Gook", ""], ["Kondratyev", "Alexander", ""], ["Lee", "Junhyeok", ""], ["Lee", "Seungjae", ""], ["Lee", "Suwoong", ""], ["Li", "Zichao", ""], ["Liang", "Zhiyu", ""], ["Liu", "Juzheng", ""], ["Liu", "Xin", ""], ["Lu", "Yang", ""], ["Lu", "Yung-Hsiang", ""], ["Malik", "Deeptanshu", ""], ["Nguyen", "Hong Hanh", ""], ["Park", "Eunbyung", ""], ["Repin", "Denis", ""], ["Shen", "Liang", ""], ["Sheng", "Tao", ""], ["Sun", "Fei", ""], ["Svitov", "David", ""], ["Thiruvathukal", "George K.", ""], ["Zhang", "Baiwu", ""], ["Zhang", "Jingchi", ""], ["Zhang", "Xiaopeng", ""], ["Zhuo", "Shaojie", ""]]}, {"id": "1904.07734", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Andreas S. Tolias", "title": "Three scenarios for continual learning", "comments": "Extended version of work presented at the NeurIPS Continual Learning\n  workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard artificial neural networks suffer from the well-known issue of\ncatastrophic forgetting, making continual or lifelong learning difficult for\nmachine learning. In recent years, numerous methods have been proposed for\ncontinual learning, but due to differences in evaluation protocols it is\ndifficult to directly compare their performance. To enable more structured\ncomparisons, we describe three continual learning scenarios based on whether at\ntest time task identity is provided and--in case it is not--whether it must be\ninferred. Any sequence of well-defined tasks can be performed according to each\nscenario. Using the split and permuted MNIST task protocols, for each scenario\nwe carry out an extensive comparison of recently proposed continual learning\nmethods. We demonstrate substantial differences between the three scenarios in\nterms of difficulty and in terms of how efficient different methods are. In\nparticular, when task identity must be inferred (i.e., class incremental\nlearning), we find that regularization-based approaches (e.g., elastic weight\nconsolidation) fail and that replaying representations of previous experiences\nseems required for solving this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:22:36 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1904.07786", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A Pattern-Hierarchy Classifier for Reduced Teaching", "comments": null, "journal-ref": "WSEAS Transactions on Computers, ISSN / E-ISSN: 1109-2750 /\n  2224-2872, Volume 19, 2020, Art. #23, pp. 183-193", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a design that can be used for Explainable AI. The lower\nlevel is a nested ensemble of patterns created by self-organisation. The upper\nlevel is a hierarchical tree, where nodes are linked through individual\nconcepts, so there is a transition from mixed ensemble masses to specific\ncategories. Lower-level pattern ensembles are learned in an unsupervsised\nmanner and then split into branches when it is clear that the category has\nchanged. Links between the two levels define that the concepts are learned and\nmissing links define that they are guessed only. This paper proposes some new\nclustering algorithms for producing the pattern ensembles, that are themselves\nan ensemble which converges through aggregations. Multiple solutions are also\ncombined, to make the final result more robust. One measure of success is how\ncoherent these ensembles are, which means that every data row in the cluster\nbelongs to the same category. The total number of clusters is also important\nand the teaching phase can correct the ensemble estimates with respect to both\nof these. A teaching phase would then help the classifier to learn the true\ncategory for each input row. During this phase, any classifier can learn or\ninfer correct classifications from some other classifier's knowledge, thereby\nreducing the required number of presentations. As the information is added,\ncross-referencing between the two structures allows it to be used more widely,\nwhere a unique structure can build up that would not be possible by either\nmethod separately.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:08:24 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 14:56:32 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 09:41:56 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1904.07802", "submitter": "Quentin Debard", "authors": "Quentin Debard, Jilles Steeve Dibangoye, St\\'ephane Canu, Christian\n  Wolf", "title": "Learning 3D Navigation Protocols on Touch Interfaces with Cooperative\n  Multi-Agent Reinforcement Learning", "comments": "17 pages, 8 figures. Accepted at The European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases 2019\n  (ECMLPKDD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using touch devices to navigate in virtual 3D environments such as computer\nassisted design (CAD) models or geographical information systems (GIS) is\ninherently difficult for humans, as the 3D operations have to be performed by\nthe user on a 2D touch surface. This ill-posed problem is classically solved\nwith a fixed and handcrafted interaction protocol, which must be learned by the\nuser. We propose to automatically learn a new interaction protocol allowing to\nmap a 2D user input to 3D actions in virtual environments using reinforcement\nlearning (RL). A fundamental problem of RL methods is the vast amount of\ninteractions often required, which are difficult to come by when humans are\ninvolved. To overcome this limitation, we make use of two collaborative agents.\nThe first agent models the human by learning to perform the 2D finger\ntrajectories. The second agent acts as the interaction protocol, interpreting\nand translating to 3D operations the 2D finger trajectories from the first\nagent. We restrict the learned 2D trajectories to be similar to a training set\nof collected human gestures by first performing state representation learning,\nprior to reinforcement learning. This state representation learning is\naddressed by projecting the gestures into a latent space learned by a\nvariational auto encoder (VAE).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:33:04 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 20:44:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Debard", "Quentin", ""], ["Dibangoye", "Jilles Steeve", ""], ["Canu", "St\u00e9phane", ""], ["Wolf", "Christian", ""]]}, {"id": "1904.07817", "submitter": "Manuel Gra\\~na", "authors": "Borja Fernandez-Gauna, Manuel Gra\\~na and Roland S. Zimmermann", "title": "Simion Zoo: A Workbench for Distributed Experimentation with\n  Reinforcement Learning for Continuous Control Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Simion Zoo, a Reinforcement Learning (RL) workbench that provides\na complete set of tools to design, run, and analyze the results,both\nstatistically and visually, of RL control applications. The main features that\nset apart Simion Zoo from similar software packages are its easy-to-use GUI,\nits support for distributed execution including deployment over graphics\nprocessing units (GPUs) , and the possibility to explore concurrently the RL\nmetaparameter space, which is key to successful RL experimentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:48:23 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Fernandez-Gauna", "Borja", ""], ["Gra\u00f1a", "Manuel", ""], ["Zimmermann", "Roland S.", ""]]}, {"id": "1904.07852", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Jean Kossaifi and Georgios Tzimiropoulos and Maja\n  Pantic", "title": "Matrix and tensor decompositions for training binary neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on improving the training of binary neural networks in which\nboth activations and weights are binary. While prior methods for neural network\nbinarization binarize each filter independently, we propose to instead\nparametrize the weight tensor of each layer using matrix or tensor\ndecomposition. The binarization process is then performed using this latent\nparametrization, via a quantization function (e.g. sign function) applied to\nthe reconstructed weights. A key feature of our method is that while the\nreconstruction is binarized, the computation in the latent factorized space is\ndone in the real domain. This has several advantages: (i) the latent\nfactorization enforces a coupling of the filters before binarization, which\nsignificantly improves the accuracy of the trained models. (ii) while at\ntraining time, the binary weights of each convolutional layer are parametrized\nusing real-valued matrix or tensor decomposition, during inference we simply\nuse the reconstructed (binary) weights. As a result, our method does not\nsacrifice any advantage of binary networks in terms of model compression and\nspeeding-up inference. As a further contribution, instead of computing the\nbinary weight scaling factors analytically, as in prior work, we propose to\nlearn them discriminatively via back-propagation. Finally, we show that our\napproach significantly outperforms existing methods when tested on the\nchallenging tasks of (a) human pose estimation (more than 4% improvements) and\n(b) ImageNet classification (up to 5% performance gains).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:57:27 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bulat", "Adrian", ""], ["Kossaifi", "Jean", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1904.07934", "submitter": "David Acuna", "authors": "David Acuna, Amlan Kar, Sanja Fidler", "title": "Devil is in the Edges: Learning Semantic Boundaries from Noisy\n  Annotations", "comments": "Accepted as a CVPR 2019 oral paper (Project Page:\n  https://nv-tlabs.github.io/STEAL/)", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of semantic boundary prediction, which aims to identify\npixels that belong to object(class) boundaries. We notice that relevant\ndatasets consist of a significant level of label noise, reflecting the fact\nthat precise annotations are laborious to get and thus annotators trade-off\nquality with efficiency. We aim to learn sharp and precise semantic boundaries\nby explicitly reasoning about annotation noise during training. We propose a\nsimple new layer and loss that can be used with existing learning-based\nboundary detectors. Our layer/loss enforces the detector to predict a maximum\nresponse along the normal direction at an edge, while also regularizing its\ndirection. We further reason about true object boundaries during training using\na level set formulation, which allows the network to learn from misaligned\nlabels in an end-to-end fashion. Experiments show that we improve over the\nCASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in\nterms of AP, outperforming all current state-of-the-art methods including those\nthat deal with alignment. Furthermore, we show that our learned network can be\nused to significantly improve coarse segmentation labels, lending itself as an\nefficient way to label new data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 19:16:57 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 17:30:52 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Acuna", "David", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1904.07961", "submitter": "Liang Wang", "authors": "Liang Wang, Peiqiu Huang, Kezhi Wang, Guopeng Zhang, Lei Zhang, Nauman\n  Aslam, and Kun Yang", "title": "RL-Based User Association and Resource Allocation for Multi-UAV enabled\n  MEC", "comments": "This paper was accepted by IWCMC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, multi-unmanned aerial vehicle (UAV) enabled mobile edge\ncomputing (MEC), i.e., UAVE is studied, where several UAVs are deployed as\nflying MEC platform to provide computing resource to ground user equipments\n(UEs). Compared to the traditional fixed location MEC, UAV enabled MEC (i.e.,\nUAVE) is particular useful in case of temporary events, emergency situations\nand on-demand services, due to its high flexibility, low cost and easy\ndeployment features. However, operation of UAVE faces several challenges, two\nof which are how to achieve both 1) the association between multiple UEs and\nUAVs and 2) the resource allocation from UAVs to UEs, while minimizing the\nenergy consumption for all the UEs. To address this, we formulate the above\nproblem into a mixed integer nonlinear programming (MINLP), which is difficult\nto be solved in general, especially in the large-scale scenario. We then\npropose a Reinforcement Learning (RL)-based user Association and resource\nAllocation (RLAA) algorithm to tackle this problem efficiently and effectively.\nNumerical results show that the proposed RLAA can achieve the optimal\nperformance with comparison to the exhaustive search in small scale, and have\nconsiderable performance gain over other typical algorithms in large-scale\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:15:39 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wang", "Liang", ""], ["Huang", "Peiqiu", ""], ["Wang", "Kezhi", ""], ["Zhang", "Guopeng", ""], ["Zhang", "Lei", ""], ["Aslam", "Nauman", ""], ["Yang", "Kun", ""]]}, {"id": "1904.08010", "submitter": "Mathieu Roche", "authors": "Mathieu Roche", "title": "How to define co-occurrence in different domains of study?", "comments": "CICLING'2018 (International Conference on Computational Linguistics\n  and Intelligent Text Processing) - March 18 to 24, 2018 - Hanoi, Vietnam (not\n  published in CICLING proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper presents a comparative study of co-occurrences. Some\nsimilarities and differences in the definition exist depending on the research\ndomain (e.g. linguistics, NLP, computer science). This paper discusses these\npoints, and deals with the methodological aspects in order to identify\nco-occurrences in a multidisciplinary paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:16:56 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Roche", "Mathieu", ""]]}, {"id": "1904.08056", "submitter": "Lei Liu", "authors": "Lei Liu, Jie Jiang, Wenjing Jia, Saeed Amirgholipour, Michelle\n  Zeibots, Xiangjian He", "title": "DENet: A Universal Network for Counting Crowd with Varying Densities and\n  Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting people or objects with significantly varying scales and densities\nhas attracted much interest from the research community and yet it remains an\nopen problem. In this paper, we propose a simple but an efficient and effective\nnetwork, named DENet, which is composed of two components, i.e., a detection\nnetwork (DNet) and an encoder-decoder estimation network (ENet). We first run\nDNet on an input image to detect and count individuals who can be segmented\nclearly. Then, ENet is utilized to estimate the density maps of the remaining\nareas, where the numbers of individuals cannot be detected. We propose a\nmodified Xception as an encoder for feature extraction and a combination of\ndilated convolution and transposed convolution as a decoder. In the\nShanghaiTech Part A, UCF and WorldExpo'10 datasets, our DENet achieves lower\nMean Absolute Error (MAE) than those of the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 02:34:04 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Liu", "Lei", ""], ["Jiang", "Jie", ""], ["Jia", "Wenjing", ""], ["Amirgholipour", "Saeed", ""], ["Zeibots", "Michelle", ""], ["He", "Xiangjian", ""]]}, {"id": "1904.08067", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\n  Mendu, Laura E. Barnes, Donald E. Brown", "title": "Text Classification Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": "10.3390/info10040150", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:29:05 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 01:20:53 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 18:28:33 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 22:51:18 GMT"}, {"version": "v5", "created": "Wed, 20 May 2020 16:27:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Kowsari", "Kamran", ""], ["Meimandi", "Kiana Jafari", ""], ["Heidarysafa", "Mojtaba", ""], ["Mendu", "Sanjana", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.08109", "submitter": "Lijing Song", "authors": "Liu Yang, Lijing Song", "title": "Contextual Aware Joint Probability Model Towards Question Answering\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERT's capability of\ncontext-aware word embeddings and BiDAF's context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual word representation at word and character\nlevels, for better comprehension of both question and context and their\ncorrelations. We also propose our original joint posterior probability\npredictor module and its associated loss functions. Our best model so far\nobtains F1 score of 75.842% and EM score of 72.24% on the test PCE leaderboad.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 07:16:10 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Yang", "Liu", ""], ["Song", "Lijing", ""]]}, {"id": "1904.08117", "submitter": "Hua Wei", "authors": "Hua Wei, Guanjie Zheng, Vikash Gayah, Zhenhui Li", "title": "A Survey on Traffic Signal Control Methods", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic signal control is an important and challenging real-world problem,\nwhich aims to minimize the travel time of vehicles by coordinating their\nmovements at the road intersections. Current traffic signal control systems in\nuse still rely heavily on oversimplified information and rule-based methods,\nalthough we now have richer data, more computing power and advanced methods to\ndrive the development of intelligent transportation. With the growing interest\nin intelligent transportation using machine learning methods like reinforcement\nlearning, this survey covers the widely acknowledged transportation approaches\nand a comprehensive list of recent literature on reinforcement for traffic\nsignal control. We hope this survey can foster interdisciplinary research on\nthis important topic.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:07:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 22:23:49 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 16:29:35 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Wei", "Hua", ""], ["Zheng", "Guanjie", ""], ["Gayah", "Vikash", ""], ["Li", "Zhenhui", ""]]}, {"id": "1904.08123", "submitter": "Avi Rosenfeld", "authors": "Avi Rosenfeld, Ariella Richardson", "title": "Explainability in Human-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a taxonomy of explainability in Human-Agent Systems. We\nconsider fundamental questions about the Why, Who, What, When and How of\nexplainability. First, we define explainability, and its relationship to the\nrelated terms of interpretability, transparency, explicitness, and\nfaithfulness. These definitions allow us to answer why explainability is needed\nin the system, whom it is geared to and what explanations can be generated to\nmeet this need. We then consider when the user should be presented with this\ninformation. Last, we consider how objective and subjective measures can be\nused to evaluate the entire system. This last question is the most encompassing\nas it will need to evaluate all other issues regarding explainability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:18:12 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rosenfeld", "Avi", ""], ["Richardson", "Ariella", ""]]}, {"id": "1904.08141", "submitter": "Shuangjie Xu", "authors": "Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu and Pan Zhou", "title": "MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation", "comments": "accepted to CVPR 2019 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semi-supervised video object segmentation (VOS),\nwhere the masks of objects of interests are given in the first frame of an\ninput video. To deal with challenging cases where objects are occluded or\nmissing, previous work relies on greedy data association strategies that make\ndecisions for each frame individually. In this paper, we propose a novel\napproach to defer the decision making for a target object in each frame, until\na global view can be established with the entire video being taken into\nconsideration. Our approach is in the same spirit as Multiple Hypotheses\nTracking (MHT) methods, making several critical adaptations for the VOS\nproblem. We employ the bounding box (bbox) hypothesis for tracking tree\nformation, and the multiple hypotheses are spawned by propagating the preceding\nbbox into the detected bbox proposals within a gated region starting from the\ninitial object mask in the first frame. The gated region is determined by a\ngating scheme which takes into account a more comprehensive motion model rather\nthan the simple Kalman filtering model in traditional MHT. To further design\nmore customized algorithms tailored for VOS, we develop a novel mask\npropagation score instead of the appearance similarity score that could be\nbrittle due to large deformations. The mask propagation score, together with\nthe motion score, determines the affinity between the hypotheses during tree\npruning. Finally, a novel mask merging strategy is employed to handle mask\nconflicts between objects. Extensive experiments on challenging datasets\ndemonstrate the effectiveness of the proposed method, especially in the case of\nobject missing.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 08:52:03 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Xu", "Shuangjie", ""], ["Liu", "Daizong", ""], ["Bao", "Linchao", ""], ["Liu", "Wei", ""], ["Zhou", "Pan", ""]]}, {"id": "1904.08149", "submitter": "Ozan \\c{C}atal", "authors": "Ozan \\c{C}atal, Johannes Nauta, Tim Verbelen, Pieter Simoens and Bart\n  Dhoedt", "title": "Bayesian policy selection using active inference", "comments": "ICLR 2019 Workshop on Structure & priors in reinforcement learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to take actions based on observations is a core requirement for\nartificial agents to be able to be successful and robust at their task.\nReinforcement Learning (RL) is a well-known technique for learning such\npolicies. However, current RL algorithms often have to deal with reward\nshaping, have difficulties generalizing to other environments and are most\noften sample inefficient. In this paper, we explore active inference and the\nfree energy principle, a normative theory from neuroscience that explains how\nself-organizing biological systems operate by maintaining a model of the world\nand casting action selection as an inference problem. We apply this concept to\na typical problem known to the RL community, the mountain car problem, and show\nhow active inference encompasses both RL and learning from demonstrations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:18:07 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 14:28:32 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["\u00c7atal", "Ozan", ""], ["Nauta", "Johannes", ""], ["Verbelen", "Tim", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1904.08159", "submitter": "Daniel Koguciuk M.Sc.Eng.", "authors": "Daniel Koguciuk and {\\L}ukasz Chechli\\'nski and Tarek El-Gaaly", "title": "3D Object Recognition with Ensemble Learning --- A Study of Point\n  Cloud-Based Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present an analysis of model-based ensemble learning for 3D\npoint-cloud object classification and detection. An ensemble of multiple model\ninstances is known to outperform a single model instance, but there is little\nstudy of the topic of ensemble learning for 3D point clouds. First, an ensemble\nof multiple model instances trained on the same part of the\n$\\textit{ModelNet40}$ dataset was tested for seven deep learning, point\ncloud-based classification algorithms: $\\textit{PointNet}$,\n$\\textit{PointNet++}$, $\\textit{SO-Net}$, $\\textit{KCNet}$,\n$\\textit{DeepSets}$, $\\textit{DGCNN}$, and $\\textit{PointCNN}$. Second, the\nensemble of different architectures was tested. Results of our experiments show\nthat the tested ensemble learning methods improve over state-of-the-art on the\n$\\textit{ModelNet40}$ dataset, from $92.65\\%$ to $93.64\\%$ for the ensemble of\nsingle architecture instances, $94.03\\%$ for two different architectures, and\n$94.15\\%$ for five different architectures. We show that the ensemble of two\nmodels with different architectures can be as effective as the ensemble of 10\nmodels with the same architecture. Third, a study on classic bagging i.e. with\ndifferent subsets used for training multiple model instances) was tested and\nsources of ensemble accuracy growth were investigated for best-performing\narchitecture, i.e. $\\textit{SO-Net}$. We also investigate the ensemble learning\nof $\\textit{Frustum PointNet}$ approach in the task of 3D object detection,\nincreasing the average precision of 3D box detection on the $\\textit{KITTI}$\ndataset from $63.1\\%$ to $66.5\\%$ using only three model instances. We measure\nthe inference time of all 3D classification architectures on a $\\textit{Nvidia\nJetson TX2}$, a common embedded computer for mobile robots, to allude to the\nuse of these models in real-life applications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:51:12 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 22:46:23 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Koguciuk", "Daniel", ""], ["Chechli\u0144ski", "\u0141ukasz", ""], ["El-Gaaly", "Tarek", ""]]}, {"id": "1904.08166", "submitter": "Julian Stier", "authors": "Julian Stier, Gabriele Gianini, Michael Granitzer, Konstantin Ziegler", "title": "Analysing Neural Network Topologies: a Game Theoretic Approach", "comments": null, "journal-ref": "Procedia Computer Science 126 (2018): 234-243", "doi": "10.1016/j.procs.2018.07.257", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks have shown impressive success in very different\napplication cases. Choosing a proper network architecture is a critical\ndecision for a network's success, usually done in a manual manner. As a\nstraightforward strategy, large, mostly fully connected architectures are\nselected, thereby relying on a good optimization strategy to find proper\nweights while at the same time avoiding overfitting. However, large parts of\nthe final network are redundant. In the best case, large parts of the network\nbecome simply irrelevant for later inferencing. In the worst case, highly\nparameterized architectures hinder proper optimization and allow the easy\ncreation of adverserial examples fooling the network. A first step in removing\nirrelevant architectural parts lies in identifying those parts, which requires\nmeasuring the contribution of individual components such as neurons. In\nprevious work, heuristics based on using the weight distribution of a neuron as\ncontribution measure have shown some success, but do not provide a proper\ntheoretical understanding. Therefore, in our work we investigate game theoretic\nmeasures, namely the Shapley value (SV), in order to separate relevant from\nirrelevant parts of an artificial neural network. We begin by designing a\ncoalitional game for an artificial neural network, where neurons form\ncoalitions and the average contributions of neurons to coalitions yield to the\nShapley value. In order to measure how well the Shapley value measures the\ncontribution of individual neurons, we remove low-contributing neurons and\nmeasure its impact on the network performance. In our experiments we show that\nthe Shapley value outperforms other heuristics for measuring the contribution\nof neurons.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 10:28:21 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Stier", "Julian", ""], ["Gianini", "Gabriele", ""], ["Granitzer", "Michael", ""], ["Ziegler", "Konstantin", ""]]}, {"id": "1904.08177", "submitter": "Jia Li", "authors": "Jia Li, Xing Wei, Guoqiang Yang, Xiao Sun, Changliang Li", "title": "Downhole Track Detection via Multiscale Conditional Generative\n  Adversarial Nets", "comments": "arXiv admin note: text overlap with arXiv:1711.11585 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent mine disasters cause a large number of casualties and property\nlosses. Autonomous driving is a fundamental measure for solving this problem,\nand track detection is one of the key technologies for computer vision to\nachieve downhole automatic driving. The track detection result based on the\ntraditional convolutional neural network (CNN) algorithm lacks the detailed and\nunique description of the object and relies too much on visual postprocessing\ntechnology. Therefore, this paper proposes a track detection algorithm based on\na multiscale conditional generative adversarial network (CGAN). The generator\nis decomposed into global and local parts using a multigranularity structure in\nthe generator network. A multiscale shared convolution structure is adopted in\nthe discriminator network to further supervise training the generator. Finally,\nthe Monte Carlo search technique is introduced to search the intermediate state\nof the generator, and the result is sent to the discriminator for comparison.\nCompared with the existing work, our model achieved 82.43\\% pixel accuracy and\nan average intersection-over-union (IOU) of 0.6218, and the detection of the\ntrack reached 95.01\\% accuracy in the downhole roadway scene test set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 10:50:37 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Jia", ""], ["Wei", "Xing", ""], ["Yang", "Guoqiang", ""], ["Sun", "Xiao", ""], ["Li", "Changliang", ""]]}, {"id": "1904.08303", "submitter": "Oleh Andriichuk", "authors": "Oleh Andriichuk, Vitaliy Tsyganok, Dmitry Lande, Oleg Chertov,\n  Yaroslava Porplenko", "title": "Usage of Decision Support Systems for Conflicts Modelling during\n  Information Operations Recognition", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of decision support systems for conflict modeling in information\noperations recognition is presented. An information operation is considered as\na complex weakly structured system. The model of conflict between two subjects\nis proposed based on the second-order rank reflexive model. The method is\ndescribed for construction of the design pattern for knowledge bases of\ndecision support systems. In the talk, the methodology is proposed for using of\ndecision support systems for modeling of conflicts in information operations\nrecognition based on the use of expert knowledge and content monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:58:51 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Andriichuk", "Oleh", ""], ["Tsyganok", "Vitaliy", ""], ["Lande", "Dmitry", ""], ["Chertov", "Oleg", ""], ["Porplenko", "Yaroslava", ""]]}, {"id": "1904.08311", "submitter": "Gakuto Kurata", "authors": "Gakuto Kurata, Kartik Audhkhasi", "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and\n  Knowledge Distillation", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional automatic speech recognition (ASR) systems trained from\nframe-level alignments can easily leverage posterior fusion to improve ASR\naccuracy and build a better single model with knowledge distillation.\nEnd-to-end ASR systems trained using the Connectionist Temporal Classification\n(CTC) loss do not require frame-level alignment and hence simplify model\ntraining. However, sparse and arbitrary posterior spike timings from CTC models\npose a new set of challenges in posterior fusion from multiple models and\nknowledge distillation between CTC models. We propose a method to train a CTC\nmodel so that its spike timings are guided to align with those of a pre-trained\nguiding CTC model. As a result, all models that share the same guiding model\nhave aligned spike timings. We show the advantage of our method in various\nscenarios including posterior fusion of CTC models and knowledge distillation\nbetween CTC models with different architectures. With the 300-hour Switchboard\ntraining data, the single word CTC model distilled from multiple models\nimproved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the Hub5 2000\nSwitchboard/CallHome test sets without using any data augmentation, language\nmodel, or complex decoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:18:23 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 13:14:30 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Kurata", "Gakuto", ""], ["Audhkhasi", "Kartik", ""]]}, {"id": "1904.08377", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Lei Tai, Ming Liu, Bertram E. Shi", "title": "Gaze Training by Modulated Dropout Improves Imitation Learning", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning by behavioral cloning is a prevalent method that has\nachieved some success in vision-based autonomous driving. The basic idea behind\nbehavioral cloning is to have the neural network learn from observing a human\nexpert's behavior. Typically, a convolutional neural network learns to predict\nthe steering commands from raw driver-view images by mimicking the behaviors of\nhuman drivers. However, there are other cues, such as gaze behavior, available\nfrom human drivers that have yet to be exploited. Previous researches have\nshown that novice human learners can benefit from observing experts' gaze\npatterns. We present here that deep neural networks can also profit from this.\nWe propose a method, gaze-modulated dropout, for integrating this gaze\ninformation into a deep driving network implicitly rather than as an additional\ninput. Our experimental results demonstrate that gaze-modulated dropout\nenhances the generalization capability of the network to unseen scenes.\nPrediction error in steering commands is reduced by 23.5% compared to uniform\ndropout. Running closed loop in the simulator, the gaze-modulated dropout net\nincreased the average distance travelled between infractions by 58.5%.\nConsistent with these results, the gaze-modulated dropout net shows lower model\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:25:37 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 11:36:33 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1904.08405", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi,\n  Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Joerg Conradt,\n  Kostas Daniilidis, Davide Scaramuzza", "title": "Event-based Vision: A Survey", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3008413", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors that differ from conventional frame\ncameras: Instead of capturing images at a fixed rate, they asynchronously\nmeasure per-pixel brightness changes, and output a stream of events that encode\nthe time, location and sign of the brightness changes. Event cameras offer\nattractive properties compared to traditional cameras: high temporal resolution\n(in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low\npower consumption, and high pixel bandwidth (on the order of kHz) resulting in\nreduced motion blur. Hence, event cameras have a large potential for robotics\nand computer vision in challenging scenarios for traditional cameras, such as\nlow-latency, high speed, and high dynamic range. However, novel methods are\nrequired to process the unconventional output of these sensors in order to\nunlock their potential. This paper provides a comprehensive overview of the\nemerging field of event-based vision, with a focus on the applications and the\nalgorithms developed to unlock the outstanding properties of event cameras. We\npresent event cameras from their working principle, the actual sensors that are\navailable and the tasks that they have been used for, from low-level vision\n(feature detection and tracking, optic flow, etc.) to high-level vision\n(reconstruction, segmentation, recognition). We also discuss the techniques\ndeveloped to process events, including learning-based techniques, as well as\nspecialized processors for these novel sensors, such as spiking neural\nnetworks. Additionally, we highlight the challenges that remain to be tackled\nand the opportunities that lie ahead in the search for a more efficient,\nbio-inspired way for machines to perceive and interact with the world.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:59:34 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:52:38 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 10:55:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gallego", "Guillermo", ""], ["Delbruck", "Tobi", ""], ["Orchard", "Garrick", ""], ["Bartolozzi", "Chiara", ""], ["Taba", "Brian", ""], ["Censi", "Andrea", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew", ""], ["Conradt", "Joerg", ""], ["Daniilidis", "Kostas", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1904.08415", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "An Exponential Lower Bound for the Runtime of the cGA on Jump Functions", "comments": "To appear in the Proceedings of FOGA 2019. arXiv admin note: text\n  overlap with arXiv:1903.10983", "journal-ref": null, "doi": "10.1145/3299904.3340304", "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first runtime analysis of an estimation-of-distribution algorithm\n(EDA) on the multi-modal jump function class, Hasen\\\"ohrl and Sutton (GECCO\n2018) proved that the runtime of the compact genetic algorithm with suitable\nparameter choice on jump functions with high probability is at most polynomial\n(in the dimension) if the jump size is at most logarithmic (in the dimension),\nand is at most exponential in the jump size if the jump size is\nsuper-logarithmic. The exponential runtime guarantee was achieved with a\nhypothetical population size that is also exponential in the jump size.\nConsequently, this setting cannot lead to a better runtime.\n  In this work, we show that any choice of the hypothetical population size\nleads to a runtime that, with high probability, is at least exponential in the\njump size. This result might be the first non-trivial exponential lower bound\nfor EDAs that holds for arbitrary parameter settings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:39:23 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 13:59:44 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1904.08468", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "Towards Evolutionary Theorem Proving for Isabelle/HOL", "comments": "2 pages, This is a pre-print of our poster-only paper accepted at\n  GECCO'19. For the final version, please visit the corresponding ACM website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanized theorem proving is becoming the basis of reliable systems\nprogramming and rigorous mathematics. Despite decades of progress in proof\nautomation, writing mechanized proofs still requires engineers' expertise and\nremains labor intensive. Recently, researchers have extracted heuristics of\ninteractive proof development from existing large proof corpora using\nsupervised learning. However, such existing proof corpora present only one way\nof proving conjectures, while there are often multiple equivalently effective\nways to prove one conjecture. In this abstract, we identify challenges in\ndiscovering heuristics for automatic proof search and propose our novel\napproach to improve heuristics of automatic proof search in Isabelle/HOL using\nevolutionary computation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:37:28 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "1904.08473", "submitter": "Yao Liu", "authors": "Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill", "title": "Off-Policy Policy Gradient with State Distribution Correction", "comments": "to appear at UAI 18; camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy policy optimization in Markov decision\nprocesses, and develop a novel off-policy policy gradient method. Prior\noff-policy policy gradient approaches have generally ignored the mismatch\nbetween the distribution of states visited under the behavior policy used to\ncollect data, and what would be the distribution of states under the learned\npolicy. Here we build on recent progress for estimating the ratio of the state\ndistributions under behavior and evaluation policies for policy evaluation, and\npresent an off-policy policy gradient optimization technique that can account\nfor this mismatch in distributions. We present an illustrative example of why\nthis is important and a theoretical convergence guarantee for our approach.\nEmpirically, we compare our method in simulations to several strong baselines\nwhich do not correct for this mismatch, significantly improving in the quality\nof the policy discovered.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 19:46:02 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 05:30:14 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Liu", "Yao", ""], ["Swaminathan", "Adith", ""], ["Agarwal", "Alekh", ""], ["Brunskill", "Emma", ""]]}, {"id": "1904.08499", "submitter": "Huibing Wang", "authors": "Huibing Wang, Jinjia Peng and Xianping Fu", "title": "Co-regularized Multi-view Sparse Reconstruction Embedding for Dimension\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of information technology, we have witnessed an age of\ndata explosion which produces a large variety of data filled with redundant\ninformation. Because dimension reduction is an essential tool which embeds\nhigh-dimensional data into a lower-dimensional subspace to avoid redundant\ninformation, it has attracted interests from researchers all over the world.\nHowever, facing with features from multiple views, it's difficult for most\ndimension reduction methods to fully comprehended multi-view features and\nintegrate compatible and complementary information from these features to\nconstruct low-dimensional subspace directly. Furthermore, most multi-view\ndimension reduction methods cannot handle features from nonlinear spaces with\nhigh dimensions. Therefore, how to construct a multi-view dimension reduction\nmethods which can deal with multi-view features from high-dimensional nonlinear\nspace is of vital importance but challenging. In order to address this problem,\nwe proposed a novel method named Co-regularized Multi-view Sparse\nReconstruction Embedding (CMSRE) in this paper. By exploiting correlations of\nsparse reconstruction from multiple views, CMSRE is able to learn local sparse\nstructures of nonlinear manifolds from multiple views and constructs\nsignificative low-dimensional representations for them. Due to the proposed\nco-regularized scheme, correlations of sparse reconstructions from multiple\nviews are preserved by CMSRE as much as possible. Furthermore, sparse\nrepresentation produces more meaningful correlations between features from each\nsingle view, which helps CMSRE to gain better performances. Various evaluations\nbased on the applications of document classification, face recognition and\nimage retrieval can demonstrate the effectiveness of the proposed approach on\nmulti-view dimension reduction.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 05:16:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Fu", "Xianping", ""]]}, {"id": "1904.08502", "submitter": "Davis Wertheimer", "authors": "Davis Wertheimer and Bharath Hariharan", "title": "Few-Shot Learning with Localization in Realistic Settings", "comments": "Appearing in CVPR 2019; added references in covariance pooling\n  sections, added link to code in supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recognition methods typically require large,\nartificially-balanced training classes, while few-shot learning methods are\ntested on artificially small ones. In contrast to both extremes, real world\nrecognition problems exhibit heavy-tailed class distributions, with cluttered\nscenes and a mix of coarse and fine-grained class distinctions. We show that\nprior methods designed for few-shot learning do not work out of the box in\nthese challenging conditions, based on a new \"meta-iNat\" benchmark. We\nintroduce three parameter-free improvements: (a) better training procedures\nbased on adapting cross-validation to meta-learning, (b) novel architectures\nthat localize objects using limited bounding box annotations before\nclassification, and (c) simple parameter-free expansions of the feature space\nbased on bilinear pooling. Together, these improvements double the accuracy of\nstate-of-the-art models on meta-iNat while generalizing to prior benchmarks,\ncomplex neural architectures, and settings with substantial domain shift.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:20:38 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 18:12:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wertheimer", "Davis", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1904.08503", "submitter": "Assaf Arbelle", "authors": "Assaf Arbelle, Eliav Elul and Tammy Riklin Raviv", "title": "QANet -- Quality Assurance Network for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Deep Learning framework, which quantitatively estimates\nimage segmentation quality without the need for human inspection or labeling.\nWe refer to this method as a Quality Assurance Network -- QANet. Specifically,\ngiven an image and a `proposed' corresponding segmentation, obtained by any\nmethod including manual annotation, the QANet solves a regression problem in\norder to estimate a predefined quality measure with respect to the unknown\nground truth. The QANet is by no means yet another segmentation method.\nInstead, it performs a multi-level, multi-feature comparison of an\nimage-segmentation pair based on a unique network architecture, called the\nRibCage.\n  To demonstrate the strength of the QANet, we addressed the evaluation of\ninstance segmentation using two different datasets from different domains,\nnamely, high throughput live cell microscopy images from the Cell Segmentation\nBenchmark and natural images of plants from the Leaf Segmentation Challenge.\nWhile synthesized segmentations were used to train the QANet, it was tested on\nsegmentations obtained by publicly available methods that participated in the\ndifferent challenges. We show that the QANet accurately estimates the scores of\nthe evaluated segmentations with respect to the hidden ground truth, as\npublished by the challenges' organizers.\n  The code is available at: TBD.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:38:57 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 12:57:36 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 08:39:52 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 08:43:24 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 19:09:56 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Arbelle", "Assaf", ""], ["Elul", "Eliav", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1904.08528", "submitter": "Reazul Hasan Russel", "authors": "Reazul H. Russel and Tianyi Gu and Marek Petrik", "title": "Robust Exploration with Tight Bayesian Plausibility Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimism about the poorly understood states and actions is the main driving\nforce of exploration for many provably-efficient reinforcement learning\nalgorithms. We propose optimism in the face of sensible value functions (OFVF)-\na novel data-driven Bayesian algorithm to constructing Plausibility sets for\nMDPs to explore robustly minimizing the worst case exploration cost. The method\ncomputes policies with tighter optimistic estimates for exploration by\nintroducing two new ideas. First, it is based on Bayesian posterior\ndistributions rather than distribution-free bounds. Second, OFVF does not\nconstruct plausibility sets as simple confidence intervals. Confidence\nintervals as plausibility sets are a sufficient but not a necessary condition.\nOFVF uses the structure of the value function to optimize the location and\nshape of the plausibility set to guarantee upper bounds directly without\nnecessarily enforcing the requirement for the set to be a confidence interval.\nOFVF proceeds in an episodic manner, where the duration of the episode is fixed\nand known. Our algorithm is inherently Bayesian and can leverage prior\ninformation. Our theoretical analysis shows the robustness of OFVF, and the\nempirical results demonstrate its practical promise.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:54:50 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Russel", "Reazul H.", ""], ["Gu", "Tianyi", ""], ["Petrik", "Marek", ""]]}, {"id": "1904.08537", "submitter": "Matthew Purri", "authors": "Matthew Purri, Jia Xue, Kristin Dana, Matthew Leotta, Dan Lipsa,\n  Zhixin Li, Bo Xu, Jie Shan", "title": "Material Segmentation of Multi-View Satellite Imagery", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material recognition methods use image context and local cues for pixel-wise\nclassification. In many cases only a single image is available to make a\nmaterial prediction. Image sequences, routinely acquired in applications such\nas mutliview stereo, can provide a sampling of the underlying reflectance\nfunctions that reveal pixel-level material attributes. We investigate\nmulti-view material segmentation using two datasets generated for building\nmaterial segmentation and scene material segmentation from the SpaceNet\nChallenge satellite image dataset. In this paper, we explore the impact of\nmulti-angle reflectance information by introducing the \\textit{reflectance\nresidual encoding}, which captures both the multi-angle and multispectral\ninformation present in our datasets. The residuals are computed by differencing\nthe sparse-sampled reflectance function with a dictionary of pre-defined\ndense-sampled reflectance functions. Our proposed reflectance residual features\nimproves material segmentation performance when integrated into pixel-wise and\nsemantic segmentation architectures. At test time, predictions from individual\nsegmentations are combined through softmax fusion and refined by building\nsegment voting. We demonstrate robust and accurate pixelwise segmentation\nresults using the proposed material segmentation pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:52:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Purri", "Matthew", ""], ["Xue", "Jia", ""], ["Dana", "Kristin", ""], ["Leotta", "Matthew", ""], ["Lipsa", "Dan", ""], ["Li", "Zhixin", ""], ["Xu", "Bo", ""], ["Shan", "Jie", ""]]}, {"id": "1904.08621", "submitter": "Guangliang Li", "authors": "Guangliang Li, Randy Gomez, Keisuke Nakamura, Jinying Lin, Qilei\n  Zhang, Bo He", "title": "Improving Interactive Reinforcement Agent Planning with Human\n  Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TAMER has proven to be a powerful interactive reinforcement learning method\nfor allowing ordinary people to teach and personalize autonomous agents'\nbehavior by providing evaluative feedback. However, a TAMER agent planning with\nUCT---a Monte Carlo Tree Search strategy, can only update states along its path\nand might induce high learning cost especially for a physical robot. In this\npaper, we propose to drive the agent's exploration along the optimal path and\nreduce the learning cost by initializing the agent's reward function via\ninverse reinforcement learning from demonstration. We test our proposed method\nin the RL benchmark domain---Grid World---with different discounts on human\nreward. Our results show that learning from demonstration can allow a TAMER\nagent to learn a roughly optimal policy up to the deepest search and encourage\nthe agent to explore along the optimal path. In addition, we find that learning\nfrom demonstration can improve the learning efficiency by reducing total\nfeedback, the number of incorrect actions and increasing the ratio of correct\nactions to obtain an optimal policy, allowing a TAMER agent to converge faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:45:36 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Guangliang", ""], ["Gomez", "Randy", ""], ["Nakamura", "Keisuke", ""], ["Lin", "Jinying", ""], ["Zhang", "Qilei", ""], ["He", "Bo", ""]]}, {"id": "1904.08626", "submitter": "Elena Camossi", "authors": "Maximilian Zocholl, Elena Camossi, Anne-Laure Jousselme, Cyril Ray", "title": "Ontology-based Design of Experiments on Big Data Solutions", "comments": "Pre-print and extended version of the poster paper presented at the\n  14th International Conference on Semantic Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Big data solutions are designed to cope with data of huge Volume and wide\nVariety, that need to be ingested at high Velocity and have potential Veracity\nissues, challenging characteristics that are usually referred to as the \"4Vs of\nBig Data\". In order to evaluate possibly complex big data solutions, stress\ntests require to assess a large number of combinations of sub-components\njointly with the possible big data variations. A formalization of the Design of\nExperiments (DoE) on big data solutions is aimed at ensuring the\nreproducibility of the experiments, facilitating their partitioning in\nsub-experiments and guaranteeing the consistency of their outcomes in a global\nassessment. In this paper, an ontology-based approach is proposed to support\nthe evaluation of a big data system in two ways. Firstly, the approach\nformalizes a decomposition and recombination of the big data solution, allowing\nfor the aggregation of component evaluation results at inter-component level.\nSecondly, existing work on DoE is translated into an ontology for supporting\nthe selection of experiments. The proposed ontology-based approach offers the\npossibility to combine knowledge from the evaluation domain and the application\ndomain. It exploits domain and inter-domain specific restrictions on the factor\ncombinations in order to reduce the number of experiments. Contrary to existing\napproaches, the proposed use of ontologies is not limited to the assertional\ndescription and exploitation of past experiments but offers richer\nterminological descriptions for the development of a DoE from scratch. As an\napplication example, a maritime big data solution to the problem of detecting\nand predicting vessel suspicious behaviour through mobility analysis is\nselected. The article is concluded with a sketch of future works.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:52:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Zocholl", "Maximilian", ""], ["Camossi", "Elena", ""], ["Jousselme", "Anne-Laure", ""], ["Ray", "Cyril", ""]]}, {"id": "1904.08637", "submitter": "Sungjin Lee", "authors": "Sungjin Lee, Qi Zhu, Ryuichi Takanobu, Xiang Li, Yaoqin Zhang, Zheng\n  Zhang, Jinchao Li, Baolin Peng, Xiujun Li, Minlie Huang and Jianfeng Gao", "title": "ConvLab: Multi-Domain End-to-End Dialog System Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ConvLab, an open-source multi-domain end-to-end dialog system\nplatform, that enables researchers to quickly set up experiments with reusable\ncomponents and compare a large set of different approaches, ranging from\nconventional pipeline systems to end-to-end neural models, in common\nenvironments. ConvLab offers a set of fully annotated datasets and associated\npre-trained reference models. As a showcase, we extend the MultiWOZ dataset\nwith user dialog act annotations to train all component models and demonstrate\nhow ConvLab makes it easy and effortless to conduct complicated experiments in\nmulti-domain end-to-end dialog settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 08:35:49 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lee", "Sungjin", ""], ["Zhu", "Qi", ""], ["Takanobu", "Ryuichi", ""], ["Li", "Xiang", ""], ["Zhang", "Yaoqin", ""], ["Zhang", "Zheng", ""], ["Li", "Jinchao", ""], ["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Huang", "Minlie", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1904.08755", "submitter": "Chris Choy", "authors": "Christopher Choy, JunYoung Gwak, Silvio Savarese", "title": "4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks", "comments": "CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotics and VR/AR applications, 3D-videos are readily-available\nsources of input (a continuous sequence of depth images, or LIDAR scans).\nHowever, those 3D-videos are processed frame-by-frame either through 2D\nconvnets or 3D perception algorithms. In this work, we propose 4-dimensional\nconvolutional neural networks for spatio-temporal perception that can directly\nprocess such 3D-videos using high-dimensional convolutions. For this, we adopt\nsparse tensors and propose the generalized sparse convolution that encompasses\nall discrete convolutions. To implement the generalized sparse convolution, we\ncreate an open-source auto-differentiation library for sparse tensors that\nprovides extensive functions for high-dimensional convolutional neural\nnetworks. We create 4D spatio-temporal convolutional neural networks using the\nlibrary and validate them on various 3D semantic segmentation benchmarks and\nproposed 4D datasets for 3D-video perception. To overcome challenges in the 4D\nspace, we propose the hybrid kernel, a special case of the generalized sparse\nconvolution, and the trilateral-stationary conditional random field that\nenforces spatio-temporal consistency in the 7D space-time-chroma space.\nExperimentally, we show that convolutional neural networks with only\ngeneralized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by\na large margin. Also, we show that on 3D-videos, 4D spatio-temporal\nconvolutional neural networks are robust to noise, outperform 3D convolutional\nneural networks and are faster than the 3D counterpart in some cases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:19:50 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 02:17:59 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 03:26:42 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 23:00:57 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Choy", "Christopher", ""], ["Gwak", "JunYoung", ""], ["Savarese", "Silvio", ""]]}, {"id": "1904.08804", "submitter": "George Panagopoulos", "authors": "George Panagopoulos, Fragkiskos D. Malliaros, Michalis Vazirgiannis", "title": "Multi-task Learning for Influence Estimation and Maximization", "comments": "IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of influence maximization when the social network is\naccompanied by diffusion cascades. In prior works, such information is used to\ncompute influence probabilities, which is utilized by stochastic diffusion\nmodels in influence maximization. Motivated by the recent criticism on the\neffectiveness of diffusion models as well as the galloping advancements in\ninfluence learning, we propose IMINFECTOR (Influence Maximization with\nINFluencer vECTORs), a unified approach that uses representations learned from\ndiffusion cascades to perform model-independent influence maximization that\nscales in real-world datasets. The first part of our methodology is a\nmulti-task neural network that learns embeddings of nodes that initiate\ncascades (influencer vectors) and embeddings of nodes that participate in them\n(susceptible vectors). The norm of an influencer vector captures the ability of\nthe node to create lengthy cascades and is used to estimate the expected\ninfluence spread and reduce the number of candidate seeds. In addition, the\ncombination of influencer and susceptible vectors form the diffusion\nprobabilities between nodes. These are used to reformulate the network as a\nbipartite graph and propose a greedy solution to influence maximization that\nretains the theoretical guarantees.We a pply our method in three sizable\nnetworks with diffusion cascades and evaluate it using cascades from future\ntime steps. IMINFECTOR is able to scale in all of them and outperforms various\ncompetitive algorithms and metrics from the diverse landscape of influence\nmaximization in terms of efficiency and seed set quality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:30:54 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 12:15:22 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 14:46:19 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Panagopoulos", "George", ""], ["Malliaros", "Fragkiskos D.", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1904.08809", "submitter": "Dario  Izzo", "authors": "Dario Izzo, Ekin \\\"Ozt\\\"urk, Marcus M\\\"artens", "title": "Interplanetary Transfers via Deep Representations of the Optimal Policy\n  and/or of the Value Function", "comments": null, "journal-ref": null, "doi": "10.1145/3319619.3326834", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of applications to interplanetary trajectories have been recently\nproposed based on deep networks. These approaches often rely on the\navailability of a large number of optimal trajectories to learn from. In this\npaper we introduce a new method to quickly create millions of optimal\nspacecraft trajectories from a single nominal trajectory. Apart from the\ngeneration of the nominal trajectory, no additional optimal control problems\nneed to be solved as all the trajectories, by construction, satisfy\nPontryagin's minimum principle and the relevant transversality conditions. We\nthen consider deep feed forward neural networks and benchmark three learning\nmethods on the created dataset: policy imitation, value function learning and\nvalue function gradient learning. Our results are shown for the case of the\ninterplanetary trajectory optimization problem of reaching Venus orbit, with\nthe nominal trajectory starting from the Earth. We find that both policy\nimitation and value function gradient learning are able to learn the optimal\nstate feedback, while in the case of value function learning the optimal policy\nis not captured, only the final value of the optimal propellant mass is.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:33:34 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Izzo", "Dario", ""], ["\u00d6zt\u00fcrk", "Ekin", ""], ["M\u00e4rtens", "Marcus", ""]]}, {"id": "1904.08873", "submitter": "Andrew Jaegle", "authors": "David Barack and Andrew Jaegle", "title": "Codes, Functions, and Causes: A Critique of Brette's Conceptual Analysis\n  of Coding", "comments": "Invited commentary on Romain Brette: \"Is coding a relevant metaphor\n  for the brain?\" (forthcoming in Behavioral and Brain Sciences). 4 pages,\n  including bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article, Brette argues that coding as a concept is inappropriate\nfor explanations of neurocognitive phenomena. Here, we argue that Brette's\nconceptual analysis mischaracterizes the structure of causal claims in coding\nand other forms of analysis-by-decomposition. We argue that analyses of this\nform are permissible, conceptually coherent, and offer essential tools for\nbuilding and developing models of neurocognitive systems like the brain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:27:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Barack", "David", ""], ["Jaegle", "Andrew", ""]]}, {"id": "1904.08935", "submitter": "Alan Gee", "authors": "Alan H. Gee, Diego Garcia-Olano, Joydeep Ghosh, and David Paydarfar", "title": "Explaining Deep Classification of Time-Series Data with Learned\n  Prototypes", "comments": "The first two authors contributed equally. Accepted May 20, Presented\n  Jun 14, 2019 at the ICML Time-series Workshop in Long Beach, CA, USA.\n  Accepted June 15, Presented Aug 11, 2019 at the IJCAI Workshop on Knowledge\n  Discovery in Healthcare Data in Macao, China. Formal proceedings available in\n  the CEUR Workshop Proceedings (http://ceur-ws.org/Vol-2429/)", "journal-ref": "Proceedings of the 4th International Workshop on Knowledge\n  Discovery in Healthcare Data, co-located with the 28th International Joint\n  Conference on Artificial Intelligence (IJCAI 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of deep learning networks raises a need for explainable AI so\nthat users and domain experts can be confident applying them to high-risk\ndecisions. In this paper, we leverage data from the latent space induced by\ndeep learning models to learn stereotypical representations or \"prototypes\"\nduring training to elucidate the algorithmic decision-making process. We study\nhow leveraging prototypes effect classification decisions of two dimensional\ntime-series data in a few different settings: (1) electrocardiogram (ECG)\nwaveforms to detect clinical bradycardia, a slowing of heart rate, in preterm\ninfants, (2) respiration waveforms to detect apnea of prematurity, and (3)\naudio waveforms to classify spoken digits. We improve upon existing models by\noptimizing for increased prototype diversity and robustness, visualize how\nthese prototypes in the latent space are used by the model to distinguish\nclasses, and show that prototypes are capable of learning features on two\ndimensional time-series data to produce explainable insights during\nclassification tasks. We show that the prototypes are capable of learning\nreal-world features - bradycardia in ECG, apnea in respiration, and\narticulation in speech - as well as features within sub-classes. Our novel work\nleverages learned prototypical framework on two dimensional time-series data to\nproduce explainable insights during classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:14:45 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 04:48:06 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 02:47:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Gee", "Alan H.", ""], ["Garcia-Olano", "Diego", ""], ["Ghosh", "Joydeep", ""], ["Paydarfar", "David", ""]]}, {"id": "1904.08939", "submitter": "Anh Nguyen", "authors": "Anh Nguyen and Jason Yosinski and Jeff Clune", "title": "Understanding Neural Networks via Feature Visualization: A survey", "comments": "A book chapter in an Interpretable ML book\n  (http://www.interpretable-ml.org/book/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuroscience method to understanding the brain is to find and study the\npreferred stimuli that highly activate an individual cell or groups of cells.\nRecent advances in machine learning enable a family of methods to synthesize\npreferred stimuli that cause a neuron in an artificial or biological brain to\nfire strongly. Those methods are known as Activation Maximization (AM) or\nFeature Visualization via Optimization. In this chapter, we (1) review existing\nAM techniques in the literature; (2) discuss a probabilistic interpretation for\nAM; and (3) review the applications of AM in debugging and explaining networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:46:26 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1904.08950", "submitter": "Xiaochuang Han", "authors": "Xiaochuang Han, Eunsol Choi, Chenhao Tan", "title": "No Permanent Friends or Enemies: Tracking Relationships between Nations\n  from News", "comments": "NAACL 2019; code available at https://github.com/BoulderDS/LARN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dynamics of international politics is important yet\nchallenging for civilians. In this work, we explore unsupervised neural models\nto infer relations between nations from news articles. We extend existing\nmodels by incorporating shallow linguistics information and propose a new\nautomatic evaluation metric that aligns relationship dynamics with manually\nannotated key events. As understanding international relations requires\ncarefully analyzing complex relationships, we conduct in-person human\nevaluations with three groups of participants. Overall, humans prefer the\noutputs of our model and give insightful feedback that suggests future\ndirections for human-centered models. Furthermore, our model reveals\ninteresting regional differences in news coverage. For instance, with respect\nto US-China relations, Singaporean media focus more on \"strengthening\" and\n\"purchasing\", while US media focus more on \"criticizing\" and \"denouncing\".\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 18:00:30 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Han", "Xiaochuang", ""], ["Choi", "Eunsol", ""], ["Tan", "Chenhao", ""]]}, {"id": "1904.08972", "submitter": "Ahmed Khalifa", "authors": "Ahmed Khalifa, Michael Cerny Green, Gabriella Barros, Julian Togelius", "title": "Intentional Computational Level Design", "comments": "8 pages, 10 figures, 3 tables, GECCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The procedural generation of levels and content in video games is a\nchallenging AI problem. Often such generation relies on an intelligent way of\nevaluating the content being generated so that constraints are satisfied and/or\nobjectives maximized. In this work, we address the problem of creating levels\nthat are not only playable but also revolve around specific mechanics in the\ngame. We use constrained evolutionary algorithms and quality-diversity\nalgorithms to generate small sections of Super Mario Bros levels called scenes,\nusing three different simulation approaches: Limited Agents, Punishing Model,\nand Mechanics Dimensions. All three approaches are able to create scenes that\ngive opportunity for a player to encounter or use targeted mechanics with\ndifferent properties. We conclude by discussing the advantages and\ndisadvantages of each approach and compare them to each other.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:10:58 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Khalifa", "Ahmed", ""], ["Green", "Michael Cerny", ""], ["Barros", "Gabriella", ""], ["Togelius", "Julian", ""]]}, {"id": "1904.08980", "submitter": "Felipe Codevilla", "authors": "Felipe Codevilla, Eder Santana, Antonio M. L\\'opez and Adrien Gaidon", "title": "Exploring the Limitations of Behavior Cloning for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving requires reacting to a wide variety of complex environment conditions\nand agent behaviors. Explicitly modeling each possible scenario is unrealistic.\nIn contrast, imitation learning can, in theory, leverage data from large fleets\nof human-driven cars. Behavior cloning in particular has been successfully used\nto learn simple visuomotor policies end-to-end, but scaling to the full\nspectrum of driving behaviors remains an unsolved problem. In this paper, we\npropose a new benchmark to experimentally investigate the scalability and\nlimitations of behavior cloning. We show that behavior cloning leads to\nstate-of-the-art results, including in unseen environments, executing complex\nlateral and longitudinal maneuvers without these reactions being explicitly\nprogrammed. However, we confirm well-known limitations (due to dataset bias and\noverfitting), new generalization issues (due to dynamic objects and the lack of\na causal model), and training instability requiring further research before\nbehavior cloning can graduate to real-world driving. The code of the studied\nbehavior cloning approaches can be found at\nhttps://github.com/felipecode/coiltraine .\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 19:29:56 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Codevilla", "Felipe", ""], ["Santana", "Eder", ""], ["L\u00f3pez", "Antonio M.", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1904.08993", "submitter": "Andrew Cropper", "authors": "Andrew Cropper", "title": "Playgol: learning programs through play", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children learn though play. We introduce the analogous idea of learning\nprograms through play. In this approach, a program induction system (the\nlearner) is given a set of tasks and initial background knowledge. Before\nsolving the tasks, the learner enters an unsupervised playing stage where it\ncreates its own tasks to solve, tries to solve them, and saves any solutions\n(programs) to the background knowledge. After the playing stage is finished,\nthe learner enters the supervised building stage where it tries to solve the\nuser-supplied tasks and can reuse solutions learnt whilst playing. The idea is\nthat playing allows the learner to discover reusable general programs on its\nown which can then help solve the user-supplied tasks. We claim that playing\ncan improve learning performance. We show that playing can reduce the textual\ncomplexity of target concepts which in turn reduces the sample complexity of a\nlearner. We implement our idea in Playgol, a new inductive logic programming\nsystem. We experimentally test our claim on two domains: robot planning and\nreal-world string transformations. Our experimental results suggest that\nplaying can substantially improve learning performance. We think that the idea\nof playing (or, more verbosely, unsupervised bootstrapping for supervised\nprogram induction) is an important contribution to the problem of developing\nprogram induction approaches that self-discover BK.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:14:02 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 15:24:51 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cropper", "Andrew", ""]]}, {"id": "1904.09023", "submitter": "Alex Kearney", "authors": "Alex Kearney, Oliver Oxton", "title": "Making Meaning: Semiotics Within Predictive Knowledge Architectures", "comments": "Accepted to RLDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Reinforcement Learning, there is a fledgling approach to\nconceptualizing the environment in terms of predictions. Central to this\npredictive approach is the assertion that it is possible to construct\nontologies in terms of predictions about sensation, behaviour, and time---to\ncategorize the world into entities which express all aspects of the world using\nonly predictions. This construction of ontologies is integral to predictive\napproaches to machine knowledge where objects are described exclusively in\nterms of how they are perceived. In this paper, we ground the Pericean model of\nsemiotics in terms of Reinforcement Learning Methods, describing Peirce's Three\nCategories in the notation of General Value Functions. Using the Peircean model\nof semiotics, we demonstrate that predictions alone are insufficient to\nconstruct an ontology; however, we identify predictions as being integral to\nthe meaning-making process. Moreover, we discuss how predictive knowledge\nprovides a particularly stable foundation for semiosis\\textemdash the process\nof making meaning\\textemdash and suggest a possible avenue of research to\ndesign algorithmic methods which construct semantics and meaning using\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:12:01 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kearney", "Alex", ""], ["Oxton", "Oliver", ""]]}, {"id": "1904.09024", "submitter": "Alex Kearney", "authors": "Alex Kearney, Patrick M. Pilarski", "title": "When is a Prediction Knowledge?", "comments": "Accepted to RLDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Reinforcement Learning, there is a growing collection of research\nwhich aims to express all of an agent's knowledge of the world through\npredictions about sensation, behaviour, and time. This work can be seen not\nonly as a collection of architectural proposals, but also as the beginnings of\na theory of machine knowledge in reinforcement learning. Recent work has\nexpanded what can be expressed using predictions, and developed applications\nwhich use predictions to inform decision-making on a variety of synthetic and\nreal-world problems. While promising, we here suggest that the notion of\npredictions as knowledge in reinforcement learning is as yet underdeveloped:\nsome work explicitly refers to predictions as knowledge, what the requirements\nare for considering a prediction to be knowledge have yet to be well explored.\nThis specification of the necessary and sufficient conditions of knowledge is\nimportant; even if claims about the nature of knowledge are left implicit in\ntechnical proposals, the underlying assumptions of such claims have\nconsequences for the systems we design. These consequences manifest in both the\nway we choose to structure predictive knowledge architectures, and how we\nevaluate them. In this paper, we take a first step to formalizing predictive\nknowledge by discussing the relationship of predictive knowledge learning\nmethods to existing theories of knowledge in epistemology. Specifically, we\nexplore the relationships between Generalized Value Functions and epistemic\nnotions of Justification and Truth.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:12:49 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kearney", "Alex", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1904.09039", "submitter": "Yi Tian Xu", "authors": "Yi Tian Xu, Yaqiao Li, David Meger", "title": "Human Motion Prediction via Pattern Completion in Latent Representation\n  Space", "comments": "Accepted in the 16th Conference on Computer and Robot Vision (CRV\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by ideas in cognitive science, we propose a novel and general\napproach to solve human motion understanding via pattern completion on a\nlearned latent representation space. Our model outperforms current\nstate-of-the-art methods in human motion prediction across a number of tasks,\nwith no customization. To construct a latent representation for time-series of\nvarious lengths, we propose a new and generic autoencoder based on\nsequence-to-sequence learning. While traditional inference strategies find a\ncorrelation between an input and an output, we use pattern completion, which\nviews the input as a partial pattern and to predict the best corresponding\ncomplete pattern. Our results demonstrate that this approach has advantages\nwhen combined with our autoencoder in solving human motion prediction, motion\ngeneration and action classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:36:19 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Xu", "Yi Tian", ""], ["Li", "Yaqiao", ""], ["Meger", "David", ""]]}, {"id": "1904.09067", "submitter": "Michael Cogswell", "authors": "Michael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Emergence of Compositional Language with Deep Generational Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has studied the emergence of language among deep reinforcement\nlearning agents that must collaborate to solve a task. Of particular interest\nare the factors that cause language to be compositional -- i.e., express\nmeaning by combining words which themselves have meaning. Evolutionary\nlinguists have found that in addition to structural priors like those already\nstudied in deep learning, the dynamics of transmitting language from generation\nto generation contribute significantly to the emergence of compositionality. In\nthis paper, we introduce these cultural evolutionary dynamics into language\nemergence by periodically replacing agents in a population to create a\nknowledge gap, implicitly inducing cultural transmission of language. We show\nthat this implicit cultural transmission encourages the resulting languages to\nexhibit better compositional generalization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:09:12 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 19:54:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cogswell", "Michael", ""], ["Lu", "Jiasen", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1904.09120", "submitter": "Yunze Man", "authors": "Yunze Man, Yangsibo Huang, Junyi Feng, Xi Li, Fei Wu", "title": "Deep Q Learning Driven CT Pancreas Segmentation with Geometry-Aware\n  U-Net", "comments": "in IEEE Transactions on Medical Imaging (2019)", "journal-ref": null, "doi": "10.1109/TMI.2019.2911588", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of pancreas is important for medical image analysis, yet it\nfaces great challenges of class imbalance, background distractions and\nnon-rigid geometrical features. To address these difficulties, we introduce a\nDeep Q Network(DQN) driven approach with deformable U-Net to accurately segment\nthe pancreas by explicitly interacting with contextual information and extract\nanisotropic features from pancreas. The DQN based model learns a\ncontext-adaptive localization policy to produce a visually tightened and\nprecise localization bounding box of the pancreas. Furthermore, deformable\nU-Net captures geometry-aware information of pancreas by learning geometrically\ndeformable filters for feature extraction. Experiments on NIH dataset validate\nthe effectiveness of the proposed framework in pancreas segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:36:21 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Man", "Yunze", ""], ["Huang", "Yangsibo", ""], ["Feng", "Junyi", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""]]}, {"id": "1904.09134", "submitter": "Marco Maratea", "authors": "Martin Gebser, Marco Maratea, Francesco Ricca", "title": "The Seventh Answer Set Programming Competition: Design and Results", "comments": "28 pages", "journal-ref": "Theory and Practice of Logic Programming 20 (2020) 176-204", "doi": "10.1017/S1471068419000061", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a prominent knowledge representation language\nwith roots in logic programming and non-monotonic reasoning. Biennial ASP\ncompetitions are organized in order to furnish challenging benchmark\ncollections and assess the advancement of the state of the art in ASP solving.\n  In this paper, we report on the design and results of the Seventh ASP\nCompetition, jointly organized by the University of Calabria (Italy), the\nUniversity of Genova (Italy), and the University of Potsdam (Germany), in\naffiliation with the 14th International Conference on Logic Programming and\nNon-Monotonic Reasoning (LPNMR 2017). (Under consideration for acceptance in\nTPLP).\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 09:51:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gebser", "Martin", ""], ["Maratea", "Marco", ""], ["Ricca", "Francesco", ""]]}, {"id": "1904.09201", "submitter": "Shichao Li", "authors": "Shichao Li and Kwang-Ting Cheng", "title": "Visualizing the decision-making process in deep neural decision forest", "comments": "Accepted by CVPR 2019 workshops on explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural decision forest (NDF) achieved remarkable performance on various\nvision tasks via combining decision tree and deep representation learning. In\nthis work, we first trace the decision-making process of this model and\nvisualize saliency maps to understand which portion of the input influence it\nmore for both classification and regression problems. We then apply NDF on a\nmulti-task coordinate regression problem and demonstrate the distribution of\nrouting probabilities, which is vital for interpreting NDF yet not shown for\nregression problems. The pre-trained model and code for visualization will be\navailable at https://github.com/Nicholasli1995/VisualizingNDF\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:10:03 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Li", "Shichao", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "1904.09239", "submitter": "Phan Trung Hai Nguyen", "authors": "Per Kristian Lehre and Phan Trung Hai Nguyen", "title": "Runtime Analysis of the Univariate Marginal Distribution Algorithm under\n  Low Selective Pressure and Prior Noise", "comments": "To appear at GECCO 2019, Prague, Czech Republic", "journal-ref": null, "doi": "10.1145/3321707.3321834", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a rigorous runtime analysis for the Univariate Marginal\nDistribution Algorithm on the LeadingOnes function, a well-known benchmark\nfunction in the theory community of evolutionary computation with a high\ncorrelation between decision variables. For a problem instance of size $n$, the\ncurrently best known upper bound on the expected runtime is\n$\\mathcal{O}(n\\lambda\\log\\lambda+n^2)$ (Dang and Lehre, GECCO 2015), while a\nlower bound necessary to understand how the algorithm copes with variable\ndependencies is still missing. Motivated by this, we show that the algorithm\nrequires a $e^{\\Omega(\\mu)}$ runtime with high probability and in expectation\nif the selective pressure is low; otherwise, we obtain a lower bound of\n$\\Omega(\\frac{n\\lambda}{\\log(\\lambda-\\mu)})$ on the expected runtime.\nFurthermore, we for the first time consider the algorithm on the function under\na prior noise model and obtain an $\\mathcal{O}(n^2)$ expected runtime for the\noptimal parameter settings. In the end, our theoretical results are accompanied\nby empirical findings, not only matching with rigorous analyses but also\nproviding new insights into the behaviour of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 15:49:27 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lehre", "Per Kristian", ""], ["Nguyen", "Phan Trung Hai", ""]]}, {"id": "1904.09273", "submitter": "David Johnson", "authors": "G\\\"orkem Pa\\c{c}ac{\\i}, David Johnson, Steve McKeever, Andreas Hamfelt", "title": "\"Why did you do that?\": Explaining black box models with Inductive\n  Synthesis", "comments": "12 pages, 1 figure, accepted for publication at the Solving Problems\n  with Uncertainties workshop as part of ICCS 2019, Faro, Portugal, June 12-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By their nature, the composition of black box models is opaque. This makes\nthe ability to generate explanations for the response to stimuli challenging.\nThe importance of explaining black box models has become increasingly important\ngiven the prevalence of AI and ML systems and the need to build legal and\nregulatory frameworks around them. Such explanations can also increase trust in\nthese uncertain systems. In our paper we present RICE, a method for generating\nexplanations of the behaviour of black box models by (1) probing a model to\nextract model output examples using sensitivity analysis; (2) applying\nCNPInduce, a method for inductive logic program synthesis, to generate logic\nprograms based on critical input-output pairs; and (3) interpreting the target\nprogram as a human-readable explanation. We demonstrate the application of our\nmethod by generating explanations of an artificial neural network trained to\nfollow simple traffic rules in a hypothetical self-driving car simulation. We\nconclude with a discussion on the scalability and usability of our approach and\nits potential applications to explanation-critical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 10:44:10 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Pa\u00e7ac\u0131", "G\u00f6rkem", ""], ["Johnson", "David", ""], ["McKeever", "Steve", ""], ["Hamfelt", "Andreas", ""]]}, {"id": "1904.09290", "submitter": "Peng Zhang", "authors": "Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael\n  Fu, Juan Zhao, Kai Li", "title": "FeatherNets: Convolutional Neural Networks as Light as Feather for Face\n  Anti-spoofing", "comments": "10 pages;6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face Anti-spoofing gains increased attentions recently in both academic and\nindustrial fields. With the emergence of various CNN based solutions, the\nmulti-modal(RGB, depth and IR) methods based CNN showed better performance than\nsingle modal classifiers. However, there is a need for improving the\nperformance and reducing the complexity. Therefore, an extreme light network\narchitecture(FeatherNet A/B) is proposed with a streaming module which fixes\nthe weakness of Global Average Pooling and uses less parameters. Our single\nFeatherNet trained by depth image only, provides a higher baseline with 0.00168\nACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure\nwith ``ensemble + cascade'' structure is presented to satisfy the performance\npreferred use cases. Meanwhile, the MMFD dataset is collected to provide more\nattacks and diversity to gain better generalization. We use the fusion method\nin the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the\nresult of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and\n0.9814(TPR@FPR=10e-4).\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:04:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Peng", ""], ["Zou", "Fuhao", ""], ["Wu", "Zhiwen", ""], ["Dai", "Nengli", ""], ["Mark", "Skarpness", ""], ["Fu", "Michael", ""], ["Zhao", "Juan", ""], ["Li", "Kai", ""]]}, {"id": "1904.09307", "submitter": "Varun Chandra Jammula", "authors": "Varun Chandra Jammula, Anshul Rai, Yezhou Yang", "title": "Active Adversarial Evader Tracking with a Probabilistic Pursuer under\n  the Pursuit-Evasion Game Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a mapped environment, we formulate the problem of visually tracking and\nfollowing an evader using a probabilistic framework. In this work, we consider\na non-holonomic robot with a limited visibility depth sensor in an indoor\nenvironment with obstacles. The mobile robot that follows the target is\nconsidered a pursuer and the agent being followed is considered an evader. We\npropose a probabilistic framework for both the pursuer and evader to achieve\ntheir conflicting goals. We introduce a smart evader that has information about\nthe location of the pursuer. The goal of this variant of the evader is to avoid\nbeing tracked by the pursuer by using the visibility region information\nobtained from the pursuer, to further challenge the proposed smart pursuer. To\nvalidate the efficiency of the framework, we conduct several experiments in\nsimulation by using Gazebo and evaluate the success rate of tracking an evader\nin various environments with different pursuer to evader speed ratios. Through\nour experiments we validate our hypothesis that a smart pursuer tracks an\nevader more effectively than a pursuer that just navigates in the environment\nrandomly. We also validate that an evader that is aware of the actions of the\npursuer is more successful at avoiding getting tracked by a smart pursuer than\na random evader. Finally, we empirically show that while a smart pursuer does\nincrease it's average success rate of tracking compared to a random pursuer,\nthere is an increased variance in its success rate distribution when the evader\nbecomes aware of its actions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 18:28:58 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Jammula", "Varun Chandra", ""], ["Rai", "Anshul", ""], ["Yang", "Yezhou", ""]]}, {"id": "1904.09324", "submitter": "Omer Levy", "authors": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models", "comments": "EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine translation systems generate text autoregressively from left to\nright. We, instead, use a masked language modeling objective to train a model\nto predict any subset of the target words, conditioned on both the input text\nand a partially masked target translation. This approach allows for efficient\niterative decoding, where we first predict all of the target words\nnon-autoregressively, and then repeatedly mask out and regenerate the subset of\nwords that the model is least confident about. By applying this strategy for a\nconstant number of iterations, our model improves state-of-the-art performance\nlevels for non-autoregressive and parallel decoding translation models by over\n4 BLEU on average. It is also able to reach within about 1 BLEU point of a\ntypical left-to-right transformer model, while decoding significantly faster.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:53:01 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 16:31:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ghazvininejad", "Marjan", ""], ["Levy", "Omer", ""], ["Liu", "Yinhan", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1904.09366", "submitter": "Buser Say", "authors": "Buser Say, Scott Sanner, Sylvie Thi\\'ebaux", "title": "Reward Potentials for Planning with Learned Neural Network Transition\n  Models", "comments": "To appear in the proceedings of the 25th International Conference on\n  Principles and Practice of Constraint Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal planning with respect to learned neural network (NN) models in\ncontinuous action and state spaces using mixed-integer linear programming\n(MILP) is a challenging task for branch-and-bound solvers due to the poor\nlinear relaxation of the underlying MILP model. For a given set of features,\npotential heuristics provide an efficient framework for computing bounds on\ncost (reward) functions. In this paper, we model the problem of finding optimal\npotential bounds for learned NN models as a bilevel program, and solve it using\na novel finite-time constraint generation algorithm. We then strengthen the\nlinear relaxation of the underlying MILP model by introducing constraints to\nbound the reward function based on the precomputed reward potentials.\nExperimentally, we show that our algorithm efficiently computes reward\npotentials for learned NN models, and that the overhead of computing reward\npotentials is justified by the overall strengthening of the underlying MILP\nmodel for the task of planning over long horizons.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:15:59 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 07:03:01 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 11:01:30 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 14:54:45 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Say", "Buser", ""], ["Sanner", "Scott", ""], ["Thi\u00e9baux", "Sylvie", ""]]}, {"id": "1904.09380", "submitter": "Harsh Trivedi", "authors": "Harsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, Niranjan\n  Balasubramanian", "title": "Repurposing Entailment for Multi-Hop Question Answering Tasks", "comments": "Accepted at NAACL'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) naturally reduces to an entailment problem, namely,\nverifying whether some text entails the answer to a question. However, for\nmulti-hop QA tasks, which require reasoning with multiple sentences, it remains\nunclear how best to utilize entailment models pre-trained on large scale\ndatasets such as SNLI, which are based on sentence pairs. We introduce Multee,\na general architecture that can effectively use entailment models for multi-hop\nQA tasks. Multee uses (i) a local module that helps locate important sentences,\nthereby avoiding distracting information, and (ii) a global module that\naggregates information by effectively incorporating importance weights.\nImportantly, we show that both modules can use entailment functions pre-trained\non a large scale NLI datasets. We evaluate performance on MultiRC and\nOpenBookQA, two multihop QA datasets. When using an entailment function\npre-trained on NLI datasets, Multee outperforms QA models trained only on the\ntarget QA datasets and the OpenAI transformer models. The code is available at\nhttps://github.com/StonyBrookNLP/multee.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 00:30:26 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Trivedi", "Harsh", ""], ["Kwon", "Heeyoung", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "1904.09408", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Mu Li, Alexander J. Smola", "title": "Language Models with Transformers", "comments": "12 pages, 7 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer architecture is superior to RNN-based models in computational\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\nmodels on various NLP tasks using pre-trained language models on large-scale\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\nlanguage model itself. Neither self-attention nor the positional encoding in\nthe Transformer is able to efficiently incorporate the word-level sequential\ncontext crucial to language modeling.\n  In this paper, we explore effective Transformer architectures for language\nmodel, including adding additional LSTM layers to better capture the sequential\ncontext while still keeping the computation efficient. We propose Coordinate\nArchitecture Search (CAS) to find an effective architecture through iterative\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\nstate-of-the-art LSTMs. The source code is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 06:43:14 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 04:25:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Chenguang", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1904.09422", "submitter": "Ario Santoso", "authors": "Ario Santoso, Michael Felderer", "title": "Specification-Driven Predictive Business Process Monitoring", "comments": "This article significantly extends the previous work in\n  https://doi.org/10.1007/978-3-319-91704-7_7 which has a technical report in\n  arXiv:1804.00617. This article and the previous work have a coauthor in\n  common", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive analysis in business process monitoring aims at forecasting the\nfuture information of a running business process. The prediction is typically\nmade based on the model extracted from historical process execution logs (event\nlogs). In practice, different business domains might require different kinds of\npredictions. Hence, it is important to have a means for properly specifying the\ndesired prediction tasks, and a mechanism to deal with these various prediction\ntasks. Although there have been many studies in this area, they mostly focus on\na specific prediction task. This work introduces a language for specifying the\ndesired prediction tasks, and this language allows us to express various kinds\nof prediction tasks. This work also presents a mechanism for automatically\ncreating the corresponding prediction model based on the given specification.\nDifferently from previous studies, instead of focusing on a particular\nprediction task, we present an approach to deal with various prediction tasks\nbased on the given specification of the desired prediction tasks. We also\nprovide an implementation of the approach which is used to conduct experiments\nusing real-life event logs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 09:01:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Santoso", "Ario", ""], ["Felderer", "Michael", ""]]}, {"id": "1904.09433", "submitter": "Mohammad Shojafar", "authors": "Rahim Taheri, Reza Javidan, Mohammad Shojafar, Vinod P and Mauro Conti", "title": "Can Machine Learning Model with Static Features be Fooled: an\n  Adversarial Machine Learning Approach", "comments": "20 pages, 6 figures, 5 tables", "journal-ref": "Cluster Computing 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread adoption of smartphones dramatically increases the risk of\nattacks and the spread of mobile malware, especially on the Android platform.\nMachine learning-based solutions have been already used as a tool to supersede\nsignature-based anti-malware systems. However, malware authors leverage\nfeatures from malicious and legitimate samples to estimate statistical\ndifference in-order to create adversarial examples. Hence, to evaluate the\nvulnerability of machine learning algorithms in malware detection, we propose\nfive different attack scenarios to perturb malicious applications (apps). By\ndoing this, the classification algorithm inappropriately fits the discriminant\nfunction on the set of data points, eventually yielding a higher\nmisclassification rate. Further, to distinguish the adversarial examples from\nbenign samples, we propose two defense mechanisms to counter attacks. To\nvalidate our attacks and solutions, we test our model on three different\nbenchmark datasets. We also test our methods using various classifier\nalgorithms and compare them with the state-of-the-art data poisoning method\nusing the Jacobian matrix. Promising results show that generated adversarial\nsamples can evade detection with a very high probability. Additionally, evasive\nvariants generated by our attack models when used to harden the developed\nanti-malware system improves the detection rate up to 50% when using the\nGenerative Adversarial Network (GAN) method.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 11:17:51 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 07:44:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Taheri", "Rahim", ""], ["Javidan", "Reza", ""], ["Shojafar", "Mohammad", ""], ["P", "Vinod", ""], ["Conti", "Mauro", ""]]}, {"id": "1904.09443", "submitter": "Razieh Mehri", "authors": "Razieh Mehri and Volker Haarslev and Hamidreza Chinaei", "title": "Learning the Right Expansion-ordering Heuristics for Satisfiability\n  Testing in OWL Reasoners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Ontology Language (OWL) reasoners are used to infer new logical relations\nfrom ontologies. While inferring new facts, these reasoners can be further\noptimized, e.g., by properly ordering disjuncts in disjunction expressions of\nontologies for satisfiability testing of concepts. Different expansion-ordering\nheuristics have been developed for this purpose. The built-in heuristics in\nthese reasoners determine the order for branches in search trees while each\nheuristic choice causes different effects for various ontologies depending on\nthe ontologies' syntactic structure and probably other features as well. A\nlearning-based approach that takes into account the features aims to select an\nappropriate expansion-ordering heuristic for each ontology. The proper choice\nis expected to accelerate the reasoning process for the reasoners. In this\npaper, the effect of our methodology is investigated on a well-known reasoner\nthat is JFact. Our experiments show the average speedup by a factor of one to\ntwo orders of magnitude for satisfiability testing after applying learning\nmethodology for selecting the right expansion-ordering heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 12:58:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Mehri", "Razieh", ""], ["Haarslev", "Volker", ""], ["Chinaei", "Hamidreza", ""]]}, {"id": "1904.09447", "submitter": "Martin Schmitt", "authors": "Martin Schmitt and Sahand Sharifzadeh and Volker Tresp and Hinrich\n  Sch\\\"utze", "title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs\n  and Semantic Parsing", "comments": "Accepted as long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore\nsupervised approaches to both graph-to-text generation and text-to-graph\nknowledge extraction (semantic parsing) will always suffer from a shortage of\ndomain-specific parallel graph-text data; at the same time, adapting a model\ntrained on a different domain is often impossible due to little or no overlap\nin entities and relations. This situation calls for an approach that (1) does\nnot need large amounts of annotated data and thus (2) does not need to rely on\ndomain adaptation techniques to work well in different domains. To this end, we\npresent the first approach to unsupervised text generation from KGs and show\nsimultaneously how it can be used for unsupervised semantic parsing. We\nevaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene\ngraphs from Visual Genome. Our system outperforms strong baselines for both\ntext$\\leftrightarrow$graph conversion tasks without any manual adaptation from\none dataset to the other. In additional experiments, we investigate the impact\nof using different unsupervised objectives.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 13:46:36 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 14:01:24 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 10:14:04 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 10:07:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Schmitt", "Martin", ""], ["Sharifzadeh", "Sahand", ""], ["Tresp", "Volker", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1904.09477", "submitter": "Eisa Alanazi", "authors": "Abdulaziz Alashaikh and Eisa Alanazi", "title": "Preference-based Multiobjective Virtual Machine Placement: A Ceteris\n  Paribus Approach", "comments": "a pre-print for GECCO 2019 2 pages poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work adopts the notion of Ceteris Paribus (CP) as an interpretation of\nthe Decision Maker (DM) preferences and incorporates it in a constrained\nmultiobjective problem known as virtual machine placement (VMP). VMP is an\nessential multiobjective problem in the design and operation of cloud data\ncenters concerned about placing each virtual machine to a physical machine (a\nserver) in the data center. We analyze the effectiveness of CP interpretation\non VMP problems and propose an NSGA-II variant with which preferred solutions\nare returned at almost no extra time cost.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 18:26:03 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Alashaikh", "Abdulaziz", ""], ["Alanazi", "Eisa", ""]]}, {"id": "1904.09489", "submitter": "Joel Ruben Antony Moniz", "authors": "Joel Ruben Antony Moniz, Barun Patra, Sarthak Garg", "title": "Compression and Localization in Reinforcement Learning for ATARI Games", "comments": "NeurIPS 2018 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become commonplace in the domain of reinforcement\nlearning, but are often expensive in terms of the number of parameters needed.\nWhile compressing deep neural networks has of late assumed great importance to\novercome this drawback, little work has been done to address this problem in\nthe context of reinforcement learning agents. This work aims at making first\nsteps towards model compression in an RL agent. In particular, we compress\nnetworks to drastically reduce the number of parameters in them (to sizes less\nthan 3% of their original size), further facilitated by applying a global max\npool after the final convolution layer, and propose using Actor-Mimic in the\ncontext of compression. Finally, we show that this global max-pool allows for\nweakly supervised object localization, improving the ability to identify the\nagent's points of focus.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 19:42:50 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Moniz", "Joel Ruben Antony", ""], ["Patra", "Barun", ""], ["Garg", "Sarthak", ""]]}, {"id": "1904.09503", "submitter": "Jianyu Chen", "authors": "Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka", "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban autonomous driving decision making is challenging due to complex road\ngeometry and multi-agent interactions. Current decision making methods are\nmostly manually designing the driving policy, which might result in sub-optimal\nsolutions and is expensive to develop, generalize and maintain at scale. On the\nother hand, with reinforcement learning (RL), a policy can be learned and\nimproved automatically without any manual designs. However, current RL methods\ngenerally do not work well on complex urban scenarios. In this paper, we\npropose a framework to enable model-free deep reinforcement learning in\nchallenging urban autonomous driving scenarios. We design a specific input\nrepresentation and use visual encoding to capture the low-dimensional latent\nstates. Several state-of-the-art model-free deep RL algorithms are implemented\ninto our framework, with several tricks to improve their performance. We\nevaluate our method in a challenging roundabout task with dense surrounding\nvehicles in a high-definition driving simulator. The result shows that our\nmethod can solve the task well and is significantly better than the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 22:02:45 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 21:11:58 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chen", "Jianyu", ""], ["Yuan", "Bodi", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1904.09585", "submitter": "Serhii Havrylov", "authors": "Zhifeng Hu, Serhii Havrylov, Ivan Titov, Shay B. Cohen", "title": "Obfuscation for Privacy-preserving Syntactic Parsing", "comments": "Accepted to IWPT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of homomorphic encryption is to encrypt data such that another party\ncan operate on it without being explicitly exposed to the content of the\noriginal data. We introduce an idea for a privacy-preserving transformation on\nnatural language data, inspired by homomorphic encryption. Our primary tool is\n{\\em obfuscation}, relying on the properties of natural language. Specifically,\na given English text is obfuscated using a neural model that aims to preserve\nthe syntactic relationships of the original sentence so that the obfuscated\nsentence can be parsed instead of the original one. The model works at the word\nlevel, and learns to obfuscate each word separately by changing it into a new\nword that has a similar syntactic role. The text obfuscated by our model leads\nto better performance on three syntactic parsers (two dependency and one\nconstituency parsers) in comparison to an upper-bound random substitution\nbaseline. More specifically, the results demonstrate that as more terms are\nobfuscated (by their part of speech), the substitution upper bound\nsignificantly degrades, while the neural model maintains a relatively high\nperforming parser. All of this is done without much sacrifice of privacy\ncompared to the random substitution upper bound. We also further analyze the\nresults, and discover that the substituted words have similar syntactic\nproperties, but different semantic content, compared to the original words.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 12:09:39 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:38:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hu", "Zhifeng", ""], ["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.09605", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang and Zongqing Lu", "title": "Generative Exploration and Exploitation", "comments": "AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse reward is one of the biggest challenges in reinforcement learning\n(RL). In this paper, we propose a novel method called Generative Exploration\nand Exploitation (GENE) to overcome sparse reward. GENE automatically generates\nstart states to encourage the agent to explore the environment and to exploit\nreceived reward signals. GENE can adaptively tradeoff between exploration and\nexploitation according to the varying distributions of states experienced by\nthe agent as the learning progresses. GENE relies on no prior knowledge about\nthe environment and can be combined with any RL algorithm, no matter on-policy\nor off-policy, single-agent or multi-agent. Empirically, we demonstrate that\nGENE significantly outperforms existing methods in three tasks with only binary\nrewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies\nverify the emergence of progressive exploration and automatic reversing.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:15:24 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:56:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Lu", "Zongqing", ""]]}, {"id": "1904.09612", "submitter": "Qian Yang", "authors": "Qian Yang, Aaron Steinfeld, John Zimmerman", "title": "Unremarkable AI: Fitting Intelligent Decision Support into Critical,\n  Clinical Decision-Making Processes", "comments": null, "journal-ref": "CHI Conference on Human Factors in Computing Systems Proceedings\n  2019 (CHI'19)", "doi": "10.1145/3290605.3300468", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support tools (DST) promise improved healthcare outcomes by\noffering data-driven insights. While effective in lab settings, almost all DSTs\nhave failed in practice. Empirical research diagnosed poor contextual fit as\nthe cause. This paper describes the design and field evaluation of a radically\nnew form of DST. It automatically generates slides for clinicians' decision\nmeetings with subtly embedded machine prognostics. This design took inspiration\nfrom the notion of \"Unremarkable Computing\", that by augmenting the users'\nroutines technology/AI can have significant importance for the users yet remain\nunobtrusive. Our field evaluation suggests clinicians are more likely to\nencounter and embrace such a DST. Drawing on their responses, we discuss the\nimportance and intricacies of finding the right level of unremarkableness in\nDST design, and share lessons learned in prototyping critical AI systems as a\nsituated experience.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:57:44 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Qian", ""], ["Steinfeld", "Aaron", ""], ["Zimmerman", "John", ""]]}, {"id": "1904.09654", "submitter": "Maruthi Rohit Ayyagari", "authors": "Maruthi Rohit Ayyagari", "title": "Integrating Association Rules with Decision Trees in Object-Relational\n  Databases", "comments": "8 pages, 4 figures, 7 tables, journal", "journal-ref": "International Journal of Engineering Trends and Technology 67.3\n  (2019): 102-108", "doi": "10.14445/22312803/IJCTT-V67I3P120", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has provided evidence that associative classification produces more\naccurate results compared to other classification models. The Classification\nBased on Association (CBA) is one of the famous Associative Classification\nalgorithms that generates accurate classifiers. However, current association\nclassification algorithms reside external to databases, which reduces the\nflexibility of enterprise analytics systems. This paper implements the CBA in\nOracle database using two variant models: hardcoding the CBA in Oracle Data\nMining (ODM) package and Integrating Oracle Apriori model with the Oracle\nDecision tree model. We compared the proposed model performance with Naive\nBayes, Support Vector Machine, Random Forests, and Decision Tree over 18\ndatasets from UCI. Results showed that our models outperformed the original CBA\nmodel with 1 percent and is competitive to chosen classification models over\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 20:04:58 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ayyagari", "Maruthi Rohit", ""]]}, {"id": "1904.09673", "submitter": "Hongji Huang", "authors": "Hongji Huang, Song Guo, Guan Gui, Zhen Yang, Jianhua Zhang, Hikmet\n  Sari, and Fumiyuki Adachi", "title": "Deep Learning for Physical-Layer 5G Wireless Techniques: Opportunities,\n  Challenges and Solutions", "comments": "Submitted a possible publication to IEEE Wireless Communications\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new demands for high-reliability and ultra-high capacity wireless\ncommunication have led to extensive research into 5G communications. However,\nthe current communication systems, which were designed on the basis of\nconventional communication theories, signficantly restrict further performance\nimprovements and lead to severe limitations. Recently, the emerging deep\nlearning techniques have been recognized as a promising tool for handling the\ncomplicated communication systems, and their potential for optimizing wireless\ncommunications has been demonstrated. In this article, we first review the\ndevelopment of deep learning solutions for 5G communication, and then propose\nefficient schemes for deep learning-based 5G scenarios. Specifically, the key\nideas for several important deep learningbased communication methods are\npresented along with the research opportunities and challenges. In particular,\nnovel communication frameworks of non-orthogonal multiple access (NOMA),\nmassive multiple-input multiple-output (MIMO), and millimeter wave (mmWave) are\ninvestigated, and their superior performances are demonstrated. We vision that\nthe appealing deep learning-based wireless physical layer frameworks will bring\na new direction in communication theories and that this work will move us\nforward along this road.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 22:52:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Huang", "Hongji", ""], ["Guo", "Song", ""], ["Gui", "Guan", ""], ["Yang", "Zhen", ""], ["Zhang", "Jianhua", ""], ["Sari", "Hikmet", ""], ["Adachi", "Fumiyuki", ""]]}, {"id": "1904.09681", "submitter": "Evangelos Pournaras", "authors": "Jovan Nikolic, Evangelos Pournaras", "title": "Structural Self-adaptation for Decentralized Pervasive Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication structure plays a key role in the learning capability of\ndecentralized systems. Structural self-adaptation, by means of\nself-organization, changes the order as well as the input information of the\nagents' collective decision-making. This paper studies the role of agents'\nrepositioning on the same communication structure, i.e. a tree, as the means to\nexpand the learning capacity in complex combinatorial optimization problems,\nfor instance, load-balancing power demand to prevent blackouts or efficient\nutilization of bike sharing stations. The optimality of structural\nself-adaptations is rigorously studied by constructing a novel large-scale\nbenchmark that consists of 4000 agents with synthetic and real-world data\nperforming 4 million structural self-adaptations during which almost 320\nbillion learning messages are exchanged. Based on this benchmark dataset, 124\ndeterministic structural criteria, applied as learning meta-features, are\nsystematically evaluated as well as two online structural self-adaptation\nstrategies designed to expand learning capacity. Experimental evaluation\nidentifies metrics that capture agents with influential information and their\noptimal positioning. Significant gain in learning performance is observed for\nthe two strategies especially under low-performing initialization. Strikingly,\nthe strategy that triggers structural self-adaptation in a more exploratory\nfashion is the most cost-effective.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 23:55:01 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 11:31:09 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Nikolic", "Jovan", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "1904.09705", "submitter": "Yu-Ping Ruan", "authors": "Yu-Ping Ruan, Xiaodan Zhu, Zhen-Hua Ling, Zhan Shi, Quan Liu and Si\n  Wei", "title": "Exploring Unsupervised Pretraining and Sentence Structure Modelling for\n  Winograd Schema Challenge", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Winograd Schema Challenge (WSC) was proposed as an AI-hard problem in testing\ncomputers' intelligence on common sense representation and reasoning. This\npaper presents the new state-of-theart on WSC, achieving an accuracy of 71.1%.\nWe demonstrate that the leading performance benefits from jointly modelling\nsentence structures, utilizing knowledge learned from cutting-edge pretraining\nmodels, and performing fine-tuning. We conduct detailed analyses, showing that\nfine-tuning is critical for achieving the performance, but it helps more on the\nsimpler associative problems. Modelling sentence dependency structures,\nhowever, consistently helps on the harder non-associative subset of WSC.\nAnalysis also shows that larger fine-tuning datasets yield better performances,\nsuggesting the potential benefit of future work on annotating more Winograd\nschema sentences.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 03:00:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ruan", "Yu-Ping", ""], ["Zhu", "Xiaodan", ""], ["Ling", "Zhen-Hua", ""], ["Shi", "Zhan", ""], ["Liu", "Quan", ""], ["Wei", "Si", ""]]}, {"id": "1904.09775", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Meisam Razaviyayn and Maziar Sanjabi", "title": "Training generative networks using random discriminators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Generative Adversarial Networks (GANs) have drawn a lot of\nattentions for learning the underlying distribution of data in various\napplications. Despite their wide applicability, training GANs is notoriously\ndifficult. This difficulty is due to the min-max nature of the resulting\noptimization problem and the lack of proper tools of solving general\n(non-convex, non-concave) min-max optimization problems. In this paper, we try\nto alleviate this problem by proposing a new generative network that relies on\nthe use of random discriminators instead of adversarial design. This design\nhelps us to avoid the min-max formulation and leads to an optimization problem\nthat is stable and could be solved efficiently. The performance of the proposed\nmethod is evaluated using handwritten digits (MNIST) and Fashion products\n(Fashion-MNIST) data sets. While the resulting images are not as sharp as\nadversarial training, the use of random discriminator leads to a much faster\nalgorithm as compared to the adversarial counterpart. This observation, at the\nminimum, illustrates the potential of the random discriminator approach for\nwarm-start in training GANs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 08:53:18 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Barazandeh", "Babak", ""], ["Razaviyayn", "Meisam", ""], ["Sanjabi", "Maziar", ""]]}, {"id": "1904.09807", "submitter": "Christian H\\\"ager", "authors": "Christian H\\\"ager, Henry D. Pfister, Rick M. B\\\"utler, Gabriele Liga,\n  Alex Alvarado", "title": "Revisiting Multi-Step Nonlinearity Compensation with Machine Learning", "comments": "4 pages, 3 figures, This is a preprint of a paper submitted to the\n  2019 European Conference on Optical Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the efficient compensation of fiber nonlinearity, one of the guiding\nprinciples appears to be: fewer steps are better and more efficient. We\nchallenge this assumption and show that carefully designed multi-step\napproaches can lead to better performance-complexity trade-offs than their\nfew-step counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 12:01:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["H\u00e4ger", "Christian", ""], ["Pfister", "Henry D.", ""], ["B\u00fctler", "Rick M.", ""], ["Liga", "Gabriele", ""], ["Alvarado", "Alex", ""]]}, {"id": "1904.09828", "submitter": "Alex Churchill", "authors": "Alex Churchill, Stella Biderman, Austin Herrick", "title": "Magic: The Gathering is Turing Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\textit{Magic: The Gathering}$ is a popular and famously complicated trading\ncard game about magical combat. In this paper we show that optimal play in\nreal-world $\\textit{Magic}$ is at least as hard as the Halting Problem, solving\na problem that has been open for a decade. To do this, we present a methodology\nfor embedding an arbitrary Turing machine into a game of $\\textit{Magic}$ such\nthat the first player is guaranteed to win the game if and only if the Turing\nmachine halts. Our result applies to how real $\\textit{Magic}$ is played, can\nbe achieved using standard-size tournament-legal decks, and does not rely on\nstochasticity or hidden information. Our result is also highly unusual in that\nall moves of both players are forced in the construction. This shows that even\nrecognising who will win a game in which neither player has a non-trivial\ndecision to make for the rest of the game is undecidable. We conclude with a\ndiscussion of the implications for a unified computational theory of games and\nremarks about the playability of such a board in a tournament setting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 23:48:09 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 10:16:57 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Churchill", "Alex", ""], ["Biderman", "Stella", ""], ["Herrick", "Austin", ""]]}, {"id": "1904.09829", "submitter": "Riccardo Franco", "authors": "Riccardo Franco", "title": "First steps to a constructor theory of cognition", "comments": "arXiv admin note: text overlap with arXiv:1405.5563,\n  arXiv:1507.03287, arXiv:1601.06610 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article applies the conceptual framework of constructor theory of\ninformation to cognition theory. The main result of this work is that cognition\ntheory, in specific situations concerning for example the conjunction fallacy\nheuristic, requires the use of superinformation media, just as quantum theory.\nThis result entails that quantum and cognition theories can be considered as\nelements of a general class of superinformation-based subsidiary theories.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:39:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Franco", "Riccardo", ""]]}, {"id": "1904.09837", "submitter": "Md. Noor-E-Alam", "authors": "Md Mahmudul Hassan, Dizuo Jiang, A. M. M. Sharif Ullah and Md.\n  Noor-E-Alam", "title": "Resilient Supplier Selection in Logistics 4.0 with Heterogeneous\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supplier selection problem has gained extensive attention in the prior\nstudies. However, research based on Fuzzy Multi-Attribute Decision Making\n(F-MADM) approach in ranking resilient suppliers in logistic 4 is still in its\ninfancy. Traditional MADM approach fails to address the resilient supplier\nselection problem in logistic 4 primarily because of the large amount of data\nconcerning some attributes that are quantitative, yet difficult to process\nwhile making decisions. Besides, some qualitative attributes prevalent in\nlogistic 4 entail imprecise perceptual or judgmental decision relevant\ninformation, and are substantially different than those considered in\ntraditional suppler selection problems. This study develops a Decision Support\nSystem (DSS) that will help the decision maker to incorporate and process such\nimprecise heterogeneous data in a unified framework to rank a set of resilient\nsuppliers in the logistic 4 environment. The proposed framework induces a\ntriangular fuzzy number from large-scale temporal data using\nprobability-possibility consistency principle. Large number of non-temporal\ndata presented graphically are computed by extracting granular information that\nare imprecise in nature. Fuzzy linguistic variables are used to map the\nqualitative attributes. Finally, fuzzy based TOPSIS method is adopted to\ngenerate the ranking score of alternative suppliers. These ranking scores are\nused as input in a Multi-Choice Goal Programming (MCGP) model to determine\noptimal order allocation for respective suppliers. Finally, a sensitivity\nanalysis assesses how the Suppliers Cost versus Resilience Index (SCRI) changes\nwhen differential priorities are set for respective cost and resilience\nattributes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:33:37 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 21:18:53 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2019 04:04:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Hassan", "Md Mahmudul", ""], ["Jiang", "Dizuo", ""], ["Ullah", "A. M. M. Sharif", ""], ["Noor-E-Alam", "Md.", ""]]}, {"id": "1904.09845", "submitter": "Mohannad Babli", "authors": "Mohannad Babli and Eva Onaindia", "title": "A context-aware knowledge acquisition for planning applications using\n  ontologies", "comments": "13 pages, 11 Figures, conference. arXiv admin note: text overlap with\n  arXiv:1904.03606", "journal-ref": "33rd International Business Information Management (IBIMA),\n  INNOVATION MANAGEMENT AND EDUCATION EXCELLENCE THROUGH VISION 2020 (pp.\n  3199-3208)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated planning technology has developed significantly. Designing a\nplanning model that allows an automated agent to be capable of reacting\nintelligently to unexpected events in a real execution environment yet remains\na challenge. This article describes a domain-independent approach to allow the\nagent to be context-aware of its execution environment and the task it\nperforms, acquire new information that is guaranteed to be related and more\nimportantly manageable, and integrate such information into its model through\nthe use of ontologies and semantic operations to autonomously formulate new\nobjectives, resulting in a more human-like behaviour for handling unexpected\nevents in the context of opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 13:48:02 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Babli", "Mohannad", ""], ["Onaindia", "Eva", ""]]}, {"id": "1904.09862", "submitter": "Khaled Saleh", "authors": "Khaled Saleh, Mohammed Hossny, Saeid Nahavandi", "title": "Real-time Intent Prediction of Pedestrians for Autonomous Ground\n  Vehicles via Spatio-Temporal DenseNet", "comments": "Accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviors and intentions of humans are one of the main\nchallenges autonomous ground vehicles still faced with. More specifically, when\nit comes to complex environments such as urban traffic scenes, inferring the\nintentions and actions of vulnerable road users such as pedestrians become even\nharder. In this paper, we address the problem of intent action prediction of\npedestrians in urban traffic environments using only image sequences from a\nmonocular RGB camera. We propose a real-time framework that can accurately\ndetect, track and predict the intended actions of pedestrians based on a\ntracking-by-detection technique in conjunction with a novel spatio-temporal\nDenseNet model. We trained and evaluated our framework based on real data\ncollected from urban traffic environments. Our framework has shown resilient\nand competitive results in comparison to other baseline approaches. Overall, we\nachieved an average precision score of 84.76% with a real-time performance at\n20 FPS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 13:30:34 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Saleh", "Khaled", ""], ["Hossny", "Mohammed", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1904.09899", "submitter": "Tim Lyon", "authors": "Kees van Berkel and Tim Lyon", "title": "Cut-free Calculi and Relational Semantics for Temporal STIT Logics", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-19570-0_52", "report-no": null, "categories": "cs.LO cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present cut-free labelled sequent calculi for a central formalism in\nlogics of agency: STIT logics with temporal operators. These include sequent\nsystems for Ldm, Tstit and Xstit. All calculi presented possess essential\nstructural properties such as contraction- and cut-admissibility. The labelled\ncalculi G3Ldm and G3TSTIT are shown sound and complete relative to irreflexive\ntemporal frames. Additionally, we extend current results by showing that also\nXSTIT can be characterized through relational frames, omitting the use of BT+AC\nframes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 14:34:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["van Berkel", "Kees", ""], ["Lyon", "Tim", ""]]}, {"id": "1904.09981", "submitter": "Yang Gao", "authors": "Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, Yue Hu", "title": "GraphNAS: Graph Neural Architecture Search with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been popularly used for analyzing\nnon-Euclidean data such as social network data and biological data. Despite\ntheir success, the design of graph neural networks requires a lot of manual\nwork and domain knowledge. In this paper, we propose a Graph Neural\nArchitecture Search method (GraphNAS for short) that enables automatic search\nof the best graph neural architecture based on reinforcement learning.\nSpecifically, GraphNAS first uses a recurrent network to generate\nvariable-length strings that describe the architectures of graph neural\nnetworks, and then trains the recurrent network with reinforcement learning to\nmaximize the expected accuracy of the generated architectures on a validation\ndata set. Extensive experimental results on node classification tasks in both\ntransductive and inductive learning settings demonstrate that GraphNAS can\nachieve consistently better performance on the Cora, Citeseer, Pubmed citation\nnetwork, and protein-protein interaction network. On node classification tasks,\nGraphNAS can design a novel network architecture that rivals the best\nhuman-invented architecture in terms of test set accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:13:10 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 03:00:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gao", "Yang", ""], ["Yang", "Hong", ""], ["Zhang", "Peng", ""], ["Zhou", "Chuan", ""], ["Hu", "Yue", ""]]}, {"id": "1904.10066", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann and Sander G. van Dijk and Rebecca Miko and\n  Daniel Barry and George M. Evans and Alessandra Rossi and Daniel Polani", "title": "Bold Hearts Team Description for RoboCup 2019 (Humanoid Kid Size League)", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participated in the RoboCup 2018 competition in Montreal with our newly\ndeveloped BoldBot based on the Darwin-OP and mostly self-printed custom parts.\nThis paper is about the lessons learnt from that competition and further\ndevelopments for the RoboCup 2019 competition. Firstly, we briefly introduce\nthe team along with an overview of past achievements. We then present a simple,\nstandalone 2D simulator we use for simplifying the entry for new members with\nmaking basic RoboCup concepts quickly accessible. We describe our approach for\nsemantic-segmentation for our vision used in the 2018 competition, which\nreplaced the lookup-table (LUT) implementation we had before. We also discuss\nthe extra structural support we plan to add to the printed parts of the BoldBot\nand our transition to ROS 2 as our new middleware. Lastly, we will present a\ncollection of open-source contributions of our team.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:10:25 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["van Dijk", "Sander G.", ""], ["Miko", "Rebecca", ""], ["Barry", "Daniel", ""], ["Evans", "George M.", ""], ["Rossi", "Alessandra", ""], ["Polani", "Daniel", ""]]}, {"id": "1904.10079", "submitter": "William Guss", "authors": "William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru\n  Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan\n  Salakhutdinov, Nicholay Topin, Manuela Veloso, Phillip Wang", "title": "The MineRL 2019 Competition on Sample Efficient Reinforcement Learning\n  using Human Priors", "comments": "accepted at NeurIPS 2019, 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though deep reinforcement learning has led to breakthroughs in many difficult\ndomains, these successes have required an ever-increasing number of samples. As\nstate-of-the-art reinforcement learning (RL) systems require an exponentially\nincreasing number of samples, their development is restricted to a continually\nshrinking segment of the AI community. Likewise, many of these systems cannot\nbe applied to real-world problems, where environment samples are expensive.\nResolution of these limitations requires new, sample-efficient methods. To\nfacilitate research in this direction, we introduce the MineRL Competition on\nSample Efficient Reinforcement Learning using Human Priors.\n  The primary goal of the competition is to foster the development of\nalgorithms which can efficiently leverage human demonstrations to drastically\nreduce the number of samples needed to solve complex, hierarchical, and sparse\nenvironments. To that end, we introduce: (1) the Minecraft ObtainDiamond task,\na sequential decision making environment requiring long-term planning,\nhierarchical control, and efficient exploration methods; and (2) the MineRL-v0\ndataset, a large-scale collection of over 60 million state-action pairs of\nhuman demonstrations that can be resimulated into embodied trajectories with\narbitrary modifications to game state and visuals.\n  Participants will compete to develop systems which solve the ObtainDiamond\ntask with a limited number of samples from the environment simulator, Malmo.\nThe competition is structured into two rounds in which competitors are provided\nseveral paired versions of the dataset and environment with different game\ntextures. At the end of each round, competitors will submit containerized\nversions of their learning algorithms and they will then be trained/evaluated\nfrom scratch on a hold-out dataset-environment pair for a total of 4-days on a\nprespecified hardware platform.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:18:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 18:24:24 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 07:47:28 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Guss", "William H.", ""], ["Codel", "Cayden", ""], ["Hofmann", "Katja", ""], ["Houghton", "Brandon", ""], ["Kuno", "Noboru", ""], ["Milani", "Stephanie", ""], ["Mohanty", "Sharada", ""], ["Liebana", "Diego Perez", ""], ["Salakhutdinov", "Ruslan", ""], ["Topin", "Nicholay", ""], ["Veloso", "Manuela", ""], ["Wang", "Phillip", ""]]}, {"id": "1904.10098", "submitter": "Jie Chen", "authors": "Yue Yu, Jie Chen, Tian Gao, Mo Yu", "title": "DAG-GNN: DAG Structure Learning with Graph Neural Networks", "comments": "ICML2019. Code is available at\n  https://github.com/fishmoon1234/DAG-GNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a faithful directed acyclic graph (DAG) from samples of a joint\ndistribution is a challenging combinatorial problem, owing to the intractable\nsearch space superexponential in the number of graph nodes. A recent\nbreakthrough formulates the problem as a continuous optimization with a\nstructural constraint that ensures acyclicity (Zheng et al., 2018). The authors\napply the approach to the linear structural equation model (SEM) and the\nleast-squares loss function that are statistically well justified but\nnevertheless limited. Motivated by the widespread success of deep learning that\nis capable of capturing complex nonlinear mappings, in this work we propose a\ndeep generative model and apply a variant of the structural constraint to learn\nthe DAG. At the heart of the generative model is a variational autoencoder\nparameterized by a novel graph neural network architecture, which we coin\nDAG-GNN. In addition to the richer capacity, an advantage of the proposed model\nis that it naturally handles discrete variables as well as vector-valued ones.\nWe demonstrate that on synthetic data sets, the proposed method learns more\naccurate graphs for nonlinearly generated samples; and on benchmark data sets\nwith discrete variables, the learned graphs are reasonably close to the global\noptima. The code is available at \\url{https://github.com/fishmoon1234/DAG-GNN}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 23:58:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Yu", "Yue", ""], ["Chen", "Jie", ""], ["Gao", "Tian", ""], ["Yu", "Mo", ""]]}, {"id": "1904.10126", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Boon Leong Lan, Wai Yee Chan, Kwan-Hoong Ng, Maxine\n  Tan", "title": "Lung Nodule Classification using Deep Local-Global Networks", "comments": "Code and dataset available here\n  https://github.com/mundher/local-global", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Lung nodules have very diverse shapes and sizes, which makes\nclassifying them as benign/malignant a challenging problem. In this paper, we\npropose a novel method to predict the malignancy of nodules that have the\ncapability to analyze the shape and size of a nodule using a global feature\nextractor, as well as the density and structure of the nodule using a local\nfeature extractor. Methods: We propose to use Residual Blocks with a 3x3 kernel\nsize for local feature extraction, and Non-Local Blocks to extract the global\nfeatures. The Non-Local Block has the ability to extract global features\nwithout using a huge number of parameters. The key idea behind the Non-Local\nBlock is to apply matrix multiplications between features on the same feature\nmaps. Results: We trained and validated the proposed method on the LIDC-IDRI\ndataset which contains 1,018 computed tomography (CT) scans. We followed a\nrigorous procedure for experimental setup namely, 10-fold cross-validation and\nignored the nodules that had been annotated by less than 3 radiologists. The\nproposed method achieved state-of-the-art results with AUC=95.62%, while\nsignificantly outperforming other baseline methods. Conclusions: Our proposed\nDeep Local-Global network has the capability to accurately extract both local\nand global features. Our new method outperforms state-of-the-art architecture\nincluding Densenet and Resnet with transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:49:37 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Lan", "Boon Leong", ""], ["Chan", "Wai Yee", ""], ["Ng", "Kwan-Hoong", ""], ["Tan", "Maxine", ""]]}, {"id": "1904.10128", "submitter": "Peng Gao", "authors": "Peng Gao, Ruyue Yuan, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang", "title": "Siamese Attentional Keypoint Network for High Performance Visual\n  Tracking", "comments": "Accepted by Knowledge-Based SYSTEMS", "journal-ref": null, "doi": "10.1016/j.knosys.2019.105448", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the impacts of three main aspects of visual\ntracking, i.e., the backbone network, the attentional mechanism, and the\ndetection component, and propose a Siamese Attentional Keypoint Network, dubbed\nSATIN, for efficient tracking and accurate localization. Firstly, a new Siamese\nlightweight hourglass network is specially designed for visual tracking. It\ntakes advantage of the benefits of the repeated bottom-up and top-down\ninference to capture more global and local contextual information at multiple\nscales. Secondly, a novel cross-attentional module is utilized to leverage both\nchannel-wise and spatial intermediate attentional information, which can\nenhance both discriminative and localization capabilities of feature maps.\nThirdly, a keypoints detection approach is invented to trace any target object\nby detecting the top-left corner point, the centroid point, and the\nbottom-right corner point of its bounding box. Therefore, our SATIN tracker not\nonly has a strong capability to learn more effective object representations,\nbut also is computational and memory storage efficiency, either during the\ntraining or testing stages. To the best of our knowledge, we are the first to\npropose this approach. Without bells and whistles, experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nrecent benchmark datasets, at a speed far exceeding 27 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:02:34 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 03:03:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gao", "Peng", ""], ["Yuan", "Ruyue", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Fujita", "Hamido", ""], ["Zhang", "Yan", ""]]}, {"id": "1904.10239", "submitter": "Stefano Nichele", "authors": "Anders Braarud Hanssen and Stefano Nichele", "title": "Ethics of Artificial Intelligence Demarcations", "comments": "Proceedings of the Norwegian AI Symposium 2019 (NAIS 2019),\n  Trondheim, Norway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a set of key demarcations, particularly important\nwhen discussing ethical and societal issues of current AI research and\napplications. Properly distinguishing issues and concerns related to Artificial\nGeneral Intelligence and weak AI, between symbolic and connectionist AI, AI\nmethods, data and applications are prerequisites for an informed debate. Such\ndemarcations would not only facilitate much-needed discussions on ethics on\ncurrent AI technologies and research. In addition sufficiently establishing\nsuch demarcations would also enhance knowledge-sharing and support rigor in\ninterdisciplinary research between technical and social sciences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 10:41:11 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 11:59:10 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Hanssen", "Anders Braarud", ""], ["Nichele", "Stefano", ""]]}, {"id": "1904.10386", "submitter": "Mario Gleirscher", "authors": "Mario Gleirscher", "title": "Risk Structures: Towards Engineering Risk-aware Autonomous Systems", "comments": null, "journal-ref": null, "doi": "10.1007/s00165-021-00545-4", "report-no": null, "categories": "cs.SE cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by widely-used techniques of causal modelling in risk, failure, and\naccident analysis, this work discusses a compositional framework for risk\nmodelling. Risk models capture fragments of the space of risky events likely to\noccur when operating a machine in a given environment. Moreover, one can build\nsuch models into machines such as autonomous robots, to equip them with the\nability of risk-aware perception, monitoring, decision making, and control.\nWith the notion of a risk factor as the modelling primitive, the framework\nprovides several means to construct and shape risk models. Relational and\nalgebraic properties are investigated and proofs support the validity and\nconsistency of these properties over the corresponding models. Several examples\nthroughout the discussion illustrate the applicability of the concepts.\nOverall, this work focuses on the qualitative treatment of risk with the\noutlook of transferring these results to probabilistic refinements of the\ndiscussed framework.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:29:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gleirscher", "Mario", ""]]}, {"id": "1904.10396", "submitter": "Adam Santoro", "authors": "Adam Santoro, Felix Hill, David Barrett, David Raposo, Matthew\n  Botvinick, Timothy Lillicrap", "title": "Is coding a relevant metaphor for building AI? A commentary on \"Is\n  coding a relevant metaphor for the brain?\", by Romain Brette", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brette contends that the neural coding metaphor is an invalid basis for\ntheories of what the brain does. Here, we argue that it is an insufficient\nguide for building an artificial intelligence that learns to accomplish short-\nand long-term goals in a complex, changing environment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 14:58:52 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Santoro", "Adam", ""], ["Hill", "Felix", ""], ["Barrett", "David", ""], ["Raposo", "David", ""], ["Botvinick", "Matthew", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1904.10405", "submitter": "Michael Kohlhase", "authors": "Jacques Carette and William M. Farmer and Michael Kohlhase and Florian\n  Rabe", "title": "Big Math and the One-Brain Barrier A Position Paper and Architecture\n  Proposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, a class of important mathematical results have\nrequired an ever increasing amount of human effort to carry out. For some, the\nhelp of computers is now indispensable. We analyze the implications of this\ntrend towards \"big mathematics\", its relation to human cognition, and how\nmachine support for big math can be organized. The central contribution of this\nposition paper is an information model for \"doing mathematics\", which posits\nthat humans very efficiently integrate four aspects: inference, computation,\ntabulation, and narration around a well-organized core of mathematical\nknowledge. The challenge for mathematical software systems is that these four\naspects need to be integrated as well. We briefly survey the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:15:52 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 05:02:06 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Carette", "Jacques", ""], ["Farmer", "William M.", ""], ["Kohlhase", "Michael", ""], ["Rabe", "Florian", ""]]}, {"id": "1904.10503", "submitter": "Michael Sigamani", "authors": "Cihan Dogan, Aimore Dutra, Adam Gara, Alfredo Gemma, Lei Shi, Michael\n  Sigamani, Ella Walters", "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained Named Entity Recognition is a task whereby we detect and\nclassify entity mentions to a large set of types. These types can span diverse\ndomains such as finance, healthcare, and politics. We observe that when the\ntype set spans several domains the accuracy of the entity detection becomes a\nlimitation for supervised learning models. The primary reason being the lack of\ndatasets where entity boundaries are properly annotated, whilst covering a\nlarge spectrum of entity types. Furthermore, many named entity systems suffer\nwhen considering the categorization of fine grained entity types. Our work\nattempts to address these issues, in part, by combining state-of-the-art deep\nlearning models (ELMo) with an expansive knowledge base (Wikidata). Using our\nframework, we cross-validate our model on the 112 fine-grained entity types\nbased on the hierarchy given from the Wiki(gold) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:18:26 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Dogan", "Cihan", ""], ["Dutra", "Aimore", ""], ["Gara", "Adam", ""], ["Gemma", "Alfredo", ""], ["Shi", "Lei", ""], ["Sigamani", "Michael", ""], ["Walters", "Ella", ""]]}, {"id": "1904.10551", "submitter": "Arjun Pakrashi", "authors": "Arjun Pakrashi, Brian Mac Namee", "title": "CascadeML: An Automatic Neural Network Architecture Evolution and\n  Training Algorithm for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is an approach which allows a datapoint to be\nlabelled with more than one class at the same time. A common but trivial\napproach is to train individual binary classifiers per label, but the\nperformance can be improved by considering associations within the labels. Like\nwith any machine learning algorithm, hyperparameter tuning is important to\ntrain a good multi-label classifier model. The task of selecting the best\nhyperparameter settings for an algorithm is an optimisation problem. Very\nlimited work has been done on automatic hyperparameter tuning and AutoML in the\nmulti-label domain. This paper attempts to fill this gap by proposing a neural\nnetwork algorithm, CascadeML, to train multi-label neural network based on\ncascade neural networks. This method requires minimal or no hyperparameter\ntuning and also considers pairwise label associations. The cascade algorithm\ngrows the network architecture incrementally in a two phase process as it\nlearns the weights using adaptive first order gradient algorithm, therefore\nomitting the requirement of preselecting the number of hidden layers, nodes and\nthe learning rate. The method was tested on 10 multi-label datasets and\ncompared with other multi-label classification algorithms. Results show that\nCascadeML performs very well without hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:05:51 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Pakrashi", "Arjun", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1904.10552", "submitter": "Arjun Pakrashi", "authors": "Arjun Pakrashi, Brian Mac Namee", "title": "ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor\n  fusion properties of the Kalman filter", "comments": "The paper is under consideration at Information Fusion, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of ensemble classification methods in multi-class\nclassification problems, ensemble methods based on approaches other than\nbagging have not been widely explored for multi-label classification problems.\nThe Kalman Filter-based Heuristic Ensemble (KFHE) is a recent ensemble method\nthat exploits the sensor fusion properties of the Kalman filter to combine\nseveral classifier models, and that has been shown to be very effective. This\nwork proposes a multi-label version of KFHE, ML-KFHE, demonstrating the\neffectiveness of the KFHE method on multi-label datasets. Two variants are\nintroduced based on the underlying component classifier algorithm,\nML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the\nunderlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC\nsequentially trains multiple HOMER and CC multi-label classifiers and\naggregates their outputs using the sensor fusion properties of the Kalman\nfilter. Experiments and detailed analysis performed on thirteen multi-label\ndatasets and eight other algorithms, including state-of-the-art ensemble\nmethods, show that for both versions, the ML-KFHE framework improves the\nensembling process significantly with respect to bagging based combinations of\nHOMER and CC, thus demonstrating the effectiveness of ML-KFHE. Also, the\nML-KFHE-HOMER variant was found to perform consistently and significantly\nbetter than existing multi-label methods including existing approaches based on\nensembles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 22:10:50 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 20:56:54 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:49:56 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pakrashi", "Arjun", ""], ["Mac Namee", "Brian", ""]]}, {"id": "1904.10610", "submitter": "Yu-Ping Ruan", "authors": "Yu-Ping Ruan, Zhen-Hua Ling, Quan Liu, Zhigang Chen, Nitin Indurkhya", "title": "Condition-Transforming Variational AutoEncoder for Conversation Response\n  Generation", "comments": "ICASSP 2019, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model, called condition-transforming variational\nautoencoder (CTVAE), to improve the performance of conversation response\ngeneration using conditional variational autoencoders (CVAEs). In conventional\nCVAEs , the prior distribution of latent variable z follows a multivariate\nGaussian distribution with mean and variance modulated by the input conditions.\nPrevious work found that this distribution tends to become condition\nindependent in practical application. In our proposed CTVAE model, the latent\nvariable z is sampled by performing a non-lineartransformation on the\ncombination of the input conditions and the samples from a\ncondition-independent prior distribution N (0; I). In our objective\nevaluations, the CTVAE model outperforms the CVAE model on fluency metrics and\nsurpasses a sequence-to-sequence (Seq2Seq) model on diversity metrics. In\nsubjective preference tests, our proposed CTVAE model performs significantly\nbetter than CVAE and Seq2Seq models on generating fluency, informative and\ntopic relevant responses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 02:26:48 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Ruan", "Yu-Ping", ""], ["Ling", "Zhen-Hua", ""], ["Liu", "Quan", ""], ["Chen", "Zhigang", ""], ["Indurkhya", "Nitin", ""]]}, {"id": "1904.10644", "submitter": "Yu Chen", "authors": "Yu Chen and Tom Diethe and Neil Lawrence", "title": "Facilitating Bayesian Continual Learning by Natural Gradients and Stein\n  Gradients", "comments": null, "journal-ref": "Continual Learning Workshop of 32nd Conference on Neural\n  Information Processing Systems (NeurIPS 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to enable machine learning models to learn a general\nsolution space for past and future tasks in a sequential manner. Conventional\nmodels tend to forget the knowledge of previous tasks while learning a new\ntask, a phenomenon known as catastrophic forgetting. When using Bayesian models\nin continual learning, knowledge from previous tasks can be retained in two\nways: 1). posterior distributions over the parameters, containing the knowledge\ngained from inference in previous tasks, which then serve as the priors for the\nfollowing task; 2). coresets, containing knowledge of data distributions of\nprevious tasks. Here, we show that Bayesian continual learning can be\nfacilitated in terms of these two means through the use of natural gradients\nand Stein gradients respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:18:32 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Yu", ""], ["Diethe", "Tom", ""], ["Lawrence", "Neil", ""]]}, {"id": "1904.10653", "submitter": "Xu Zhu", "authors": "Xu Zhu", "title": "Stochastic Lipschitz Q-Learning", "comments": "The papers have been removed and we refer the readers to\n  arXiv:1901.09277. arXiv admin note: author list truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an episodic Markov Decision Process (MDP) problem, an online algorithm\nchooses from a set of actions in a sequence of $H$ trials, where $H$ is the\nepisode length, in order to maximize the total payoff of the chosen actions.\nQ-learning, as the most popular model-free reinforcement learning (RL)\nalgorithm, directly parameterizes and updates value functions without\nexplicitly modeling the environment. Recently, [Jin et al. 2018] studies the\nsample complexity of Q-learning with finite states and actions. Their algorithm\nachieves nearly optimal regret, which shows that Q-learning can be made sample\nefficient. However, MDPs with large discrete states and actions [Silver et al.\n2016] or continuous spaces [Mnih et al. 2013] cannot learn efficiently in this\nway. Hence, it is critical to develop new algorithms to solve this dilemma with\nprovable guarantee on the sample complexity. With this motivation, we propose a\nnovel algorithm that works for MDPs with a more general setting, which has\ninfinitely many states and actions and assumes that the payoff function and\ntransition kernel are Lipschitz continuous. We also provide corresponding\ntheory justification for our algorithm. It achieves the regret\n$\\tilde{\\mathcal{O}}(K^{\\frac{d+1}{d+2}}\\sqrt{H^3}),$ where $K$ denotes the\nnumber of episodes and $d$ denotes the dimension of the joint space. To the\nbest of our knowledge, this is the first analysis in the model-free setting\nwhose established regret matches the lower bound up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:25:42 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 00:50:11 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhu", "Xu", ""]]}, {"id": "1904.10709", "submitter": "Bin Zhao", "authors": "Bin Zhao, Xuelong Li, Xiaoqiang Lu, Zhigang Wang", "title": "A CNN-RNN Architecture for Multi-Label Weather Recognition", "comments": "One weather recognition dataset is constructed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather Recognition plays an important role in our daily lives and many\ncomputer vision applications. However, recognizing the weather conditions from\na single image remains challenging and has not been studied thoroughly.\nGenerally, most previous works treat weather recognition as a single-label\nclassification task, namely, determining whether an image belongs to a specific\nweather class or not. This treatment is not always appropriate, since more than\none weather conditions may appear simultaneously in a single image. To address\nthis problem, we make the first attempt to view weather recognition as a\nmulti-label classification task, i.e., assigning an image more than one labels\naccording to the displayed weather conditions. Specifically, a CNN-RNN based\nmulti-label classification approach is proposed in this paper. The\nconvolutional neural network (CNN) is extended with a channel-wise attention\nmodel to extract the most correlated visual features. The Recurrent Neural\nNetwork (RNN) further processes the features and excavates the dependencies\namong weather classes. Finally, the weather labels are predicted step by step.\nBesides, we construct two datasets for the weather recognition task and explore\nthe relationships among different weather conditions. Experimental results\ndemonstrate the superiority and effectiveness of the proposed approach. The new\nconstructed datasets will be available at\nhttps://github.com/wzgwzg/Multi-Label-Weather-Recognition.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 09:27:29 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhao", "Bin", ""], ["Li", "Xuelong", ""], ["Lu", "Xiaoqiang", ""], ["Wang", "Zhigang", ""]]}, {"id": "1904.10762", "submitter": "Linsen Dong", "authors": "Linsen Dong, Guanyu Gao, Xinyi Zhang, Liangyu Chen, and Yonggang Wen", "title": "Baconian: A Unified Open-source Framework for Model-Based Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Based Reinforcement Learning (MBRL) is one category of Reinforcement\nLearning (RL) algorithms which can improve sampling efficiency by modeling and\napproximating system dynamics. It has been widely adopted in the research of\nrobotics, autonomous driving, etc. Despite its popularity, there still lacks\nsome sophisticated and reusable open-source frameworks to facilitate MBRL\nresearch and experiments. To fill this gap, we develop a flexible and\nmodularized framework, Baconian, which allows researchers to easily implement a\nMBRL testbed by customizing or building upon our provided modules and\nalgorithms. Our framework can free users from re-implementing popular MBRL\nalgorithms from scratch thus greatly save users' efforts on MBRL experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 05:35:50 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 10:55:55 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 15:47:28 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 03:40:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dong", "Linsen", ""], ["Gao", "Guanyu", ""], ["Zhang", "Xinyi", ""], ["Chen", "Liangyu", ""], ["Wen", "Yonggang", ""]]}, {"id": "1904.10788", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Seokhyun Byun, Subhadeep Dey, Kyomin Jung", "title": "Speech Emotion Recognition Using Multi-hop Attention Mechanism", "comments": "5 pages, Accepted as a conference paper at ICASSP 2019 (oral\n  presentation)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683483", "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in exploiting textual and acoustic data of\nan utterance for the speech emotion classification task. The baseline approach\nmodels the information from audio and text independently using two deep neural\nnetworks (DNNs). The outputs from both the DNNs are then fused for\nclassification. As opposed to using knowledge from both the modalities\nseparately, we propose a framework to exploit acoustic information in tandem\nwith lexical data. The proposed framework uses two bi-directional long\nshort-term memory (BLSTM) for obtaining hidden representations of the\nutterance. Furthermore, we propose an attention mechanism, referred to as the\nmulti-hop, which is trained to automatically infer the correlation between the\nmodalities. The multi-hop attention first computes the relevant segments of the\ntextual data corresponding to the audio signal. The relevant textual data is\nthen applied to attend parts of the audio signal. To evaluate the performance\nof the proposed system, experiments are performed in the IEMOCAP dataset.\nExperimental results show that the proposed technique outperforms the\nstate-of-the-art system by 6.5% relative improvement in terms of weighted\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 13:09:21 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:34:00 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Byun", "Seokhyun", ""], ["Dey", "Subhadeep", ""], ["Jung", "Kyomin", ""]]}, {"id": "1904.10820", "submitter": "Lisa Beinborn", "authors": "Lisa Beinborn and Rochelle Choenni", "title": "Semantic Drift in Multilingual Representations", "comments": "Almost final version. Paper will appear in the Computational\n  Linguistics Journal, Volume 46, Issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilingual representations have mostly been evaluated based on their\nperformance on specific tasks. In this article, we look beyond engineering\ngoals and analyze the relations between languages in computational\nrepresentations. We introduce a methodology for comparing languages based on\ntheir organization of semantic concepts. We propose to conduct an adapted\nversion of representational similarity analysis of a selected set of concepts\nin computational multilingual representations. Using this analysis method, we\ncan reconstruct a phylogenetic tree that closely resembles those assumed by\nlinguistic experts. These results indicate that multilingual distributional\nrepresentations which are only trained on monolingual text and bilingual\ndictionaries preserve relations between languages without the need for any\netymological information. In addition, we propose a measure to identify\nsemantic drift between language families. We perform experiments on word-based\nand sentence-based multilingual models and provide both quantitative results\nand qualitative examples. Analyses of semantic drift in multilingual\nrepresentations can serve two purposes: they can indicate unwanted\ncharacteristics of the computational models and they provide a quantitative\nmeans to study linguistic phenomena across languages. The code is available at\nhttps://github.com/beinborn/SemanticDrift.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:55:42 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 09:03:43 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 13:31:59 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 19:48:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Beinborn", "Lisa", ""], ["Choenni", "Rochelle", ""]]}, {"id": "1904.10876", "submitter": "Valerio Lorini", "authors": "V. Lorini (European Commission, Joint Research Centre (JRC), Ispra,\n  Italy, Universitat Pompeu Fabra, Barcelona, Spain), C. Castillo (Universitat\n  Pompeu Fabra, Barcelona, Spain), F. Dottori (European Commission, Joint\n  Research Centre (JRC), Ispra, Italy), M. Kalas (KAJO, Bytca, Slovakia), D.\n  Nappo (European Commission, Joint Research Centre (JRC), Ispra, Italy), P.\n  Salamon (European Commission, Joint Research Centre (JRC), Ispra, Italy)", "title": "Integrating Social Media into a Pan-European Flood Awareness System: A\n  Multilingual Approach", "comments": "accepted at ISCRAM2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a prototype system that integrates social media analysis\ninto the European Flood Awareness System (EFAS). This integration allows the\ncollection of social media data to be automatically triggered by flood risk\nwarnings determined by a hydro-meteorological model. Then, we adopt a\nmulti-lingual approach to find flood-related messages by employing two\nstate-of-the-art methodologies: language-agnostic word embeddings and\nlanguage-aligned word embeddings. Both approaches can be used to bootstrap a\nclassifier of social media messages for a new language with little or no\nlabeled data. Finally, we describe a method for selecting relevant and\nrepresentative messages and displaying them back in the interface of EFAS.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:40:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Lorini", "V.", "", "European Commission, Joint Research Centre"], ["Castillo", "C.", "", "Universitat\n  Pompeu Fabra, Barcelona, Spain"], ["Dottori", "F.", "", "European Commission, Joint\n  Research Centre"], ["Kalas", "M.", "", "KAJO, Bytca, Slovakia"], ["Nappo", "D.", "", "European Commission, Joint Research Centre"], ["Salamon", "P.", "", "European Commission, Joint Research Centre"]]}, {"id": "1904.10960", "submitter": "Yasuhiko Tachibana", "authors": "Yasuhiko Tachibana (1 and 2), Akifumi Hagiwara (2 and 3), Masaaki Hori\n  (2), Jeff Kershaw (1), Misaki Nakazawa (2), Tokuhiko Omatsu (1), Riwa\n  Kishimoto (1), Kazumasa Yokoyama (4), Nobutaka Hattori (4), Shigeki Aoki (2),\n  Tatsuya Higashi (5), and Takayuki Obata (1 and 5) ((1) Applied MRI Research,\n  Department of Molecular imaging and Theranostics, National Institute of\n  Radiological Sciences, QST, (2) Department of Radiology, Juntendo University\n  School of Medicine, (3) Department of Radiology, Graduate School of Medicine,\n  The University of Tokyo, (4) Department of Neurology, Juntendo University\n  School of Medicine, (5) Department of Molecular imaging and Theranostics,\n  National Institute of Radiological Sciences, QST)", "title": "The utility of a convolutional neural network for generating a myelin\n  volume index map from rapid simultaneous relaxometry imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Purpose: A current algorithm to obtain a synthetic myelin\nvolume fraction map (SyMVF) from rapid simultaneous relaxometry imaging (RSRI)\nhas a potential problem, that it does not incorporate information from\nsurrounding pixels. The purpose of this study was to develop a method that\nutilizes a convolutional neural network (CNN) to overcome this problem.\nMethods: RSRI and magnetization transfer images from 20 healthy volunteers were\nincluded. A CNN was trained to reconstruct RSRI-related metric maps into a\nmyelin volume-related index (generated myelin volume index: GenMVI) map using\nthe myelin volume index map calculated from magnetization transfer images\n(MTMVI) as reference. The SyMVF and GenMVI maps were statistically compared by\ntesting how well they correlated with the MTMVI map. The correlations were\nevaluated based on: (i) averaged values obtained from 164 atlas-based ROIs, and\n(ii) pixel-based comparison for ROIs defined in four different tissue types\n(cortical and subcortical gray matter, white matter, and whole brain). Results:\nFor atlas-based ROIs, the overall correlation with the MTMVI map was higher for\nthe GenMVI map than for the SyMVF map. In the pixel-based comparison,\ncorrelation with the MTMVI map was stronger for the GenMVI map than for the\nSyMVF map, and the difference in the distribution for the volunteers was\nsignificant (Wilcoxon sign-rank test, P<.001) in all tissue types. Conclusion:\nThe proposed method is useful, as it can incorporate more specific information\nabout local tissue properties than the existing method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 06:50:22 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tachibana", "Yasuhiko", "", "1 and 2"], ["Hagiwara", "Akifumi", "", "2 and 3"], ["Hori", "Masaaki", "", "1 and 5"], ["Kershaw", "Jeff", "", "1 and 5"], ["Nakazawa", "Misaki", "", "1 and 5"], ["Omatsu", "Tokuhiko", "", "1 and 5"], ["Kishimoto", "Riwa", "", "1 and 5"], ["Yokoyama", "Kazumasa", "", "1 and 5"], ["Hattori", "Nobutaka", "", "1 and 5"], ["Aoki", "Shigeki", "", "1 and 5"], ["Higashi", "Tatsuya", "", "1 and 5"], ["Obata", "Takayuki", "", "1 and 5"]]}, {"id": "1904.11017", "submitter": "Mohd Hafiz Hasan", "authors": "Mohd Hafiz Hasan and Pascal Van Hentenryck and Antoine Legrain", "title": "The Commute Trip Sharing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parking pressure has been steadily increasing in cities as well as in\nuniversity and corporate campuses. To relieve this pressure, this paper studies\na car-pooling platform that would match riders and drivers, while guaranteeing\na ride back and exploiting spatial and temporal locality. In particular, the\npaper formalizes the Commute Trip Sharing Problem (CTSP) to find a routing plan\nthat maximizes ride sharing for a set of commute trips. The CTSP is a\ngeneralization of the vehicle routing problem with routes that satisfy time\nwindow, capacity, pairing, precedence, ride duration, and driver constraints.\nThe paper introduces two exact algorithms for the CTPS: A route-enumeration\nalgorithm and a branch-and-price algorithm. Experimental results show that, on\na high-fidelity, real-world dataset of commute trips from a mid-size city, both\nalgorithms optimally solve small and medium-sized problems and produce\nhigh-quality solutions for larger problem instances. The results show that car\npooling, if widely adopted, has the potential to reduce vehicle usage by up to\n57% and decrease vehicle miles traveled by up to 46% while only incurring a 22%\nincrease in average ride time per commuter for the trips considered.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 18:59:01 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 14:06:30 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Hasan", "Mohd Hafiz", ""], ["Van Hentenryck", "Pascal", ""], ["Legrain", "Antoine", ""]]}, {"id": "1904.11099", "submitter": "Daniel Huang", "authors": "Daniel Huang", "title": "On Learning to Prove", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of learning a first-order theorem\nprover that uses a representation of beliefs in mathematical claims to\nconstruct proofs. The inspiration for doing so comes from the practices of\nhuman mathematicians where \"plausible reasoning\" is applied in addition to\ndeductive reasoning to find proofs.\n  Towards this end, we introduce a representation of beliefs that assigns\nprobabilities to the exhaustive and mutually exclusive first-order\npossibilities found in Hintikka's theory of distributive normal forms. The\nrepresentation supports Bayesian update, induces a distribution on statements\nthat does not enforce that logically equivalent statements are assigned the\nsame probability, and suggests an embedding of statements into an associated\nHilbert space.\n  We then examine conjecturing as model selection and an alternating-turn game\nof determining consistency. The game is amenable (in principle) to self-play\ntraining to learn beliefs and derive a prover that is complete when logical\nomniscience is attained and sound when beliefs are reasonable. The\nrepresentation has super-exponential space requirements as a function of\nquantifier depth so the ideas in this paper should be taken as theoretical. We\nwill comment on how abstractions can be used to control the space requirements\nat the cost of completeness.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 23:54:59 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 16:55:51 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 05:57:10 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Huang", "Daniel", ""]]}, {"id": "1904.11102", "submitter": "Ahmed Qureshi", "authors": "Mayur J. Bency, Ahmed H. Qureshi, Michael C. Yip", "title": "Neural Path Planning: Fixed Time, Near-Optimal Path Generation via\n  Oracle Imitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and efficient path generation is critical for robots operating in\ncomplex environments. This motion planning problem is often performed in a\nrobot's actuation or configuration space, where popular pathfinding methods\nsuch as A*, RRT*, get exponentially more computationally expensive to execute\nas the dimensionality increases or the spaces become more cluttered and\ncomplex. On the other hand, if one were to save the entire set of paths\nconnecting all pair of locations in the configuration space a priori, one would\nrun out of memory very quickly. In this work, we introduce a novel way of\nproducing fast and optimal motion plans for static environments by using a\nstepping neural network approach, called OracleNet. OracleNet uses Recurrent\nNeural Networks to determine end-to-end trajectories in an iterative manner\nthat implicitly generates optimal motion plans with minimal loss in performance\nin a compact form. The algorithm is straightforward in implementation while\nconsistently generating near-optimal paths in a single, iterative, end-to-end\nroll-out. In practice, OracleNet generally has fixed-time execution regardless\nof the configuration space complexity while outperforming popular pathfinding\nalgorithms in complex environments and higher dimensions\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 00:05:24 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Bency", "Mayur J.", ""], ["Qureshi", "Ahmed H.", ""], ["Yip", "Michael C.", ""]]}, {"id": "1904.11106", "submitter": "Solimul Chowdhury", "authors": "Md Solimul Chowdhury and Martin M\\\"uller and Jia-Huai You", "title": "Characterization of Glue Variables in CDCL SAT Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A state-of-the-art criterion to evaluate the importance of a given learned\nclause is called Literal Block Distance (LBD) score. It measures the number of\ndistinct decision levels in a given learned clause. The lower the LBD score of\na learned clause, the better is its quality. The learned clauses with LBD score\nof 2, called glue clauses, are known to possess high pruning power which are\nnever deleted from the clause databases of the modern CDCL SAT solvers. In this\nwork, we relate glue clauses to decision variables. We call the variables that\nappeared in at least one glue clause up to the current search state Glue\nVariables. We first show experimentally, by running the state-of-the-art CDCL\nSAT solver MapleLCMDist on benchmarks from SAT Competition-2017 and 2018, that\nbranching decisions with glue variables are categorically more inference and\nconflict efficient than nonglue variables. Based on this observation, we\ndevelop a structure aware CDCL variable bumping scheme, which bumps the\nactivity score of a glue variable based on its appearance count in the glue\nclauses that are learned so far by the search. Empirical evaluation shows\neffectiveness of the new method over the main track instances from SAT\nCompetition 2017 and 2018.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 00:52:06 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Chowdhury", "Md Solimul", ""], ["M\u00fcller", "Martin", ""], ["You", "Jia-Huai", ""]]}, {"id": "1904.11115", "submitter": "Daniel Lopez-Martinez", "authors": "Daniel Lopez-Martinez and Patrick Eschenfeldt and Sassan Ostvar and\n  Myles Ingram and Chin Hur and Rosalind Picard", "title": "Deep Reinforcement Learning for Optimal Critical Care Pain Management\n  with Morphine using Dueling Double-Deep Q Networks", "comments": "2019 41st Annual International Conference of the IEEE Engineering in\n  Medicine & Biology Society (EMBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opioids are the preferred medications for the treatment of pain in the\nintensive care unit. While undertreatment leads to unrelieved pain and poor\nclinical outcomes, excessive use of opioids puts patients at risk of\nexperiencing multiple adverse effects. In this work, we present a sequential\ndecision making framework for opioid dosing based on deep reinforcement\nlearning. It provides real-time clinically interpretable dosing\nrecommendations, personalized according to each patient's evolving pain and\nphysiological condition. We focus on morphine, one of the most commonly\nprescribed opioids. To train and evaluate the model, we used retrospective data\nfrom the publicly available MIMIC-3 database. Our results demonstrate that\nreinforcement learning may be used to aid decision making in the intensive care\nsetting by providing personalized pain management interventions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 01:26:24 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Lopez-Martinez", "Daniel", ""], ["Eschenfeldt", "Patrick", ""], ["Ostvar", "Sassan", ""], ["Ingram", "Myles", ""], ["Hur", "Chin", ""], ["Picard", "Rosalind", ""]]}, {"id": "1904.11128", "submitter": "Yunxiang Zhao", "authors": "Yunxiang Zhao, Jianzhong Qi, Rui Zhang", "title": "CBHE: Corner-based Building Height Estimation for Complex Street Scene\n  Images", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313394", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building height estimation is important in many applications such as 3D city\nreconstruction, urban planning, and navigation. Recently, a new building height\nestimation method using street scene images and 2D maps was proposed. This\nmethod is more scalable than traditional methods that use high-resolution\noptical data, LiDAR data, or RADAR data which are expensive to obtain. The\nmethod needs to detect building rooflines and then compute building height via\nthe pinhole camera model. We observe that this method has limitations in\nhandling complex street scene images in which buildings overlap with each other\nand the rooflines are difficult to locate. We propose CBHE, a building height\nestimation algorithm considering both building corners and rooflines. CBHE\nfirst obtains building corner and roofline candidates in street scene images\nbased on building footprints from 2D maps and the camera parameters. Then, we\nuse a deep neural network named BuildingNet to classify and filter corner and\nroofline candidates. Based on the valid corners and rooflines from BuildingNet,\nCBHE computes building height via the pinhole camera model. Experimental\nresults show that the proposed BuildingNet yields a higher accuracy on building\ncorner and roofline candidate filtering compared with the state-of-the-art open\nset classifiers. Meanwhile, CBHE outperforms the baseline algorithm by over 10%\nin building height estimation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:30:50 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhao", "Yunxiang", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""]]}, {"id": "1904.11131", "submitter": "Xu Zhu", "authors": "Xu Zhu", "title": "Lipschitz Bandit Optimization with Improved Efficiency", "comments": "The papers have been removed and we refer the readers to\n  arXiv:1901.09277. arXiv admin note: author list truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Lipschitz bandit optimization problem with an emphasis on\npractical efficiency. Although there is rich literature on regret analysis of\nthis type of problem, e.g., [Kleinberg et al. 2008, Bubeck et al. 2011,\nSlivkins 2014], their proposed algorithms suffer from serious practical\nproblems including extreme time complexity and dependence on oracle\nimplementations. With this motivation, we propose a novel algorithm with an\nUpper Confidence Bound (UCB) exploration, namely Tree UCB-Hoeffding, using\nadaptive partitions. Our partitioning scheme is easy to implement and does not\nrequire any oracle settings. With a tree-based search strategy, the total\ncomputational cost can be improved to $\\mathcal{O}(T\\log T)$ for the first $T$\niterations. In addition, our algorithm achieves the regret lower bound up to a\nlogarithmic factor.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 02:38:33 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 00:49:54 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhu", "Xu", ""]]}, {"id": "1904.11157", "submitter": "Rohit Jena", "authors": "Rohit Jena", "title": "Out of the Box: A combined approach for handling occlusion in Human Pose\n  Estimation", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human Pose estimation is a challenging problem, especially in the case of 3D\npose estimation from 2D images due to many different factors like occlusion,\ndepth ambiguities, intertwining of people, and in general crowds. 2D\nmulti-person human pose estimation in the wild also suffers from the same\nproblems - occlusion, ambiguities, and disentanglement of people's body parts.\nBeing a fundamental problem with loads of applications, including but not\nlimited to surveillance, economical motion capture for video games and movies,\nand physiotherapy, this is an interesting problem to be solved both from a\npractical perspective and from an intellectual perspective as well. Although\nthere are cases where no pose estimation can ever predict with 100% accuracy\n(cases where even humans would fail), there are several algorithms that have\nbrought new state-of-the-art performance in human pose estimation in the wild.\nWe look at a few algorithms with different approaches and also formulate our\nown approach to tackle a consistently bugging problem, i.e. occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 05:10:18 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jena", "Rohit", ""]]}, {"id": "1904.11187", "submitter": "Parth Shah", "authors": "Parth Shah, Vishvajit Bakrola, Supriya Pati", "title": "Optimal Approach for Image Recognition using Deep Convolutional\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent time deep learning has achieved huge popularity due to its\nperformance in various machine learning algorithms. Deep learning as\nhierarchical or structured learning attempts to model high level abstractions\nin data by using a group of processing layers. The foundation of deep learning\narchitectures is inspired by the understanding of information processing and\nneural responses in human brain. The architectures are created by stacking\nmultiple linear or non-linear operations. The article mainly focuses on the\nstate-of-art deep learning models and various real world applications specific\ntraining methods. Selecting optimal architecture for specific problem is a\nchallenging task, at a closing stage of the article we proposed optimal\napproach to deep convolutional architecture for the application of image\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 07:40:30 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Shah", "Parth", ""], ["Bakrola", "Vishvajit", ""], ["Pati", "Supriya", ""]]}, {"id": "1904.11223", "submitter": "Matteo Manica", "authors": "Matteo Manica, Ali Oskooei, Jannis Born, Vigneshwari Subramanian,\n  Julio S\\'aez-Rodr\\'iguez, Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "Towards Explainable Anticancer Compound Sensitivity Prediction via\n  Multimodal Attention-based Convolutional Encoders", "comments": "11 pages, 5 figures, 1 table, Workshop on Computational Biology at\n  the International Conference on Machine Learning (ICML), Long Beach, CA, 2019", "journal-ref": "Mol. Pharmaceutics 2019", "doi": "10.1021/acs.molpharmaceut.9b00520", "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In line with recent advances in neural drug design and sensitivity\nprediction, we propose a novel architecture for interpretable prediction of\nanticancer compound sensitivity using a multimodal attention-based\nconvolutional encoder. Our model is based on the three key pillars of drug\nsensitivity: compounds' structure in the form of a SMILES sequence, gene\nexpression profiles of tumors and prior knowledge on intracellular interactions\nfrom protein-protein interaction networks. We demonstrate that our multiscale\nconvolutional attention-based (MCA) encoder significantly outperforms a\nbaseline model trained on Morgan fingerprints, a selection of encoders based on\nSMILES as well as previously reported state of the art for multimodal drug\nsensitivity prediction (R2 = 0.86 and RMSE = 0.89). Moreover, the\nexplainability of our approach is demonstrated by a thorough analysis of the\nattention weights. We show that the attended genes significantly enrich\napoptotic processes and that the drug attention is strongly correlated with a\nstandard chemical structure similarity index. Finally, we report a case study\nof two receptor tyrosine kinase (RTK) inhibitors acting on a leukemia cell\nline, showcasing the ability of the model to focus on informative genes and\nsubmolecular regions of the two compounds. The demonstrated generalizability\nand the interpretability of our model testify its potential for in-silico\nprediction of anticancer compound efficacy on unseen cancer cells, positioning\nit as a valid solution for the development of personalized therapies as well as\nfor the evaluation of candidate compounds in de novo drug design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 09:14:52 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 20:03:13 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 14:00:55 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Manica", "Matteo", ""], ["Oskooei", "Ali", ""], ["Born", "Jannis", ""], ["Subramanian", "Vigneshwari", ""], ["S\u00e1ez-Rodr\u00edguez", "Julio", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "1904.11272", "submitter": "Guanzhi Wang", "authors": "Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang", "title": "LADN: Local Adversarial Disentangling Network for Facial Makeup and\n  De-Makeup", "comments": "Qiao and Guanzhi have equal contribution. Accepted to ICCV 2019.\n  Project website: https://georgegu1997.github.io/LADN-project-page/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a local adversarial disentangling network (LADN) for facial makeup\nand de-makeup. Central to our method are multiple and overlapping local\nadversarial discriminators in a content-style disentangling network for\nachieving local detail transfer between facial images, with the use of\nasymmetric loss functions for dramatic makeup styles with high-frequency\ndetails. Existing techniques do not demonstrate or fail to transfer\nhigh-frequency details in a global adversarial setting, or train a single local\ndiscriminator only to ensure image structure consistency and thus work only for\nrelatively simple styles. Unlike others, our proposed local adversarial\ndiscriminators can distinguish whether the generated local image details are\nconsistent with the corresponding regions in the given reference image in\ncross-image style transfer in an unsupervised setting. Incorporating these\ntechnical contributions, we achieve not only state-of-the-art results on\nconventional styles but also novel results involving complex and dramatic\nstyles with high-frequency details covering large areas across multiple facial\nfeatures. A carefully designed dataset of unpaired before and after makeup\nimages is released.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:52:06 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 07:31:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Gu", "Qiao", ""], ["Wang", "Guanzhi", ""], ["Chiu", "Mang Tik", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1904.11366", "submitter": "Yu Ye", "authors": "Yu Ye, Ming Xiao, Mikael Skoglund", "title": "Decentralized Multi-Task Learning Based on Extreme Learning Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-task learning (MTL), related tasks learn jointly to improve\ngeneralization performance. To exploit the high learning speed of extreme\nlearning machines (ELMs), we apply the ELM framework to the MTL problem, where\nthe output weights of ELMs for all the tasks are learned collaboratively. We\nfirst present the ELM based MTL problem in the centralized setting, which is\nsolved by the proposed MTL-ELM algorithm. Due to the fact that many data sets\nof different tasks are geo-distributed, decentralized machine learning is\nstudied. We formulate the decentralized MTL problem based on ELM as majorized\nmulti-block optimization with coupled bi-convex objective functions. To solve\nthe problem, we propose the DMTL-ELM algorithm, which is a hybrid Jacobian and\nGauss-Seidel Proximal multi-block alternating direction method of multipliers\n(ADMM). Further, to reduce the computation load of DMTL-ELM, DMTL-ELM with\nfirst-order approximation (FO-DMTL-ELM) is presented. Theoretical analysis\nshows that the convergence to the stationary point of DMTL-ELM and FO-DMTL-ELM\ncan be guaranteed conditionally. Through simulations, we demonstrate the\nconvergence of proposed MTL-ELM, DMTL-ELM, and FO-DMTL-ELM algorithms, and also\nshow that they can outperform existing MTL methods. Moreover, by adjusting the\ndimension of hidden feature space, there exists a trade-off between\ncommunication load and learning accuracy for DMTL-ELM.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 14:19:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Ye", "Yu", ""], ["Xiao", "Ming", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1904.11412", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil", "title": "Assistive System in Conversational Agent for Health Coaching: The\n  CoachAI Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With increasing physicians' workload and patients' needs for care, there is a\nneed for technology that facilitates physicians work and performs continues\nfollow-up with patients. Existing approaches focus merely on improving\npatient's condition, and none have considered managing physician's workload.\nThis paper presents an initial evaluation of a conversational agent assisted\ncoaching platform intended to manage physicians' fatigue and provide continuous\nfollow-up to patients. We highlight the approach adapted to build the chatbot\ndialogue and the coaching platform. We will particularly discuss the activity\nrecommender algorithms used to suggest insights about patients' condition and\nactivities based on previously collected data. The paper makes three\ncontributions: (1) present the conversational agent as an assistive virtual\ncoach, (2) decrease physicians workload and continuous follow up with patients,\nall by handling some repetitive physician tasks and performing initial follow\nup with the patient, (3) present the activity recommender that tracks previous\nactivities and patient information and provides useful insights about possible\nactivity and patient match to the coach. Future work focuses on integrating the\nrecommender model with the CoachAI platform and test the prototype with\npatient's in collaboration with an ambulatory clinic.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:39:49 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1904.11439", "submitter": "Mingde Zhao", "authors": "Mingde Zhao, Sitao Luan, Ian Porada, Xiao-Wen Chang, and Doina Precup", "title": "META-Learning State-based Eligibility Traces for More Sample-Efficient\n  Policy Evaluation", "comments": "Accepted by AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal-Difference (TD) learning is a standard and very successful\nreinforcement learning approach, at the core of both algorithms that learn the\nvalue of a given policy, as well as algorithms which learn how to improve\npolicies. TD-learning with eligibility traces provides a way to boost sample\nefficiency by temporal credit assignment, i.e. deciding which portion of a\nreward should be assigned to predecessor states that occurred at different\nprevious times, controlled by a parameter $\\lambda$. However, tuning this\nparameter can be time-consuming, and not tuning it can lead to inefficient\nlearning. For better sample efficiency of TD-learning, we propose a\nmeta-learning method for adjusting the eligibility trace parameter, in a\nstate-dependent manner. The adaptation is achieved with the help of auxiliary\nlearners that learn distributional information about the update targets online,\nincurring roughly the same computational complexity per step as the usual value\nlearner. Our approach can be used both in on-policy and off-policy learning. We\nprove that, under some assumptions, the proposed method improves the overall\nquality of the update targets, by minimizing the overall target error. This\nmethod can be viewed as a plugin to assist prediction with function\napproximation by meta-learning feature (observation)-based $\\lambda$ online, or\neven in the control case to assist policy improvement. Our empirical evaluation\ndemonstrates significant performance improvements, as well as improved\nrobustness of the proposed algorithm to learning rate variation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:32:21 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 22:49:24 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 01:52:16 GMT"}, {"version": "v4", "created": "Fri, 18 Oct 2019 03:19:57 GMT"}, {"version": "v5", "created": "Thu, 27 Feb 2020 03:49:03 GMT"}, {"version": "v6", "created": "Sat, 16 May 2020 18:15:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhao", "Mingde", ""], ["Luan", "Sitao", ""], ["Porada", "Ian", ""], ["Chang", "Xiao-Wen", ""], ["Precup", "Doina", ""]]}, {"id": "1904.11454", "submitter": "Murat Cubuktepe", "authors": "Bo Wu, Murat Cubuktepe, Suda Bharadwaj, Ufuk Topcu", "title": "Reward-Based Deception with Cognitive Bias", "comments": "Submitted to CDC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deception plays a key role in adversarial or strategic interactions for the\npurpose of self-defence and survival. This paper introduces a general framework\nand solution to address deception. Most existing approaches for deception\nconsider obfuscating crucial information to rational adversaries with abundant\nmemory and computation resources. In this paper, we consider deceiving\nadversaries with bounded rationality and in terms of expected rewards. This\nproblem is commonly encountered in many applications especially involving human\nadversaries. Leveraging the cognitive bias of humans in reward evaluation under\nstochastic outcomes, we introduce a framework to optimally assign resources of\na limited quantity to optimally defend against human adversaries. Modeling such\ncognitive biases follows the so-called prospect theory from behavioral\npsychology literature. Then we formulate the resource allocation problem as a\nsignomial program to minimize the defender's cost in an environment modeled as\na Markov decision process. We use police patrol hour assignment as an\nillustrative example and provide detailed simulation results based on\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:51:24 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Wu", "Bo", ""], ["Cubuktepe", "Murat", ""], ["Bharadwaj", "Suda", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1904.11455", "submitter": "Tom Schaul", "authors": "Tom Schaul, Diana Borsa, Joseph Modayil, Razvan Pascanu", "title": "Ray Interference: a Source of Plateaus in Deep Reinforcement Learning", "comments": "Full version of RLDM abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than proposing a new method, this paper investigates an issue present\nin existing learning algorithms. We study the learning dynamics of\nreinforcement learning (RL), specifically a characteristic coupling between\nlearning and data generation that arises because RL agents control their future\ndata distribution. In the presence of function approximation, this coupling can\nlead to a problematic type of 'ray interference', characterized by learning\ndynamics that sequentially traverse a number of performance plateaus,\neffectively constraining the agent to learn one thing at a time even when\nlearning in parallel is better. We establish the conditions under which ray\ninterference occurs, show its relation to saddle points and obtain the exact\nlearning dynamics in a restricted setting. We characterize a number of its\nproperties and discuss possible remedies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:54:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Schaul", "Tom", ""], ["Borsa", "Diana", ""], ["Modayil", "Joseph", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1904.11475", "submitter": "Ilya Gusev", "authors": "Ilya Gusev", "title": "Importance of Copying Mechanism for News Headline Generation", "comments": null, "journal-ref": "Computational Linguistics and Intellectual Technologies, Papers\n  from the Annual International Conference \"Dialogue\" (2019) Issue 18, 229-236", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News headline generation is an essential problem of text summarization\nbecause it is constrained, well-defined, and is still hard to solve. Models\nwith a limited vocabulary can not solve it well, as new named entities can\nappear regularly in the news and these entities often should be in the\nheadline. News articles in morphologically rich languages such as Russian\nrequire model modifications due to a large number of possible word forms. This\nstudy aims to validate that models with a possibility of copying words from the\noriginal article performs better than models without such an option. The\nproposed model achieves a mean ROUGE score of 23 on the provided test dataset,\nwhich is 8 points greater than the result of a similar model without a copying\nmechanism. Moreover, the resulting model performs better than any known model\non the new dataset of Russian news.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:39:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gusev", "Ilya", ""]]}, {"id": "1904.11483", "submitter": "Maxime Bouton", "authors": "Maxime Bouton, Alireza Nakhaei, Kikuo Fujimura, Mykel J. Kochenderfer", "title": "Safe Reinforcement Learning with Scene Decomposition for Navigating\n  Complex Urban Environments", "comments": "8 pages; 7 figures", "journal-ref": "IEEE Intelligent Vehicles Symposium (IV), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating urban environments represents a complex task for automated\nvehicles. They must reach their goal safely and efficiently while considering a\nmultitude of traffic participants. We propose a modular decision making\nalgorithm to autonomously navigate intersections, addressing challenges of\nexisting rule-based and reinforcement learning (RL) approaches. We first\npresent a safe RL algorithm relying on a model-checker to ensure safety\nguarantees. To make the decision strategy robust to perception errors and\nocclusions, we introduce a belief update technique using a learning based\napproach. Finally, we use a scene decomposition approach to scale our algorithm\nto environments with multiple traffic participants. We empirically demonstrate\nthat our algorithm outperforms rule-based methods and reinforcement learning\ntechniques on a complex intersection scenario.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:52:28 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Bouton", "Maxime", ""], ["Nakhaei", "Alireza", ""], ["Fujimura", "Kikuo", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1904.11491", "submitter": "Han Hu", "authors": "Han Hu and Zheng Zhang and Zhenda Xie and Stephen Lin", "title": "Local Relation Networks for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolution layer has been the dominant feature extractor in computer\nvision for years. However, the spatial aggregation in convolution is basically\na pattern matching process that applies fixed filters which are inefficient at\nmodeling visual elements with varying spatial distributions. This paper\npresents a new image feature extractor, called the local relation layer, that\nadaptively determines aggregation weights based on the compositional\nrelationship of local pixel pairs. With this relational approach, it can\ncomposite visual elements into higher-level entities in a more efficient manner\nthat benefits semantic inference. A network built with local relation layers,\ncalled the Local Relation Network (LR-Net), is found to provide greater\nmodeling capacity than its counterpart built with regular convolution on\nlarge-scale recognition tasks such as ImageNet classification.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:35 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hu", "Han", ""], ["Zhang", "Zheng", ""], ["Xie", "Zhenda", ""], ["Lin", "Stephen", ""]]}, {"id": "1904.11492", "submitter": "Yue Cao", "authors": "Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu", "title": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Non-Local Network (NLNet) presents a pioneering approach for capturing\nlong-range dependencies, via aggregating query-specific global context to each\nquery position. However, through a rigorous empirical analysis, we have found\nthat the global contexts modeled by non-local network are almost the same for\ndifferent query positions within an image. In this paper, we take advantage of\nthis finding to create a simplified network based on a query-independent\nformulation, which maintains the accuracy of NLNet but with significantly less\ncomputation. We further observe that this simplified design shares similar\nstructure with Squeeze-Excitation Network (SENet). Hence we unify them into a\nthree-step general framework for global context modeling. Within the general\nframework, we design a better instantiation, called the global context (GC)\nblock, which is lightweight and can effectively model the global context. The\nlightweight property allows us to apply it for multiple layers in a backbone\nnetwork to construct a global context network (GCNet), which generally\noutperforms both simplified NLNet and SENet on major benchmarks for various\nrecognition tasks. The code and configurations are released at\nhttps://github.com/xvjiarui/GCNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 17:59:42 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Yue", ""], ["Xu", "Jiarui", ""], ["Lin", "Stephen", ""], ["Wei", "Fangyun", ""], ["Hu", "Han", ""]]}, {"id": "1904.11566", "submitter": "Maxime Bouton", "authors": "Markus Schratter, Maxime Bouton, Mykel J. Kochenderfer, Daniel\n  Watzenig", "title": "Pedestrian Collision Avoidance System for Scenarios with Occlusions", "comments": "7 pages; 8 figures", "journal-ref": "IEEE Intelligent Vehicles Symposium (IV), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe autonomous driving in urban areas requires robust algorithms to avoid\ncollisions with other traffic participants with limited perception ability.\nCurrent deployed approaches relying on Autonomous Emergency Braking (AEB)\nsystems are often overly conservative. In this work, we formulate the problem\nas a partially observable Markov decision process (POMDP), to derive a policy\nrobust to uncertainty in the pedestrian location. We investigate how to\nintegrate such a policy with an AEB system that operates only when a collision\nis unavoidable. In addition, we propose a rigorous evaluation methodology on a\nset of well defined scenarios. We show that combining the two approaches\nprovides a robust autonomous braking system that reduces unnecessary braking\ncaused by using the AEB system on its own.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:02:09 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Schratter", "Markus", ""], ["Bouton", "Maxime", ""], ["Kochenderfer", "Mykel J.", ""], ["Watzenig", "Daniel", ""]]}, {"id": "1904.11574", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "comments": "ACL 2020 camera-ready (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the task of Spatio-Temporal Video Question Answering, which\nrequires intelligent systems to simultaneously retrieve relevant moments and\ndetect referenced visual concepts (people and objects) to answer natural\nlanguage questions about videos. We first augment the TVQA dataset with 310.8K\nbounding boxes, linking depicted objects to visual concepts in questions and\nanswers. We name this augmented version as TVQA+. We then propose\nSpatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework\nthat grounds evidence in both spatial and temporal domains to answer questions\nabout videos. Comprehensive experiments and analyses demonstrate the\neffectiveness of our framework and how the rich annotations in our TVQA+\ndataset can contribute to the question answering task. Moreover, by performing\nthis joint task, our model is able to produce insightful and interpretable\nspatio-temporal attention visualizations. Dataset and code are publicly\navailable at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 20:37:26 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 19:43:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "1904.11610", "submitter": "Charlie Welch", "authors": "Charles Welch, Ver\\'onica P\\'erez-Rosas, Jonathan K. Kummerfeld, Rada\n  Mihalcea", "title": "Look Who's Talking: Inferring Speaker Attributes from Personal\n  Longitudinal Dialog", "comments": "15 pages accepted to CICLing 2019", "journal-ref": "Proceedings of the 20th International Conference on Computational\n  Linguistics and Intelligent Text Processing (CICLing 2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a large dialog corpus obtained from the conversation history of a\nsingle individual with 104 conversation partners. The corpus consists of half a\nmillion instant messages, across several messaging platforms. We focus our\nanalyses on seven speaker attributes, each of which partitions the set of\nspeakers, namely: gender; relative age; family member; romantic partner;\nclassmate; co-worker; and native to the same country. In addition to the\ncontent of the messages, we examine conversational aspects such as the time\nmessages are sent, messaging frequency, psycholinguistic word categories,\nlinguistic mirroring, and graph-based features reflecting how people in the\ncorpus mention each other. We present two sets of experiments predicting each\nattribute using (1) short context windows; and (2) a larger set of messages. We\nfind that using all features leads to gains of 9-14% over using message text\nonly.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:12:43 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Welch", "Charles", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Kummerfeld", "Jonathan K.", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1904.11621", "submitter": "Amlan Kar", "authors": "Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan,\n  Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler", "title": "Meta-Sim: Learning to Generate Synthetic Datasets", "comments": "Webpage: https://nv-tlabs.github.io/meta-sim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training models to high-end performance requires availability of large\nlabeled datasets, which are expensive to get. The goal of our work is to\nautomatically synthesize labeled datasets that are relevant for a downstream\ntask. We propose Meta-Sim, which learns a generative model of synthetic scenes,\nand obtain images as well as its corresponding ground-truth via a graphics\nengine. We parametrize our dataset generator with a neural network, which\nlearns to modify attributes of scene graphs obtained from probabilistic scene\ngrammars, so as to minimize the distribution gap between its rendered outputs\nand target data. If the real dataset comes with a small labeled validation set,\nwe additionally aim to optimize a meta-objective, i.e. downstream task\nperformance. Experiments show that the proposed method can greatly improve\ncontent generation quality over a human-engineered probabilistic scene grammar,\nboth qualitatively and quantitatively as measured by performance on a\ndownstream task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:18:36 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kar", "Amlan", ""], ["Prakash", "Aayush", ""], ["Liu", "Ming-Yu", ""], ["Cameracci", "Eric", ""], ["Yuan", "Justin", ""], ["Rusiniak", "Matt", ""], ["Acuna", "David", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1904.11622", "submitter": "Vincent Chen", "authors": "Vincent S. Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein,\n  Christopher Re, Li Fei-Fei", "title": "Scene Graph Prediction with Limited Labels", "comments": "ICCV 2019, 10 pages, 9 figures", "journal-ref": "International Conference on Computer Vision, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual knowledge bases such as Visual Genome power numerous applications in\ncomputer vision, including visual question answering and captioning, but suffer\nfrom sparse, incomplete relationships. All scene graph models to date are\nlimited to training on a small set of visual relationships that have thousands\nof training labels each. Hiring human annotators is expensive, and using\ntextual knowledge base completion methods are incompatible with visual data. In\nthis paper, we introduce a semi-supervised method that assigns probabilistic\nrelationship labels to a large number of unlabeled images using few labeled\nexamples. We analyze visual relationships to suggest two types of\nimage-agnostic features that are used to generate noisy heuristics, whose\noutputs are aggregated using a factor graph-based generative model. With as few\nas 10 labeled examples per relationship, the generative model creates enough\ntraining data to train any existing state-of-the-art scene graph model. We\ndemonstrate that our method outperforms all baseline approaches on scene graph\nprediction by 5.16 recall@100 for PREDCLS. In our limited label setting, we\ndefine a complexity metric for relationships that serves as an indicator (R^2 =\n0.778) for conditions under which our method succeeds over transfer learning,\nthe de-facto approach for training with limited labels.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:26:25 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 04:58:45 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 21:52:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Chen", "Vincent S.", ""], ["Varma", "Paroma", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Re", "Christopher", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1904.11685", "submitter": "Kai Wang", "authors": "Xiang Wang and Kai Wang and Shiguo Lian", "title": "A Survey on Face Data Augmentation", "comments": "26 pages, 22 figures. Neural Comput & Applic (2020)", "journal-ref": null, "doi": "10.1007/s00521-020-04748-3", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality and size of training set have great impact on the results of deep\nlearning-based face related tasks. However, collecting and labeling adequate\nsamples with high quality and balanced distributions still remains a laborious\nand expensive work, and various data augmentation techniques have thus been\nwidely used to enrich the training dataset. In this paper, we systematically\nreview the existing works of face data augmentation from the perspectives of\nthe transformation types and methods, with the state-of-the-art approaches\ninvolved. Among all these approaches, we put the emphasis on the deep\nlearning-based works, especially the generative adversarial networks which have\nbeen recognized as more powerful and effective tools in recent years. We\npresent their principles, discuss the results and show their applications as\nwell as limitations. Different evaluation metrics for evaluating these\napproaches are also introduced. We point out the challenges and opportunities\nin the field of face data augmentation, and provide brief yet insightful\ndiscussions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:23:35 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wang", "Xiang", ""], ["Wang", "Kai", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.11694", "submitter": "Jiayuan Mao", "authors": "Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou", "title": "Neural Logic Machines", "comments": "ICLR 2019. Project page:\n  https://sites.google.com/view/neural-logic-machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for\nboth inductive learning and logic reasoning. NLMs exploit the power of both\nneural networks---as function approximators, and logic programming---as a\nsymbolic processor for objects with properties, relations, logic connectives,\nand quantifiers. After being trained on small-scale tasks (such as sorting\nshort arrays), NLMs can recover lifted rules, and generalize to large-scale\ntasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect\ngeneralization in a number of tasks, from relational reasoning tasks on the\nfamily tree and general graphs, to decision making tasks including sorting\narrays, finding shortest paths, and playing the blocks world. Most of these\ntasks are hard to accomplish for neural networks or inductive logic programming\nalone.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:52:53 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dong", "Honghua", ""], ["Mao", "Jiayuan", ""], ["Lin", "Tian", ""], ["Wang", "Chong", ""], ["Li", "Lihong", ""], ["Zhou", "Denny", ""]]}, {"id": "1904.11713", "submitter": "Andreas St\\\"ockel", "authors": "Andreas St\\\"ockel, Chris Eliasmith", "title": "Passive nonlinear dendritic interactions as a general computational\n  resource in functional spiking neural networks", "comments": null, "journal-ref": null, "doi": "10.1162/neco_a_01338", "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear interactions in the dendritic tree play a key role in neural\ncomputation. Nevertheless, modeling frameworks aimed at the construction of\nlarge-scale, functional spiking neural networks, such as the Neural Engineering\nFramework, tend to assume a linear superposition of post-synaptic currents. In\nthis paper, we present a series of extensions to the Neural Engineering\nFramework that facilitate the construction of networks incorporating Dale's\nprinciple and nonlinear conductance-based synapses. We apply these extensions\nto a two-compartment LIF neuron that can be seen as a simple model of passive\ndendritic computation. We show that it is possible to incorporate neuron models\nwith input-dependent nonlinearities into the Neural Engineering Framework\nwithout compromising high-level function and that nonlinear post-synaptic\ncurrents can be systematically exploited to compute a wide variety of\nmultivariate, bandlimited functions, including the Euclidean norm, controlled\nshunting, and non-negative multiplication. By avoiding an additional source of\nspike noise, the function-approximation accuracy of a single layer of\ntwo-compartment LIF neurons is on a par with or even surpasses that of\ntwo-layer spiking neural networks up to a certain target function bandwidth.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:32:29 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 01:38:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["St\u00f6ckel", "Andreas", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1904.11737", "submitter": "Ramon Fraga Pereira", "authors": "Ramon Fraga Pereira, Nir Oren, and Felipe Meneguzzi", "title": "Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in\n  Discrete Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing whether an agent has abandoned a goal or is actively pursuing it is\nimportant when multiple agents are trying to achieve joint goals, or when\nagents commit to achieving goals for each other. Making such a determination\nfor a single goal by observing only plan traces is not trivial as agents often\ndeviate from optimal plans for various reasons, including the pursuit of\nmultiple goals or the inability to act optimally. In this article, we develop\nan approach based on domain independent heuristics from automated planning,\nlandmarks, and fact partitions to identify sub-optimal action steps - with\nrespect to a plan - within a plan execution trace. Such capability is very\nimportant in domains where multiple agents cooperate and delegate tasks among\nthemselves, e.g. through social commitments, and need to ensure that a\ndelegating agent can infer whether or not another agent is actually progressing\ntowards a delegated task. We demonstrate how an agent can use our technique to\ndetermine - by observing a trace - whether an agent is honouring a commitment.\nWe empirically show, for a number of representative domains, that our approach\ninfers sub-optimal action steps with very high accuracy and detects commitment\nabandonment in nearly all cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:36:26 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 16:38:44 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Pereira", "Ramon Fraga", ""], ["Oren", "Nir", ""], ["Meneguzzi", "Felipe", ""]]}, {"id": "1904.11738", "submitter": "Chun Kit Yeung", "authors": "Chun-Kit Yeung", "title": "Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using\n  Item Response Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based knowledge tracing model has been shown to outperform\ntraditional knowledge tracing model without the need for human-engineered\nfeatures, yet its parameters and representations have long been criticized for\nnot being explainable. In this paper, we propose Deep-IRT which is a synthesis\nof the item response theory (IRT) model and a knowledge tracing model that is\nbased on the deep neural network architecture called dynamic key-value memory\nnetwork (DKVMN) to make deep learning based knowledge tracing explainable.\nSpecifically, we use the DKVMN model to process the student's learning\ntrajectory and estimate the student ability level and the item difficulty level\nover time. Then, we use the IRT model to estimate the probability that a\nstudent will answer an item correctly using the estimated student ability and\nthe item difficulty. Experiments show that the Deep-IRT model retains the\nperformance of the DKVMN model, while it provides a direct psychological\ninterpretation of both students and items.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:38:37 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Yeung", "Chun-Kit", ""]]}, {"id": "1904.11739", "submitter": "Ramon Fraga Pereira", "authors": "Ramon Fraga Pereira, Nir Oren, and Felipe Meneguzzi", "title": "Landmark-Based Approaches for Goal Recognition as Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of recognizing goals and plans from missing and full observations\ncan be done efficiently by using automated planning techniques. In many\napplications, it is important to recognize goals and plans not only accurately,\nbut also quickly. To address this challenge, we develop novel goal recognition\napproaches based on planning techniques that rely on planning landmarks. In\nautomated planning, landmarks are properties (or actions) that cannot be\navoided to achieve a goal. We show the applicability of a number of planning\ntechniques with an emphasis on landmarks for goal and plan recognition tasks in\ntwo settings: (1) we use the concept of landmarks to develop goal recognition\nheuristics; and (2) we develop a landmark-based filtering method to refine\nexisting planning-based goal and plan recognition approaches. These recognition\napproaches are empirically evaluated in experiments over several classical\nplanning domains. We show that our goal recognition approaches yield not only\naccuracy comparable to (and often higher than) other state-of-the-art\ntechniques, but also substantially faster recognition time over such\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:40:37 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 01:57:46 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Pereira", "Ramon Fraga", ""], ["Oren", "Nir", ""], ["Meneguzzi", "Felipe", ""]]}, {"id": "1904.11740", "submitter": "Gemma Roig", "authors": "Kshitij Dwivedi, Gemma Roig", "title": "Representation Similarity Analysis for Efficient Task taxonomy &\n  Transfer Learning", "comments": "Accepted at CVPR 2019. Code available at\n  https://github.com/kshitijd20/RSA-CVPR19-release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is widely used in deep neural network models when there are\nfew labeled examples available. The common approach is to take a pre-trained\nnetwork in a similar task and finetune the model parameters. This is usually\ndone blindly without a pre-selection from a set of pre-trained models, or by\nfinetuning a set of models trained on different tasks and selecting the best\nperforming one by cross-validation. We address this problem by proposing an\napproach to assess the relationship between visual tasks and their\ntask-specific models. Our method uses Representation Similarity Analysis (RSA),\nwhich is commonly used to find a correlation between neuronal responses from\nbrain data and models. With RSA we obtain a similarity score among tasks by\ncomputing correlations between models trained on different tasks. Our method is\nefficient as it requires only pre-trained models, and a few images with no\nfurther training. We demonstrate the effectiveness and efficiency of our method\nfor generating task taxonomy on Taskonomy dataset. We next evaluate the\nrelationship of RSA with the transfer learning performance on Taskonomy tasks\nand a new task: Pascal VOC semantic segmentation. Our results reveal that\nmodels trained on tasks with higher similarity score show higher transfer\nlearning performance. Surprisingly, the best transfer learning result for\nPascal VOC semantic segmentation is not obtained from the pre-trained model on\nsemantic segmentation, probably due to the domain differences, and our method\nsuccessfully selects the high performing models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:43:11 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dwivedi", "Kshitij", ""], ["Roig", "Gemma", ""]]}, {"id": "1904.11753", "submitter": "Naoto Sato", "authors": "Naoto Sato, Hironobu Kuruma, Yuichiroh Nakagawa, Hideto Ogawa", "title": "Formal Verification of Decision-Tree Ensemble Model and Detection of its\n  Violating-input-value Ranges", "comments": null, "journal-ref": "IEICE Transaction D, Feb, 2020", "doi": "10.1587/transinf.2019EDP7120", "report-no": "Vol.E103-D, No.02, pp.363-378", "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one type of machine-learning model, a \"decision-tree ensemble model\"\n(DTEM) is represented by a set of decision trees. A DTEM is mainly known to be\nvalid for structured data; however, like other machine-learning models, it is\ndifficult to train so that it returns the correct output value for any input\nvalue. Accordingly, when a DTEM is used in regard to a system that requires\nreliability, it is important to comprehensively detect input values that lead\nto malfunctions of a system (failures) during development and take appropriate\nmeasures. One conceivable solution is to install an input filter that controls\nthe input to the DTEM, and to use separate software to process input values\nthat may lead to failures. To develop the input filter, it is necessary to\nspecify the filtering condition of the input value that leads to the\nmalfunction of the system. Given that necessity, in this paper, we propose a\nmethod for formally verifying a DTEM and, according to the result of the\nverification, if an input value leading to a failure is found, extracting the\nrange in which such an input value exists. The proposed method can\ncomprehensively extract the range in which the input value leading to the\nfailure exists; therefore, by creating an input filter based on that range, it\nis possible to prevent the failure occurring in the system. In this paper, the\nalgorithm of the proposed method is described, and the results of a case study\nusing a dataset of house prices are presented. On the basis of those results,\nthe feasibility of the proposed method is demonstrated, and its scalability is\nevaluated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:38:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Sato", "Naoto", ""], ["Kuruma", "Hironobu", ""], ["Nakagawa", "Yuichiroh", ""], ["Ogawa", "Hideto", ""]]}, {"id": "1904.11757", "submitter": "Jan-Hendrik Lorenz", "authors": "Jan-Hendrik Lorenz, Julian Nickerl", "title": "The Potential of Restarts for ProbSAT", "comments": "Eurocast 2019", "journal-ref": null, "doi": "10.1007/978-3-030-45093-9_43", "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyses the potential of restarts for probSAT, a quite successful\nalgorithm for k-SAT, by estimating its runtime distributions on random 3-SAT\ninstances that are close to the phase transition. We estimate an optimal\nrestart time from empirical data, reaching a potential speedup factor of 1.39.\nCalculating restart times from fitted probability distributions reduces this\nfactor to a maximum of 1.30. A spin-off result is that the Weibull distribution\napproximates the runtime distribution for over 93% of the used instances well.\nA machine learning pipeline is presented to compute a restart time for a\nfixed-cutoff strategy to exploit this potential. The main components of the\npipeline are a random forest for determining the distribution type and a neural\nnetwork for the distribution's parameters. ProbSAT performs statistically\nsignificantly better than Luby's restart strategy and the policy without\nrestarts when using the presented approach. The structure is particularly\nadvantageous on hard problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:51:11 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Lorenz", "Jan-Hendrik", ""], ["Nickerl", "Julian", ""]]}, {"id": "1904.11761", "submitter": "Robert Pinsler", "authors": "Robert Pinsler, Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee", "title": "Factored Contextual Policy Search with Bayesian Optimization", "comments": "To appear in ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different task\ncontexts. Contextual policy search offers data-efficient learning and\ngeneralization by explicitly conditioning the policy on a parametric context\nspace. In this paper, we further structure the contextual policy\nrepresentation. We propose to factor contexts into two components: target\ncontexts that describe the task objectives, e.g. target position for throwing a\nball; and environment contexts that characterize the environment, e.g. initial\nposition or mass of the ball. Our key observation is that experience can be\ndirectly generalized over target contexts. We show that this can be easily\nexploited in contextual policy search algorithms. In particular, we apply\nfactorization to a Bayesian optimization approach to contextual policy search\nboth in sampling-based and active learning settings. Our simulation results\nshow faster learning and better generalization in various robotic domains. See\nour supplementary video: https://youtu.be/MNTbBAOufDY.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:04:26 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Pinsler", "Robert", ""], ["Karkus", "Peter", ""], ["Kupcsik", "Andras", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1904.11943", "submitter": "Guandao Yang", "authors": "Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew\n  Gordon Wilson, Christopher De Sa", "title": "SWALP : Stochastic Weight Averaging in Low-Precision Training", "comments": "Published at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low precision operations can provide scalability, memory savings,\nportability, and energy efficiency. This paper proposes SWALP, an approach to\nlow precision training that averages low-precision SGD iterates with a modified\nlearning rate schedule. SWALP is easy to implement and can match the\nperformance of full-precision SGD even with all numbers quantized down to 8\nbits, including the gradient accumulators. Additionally, we show that SWALP\nconverges arbitrarily close to the optimal solution for quadratic objectives,\nand to a noise ball asymptotically smaller than low precision SGD in strongly\nconvex settings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:22:06 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 16:00:08 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yang", "Guandao", ""], ["Zhang", "Tianyi", ""], ["Kirichenko", "Polina", ""], ["Bai", "Junwen", ""], ["Wilson", "Andrew Gordon", ""], ["De Sa", "Christopher", ""]]}, {"id": "1904.11950", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Fuchun Sun, Tao Kong, Bin Fang and Wenchang Zhang", "title": "Attention-based Transfer Learning for Brain-computer Interface", "comments": "In Proceedings of IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP) 2019, 12 - 17 May, 2019, Brighton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different functional areas of the human brain play different roles in brain\nactivity, which has not been paid sufficient research attention in the\nbrain-computer interface (BCI) field. This paper presents a new approach for\nelectroencephalography (EEG) classification that applies attention-based\ntransfer learning. Our approach considers the importance of different brain\nfunctional areas to improve the accuracy of EEG classification, and provides an\nadditional way to automatically identify brain functional areas associated with\nnew activities without the involvement of a medical professional. We\ndemonstrate empirically that our approach out-performs state-of-the-art\napproaches in the task of EEG classification, and the results of visualization\nindicate that our approach can detect brain functional areas related to a\ncertain task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:10:09 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Tan", "Chuanqi", ""], ["Sun", "Fuchun", ""], ["Kong", "Tao", ""], ["Fang", "Bin", ""], ["Zhang", "Wenchang", ""]]}, {"id": "1904.11961", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil, Gianluca Schiavo, Yunlong Wang", "title": "CoachAI: A Conversational Agent Assisted Health Coaching Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Poor lifestyle represents a health risk factor and is the leading cause of\nmorbidity and chronic conditions. The impact of poor lifestyle can be\nsignificantly altered by individual behavior change. Although the current shift\nin healthcare towards a long lasting modifiable behavior, however, with\nincreasing caregiver workload and individuals' continuous needs of care, there\nis a need to ease caregiver's work while ensuring continuous interaction with\nusers. This paper describes the design and validation of CoachAI, a\nconversational agent assisted health coaching system to support health\nintervention delivery to individuals and groups. CoachAI instantiates a text\nbased healthcare chatbot system that bridges the remote human coach and the\nusers. This research provides three main contributions to the preventive\nhealthcare and healthy lifestyle promotion: (1) it presents the conversational\nagent to aid the caregiver; (2) it aims to decrease caregiver's workload and\nenhance care given to users, by handling (automating) repetitive caregiver\ntasks; and (3) it presents a domain independent mobile health conversational\nagent for health intervention delivery. We will discuss our approach and\nanalyze the results of a one month validation study on physical activity,\nhealthy diet and stress management.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:44:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Fadhil", "Ahmed", ""], ["Schiavo", "Gianluca", ""], ["Wang", "Yunlong", ""]]}, {"id": "1904.12004", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Rudy Bunel, Krishnamurthy Dvijotham, Po-Sen Huang,\n  Edward Grefenstette, Pushmeet Kohli", "title": "Knowing When to Stop: Evaluation and Verification of Conformity to\n  Output-size Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in\nreal world applications. While the ability of these neural architectures to\nproduce variable-length outputs makes them extremely effective for problems\nlike Machine Translation and Image Captioning, it also leaves them vulnerable\nto failures of the form where the model produces outputs of undesirable length.\nThis behavior can have severe consequences such as usage of increased\ncomputation and induce faults in downstream modules that expect outputs of a\ncertain length. Motivated by the need to have a better understanding of the\nfailures of these models, this paper proposes and studies the novel output-size\nmodulation problem and makes two key technical contributions. First, to\nevaluate model robustness, we develop an easy-to-compute differentiable proxy\nobjective that can be used with gradient-based algorithms to find\noutput-lengthening inputs. Second and more importantly, we develop a\nverification approach that can formally verify whether a network always\nproduces outputs within a certain length. Experimental results on Machine\nTranslation and Image Captioning show that our output-lengthening approach can\nproduce outputs that are 50 times longer than the input, while our verification\napproach can, given a model and input domain, prove that the output length is\nbelow a certain size.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 18:12:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Chenglong", ""], ["Bunel", "Rudy", ""], ["Dvijotham", "Krishnamurthy", ""], ["Huang", "Po-Sen", ""], ["Grefenstette", "Edward", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1904.12052", "submitter": "Hengtong Zhang", "authors": "Hengtong Zhang, Tianhang Zheng, Jing Gao, Chenglin Miao, Lu Su,\n  Yaliang Li, Kui Ren", "title": "Data Poisoning Attack against Knowledge Graph Embedding", "comments": "Fix typos and version conflicts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding (KGE) is a technique for learning continuous\nembeddings for entities and relations in the knowledge graph.Due to its benefit\nto a variety of downstream tasks such as knowledge graph completion, question\nanswering and recommendation, KGE has gained significant attention recently.\nDespite its effectiveness in a benign environment, KGE' robustness to\nadversarial attacks is not well-studied. Existing attack methods on graph data\ncannot be directly applied to attack the embeddings of knowledge graph due to\nits heterogeneity. To fill this gap, we propose a collection of data poisoning\nattack strategies, which can effectively manipulate the plausibility of\narbitrary targeted facts in a knowledge graph by adding or deleting facts on\nthe graph. The effectiveness and efficiency of the proposed attack strategies\nare verified by extensive evaluations on two widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:12:19 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 04:06:02 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhang", "Hengtong", ""], ["Zheng", "Tianhang", ""], ["Gao", "Jing", ""], ["Miao", "Chenglin", ""], ["Su", "Lu", ""], ["Li", "Yaliang", ""], ["Ren", "Kui", ""]]}, {"id": "1904.12054", "submitter": "Marc Z\\\"oller", "authors": "Marc-Andr\\'e Z\\\"oller and Marco F. Huber", "title": "Benchmark and Survey of Automated Machine Learning Frameworks", "comments": "Revised version accepted for publication at Journal of Artificial\n  Intelligence Research (JAIR)", "journal-ref": "Journal of Artificial Intelligence Research 70 (2021) 409-472", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has become a vital part in many aspects of our daily\nlife. However, building well performing machine learning applications requires\nhighly specialized data scientists and domain experts. Automated machine\nlearning (AutoML) aims to reduce the demand for data scientists by enabling\ndomain experts to build machine learning applications automatically without\nextensive knowledge of statistics and machine learning. This paper is a\ncombination of a survey on current AutoML methods and a benchmark of popular\nAutoML frameworks on real data sets. Driven by the selected frameworks for\nevaluation, we summarize and review important AutoML techniques and methods\nconcerning every step in building an ML pipeline. The selected AutoML\nframeworks are evaluated on 137 data sets from established AutoML benchmark\nsuits.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:42:56 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 11:19:24 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 09:49:10 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 15:09:26 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 15:52:33 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Z\u00f6ller", "Marc-Andr\u00e9", ""], ["Huber", "Marco F.", ""]]}, {"id": "1904.12059", "submitter": "John Collomosse", "authors": "Tu Bui, Daniel Cooper, John Collomosse, Mark Bell, Alex Green, John\n  Sheridan, Jez Higgins, Arindra Das, Jared Keller, Olivier Thereaux, Alan\n  Brown", "title": "ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes\n  on the Blockchain", "comments": "Accepted to CVPR Blockchain Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ARCHANGEL; a novel distributed ledger based system for assuring\nthe long-term integrity of digital video archives. First, we describe a novel\ndeep network architecture for computing compact temporal content hashes (TCHs)\nfrom audio-visual streams with durations of minutes or hours. Our TCHs are\nsensitive to accidental or malicious content modification (tampering) but\ninvariant to the codec used to encode the video. This is necessary due to the\ncuratorial requirement for archives to format shift video over time to ensure\nfuture accessibility. Second, we describe how the TCHs (and the models used to\nderive them) are secured via a proof-of-authority blockchain distributed across\nmultiple independent archives. We report on the efficacy of ARCHANGEL within\nthe context of a trial deployment in which the national government archives of\nthe United Kingdom, Estonia and Norway participated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:59:02 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bui", "Tu", ""], ["Cooper", "Daniel", ""], ["Collomosse", "John", ""], ["Bell", "Mark", ""], ["Green", "Alex", ""], ["Sheridan", "John", ""], ["Higgins", "Jez", ""], ["Das", "Arindra", ""], ["Keller", "Jared", ""], ["Thereaux", "Olivier", ""], ["Brown", "Alan", ""]]}, {"id": "1904.12084", "submitter": "Zhanfu Yang", "authors": "Zhanfu Yang, Fei Wang, Ziliang Chen, Guannan Wei, Tiark Rompf", "title": "Graph Neural Reasoning for 2-Quantified Boolean Formula Solvers", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the feasibility of learning GNN (Graph Neural\nNetwork) based solvers and GNN-based heuristics for specified QBF (Quantified\nBoolean Formula) problems. We design and evaluate several GNN architectures for\n2QBF formulae, and conjecture that GNN has limitations in learning 2QBF\nsolvers. Then we show how to learn a heuristic CEGAR 2QBF solver. We further\nexplore generalizing GNN-based heuristics to larger unseen instances, and\nuncover some interesting challenges. In summary, this paper provides a\ncomprehensive surveying view of applying GNN-embeddings to specified QBF\nsolvers, and aims to offer guidance in applying ML to more complicated symbolic\nreasoning problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 01:30:50 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Zhanfu", ""], ["Wang", "Fei", ""], ["Chen", "Ziliang", ""], ["Wei", "Guannan", ""], ["Rompf", "Tiark", ""]]}, {"id": "1904.12106", "submitter": "Jifan Chen", "authors": "Jifan Chen and Greg Durrett", "title": "Understanding Dataset Design Choices for Multi-hop Reasoning", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multi-hop reasoning has been a key challenge for reading\ncomprehension models, leading to the design of datasets that explicitly focus\non it. Ideally, a model should not be able to perform well on a multi-hop\nquestion answering task without doing multi-hop reasoning. In this paper, we\ninvestigate two recently proposed datasets, WikiHop and HotpotQA. First, we\nexplore sentence-factored models for these tasks; by design, these models\ncannot do multi-hop reasoning, but they are still able to solve a large number\nof examples in both datasets. Furthermore, we find spurious correlations in the\nunmasked version of WikiHop, which make it easy to achieve high performance\nconsidering only the questions and answers. Finally, we investigate one key\ndifference between these datasets, namely span-based vs. multiple-choice\nformulations of the QA task. Multiple-choice versions of both datasets can be\neasily gamed, and two models we examine only marginally exceed a baseline in\nthis setting. Overall, while these datasets are useful testbeds,\nhigh-performing models may not be learning as much multi-hop reasoning as\npreviously thought.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 04:36:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Jifan", ""], ["Durrett", "Greg", ""]]}, {"id": "1904.12134", "submitter": "Jacopo Arpetti", "authors": "Otello Ardovino, Jacopo Arpetti, Marco Delmastro", "title": "Regulating AI: do we need new tools?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.AI q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Intelligence paradigm (hereinafter referred to as \"AI\") builds\non the analysis of data able, among other things, to snap pictures of the\nindividuals' behaviors and preferences. Such data represent the most valuable\ncurrency in the digital ecosystem, where their value derives from their being a\nfundamental asset in order to train machines with a view to developing AI\napplications. In this environment, online providers attract users by offering\nthem services for free and getting in exchange data generated right through the\nusage of such services. This swap, characterized by an implicit nature,\nconstitutes the focus of the present paper, in the light of the disequilibria,\nas well as market failures, that it may bring about. We use mobile apps and the\nrelated permission system as an ideal environment to explore, via econometric\ntools, those issues. The results, stemming from a dataset of over one million\nobservations, show that both buyers and sellers are aware that access to\ndigital services implicitly implies an exchange of data, although this does not\nhave a considerable impact neither on the level of downloads (demand), nor on\nthe level of the prices (supply). In other words, the implicit nature of this\nexchange does not allow market indicators to work efficiently. We conclude that\ncurrent policies (e.g. transparency rules) may be inherently biased and we put\nforward suggestions for a new approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 09:30:13 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ardovino", "Otello", ""], ["Arpetti", "Jacopo", ""], ["Delmastro", "Marco", ""]]}, {"id": "1904.12178", "submitter": "Maen Alzubi", "authors": "Maen Alzubi, Zsolt Csaba Johany\\'ak, Szilveszter Kov\\'acs", "title": "Fuzzy Rule Interpolation Methods and Fri Toolbox", "comments": null, "journal-ref": "Journal of Theoretical and Applied Information Technology 15th\n  November 2018. Vol.96. No 21", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FRI methods are less popular in the practical application domain. One\npossible reason is the missing common framework. There are many FRI methods\ndeveloped independently, having different interpolation concepts and features.\nOne trial for setting up a common FRI framework was the MATLAB FRI Toolbox,\ndeveloped by Johany\\'ak et. al. in 2006. The goals of this paper are divided as\nfollows: firstly, to present a brief introduction of the FRI methods. Secondly,\nto introduce a brief description of the refreshed and extended version of the\noriginal FRI Toolbox. And thirdly, to use different unified numerical benchmark\nexamples to evaluate and analyze the Fuzzy Rule Interpolation Techniques (FRI)\n(KH, KH Stabilized, MACI, IMUL, CRF, VKK, GM, FRIPOC, LESFRI, and SCALEMOVE),\nthat will be classified and compared based on different features by following\nthe abnormality and linearity conditions [15].\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:44:33 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Alzubi", "Maen", ""], ["Johany\u00e1k", "Zsolt Csaba", ""], ["Kov\u00e1cs", "Szilveszter", ""]]}, {"id": "1904.12194", "submitter": "Subodh Deolekar", "authors": "Subodh Deolekar and Siby Abraham", "title": "Towards Automation of Creativity: A Machine Intelligence Approach", "comments": "31 pages, 24 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates emergence of computational creativity in the field of\nmusic. Different aspects of creativity such as producer, process, product and\npress are studied and formulated. Different notions of computational creativity\nsuch as novelty, quality and typicality of compositions as products are studied\nand evaluated. We formulate an algorithmic perception on human creativity and\npropose a prototype that is capable of demonstrating human-level creativity. We\nthen validate the proposed prototype by applying various creativity benchmarks\nwith the results obtained and compare the proposed prototype with the other\nexisting computational creative systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 18:54:51 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Deolekar", "Subodh", ""], ["Abraham", "Siby", ""]]}, {"id": "1904.12200", "submitter": "Anmol Sharma", "authors": "Anmol Sharma, Ghassan Hamarneh", "title": "Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative\n  Adversarial Network", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is being increasingly utilized to assess,\ndiagnose, and plan treatment for a variety of diseases. The ability to\nvisualize tissue in varied contrasts in the form of MR pulse sequences in a\nsingle scan provides valuable insights to physicians, as well as enabling\nautomated systems performing downstream analysis. However many issues like\nprohibitive scan time, image corruption, different acquisition protocols, or\nallergies to certain contrast materials may hinder the process of acquiring\nmultiple sequences for a patient. This poses challenges to both physicians and\nautomated systems since complementary information provided by the missing\nsequences is lost. In this paper, we propose a variant of generative\nadversarial network (GAN) capable of leveraging redundant information contained\nwithin multiple available sequences in order to generate one or more missing\nsequences for a patient scan. The proposed network is designed as a\nmulti-input, multi-output network which combines information from all the\navailable pulse sequences, implicitly infers which sequences are missing, and\nsynthesizes the missing ones in a single forward pass. We demonstrate and\nvalidate our method on two brain MRI datasets each with four sequences, and\nshow the applicability of the proposed method in simultaneously synthesizing\nall missing sequences in any possible scenario where either one, two, or three\nof the four sequences may be missing. We compare our approach with competing\nunimodal and multi-modal methods, and show that we outperform both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:15:15 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 19:08:43 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 00:20:13 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Sharma", "Anmol", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1904.12211", "submitter": "Mojtaba Nayyeri", "authors": "Mojtaba Nayyeri, Sahar Vahdati, Jens Lehmann, Hamed Shariat Yazdi", "title": "Soft Marginal TransE for Scholarly Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs), i.e. representation of information as a semantic\ngraph, provide a significant test bed for many tasks including question\nanswering, recommendation, and link prediction. Various amount of scholarly\nmetadata have been made vailable as knowledge graphs from the diversity of data\nproviders and agents. However, these high-quantities of data remain far from\nquality criteria in terms of completeness while growing at a rapid pace. Most\nof the attempts in completing such KGs are following traditional data\ndigitization, harvesting and collaborative curation approaches. Whereas,\nadvanced AI-related approaches such as embedding models - specifically designed\nfor such tasks - are usually evaluated for standard benchmarks such as Freebase\nand Wordnet. The tailored nature of such datasets prevents those approaches to\nshed the lights on more accurate discoveries. Application of such models on\ndomain-specific KGs takes advantage of enriched meta-data and provides accurate\nresults where the underlying domain can enormously benefit. In this work, the\nTransE embedding model is reconciled for a specific link prediction task on\nscholarly metadata. The results show a significant shift in the accuracy and\nperformance evaluation of the model on a dataset with scholarly metadata. The\nnewly proposed version of TransE obtains 99.9% for link prediction task while\noriginal TransE gets 95%. In terms of accuracy and Hit@10, TransE outperforms\nother embedding models such as ComplEx, TransH and TransR experimented over\nscholarly knowledge graphs\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:40:03 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Vahdati", "Sahar", ""], ["Lehmann", "Jens", ""], ["Yazdi", "Hamed Shariat", ""]]}, {"id": "1904.12242", "submitter": "Yachen Tang", "authors": "Yachen Tang, Tingting Liu, Guangyi Liu, Jie Li, Renchang Dai, and Chen\n  Yuan", "title": "Enhancement of Power Equipment Management Using Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate retrieval of the power equipment information plays an important role\nin guiding the full-lifecycle management of power system assets. Because of\ndata duplication, database decentralization, weak data relations, and sluggish\ndata updates, the power asset management system eager to adopt a new strategy\nto avoid the information losses, bias, and improve the data storage efficiency\nand extraction process. Knowledge graph has been widely developed in large part\nowing to its schema-less nature. It enables the knowledge graph to grow\nseamlessly and allows new relations addition and entities insertion when\nneeded. This study proposes an approach for constructing power equipment\nknowledge graph by merging existing multi-source heterogeneous power equipment\nrelated data. A graph-search method to illustrate exhaustive results to the\ndesired information based on the constructed knowledge graph is proposed. A\ncase of a 500 kV station example is then demonstrated to show relevant search\nresults and to explain that the knowledge graph can improve the efficiency of\npower equipment management.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:02:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Tang", "Yachen", ""], ["Liu", "Tingting", ""], ["Liu", "Guangyi", ""], ["Li", "Jie", ""], ["Dai", "Renchang", ""], ["Yuan", "Chen", ""]]}, {"id": "1904.12255", "submitter": "Suhit Kodgule", "authors": "Suhit Kodgule, Alberto Candela and David Wettergreen", "title": "Non-myopic Planetary Exploration Combining In Situ and Remote\n  Measurements", "comments": "Preprint. Under review for IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Remote sensing can provide crucial information for planetary rovers. However,\nthey must validate these orbital observations with in situ measurements.\nTypically, this involves validating hyperspectral data using a spectrometer\non-board the field robot. In order to achieve this, the robot must visit\nsampling locations that jointly improve a model of the environment while\nsatisfying sampling constraints. However, current planners follow sub-optimal\ngreedy strategies that are not scalable to larger regions. We demonstrate how\nthe problem can be effectively defined in an MDP framework and propose a\nplanning algorithm based on Monte Carlo Tree Search, which is devoid of the\ncommon drawbacks of existing planners and also provides superior performance.\nWe evaluate our approach using hyperspectral imagery of a well-studied geologic\nsite in Cuprite, Nevada.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 03:59:01 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kodgule", "Suhit", ""], ["Candela", "Alberto", ""], ["Wettergreen", "David", ""]]}, {"id": "1904.12304", "submitter": "Muhammad Sarmad", "authors": "Muhammad Sarmad, Hyunjoo Jenny Lee, Young Min Kim", "title": "RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for\n  Real-Time Point Cloud Shape Completion", "comments": "Accepted to IEEE CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present RL-GAN-Net, where a reinforcement learning (RL) agent provides\nfast and robust control of a generative adversarial network (GAN). Our\nframework is applied to point cloud shape completion that converts noisy,\npartial point cloud data into a high-fidelity completed shape by controlling\nthe GAN. While a GAN is unstable and hard to train, we circumvent the problem\nby (1) training the GAN on the latent space representation whose dimension is\nreduced compared to the raw point cloud input and (2) using an RL agent to find\nthe correct input to the GAN to generate the latent space representation of the\nshape that best fits the current input of incomplete point cloud. The suggested\npipeline robustly completes point cloud with large missing regions. To the best\nof our knowledge, this is the first attempt to train an RL agent to control the\nGAN, which effectively learns the highly nonlinear mapping from the input noise\nof the GAN to the latent space of point cloud. The RL agent replaces the need\nfor complex optimization and consequently makes our technique real time.\nAdditionally, we demonstrate that our pipelines can be used to enhance the\nclassification accuracy of point cloud with missing data.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 11:08:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sarmad", "Muhammad", ""], ["Lee", "Hyunjoo Jenny", ""], ["Kim", "Young Min", ""]]}, {"id": "1904.12371", "submitter": "Sebastian Junges", "authors": "Milan \\v{C}e\\v{s}ka, Christian Hensel, Sebastian Junges, Joost-Pieter\n  Katoen", "title": "Counterexample-Driven Synthesis for Probabilistic Program Sketches", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programs are key to deal with uncertainty in e.g. controller\nsynthesis. They are typically small but intricate. Their development is complex\nand error prone requiring quantitative reasoning over a myriad of alternative\ndesigns. To mitigate this complexity, we adopt counterexample-guided inductive\nsynthesis (CEGIS) to automatically synthesise finite-state probabilistic\nprograms. Our approach leverages efficient model checking, modern SMT solving,\nand counterexample generation at program level. Experiments on practically\nrelevant case studies show that design spaces with millions of candidate\ndesigns can be fully explored using a few thousand verification queries.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 19:15:33 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["\u010ce\u0161ka", "Milan", ""], ["Hensel", "Christian", ""], ["Junges", "Sebastian", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "1904.12383", "submitter": "Seyedeh Neelufar Payrovnaziri", "authors": "Seyedeh Neelufar Payrovnaziri, Laura A. Barrett, Daniel Bis, Jiang\n  Bian, Zhe He", "title": "Enhancing Prediction Models for One-Year Mortality in Patients with\n  Acute Myocardial Infarction and Post Myocardial Infarction Syndrome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the risk of mortality for patients with acute myocardial\ninfarction (AMI) using electronic health records (EHRs) data can help identify\nrisky patients who might need more tailored care. In our previous work, we\nbuilt computational models to predict one-year mortality of patients admitted\nto an intensive care unit (ICU) with AMI or post myocardial infarction\nsyndrome. Our prior work only used the structured clinical data from MIMIC-III,\na publicly available ICU clinical database. In this study, we enhanced our work\nby adding the word embedding features from free-text discharge summaries. Using\na richer set of features resulted in significant improvement in the performance\nof our deep learning models. The average accuracy of our deep learning models\nwas 92.89% and the average F-measure was 0.928. We further reported the impact\nof different combinations of features extracted from structured and/or\nunstructured data on the performance of the deep learning models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 20:45:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Payrovnaziri", "Seyedeh Neelufar", ""], ["Barrett", "Laura A.", ""], ["Bis", "Daniel", ""], ["Bian", "Jiang", ""], ["He", "Zhe", ""]]}, {"id": "1904.12385", "submitter": "Deniz Gunduz", "authors": "Deniz Gunduz, Paul de Kerret, Nicholas D. Sidiropoulos, David Gesbert,\n  Chandra Murthy, Mihaela van der Schaar", "title": "Machine Learning in the Air", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the recent advances in processing speed and data acquisition and\nstorage, machine learning (ML) is penetrating every facet of our lives, and\ntransforming research in many areas in a fundamental manner. Wireless\ncommunications is another success story -- ubiquitous in our lives, from\nhandheld devices to wearables, smart homes, and automobiles. While recent years\nhave seen a flurry of research activity in exploiting ML tools for various\nwireless communication problems, the impact of these techniques in practical\ncommunication systems and standards is yet to be seen. In this paper, we review\nsome of the major promises and challenges of ML in wireless communication\nsystems, focusing mainly on the physical layer. We present some of the most\nstriking recent accomplishments that ML techniques have achieved with respect\nto classical approaches, and point to promising research directions where ML is\nlikely to make the biggest impact in the near future. We also highlight the\ncomplementary problem of designing physical layer techniques to enable\ndistributed ML at the wireless network edge, which further emphasizes the need\nto understand and connect ML with fundamental concepts in wireless\ncommunications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 21:21:48 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Gunduz", "Deniz", ""], ["de Kerret", "Paul", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Gesbert", "David", ""], ["Murthy", "Chandra", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1904.12437", "submitter": "Cheng Li", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wen-mei Hwu", "title": "Challenges and Pitfalls of Machine Learning Evaluation and Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasingly complex and diverse collection of Machine Learning (ML)\nmodels as well as hardware/software stacks, collectively referred to as \"ML\nartifacts\", are being proposed - leading to a diverse landscape of ML. These ML\ninnovations proposed have outpaced researchers' ability to analyze, study and\nadapt them. This is exacerbated by the complicated and sometimes\nnon-reproducible procedures for ML evaluation. A common practice of sharing ML\nartifacts is through repositories where artifact authors post ad-hoc code and\nsome documentation, but often fail to reveal critical information for others to\nreproduce their results. This results in users' inability to compare with\nartifact authors' claims or adapt the model to his/her own use. This paper\ndiscusses common challenges and pitfalls of ML evaluation and benchmarking,\nwhich can be used as a guideline for ML model authors when sharing ML\nartifacts, and for system developers when benchmarking or designing ML systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 03:35:15 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 09:28:49 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1904.12470", "submitter": "Asher Wilk", "authors": "Asher Wilk", "title": "Teaching AI, Ethics, Law and Policy", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyberspace and development of intelligent systems using Artificial\nIntelligence (AI) creates new challenges to computer professionals, data\nscientists, regulators and policy makers. For example, self-driving cars raise\nnew technical, ethical, legal and public policy issues. This paper proposes a\ncourse named Computers, Ethics, Law, and Public Policy, and suggests a\ncurriculum for such a course. This paper presents ethical, legal, and public\npolicy issues relevant to building and using intelligent systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 07:01:50 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 15:56:02 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 16:38:44 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 18:04:31 GMT"}, {"version": "v5", "created": "Fri, 30 Aug 2019 16:13:44 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Wilk", "Asher", ""]]}, {"id": "1904.12533", "submitter": "Carsten Lutz", "authors": "Carsten Lutz and Leif Sabellek", "title": "A Complete Classification of the Complexity and Rewritability of\n  Ontology-Mediated Queries based on the Description Logic EL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an ultimately fine-grained analysis of the data complexity and\nrewritability of ontology-mediated queries (OMQs) based on an EL ontology and a\nconjunctive query (CQ). Our main results are that every such OMQ is in AC0,\nNL-complete, or PTime-complete and that containment in NL coincides with\nrewritability into linear Datalog (whereas containment in AC0 coincides with\nrewritability into first-order logic). We establish natural characterizations\nof the three cases in terms of bounded depth and (un)bounded pathwidth, and\nshow that every of the associated meta problems such as deciding wether a given\nOMQ is rewritable into linear Datalog is ExpTime-complete. We also give a way\nto construct linear Datalog rewritings when they exist and prove that there is\nno constant Datalog rewritings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:34:54 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Lutz", "Carsten", ""], ["Sabellek", "Leif", ""]]}, {"id": "1904.12534", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit", "title": "Casting Geometric Constraints in Semantic Segmentation as\n  Semi-Supervised Learning", "comments": "To be presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective method to learn to segment new indoor\nscenes from video frames: State-of-the-art methods trained on one dataset, even\nas large as the SUNRGB-D dataset, can perform poorly when applied to images\nthat are not part of the dataset, because of the dataset bias, a common\nphenomenon in computer vision. To make semantic segmentation more useful in\npractice, one can exploit geometric constraints. Our main contribution is to\nshow that these constraints can be cast conveniently as semi-supervised terms,\nwhich enforce the fact that the same class should be predicted for the\nprojections of the same 3D location in different images. This is interesting as\nwe can exploit general existing techniques developed for semi-supervised\nlearning to efficiently incorporate the constraints. We show that this approach\ncan efficiently and accurately learn to segment target sequences of ScanNet and\nour own target sequences using only annotations from SUNRGB-D, and geometric\nrelations between the video frames of target sequences.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:36:12 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 11:15:12 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 08:54:00 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1904.12535", "submitter": "Ping Li", "authors": "Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, Ping Li", "title": "Logician: A Unified End-to-End Neural Approach for Open-Domain\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of open information extraction (OIE)\nfor extracting entity and relation level intermediate structures from sentences\nin open-domain. We focus on four types of valuable intermediate structures\n(Relation, Attribute, Description, and Concept), and propose a unified\nknowledge expression form, SAOKE, to express them. We publicly release a data\nset which contains more than forty thousand sentences and the corresponding\nfacts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is\nthe largest publicly available human labeled data set for open information\nextraction tasks. Using this labeled SAOKE data set, we train an end-to-end\nneural model using the sequenceto-sequence paradigm, called Logician, to\ntransform sentences into facts. For each sentence, different to existing\nalgorithms which generally focus on extracting each single fact without\nconcerning other possible facts, Logician performs a global optimization over\nall possible involved facts, in which facts not only compete with each other to\nattract the attention of words, but also cooperate to share words. An\nexperimental study on various types of open domain relation extraction tasks\nreveals the consistent superiority of Logician to other states-of-the-art\nalgorithms. The experiments verify the reasonableness of SAOKE format, the\nvaluableness of SAOKE data set, the effectiveness of the proposed Logician\nmodel, and the feasibility of the methodology to apply end-to-end learning\nparadigm on supervised data sets for the challenging tasks of open information\nextraction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:37:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sun", "Mingming", ""], ["Li", "Xu", ""], ["Wang", "Xin", ""], ["Fan", "Miao", ""], ["Feng", "Yue", ""], ["Li", "Ping", ""]]}, {"id": "1904.12584", "submitter": "Jiayuan Mao", "authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun\n  Wu", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "comments": "ICLR 2019 (Oral). Project page: http://nscl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns\nvisual concepts, words, and semantic parsing of sentences without explicit\nsupervision on any of them; instead, our model learns by simply looking at\nimages and reading paired questions and answers. Our model builds an\nobject-based scene representation and translates sentences into executable,\nsymbolic programs. To bridge the learning of two modules, we use a\nneuro-symbolic reasoning module that executes these programs on the latent\nscene representation. Analogical to human concept learning, the perception\nmodule learns visual concepts based on the language description of the object\nbeing referred to. Meanwhile, the learned visual concepts facilitate learning\nnew words and parsing new sentences. We use curriculum learning to guide the\nsearching over the large compositional space of images and language. Extensive\nexperiments demonstrate the accuracy and efficiency of our model on learning\nvisual concepts, word representations, and semantic parsing of sentences.\nFurther, our method allows easy generalization to new object attributes,\ncompositions, language concepts, scenes and questions, and even new program\ndomains. It also empowers applications including visual question answering and\nbidirectional image-text retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:50:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mao", "Jiayuan", ""], ["Gan", "Chuang", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1904.12632", "submitter": "Pablo Barros", "authors": "Pablo Barros, German I. Parisi and Stefan Wermter", "title": "A Personalized Affective Memory Neural Model for Improving Emotion\n  Recognition", "comments": "Accepted by the International Conference on Machine Learning 2019\n  (ICML2019)", "journal-ref": "in PMLR 97:485-494 (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent models of emotion recognition strongly rely on supervised deep\nlearning solutions for the distinction of general emotion expressions. However,\nthey are not reliable when recognizing online and personalized facial\nexpressions, e.g., for person-specific affective understanding. In this paper,\nwe present a neural model based on a conditional adversarial autoencoder to\nlearn how to represent and edit general emotion expressions. We then propose\nGrow-When-Required networks as personalized affective memories to learn\nindividualized aspects of emotion expressions. Our model achieves\nstate-of-the-art performance on emotion recognition when evaluated on\n\\textit{in-the-wild} datasets. Furthermore, our experiments include ablation\nstudies and neural visualizations in order to explain the behavior of our\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 09:42:26 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 08:42:13 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Barros", "Pablo", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1904.12634", "submitter": "Jianwu Fang", "authors": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang and Sen Li", "title": "DADA-2000: Can Driving Accident be Predicted by Driver Attention?\n  Analyzed by A Benchmark", "comments": "Submitted to ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver attention prediction is currently becoming the focus in safe driving\nresearch community, such as the DR(eye)VE project and newly emerged Berkeley\nDeepDrive Attention (BDD-A) database in critical situations. In safe driving,\nan essential task is to predict the incoming accidents as early as possible.\nBDD-A was aware of this problem and collected the driver attention in\nlaboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses\nthe critical situations which do not encounter actual accidents, and just faces\nthe driver attention prediction task, without a close step for accident\nprediction. In contrast to this, we explore the view of drivers' eyes for\ncapturing multiple kinds of accidents, and construct a more diverse and larger\nvideo benchmark than ever before with the driver attention and the driving\naccident annotation simultaneously (named as DADA-2000), which has 2000 video\nclips owning about 658,476 frames on 54 kinds of accidents. These clips are\ncrowd-sourced and captured in various occasions (highway, urban, rural, and\ntunnel), weather (sunny, rainy and snowy) and light conditions (daytime and\nnighttime). For the driver attention representation, we collect the maps of\nfixations, saccade scan path and focusing time. The accidents are annotated by\ntheir categories, the accident window in clips and spatial locations of the\ncrash-objects. Based on the analysis, we obtain a quantitative and positive\nanswer for the question in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 03:33:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fang", "Jianwu", ""], ["Yan", "Dingxin", ""], ["Qiao", "Jiahuan", ""], ["Xue", "Jianru", ""], ["Wang", "He", ""], ["Li", "Sen", ""]]}, {"id": "1904.12635", "submitter": "Leroy Cronin Prof", "authors": "Jonathan Grizou, Laurie J. Points, Abhishek Sharma and Leroy Cronin", "title": "Exploration of Self-Propelling Droplets Using a Curiosity Driven Robotic\n  Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a chemical robotic assistant equipped with a curiosity algorithm\n(CA) that can efficiently explore the state a complex chemical system can\nexhibit. The CA-robot is designed to explore formulations in an open-ended way\nwith no explicit optimization target. By applying the CA-robot to the study of\nself-propelling multicomponent oil-in-water droplets, we are able to observe an\norder of magnitude more variety of droplet behaviours than possible with a\nrandom parameter search and given the same budget. We demonstrate that the\nCA-robot enabled the discovery of a sudden and highly specific response of\ndroplets to slight temperature changes. Six modes of self-propelled droplets\nmotion were identified and classified using a time-temperature phase diagram\nand probed using a variety of techniques including NMR. This work illustrates\nhow target free search can significantly increase the rate of unpredictable\nobservations leading to new discoveries with potential applications in\nformulation chemistry.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 22:17:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Grizou", "Jonathan", ""], ["Points", "Laurie J.", ""], ["Sharma", "Abhishek", ""], ["Cronin", "Leroy", ""]]}, {"id": "1904.12641", "submitter": "Qi Wang", "authors": "Yuan Yuan, Yuwei Lu, Qi Wang", "title": "Tracking as A Whole: Multi-Target Tracking by Modeling Group Behavior\n  with Sequential Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2017.2686871", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based vehicle detection and tracking is one of the most important\ncomponents for Intelligent Transportation Systems (ITS). When it comes to road\njunctions, the problem becomes even more difficult due to the occlusions and\ncomplex interactions among vehicles. In order to get a precise detection and\ntracking result, in this work we propose a novel tracking-by-detection\nframework. In the detection stage, we present a sequential detection model to\ndeal with serious occlusions. In the tracking stage, we model group behavior to\ntreat complex interactions with overlaps and ambiguities. The main\ncontributions of this paper are twofold: 1) Shape prior is exploited in the\nsequential detection model to tackle occlusions in crowded scene. 2) Traffic\nforce is defined in the traffic scene to model group behavior, and it can\nassist to handle complex interactions among vehicles. We evaluate the proposed\napproach on real surveillance videos at road junctions and the performance has\ndemonstrated the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:39:00 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yuan", "Yuan", ""], ["Lu", "Yuwei", ""], ["Wang", "Qi", ""]]}, {"id": "1904.12642", "submitter": "Qi Wang", "authors": "Yuwei Lu, Yuan Yuan, Qi Wang", "title": "Forward Vehicle Collision Warning Based on Quick Camera Calibration", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2018.8461620", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward Vehicle Collision Warning (FCW) is one of the most important\nfunctions for autonomous vehicles. In this procedure, vehicle detection and\ndistance measurement are core components, requiring accurate localization and\nestimation. In this paper, we propose a simple but efficient forward vehicle\ncollision warning framework by aggregating monocular distance measurement and\nprecise vehicle detection. In order to obtain forward vehicle distance, a quick\ncamera calibration method which only needs three physical points to calibrate\nrelated camera parameters is utilized. As for the forward vehicle detection, a\nmulti-scale detection algorithm that regards the result of calibration as\ndistance priori is proposed to improve the precision. Intensive experiments are\nconducted in our established real scene dataset and the results have\ndemonstrated the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:39:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lu", "Yuwei", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1904.12659", "submitter": "Maosen Li", "authors": "Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Actional-Structural Graph Convolutional Networks for Skeleton-based\n  Action Recognition", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition with skeleton data has recently attracted much attention\nin computer vision. Previous studies are mostly based on fixed skeleton graphs,\nonly capturing local physical dependencies among joints, which may miss\nimplicit joint correlations. To capture richer dependencies, we introduce an\nencoder-decoder structure, called A-link inference module, to capture\naction-specific latent dependencies, i.e. actional links, directly from\nactions. We also extend the existing skeleton graphs to represent higher-order\ndependencies, i.e. structural links. Combing the two types of links into a\ngeneralized skeleton graph, we further propose the actional-structural graph\nconvolution network (AS-GCN), which stacks actional-structural graph\nconvolution and temporal convolution as a basic building block, to learn both\nspatial and temporal features for action recognition. A future pose prediction\nhead is added in parallel to the recognition head to help capture more detailed\naction patterns through self-supervision. We validate AS-GCN in action\nrecognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed\nAS-GCN achieves consistently large improvement compared to the state-of-the-art\nmethods. As a side product, AS-GCN also shows promising results for future pose\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:20:07 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Maosen", ""], ["Chen", "Siheng", ""], ["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1904.12690", "submitter": "Ruairidh Battleday", "authors": "Ruairidh M. Battleday, Joshua C. Peterson, and Thomas L. Griffiths", "title": "Capturing human categorization of natural images at scale by combining\n  deep networks and cognitive models", "comments": "29 pages; 4 figures. arXiv admin note: text overlap with\n  arXiv:1711.04855", "journal-ref": null, "doi": "10.1038/s41467-020-18946-z", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human categorization is one of the most important and successful targets of\ncognitive modeling in psychology, yet decades of development and assessment of\ncompeting models have been contingent on small sets of simple, artificial\nexperimental stimuli. Here we extend this modeling paradigm to the domain of\nnatural images, revealing the crucial role that stimulus representation plays\nin categorization and its implications for conclusions about how people form\ncategories. Applying psychological models of categorization to natural images\nrequired two significant advances. First, we conducted the first large-scale\nexperimental study of human categorization, involving over 500,000 human\ncategorization judgments of 10,000 natural images from ten non-overlapping\nobject categories. Second, we addressed the traditional bottleneck of\nrepresenting high-dimensional images in cognitive models by exploring the best\nof current supervised and unsupervised deep and shallow machine learning\nmethods. We find that selecting sufficiently expressive, data-driven\nrepresentations is crucial to capturing human categorization, and using these\nrepresentations allows simple models that represent categories with abstract\nprototypes to outperform the more complex memory-based exemplar accounts of\ncategorization that have dominated in studies using less naturalistic stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:47:59 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Battleday", "Ruairidh M.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1904.12691", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Shimon Whiteson", "title": "DAC: The Double Actor-Critic Architecture for Learning Options", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reformulate the option framework as two parallel augmented MDPs. Under\nthis novel formulation, all policy optimization algorithms can be used off the\nshelf to learn intra-option policies, option termination conditions, and a\nmaster policy over options. We apply an actor-critic algorithm on each\naugmented MDP, yielding the Double Actor-Critic (DAC) architecture.\nFurthermore, we show that, when state-value functions are used as critics, one\ncritic can be expressed in terms of the other, and hence only one critic is\nnecessary. We conduct an empirical study on challenging robot simulation tasks.\nIn a transfer learning setting, DAC outperforms both its hierarchy-free\ncounterpart and previous gradient-based option learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:57:47 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:38:00 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:23:29 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 14:54:53 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 15:37:19 GMT"}, {"version": "v6", "created": "Tue, 10 Sep 2019 08:29:53 GMT"}, {"version": "v7", "created": "Wed, 11 Sep 2019 08:34:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zhang", "Shangtong", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1904.12738", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Danilo Vasconcellos Vargas and Venkanna U", "title": "Self Training Autonomous Driving Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically, driving is a Markov Decision Process which suits well the\nreinforcement learning paradigm. In this paper, we propose a novel agent which\nlearns to drive a vehicle without any human assistance. We use the concept of\nreinforcement learning and evolutionary strategies to train our agent in a 2D\nsimulation environment. Our model's architecture goes beyond the World Model's\nby introducing difference images in the auto encoder. This novel involvement of\ndifference images in the auto-encoder gives better representation of the latent\nspace with respect to the motion of vehicle and helps an autonomous agent to\nlearn more efficiently how to drive a vehicle. Results show that our method\nrequires fewer (96% less) total agents, (87.5% less) agents per generations,\n(70% less) generations and (90% less) rollouts than the original architecture\nwhile achieving the same accuracy of the original.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 05:22:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""], ["U", "Venkanna", ""]]}, {"id": "1904.12795", "submitter": "Anna Fr\\\"uhst\\\"uck", "authors": "Anna Fr\\\"uhst\\\"uck and Ibraheem Alhashim and Peter Wonka", "title": "TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures", "comments": "Code is available at http://github.com/afruehstueck/tileGAN", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)", "doi": "10.1145/3306346.3322993", "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of texture synthesis in the setting where many input\nimages are given and a large-scale output is required. We build on recent\ngenerative adversarial networks and propose two extensions in this paper.\nFirst, we propose an algorithm to combine outputs of GANs trained on a smaller\nresolution to produce a large-scale plausible texture map with virtually no\nboundary artifacts. Second, we propose a user interface to enable artistic\ncontrol. Our quantitative and qualitative results showcase the generation of\nsynthesized high-resolution maps consisting of up to hundreds of megapixels as\na case in point.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:15:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fr\u00fchst\u00fcck", "Anna", ""], ["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "1904.12796", "submitter": "Xin Xin", "authors": "Xin Xin, Xiangnan He, Yongfeng Zhang, Yongdong Zhang, Joemon Jose", "title": "Relational Collaborative Filtering:Modeling Multiple Item Relations for\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing item-based collaborative filtering (ICF) methods leverage only the\nrelation of collaborative similarity. Nevertheless, there exist multiple\nrelations between items in real-world scenarios. Distinct from the\ncollaborative similarity that implies co-interact patterns from the user\nperspective, these relations reveal fine-grained knowledge on items from\ndifferent perspectives of meta-data, functionality, etc. However, how to\nincorporate multiple item relations is less explored in recommendation\nresearch. In this work, we propose Relational Collaborative Filtering (RCF), a\ngeneral framework to exploit multiple relations between items in recommender\nsystem. We find that both the relation type and the relation value are crucial\nin inferring user preference. To this end, we develop a two-level hierarchical\nattention mechanism to model user preference. The first-level attention\ndiscriminates which types of relations are more important, and the second-level\nattention considers the specific relation values to estimate the contribution\nof a historical item in recommending the target item. To make the item\nembeddings be reflective of the relational structure between items, we further\nformulate a task to preserve the item relations, and jointly train it with the\nrecommendation task of preference modeling. Empirical results on two real\ndatasets demonstrate the strong performance of RCF. Furthermore, we also\nconduct qualitative analyses to show the benefits of explanations brought by\nthe modeling of multiple item relations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:20:23 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 16:01:19 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 15:24:03 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Xin", "Xin", ""], ["He", "Xiangnan", ""], ["Zhang", "Yongfeng", ""], ["Zhang", "Yongdong", ""], ["Jose", "Joemon", ""]]}, {"id": "1904.12848", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le", "title": "Unsupervised Data Augmentation for Consistency Training", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning lately has shown much promise in improving deep\nlearning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to\nconstrain model predictions to be invariant to input noise. In this work, we\npresent a new perspective on how to effectively noise unlabeled examples and\nargue that the quality of noising, specifically those produced by advanced data\naugmentation methods, plays a crucial role in semi-supervised learning. By\nsubstituting simple noising operations with advanced data augmentation methods\nsuch as RandAugment and back-translation, our method brings substantial\nimprovements across six language and three vision tasks under the same\nconsistency training framework. On the IMDb text classification dataset, with\nonly 20 labeled examples, our method achieves an error rate of 4.20,\noutperforming the state-of-the-art model trained on 25,000 labeled examples. On\na standard semi-supervised learning benchmark, CIFAR-10, our method outperforms\nall previous approaches and achieves an error rate of 5.43 with only 250\nexamples. Our method also combines well with transfer learning, e.g., when\nfinetuning from BERT, and yields improvements in high-data regime, such as\nImageNet, whether when there is only 10% labeled data or when a full labeled\nset with 1.3M extra unlabeled examples is used. Code is available at\nhttps://github.com/google-research/uda.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 17:56:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:53:48 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 15:32:11 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 15:40:40 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 17:58:43 GMT"}, {"version": "v6", "created": "Thu, 5 Nov 2020 15:11:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""], ["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""]]}, {"id": "1904.12901", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Daniel Mankowitz, Todd Hester", "title": "Challenges of Real-World Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has proven its worth in a series of artificial\ndomains, and is beginning to show some successes in real-world scenarios.\nHowever, much of the research advances in RL are often hard to leverage in\nreal-world systems due to a series of assumptions that are rarely satisfied in\npractice. We present a set of nine unique challenges that must be addressed to\nproductionize RL to real world problems. For each of these challenges, we\nspecify the exact meaning of the challenge, present some approaches from the\nliterature, and specify some metrics for evaluating that challenge. An approach\nthat addresses all nine challenges would be applicable to a large number of\nreal world problems. We also present an example domain that has been modified\nto present these challenges as a testbed for practical RL research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:40:15 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Mankowitz", "Daniel", ""], ["Hester", "Todd", ""]]}, {"id": "1904.12907", "submitter": "Haonan Chen", "authors": "Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, Ron Alterovitz", "title": "Enabling Robots to Understand Incomplete Natural Language Instructions\n  Using Commonsense Reasoning", "comments": "7 pages, 4 figures, ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling robots to understand instructions provided via spoken natural\nlanguage would facilitate interaction between robots and people in a variety of\nsettings in homes and workplaces. However, natural language instructions are\noften missing information that would be obvious to a human based on\nenvironmental context and common sense, and hence does not need to be\nexplicitly stated. In this paper, we introduce Language-Model-based Commonsense\nReasoning (LMCR), a new method which enables a robot to listen to a natural\nlanguage instruction from a human, observe the environment around it, and\nautomatically fill in information missing from the instruction using\nenvironmental context and a new commonsense reasoning approach. Our approach\nfirst converts an instruction provided as unconstrained natural language into a\nform that a robot can understand by parsing it into verb frames. Our approach\nthen fills in missing information in the instruction by observing objects in\nits vicinity and leveraging commonsense reasoning. To learn commonsense\nreasoning automatically, our approach distills knowledge from large\nunstructured textual corpora by training a language model. Our results show the\nfeasibility of a robot learning commonsense knowledge automatically from\nweb-based textual corpora, and the power of learned commonsense reasoning\nmodels in enabling a robot to autonomously perform tasks based on incomplete\nnatural language instructions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:59:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:47:13 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chen", "Haonan", ""], ["Tan", "Hao", ""], ["Kuntz", "Alan", ""], ["Bansal", "Mohit", ""], ["Alterovitz", "Ron", ""]]}, {"id": "1904.12937", "submitter": "Manuel Baltieri Mr", "authors": "Manuel Baltieri, Christopher L. Buckley", "title": "Generative models as parsimonious descriptions of sensorimotor loops", "comments": "Commentary on Brette (2019) https://doi.org/10.1017/S0140525X19000049", "journal-ref": "Behav Brain Sci 42 (2019) e218", "doi": "10.1017/S0140525X19001353", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian brain hypothesis, predictive processing and variational free\nenergy minimisation are typically used to describe perceptual processes based\non accurate generative models of the world. However, generative models need not\nbe veridical representations of the environment. We suggest that they can (and\nshould) be used to describe sensorimotor relationships relevant for behaviour\nrather than precise accounts of the world.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:27:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Baltieri", "Manuel", ""], ["Buckley", "Christopher L.", ""]]}, {"id": "1904.12958", "submitter": "Cheol Young Park", "authors": "Cheol Young Park, Shou Matsumoto, Jubyung Ha, YoungWon Park", "title": "Predictive Situation Awareness for Ebola Virus Disease using a\n  Collective Intelligence Multi-Model Integration Platform: Bayes Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The humanity has been facing a plethora of challenges associated with\ninfectious diseases, which kill more than 6 million people a year. Although\ncontinuous efforts have been applied to relieve the potential damages from such\nmisfortunate events, it is unquestionable that there are many persisting\nchallenges yet to overcome. One related issue we particularly address here is\nthe assessment and prediction of such epidemics. In this field of study,\ntraditional and ad-hoc models frequently fail to provide proper predictive\nsituation awareness (PSAW), characterized by understanding the current\nsituations and predicting the future situations. Comprehensive PSAW for\ninfectious disease can support decision making and help to hinder disease\nspread. In this paper, we develop a computing system platform focusing on\ncollective intelligence causal modeling, in order to support PSAW in the domain\nof infectious disease. Analyses of global epidemics require integration of\nmultiple different data and models, which can be originated from multiple\nindependent researchers. These models should be integrated to accurately assess\nand predict the infectious disease in terms of holistic view. The system shall\nprovide three main functions: (1) collaborative causal modeling, (2) causal\nmodel integration, and (3) causal model reasoning. These functions are\nsupported by subject-matter expert and artificial intelligence (AI), with\nuncertainty treatment. Subject-matter experts, as collective intelligence,\ndevelop causal models and integrate them as one joint causal model. The\nintegrated causal model shall be used to reason about: (1) the past, regarding\nhow the causal factors have occurred; (2) the present, regarding how the spread\nis going now; and (3) the future, regarding how it will proceed. Finally, we\nintroduce one use case of predictive situation awareness for the Ebola virus\ndisease.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 21:41:18 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 12:47:40 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Park", "Cheol Young", ""], ["Matsumoto", "Shou", ""], ["Ha", "Jubyung", ""], ["Park", "YoungWon", ""]]}, {"id": "1904.12991", "submitter": "Yiming Sun", "authors": "Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, Madeleine Udell", "title": "\"Why Should You Trust My Explanation?\" Understanding Uncertainty in LIME\n  Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for interpreting machine learning black-box models increase the\noutcomes' transparency and in turn generates insight into the reliability and\nfairness of the algorithms. However, the interpretations themselves could\ncontain significant uncertainty that undermines the trust in the outcomes and\nraises concern about the model's reliability. Focusing on the method \"Local\nInterpretable Model-agnostic Explanations\" (LIME), we demonstrate the presence\nof two sources of uncertainty, namely the randomness in its sampling procedure\nand the variation of interpretation quality across different input data points.\nSuch uncertainty is present even in models with high training and test\naccuracy. We apply LIME to synthetic data and two public data sets, text\nclassification in 20 Newsgroup and recidivism risk-scoring in COMPAS, to\nsupport our argument.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 23:49:19 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:46:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Yujia", ""], ["Song", "Kuangyan", ""], ["Sun", "Yiming", ""], ["Tan", "Sarah", ""], ["Udell", "Madeleine", ""]]}, {"id": "1904.13006", "submitter": "Naman Shah", "authors": "Naman Shah, Deepak Kala Vasudevan, Kislay Kumar, Pranav Kamojjhala,\n  Siddharth Srivastava", "title": "Anytime Integrated Task and Motion Policies for Stochastic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to solve complex, long-horizon tasks, intelligent robots need to\ncarry out high-level, abstract planning and reasoning in conjunction with\nmotion planning. However, abstract models are typically lossy and plans or\npolicies computed using them can be unexecutable. These problems are\nexacerbated in stochastic situations where the robot needs to reason about, and\nplan for multiple contingencies.\n  We present a new approach for integrated task and motion planning in\nstochastic settings. In contrast to prior work in this direction, we show that\nour approach can effectively compute integrated task and motion policies whose\nbranching structures encoding agent behaviors handling multiple execution-time\ncontingencies. We prove that our algorithm is probabilistically complete and\ncan compute feasible solution policies in an anytime fashion so that the\nprobability of encountering an unresolved contingency decreases over time.\nEmpirical results on a set of challenging problems show the utility and scope\nof our methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 01:15:02 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 20:26:07 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 22:52:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Shah", "Naman", ""], ["Vasudevan", "Deepak Kala", ""], ["Kumar", "Kislay", ""], ["Kamojjhala", "Pranav", ""], ["Srivastava", "Siddharth", ""]]}, {"id": "1904.13015", "submitter": "Sanghyun Yi", "authors": "Sanghyun Yi, Rahul Goel, Chandra Khatri, Alessandra Cervone, Tagyoung\n  Chung, Behnam Hedayatnia, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur", "title": "Towards Coherent and Engaging Spoken Dialog Response Generation Using\n  Automatic Conversation Evaluators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder based neural architectures serve as the basis of\nstate-of-the-art approaches in end-to-end open domain dialog systems. Since\nmost of such systems are trained with a maximum likelihood~(MLE) objective they\nsuffer from issues such as lack of generalizability and the generic response\nproblem, i.e., a system response that can be an answer to a large number of\nuser utterances, e.g., \"Maybe, I don't know.\" Having explicit feedback on the\nrelevance and interestingness of a system response at each turn can be a useful\nsignal for mitigating such issues and improving system quality by selecting\nresponses from different approaches. Towards this goal, we present a system\nthat evaluates chatbot responses at each dialog turn for coherence and\nengagement. Our system provides explicit turn-level dialog quality feedback,\nwhich we show to be highly correlated with human evaluation. To show that\nincorporating this feedback in the neural response generation models improves\ndialog quality, we present two different and complementary mechanisms to\nincorporate explicit feedback into a neural response generation model:\nreranking and direct modification of the loss function during training. Our\nstudies show that a response generation model that incorporates these combined\nfeedback mechanisms produce more engaging and coherent responses in an\nopen-domain spoken dialog setting, significantly improving the response quality\nusing both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:03:05 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 18:50:50 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2019 18:24:42 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 03:06:41 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yi", "Sanghyun", ""], ["Goel", "Rahul", ""], ["Khatri", "Chandra", ""], ["Cervone", "Alessandra", ""], ["Chung", "Tagyoung", ""], ["Hedayatnia", "Behnam", ""], ["Venkatesh", "Anu", ""], ["Gabriel", "Raefer", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1904.13078", "submitter": "Masanari Kimura", "authors": "Masanari Kimura, Masayuki Tanaka", "title": "Interpretation of Feature Space using Multi-Channel Attentional\n  Sub-Networks", "comments": "CVPR2019 Workshop on Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved impressive results in various\ntasks, but interpreting the internal mechanism is a challenging problem. To\ntackle this problem, we exploit a multi-channel attention mechanism in feature\nspace. Our network architecture allows us to obtain an attention mask for each\nfeature while existing CNN visualization methods provide only a common\nattention mask for all features. We apply the proposed multi-channel attention\nmechanism to multi-attribute recognition task. We can obtain different\nattention mask for each feature and for each attribute. Those analyses give us\ndeeper insight into the feature space of CNNs. The experimental results for the\nbenchmark dataset show that the proposed method gives high interpretability to\nhumans while accurately grasping the attributes of the data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:13:52 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kimura", "Masanari", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "1904.13101", "submitter": "Amjad Ibrahim", "authors": "Amjad Ibrahim, Simon Rehwald, Alexander Pretschner", "title": "Efficiently Checking Actual Causality with SAT Solving", "comments": "18 pages, In: Dependable Software Systems Engineering, p. to appear\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent formal approaches towards causality have made the concept ready for\nincorporation into the technical world. However, causality reasoning is\ncomputationally hard; and no general algorithmic approach exists that\nefficiently infers the causes for effects. Thus, checking causality in the\ncontext of complex, multi-agent, and distributed socio-technical systems is a\nsignificant challenge. Therefore, we conceptualize an intelligent and novel\nalgorithmic approach towards checking causality in acyclic causal models with\nbinary variables, utilizing the optimization power in the solvers of the\nBoolean Satisfiability Problem (SAT). We present two SAT encodings, and an\nempirical evaluation of their efficiency and scalability. We show that\ncausality is computed efficiently in less than 5 seconds for models that\nconsist of more than 4000 variables.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 08:38:10 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ibrahim", "Amjad", ""], ["Rehwald", "Simon", ""], ["Pretschner", "Alexander", ""]]}, {"id": "1904.13102", "submitter": "Jinqiang Bai", "authors": "Zhaoxiang Liu, Zezhou Chen, Jinqiang Bai, Shaohua Li, Shiguo Lian", "title": "Facial Pose Estimation by Deep Learning from Label Distributions", "comments": "9 pages,5 figures, Accepted by ICCV 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial pose estimation has gained a lot of attentions in many practical\napplications, such as human-robot interaction, gaze estimation and driver\nmonitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is\nbecoming more and more popular. However, facial pose estimation suffers from a\nkey challenge: the lack of sufficient training data for many poses, especially\nfor large poses. Inspired by the observation that the faces under close poses\nlook similar, we reformulate the facial pose estimation as a label distribution\nlearning problem, considering each face image as an example associated with a\nGaussian label distribution rather than a single label, and construct a\nconvolutional neural network which is trained with a multi-loss function on\nAFLW dataset and 300W-LP dataset to predict the facial poses directly from\ncolor image. Extensive experiments are conducted on several popular benchmarks,\nincluding AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant\nadvantage over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 08:38:20 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:27:03 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 08:11:31 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 02:48:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Zhaoxiang", ""], ["Chen", "Zezhou", ""], ["Bai", "Jinqiang", ""], ["Li", "Shaohua", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.13164", "submitter": "Yeting Li", "authors": "Chunmei Dong and Yeting Li and Haiming Chen", "title": "Learning Restricted Regular Expressions with Interleaving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advantages for the presence of an XML schema for XML documents are\nnumerous. However, many XML documents in practice are not accompanied by a\nschema or by a valid schema. Relax NG is a popular and powerful schema\nlanguage, which supports the unconstrained interleaving operator. Focusing on\nthe inference of Relax NG, we propose a new subclass of regular expressions\nwith interleaving and design a polynomial inference algorithm. Then we\nconducted a series of experiments based on large-scale real data and on three\nXML data corpora, and experimental results show that our subclass has a better\npracticality than previous ones, and the regular expressions inferred by our\nalgorithm are more precise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 11:29:00 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dong", "Chunmei", ""], ["Li", "Yeting", ""], ["Chen", "Haiming", ""]]}, {"id": "1904.13173", "submitter": "Erisa Karafili", "authors": "Erisa Karafili, Linna Wang, Emil C. Lupu", "title": "An Argumentation-Based Reasoner to Assist Digital Investigation and\n  Attribution of Cyber-Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expect an increase in the frequency and severity of cyber-attacks that\ncomes along with the need for efficient security countermeasures. The process\nof attributing a cyber-attack helps to construct efficient and targeted\nmitigating and preventive security measures. In this work, we propose an\nargumentation-based reasoner (ABR) as a proof-of-concept tool that can help a\nforensics analyst during the analysis of forensic evidence and the attribution\nprocess. Given the evidence collected from a cyber-attack, our reasoner can\nassist the analyst during the investigation process, by helping him/her to\nanalyze the evidence and identify who performed the attack. Furthermore, it\nsuggests to the analyst where to focus further analyses by giving hints of the\nmissing evidence or new investigation paths to follow. ABR is the first\nautomatic reasoner that can combine both technical and social evidence in the\nanalysis of a cyber-attack, and that can also cope with incomplete and\nconflicting information. To illustrate how ABR can assist in the analysis and\nattribution of cyber-attacks we have used examples of cyber-attacks and their\nanalyses as reported in publicly available reports and online literature. We do\nnot mean to either agree or disagree with the analyses presented therein or\nreach attribution conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 11:42:44 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 10:17:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Karafili", "Erisa", ""], ["Wang", "Linna", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1904.13196", "submitter": "Marjan Alirezaie", "authors": "Marjan Alirezaie, Martin L\\\"angkvist, Michael Sioutis, Amy Loutfi", "title": "Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding why machine learning algorithms may fail is usually the task of\nthe human expert that uses domain knowledge and contextual information to\ndiscover systematic shortcomings in either the data or the algorithm. In this\npaper, we propose a semantic referee, which is able to extract qualitative\nfeatures of the errors emerging from deep machine learning frameworks and\nsuggest corrections. The semantic referee relies on ontological reasoning about\nspatial knowledge in order to characterize errors in terms of their spatial\nrelations with the environment. Using semantics, the reasoner interacts with\nthe learning algorithm as a supervisor. In this paper, the proposed method of\nthe interaction between a neural network classifier and a semantic referee\nshows how to improve the performance of semantic segmentation for satellite\nimagery data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 12:44:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Alirezaie", "Marjan", ""], ["L\u00e4ngkvist", "Martin", ""], ["Sioutis", "Michael", ""], ["Loutfi", "Amy", ""]]}, {"id": "1904.13215", "submitter": "Corina P\\u{a}s\\u{a}reanu", "authors": "Divya Gopinath, Hayes Converse, Corina S. Pasareanu and Ankur Taly", "title": "Property Inference for Deep Neural Networks", "comments": "Errata: This version updates the ASE'19 conference version by\n  correcting the definition of the three properties that were checked for\n  ACASXU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for automatically inferring formal properties of\nfeed-forward neural networks. We observe that a significant part (if not all)\nof the logic of feed forward networks is captured in the activation status\n('on' or 'off') of its neurons. We propose to extract patterns based on neuron\ndecisions as preconditions that imply certain desirable output property e.g.,\nthe prediction being a certain class. We present techniques to extract input\nproperties, encoding convex predicates on the input space that imply given\noutput properties and layer properties, representing network properties\ncaptured in the hidden layers that imply the desired output behavior. We apply\nour techniques on networks for the MNIST and ACASXU applications. Our\nexperiments highlight the use of the inferred properties in a variety of tasks,\nsuch as explaining predictions, providing robustness guarantees, simplifying\nproofs, and network distillation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:37:21 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 21:25:04 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 22:32:54 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gopinath", "Divya", ""], ["Converse", "Hayes", ""], ["Pasareanu", "Corina S.", ""], ["Taly", "Ankur", ""]]}, {"id": "1904.13223", "submitter": "Morteza Haghir Chehreghani", "authors": "Morteza Haghir Chehreghani", "title": "Unsupervised Representation Learning with Minimax Distance Measures", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Minimax distances to extract in a nonparametric way\nthe features that capture the unknown underlying patterns and structures in the\ndata. We develop a general-purpose and computationally efficient framework to\nemploy Minimax distances with many machine learning methods that perform on\nnumerical data. We study both computing the pairwise Minimax distances for all\npairs of objects and as well as computing the Minimax distances of all the\nobjects to/from a fixed (test) object.\n  We first efficiently compute the pairwise Minimax distances between the\nobjects, using the equivalence of Minimax distances over a graph and over a\nminimum spanning tree constructed on that. Then, we perform an embedding of the\npairwise Minimax distances into a new vector space, such that their squared\nEuclidean distances in the new space equal to the pairwise Minimax distances in\nthe original space. We also study the case of having multiple pairwise Minimax\nmatrices, instead of a single one. Thereby, we propose an embedding via first\nsumming up the centered matrices and then performing an eigenvalue\ndecomposition to obtain the relevant features.\n  In the following, we study computing Minimax distances from a fixed (test)\nobject which can be used for instance in K-nearest neighbor search. Similar to\nthe case of all-pair pairwise Minimax distances, we develop an efficient and\ngeneral-purpose algorithm that is applicable with any arbitrary base distance\nmeasure. Moreover, we investigate in detail the edges selected by the Minimax\ndistances and thereby explore the ability of Minimax distances in detecting\noutlier objects.\n  Finally, for each setting, we perform several experiments to demonstrate the\neffectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 16:13:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 12:08:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Chehreghani", "Morteza Haghir", ""]]}, {"id": "1904.13233", "submitter": "Daniel Cunnington", "authors": "Daniel Cunnington, Graham White, Geeth de Mel", "title": "Synthetic Ground Truth Generation for Evaluating Generative Policy\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Policy-based Models aim to enable a coalition of systems, be they\ndevices or services to adapt according to contextual changes such as\nenvironmental factors, user preferences and different tasks whilst adhering to\nvarious constraints and regulations as directed by a managing party or the\ncollective vision of the coalition. Recent developments have proposed new\narchitectures to realize the potential of GPMs but as the complexity of systems\nand their associated requirements increases, there is an emerging requirement\nto have scenarios and associated datasets to realistically evaluate GPMs with\nrespect to the properties of the operating environment, be it the future\nbattlespace or an autonomous organization. In order to address this\nrequirement, in this paper, we present a method of applying an agile knowledge\nrepresentation framework to model requirements, both individualistic and\ncollective that enables synthetic generation of ground truth data such that\nadvanced GPMs can be evaluated robustly in complex environments. We also\nrelease conceptual models, annotated datasets, as well as means to extend the\ndata generation approach so that similar datasets can be developed for varying\ncomplexities and different situations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:41:58 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Cunnington", "Daniel", ""], ["White", "Graham", ""], ["de Mel", "Geeth", ""]]}, {"id": "1904.13247", "submitter": "Durdane Kocacoban", "authors": "Durdane Kocacoban, James Cussens", "title": "Online Causal Structure Learning in the Presence of Latent Variables", "comments": "16 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two online causal structure learning algorithms which can track\nchanges in a causal structure and process data in a dynamic real-time manner.\nStandard causal structure learning algorithms assume that causal structure does\nnot change during the data collection process, but in real-world scenarios, it\ndoes often change. Therefore, it is inappropriate to handle such changes with\nexisting batch-learning approaches, and instead, a structure should be learned\nin an online manner. The online causal structure learning algorithms we present\nhere can revise correlation values without reprocessing the entire dataset and\nuse an existing model to avoid relearning the causal links in the prior model,\nwhich still fit data. Proposed algorithms are tested on synthetic and\nreal-world datasets, the latter being a seasonally adjusted commodity price\nindex dataset for the U.S. The online causal structure learning algorithms\noutperformed standard FCI by a large margin in learning the changed causal\nstructure correctly and efficiently when latent variables were present.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:49:43 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 18:17:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kocacoban", "Durdane", ""], ["Cussens", "James", ""]]}, {"id": "1904.13255", "submitter": "Kacper Kielak", "authors": "Kacper Kielak", "title": "Generative Adversarial Imagination for Sample Efficient Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning has seen great advancements in the past five years.\nThe successful introduction of deep learning in place of more traditional\nmethods allowed reinforcement learning to scale to very complex domains\nachieving super-human performance in environments like the game of Go or\nnumerous video games. Despite great successes in multiple domains, these new\nmethods suffer from their own issues that make them often inapplicable to the\nreal world problems. Extreme lack of data efficiency, together with huge\nvariance and difficulty in enforcing safety constraints, is one of the three\nmost prominent issues in the field. Usually, millions of data points sampled\nfrom the environment are necessary for these algorithms to converge to\nacceptable policies.\n  This thesis proposes novel Generative Adversarial Imaginative Reinforcement\nLearning algorithm. It takes advantage of the recent introduction of highly\neffective generative adversarial models, and Markov property that underpins\nreinforcement learning setting, to model dynamics of the real environment\nwithin the internal imagination module. Rollouts from the imagination are then\nused to artificially simulate the real environment in a standard reinforcement\nlearning process to avoid, often expensive and dangerous, trial and error in\nthe real environment. Experimental results show that the proposed algorithm\nmore economically utilises experience from the real environment than the\ncurrent state-of-the-art Rainbow DQN algorithm, and thus makes an important\nstep towards sample efficient deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:53:29 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 18:34:54 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kielak", "Kacper", ""]]}, {"id": "1904.13308", "submitter": "Oleh Andriichuk", "authors": "Oleh Dmytrenko, Dmitry Lande, Oleh Andriichuk", "title": "Method for Searching of an Optimal Scenario of Impact in Cognitive Maps\n  during Information Operations Recognition", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of choosing the optimal scenario of\nthe impact between nodes based on of the introduced criteria for the optimality\nof the impact. Two criteria for the optimality of the impact, which are called\nthe force of impact and the speed of implementation of the scenario, are\nconsidered. To obtain a unique solution of the problem, a multi-criterial\nassessment of the received scenarios using the Pareto principle was applied.\nBased on the criteria of a force of impact and the speed of implementation of\nthe scenario, the choice of the optimal scenario of impact was justified. The\nresults and advantages of the proposed approach in comparison with the Kosko\nmodel are presented.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:58:05 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dmytrenko", "Oleh", ""], ["Lande", "Dmitry", ""], ["Andriichuk", "Oleh", ""]]}, {"id": "1904.13310", "submitter": "Hojjat Salehinejad", "authors": "Alex Labach, Hojjat Salehinejad, Shahrokh Valaee", "title": "Survey of Dropout Methods for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout methods are a family of stochastic techniques used in neural network\ntraining or inference that have generated significant research interest and are\nwidely used in practice. They have been successfully applied in neural network\nregularization, model compression, and in measuring the uncertainty of neural\nnetwork outputs. While original formulated for dense neural network layers,\nrecent advances have made dropout methods also applicable to convolutional and\nrecurrent neural network layers. This paper summarizes the history of dropout\nmethods, their various applications, and current areas of research interest.\nImportant proposed methods are described in additional detail.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:21:52 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 04:36:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Labach", "Alex", ""], ["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "1904.13316", "submitter": "Teresa Scantamburlo", "authors": "Nello Cristianini and Teresa Scantamburlo", "title": "On Social Machines for Algorithmic Regulation", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous mechanisms have been proposed to regulate certain aspects of\nsociety and are already being used to regulate business organisations. We take\nseriously recent proposals for algorithmic regulation of society, and we\nidentify the existing technologies that can be used to implement them, most of\nthem originally introduced in business contexts. We build on the notion of\n'social machine' and we connect it to various ongoing trends and ideas,\nincluding crowdsourced task-work, social compiler, mechanism design, reputation\nmanagement systems, and social scoring. After showing how all the building\nblocks of algorithmic regulation are already well in place, we discuss possible\nimplications for human autonomy and social order. The main contribution of this\npaper is to identify convergent social and technical trends that are leading\ntowards social regulation by algorithms, and to discuss the possible social,\npolitical, and ethical consequences of taking this path.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:31:09 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Cristianini", "Nello", ""], ["Scantamburlo", "Teresa", ""]]}, {"id": "1904.13324", "submitter": "Pedro Zuidberg Dos Martires", "authors": "Ozan Arkan Can and Pedro Zuidberg Dos Martires and Andreas Persson and\n  Julian Gaal and Amy Loutfi and Luc De Raedt and Deniz Yuret and Alessandro\n  Saffiotti", "title": "Learning from Implicit Information in Natural Language Instructions for\n  Robotic Manipulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction often occurs in the form of instructions given from a\nhuman to a robot. For a robot to successfully follow instructions, a common\nrepresentation of the world and objects in it should be shared between humans\nand the robot so that the instructions can be grounded. Achieving this\nrepresentation can be done via learning, where both the world representation\nand the language grounding are learned simultaneously. However, in robotics\nthis can be a difficult task due to the cost and scarcity of data. In this\npaper, we tackle the problem by separately learning the world representation of\nthe robot and the language grounding. While this approach can address the\nchallenges in getting sufficient data, it may give rise to inconsistencies\nbetween both learned components. Therefore, we further propose Bayesian\nlearning to resolve such inconsistencies between the natural language grounding\nand a robot's world representation by exploiting spatio-relational information\nthat is implicitly present in instructions given by a human. Moreover, we\ndemonstrate the feasibility of our approach on a scenario involving a robotic\narm in the physical world.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:37:17 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Can", "Ozan Arkan", ""], ["Martires", "Pedro Zuidberg Dos", ""], ["Persson", "Andreas", ""], ["Gaal", "Julian", ""], ["Loutfi", "Amy", ""], ["De Raedt", "Luc", ""], ["Yuret", "Deniz", ""], ["Saffiotti", "Alessandro", ""]]}, {"id": "1904.13333", "submitter": "Gerard Serra", "authors": "Gerard Serra and David Miralles", "title": "Coevo: a collaborative design platform with artificial agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Coevo, an online platform that allows both humans and artificial\nagents to design shapes that solve different tasks. Our goal is to explore\ncommon shared design tools that can be used by humans and artificial agents in\na context of creation. This approach can provide a better knowledge transfer\nand interaction with artificial agents since a common language of design is\ndefined. In this paper, we outline the main components of this platform and\ndiscuss the definition of a human-centered language to enhance human-AI\ncollaboration in co-creation scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:55:34 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Serra", "Gerard", ""], ["Miralles", "David", ""]]}, {"id": "1904.13389", "submitter": "Lin Chen", "authors": "MohammadHossein Bateni, Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab\n  S. Mirrokni, Afshin Rostamizadeh", "title": "Categorical Feature Compression via Submodular Optimization", "comments": "Accepted to ICML 2019. Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, learning from categorical features with very large\nvocabularies (e.g., 28 million for the Criteo click prediction dataset) has\nbecome a practical challenge for machine learning researchers and\npractitioners. We design a highly-scalable vocabulary compression algorithm\nthat seeks to maximize the mutual information between the compressed\ncategorical feature and the target binary labels and we furthermore show that\nits solution is guaranteed to be within a $1-1/e \\approx 63\\%$ factor of the\nglobal optimal solution. To achieve this, we introduce a novel\nre-parametrization of the mutual information objective, which we prove is\nsubmodular, and design a data structure to query the submodular function in\namortized $O(\\log n )$ time (where $n$ is the input vocabulary size). Our\ncomplete algorithm is shown to operate in $O(n \\log n )$ time. Additionally, we\ndesign a distributed implementation in which the query data structure is\ndecomposed across $O(k)$ machines such that each machine only requires $O(\\frac\nn k)$ space, while still preserving the approximation guarantee and using only\nlogarithmic rounds of computation. We also provide analysis of simple\nalternative heuristic compression methods to demonstrate they cannot achieve\nany approximation guarantee. Using the large-scale Criteo learning task, we\ndemonstrate better performance in retaining mutual information and also verify\ncompetitive learning performance compared to other baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:45:13 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Bateni", "MohammadHossein", ""], ["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Thomas", ""], ["Mirrokni", "Vahab S.", ""], ["Rostamizadeh", "Afshin", ""]]}]