[{"id": "2011.00057", "submitter": "Nishant Raj", "authors": "Harshit Jain and Nishant Raj and Suyash Mishra", "title": "A Sui Generis QA Approach using RoBERTa for Adverse Drug Event\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extraction of adverse drug events from biomedical literature and other\ntextual data is an important component to monitor drug-safety and this has\nattracted attention of many researchers in healthcare. Existing works are more\npivoted around entity-relation extraction using bidirectional long short term\nmemory networks (Bi-LSTM) which does not attain the best feature\nrepresentations. In this paper, we introduce a question answering framework\nthat exploits the robustness, masking and dynamic attention capabilities of\nRoBERTa by a technique of domain adaptation and attempt to overcome the\naforementioned limitations. Our model outperforms the prior work by 9.53%\nF1-Score.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:09:48 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Jain", "Harshit", ""], ["Raj", "Nishant", ""], ["Mishra", "Suyash", ""]]}, {"id": "2011.00073", "submitter": "Mohamad Nasr-Azadani", "authors": "Yao Yang, Andrew Nam, Mohamad M. Nasr-Azadani, Teresa Tung", "title": "Resource-Aware Pareto-Optimal Automated Machine Learning Platform", "comments": "Accepted for International Seminar on Research of Information\n  Technology and Intelligent Systems (ISRITI), IEEE. December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a novel platform Resource-Aware AutoML\n(RA-AutoML) which enables flexible and generalized algorithms to build machine\nlearning models subjected to multiple objectives, as well as resource and\nhard-ware constraints. RA-AutoML intelligently conducts Hyper-Parameter\nSearch(HPS) as well as Neural Architecture Search (NAS) to build models\noptimizing predefined objectives. RA-AutoML is a versatile framework that\nallows user to prescribe many resource/hardware constraints along with\nobjectives demanded by the problem at hand or business requirements. At its\ncore, RA-AutoML relies on our in-house search-engine algorithm,MOBOGA, which\ncombines a modified constraint-aware Bayesian Optimization and Genetic\nAlgorithm to construct Pareto optimal candidates. Our experiments on CIFAR-10\ndataset shows very good accuracy compared to results obtained by state-of-art\nneural network models, while subjected to resource constraints in the form of\nmodel size.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:37:48 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Yao", ""], ["Nam", "Andrew", ""], ["Nasr-Azadani", "Mohamad M.", ""], ["Tung", "Teresa", ""]]}, {"id": "2011.00105", "submitter": "Kun Qian", "authors": "Kun Qian, Poornima Chozhiyath Raman, Yunyao Li, Lucian Popa", "title": "Learning Structured Representations of Entity Names using Active\n  Learning and Weak Supervision", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured representations of entity names are useful for many entity-related\ntasks such as entity normalization and variant generation. Learning the\nimplicit structured representations of entity names without context and\nexternal knowledge is particularly challenging. In this paper, we present a\nnovel learning framework that combines active learning and weak supervision to\nsolve this problem. Our experimental evaluation show that this framework\nenables the learning of high-quality models from merely a dozen or so labeled\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 21:01:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Qian", "Kun", ""], ["Raman", "Poornima Chozhiyath", ""], ["Li", "Yunyao", ""], ["Popa", "Lucian", ""]]}, {"id": "2011.00155", "submitter": "Kei Ota", "authors": "Kei Ota, Devesh K. Jha, Tadashi Onishi, Asako Kanezaki, Yusuke\n  Yoshiyasu, Yoko Sasaki, Toshisada Mariyama, Daniel Nikovski", "title": "Deep Reactive Planning in Dynamic Environments", "comments": "15 pages, 5 figures. Accepted at CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main novelty of the proposed approach is that it allows a robot to learn\nan end-to-end policy which can adapt to changes in the environment during\nexecution. While goal conditioning of policies has been studied in the RL\nliterature, such approaches are not easily extended to cases where the robot's\ngoal can change during execution. This is something that humans are naturally\nable to do. However, it is difficult for robots to learn such reflexes (i.e.,\nto naturally respond to dynamic environments), especially when the goal\nlocation is not explicitly provided to the robot, and instead needs to be\nperceived through a vision sensor. In the current work, we present a method\nthat can achieve such behavior by combining traditional kinematic planning,\ndeep learning, and deep reinforcement learning in a synergistic fashion to\ngeneralize to arbitrary environments. We demonstrate the proposed approach for\nseveral reaching and pick-and-place tasks in simulation, as well as on a real\nsystem of a 6-DoF industrial manipulator. A video describing our work could be\nfound \\url{https://youtu.be/hE-Ew59GRPQ}.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 00:46:13 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 21:31:44 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Ota", "Kei", ""], ["Jha", "Devesh K.", ""], ["Onishi", "Tadashi", ""], ["Kanezaki", "Asako", ""], ["Yoshiyasu", "Yusuke", ""], ["Sasaki", "Yoko", ""], ["Mariyama", "Toshisada", ""], ["Nikovski", "Daniel", ""]]}, {"id": "2011.00160", "submitter": "Gustavo Zanoni Felipe", "authors": "Gustavo Z. Felipe, Jacqueline N. Zanoni, Camila C.\n  Sehaber-Sierakowski, Gleison D. P. Bossolani, Sara R. G. Souza, Franklin C.\n  Flores, Luiz E. S. Oliveira, Rodolfo M. Pereira, Yandre M. G. Costa", "title": "Automatic Chronic Degenerative Diseases Identification Using Enteric\n  Nervous System Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies recently accomplished on the Enteric Nervous System have shown that\nchronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus,\nthe development of recognition methods able to identify whether or not the EGC\nare affected by these type of diseases may be helpful in its diagnoses. In this\nwork, we propose the use of pattern recognition and machine learning techniques\nto evaluate if a given animal EGC image was obtained from a healthy individual\nor one affect by a chronic degenerative disease. In the proposed approach, we\nhave performed the classification task with handcrafted features and deep\nlearning based techniques, also known as non-handcrafted features. The\nhandcrafted features were obtained from the textural content of the ECG images\nusing texture descriptors, such as the Local Binary Pattern (LBP). Moreover,\nthe representation learning techniques employed in the approach are based on\ndifferent Convolutional Neural Network (CNN) architectures, such as AlexNet and\nVGG16, with and without transfer learning. The complementarity between the\nhandcrafted and non-handcrafted features was also evaluated with late fusion\ntechniques. The datasets of EGC images used in the experiments, which are also\ncontributions of this paper, are composed of three different chronic\ndegenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The\nexperimental results, supported by statistical analysis, shown that the\nproposed approach can distinguish healthy cells from the sick ones with a\nrecognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13%\n(Diabetes Mellitus), being achieved by combining classifiers obtained both\nfeature scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:04:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Felipe", "Gustavo Z.", ""], ["Zanoni", "Jacqueline N.", ""], ["Sehaber-Sierakowski", "Camila C.", ""], ["Bossolani", "Gleison D. P.", ""], ["Souza", "Sara R. G.", ""], ["Flores", "Franklin C.", ""], ["Oliveira", "Luiz E. S.", ""], ["Pereira", "Rodolfo M.", ""], ["Costa", "Yandre M. G.", ""]]}, {"id": "2011.00164", "submitter": "Fanhua Shang", "authors": "Tao Xu, Fanhua Shang, Yuanyuan Liu, Hongying Liu, Longjie Shen, Maoguo\n  Gong", "title": "Differentially Private ADMM Algorithms for Machine Learning", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study efficient differentially private alternating\ndirection methods of multipliers (ADMM) via gradient perturbation for many\nmachine learning problems. For smooth convex loss functions with (non)-smooth\nregularization, we propose the first differentially private ADMM (DP-ADMM)\nalgorithm with performance guarantee of $(\\epsilon,\\delta)$-differential\nprivacy ($(\\epsilon,\\delta)$-DP). From the viewpoint of theoretical analysis,\nwe use the Gaussian mechanism and the conversion relationship between R\\'enyi\nDifferential Privacy (RDP) and DP to perform a comprehensive privacy analysis\nfor our algorithm. Then we establish a new criterion to prove the convergence\nof the proposed algorithms including DP-ADMM. We also give the utility analysis\nof our DP-ADMM. Moreover, we propose an accelerated DP-ADMM (DP-AccADMM) with\nthe Nesterov's acceleration technique. Finally, we conduct numerical\nexperiments on many real-world datasets to show the privacy-utility tradeoff of\nthe two proposed algorithms, and all the comparative analysis shows that\nDP-AccADMM converges faster and has a better utility than DP-ADMM, when the\nprivacy budget $\\epsilon$ is larger than a threshold.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:37:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Xu", "Tao", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Liu", "Hongying", ""], ["Shen", "Longjie", ""], ["Gong", "Maoguo", ""]]}, {"id": "2011.00165", "submitter": "Esmaeil Seraj", "authors": "Esmaeil Seraj, Xiyang Wu, Matthew Gombolay", "title": "FireCommander: An Interactive, Probabilistic Multi-agent Environment for\n  Joint Perception-Action Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this tutorial is to help individuals use the\n\\underline{FireCommander} game environment for research applications. The\nFireCommander is an interactive, probabilistic joint perception-action\nreconnaissance environment in which a composite team of agents (e.g., robots)\ncooperate to fight dynamic, propagating firespots (e.g., targets). In\nFireCommander game, a team of agents must be tasked to optimally deal with a\nwildfire situation in an environment with propagating fire areas and some\nfacilities such as houses, hospitals, power stations, etc. The team of agents\ncan accomplish their mission by first sensing (e.g., estimating fire states),\ncommunicating the sensed fire-information among each other and then taking\naction to put the firespots out based on the sensed information (e.g., dropping\nwater on estimated fire locations). The FireCommander environment can be useful\nfor research topics spanning a wide range of applications from Reinforcement\nLearning (RL) and Learning from Demonstration (LfD), to Coordination,\nPsychology, Human-Robot Interaction (HRI) and Teaming. There are four important\nfacets of the FireCommander environment that overall, create a non-trivial\ngame: (1) Complex Objectives: Multi-objective Stochastic Environment,\n(2)Probabilistic Environment: Agents' actions result in probabilistic\nperformance, (3) Hidden Targets: Partially Observable Environment and, (4)\nUni-task Robots: Perception-only and Action-only agents. The FireCommander\nenvironment is first-of-its-kind in terms of including Perception-only and\nAction-only agents for coordination. It is a general multi-purpose game that\ncan be useful in a variety of combinatorial optimization problems and\nstochastic games, such as applications of Reinforcement Learning (RL), Learning\nfrom Demonstration (LfD) and Inverse RL (iRL).\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 02:06:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Seraj", "Esmaeil", ""], ["Wu", "Xiyang", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2011.00192", "submitter": "Zitao Liu", "authors": "Haochen Liu, Zitao Liu, Zhongqin Wu, Jiliang Tang", "title": "Personalized Multimodal Feedback Generation in Education", "comments": "Accepted in The 28th International Conference on Computational\n  Linguistics (COLING 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic evaluation for school assignments is an important application\nof AI in the education field. In this work, we focus on the task of\npersonalized multimodal feedback generation, which aims to generate\npersonalized feedback for various teachers to evaluate students' assignments\ninvolving multimodal inputs such as images, audios, and texts. This task\ninvolves the representation and fusion of multimodal information and natural\nlanguage generation, which presents the challenges from three aspects: 1) how\nto encode and integrate multimodal inputs; 2) how to generate feedback specific\nto each modality; and 3) how to realize personalized feedback generation. In\nthis paper, we propose a novel Personalized Multimodal Feedback Generation\nNetwork (PMFGN) armed with a modality gate mechanism and a personalized bias\nmechanism to address these challenges. The extensive experiments on real-world\nK-12 education data show that our model significantly outperforms several\nbaselines by generating more accurate and diverse feedback. In addition,\ndetailed ablation experiments are conducted to deepen our understanding of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 05:26:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Haochen", ""], ["Liu", "Zitao", ""], ["Wu", "Zhongqin", ""], ["Tang", "Jiliang", ""]]}, {"id": "2011.00215", "submitter": "Shuyin Xia", "authors": "Shuyin Xia, Wenhua Li, Guoyin Wang, Xinbo Gao, Changqing Zhang,\n  Elisabeth Giem", "title": "LRA: an accelerated rough set framework based on local redundancy of\n  attribute for feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and prove the theorem regarding the stability of\nattributes in a decision system. Based on the theorem, we propose the LRA\nframework for accelerating rough set algorithms. It is a general-purpose\nframework which can be applied to almost all rough set methods significantly .\nTheoretical analysis guarantees high efficiency. Note that the enhancement of\nefficiency will not lead to any decrease of the classification accuracy.\nBesides, we provide a simpler prove for the positive approximation acceleration\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 08:50:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Xia", "Shuyin", ""], ["Li", "Wenhua", ""], ["Wang", "Guoyin", ""], ["Gao", "Xinbo", ""], ["Zhang", "Changqing", ""], ["Giem", "Elisabeth", ""]]}, {"id": "2011.00235", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih", "title": "Domain-specific Knowledge Graphs: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have made a qualitative leap and effected a real\nrevolution in knowledge representation. This is leveraged by the underlying\nstructure of the KG which underpins a better comprehension, reasoning and\ninterpretation of knowledge for both human and machine. Therefore, KGs continue\nto be used as the main means of tackling a plethora of real-life problems in\nvarious domains. However, there is no consensus in regard to a plausible and\ninclusive definition of a domain-specific KG. Further, in conjunction with\nseveral limitations and deficiencies, various domain-specific KG construction\napproaches are far from perfect. This survey is the first to offer a\ncomprehensive definition of a domain-specific KG. Also, the paper presents a\nthorough review of the state-of-the-art approaches drawn from academic works\nrelevant to seven domains of knowledge. An examination of current approaches\nreveals a range of limitations and deficiencies. At the same time, uncharted\nterritories on the research map are highlighted to tackle extant issues in the\nliterature and point to directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 10:39:53 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:02:24 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 13:25:56 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abu-Salih", "Bilal", ""]]}, {"id": "2011.00261", "submitter": "Cheng Fu Dr.", "authors": "Cheng Fu and Robert Weibel", "title": "Towards Measuring Place Function Similarity at Fine Spatial Granularity\n  with Trajectory Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling place functions from a computational perspective is a prevalent\nresearch topic. Trajectory embedding, as a neural-network-backed dimension\nreduction technology, allows the possibility to put places with similar social\nfunctions at close locations in the embedding space if the places share similar\nchronological context as part of a trajectory. The embedding similarity was\npreviously proposed as a new metric for measuring the similarity of place\nfunctions. This study explores if this approach is meaningful for geographical\nunits at a much smaller geographical granularity compared to previous studies.\nIn addition, this study investigates if the geographical distance can influence\nthe embedding similarity. The empirical evaluations based on a big vehicle\ntrajectory data set confirm that the embedding similarity can be a metric proxy\nfor place functions. However, the results also show that the embedding\nsimilarity is still bounded by the distance at the local scale.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 12:59:46 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 16:13:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Fu", "Cheng", ""], ["Weibel", "Robert", ""]]}, {"id": "2011.00330", "submitter": "Brijen Thananjeyan", "authors": "Brijen Thananjeyan, Kirthevasan Kandasamy, Ion Stoica, Michael I.\n  Jordan, Ken Goldberg, Joseph E. Gonzalez", "title": "Resource Allocation in Multi-armed Bandit Exploration: Overcoming\n  Sublinear Scaling with Adaptive Parallelism", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exploration in stochastic multi-armed bandits when we have access to\na divisible resource that can be allocated in varying amounts to arm pulls. We\nfocus in particular on the allocation of distributed computing resources, where\nwe may obtain results faster by allocating more resources per pull, but might\nhave reduced throughput due to nonlinear scaling. For example, in\nsimulation-based scientific studies, an expensive simulation can be sped up by\nrunning it on multiple cores. This speed-up however, is partly offset by the\ncommunication among cores, which results in lower throughput than if fewer\ncores were allocated per trial to run more trials in parallel. In this paper,\nwe explore these trade-offs in two settings. First, in a fixed confidence\nsetting, we need to find the best arm with a given target success probability\nas quickly as possible. We propose an algorithm which trades off between\ninformation accumulation and throughput and show that the time taken can be\nupper bounded by the solution of a dynamic program whose inputs are the gaps\nbetween the sub-optimal and optimal arms. We also prove a matching hardness\nresult. Second, we present an algorithm for a fixed deadline setting, where we\nare given a time deadline and need to maximize the probability of finding the\nbest arm. We corroborate our theoretical insights with simulation experiments\nthat show that the algorithms consistently match or outperform baseline\nalgorithms on a variety of problem instances.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:19:29 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:25:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Thananjeyan", "Brijen", ""], ["Kandasamy", "Kirthevasan", ""], ["Stoica", "Ion", ""], ["Jordan", "Michael I.", ""], ["Goldberg", "Ken", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2011.00341", "submitter": "Yasin Almalioglu", "authors": "Yasin Almalioglu, Angel Santamaria-Navarro, Benjamin Morrell,\n  Ali-akbar Agha-mohammadi", "title": "Unsupervised Deep Persistent Monocular Visual Odometry and Depth\n  Estimation in Extreme Environments", "comments": "Submitted to ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, unsupervised deep learning approaches have received\nsignificant attention to estimate the depth and visual odometry (VO) from\nunlabelled monocular image sequences. However, their performance is limited in\nchallenging environments due to perceptual degradation, occlusions and rapid\nmotions. Moreover, the existing unsupervised methods suffer from the lack of\nscale-consistency constraints across frames, which causes that the VO\nestimators fail to provide persistent trajectories over long sequences. In this\nstudy, we propose an unsupervised monocular deep VO framework that predicts\nsix-degrees-of-freedom pose camera motion and depth map of the scene from\nunlabelled RGB image sequences. We provide detailed quantitative and\nqualitative evaluations of the proposed framework on a) a challenging dataset\ncollected during the DARPA Subterranean challenge; and b) the benchmark KITTI\nand Cityscapes datasets. The proposed approach outperforms both traditional and\nstate-of-the-art unsupervised deep VO methods providing better results for both\npose estimation and depth recovery. The presented approach is part of the\nsolution used by the COSTAR team participating at the DARPA Subterranean\nChallenge.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 19:10:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Santamaria-Navarro", "Angel", ""], ["Morrell", "Benjamin", ""], ["Agha-mohammadi", "Ali-akbar", ""]]}, {"id": "2011.00345", "submitter": "Thomas Kober", "authors": "Thomas Kober and Malihe Alikhani and Matthew Stone and Mark Steedman", "title": "Aspectuality Across Genre: A Distributional Semantics Approach", "comments": "to appear at Coling 2020 in oh so lovely virtual Barcelona :)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of the lexical aspect of verbs in English plays a crucial\nrole for recognizing textual entailment and learning discourse-level\ninferences. We show that two elementary dimensions of aspectual class, states\nvs. events, and telic vs. atelic events, can be modelled effectively with\ndistributional semantics. We find that a verb's local context is most\nindicative of its aspectual class, and demonstrate that closed class words tend\nto be stronger discriminating contexts than content words. Our approach\noutperforms previous work on three datasets. Lastly, we contribute a dataset of\nhuman--human conversations annotated with lexical aspect and present\nexperiments that show the correlation of telicity with genre and discourse\ngoals.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 19:37:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kober", "Thomas", ""], ["Alikhani", "Malihe", ""], ["Stone", "Matthew", ""], ["Steedman", "Mark", ""]]}, {"id": "2011.00368", "submitter": "Hossein Rahmani", "authors": "Maryam Dialameh and Ali Hamzeh and Hossein Rahmani", "title": "DL-Reg: A Deep Learning Regularization Technique using Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization plays a vital role in the context of deep learning by\npreventing deep neural networks from the danger of overfitting. This paper\nproposes a novel deep learning regularization method named as DL-Reg, which\ncarefully reduces the nonlinearity of deep networks to a certain extent by\nexplicitly enforcing the network to behave as much linear as possible. The key\nidea is to add a linear constraint to the objective function of the deep neural\nnetworks, which is simply the error of a linear mapping from the inputs to the\noutputs of the model. More precisely, the proposed DL-Reg carefully forces the\nnetwork to behave in a linear manner. This linear constraint, which is further\nadjusted by a regularization factor, prevents the network from the risk of\noverfitting. The performance of DL-Reg is evaluated by training\nstate-of-the-art deep network models on several benchmark datasets. The\nexperimental results show that the proposed regularization method: 1) gives\nmajor improvements over the existing regularization techniques, and 2)\nsignificantly improves the performance of deep neural networks, especially in\nthe case of small-sized training datasets.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 21:53:24 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 23:22:48 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Dialameh", "Maryam", ""], ["Hamzeh", "Ali", ""], ["Rahmani", "Hossein", ""]]}, {"id": "2011.00379", "submitter": "Jialu Wang", "authors": "Jialu Wang, Yang Liu, Caleb Levy", "title": "Fair Classification with Group-Dependent Label Noise", "comments": "11 pages, 9 tables", "journal-ref": null, "doi": "10.1145/3442188.3445915", "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines how to train fair classifiers in settings where training\nlabels are corrupted with random noise, and where the error rates of corruption\ndepend both on the label class and on the membership function for a protected\nsubgroup. Heterogeneous label noise models systematic biases towards particular\ngroups when generating annotations. We begin by presenting analytical results\nwhich show that naively imposing parity constraints on demographic disparity\nmeasures, without accounting for heterogeneous and group-dependent error rates,\ncan decrease both the accuracy and the fairness of the resulting classifier.\nOur experiments demonstrate these issues arise in practice as well. We address\nthese problems by performing empirical risk minimization with carefully defined\nsurrogate loss functions and surrogate constraints that help avoid the pitfalls\nintroduced by heterogeneous label noise. We provide both theoretical and\nempirical justifications for the efficacy of our methods. We view our results\nas an important example of how imposing fairness on biased data sets without\nproper care can do at least as much harm as it does good.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:35:01 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 00:01:56 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wang", "Jialu", ""], ["Liu", "Yang", ""], ["Levy", "Caleb", ""]]}, {"id": "2011.00382", "submitter": "Dong-Ki Kim", "authors": "Dong-Ki Kim, Miao Liu, Matthew Riemer, Chuangchuang Sun, Marwa\n  Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, Jonathan P. How", "title": "A Policy Gradient Algorithm for Learning to Learn in Multiagent\n  Reinforcement Learning", "comments": "Accepted to ICML 2021. Code at https://github.com/dkkim93/meta-mapg\n  and Videos at https://sites.google.com/view/meta-mapg/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in multiagent reinforcement learning is to learn\nbeneficial behaviors in a shared environment with other simultaneously learning\nagents. In particular, each agent perceives the environment as effectively\nnon-stationary due to the changing policies of other agents. Moreover, each\nagent is itself constantly learning, leading to natural non-stationarity in the\ndistribution of experiences encountered. In this paper, we propose a novel\nmeta-multiagent policy gradient theorem that directly accounts for the\nnon-stationary policy dynamics inherent to multiagent learning settings. This\nis achieved by modeling our gradient updates to consider both an agent's own\nnon-stationary policy dynamics and the non-stationary policy dynamics of other\nagents in the environment. We show that our theoretically grounded approach\nprovides a general solution to the multiagent learning problem, which\ninherently comprises all key aspects of previous state of the art approaches on\nthis topic. We test our method on a diverse suite of multiagent benchmarks and\ndemonstrate a more efficient ability to adapt to new agents as they learn than\nbaseline methods across the full spectrum of mixed incentive, competitive, and\ncooperative domains.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:50:21 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 22:08:50 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 17:45:17 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 16:46:58 GMT"}, {"version": "v5", "created": "Fri, 11 Jun 2021 22:21:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kim", "Dong-Ki", ""], ["Liu", "Miao", ""], ["Riemer", "Matthew", ""], ["Sun", "Chuangchuang", ""], ["Abdulhai", "Marwa", ""], ["Habibi", "Golnaz", ""], ["Lopez-Cot", "Sebastian", ""], ["Tesauro", "Gerald", ""], ["How", "Jonathan P.", ""]]}, {"id": "2011.00393", "submitter": "Skanda Shridhar", "authors": "Skanda Shridhar, Yuhang Ma, Tara Stentz, Zhengdi Shen, Galen Clark\n  Haynes, Neil Traft", "title": "Beelines: Motion Prediction Metrics for Self-Driving Safety and Comfort", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The commonly used metrics for motion prediction do not correlate well with a\nself-driving vehicle's system-level performance. The most common metrics are\naverage displacement error (ADE) and final displacement error (FDE), which omit\nmany features, making them poor self-driving performance indicators. Since\nhigh-fidelity simulations and track testing can be resource-intensive, the use\nof prediction metrics better correlated with full-system behavior allows for\nswifter iteration cycles. In this paper, we offer a conceptual framework for\nprediction evaluation highly specific to self-driving. We propose two\ncomplementary metrics that quantify the effects of motion prediction on safety\n(related to recall) and comfort (related to precision). Using a simulator, we\ndemonstrate that our safety metric has a significantly better signal-to-noise\nratio than displacement error in identifying unsafe events.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 01:09:23 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 15:56:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shridhar", "Skanda", ""], ["Ma", "Yuhang", ""], ["Stentz", "Tara", ""], ["Shen", "Zhengdi", ""], ["Haynes", "Galen Clark", ""], ["Traft", "Neil", ""]]}, {"id": "2011.00401", "submitter": "Sam Toyer", "authors": "Sam Toyer, Rohin Shah, Andrew Critch, Stuart Russell", "title": "The MAGICAL Benchmark for Robust Imitation", "comments": "NeurIPS 2020 conference paper (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imitation Learning (IL) algorithms are typically evaluated in the same\nenvironment that was used to create demonstrations. This rewards precise\nreproduction of demonstrations in one particular environment, but provides\nlittle information about how robustly an algorithm can generalise the\ndemonstrator's intent to substantially different deployment settings. This\npaper presents the MAGICAL benchmark suite, which permits systematic evaluation\nof generalisation by quantifying robustness to different kinds of distribution\nshift that an IL algorithm is likely to encounter in practice. Using the\nMAGICAL suite, we confirm that existing IL algorithms overfit significantly to\nthe context in which demonstrations are provided. We also show that standard\nmethods for reducing overfitting are effective at creating narrow perceptual\ninvariances, but are not sufficient to enable transfer to contexts that require\nsubstantially different behaviour, which suggests that new approaches will be\nneeded in order to robustly generalise demonstrator intent. Code and data for\nthe MAGICAL suite is available at https://github.com/qxcv/magical/.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 02:04:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Toyer", "Sam", ""], ["Shah", "Rohin", ""], ["Critch", "Andrew", ""], ["Russell", "Stuart", ""]]}, {"id": "2011.00415", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner, Dino Sejdinovic, Yarin Gal", "title": "Inter-domain Deep Gaussian Processes", "comments": "Published in Proceedings of the 37th International Conference on\n  Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-domain Gaussian processes (GPs) allow for high flexibility and low\ncomputational cost when performing approximate inference in GP models. They are\nparticularly suitable for modeling data exhibiting global structure but are\nlimited to stationary covariance functions and thus fail to model\nnon-stationary data effectively. We propose Inter-domain Deep Gaussian\nProcesses, an extension of inter-domain shallow GPs that combines the\nadvantages of inter-domain and deep Gaussian processes (DGPs), and demonstrate\nhow to leverage existing approximate inference methods to perform simple and\nscalable approximate inference using inter-domain features in DGPs. We assess\nthe performance of our method on a range of regression tasks and demonstrate\nthat it outperforms inter-domain shallow GPs and conventional DGPs on\nchallenging large-scale real-world datasets exhibiting both global structure as\nwell as a high-degree of non-stationarity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 04:03:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Sejdinovic", "Dino", ""], ["Gal", "Yarin", ""]]}, {"id": "2011.00416", "submitter": "Zhijing Jin", "authors": "Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, Rada Mihalcea", "title": "Deep Learning for Text Style Transfer: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text style transfer (TST) is an important task in natural language generation\n(NLG), which aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing (NLP), and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of TST. Our curated paper\nlist is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 04:04:43 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 14:21:58 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 05:42:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jin", "Di", ""], ["Jin", "Zhijing", ""], ["Hu", "Zhiting", ""], ["Vechtomova", "Olga", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2011.00440", "submitter": "Brendan Tidd", "authors": "Brendan Tidd, Nicolas Hudson, Akansel Cosgun, Jurgen Leitner", "title": "Learning When to Switch: Composing Controllers to Traverse a Sequence of\n  Terrain Artifacts", "comments": "2021 IEEE International Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legged robots often use separate control policies that are highly engineered\nfor traversing difficult terrain such as stairs, gaps, and steps, where\nswitching between policies is only possible when the robot is in a region that\nis common to adjacent controllers. Deep Reinforcement Learning (DRL) is a\npromising alternative to hand-crafted control design, though typically requires\nthe full set of test conditions to be known before training. DRL policies can\nresult in complex (often unrealistic) behaviours that have few or no\noverlapping regions between adjacent policies, making it difficult to switch\nbehaviours. In this work we develop multiple DRL policies with Curriculum\nLearning (CL), each that can traverse a single respective terrain condition,\nwhile ensuring an overlap between policies. We then train a network for each\ndestination policy that estimates the likelihood of successfully switching from\nany other policy. We evaluate our switching method on a previously unseen\ncombination of terrain artifacts and show that it performs better than\nheuristic methods. While our method is trained on individual terrain types, it\nperforms comparably to a Deep Q Network trained on the full set of terrain\nconditions. This approach allows the development of separate policies in\nconstrained conditions with embedded prior knowledge about each behaviour, that\nis scalable to any number of behaviours, and prepares DRL methods for\napplications in the real world\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 06:34:42 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Tidd", "Brendan", ""], ["Hudson", "Nicolas", ""], ["Cosgun", "Akansel", ""], ["Leitner", "Jurgen", ""]]}, {"id": "2011.00443", "submitter": "Varun Behera", "authors": "Ashish Ranjan, Varun Nagesh Jolly Behera, Motahar Reza", "title": "A Parallel Approach for Real-Time Face Recognition from a Large Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new facial recognition system, capable of identifying a person,\nprovided their likeness has been previously stored in the system, in real time.\nThe system is based on storing and comparing facial embeddings of the subject,\nand identifying them later within a live video feed. This system is highly\naccurate, and is able to tag people with their ID in real time. It is able to\ndo so, even when using a database containing thousands of facial embeddings, by\nusing a parallelized searching technique. This makes the system quite fast and\nallows it to be highly scalable.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 07:40:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ranjan", "Ashish", ""], ["Behera", "Varun Nagesh Jolly", ""], ["Reza", "Motahar", ""]]}, {"id": "2011.00446", "submitter": "Anqiao Li", "authors": "Anqiao Li, Zhicheng Wang, Jun Wu, Qiuguo Zhu", "title": "Efficient Learning of Control Policies for Robust Quadruped Bounding\n  using Pretrained Neural Networks", "comments": "7pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounding is one of the important gaits in quadrupedal locomotion for\nnegotiating obstacles. However, due to a large number of robot and\nenvironmental constraints, conventional planning and control has limited\nability to adapt bounding gaits on various terrains in real-time. We proposed\nan efficient approach to learn robust bounding gaits by first pretraining the\ndeep neural network (DNN) using data from a robot that used conventional\nmodel-based controllers. Next, the pretrained DNN weights are optimized further\nvia deep reinforcement learning (DRL). Also, we designed a reward function\nconsidering contact points to enforce the gait symmetry and periodicity, and\nused feature engineering to improve input features of the DRL model and the\nbounding performance. The DNN-based feedback controller was learned in\nsimulation first and deployed directly on the real Jueying-Mini robot\nsuccessfully, which was computationally more efficient and performed much\nbetter than the previous model-based control in terms of robustness and\nstability in both indoor and outdoor experiments.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 08:06:46 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:01:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Li", "Anqiao", ""], ["Wang", "Zhicheng", ""], ["Wu", "Jun", ""], ["Zhu", "Qiuguo", ""]]}, {"id": "2011.00449", "submitter": "Suyu Ge", "authors": "Suyu Ge, Lu Cheng, Huan Liu", "title": "Improving Cyberbully Detection with User Interaction", "comments": "The Web Conference 2021", "journal-ref": null, "doi": "10.1145/3442381.3449828", "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyberbullying, identified as intended and repeated online bullying behavior,\nhas become increasingly prevalent in the past few decades. Despite the\nsignificant progress made thus far, the focus of most existing work on\ncyberbullying detection lies in the independent content analysis of different\ncomments within a social media session. We argue that such leading notions of\nanalysis suffer from three key limitations: they overlook the temporal\ncorrelations among different comments; they only consider the content within a\nsingle comment rather than the topic coherence across comments; they remain\ngeneric and exploit limited interactions between social media users. In this\nwork, we observe that user comments in the same session may be inherently\nrelated, e.g., discussing similar topics, and their interaction may evolve over\ntime. We also show that modeling such topic coherence and temporal interaction\nare critical to capture the repetitive characteristics of bullying behavior,\nthus leading to better predicting performance. To achieve the goal, we first\nconstruct a unified temporal graph for each social media session. Drawing on\nrecent advances in graph neural network, we then propose a principled\ngraph-based approach for modeling the temporal dynamics and topic coherence\nthroughout user interactions. We empirically evaluate the effectiveness of our\napproach with the tasks of session-level bullying detection and comment-level\ncase study. Our code is released to public.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 08:47:33 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 03:16:57 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ge", "Suyu", ""], ["Cheng", "Lu", ""], ["Liu", "Huan", ""]]}, {"id": "2011.00483", "submitter": "Vitou Phy", "authors": "Vitou Phy, Yang Zhao and Akiko Aizawa", "title": "Deconstruct to Reconstruct a Configurable Evaluation Metric for\n  Open-Domain Dialogue Systems", "comments": "15 pages, 4 figures, 7 tables, Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many automatic evaluation metrics have been proposed to score the overall\nquality of a response in open-domain dialogue. Generally, the overall quality\nis comprised of various aspects, such as relevancy, specificity, and empathy,\nand the importance of each aspect differs according to the task. For instance,\nspecificity is mandatory in a food-ordering dialogue task, whereas fluency is\npreferred in a language-teaching dialogue system. However, existing metrics are\nnot designed to cope with such flexibility. For example, BLEU score\nfundamentally relies only on word overlapping, whereas BERTScore relies on\nsemantic similarity between reference and candidate response. Thus, they are\nnot guaranteed to capture the required aspects, i.e., specificity. To design a\nmetric that is flexible to a task, we first propose making these qualities\nmanageable by grouping them into three groups: understandability, sensibleness,\nand likability, where likability is a combination of qualities that are\nessential for a task. We also propose a simple method to composite metrics of\neach aspect to obtain a single metric called USL-H, which stands for\nUnderstandability, Sensibleness, and Likability in Hierarchy. We demonstrated\nthat USL-H score achieves good correlations with human judgment and maintains\nits configurability towards different aspects and metrics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 11:34:50 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Phy", "Vitou", ""], ["Zhao", "Yang", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2011.00485", "submitter": "Marco Wiering", "authors": "Xiangxie Zhang, Ben Beinke, Berlian Al Kindhi and Marco Wiering", "title": "Comparing Machine Learning Algorithms with or without Feature Extraction\n  for DNA Classification", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classification of DNA sequences is a key research area in bioinformatics\nas it enables researchers to conduct genomic analysis and detect possible\ndiseases. In this paper, three state-of-the-art algorithms, namely\nConvolutional Neural Networks, Deep Neural Networks, and N-gram Probabilistic\nModels, are used for the task of DNA classification. Furthermore, we introduce\na novel feature extraction method based on the Levenshtein distance and\nrandomly generated DNA sub-sequences to compute information-rich features from\nthe DNA sequences. We also use an existing feature extraction method based on\n3-grams to represent amino acids and combine both feature extraction methods\nwith a multitude of machine learning algorithms. Four different data sets, each\nconcerning viral diseases such as Covid-19, AIDS, Influenza, and Hepatitis C,\nare used for evaluating the different approaches. The results of the\nexperiments show that all methods obtain high accuracies on the different DNA\ndatasets. Furthermore, the domain-specific 3-gram feature extraction method\nleads in general to the best results in the experiments, while the newly\nproposed technique outperforms all other methods on the smallest Covid-19\ndataset\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 12:04:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Xiangxie", ""], ["Beinke", "Ben", ""], ["Kindhi", "Berlian Al", ""], ["Wiering", "Marco", ""]]}, {"id": "2011.00496", "submitter": "Niv Pekar", "authors": "Niv Pekar, Yaniv Benny, Lior Wolf", "title": "Generating Correct Answers for Progressive Matrices Intelligence Tests", "comments": "To appear in the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raven's Progressive Matrices are multiple-choice intelligence tests, where\none tries to complete the missing location in a $3\\times 3$ grid of abstract\nimages. Previous attempts to address this test have focused solely on selecting\nthe right answer out of the multiple choices. In this work, we focus, instead,\non generating a correct answer given the grid, without seeing the choices,\nwhich is a harder task, by definition. The proposed neural model combines\nmultiple advances in generative models, including employing multiple pathways\nthrough the same network, using the reparameterization trick along two pathways\nto make their encoding compatible, a dynamic application of variational losses,\nand a complex perceptual loss that is coupled with a selective backpropagation\nprocedure. Our algorithm is able not only to generate a set of plausible\nanswers, but also to be competitive to the state of the art methods in\nmultiple-choice tests.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 13:21:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pekar", "Niv", ""], ["Benny", "Yaniv", ""], ["Wolf", "Lior", ""]]}, {"id": "2011.00515", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner, Oscar Key, Yarin Gal, Tom Rainforth", "title": "On Signal-to-Noise Ratio Issues in Variational Inference for Deep\n  Gaussian Processes", "comments": "Published in Proceedings of the 38th International Conference on\n  Machine Learning (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient estimates used in training Deep Gaussian Processes\n(DGPs) with importance-weighted variational inference are susceptible to\nsignal-to-noise ratio (SNR) issues. Specifically, we show both theoretically\nand via an extensive empirical evaluation that the SNR of the gradient\nestimates for the latent variable's variational parameters decreases as the\nnumber of importance samples increases. As a result, these gradient estimates\ndegrade to pure noise if the number of importance samples is too large. To\naddress this pathology, we show how doubly reparameterized gradient estimators,\noriginally proposed for training variational autoencoders, can be adapted to\nthe DGP setting and that the resultant estimators completely remedy the SNR\nissue, thereby providing more reliable training. Finally, we demonstrate that\nour fix can lead to consistent improvements in the predictive performance of\nDGP models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 12:14:08 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Key", "Oscar", ""], ["Gal", "Yarin", ""], ["Rainforth", "Tom", ""]]}, {"id": "2011.00518", "submitter": "Rujing Yao", "authors": "Rujing Yao, Yingchun Ye, Ji Zhang, Shuxiao Li and Ou Wu", "title": "AI Marker-based Large-scale AI Literature Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge contained in academic literature is interesting to mine.\nInspired by the idea of molecular markers tracing in the field of biochemistry,\nthree named entities, namely, methods, datasets and metrics are used as AI\nmarkers for AI literature. These entities can be used to trace the research\nprocess described in the bodies of papers, which opens up new perspectives for\nseeking and mining more valuable academic information. Firstly, the entity\nextraction model is used in this study to extract AI markers from large-scale\nAI literature. Secondly, original papers are traced for AI markers. Statistical\nand propagation analysis are performed based on tracing results. Finally, the\nco-occurrences of AI markers are used to achieve clustering. The evolution\nwithin method clusters and the influencing relationships amongst different\nresearch scene clusters are explored. The above-mentioned mining based on AI\nmarkers yields many meaningful discoveries. For example, the propagation of\neffective methods on the datasets is rapidly increasing with the development of\ntime; effective methods proposed by China in recent years have increasing\ninfluence on other countries, whilst France is the opposite. Saliency\ndetection, a classic computer vision research scene, is the least likely to be\naffected by other research scenes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:48:46 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 04:13:16 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yao", "Rujing", ""], ["Ye", "Yingchun", ""], ["Zhang", "Ji", ""], ["Li", "Shuxiao", ""], ["Wu", "Ou", ""]]}, {"id": "2011.00524", "submitter": "Kayla Boggess", "authors": "Kayla Boggess, Shenghui Chen, Lu Feng", "title": "Towards Personalized Explanation of Robot Path Planning via User\n  Feedback", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior studies have found that explaining robot decisions and actions helps to\nincrease system transparency, improve user understanding, and enable effective\nhuman-robot collaboration. In this paper, we present a system for generating\npersonalized explanations of robot path planning via user feedback. We consider\na robot navigating in an environment modeled as a Markov decision process\n(MDP), and develop an algorithm to automatically generate a personalized\nexplanation of an optimal MDP policy, based on the user preference regarding\nfour elements (i.e., objective, locality, specificity, and corpus). In\naddition, we design the system to interact with users via answering users'\nfurther questions about the generated explanations. Users have the option to\nupdate their preferences to view different explanations. The system is capable\nof detecting and resolving any preference conflict via user interaction. The\nresults of an online user study show that the generated personalized\nexplanations improve user satisfaction, while the majority of users liked the\nsystem's capabilities of question-answering and conflict detection/resolution.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:10:43 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:12:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Boggess", "Kayla", ""], ["Chen", "Shenghui", ""], ["Feng", "Lu", ""]]}, {"id": "2011.00534", "submitter": "Julien Dupeyroux", "authors": "Julien Dupeyroux, Jesse Hagenaars, Federico Paredes-Vall\\'es, and\n  Guido de Croon", "title": "Neuromorphic control for optic-flow-based landings of MAVs using the\n  Loihi processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic processors like Loihi offer a promising alternative to\nconventional computing modules for endowing constrained systems like micro air\nvehicles (MAVs) with robust, efficient and autonomous skills such as take-off\nand landing, obstacle avoidance, and pursuit. However, a major challenge for\nusing such processors on robotic platforms is the reality gap between\nsimulation and the real world. In this study, we present for the very first\ntime a fully embedded application of the Loihi neuromorphic chip prototype in a\nflying robot. A spiking neural network (SNN) was evolved to compute the thrust\ncommand based on the divergence of the ventral optic flow field to perform\nautonomous landing. Evolution was performed in a Python-based simulator using\nthe PySNN library. The resulting network architecture consists of only 35\nneurons distributed among 3 layers. Quantitative analysis between simulation\nand Loihi reveals a root-mean-square error of the thrust setpoint as low as\n0.005 g, along with a 99.8% matching of the spike sequences in the hidden\nlayer, and 99.7% in the output layer. The proposed approach successfully\nbridges the reality gap, offering important insights for future neuromorphic\napplications in robotics. Supplementary material is available at\nhttps://mavlab.tudelft.nl/loihi/.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:25:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dupeyroux", "Julien", ""], ["Hagenaars", "Jesse", ""], ["Paredes-Vall\u00e9s", "Federico", ""], ["de Croon", "Guido", ""]]}, {"id": "2011.00551", "submitter": "Victor Zuanazzi", "authors": "Victor Zuanazzi, Joris van Vugt, Olaf Booij and Pascal Mettes", "title": "Adversarial Self-Supervised Scene Flow Estimation", "comments": "Published at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work proposes a metric learning approach for self-supervised scene flow\nestimation. Scene flow estimation is the task of estimating 3D flow vectors for\nconsecutive 3D point clouds. Such flow vectors are fruitful, \\eg for\nrecognizing actions, or avoiding collisions. Training a neural network via\nsupervised learning for scene flow is impractical, as this requires manual\nannotations for each 3D point at each new timestamp for each scene. To that\nend, we seek for a self-supervised approach, where a network learns a latent\nmetric to distinguish between points translated by flow estimations and the\ntarget point cloud. Our adversarial metric learning includes a multi-scale\ntriplet loss on sequences of two-point clouds as well as a cycle consistency\nloss. Furthermore, we outline a benchmark for self-supervised scene flow\nestimation: the Scene Flow Sandbox. The benchmark consists of five datasets\ndesigned to study individual aspects of flow estimation in progressive order of\ncomplexity, from a moving object to real-world scenes. Experimental evaluation\non the benchmark shows that our approach obtains state-of-the-art\nself-supervised scene flow results, outperforming recent neighbor-based\napproaches. We use our proposed benchmark to expose shortcomings and draw\ninsights on various training setups. We find that our setup captures motion\ncoherence and preserves local geometries. Dealing with occlusions, on the other\nhand, is still an open challenge.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:37:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zuanazzi", "Victor", ""], ["van Vugt", "Joris", ""], ["Booij", "Olaf", ""], ["Mettes", "Pascal", ""]]}, {"id": "2011.00554", "submitter": "Aniket Bera", "authors": "Vishnu Sashank Dorbala, Arjun Srinivasan, and Aniket Bera", "title": "Can a Robot Trust You? A DRL-Based Approach to Trust-Driven Human-Guided\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are known to construct cognitive maps of their everyday surroundings\nusing a variety of perceptual inputs. As such, when a human is asked for\ndirections to a particular location, their wayfinding capability in converting\nthis cognitive map into directional instructions is challenged. Owing to\nspatial anxiety, the language used in the spoken instructions can be vague and\noften unclear. To account for this unreliability in navigational guidance, we\npropose a novel Deep Reinforcement Learning (DRL) based trust-driven robot\nnavigation algorithm that learns humans' trustworthiness to perform a language\nguided navigation task. Our approach seeks to answer the question as to whether\na robot can trust a human's navigational guidance or not. To this end, we look\nat training a policy that learns to navigate towards a goal location using only\ntrustworthy human guidance, driven by its own robot trust metric. We look at\nquantifying various affective features from language-based instructions and\nincorporate them into our policy's observation space in the form of a human\ntrust metric. We utilize both these trust metrics into an optimal cognitive\nreasoning scheme that decides when and when not to trust the given guidance.\nOur results show that the learned policy can navigate the environment in an\noptimal, time-efficient manner as opposed to an explorative approach that\nperforms the same task. We showcase the efficacy of our results both in\nsimulation and a real-world environment.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:43:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dorbala", "Vishnu Sashank", ""], ["Srinivasan", "Arjun", ""], ["Bera", "Aniket", ""]]}, {"id": "2011.00559", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Sarthak Gupte, Marcos Zampieri, Ifeoma Nwogu", "title": "WLV-RIT at HASOC-Dravidian-CodeMix-FIRE2020: Offensive Language\n  Identification in Code-switched YouTube Comments", "comments": "Accepted to FIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the WLV-RIT entry to the Hate Speech and Offensive\nContent Identification in Indo-European Languages (HASOC) shared task 2020. The\nHASOC 2020 organizers provided participants with annotated datasets containing\nsocial media posts of code-mixed in Dravidian languages (Malayalam-English and\nTamil-English). We participated in task 1: Offensive comment identification in\nCode-mixed Malayalam Youtube comments. In our methodology, we take advantage of\navailable English data by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions to Malayalam data. We further improve the\nresults using various fine tuning strategies. Our system achieved 0.89 weighted\naverage F1 score for the test set and it ranked 5th place out of 12\nparticipants.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:52:08 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Gupte", "Sarthak", ""], ["Zampieri", "Marcos", ""], ["Nwogu", "Ifeoma", ""]]}, {"id": "2011.00569", "submitter": "C.-H. Huck Yang", "authors": "Jia-Hong Huang, Chao-Han Huck Yang, Fangyu Liu, Meng Tian, Yi-Chieh\n  Liu, Ting-Wei Wu, I-Hung Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang,\n  Jesper Tegner, Marcel Worring", "title": "DeepOpht: Medical Report Generation for Retinal Images via Deep Models\n  and Visual Explanation", "comments": "Accepted to IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an AI-based method that intends to improve the\nconventional retinal disease treatment procedure and help ophthalmologists\nincrease diagnosis efficiency and accuracy. The proposed method is composed of\na deep neural networks-based (DNN-based) module, including a retinal disease\nidentifier and clinical description generator, and a DNN visual explanation\nmodule. To train and validate the effectiveness of our DNN-based module, we\npropose a large-scale retinal disease image dataset. Also, as ground truth, we\nprovide a retinal image dataset manually labeled by ophthalmologists to\nqualitatively show, the proposed AI-based method is effective. With our\nexperimental results, we show that the proposed method is quantitatively and\nqualitatively effective. Our method is capable of creating meaningful retinal\nimage descriptions and visual explanations that are clinically relevant.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:28:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Yang", "Chao-Han Huck", ""], ["Liu", "Fangyu", ""], ["Tian", "Meng", ""], ["Liu", "Yi-Chieh", ""], ["Wu", "Ting-Wei", ""], ["Lin", "I-Hung", ""], ["Wang", "Kang", ""], ["Morikawa", "Hiromasa", ""], ["Chang", "Hernghua", ""], ["Tegner", "Jesper", ""], ["Worring", "Marcel", ""]]}, {"id": "2011.00583", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Jun Wang", "title": "An Overview of Multi-Agent Reinforcement Learning from Game Theoretical\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the remarkable success of the AlphaGO series, 2019 was a booming\nyear that witnessed significant advances in multi-agent reinforcement learning\n(MARL) techniques. MARL corresponds to the learning problem in a multi-agent\nsystem in which multiple agents learn simultaneously. It is an\ninterdisciplinary domain with a long history that includes game theory, machine\nlearning, stochastic control, psychology, and optimisation. Although MARL has\nachieved considerable empirical success in solving real-world games, there is a\nlack of a self-contained overview in the literature that elaborates the game\ntheoretical foundations of modern MARL methods and summarises the recent\nadvances. In fact, the majority of existing surveys are outdated and do not\nfully cover the recent developments since 2010. In this work, we provide a\nmonograph on MARL that covers both the fundamentals and the latest developments\nin the research frontier. The goal of our monograph is to provide a\nself-contained assessment of the current state-of-the-art MARL techniques from\na game theoretical perspective. We expect this work to serve as a stepping\nstone for both new researchers who are about to enter this fast-growing domain\nand existing domain experts who want to obtain a panoramic view and identify\nnew directions based on recent advances.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:24:40 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 17:50:27 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 01:43:32 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yang", "Yaodong", ""], ["Wang", "Jun", ""]]}, {"id": "2011.00597", "submitter": "Simon Ging", "authors": "Simon Ging (1), Mohammadreza Zolfaghari (1), Hamed Pirsiavash (2),\n  Thomas Brox (1) ((1) University of Freiburg, (2) University of Maryland\n  Baltimore County)", "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation\n  Learning", "comments": "27 pages, 5 figures, 19 tables. To be published in the 34th\n  conference on Neural Information Processing Systems (NeurIPS 2020). The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world video-text tasks involve different levels of granularity,\nsuch as frames and words, clip and sentences or videos and paragraphs, each\nwith distinct semantics. In this paper, we propose a Cooperative hierarchical\nTransformer (COOT) to leverage this hierarchy information and model the\ninteractions between different levels of granularity and different modalities.\nThe method consists of three major components: an attention-aware feature\naggregation layer, which leverages the local temporal context (intra-level,\ne.g., within a clip), a contextual transformer to learn the interactions\nbetween low-level and high-level semantics (inter-level, e.g. clip-video,\nsentence-paragraph), and a cross-modal cycle-consistency loss to connect video\nand text. The resulting method compares favorably to the state of the art on\nseveral benchmarks while having few parameters. All code is available\nopen-source at https://github.com/gingsi/coot-videotext\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:54:09 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ging", "Simon", ""], ["Zolfaghari", "Mohammadreza", ""], ["Pirsiavash", "Hamed", ""], ["Brox", "Thomas", ""]]}, {"id": "2011.00603", "submitter": "Guilherme Alves", "authors": "Guilherme Alves, Vaishnavi Bhargava, Miguel Couceiro, Amedeo Napoli", "title": "Making ML models fairer through explanations: the case of LimeOut", "comments": "11 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithmic decisions are now being used on a daily basis, and based on\nMachine Learning (ML) processes that may be complex and biased. This raises\nseveral concerns given the critical impact that biased decisions may have on\nindividuals or on society as a whole. Not only unfair outcomes affect human\nrights, they also undermine public trust in ML and AI. In this paper we address\nfairness issues of ML models based on decision outcomes, and we show how the\nsimple idea of \"feature dropout\" followed by an \"ensemble approach\" can improve\nmodel fairness. To illustrate, we will revisit the case of \"LimeOut\" that was\nproposed to tackle \"process fairness\", which measures a model's reliance on\nsensitive or discriminatory features. Given a classifier, a dataset and a set\nof sensitive features, LimeOut first assesses whether the classifier is fair by\nchecking its reliance on sensitive features using \"Lime explanations\". If\ndeemed unfair, LimeOut then applies feature dropout to obtain a pool of\nclassifiers. These are then combined into an ensemble classifier that was\nempirically shown to be less dependent on sensitive features without\ncompromising the classifier's accuracy. We present different experiments on\nmultiple datasets and several state of the art classifiers, which show that\nLimeOut's classifiers improve (or at least maintain) not only process fairness\nbut also other fairness metrics such as individual and group fairness, equal\nopportunity, and demographic parity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 19:07:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alves", "Guilherme", ""], ["Bhargava", "Vaishnavi", ""], ["Couceiro", "Miguel", ""], ["Napoli", "Amedeo", ""]]}, {"id": "2011.00620", "submitter": "Maxwell Forbes", "authors": "Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, Yejin Choi", "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms", "comments": "Published at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social norms -- the unspoken commonsense rules about acceptable social\nbehavior -- are crucial in understanding the underlying causes and intents of\npeople's actions in narratives. For example, underlying an action such as\n\"wanting to call cops on my neighbors\" are social norms that inform our\nconduct, such as \"It is expected that you report crimes.\"\n  We present Social Chemistry, a new conceptual formalism to study people's\neveryday social norms and moral judgments over a rich spectrum of real life\nsituations described in natural language. We introduce Social-Chem-101, a\nlarge-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run\na blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further\nbroken down with 12 different dimensions of people's judgments, including\nsocial judgments of good and bad, moral foundations, expected cultural\npressure, and assumed legality, which together amount to over 4.5 million\nannotations of categorical labels and free-text descriptions.\n  Comprehensive empirical results based on state-of-the-art neural models\ndemonstrate that computational modeling of social norms is a promising research\ndirection. Our model framework, Neural Norm Transformer, learns and generalizes\nSocial-Chem-101 to successfully reason about previously unseen situations,\ngenerating relevant (and potentially novel) attribute-aware social\nrules-of-thumb.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 20:16:45 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 18:59:00 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Forbes", "Maxwell", ""], ["Hwang", "Jena D.", ""], ["Shwartz", "Vered", ""], ["Sap", "Maarten", ""], ["Choi", "Yejin", ""]]}, {"id": "2011.00678", "submitter": "Shuhao Gu", "authors": "Shuhao Gu and Yang Feng", "title": "Investigating Catastrophic Forgetting During Continual Training for\n  Neural Machine Translation", "comments": "Coling2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) models usually suffer from catastrophic\nforgetting during continual training where the models tend to gradually forget\npreviously learned knowledge and swing to fit the newly added data which may\nhave a different distribution, e.g. a different domain. Although many methods\nhave been proposed to solve this problem, we cannot get to know what causes\nthis phenomenon yet. Under the background of domain adaptation, we investigate\nthe cause of catastrophic forgetting from the perspectives of modules and\nparameters (neurons). The investigation on the modules of the NMT model shows\nthat some modules have tight relation with the general-domain knowledge while\nsome other modules are more essential in the domain adaptation. And the\ninvestigation on the parameters shows that some parameters are important for\nboth the general-domain and in-domain translation and the great change of them\nduring continual training brings about the performance decline in\ngeneral-domain. We conduct experiments across different language pairs and\ndomains to ensure the validity and reliability of our findings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 01:55:06 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 08:49:42 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 06:56:52 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gu", "Shuhao", ""], ["Feng", "Yang", ""]]}, {"id": "2011.00681", "submitter": "Evangelia Spiliopoulou", "authors": "Evangelia Spiliopoulou and Salvador Medina Maza and Eduard Hovy and\n  Alexander Hauptmann", "title": "Event-Related Bias Removal for Real-time Disaster Events", "comments": "To appear in EMNLP Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has become an important tool to share information about crisis\nevents such as natural disasters and mass attacks. Detecting actionable posts\nthat contain useful information requires rapid analysis of huge volume of data\nin real-time. This poses a complex problem due to the large amount of posts\nthat do not contain any actionable information. Furthermore, the classification\nof information in real-time systems requires training on out-of-domain data, as\nwe do not have any data from a new emerging crisis. Prior work focuses on\nmodels pre-trained on similar event types. However, those models capture\nunnecessary event-specific biases, like the location of the event, which affect\nthe generalizability and performance of the classifiers on new unseen data from\nan emerging new event. In our work, we train an adversarial neural model to\nremove latent event-specific biases and improve the performance on tweet\nimportance classification.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 02:03:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Spiliopoulou", "Evangelia", ""], ["Maza", "Salvador Medina", ""], ["Hovy", "Eduard", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2011.00702", "submitter": "Rafael Pinto", "authors": "Rafael Pinto", "title": "Fast Reinforcement Learning with Incremental Gaussian Mixture Models", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel algorithm that integrates a data-efficient\nfunction approximator with reinforcement learning in continuous state spaces.\nAn online and incremental algorithm capable of learning from a single pass\nthrough data, called Incremental Gaussian Mixture Network (IGMN), was employed\nas a sample-efficient function approximator for the joint state and Q-values\nspace, all in a single model, resulting in a concise and data-efficient\nalgorithm, i.e., a reinforcement learning algorithm that learns from very few\ninteractions with the environment. Results are analyzed to explain the\nproperties of the obtained algorithm, and it is observed that the use of the\nIGMN function approximator brings some important advantages to reinforcement\nlearning in relation to conventional neural networks trained by gradient\ndescent methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:18:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pinto", "Rafael", ""]]}, {"id": "2011.00748", "submitter": "Bei Wang", "authors": "Ilkin Safarli, Youjia Zhou, Bei Wang", "title": "Interpreting Graph Drawing with Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning techniques to graph drawing has become an emergent\narea of research in visualization. In this paper, we interpret graph drawing as\na multi-agent reinforcement learning (MARL) problem. We first demonstrate that\na large number of classic graph drawing algorithms, including force-directed\nlayouts and stress majorization, can be interpreted within the framework of\nMARL. Using this interpretation, a node in the graph is assigned to an agent\nwith a reward function. Via multi-agent reward maximization, we obtain an\naesthetically pleasing graph layout that is comparable to the outputs of\nclassic algorithms. The main strength of a MARL framework for graph drawing is\nthat it not only unifies a number of classic drawing algorithms in a general\nformulation but also supports the creation of novel graph drawing algorithms by\nintroducing a diverse set of reward functions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 05:00:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Safarli", "Ilkin", ""], ["Zhou", "Youjia", ""], ["Wang", "Bei", ""]]}, {"id": "2011.00756", "submitter": "Joanne Kim", "authors": "Joanne Taery Kim and Sehoon Ha", "title": "Observation Space Matters: Benchmark and Optimization Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep reinforcement learning (deep RL) enable researchers\nto solve challenging control problems, from simulated environments to\nreal-world robotic tasks. However, deep RL algorithms are known to be sensitive\nto the problem formulation, including observation spaces, action spaces, and\nreward functions. There exist numerous choices for observation spaces but they\nare often designed solely based on prior knowledge due to the lack of\nestablished principles. In this work, we conduct benchmark experiments to\nverify common design choices for observation spaces, such as Cartesian\ntransformation, binary contact flags, a short history, or global positions.\nThen we propose a search algorithm to find the optimal observation spaces,\nwhich examines various candidate observation spaces and removes unnecessary\nobservation channels with a Dropout-Permutation test. We demonstrate that our\nalgorithm significantly improves learning speed compared to manually designed\nobservation spaces. We also analyze the proposed algorithm by evaluating\ndifferent hyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 05:40:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kim", "Joanne Taery", ""], ["Ha", "Sehoon", ""]]}, {"id": "2011.00766", "submitter": "Jungwoo Lim", "authors": "Jungwoo Lim, Dongsuk Oh, Yoonna Jang, Kisu Yang, Heuiseok Lim", "title": "I Know What You Asked: Graph Path Learning using AMR for Commonsense\n  Reasoning", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CommonsenseQA is a task in which a correct answer is predicted through\ncommonsense reasoning with pre-defined knowledge. Most previous works have\naimed to improve the performance with distributed representation without\nconsidering the process of predicting the answer from the semantic\nrepresentation of the question. To shed light upon the semantic interpretation\nof the question, we propose an AMR-ConceptNet-Pruned (ACP) graph. The ACP graph\nis pruned from a full integrated graph encompassing Abstract Meaning\nRepresentation (AMR) graph generated from input questions and an external\ncommonsense knowledge graph, ConceptNet (CN). Then the ACP graph is exploited\nto interpret the reasoning path as well as to predict the correct answer on the\nCommonsenseQA task. This paper presents the manner in which the commonsense\nreasoning process can be interpreted with the relations and concepts provided\nby the ACP graph. Moreover, ACP-based models are shown to outperform the\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:22:01 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 04:13:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Lim", "Jungwoo", ""], ["Oh", "Dongsuk", ""], ["Jang", "Yoonna", ""], ["Yang", "Kisu", ""], ["Lim", "Heuiseok", ""]]}, {"id": "2011.00775", "submitter": "Ruo Ando", "authors": "Ruo Ando, Yoshiyasu Takefuji", "title": "A Curious New Result of Resolution Strategies in Negation-Limited\n  Inverters Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, negation-limited inverters problem is known as a puzzle of\nconstructing an inverter with AND gates and OR gates and a few inverters. In\nthis paper, we introduce a curious new result about the effectiveness of two\npowerful ATP (Automated Theorem Proving) strategies on tackling negation\nlimited inverter problem. Two resolution strategies are UR (Unit Resulting)\nresolution and hyper-resolution. In experiment, we come two kinds of automated\ncircuit construction: 3 input/output inverters and 4 input/output BCD Counter\nCircuit. Both circuits are constructed with a few limited inverters. Curiously,\nit has been turned out that UR resolution is drastically faster than\nhyper-resolution in the measurement of the size of SOS (Set of Support).\nBesides, we discuss the syntactic and semantic criteria which might causes\nconsiderable difference of computation cost between UR resolution and\nhyper-resolution.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:52:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ando", "Ruo", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "2011.00780", "submitter": "Chao-Wei Huang", "authors": "Chao-Wei Huang and Yun-Nung Chen", "title": "Adapting Pretrained Transformer to Lattices for Spoken Language\n  Understanding", "comments": "ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattices are compact representations that encode multiple hypotheses, such as\nspeech recognition results or different word segmentations. It is shown that\nencoding lattices as opposed to 1-best results generated by automatic speech\nrecognizer (ASR) boosts the performance of spoken language understanding (SLU).\nRecently, pretrained language models with the transformer architecture have\nachieved the state-of-the-art results on natural language understanding, but\ntheir ability of encoding lattices has not been explored. Therefore, this paper\naims at adapting pretrained transformers to lattice inputs in order to perform\nunderstanding tasks specifically for spoken language. Our experiments on the\nbenchmark ATIS dataset show that fine-tuning pretrained transformers with\nlattice inputs yields clear improvement over fine-tuning with 1-best results.\nFurther evaluation demonstrates the effectiveness of our methods under\ndifferent acoustic conditions. Our code is available at\nhttps://github.com/MiuLab/Lattice-SLU\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:14:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huang", "Chao-Wei", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2011.00781", "submitter": "Abhinav Sharma", "authors": "Abhinav Sharma, Advait Deshpande, Yanming Wang, Xinyi Xu, Prashan\n  Madumal, Anbin Hou", "title": "Searching k-Optimal Goals for an Orienteering Problem on a Specialized\n  Graph with Budget Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel non-randomized anytime orienteering algorithm for finding\nk-optimal goals that maximize reward on a specialized graph with budget\nconstraints. This specialized graph represents a real-world scenario which is\nanalogous to an orienteering problem of finding k-most optimal goal states.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:15:41 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sharma", "Abhinav", ""], ["Deshpande", "Advait", ""], ["Wang", "Yanming", ""], ["Xu", "Xinyi", ""], ["Madumal", "Prashan", ""], ["Hou", "Anbin", ""]]}, {"id": "2011.00791", "submitter": "Han Zheng", "authors": "Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua Lu, Chengqi\n  Zhang", "title": "Cooperative Heterogeneous Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous deep reinforcement learning agents have been proposed, and each of\nthem has its strengths and flaws. In this work, we present a Cooperative\nHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a\npolicy by integrating the advantages of heterogeneous agents. Specifically, we\npropose a cooperative learning framework that classifies heterogeneous agents\ninto two classes: global agents and local agents. Global agents are off-policy\nagents that can utilize experiences from the other agents. Local agents are\neither on-policy agents or population-based evolutionary algorithms (EAs)\nagents that can explore the local area effectively. We employ global agents,\nwhich are sample-efficient, to guide the learning of local agents so that local\nagents can benefit from sample-efficient agents and simultaneously maintain\ntheir advantages, e.g., stability. Global agents also benefit from effective\nlocal searches. Experimental studies on a range of continuous control tasks\nfrom the Mujoco benchmark show that CHDRL achieves better performance compared\nwith state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:39:09 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zheng", "Han", ""], ["Wei", "Pengfei", ""], ["Jiang", "Jing", ""], ["Long", "Guodong", ""], ["Lu", "Qinghua", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2011.00792", "submitter": "Marcel Wever", "authors": "Eyke H\\\"ullermeier, Marcel Wever, Eneldo Loza Mencia, Johannes\n  F\\\"urnkranz, Michael Rapp", "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is the task of assigning a subset of labels to a\ngiven query instance. For evaluating such predictions, the set of predicted\nlabels needs to be compared to the ground-truth label set associated with that\ninstance, and various loss functions have been proposed for this purpose. In\naddition to assessing predictive accuracy, a key concern in this regard is to\nfoster and to analyze a learner's ability to capture label dependencies. In\nthis paper, we introduce a new class of loss functions for multi-label\nclassification, which overcome disadvantages of commonly used losses such as\nHamming and subset 0/1. To this end, we leverage the mathematical framework of\nnon-additive measures and integrals. Roughly speaking, a non-additive measure\nallows for modeling the importance of correct predictions of label subsets\n(instead of single labels), and thereby their impact on the overall evaluation,\nin a flexible way - by giving full importance to single labels and the entire\nlabel set, respectively, Hamming and subset 0/1 are rather extreme in this\nregard. We present concrete instantiations of this class, which comprise\nHamming and subset 0/1 as special cases, and which appear to be especially\nappealing from a modeling perspective. The assessment of multi-label\nclassifiers in terms of these losses is illustrated in an empirical study.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:42:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["H\u00fcllermeier", "Eyke", ""], ["Wever", "Marcel", ""], ["Mencia", "Eneldo Loza", ""], ["F\u00fcrnkranz", "Johannes", ""], ["Rapp", "Michael", ""]]}, {"id": "2011.00819", "submitter": "Yoan Russac", "authors": "Yoan Russac (DI-ENS, CNRS, PSL, VALDA), Louis Faury, Olivier Capp\\'e\n  (DI-ENS, VALDA), Aur\\'elien Garivier (UMPA-ENSL)", "title": "Self-Concordant Analysis of Generalized Linear Bandits with Forgetting", "comments": null, "journal-ref": "AISTATS 2021 - International Conference on Artificial Intelligence\n  and Statistics, Apr 2021, San Diego / Virtual, United States", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual sequential decision problems with categorical or numerical\nobservations are ubiquitous and Generalized Linear Bandits (GLB) offer a solid\ntheoretical framework to address them. In contrast to the case of linear\nbandits, existing algorithms for GLB have two drawbacks undermining their\napplicability. First, they rely on excessively pessimistic concentration bounds\ndue to the non-linear nature of the model. Second, they require either\nnon-convex projection steps or burn-in phases to enforce boundedness of the\nestimators. Both of these issues are worsened when considering non-stationary\nmodels, in which the GLB parameter may vary with time. In this work, we focus\non self-concordant GLB (which include logistic and Poisson regression) with\nforgetting achieved either by the use of a sliding window or exponential\nweights. We propose a novel confidence-based algorithm for the maximum-likehood\nestimator with forgetting and analyze its perfomance in abruptly changing\nenvironments. These results as well as the accompanying numerical simulations\nhighlight the potential of the proposed approach to address non-stationarity in\nGLB.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:36:39 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:37:14 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Russac", "Yoan", "", "DI-ENS, CNRS, PSL, VALDA"], ["Faury", "Louis", "", "DI-ENS, VALDA"], ["Capp\u00e9", "Olivier", "", "DI-ENS, VALDA"], ["Garivier", "Aur\u00e9lien", "", "UMPA-ENSL"]]}, {"id": "2011.00825", "submitter": "Hayan Yin", "authors": "Haiyan Yin and Yingzhen Li and Sinno Jialin Pan and Cheng Zhang and\n  Sebastian Tschiatschek", "title": "Reinforcement Learning with Efficient Active Feature Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving real-life sequential decision making problems under partial\nobservability involves an exploration-exploitation problem. To be successful,\nan agent needs to efficiently gather valuable information about the state of\nthe world for making rewarding decisions. However, in real-life, acquiring\nvaluable information is often highly costly, e.g., in the medical domain,\ninformation acquisition might correspond to performing a medical test on a\npatient. This poses a significant challenge for the agent to perform optimally\nfor the task while reducing the cost for information acquisition. In this\npaper, we propose a model-based reinforcement learning framework that learns an\nactive feature acquisition policy to solve the exploration-exploitation problem\nduring its execution. Key to the success is a novel sequential variational\nauto-encoder that learns high-quality representations from partially observed\nstates, which are then used by the policy to maximize the task reward in a cost\nefficient manner. We demonstrate the efficacy of our proposed framework in a\ncontrol domain as well as using a medical simulator. In both tasks, our\nproposed method outperforms conventional baselines and results in policies with\ngreater cost efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:46:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yin", "Haiyan", ""], ["Li", "Yingzhen", ""], ["Pan", "Sinno Jialin", ""], ["Zhang", "Cheng", ""], ["Tschiatschek", "Sebastian", ""]]}, {"id": "2011.00851", "submitter": "Yuchen Zhao", "authors": "Yuchen Zhao, Hanyang Liu, Honglin Li, Payam Barnaghi, Hamed Haddadi", "title": "Semi-supervised Federated Learning for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning models on in-home IoT sensory data is commonly used to\nrecognise human activities. Recently, federated learning systems that use edge\ndevices as clients to support local human activity recognition have emerged as\na new paradigm to combine local (individual-level) and global (group-level)\nmodels. This approach provides better scalability and generalisability and also\noffers better privacy compared with the traditional centralised analysis and\nlearning models. The assumption behind federated learning, however, relies on\nsupervised learning on clients. This requires a large volume of labelled data,\nwhich is difficult to collect in uncontrolled IoT environments such as remote\nin-home monitoring.\n  In this paper, we propose an activity recognition system that uses\nsemi-supervised federated learning, wherein clients conduct unsupervised\nlearning on autoencoders with unlabelled local data to learn general\nrepresentations, and a cloud server conducts supervised learning on an activity\nclassifier with labelled data. Our experimental results show that using a long\nshort-term memory autoencoder and a Softmax classifier, the accuracy of our\nproposed system is higher than that of both centralised systems and\nsemi-supervised federated learning using data augmentation. The accuracy is\nalso comparable to that of supervised federated learning systems. Meanwhile, we\ndemonstrate that our system can reduce the number of needed labels and the size\nof local models, and has faster local activity recognition speed than\nsupervised federated learning does.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:47:14 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 15:36:41 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 10:47:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhao", "Yuchen", ""], ["Liu", "Hanyang", ""], ["Li", "Honglin", ""], ["Barnaghi", "Payam", ""], ["Haddadi", "Hamed", ""]]}, {"id": "2011.00866", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Praveenkumar Kanumala, Stephen Guo, Kannan\n  Achan", "title": "An End-to-End ML System for Personalized Conversational Voice Models in\n  Walmart E-Commerce", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for and making decisions about products is becoming increasingly\neasier in the e-commerce space, thanks to the evolution of recommender systems.\nPersonalization and recommender systems have gone hand-in-hand to help\ncustomers fulfill their shopping needs and improve their experiences in the\nprocess. With the growing adoption of conversational platforms for shopping, it\nhas become important to build personalized models at scale to handle the large\ninflux of data and perform inference in real-time. In this work, we present an\nend-to-end machine learning system for personalized conversational voice\ncommerce. We include components for implicit feedback to the model, model\ntraining, evaluation on update, and a real-time inference engine. Our system\npersonalizes voice shopping for Walmart Grocery customers and is currently\navailable via Google Assistant, Siri and Google Home devices.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:14:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Kanumala", "Praveenkumar", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "2011.00869", "submitter": "Andr\\'as Horv\\'ath", "authors": "D\\'ora Babicz, Soma Kont\\'ar, M\\'ark Pet\\H{o}, Andr\\'as F\\\"ul\\\"op,\n  Gergely Szab\\'o, Andr\\'as Horv\\'ath", "title": "Receptive Field Size Optimization with Continuous Time Pooling", "comments": "Paper accepted for WACV : Workshop on Applications of Computer Vision\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pooling operation is a cornerstone element of convolutional neural\nnetworks. These elements generate receptive fields for neurons, in which local\nperturbations should have minimal effect on the output activations, increasing\nrobustness and invariance of the network. In this paper we will present an\naltered version of the most commonly applied method, maximum pooling, where\npooling in theory is substituted by a continuous time differential equation,\nwhich generates a location sensitive pooling operation, more similar to\nbiological receptive fields. We will present how this continuous method can be\napproximated numerically using discrete operations which fit ideally on a GPU.\nIn our approach the kernel size is substituted by diffusion strength which is a\ncontinuous valued parameter, this way it can be optimized by gradient descent\nalgorithms. We will evaluate the effect of continuous pooling on accuracy and\ncomputational need using commonly applied network architectures and datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:21:51 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 21:49:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Babicz", "D\u00f3ra", ""], ["Kont\u00e1r", "Soma", ""], ["Pet\u0151", "M\u00e1rk", ""], ["F\u00fcl\u00f6p", "Andr\u00e1s", ""], ["Szab\u00f3", "Gergely", ""], ["Horv\u00e1th", "Andr\u00e1s", ""]]}, {"id": "2011.00890", "submitter": "Yaoyiran Li", "authors": "Yaoyiran Li, Edoardo M. Ponti, Ivan Vuli\\'c and Anna Korhonen", "title": "Emergent Communication Pretraining for Few-Shot Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While state-of-the-art models that rely upon massively multilingual\npretrained encoders achieve sample efficiency in downstream applications, they\nstill require abundant amounts of unlabelled text. Nevertheless, most of the\nworld's languages lack such resources. Hence, we investigate a more radical\nform of unsupervised knowledge transfer in the absence of linguistic data. In\nparticular, for the first time we pretrain neural networks via emergent\ncommunication from referential games. Our key assumption is that grounding\ncommunication on images---as a crude approximation of real-world\nenvironments---inductively biases the model towards learning natural languages.\nOn the one hand, we show that this substantially benefits machine translation\nin few-shot settings. On the other hand, this also provides an extrinsic\nevaluation protocol to probe the properties of emergent languages ex vitro.\nIntuitively, the closer they are to natural languages, the higher the gains\nfrom pretraining on them should be. For instance, in this work we measure the\ninfluence of communication success and maximum sequence length on downstream\nperformances. Finally, we introduce a customised adapter layer and annealing\nstrategies for the regulariser of maximum-a-posteriori inference during\nfine-tuning. These turn out to be crucial to facilitate knowledge transfer and\nprevent catastrophic forgetting. Compared to a recurrent baseline, our method\nyields gains of $59.0\\%$$\\sim$$147.6\\%$ in BLEU score with only $500$ NMT\ntraining instances and $65.1\\%$$\\sim$$196.7\\%$ with $1,000$ NMT training\ninstances across four language pairs. These proof-of-concept results reveal the\npotential of emergent communication pretraining for both natural language\nprocessing tasks in resource-poor settings and extrinsic evaluation of\nartificial languages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:57:53 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Yaoyiran", ""], ["Ponti", "Edoardo M.", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""]]}, {"id": "2011.00905", "submitter": "Simon Razniewski", "authors": "Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum", "title": "Advanced Semantics for Commonsense Knowledge Extraction", "comments": "Web interface available at https://ascent.mpi-inf.mpg.de", "journal-ref": "WWW 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge (CSK) about concepts and their properties is useful for\nAI applications such as robust chatbots. Prior works like ConceptNet, TupleKB\nand others compiled large CSK collections, but are restricted in their\nexpressiveness to subject-predicate-object (SPO) triples with simple concepts\nfor S and monolithic strings for P and O. Also, these projects have either\nprioritized precision or recall, but hardly reconcile these complementary\ngoals. This paper presents a methodology, called Ascent, to automatically build\na large-scale knowledge base (KB) of CSK assertions, with advanced\nexpressiveness and both better precision and recall than prior works. Ascent\ngoes beyond triples by capturing composite concepts with subgroups and aspects,\nand by refining assertions with semantic facets. The latter are important to\nexpress temporal and spatial validity of assertions and further qualifiers.\nAscent combines open information extraction with judicious cleaning using\nlanguage models. Intrinsic evaluation shows the superior size and quality of\nthe Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of Ascent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:37:17 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:41:40 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Nguyen", "Tuan-Phong", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2011.00956", "submitter": "Binbin Jin", "authors": "Binbin Jin, Defu Lian, Zheng Liu, Qi Liu, Jianhui Ma, Xing Xie, Enhong\n  Chen", "title": "Sampling-Decomposable Generative Adversarial Recommender", "comments": "Thirty-fourth Conference on Neural Information Processing Systems\n  (NeurIPS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation techniques are important approaches for alleviating\ninformation overload. Being often trained on implicit user feedback, many\nrecommenders suffer from the sparsity challenge due to the lack of explicitly\nnegative samples. The GAN-style recommenders (i.e., IRGAN) addresses the\nchallenge by learning a generator and a discriminator adversarially, such that\nthe generator produces increasingly difficult samples for the discriminator to\naccelerate optimizing the discrimination objective. However, producing samples\nfrom the generator is very time-consuming, and our empirical study shows that\nthe discriminator performs poor in top-k item recommendation. To this end, a\ntheoretical analysis is made for the GAN-style algorithms, showing that the\ngenerator of limit capacity is diverged from the optimal generator. This may\ninterpret the limitation of discriminator's performance. Based on these\nfindings, we propose a Sampling-Decomposable Generative Adversarial Recommender\n(SD-GAR). In the framework, the divergence between some generator and the\noptimum is compensated by self-normalized importance sampling; the efficiency\nof sample generation is improved with a sampling-decomposable generator, such\nthat each sample can be generated in O(1) with the Vose-Alias method.\nInterestingly, due to decomposability of sampling, the generator can be\noptimized with the closed-form solutions in an alternating manner, being\ndifferent from policy gradient in the GAN-style algorithms. We extensively\nevaluate the proposed algorithm with five real-world recommendation datasets.\nThe results show that SD-GAR outperforms IRGAN by 12.4% and the SOTA\nrecommender by 10% on average. Moreover, discriminator training can be 20x\nfaster on the dataset with more than 120K items.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:19:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Jin", "Binbin", ""], ["Lian", "Defu", ""], ["Liu", "Zheng", ""], ["Liu", "Qi", ""], ["Ma", "Jianhui", ""], ["Xie", "Xing", ""], ["Chen", "Enhong", ""]]}, {"id": "2011.00958", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Bo Zhou, Paul Lukowicz, Andrey Somov", "title": "Collection and Validation of Psycophysiological Data from Professional\n  and Amateur Players: a Multimodal eSports Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper training and analytics in eSports require accurately collected and\nannotated data. Most eSports research focuses exclusively on in-game data\nanalysis, and there is a lack of prior work involving eSports athletes'\npsychophysiological data. In this paper, we present a dataset collected from\nprofessional and amateur teams in 22 matches in League of Legends video game.\nRecorded data include the players' physiological activity, e.g. movements,\npulse, saccades, obtained from various sensors, self-reported after-match\nsurvey, and in-game data. An important feature of the dataset is simultaneous\ndata collection from five players, which facilitates the analysis of sensor\ndata on a team level. Upon the collection of dataset we carried out its\nvalidation. In particular, we demonstrate that stress and concentration levels\nfor professional players are less correlated, meaning more independent\nplaystyle. Also, we show that the absence of team communication does not affect\nthe professional players as much as amateur ones. To investigate other possible\nuse cases of the dataset, we have trained classical machine learning algorithms\nfor skill prediction and player re-identification using 3-minute sessions of\nsensor data. Best models achieved 0.856 and 0.521 (0.10 for a chance level)\naccuracy scores on a validation set for skill prediction and player re-id\nproblems, respectively. The dataset is available at\nhttps://github.com/asmerdov/eSports_Sensors_Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:25:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Smerdov", "Anton", ""], ["Zhou", "Bo", ""], ["Lukowicz", "Paul", ""], ["Somov", "Andrey", ""]]}, {"id": "2011.00971", "submitter": "Jiayuan Gu", "authors": "Tongzhou Mu, Jiayuan Gu, Zhiwei Jia, Hao Tang, Hao Su", "title": "Refactoring Policy for Compositional Generalizability using\n  Self-Supervised Object Proposals", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to learn a policy with compositional generalizability. We\npropose a two-stage framework, which refactorizes a high-reward teacher policy\ninto a generalizable student policy with strong inductive bias. Particularly,\nwe implement an object-centric GNN-based student policy, whose input objects\nare learned from images through self-supervised learning. Empirically, we\nevaluate our approach on four difficult tasks that require compositional\ngeneralizability, and achieve superior performance compared to baselines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:46:08 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mu", "Tongzhou", ""], ["Gu", "Jiayuan", ""], ["Jia", "Zhiwei", ""], ["Tang", "Hao", ""], ["Su", "Hao", ""]]}, {"id": "2011.01010", "submitter": "Hal Ashton", "authors": "Hal Ashton", "title": "Causal Campbell-Goodhart's law and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Campbell-Goodhart's law relates to the causal inference error whereby\ndecision-making agents aim to influence variables which are correlated to their\ngoal objective but do not reliably cause it. This is a well known error in\nEconomics and Political Science but not widely labelled in Artificial\nIntelligence research. Through a simple example, we show how off-the-shelf deep\nReinforcement Learning (RL) algorithms are not necessarily immune to this\ncognitive error. The off-policy learning method is tricked, whilst the\non-policy method is not. The practical implication is that naive application of\nRL to complex real life problems can result in the same types of policy errors\nthat humans make. Great care should be taken around understanding the causal\nmodel that underpins a solution derived from Reinforcement Learning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:42:20 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 10:19:00 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ashton", "Hal", ""]]}, {"id": "2011.01014", "submitter": "Berk Kapicioglu", "authors": "Berk Kapicioglu, Ramiz Iqbal, Tarik Koc, Louis Nicolas Andre,\n  Katharina Sophia Volz", "title": "Chess2vec: Learning Vector Representations for Chess", "comments": "Relational Representation Learning Workshop, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct the first study of its kind to generate and evaluate vector\nrepresentations for chess pieces. In particular, we uncover the latent\nstructure of chess pieces and moves, as well as predict chess moves from chess\npositions. We share preliminary results which anticipate our ongoing work on a\nneural network architecture that learns these embeddings directly from\nsupervised feedback.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:50:48 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kapicioglu", "Berk", ""], ["Iqbal", "Ramiz", ""], ["Koc", "Tarik", ""], ["Andre", "Louis Nicolas", ""], ["Volz", "Katharina Sophia", ""]]}, {"id": "2011.01043", "submitter": "Utkarsh Desai", "authors": "Raunak Sinha, Utkarsh Desai, Srikanth Tamilselvam, Senthil Mani", "title": "Evaluation of Siamese Networks for Semantic Code Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in the number of open repositories and discussion forums,\nthe use of natural language for semantic code search has become increasingly\ncommon. The accuracy of the results returned by such systems, however, can be\nlow due to 1) limited shared vocabulary between code and user query and 2)\ninadequate semantic understanding of user query and its relation to code\nsyntax. Siamese networks are well suited to learning such joint relations\nbetween data, but have not been explored in the context of code search. In this\nwork, we evaluate Siamese networks for this task by exploring multiple\nextraction network architectures. These networks independently process code and\ntext descriptions before passing them to a Siamese network to learn embeddings\nin a common space. We experiment on two different datasets and discover that\nSiamese networks can act as strong regularizers on networks that extract rich\ninformation from code and text, which in turn helps achieve impressive\nperformance on code search beating previous baselines on $2$ programming\nlanguages. We also analyze the embedding space of these networks and provide\ndirections to fully leverage the power of Siamese networks for semantic code\nsearch.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 06:07:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sinha", "Raunak", ""], ["Desai", "Utkarsh", ""], ["Tamilselvam", "Srikanth", ""], ["Mani", "Senthil", ""]]}, {"id": "2011.01046", "submitter": "Yuxuan Li", "authors": "Nan Lin, Yuxuan Li, Yujun Zhu, Ruolin Wang, Xiayu Zhang, Jianmin Ji,\n  Keke Tang, Xiaoping Chen, Xinming Zhang", "title": "NEARL: Non-Explicit Action Reinforcement Learning for Robotic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, reinforcement learning methods predict the next action based\non the current state. However, in many situations, directly applying actions to\ncontrol systems or robots is dangerous and may lead to unexpected behaviors\nbecause action is rather low-level. In this paper, we propose a novel\nhierarchical reinforcement learning framework without explicit action. Our meta\npolicy tries to manipulate the next optimal state and actual action is produced\nby the inverse dynamics model. To stabilize the training process, we integrate\nadversarial learning and information bottleneck into our framework. Under our\nframework, widely available state-only demonstrations can be exploited\neffectively for imitation learning. Also, prior knowledge and constraints can\nbe applied to meta policy. We test our algorithm in simulation tasks and its\ncombination with imitation learning. The experimental results show the\nreliability and robustness of our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:28:19 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lin", "Nan", ""], ["Li", "Yuxuan", ""], ["Zhu", "Yujun", ""], ["Wang", "Ruolin", ""], ["Zhang", "Xiayu", ""], ["Ji", "Jianmin", ""], ["Tang", "Keke", ""], ["Chen", "Xiaoping", ""], ["Zhang", "Xinming", ""]]}, {"id": "2011.01075", "submitter": "Nan Jiang", "authors": "Philip Amortila, Nan Jiang, Tengyang Xie", "title": "A Variant of the Wang-Foster-Kakade Lower Bound for the Discounted\n  Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Wang et al. (2020) showed a highly intriguing hardness result for\nbatch reinforcement learning (RL) with linearly realizable value function and\ngood feature coverage in the finite-horizon case. In this note we show that\nonce adapted to the discounted setting, the construction can be simplified to a\n2-state MDP with 1-dimensional features, such that learning is impossible even\nwith an infinite amount of data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:04:42 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 03:53:02 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Amortila", "Philip", ""], ["Jiang", "Nan", ""], ["Xie", "Tengyang", ""]]}, {"id": "2011.01102", "submitter": "Liangming Pan", "authors": "Yuxi Xie, Liangming Pan, Dongzhe Wang, Min-Yen Kan, Yansong Feng", "title": "Exploring Question-Specific Rewards for Generating Deep Questions", "comments": "COLING 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent question generation (QG) approaches often utilize the\nsequence-to-sequence framework (Seq2Seq) to optimize the log-likelihood of\nground-truth questions using teacher forcing. However, this training objective\nis inconsistent with actual question quality, which is often reflected by\ncertain global properties such as whether the question can be answered by the\ndocument. As such, we directly optimize for QG-specific objectives via\nreinforcement learning to improve question quality. We design three different\nrewards that target to improve the fluency, relevance, and answerability of\ngenerated questions. We conduct both automatic and human evaluations in\naddition to a thorough analysis to explore the effect of each QG-specific\nreward. We find that optimizing question-specific rewards generally leads to\nbetter performance in automatic evaluation metrics. However, only the rewards\nthat correlate well with human judgement (e.g., relevance) lead to real\nimprovement in question quality. Optimizing for the others, especially\nanswerability, introduces incorrect bias to the model, resulting in poor\nquestion quality. Our code is publicly available at\nhttps://github.com/YuxiXie/RL-for-Question-Generation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:37:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Xie", "Yuxi", ""], ["Pan", "Liangming", ""], ["Wang", "Dongzhe", ""], ["Kan", "Min-Yen", ""], ["Feng", "Yansong", ""]]}, {"id": "2011.01103", "submitter": "Danilo Dess\\`i", "authors": "Danilo Dess\\`i, Francesco Osborne, Diego Reforgiato Recupero, Davide\n  Buscaldi, Enrico Motta", "title": "Generating Knowledge Graphs by Employing Natural Language Processing and\n  Machine Learning Techniques within the Scholarly Domain", "comments": "Accepted for publication in Future Generation Computer Systems\n  journal - Special Issue on Machine Learning and Knowledge Graphs", "journal-ref": null, "doi": "10.1016/j.future.2020.10.026", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous growth of scientific literature brings innovations and, at the\nsame time, raises new challenges. One of them is related to the fact that its\nanalysis has become difficult due to the high volume of published papers for\nwhich manual effort for annotations and management is required. Novel\ntechnological infrastructures are needed to help researchers, research policy\nmakers, and companies to time-efficiently browse, analyse, and forecast\nscientific research. Knowledge graphs i.e., large networks of entities and\nrelationships, have proved to be effective solution in this space. Scientific\nknowledge graphs focus on the scholarly domain and typically contain metadata\ndescribing research publications such as authors, venues, organizations,\nresearch topics, and citations. However, the current generation of knowledge\ngraphs lacks of an explicit representation of the knowledge presented in the\nresearch papers. As such, in this paper, we present a new architecture that\ntakes advantage of Natural Language Processing and Machine Learning methods for\nextracting entities and relationships from research publications and integrates\nthem in a large-scale knowledge graph. Within this research work, we i) tackle\nthe challenge of knowledge extraction by employing several state-of-the-art\nNatural Language Processing and Text Mining tools, ii) describe an approach for\nintegrating entities and relationships generated by these tools, iii) show the\nadvantage of such an hybrid system over alternative approaches, and vi) as a\nchosen use case, we generated a scientific knowledge graph including 109,105\ntriples, extracted from 26,827 abstracts of papers within the Semantic Web\ndomain. As our approach is general and can be applied to any domain, we expect\nthat it can facilitate the management, analysis, dissemination, and processing\nof scientific knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:31:40 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Dess\u00ec", "Danilo", ""], ["Osborne", "Francesco", ""], ["Recupero", "Diego Reforgiato", ""], ["Buscaldi", "Davide", ""], ["Motta", "Enrico", ""]]}, {"id": "2011.01112", "submitter": "Shuochao Yao", "authors": "Shuochao Yao, Yifan Hao, Yiran Zhao, Huajie Shao, Dongxin Liu,\n  Shengzhong Liu, Tianshi Wang, Jinyang Li, Tarek Abdelzaher", "title": "Scheduling Real-time Deep Learning Services as Imprecise Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an efficient real-time scheduling algorithm for\nintelligent real-time edge services, defined as those that perform machine\nintelligence tasks, such as voice recognition, LIDAR processing, or machine\nvision, on behalf of local embedded devices that are themselves unable to\nsupport extensive computations. The work contributes to a recent direction in\nreal-time computing that develops scheduling algorithms for machine\nintelligence tasks with anytime prediction. We show that deep neural network\nworkflows can be cast as imprecise computations, each with a mandatory part and\n(several) optional parts whose execution utility depends on input data. The\ngoal of the real-time scheduler is to maximize the average accuracy of deep\nneural network outputs while meeting task deadlines, thanks to opportunistic\nshedding of the least necessary optional parts. The work is motivated by the\nproliferation of increasingly ubiquitous but resource-constrained embedded\ndevices (for applications ranging from autonomous cars to the Internet of\nThings) and the desire to develop services that endow them with intelligence.\nExperiments on recent GPU hardware and a state of the art deep neural network\nfor machine vision illustrate that our scheme can increase the overall accuracy\nby 10%-20% while incurring (nearly) no deadline misses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:43:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yao", "Shuochao", ""], ["Hao", "Yifan", ""], ["Zhao", "Yiran", ""], ["Shao", "Huajie", ""], ["Liu", "Dongxin", ""], ["Liu", "Shengzhong", ""], ["Wang", "Tianshi", ""], ["Li", "Jinyang", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "2011.01129", "submitter": "Jingxi Chen", "authors": "Jingxi Chen, Amrish Baskaran, Zhongshun Zhang, Pratap Tokekar", "title": "Multi-Agent Reinforcement Learning for Persistent Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Persistent Monitoring (PM) problem seeks to find a set of trajectories\n(or controllers) for robots to persistently monitor a changing environment.\nEach robot has a limited field-of-view and may need to coordinate with others\nto ensure no point in the environment is left unmonitored for long periods of\ntime. We model the problem such that there is a penalty that accrues every time\nstep if a point is left unmonitored. However, the dynamics of the penalty are\nunknown to us. We present a Multi-Agent Reinforcement Learning (MARL) algorithm\nfor the persistent monitoring problem. Specifically, we present a Multi-Agent\nGraph Attention Proximal Policy Optimization (MA-G-PPO) algorithm that takes as\ninput the local observations of all agents combined with a low resolution\nglobal map to learn a policy for each agent. The graph attention allows agents\nto share their information with others leading to an effective joint policy.\nOur main focus is to understand how effective MARL is for the PM problem. We\ninvestigate five research questions with this broader goal. We find that\nMA-G-PPO is able to learn a better policy than the non-RL baseline in most\ncases, the effectiveness depends on agents sharing information with each other,\nand the policy learnt shows emergent behavior for the agents.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:05:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Jingxi", ""], ["Baskaran", "Amrish", ""], ["Zhang", "Zhongshun", ""], ["Tokekar", "Pratap", ""]]}, {"id": "2011.01141", "submitter": "Junghoon Kim", "authors": "Junghoon Kim, Seyyedali Hosseinalipour, Taejoon Kim, David J. Love,\n  Christopher G. Brinton", "title": "Multi-IRS-assisted Multi-Cell Uplink MIMO Communications under Imperfect\n  CSI: A Deep Reinforcement Learning Approach", "comments": "7 pages, 3 figures, Accepted for publication in Proceedings of IEEE\n  International Conference on Communications (ICC) Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of intelligent reflecting surfaces (IRSs) in wireless networks\nhave attracted significant attention recently. Most of the relevant literature\nis focused on the single cell setting where a single IRS is deployed and\nperfect channel state information (CSI) is assumed. In this work, we develop a\nnovel methodology for multi-IRS-assisted multi-cell networks in the uplink. We\nconsider the scenario in which (i) channels are dynamic and (ii) only partial\nCSI is available at each base station (BS); specifically, scalar effective\nchannel powers from only a subset of user equipments (UE). We formulate the\nsum-rate maximization problem aiming to jointly optimize the IRS reflect\nbeamformers, BS combiners, and UE transmit powers. In casting this as a\nsequential decision making problem, we propose a multi-agent deep reinforcement\nlearning algorithm to solve it, where each BS acts as an independent agent in\ncharge of tuning the local UE transmit powers, the local IRS reflect\nbeamformer, and its combiners. We introduce an efficient information-sharing\nscheme that requires limited information exchange among neighboring BSs to cope\nwith the non-stationarity caused by the coupling of actions taken by multiple\nBSs. Our numerical results show that our method obtains substantial improvement\nin average data rate compared to baseline approaches, e.g., fixed UE transmit\npower and maximum ratio combining.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:33:23 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 18:48:29 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 17:16:00 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 19:21:06 GMT"}, {"version": "v5", "created": "Wed, 24 Feb 2021 12:32:14 GMT"}, {"version": "v6", "created": "Thu, 1 Apr 2021 10:22:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kim", "Junghoon", ""], ["Hosseinalipour", "Seyyedali", ""], ["Kim", "Taejoon", ""], ["Love", "David J.", ""], ["Brinton", "Christopher G.", ""]]}, {"id": "2011.01188", "submitter": "Mohamed Mejri", "authors": "Mohamed Mejri and Aymen Mejri", "title": "RandomForestMLP: An Ensemble-Based Multi-Layer Perceptron Against Curse\n  of Dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel and practical deep learning pipeline termed\nRandomForestMLP. This core trainable classification engine consists of a\nconvolutional neural network backbone followed by an ensemble-based multi-layer\nperceptrons core for the classification task. It is designed in the context of\nself and semi-supervised learning tasks to avoid overfitting while training on\nvery small datasets. The paper details the architecture of the RandomForestMLP\nand present different strategies for neural network decision aggregation. Then,\nit assesses its robustness to overfitting when trained on realistic image\ndatasets and compares its classification performance with existing regular\nclassifiers.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:25:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mejri", "Mohamed", ""], ["Mejri", "Aymen", ""]]}, {"id": "2011.01251", "submitter": "Shelby Smith", "authors": "Shelby Smith and Chaitanya Baru", "title": "NSF Convergence Approach to Transition Basic Research into Practice", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Science Foundation Convergence Accelerator addresses\nnational-scale societal challenges through use-inspired convergence research.\nLeveraging a convergence approach the Convergence Accelerator builds upon basic\nresearch and discovery to make timely investments to strengthen the Nations\ninnovation ecosystem associated with several key R&D priority areas and\npractices to include the coronavirus disease 2019, harnessing the data\nrevolution, the future of work, and quantum technology. Artificial Intelligence\nis a key underlying theme across all of these areas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 19:00:22 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Smith", "Shelby", ""], ["Baru", "Chaitanya", ""]]}, {"id": "2011.01297", "submitter": "Paniz Behboudian", "authors": "Paniz Behboudian, Yash Satsangi, Matthew E. Taylor, Anna Harutyunyan,\n  Michael Bowling", "title": "Useful Policy Invariant Shaping from Arbitrary Advice", "comments": "9 pages, 6 figures, Adaptive and Learning Agents (ALA) 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful learning paradigm in which agents can\nlearn to maximize sparse and delayed reward signals. Although RL has had many\nimpressive successes in complex domains, learning can take hours, days, or even\nyears of training data. A major challenge of contemporary RL research is to\ndiscover how to learn with less data. Previous work has shown that domain\ninformation can be successfully used to shape the reward; by adding additional\nreward information, the agent can learn with much less data. Furthermore, if\nthe reward is constructed from a potential function, the optimal policy is\nguaranteed to be unaltered. While such potential-based reward shaping (PBRS)\nholds promise, it is limited by the need for a well-defined potential function.\nIdeally, we would like to be able to take arbitrary advice from a human or\nother agent and improve performance without affecting the optimal policy. The\nrecently introduced dynamic potential based advice (DPBA) method tackles this\nchallenge by admitting arbitrary advice from a human or other agent and\nimproves performance without affecting the optimal policy. The main\ncontribution of this paper is to expose, theoretically and empirically, a flaw\nin DPBA. Alternatively, to achieve the ideal goals, we present a simple method\ncalled policy invariant explicit shaping (PIES) and show theoretically and\nempirically that PIES succeeds where DPBA fails.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 20:29:09 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Behboudian", "Paniz", ""], ["Satsangi", "Yash", ""], ["Taylor", "Matthew E.", ""], ["Harutyunyan", "Anna", ""], ["Bowling", "Michael", ""]]}, {"id": "2011.01306", "submitter": "Duo Wang", "authors": "Nicholas Quek Wei Kiat, Duo Wang, Mateja Jamnik", "title": "Pairwise Relations Discriminator for Unsupervised Raven's Progressive\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract reasoning is a key indicator of intelligence. The ability to\nhypothesise, develop abstract concepts based on concrete observations and apply\nthis hypothesis to justify future actions has been paramount in human\ndevelopment. An existing line of research in outfitting intelligent machines\nwith abstract reasoning capabilities revolves around the Raven's Progressive\nMatrices (RPM), a multiple-choice visual puzzle where one must identify the\nmissing component which completes the pattern. There have been many\nbreakthroughs in supervised approaches to solving RPM in recent years. However,\nsince this process requires external assistance, we cannot claim that machines\nhave achieved reasoning ability comparable to humans. Namely, when the RPM rule\nthat relations can only exist row/column-wise is properly introduced, humans\ncan solve RPM problems without supervision or prior experience. In this paper,\nwe introduce a pairwise relations discriminator (PRD), a technique to develop\nunsupervised models with sufficient reasoning abilities to tackle an RPM\nproblem. PRD reframes the RPM problem into a relation comparison task, which we\ncan solve without requiring the labelling of the RPM problem. We can identify\nthe optimal candidate by adapting the application of PRD on the RPM problem.\nThe previous state-of-the-art approach \"mcpt\" in this domain achieved 28.5%\naccuracy on the RAVEN dataset \"drt\", a standard dataset for computational work\non RPM. Our approach, the PRD, establishes a new state-of-the-art benchmark\nwith an accuracy of 50.74% on the same dataset, presenting a significant\nimprovement and a step forward in equipping machines with abstract reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 20:49:46 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kiat", "Nicholas Quek Wei", ""], ["Wang", "Duo", ""], ["Jamnik", "Mateja", ""]]}, {"id": "2011.01307", "submitter": "Luke Melas-Kyriazi", "authors": "Luke Melas-Kyriazi", "title": "The Mathematical Foundations of Manifold Learning", "comments": "Undergraduate Thesis (Harvard Mathematics Department)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is a popular and quickly-growing subfield of machine\nlearning based on the assumption that one's observed data lie on a\nlow-dimensional manifold embedded in a higher-dimensional space. This thesis\npresents a mathematical perspective on manifold learning, delving into the\nintersection of kernel learning, spectral graph theory, and differential\ngeometry. Emphasis is placed on the remarkable interplay between graphs and\nmanifolds, which forms the foundation for the widely-used technique of manifold\nregularization. This work is written to be accessible to a broad mathematical\naudience, including machine learning researchers and practitioners interested\nin understanding the theorems underlying popular manifold learning algorithms\nand dimensionality reduction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:04:20 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Melas-Kyriazi", "Luke", ""]]}, {"id": "2011.01314", "submitter": "Ganesh Jawahar", "authors": "Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V.S. Lakshmanan", "title": "Automatic Detection of Machine Generated Text: A Critical Survey", "comments": "The 28th International Conference on Computational Linguistics\n  (COLING), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generative models (TGMs) excel in producing text that matches the style\nof human language reasonably well. Such TGMs can be misused by adversaries,\ne.g., by automatically generating fake news and fake product reviews that can\nlook authentic and fool humans. Detectors that can distinguish text generated\nby TGM from human written text play a vital role in mitigating such misuse of\nTGMs. Recently, there has been a flurry of works from both natural language\nprocessing (NLP) and machine learning (ML) communities to build accurate\ndetectors for English. Despite the importance of this problem, there is\ncurrently no work that surveys this fast-growing literature and introduces\nnewcomers to important research challenges. In this work, we fill this void by\nproviding a critical survey and review of this literature to facilitate a\ncomprehensive understanding of this problem. We conduct an in-depth error\nanalysis of the state-of-the-art detector and discuss research directions to\nguide future work in this exciting area.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 20:59:26 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Jawahar", "Ganesh", ""], ["Abdul-Mageed", "Muhammad", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "2011.01324", "submitter": "Peter Xenopoulos", "authors": "Peter Xenopoulos, Harish Doraiswamy, Claudio Silva", "title": "Valuing Player Actions in Counter-Strike: Global Offensive", "comments": "to be published in 2020 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Esports, despite its expanding interest, lacks fundamental sports analytics\nresources such as accessible data or proven and reproducible analytical\nframeworks. Even Counter-Strike: Global Offensive (CSGO), the second most\npopular esport, suffers from these problems. Thus, quantitative evaluation of\nCSGO players, a task important to teams, media, bettors and fans, is difficult.\nTo address this, we introduce (1) a data model for CSGO with an open-source\nimplementation; (2) a graph distance measure for defining distances in CSGO;\nand (3) a context-aware framework to value players' actions based on changes in\ntheir team's chances of winning. Using over 70 million in-game CSGO events, we\ndemonstrate our framework's consistency and independence compared to existing\nvaluation frameworks. We also provide use cases demonstrating high-impact play\nidentification and uncertainty estimation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 21:11:14 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 18:35:51 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Xenopoulos", "Peter", ""], ["Doraiswamy", "Harish", ""], ["Silva", "Claudio", ""]]}, {"id": "2011.01337", "submitter": "Pablo Barros", "authors": "Pablo Barros, Ana Tanevska, Ozge Yalcin, Alessandra Sciutti", "title": "Incorporating Rivalry in Reinforcement Learning for a Competitive Game", "comments": "Accepted at the Pre-registration Workshop @ NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in reinforcement learning with social agents have allowed us\nto achieve human-level performance on some interaction tasks. However, most\ninteractive scenarios do not have as end-goal performance alone; instead, the\nsocial impact of these agents when interacting with humans is as important and,\nin most cases, never explored properly. This preregistration study focuses on\nproviding a novel learning mechanism based on a rivalry social impact. Our\nscenario explored different reinforcement learning-based agents playing a\ncompetitive card game against human players. Based on the concept of\ncompetitive rivalry, our analysis aims to investigate if we can change the\nassessment of these agents from a human perspective.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 21:54:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Barros", "Pablo", ""], ["Tanevska", "Ana", ""], ["Yalcin", "Ozge", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "2011.01353", "submitter": "Yuheng Wang", "authors": "Yuheng Wang, Wen Jie Zhao, Jiahui Xu and Raymond Hong", "title": "Recyclable Waste Identification Using CNN Image Recognition and Gaussian\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Waste recycling is an important way of saving energy and materials in the\nproduction process. In general cases recyclable objects are mixed with\nunrecyclable objects, which raises a need for identification and\nclassification. This paper proposes a convolutional neural network (CNN) model\nto complete both tasks. The model uses transfer learning from a pretrained\nResnet-50 CNN to complete feature extraction. A subsequent fully connected\nlayer for classification was trained on the augmented TrashNet dataset [1]. In\nthe application, sliding-window is used for image segmentation in the\npre-classification stage. In the post-classification stage, the labelled sample\npoints are integrated with Gaussian Clustering to locate the object. The\nresulting model has achieved an overall detection rate of 48.4% in simulation\nand final classification accuracy of 92.4%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:26:25 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Yuheng", ""], ["Zhao", "Wen Jie", ""], ["Xu", "Jiahui", ""], ["Hong", "Raymond", ""]]}, {"id": "2011.01381", "submitter": "Dennis Wei", "authors": "Dennis Wei", "title": "Optimal Policies for the Homogeneous Selective Labels Problem", "comments": "12 pages, 1 figure. To be presented at the Workshop on Consequential\n  Decision Making in Dynamic Environments at the 34th Conference on Neural\n  Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective labels are a common feature of consequential decision-making\napplications, referring to the lack of observed outcomes under one of the\npossible decisions. This paper reports work in progress on learning decision\npolicies in the face of selective labels. The setting considered is both a\nsimplified homogeneous one, disregarding individuals' features to facilitate\ndetermination of optimal policies, and an online one, to balance costs incurred\nin learning with future utility. For maximizing discounted total reward, the\noptimal policy is shown to be a threshold policy, and the problem is one of\noptimal stopping. In contrast, for undiscounted infinite-horizon average\nreward, optimal policies have positive acceptance probability in all states.\nFuture work stemming from these results is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 23:32:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wei", "Dennis", ""]]}, {"id": "2011.01393", "submitter": "Xu Chen", "authors": "Yunpeng Weng and Xu Chen and Liang Chen and Wei Liu", "title": "GAIN: Graph Attention & Interaction Network for Inductive\n  Semi-Supervised Learning over Large-scale Graphs", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have led to state-of-the-art performance on a\nvariety of machine learning tasks such as recommendation, node classification\nand link prediction. Graph neural network models generate node embeddings by\nmerging nodes features with the aggregated neighboring nodes information. Most\nexisting GNN models exploit a single type of aggregator (e.g., mean-pooling) to\naggregate neighboring nodes information, and then add or concatenate the output\nof aggregator to the current representation vector of the center node. However,\nusing only a single type of aggregator is difficult to capture the different\naspects of neighboring information and the simple addition or concatenation\nupdate methods limit the expressive capability of GNNs. Not only that, existing\nsupervised or semi-supervised GNN models are trained based on the loss function\nof the node label, which leads to the neglect of graph structure information.\nIn this paper, we propose a novel graph neural network architecture, Graph\nAttention \\& Interaction Network (GAIN), for inductive learning on graphs.\nUnlike the previous GNN models that only utilize a single type of aggregation\nmethod, we use multiple types of aggregators to gather neighboring information\nin different aspects and integrate the outputs of these aggregators through the\naggregator-level attention mechanism. Furthermore, we design a graph\nregularized loss to better capture the topological relationship of the nodes in\nthe graph. Additionally, we first present the concept of graph feature\ninteraction and propose a vector-wise explicit feature interaction mechanism to\nupdate the node embeddings. We conduct comprehensive experiments on two\nnode-classification benchmarks and a real-world financial news dataset. The\nexperiments demonstrate our GAIN model outperforms current state-of-the-art\nperformances on all the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 00:20:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Weng", "Yunpeng", ""], ["Chen", "Xu", ""], ["Chen", "Liang", ""], ["Liu", "Wei", ""]]}, {"id": "2011.01397", "submitter": "Paulo Santos", "authors": "Danilo Perico and Paulo E. Santos and Reinaldo Bianchi", "title": "Guided Navigation from Multiple Viewpoints using Qualitative Spatial\n  Reasoning", "comments": "26 pages", "journal-ref": "Spatial Cognition and Computation - 2020", "doi": "10.1080/13875868.2020.1857386", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation is an essential ability for mobile agents to be completely\nautonomous and able to perform complex actions. However, the problem of\nnavigation for agents with limited (or no) perception of the world, or devoid\nof a fully defined motion model, has received little attention from research in\nAI and Robotics. One way to tackle this problem is to use guided navigation, in\nwhich other autonomous agents, endowed with perception, can combine their\ndistinct viewpoints to infer the localisation and the appropriate commands to\nguide a sensory deprived agent through a particular path. Due to the limited\nknowledge about the physical and perceptual characteristics of the guided\nagent, this task should be conducted on a level of abstraction allowing the use\nof a generic motion model, and high-level commands, that can be applied by any\ntype of autonomous agents, including humans. The main task considered in this\nwork is, given a group of autonomous agents perceiving their common environment\nwith their independent, egocentric and local vision sensors, the development\nand evaluation of algorithms capable of producing a set of high-level commands\n(involving qualitative directions: e.g. move left, go straight ahead) capable\nof guiding a sensory deprived robot to a goal location.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 00:34:26 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Perico", "Danilo", ""], ["Santos", "Paulo E.", ""], ["Bianchi", "Reinaldo", ""]]}, {"id": "2011.01439", "submitter": "Erik Li", "authors": "Xiaoyi Li", "title": "A Scenario-Based Development Framework for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article summarizes the research progress of scenario-based testing and\ndevelopment technology for autonomous vehicles. We systematically analyzed\nprevious research works and proposed the definition of scenario, the elements\nof the scenario ontology, the data source of the scenario, the processing\nmethod of the scenario data, and scenario-based V-Model. Moreover, we\nsummarized the automated test scenario construction method by random scenario\ngeneration and dangerous scenario generation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:06:48 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 20:09:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Li", "Xiaoyi", ""]]}, {"id": "2011.01444", "submitter": "Charupriya Sharma", "authors": "Charupriya Sharma, Zhenyu A. Liao, James Cussens, Peter van Beek", "title": "A Score-and-Search Approach to Learning Bayesian Networks with Noisy-OR\n  Relations", "comments": "Accepted to Probabilistic Graphical Models, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian network is a probabilistic graphical model that consists of a\ndirected acyclic graph (DAG), where each node is a random variable and attached\nto each node is a conditional probability distribution (CPD). A Bayesian\nnetwork can be learned from data using the well-known score-and-search\napproach, and within this approach a key consideration is how to simultaneously\nlearn the global structure in the form of the underlying DAG and the local\nstructure in the CPDs. Several useful forms of local structure have been\nidentified in the literature but thus far the score-and-search approach has\nonly been extended to handle local structure in form of context-specific\nindependence. In this paper, we show how to extend the score-and-search\napproach to the important and widely useful case of noisy-OR relations. We\nprovide an effective gradient descent algorithm to score a candidate noisy-OR\nusing the widely used BIC score and we provide pruning rules that allow the\nsearch to successfully scale to medium sized networks. Our empirical results\nprovide evidence for the success of our approach to learning Bayesian networks\nthat incorporate noisy-OR relations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:20:44 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sharma", "Charupriya", ""], ["Liao", "Zhenyu A.", ""], ["Cussens", "James", ""], ["van Beek", "Peter", ""]]}, {"id": "2011.01447", "submitter": "C.-H. Huck Yang", "authors": "Hu Hu, Chao-Han Huck Yang, Xianjun Xia, Xue Bai, Xin Tang, Yajian\n  Wang, Shutong Niu, Li Chai, Juanjuan Li, Hongning Zhu, Feng Bao, Yuanjun\n  Zhao, Sabato Marco Siniscalchi, Yannan Wang, Jun Du, Chin-Hui Lee", "title": "A Two-Stage Approach to Device-Robust Acoustic Scene Classification", "comments": "Submitted to ICASSP 2021. Code available:\n  https://github.com/MihawkHu/DCASE2020_task1", "journal-ref": "ICASSP 2021-2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": null, "report-no": "845--849", "categories": "cs.SD cs.AI cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To improve device robustness, a highly desirable key feature of a competitive\ndata-driven acoustic scene classification (ASC) system, a novel two-stage\nsystem based on fully convolutional neural networks (CNNs) is proposed. Our\ntwo-stage system leverages on an ad-hoc score combination based on two CNN\nclassifiers: (i) the first CNN classifies acoustic inputs into one of three\nbroad classes, and (ii) the second CNN classifies the same inputs into one of\nten finer-grained classes. Three different CNN architectures are explored to\nimplement the two-stage classifiers, and a frequency sub-sampling scheme is\ninvestigated. Moreover, novel data augmentation schemes for ASC are also\ninvestigated. Evaluated on DCASE 2020 Task 1a, our results show that the\nproposed ASC system attains a state-of-the-art accuracy on the development set,\nwhere our best system, a two-stage fusion of CNN ensembles, delivers a 81.9%\naverage accuracy among multi-device test data, and it obtains a significant\nimprovement on unseen devices. Finally, neural saliency analysis with class\nactivation mapping (CAM) gives new insights on the patterns learnt by our\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:27:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hu", "Hu", ""], ["Yang", "Chao-Han Huck", ""], ["Xia", "Xianjun", ""], ["Bai", "Xue", ""], ["Tang", "Xin", ""], ["Wang", "Yajian", ""], ["Niu", "Shutong", ""], ["Chai", "Li", ""], ["Li", "Juanjuan", ""], ["Zhu", "Hongning", ""], ["Bao", "Feng", ""], ["Zhao", "Yuanjun", ""], ["Siniscalchi", "Sabato Marco", ""], ["Wang", "Yannan", ""], ["Du", "Jun", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2011.01452", "submitter": "Jiacheng Wang", "authors": "Jiacheng Wang, Yong Fan, Duo Jiang, Shiqing Li", "title": "Meta-Learning for Natural Language Understanding under Continual\n  Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network has been recognized with its accomplishments on tackling\nvarious natural language understanding (NLU) tasks. Methods have been developed\nto train a robust model to handle multiple tasks to gain a general\nrepresentation of text. In this paper, we implement the model-agnostic\nmeta-learning (MAML) and Online aware Meta-learning (OML) meta-objective under\nthe continual framework for NLU tasks. We validate our methods on selected\nSuperGLUE and GLUE benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:41:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Jiacheng", ""], ["Fan", "Yong", ""], ["Jiang", "Duo", ""], ["Li", "Shiqing", ""]]}, {"id": "2011.01472", "submitter": "Narayanan Chatapuram Krishnan", "authors": "Ashish Kumar, Karan Sehgal, Prerna Garg, Vidhya Kamakshi, and\n  Narayanan C Krishnan", "title": "MACE: Model Agnostic Concept Extractor for Explaining Image\n  Classification Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have been quite successful at various image\nclassification tasks. The current methods to explain the predictions of a\npre-trained model rely on gradient information, often resulting in saliency\nmaps that focus on the foreground object as a whole. However, humans typically\nreason by dissecting an image and pointing out the presence of smaller\nconcepts. The final output is often an aggregation of the presence or absence\nof these smaller concepts. In this work, we propose MACE: a Model Agnostic\nConcept Extractor, which can explain the working of a convolutional network\nthrough smaller concepts. The MACE framework dissects the feature maps\ngenerated by a convolution network for an image to extract concept based\nprototypical explanations. Further, it estimates the relevance of the extracted\nconcepts to the pre-trained model's predictions, a critical aspect required for\nexplaining the individual class predictions, missing in existing approaches. We\nvalidate our framework using VGG16 and ResNet50 CNN architectures, and on\ndatasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments\ndemonstrate that the concepts extracted by the MACE framework increase the\nhuman interpretability of the explanations, and are faithful to the underlying\npre-trained black-box model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:40:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kumar", "Ashish", ""], ["Sehgal", "Karan", ""], ["Garg", "Prerna", ""], ["Kamakshi", "Vidhya", ""], ["Krishnan", "Narayanan C", ""]]}, {"id": "2011.01477", "submitter": "Chong Peng", "authors": "Chong Peng, Qian Zhang, Zhao Kang, Chenglizhao Chen, and Qiang Cheng", "title": "Kernel Two-Dimensional Ridge Regression for Subspace Clustering", "comments": "accepted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods have been widely studied recently. When the\ninputs are 2-dimensional (2D) data, existing subspace clustering methods\nusually convert them into vectors, which severely damages inherent structures\nand relationships from original data. In this paper, we propose a novel\nsubspace clustering method for 2D data. It directly uses 2D data as inputs such\nthat the learning of representations benefits from inherent structures and\nrelationships of the data. It simultaneously seeks image projection and\nrepresentation coefficients such that they mutually enhance each other and lead\nto powerful data representations. An efficient algorithm is developed to solve\nthe proposed objective function with provable decreasing and convergence\nproperty. Extensive experimental results verify the effectiveness of the new\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:52:46 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Peng", "Chong", ""], ["Zhang", "Qian", ""], ["Kang", "Zhao", ""], ["Chen", "Chenglizhao", ""], ["Cheng", "Qiang", ""]]}, {"id": "2011.01488", "submitter": "Deeksha Sinha", "authors": "Deeksha Sinha, Karthik Abinav Sankararama, Abbas Kazerouni, Vashist\n  Avadhanula", "title": "Multi-armed Bandits with Cost Subsidy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a novel variant of the multi-armed bandit (MAB)\nproblem, MAB with cost subsidy, which models many real-life applications where\nthe learning agent has to pay to select an arm and is concerned about\noptimizing cumulative costs and rewards. We present two applications,\nintelligent SMS routing problem and ad audience optimization problem faced by\nseveral businesses (especially online platforms), and show how our problem\nuniquely captures key features of these applications. We show that naive\ngeneralizations of existing MAB algorithms like Upper Confidence Bound and\nThompson Sampling do not perform well for this problem. We then establish a\nfundamental lower bound on the performance of any online learning algorithm for\nthis problem, highlighting the hardness of our problem in comparison to the\nclassical MAB problem. We also present a simple variant of explore-then-commit\nand establish near-optimal regret bounds for this algorithm. Lastly, we perform\nextensive numerical simulations to understand the behavior of a suite of\nalgorithms for various instances and recommend a practical guide to employ\ndifferent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 05:38:42 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 19:13:00 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 11:49:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sinha", "Deeksha", ""], ["Sankararama", "Karthik Abinav", ""], ["Kazerouni", "Abbas", ""], ["Avadhanula", "Vashist", ""]]}, {"id": "2011.01489", "submitter": "Samer Nofal", "authors": "Samer Nofal, Amani Abu Jabal, Abdullah Alfarrarjeh, Ismail Hababeh", "title": "On Computing Stable Extensions of Abstract Argumentation Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \\textit{abstract argumentation framework} ({\\sc af} for short) is a\ndirected graph $(A,R)$ where $A$ is a set of \\textit{abstract arguments} and\n$R\\subseteq A \\times A$ is the \\textit{attack} relation. Let $H=(A,R)$ be an\n{\\sc af}, $S \\subseteq A$ be a set of arguments and $S^+ = \\{y \\mid \\exists\nx\\in S \\text{ with }(x,y)\\in R\\}$. Then, $S$ is a \\textit{stable extension} in\n$H$ if and only if $S^+ = A\\setminus S$. In this paper, we present a thorough,\nformal validation of a known backtracking algorithm for listing all stable\nextensions in a given {\\sc af}.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 05:38:52 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 11:31:40 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 11:51:24 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 06:47:25 GMT"}, {"version": "v5", "created": "Tue, 15 Jun 2021 03:56:29 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nofal", "Samer", ""], ["Jabal", "Amani Abu", ""], ["Alfarrarjeh", "Abdullah", ""], ["Hababeh", "Ismail", ""]]}, {"id": "2011.01506", "submitter": "Narayanan Chatapuram Krishnan", "authors": "Rajat Sharma, Nikhil Reddy, Vidhya Kamakshi, Narayanan C Krishnan,\n  Shweta Jain", "title": "MAIRE -- A Model-Agnostic Interpretable Rule Extraction Procedure for\n  Explaining Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a novel framework for extracting model-agnostic human\ninterpretable rules to explain a classifier's output. The human interpretable\nrule is defined as an axis-aligned hyper-cuboid containing the instance for\nwhich the classification decision has to be explained. The proposed procedure\nfinds the largest (high \\textit{coverage}) axis-aligned hyper-cuboid such that\na high percentage of the instances in the hyper-cuboid have the same class\nlabel as the instance being explained (high \\textit{precision}). Novel\napproximations to the coverage and precision measures in terms of the\nparameters of the hyper-cuboid are defined. They are maximized using\ngradient-based optimizers. The quality of the approximations is rigorously\nanalyzed theoretically and experimentally. Heuristics for simplifying the\ngenerated explanations for achieving better interpretability and a greedy\nselection algorithm that combines the local explanations for creating global\nexplanations for the model covering a large part of the instance space are also\nproposed. The framework is model agnostic, can be applied to any arbitrary\nclassifier, and all types of attributes (including continuous, ordered, and\nunordered discrete). The wide-scale applicability of the framework is validated\non a variety of synthetic and real-world datasets from different domains\n(tabular, text, and image).\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:53:06 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sharma", "Rajat", ""], ["Reddy", "Nikhil", ""], ["Kamakshi", "Vidhya", ""], ["Krishnan", "Narayanan C", ""], ["Jain", "Shweta", ""]]}, {"id": "2011.01536", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov", "title": "TransQuest: Translation Quality Estimation with Cross-lingual\n  Transformers", "comments": "Accepted to COLING 2020. arXiv admin note: text overlap with\n  arXiv:2010.05318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen big advances in the field of sentence-level quality\nestimation (QE), largely as a result of using neural-based architectures.\nHowever, the majority of these methods work only on the language pair they are\ntrained on and need retraining for new language pairs. This process can prove\ndifficult from a technical point of view and is usually computationally\nexpensive. In this paper we propose a simple QE framework based on\ncross-lingual transformers, and we use it to implement and evaluate two\ndifferent neural architectures. Our evaluation shows that the proposed methods\nachieve state-of-the-art results outperforming current open-source quality\nestimation frameworks when trained on datasets from WMT. In addition, the\nframework proves very useful in transfer learning settings, especially when\ndealing with low-resourced languages, allowing us to obtain very competitive\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:34:44 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 12:20:48 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Orasan", "Constantin", ""], ["Mitkov", "Ruslan", ""]]}, {"id": "2011.01542", "submitter": "Syrine Belakaria", "authors": "Syrine Belakaria, Aryan Deshwal and Janardhan Rao Doppa", "title": "Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space\n  Entropy Search Approach", "comments": "corrected typos. arXiv admin note: text overlap with arXiv:2009.05700", "journal-ref": "The Thirty-Fourth AAAI Conference on Artificial Intelligence AAAI,\n  2020", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the novel problem of blackbox optimization of multiple objectives\nvia multi-fidelity function evaluations that vary in the amount of resources\nconsumed and their accuracy. The overall goal is to approximate the true Pareto\nset of solutions by minimizing the resources consumed for function evaluations.\nFor example, in power system design optimization, we need to find designs that\ntrade-off cost, size, efficiency, and thermal tolerance using multi-fidelity\nsimulators for design evaluations. In this paper, we propose a novel approach\nreferred as Multi-Fidelity Output Space Entropy Search for Multi-objective\nOptimization (MF-OSEMO) to solve this problem. The key idea is to select the\nsequence of candidate input and fidelity-vector pairs that maximize the\ninformation gained about the true Pareto front per unit resource cost. Our\nexperiments on several synthetic and real-world benchmark problems show that\nMF-OSEMO, with both approximations, significantly improves over the\nstate-of-the-art single-fidelity algorithms for multi-objective optimization.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:59:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Belakaria", "Syrine", ""], ["Deshwal", "Aryan", ""], ["Doppa", "Janardhan Rao", ""]]}, {"id": "2011.01549", "submitter": "Bosheng Ding", "authors": "Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kruengkrai, Thien Hai\n  Nguyen, Shafiq Joty, Luo Si, Chunyan Miao", "title": "DAGA: Data Augmentation with a Generation Approach for Low-resource\n  Tagging Tasks", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation techniques have been widely used to improve machine\nlearning performance as they enhance the generalization capability of models.\nIn this work, to generate high quality synthetic data for low-resource tagging\ntasks, we propose a novel augmentation method with language models trained on\nthe linearized labeled sentences. Our method is applicable to both supervised\nand semi-supervised settings. For the supervised settings, we conduct extensive\nexperiments on named entity recognition (NER), part of speech (POS) tagging and\nend-to-end target based sentiment analysis (E2E-TBSA) tasks. For the\nsemi-supervised settings, we evaluate our method on the NER task under the\nconditions of given unlabeled data only and unlabeled data plus a knowledge\nbase. The results show that our method can consistently outperform the\nbaselines, particularly when the given gold training data are less.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 07:49:15 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ding", "Bosheng", ""], ["Liu", "Linlin", ""], ["Bing", "Lidong", ""], ["Kruengkrai", "Canasai", ""], ["Nguyen", "Thien Hai", ""], ["Joty", "Shafiq", ""], ["Si", "Luo", ""], ["Miao", "Chunyan", ""]]}, {"id": "2011.01590", "submitter": "Petra Heck", "authors": "Petra Heck and Gerard Schouten", "title": "Turning Software Engineers into AI Engineers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industry as well as education as well as academics we see a growing need\nfor knowledge on how to apply machine learning in software applications. With\nthe educational programme ICT & AI at Fontys UAS we had to find an answer to\nthe question: \"How should we educate software engineers to become AI\nengineers?\" This paper describes our educational programme, the open source\ntools we use, and the literature it is based on. After three years of\nexperience, we present our lessons learned for both educational institutions\nand software engineers in practice.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 09:44:59 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 09:52:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Heck", "Petra", ""], ["Schouten", "Gerard", ""]]}, {"id": "2011.01607", "submitter": "Ivan Yanchin", "authors": "Ivan Yanchin, Oleg Petrov", "title": "An approach to measure route quality and refine the route during the\n  voyage using characteristic coefficients", "comments": "8 pages; 9 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a method to validate and refine the ship's route during\nthe voyage. The method is based on computing several characteristic\ncoefficients that represent and measure route properties. Thru the analysis of\nthe values of these coefficient, one can analyse the overall route quality and\ndetect possibly dangerous discrepancies between the actual route and the\nplanned route.The paper describes the proposed characteristic coefficients, the\nprocess of route refinement and the method for prediction and validation of the\nroute's future changes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:26:01 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yanchin", "Ivan", ""], ["Petrov", "Oleg", ""]]}, {"id": "2011.01619", "submitter": "Yonghao Long", "authors": "Yonghao Long, Jie Ying Wu, Bo Lu, Yueming Jin, Mathias Unberath,\n  Yun-Hui Liu, Pheng Ann Heng and Qi Dou", "title": "Relational Graph Learning on Visual and Kinematics Embeddings for\n  Accurate Gesture Recognition in Robotic Surgery", "comments": "Accepted for ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:00:10 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 05:52:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Long", "Yonghao", ""], ["Wu", "Jie Ying", ""], ["Lu", "Bo", ""], ["Jin", "Yueming", ""], ["Unberath", "Mathias", ""], ["Liu", "Yun-Hui", ""], ["Heng", "Pheng Ann", ""], ["Dou", "Qi", ""]]}, {"id": "2011.01625", "submitter": "Tom Heskes", "authors": "Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, Tom Claassen", "title": "Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual\n  Predictions of Complex Models", "comments": "Accepted at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley values underlie one of the most popular model-agnostic methods within\nexplainable artificial intelligence. These values are designed to attribute the\ndifference between a model's prediction and an average baseline to the\ndifferent features used as input to the model. Being based on solid\ngame-theoretic principles, Shapley values uniquely satisfy several desirable\nproperties, which is why they are increasingly used to explain the predictions\nof possibly complex and highly non-linear machine learning models. Shapley\nvalues are well calibrated to a user's intuition when features are independent,\nbut may lead to undesirable, counterintuitive explanations when the\nindependence assumption is violated.\n  In this paper, we propose a novel framework for computing Shapley values that\ngeneralizes recent work that aims to circumvent the independence assumption. By\nemploying Pearl's do-calculus, we show how these 'causal' Shapley values can be\nderived for general causal graphs without sacrificing any of their desirable\nproperties. Moreover, causal Shapley values enable us to separate the\ncontribution of direct and indirect effects. We provide a practical\nimplementation for computing causal Shapley values based on causal chain graphs\nwhen only partial information is available and illustrate their utility on a\nreal-world example.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:11:36 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Heskes", "Tom", ""], ["Sijben", "Evi", ""], ["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""]]}, {"id": "2011.01646", "submitter": "Ivona Zakarija", "authors": "Ivona Zakarija, Frano \\v{S}kopljanac-Ma\\v{c}ina and Bruno\n  Bla\\v{s}kovi\\'c", "title": "Automated simulation and verification of process models discovered by\n  process mining", "comments": "12 pages, 13 figures and 3 tables, published in Automatika, vol. 61,\n  no. 2, pp.312-324, 2020", "journal-ref": "AUTOMATIKA, VOL. 61, NO. 2, 2020, pp. 312-324", "doi": "10.1080/00051144.2020.1734716", "report-no": null, "categories": "cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel approach for automated analysis of process models\ndiscovered using process mining techniques. Process mining explores underlying\nprocesses hidden in the event data generated by various devices. Our proposed\nInductive machine learning method was used to build business process models\nbased on actual event log data obtained from a hotel's Property Management\nSystem (PMS). The PMS can be considered as a Multi Agent System (MAS) because\nit is integrated with a variety of external systems and IoT devices. Collected\nevent log combines data on guests stay recorded by hotel staff, as well as data\nstreams captured from telephone exchange and other external IoT devices. Next,\nwe performed automated analysis of the discovered process models using formal\nmethods. Spin model checker was used to simulate process model executions and\nautomatically verify the process model. We proposed an algorithm for the\nautomatic transformation of the discovered process model into a verification\nmodel. Additionally, we developed a generator of positive and negative\nexamples. In the verification stage, we have also used Linear temporal logic\n(LTL) to define requested system specifications. We find that the analysis\nresults will be well suited for process model repair.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:51:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zakarija", "Ivona", ""], ["\u0160kopljanac-Ma\u010dina", "Frano", ""], ["Bla\u0161kovi\u0107", "Bruno", ""]]}, {"id": "2011.01649", "submitter": "Giorgio Camerani", "authors": "Giorgio Camerani", "title": "The Long, the Short and the Random", "comments": "18 pages, 10 figures. Software implementation (in Java) available for\n  download", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We furnish solid evidence, both theoretical and empirical, towards the\nexistence of a deterministic algorithm for random sparse $\\#\\Omega(\\log n)$-SAT\ninstances, which computes the exact counting of satisfying assignments in\nsub-exponential time. The algorithm uses a nice combinatorial property that\nevery CNF formula has, which relates its number of unsatisfying assignments to\nthe space of its monotone sub-formulae.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:00:07 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 22:39:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Camerani", "Giorgio", ""]]}, {"id": "2011.01681", "submitter": "Chang Liu", "authors": "Chang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei\n  Chen, Tie-Yan Liu", "title": "Learning Causal Semantic Representation for Out-of-Distribution\n  Prediction", "comments": "Figures for CSG-ind/DA; model selection highlighted; condition and\n  intuition of identifiability; new version of OOD error bound supporting\n  CSG-ind; improved experiment implementation, with shifted-MNIST and\n  ImageCLEF-DA results updated; MDD and BNM baselines added; results on PACS\n  and VLCS datasets added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional supervised learning methods, especially deep ones, are found to\nbe sensitive to out-of-distribution (OOD) examples, largely because the learned\nrepresentation mixes the semantic factor with the variation factor due to their\ndomain-specific correlation, while only the semantic factor causes the output.\nTo address the problem, we propose a Causal Semantic Generative model (CSG)\nbased on a causal reasoning so that the two factors are modeled separately, and\ndevelop methods for OOD prediction from a single training domain, which is\ncommon and challenging. The methods are based on the causal invariance\nprinciple, with a novel design for both efficient learning and easy prediction.\nTheoretically, we prove that under certain conditions, CSG can identify the\nsemantic factor by fitting training data, and this semantic-identification\nguarantees the boundedness of OOD generalization error and the success of\nadaptation. Empirical study shows improved OOD performance over prevailing\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:16:05 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 12:18:28 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 12:30:03 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 16:17:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Chang", ""], ["Sun", "Xinwei", ""], ["Wang", "Jindong", ""], ["Tang", "Haoyue", ""], ["Li", "Tao", ""], ["Qin", "Tao", ""], ["Chen", "Wei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2011.01706", "submitter": "Yuhao Wang", "authors": "Haotian Zhang, Yuhao Wang, Jianyong Sun, Zongben Xu", "title": "Amortized Variational Deep Q Network", "comments": "Accepted to appear in the Deep Reinforcement Learning Workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration is one of the most important issues in deep\nreinforcement learning. To address this issue, recent methods consider the\nvalue function parameters as random variables, and resort variational inference\nto approximate the posterior of the parameters. In this paper, we propose an\namortized variational inference framework to approximate the posterior\ndistribution of the action value function in Deep Q Network. We establish the\nequivalence between the loss of the new model and the amortized variational\ninference loss. We realize the balance of exploration and exploitation by\nassuming the posterior as Cauchy and Gaussian, respectively in a two-stage\ntraining process. We show that the amortized framework can results in\nsignificant less learning parameters than existing state-of-the-art method.\nExperimental results on classical control tasks in OpenAI Gym and chain Markov\nDecision Process tasks show that the proposed method performs significantly\nbetter than state-of-art methods and requires much less training time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:48:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Haotian", ""], ["Wang", "Yuhao", ""], ["Sun", "Jianyong", ""], ["Xu", "Zongben", ""]]}, {"id": "2011.01754", "submitter": "Huajie Shao", "authors": "Huajie Shao, Zhisheng Xiao, Shuochao Yao, Aston Zhang, Shengzhong Liu\n  and Tarek Abdelzaher", "title": "ControlVAE: Tuning, Analytical Properties, and Performance Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.05988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the novel concept of controllable variational autoencoder\n(ControlVAE), discusses its parameter tuning to meet application needs, derives\nits key analytic properties, and offers useful extensions and applications.\nControlVAE is a new variational autoencoder (VAE) framework that combines the\nautomatic control theory with the basic VAE to stabilize the KL-divergence of\nVAE models to a specified value. It leverages a non-linear PI controller, a\nvariant of the proportional-integral-derivative (PID) control, to dynamically\ntune the weight of the KL-divergence term in the evidence lower bound (ELBO)\nusing the output KL-divergence as feedback. This allows us to precisely control\nthe KL-divergence to a desired value (set point), which is effective in\navoiding posterior collapse and learning disentangled representations. In order\nto improve the ELBO over the regular VAE, we provide simplified theoretical\nanalysis to inform setting the set point of KL-divergence for ControlVAE. We\nobserve that compared to other methods that seek to balance the two terms in\nVAE's objective, ControlVAE leads to better learning dynamics. In particular,\nit can achieve a good trade-off between reconstruction quality and\nKL-divergence. We evaluate the proposed method on three tasks: image\ngeneration, language modeling and disentangled representation learning. The\nresults show that ControlVAE can achieve much better reconstruction quality\nthan the other methods for comparable disentanglement. On the language modeling\ntask, ControlVAE can avoid posterior collapse (KL vanishing) and improve the\ndiversity of generated text. Moreover, our method can change the optimization\ntrajectory, improving the ELBO and the reconstruction quality for image\ngeneration.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 12:32:39 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Shao", "Huajie", ""], ["Xiao", "Zhisheng", ""], ["Yao", "Shuochao", ""], ["Zhang", "Aston", ""], ["Liu", "Shengzhong", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "2011.01758", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Arunkumar Byravan, Tim Hertweck, Irina Higgins,\n  Ankush Gupta, Tejas Kulkarni, Malcolm Reynolds, Denis Teplyashin, Roland\n  Hafner, Thomas Lampe, Martin Riedmiller", "title": "Representation Matters: Improving Perception and Exploration for\n  Robotics", "comments": "Published at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projecting high-dimensional environment observations into lower-dimensional\nstructured representations can considerably improve data-efficiency for\nreinforcement learning in domains with limited data such as robotics. Can a\nsingle generally useful representation be found? In order to answer this\nquestion, it is important to understand how the representation will be used by\nthe agent and what properties such a 'good' representation should have. In this\npaper we systematically evaluate a number of common learnt and hand-engineered\nrepresentations in the context of three robotics tasks: lifting, stacking and\npushing of 3D blocks. The representations are evaluated in two use-cases: as\ninput to the agent, or as a source of auxiliary tasks. Furthermore, the value\nof each representation is evaluated in terms of three properties:\ndimensionality, observability and disentanglement. We can significantly improve\nperformance in both use-cases and demonstrate that some representations can\nperform commensurate to simulator states as agent inputs. Finally, our results\nchallenge common intuitions by demonstrating that: 1) dimensionality strongly\nmatters for task generation, but is negligible for inputs, 2) observability of\ntask-relevant aspects mostly affects the input representation use-case, and 3)\ndisentanglement leads to better auxiliary tasks, but has only limited benefits\nfor input representations. This work serves as a step towards a more systematic\nunderstanding of what makes a 'good' representation for control in robotics,\nenabling practitioners to make more informed choices for developing new learned\nor hand-engineered representations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:00:36 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 18:31:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Byravan", "Arunkumar", ""], ["Hertweck", "Tim", ""], ["Higgins", "Irina", ""], ["Gupta", "Ankush", ""], ["Kulkarni", "Tejas", ""], ["Reynolds", "Malcolm", ""], ["Teplyashin", "Denis", ""], ["Hafner", "Roland", ""], ["Lampe", "Thomas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "2011.01771", "submitter": "Yuanzhe Geng", "authors": "Yuanzhe Geng, Erwu Liu, Rui Wang and Yiming Liu", "title": "Deep Reinforcement Learning Based Dynamic Route Planning for Minimizing\n  Travel Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Route planning is important in transportation. Existing works focus on\nfinding the shortest path solution or using metrics such as safety and energy\nconsumption to determine the planning. It is noted that most of these studies\nrely on prior knowledge of road network, which may be not available in certain\nsituations. In this paper, we design a route planning algorithm based on deep\nreinforcement learning (DRL) for pedestrians. We use travel time consumption as\nthe metric, and plan the route by predicting pedestrian flow in the road\nnetwork. We put an agent, which is an intelligent robot, on a virtual map.\nDifferent from previous studies, our approach assumes that the agent does not\nneed any prior information about road network, but simply relies on the\ninteraction with the environment. We propose a dynamically adjustable route\nplanning (DARP) algorithm, where the agent learns strategies through a dueling\ndeep Q network to avoid congested roads. Simulation results show that the DARP\nalgorithm saves 52% of the time under congestion condition when compared with\ntraditional shortest path planning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:10:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Geng", "Yuanzhe", ""], ["Liu", "Erwu", ""], ["Wang", "Rui", ""], ["Liu", "Yiming", ""]]}, {"id": "2011.01774", "submitter": "Scott Friedman", "authors": "Scott E. Friedman, Robert P. Goldman, Richard G. Freedman, Ugur Kuter,\n  Christopher Geib, Jeffrey Rye", "title": "Provenance-Based Assessment of Plans in Context", "comments": "9 pages, 7 figures, including in Proceedings of the 2020 ICAPS\n  Workshop on Explainable AI Planning (XAIP)", "journal-ref": "Proceedings of the 2020 ICAPS Workshop on Explainable AI Planning", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world planning domains involve diverse information sources,\nexternal entities, and variable-reliability agents, all of which may impact the\nconfidence, risk, and sensitivity of plans. Humans reviewing a plan may lack\ncontext about these factors; however, this information is available during the\ndomain generation, which means it can also be interwoven into the planner and\nits resulting plans. This paper presents a provenance-based approach to\nexplaining automated plans. Our approach (1) extends the SHOP3 HTN planner to\ngenerate dependency information, (2) transforms the dependency information into\nan established PROV-O representation, and (3) uses graph propagation and\nTMS-inspired algorithms to support dynamic and counter-factual assessment of\ninformation flow, confidence, and support. We qualified our approach's\nexplanatory scope with respect to explanation targets from the automated\nplanning literature and the information analysis literature, and we demonstrate\nits ability to assess a plan's pertinence, sensitivity, risk, assumption\nsupport, diversity, and relative confidence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:13:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Friedman", "Scott E.", ""], ["Goldman", "Robert P.", ""], ["Freedman", "Richard G.", ""], ["Kuter", "Ugur", ""], ["Geib", "Christopher", ""], ["Rye", "Jeffrey", ""]]}, {"id": "2011.01788", "submitter": "Elena Congeduti", "authors": "Elena Congeduti, Alexander Mey, Frans A. Oliehoek", "title": "Loss Bounds for Approximate Influence-Based Abstraction", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential decision making techniques hold great promise to improve the\nperformance of many real-world systems, but computational complexity hampers\ntheir principled application. Influence-based abstraction aims to gain leverage\nby modeling local subproblems together with the 'influence' that the rest of\nthe system exerts on them. While computing exact representations of such\ninfluence might be intractable, learning approximate representations offers a\npromising approach to enable scalable solutions. This paper investigates the\nperformance of such approaches from a theoretical perspective. The primary\ncontribution is the derivation of sufficient conditions on approximate\ninfluence representations that can guarantee solutions with small value loss.\nIn particular we show that neural networks trained with cross entropy are well\nsuited to learn approximate influence representations. Moreover, we provide a\nsample based formulation of the bounds, which reduces the gap to applications.\nFinally, driven by our theoretical insights, we propose approximation error\nestimators, which empirically reveal to correlate well with the value loss.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:33:10 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 10:25:38 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 15:31:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Congeduti", "Elena", ""], ["Mey", "Alexander", ""], ["Oliehoek", "Frans A.", ""]]}, {"id": "2011.01791", "submitter": "Bugra Caskurlu", "authors": "Bugra Caskurlu, Ozgun Ekici, Fatih Erdem Kizilkaya", "title": "On Singleton Congestion Games with Resilience Against Collusion", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the subclass of singleton congestion games with identical and\nincreasing cost functions, i.e., each agent tries to utilize from the least\ncrowded resource in her accessible subset of resources. Our main contribution\nis a novel approach for proving the existence of equilibrium outcomes that are\nresilient to weakly improving deviations: $(i)$ by singletons (Nash\nequilibria), $(ii)$ by the grand coalition (Pareto efficiency), and $(iii)$ by\ncoalitions with respect to an a priori given partition coalition structure\n(partition equilibria). To the best of our knowledge, this is the strongest\nexistence guarantee in the literature of congestion games that is resilient to\nweakly improving deviations by coalitions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:35:09 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Caskurlu", "Bugra", ""], ["Ekici", "Ozgun", ""], ["Kizilkaya", "Fatih Erdem", ""]]}, {"id": "2011.01813", "submitter": "Yanqi Gu", "authors": "Kenneth Stewart and Yanqi Gu", "title": "One-Shot Federated Learning with Neuromorphic Processors", "comments": "arXiv admin note: text overlap with arXiv:2008.01151", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being very low power, the use of neuromorphic processors in mobile devices to\nsolve machine learning problems is a promising alternative to traditional Von\nNeumann processors. Federated Learning enables entities such as mobile devices\nto collaboratively learn a shared model while keeping their training data\nlocal. Additionally, federated learning is a secure way of learning because\nonly the model weights need to be shared between models, keeping the data\nprivate. Here we demonstrate the efficacy of federated learning in neuromorphic\nprocessors. Neuromorphic processors benefit from the collaborative learning,\nachieving state of the art accuracy on a one-shot learning gesture recognition\ntask across individual processor models while preserving local data privacy.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:47:20 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Stewart", "Kenneth", ""], ["Gu", "Yanqi", ""]]}, {"id": "2011.01826", "submitter": "Daniel Borrajo", "authors": "Daniel Borrajo, Manuela Veloso, Sameena Shah", "title": "Simulating and classifying behavior in adversarial environments based on\n  action-state traces: an application to money laundering", "comments": "A version appeared in the Proceedings of the 2020 ACM International\n  Conference on AI in Finance (ICAIF'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many business applications involve adversarial relationships in which both\nsides adapt their strategies to optimize their opposing benefits. One of the\nkey characteristics of these applications is the wide range of strategies that\nan adversary may choose as they adapt their strategy dynamically to sustain\nbenefits and evade authorities. In this paper, we present a novel way of\napproaching these types of applications, in particular in the context of\nAnti-Money Laundering. We provide a mechanism through which diverse, realistic\nand new unobserved behavior may be generated to discover potential unobserved\nadversarial actions to enable organizations to preemptively mitigate these\nrisks. In this regard, we make three main contributions. (a) Propose a novel\nbehavior-based model as opposed to individual transactions-based models\ncurrently used by financial institutions. We introduce behavior traces as\nenriched relational representation to represent observed human behavior. (b) A\nmodelling approach that observes these traces and is able to accurately infer\nthe goals of actors by classifying the behavior into money laundering or\nstandard behavior despite significant unobserved activity. And (c) a synthetic\nbehavior simulator that can generate new previously unseen traces. The\nsimulator incorporates a high level of flexibility in the behavioral parameters\nso that we can challenge the detection algorithm. Finally, we provide\nexperimental results that show that the learning module (automated\ninvestigator) that has only partial observability can still successfully infer\nthe type of behavior, and thus the simulated goals, followed by customers based\non traces - a key aspiration for many applications today.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:30:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Borrajo", "Daniel", ""], ["Veloso", "Manuela", ""], ["Shah", "Sameena", ""]]}, {"id": "2011.01832", "submitter": "Daniel Borrajo", "authors": "Daniel Borrajo, Sriram Gopalakrishnan, Vamsi K. Potluru", "title": "Goal recognition via model-based and model-free techniques", "comments": "A version of this paper appeared in the Pre-prints of the Workshop in\n  Planning for Financial Services (FinPlan) at ICAPS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal recognition aims at predicting human intentions from a trace of\nobservations. This ability allows people or organizations to anticipate future\nactions and intervene in a positive (collaborative) or negative (adversarial)\nway. Goal recognition has been successfully used in many domains, but it has\nbeen seldom been used by financial institutions. We claim the techniques are\nripe for its wide use in finance-related tasks. The main two approaches to\nperform goal recognition are model-based (planning-based) and model-free\n(learning-based). In this paper, we adapt state-of-the-art learning techniques\nto goal recognition, and compare model-based and model-free approaches in\ndifferent domains. We analyze the experimental data to understand the\ntrade-offs of using both types of methods. The experiments show that\nplanning-based approaches are ready for some goal-recognition finance tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:44:28 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Borrajo", "Daniel", ""], ["Gopalakrishnan", "Sriram", ""], ["Potluru", "Vamsi K.", ""]]}, {"id": "2011.01840", "submitter": "Qianqian Zhang", "authors": "Qianqian Zhang, Walid Saad, Mehdi Bennis", "title": "Distributional Reinforcement Learning for mmWave Communications with\n  Intelligent Reflectors on a UAV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel communication framework that uses an unmanned aerial\nvehicle (UAV)-carried intelligent reflector (IR) is proposed to enhance\nmulti-user downlink transmissions over millimeter wave (mmWave) frequencies. In\norder to maximize the downlink sum-rate, the optimal precoding matrix (at the\nbase station) and reflection coefficient (at the IR) are jointly derived. Next,\nto address the uncertainty of mmWave channels and maintain line-of-sight links\nin a real-time manner, a distributional reinforcement learning approach, based\non quantile regression optimization, is proposed to learn the propagation\nenvironment of mmWave communications, and, then, optimize the location of the\nUAV-IR so as to maximize the long-term downlink communication capacity.\nSimulation results show that the proposed learning-based deployment of the\nUAV-IR yields a significant advantage, compared to a non-learning UAV-IR, a\nstatic IR, and a direct transmission schemes, in terms of the average data rate\nand the achievable line-of-sight probability of downlink mmWave communications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:50:37 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Qianqian", ""], ["Saad", "Walid", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2011.01843", "submitter": "Inkit Padhi", "authors": "Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh,\n  Pierre Dognin, Jerret Ross, Ravi Nair, Erik Altman", "title": "Tabular Transformers for Modeling Multivariate Time Series", "comments": "Accepted to ICASSP, 2021; https://github.com/IBM/TabFormer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular datasets are ubiquitous in data science applications. Given their\nimportance, it seems natural to apply state-of-the-art deep learning algorithms\nin order to fully unlock their potential. Here we propose neural network models\nthat represent tabular time series that can optionally leverage their\nhierarchical structure. This results in two architectures for tabular time\nseries: one for learning representations that is analogous to BERT and can be\npre-trained end-to-end and used in downstream tasks, and one that is akin to\nGPT and can be used for generation of realistic synthetic tabular sequences. We\ndemonstrate our models on two datasets: a synthetic credit card transaction\ndataset, where the learned representations are used for fraud detection and\nsynthetic data generation, and on a real pollution dataset, where the learned\nencodings are used to predict atmospheric pollutant concentrations. Code and\ndata are available at https://github.com/IBM/TabFormer.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:58:08 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 22:11:40 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Padhi", "Inkit", ""], ["Schiff", "Yair", ""], ["Melnyk", "Igor", ""], ["Rigotti", "Mattia", ""], ["Mroueh", "Youssef", ""], ["Dognin", "Pierre", ""], ["Ross", "Jerret", ""], ["Nair", "Ravi", ""], ["Altman", "Erik", ""]]}, {"id": "2011.01869", "submitter": "Bo Li", "authors": "Bo Li, Wiro J. Niessen, Stefan Klein, M. Arfan Ikram, Meike W.\n  Vernooij, Esther E. Bron", "title": "Learning unbiased group-wise registration (LUGR) and joint segmentation:\n  evaluation on longitudinal diffusion MRI", "comments": "SPIE Medical Imaging 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Analysis of longitudinal changes in imaging studies often involves both\nsegmentation of structures of interest and registration of multiple timeframes.\nThe accuracy of such analysis could benefit from a tailored framework that\njointly optimizes both tasks to fully exploit the information available in the\nlongitudinal data. Most learning-based registration algorithms, including joint\noptimization approaches, currently suffer from bias due to selection of a fixed\nreference frame and only support pairwise transformations. We here propose an\nanalytical framework based on an unbiased learning strategy for group-wise\nregistration that simultaneously registers images to the mean space of a group\nto obtain consistent segmentations. We evaluate the proposed method on\nlongitudinal analysis of a white matter tract in a brain MRI dataset with 2-3\ntime-points for 3249 individuals, i.e., 8045 images in total. The\nreproducibility of the method is evaluated on test-retest data from 97\nindividuals. The results confirm that the implicit reference image is an\naverage of the input image. In addition, the proposed framework leads to\nconsistent segmentations and significantly lower processing bias than that of a\npair-wise fixed-reference approach. This processing bias is even smaller than\nthose obtained when translating segmentations by only one voxel, which can be\nattributed to subtle numerical instabilities and interpolation. Therefore, we\npostulate that the proposed mean-space learning strategy could be widely\napplied to learning-based registration tasks. In addition, this group-wise\nframework introduces a novel way for learning-based longitudinal studies by\ndirect construction of an unbiased within-subject template and allowing\nreliable and efficient analysis of spatio-temporal imaging biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:44:15 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 09:23:00 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Li", "Bo", ""], ["Niessen", "Wiro J.", ""], ["Klein", "Stefan", ""], ["Ikram", "M. Arfan", ""], ["Vernooij", "Meike W.", ""], ["Bron", "Esther E.", ""]]}, {"id": "2011.01880", "submitter": "Nikos Pitsillos", "authors": "Nikos Pitsillos, Ameya Pore, Bjorn Sand Jensen, Gerardo\n  Aragon-Camarasa", "title": "Intrinsic Robotic Introspection: Learning Internal States From Neuron\n  Activations", "comments": "Paper accepted at the International Conference on Development and\n  Learning (ICDL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an introspective framework inspired by the process of how humans\nperform introspection. Our working assumption is that neural network\nactivations encode information, and building internal states from these\nactivations can improve the performance of an actor-critic model. We perform\nexperiments where we first train a Variational Autoencoder model to reconstruct\nthe activations of a feature extraction network and use the latent space to\nimprove the performance of an actor-critic when deciding which low-level\nrobotic behaviour to execute. We show that internal states reduce the number of\nepisodes needed by about 1300 episodes while training an actor-critic, denoting\nfaster convergence to get a high success value while completing a robotic task.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:57:28 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:44:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Pitsillos", "Nikos", ""], ["Pore", "Ameya", ""], ["Jensen", "Bjorn Sand", ""], ["Aragon-Camarasa", "Gerardo", ""]]}, {"id": "2011.01890", "submitter": "Manuel Marin-Jimenez", "authors": "Rafael Berral-Soler, Francisco J. Madrid-Cuevas, Rafael\n  Mu\\~noz-Salinas, Manuel J. Mar\\'in-Jim\\'enez", "title": "RealHePoNet: a robust single-stage ConvNet for head pose estimation in\n  the wild", "comments": "Accepted for publication at Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human head pose estimation in images has applications in many fields such as\nhuman-computer interaction or video surveillance tasks. In this work, we\naddress this problem, defined here as the estimation of both vertical\n(tilt/pitch) and horizontal (pan/yaw) angles, through the use of a single\nConvolutional Neural Network (ConvNet) model, trying to balance precision and\ninference speed in order to maximize its usability in real-world applications.\nOur model is trained over the combination of two datasets: 'Pointing'04'\n(aiming at covering a wide range of poses) and 'Annotated Facial Landmarks in\nthe Wild' (in order to improve robustness of our model for its use on\nreal-world images). Three different partitions of the combined dataset are\ndefined and used for training, validation and testing purposes. As a result of\nthis work, we have obtained a trained ConvNet model, coined RealHePoNet, that\ngiven a low-resolution grayscale input image, and without the need of using\nfacial landmarks, is able to estimate with low error both tilt and pan angles\n(~4.4{\\deg} average error on the test partition). Also, given its low inference\ntime (~6 ms per head), we consider our model usable even when paired with\nmedium-spec hardware (i.e. GTX 1060 GPU). * Code available at:\nhttps://github.com/rafabs97/headpose_final * Demo video at:\nhttps://www.youtube.com/watch?v=2UeuXh5DjAE\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:09:05 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Berral-Soler", "Rafael", ""], ["Madrid-Cuevas", "Francisco J.", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Mar\u00edn-Jim\u00e9nez", "Manuel J.", ""]]}, {"id": "2011.01900", "submitter": "Mahdi Namazifar", "authors": "Mahdi Namazifar, Gokhan Tur, Dilek Hakkani T\\\"ur", "title": "Warped Language Models for Noise Robust Language Understanding", "comments": "To appear at IEEE SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Masked Language Models (MLM) are self-supervised neural networks trained to\nfill in the blanks in a given sentence with masked tokens. Despite the\ntremendous success of MLMs for various text based tasks, they are not robust\nfor spoken language understanding, especially for spontaneous conversational\nspeech recognition noise. In this work we introduce Warped Language Models\n(WLM) in which input sentences at training time go through the same\nmodifications as in MLM, plus two additional modifications, namely inserting\nand dropping random tokens. These two modifications extend and contract the\nsentence in addition to the modifications in MLMs, hence the word \"warped\" in\nthe name. The insertion and drop modification of the input text during training\nof WLM resemble the types of noise due to Automatic Speech Recognition (ASR)\nerrors, and as a result WLMs are likely to be more robust to ASR noise. Through\ncomputational results we show that natural language understanding systems built\non top of WLMs perform better compared to those built based on MLMs, especially\nin the presence of ASR errors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:26:28 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Namazifar", "Mahdi", ""], ["Tur", "Gokhan", ""], ["T\u00fcr", "Dilek Hakkani", ""]]}, {"id": "2011.01928", "submitter": "Ayush Jain", "authors": "Ayush Jain, Andrew Szot, Joseph J. Lim", "title": "Generalization to New Actions in Reinforcement Learning", "comments": "ICML 2020. Videos and code:\n  https://sites.google.com/view/action-generalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental trait of intelligence is the ability to achieve goals in the\nface of novel circumstances, such as making decisions from new action choices.\nHowever, standard reinforcement learning assumes a fixed set of actions and\nrequires expensive retraining when given a new action set. To make learning\nagents more adaptable, we introduce the problem of zero-shot generalization to\nnew actions. We propose a two-stage framework where the agent first infers\naction representations from action information acquired separately from the\ntask. A policy flexible to varying action sets is then trained with\ngeneralization objectives. We benchmark generalization on sequential tasks,\nsuch as selecting from an unseen tool-set to solve physical reasoning puzzles\nand stacking towers with novel 3D shapes. Videos and code are available at\nhttps://sites.google.com/view/action-generalization\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:58:39 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Jain", "Ayush", ""], ["Szot", "Andrew", ""], ["Lim", "Joseph J.", ""]]}, {"id": "2011.01956", "submitter": "Nika Haghtalab", "authors": "Nika Haghtalab, Nicole Immorlica, Brendan Lucier, Jack Z. Wang", "title": "Maximizing Welfare with Incentive-Aware Evaluation Mechanisms", "comments": "Published in IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications such as college admission and insurance rate\ndetermination, we propose an evaluation problem where the inputs are controlled\nby strategic individuals who can modify their features at a cost. A learner can\nonly partially observe the features, and aims to classify individuals with\nrespect to a quality score. The goal is to design an evaluation mechanism that\nmaximizes the overall quality score, i.e., welfare, in the population, taking\nany strategic updating into account. We further study the algorithmic aspect of\nfinding the welfare maximizing evaluation mechanism under two specific settings\nin our model. When scores are linear and mechanisms use linear scoring rules on\nthe observable features, we show that the optimal evaluation mechanism is an\nappropriate projection of the quality score. When mechanisms must use linear\nthresholds, we design a polynomial time algorithm with a (1/4)-approximation\nguarantee when the underlying feature distribution is sufficiently smooth and\nadmits an oracle for finding dense regions. We extend our results to settings\nwhere the prior distribution is unknown and must be learned from samples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:00:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Haghtalab", "Nika", ""], ["Immorlica", "Nicole", ""], ["Lucier", "Brendan", ""], ["Wang", "Jack Z.", ""]]}, {"id": "2011.01961", "submitter": "Alexander Wong", "authors": "Alexander Wong, Andrew Hryniowski, and Xiao Yu Wang", "title": "Insights into Fairness through Trust: Multi-scale Trust Quantification\n  for Financial Deep Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in recent years have led to a significant\nincrease in interest and prevalence for its adoption to tackle financial\nservices tasks. One particular question that often arises as a barrier to\nadopting deep learning for financial services is whether the developed\nfinancial deep learning models are fair in their predictions, particularly in\nlight of strong governance and regulatory compliance requirements in the\nfinancial services industry. A fundamental aspect of fairness that has not been\nexplored in financial deep learning is the concept of trust, whose variations\nmay point to an egocentric view of fairness and thus provide insights into the\nfairness of models. In this study we explore the feasibility and utility of a\nmulti-scale trust quantification strategy to gain insights into the fairness of\na financial deep learning model, particularly under different scenarios at\ndifferent scales. More specifically, we conduct multi-scale trust\nquantification on a deep neural network for the purpose of credit card default\nprediction to study: 1) the overall trustworthiness of the model 2) the trust\nlevel under all possible prediction-truth relationships, 3) the trust level\nacross the spectrum of possible predictions, 4) the trust level across\ndifferent demographic groups (e.g., age, gender, and education), and 5)\ndistribution of overall trust for an individual prediction scenario. The\ninsights for this proof-of-concept study demonstrate that such a multi-scale\ntrust quantification strategy may be helpful for data scientists and regulators\nin financial services as part of the verification and certification of\nfinancial deep learning solutions to gain insights into fairness and trust of\nthese solutions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:05:07 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Wong", "Alexander", ""], ["Hryniowski", "Andrew", ""], ["Wang", "Xiao Yu", ""]]}, {"id": "2011.01969", "submitter": "JiHyun Jeong", "authors": "JiHyun Jeong and Guy Hoffman", "title": "Face-work for Human-Agent Joint Decision-Making", "comments": "In Proceedings of the AAAI 2020 Fall Symposium Series on Trust and\n  Explainability in Artificial Intelligence for Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to integrate face-work, a common social ritual related to\ntrust, into a decision-making agent that works collaboratively with a human.\nFace-work is a set of trust-building behaviors designed to \"save face\" or\nprevent others from \"losing face.\" This paper describes the design of a\ndecision-making process that explicitly considers face-work as part of its\naction selection. We also present a simulated robot arm deployed in an online\nenvironment that can be used to evaluate the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:24:06 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Jeong", "JiHyun", ""], ["Hoffman", "Guy", ""]]}, {"id": "2011.01975", "submitter": "Vladlen Koltun", "authors": "Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia\n  Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh\n  Mottaghi, Manolis Savva, Hao Su", "title": "Rearrangement: A Challenge for Embodied AI", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a framework for research and evaluation in Embodied AI. Our\nproposal is based on a canonical task: Rearrangement. A standard task can focus\nthe development of new techniques and serve as a source of trained models that\ncan be transferred to other settings. In the rearrangement task, the goal is to\nbring a given physical environment into a specified state. The goal state can\nbe specified by object poses, by images, by a description in language, or by\nletting the agent experience the environment in the goal state. We characterize\nrearrangement scenarios along different axes and describe metrics for\nbenchmarking rearrangement performance. To facilitate research and exploration,\nwe present experimental testbeds of rearrangement scenarios in four different\nsimulation environments. We anticipate that other datasets will be released and\nnew simulation platforms will be built to support training of rearrangement\nagents and their deployment on physical systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:42:32 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Batra", "Dhruv", ""], ["Chang", "Angel X.", ""], ["Chernova", "Sonia", ""], ["Davison", "Andrew J.", ""], ["Deng", "Jia", ""], ["Koltun", "Vladlen", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""], ["Mordatch", "Igor", ""], ["Mottaghi", "Roozbeh", ""], ["Savva", "Manolis", ""], ["Su", "Hao", ""]]}, {"id": "2011.02036", "submitter": "Sandhya Tripathi", "authors": "Sandhya Tripathi, Bradley A. Fritz, Mohamed Abdelhack, Michael S.\n  Avidan, Yixin Chen, Christopher R. King", "title": "(Un)fairness in Post-operative Complication Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the current ongoing debate about fairness, explainability and\ntransparency of machine learning models, their application in high-impact\nclinical decision-making systems must be scrutinized. We consider a real-life\nexample of risk estimation before surgery and investigate the potential for\nbias or unfairness of a variety of algorithms. Our approach creates transparent\ndocumentation of potential bias so that the users can apply the model\ncarefully. We augment a model-card like analysis using propensity scores with a\ndecision-tree based guide for clinicians that would identify predictable\nshortcomings of the model. In addition to functioning as a guide for users, we\npropose that it can guide the algorithm development and informatics team to\nfocus on data sources and structures that can address these shortcomings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 22:11:19 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tripathi", "Sandhya", ""], ["Fritz", "Bradley A.", ""], ["Abdelhack", "Mohamed", ""], ["Avidan", "Michael S.", ""], ["Chen", "Yixin", ""], ["King", "Christopher R.", ""]]}, {"id": "2011.02073", "submitter": "Xubo Lyu", "authors": "Xubo Lyu, Site Li, Seth Siriya, Ye Pu, Mo Chen", "title": "MBB: Model-Based Baseline for Efficient Reinforcement Learning", "comments": "Submitted to the 2021 International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning (RL) is capable of learning control\npolicies for high-dimensional, complex robotic tasks, but tends to be\ndata-inefficient. Model-based RL and optimal control have been proven to be\nmuch more data-efficient if an accurate model of the system and environment is\nknown, but can be difficult to scale to expressive models for high-dimensional\nproblems. In this paper, we propose a novel approach to alleviate data\ninefficiency of model-free RL by warm-starting the learning process using a\nlower-dimensional model-based solutions. Particularly, we propose a baseline\nfunction that is initialized via supervision from a low-dimensional value\nfunction. Such a lower-dimensional value function can be obtained by applying\nmodel-based techniques on a low-dimensional problem featuring a known\napproximate system model. Therefore, our approach exploits the model priors\nfrom a simplified problem space implicitly and avoids the direct use of\nhigh-dimensional, expressive models. We demonstrate our approach on two\nrepresentative robotic learning tasks and observe significant improvement in\nperformance and efficiency, and analyze our method empirically with a third\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:11:56 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 00:20:04 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lyu", "Xubo", ""], ["Li", "Site", ""], ["Siriya", "Seth", ""], ["Pu", "Ye", ""], ["Chen", "Mo", ""]]}, {"id": "2011.02076", "submitter": "Marcus Hoerger", "authors": "Marcus Hoerger, Hanna Kurniawati", "title": "An On-Line POMDP Solver for Continuous Observation Spaces", "comments": "Submitted to The 2021 International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning under partial obervability is essential for autonomous robots. A\nprincipled way to address such planning problems is the Partially Observable\nMarkov Decision Process (POMDP). Although solving POMDPs is computationally\nintractable, substantial advancements have been achieved in developing\napproximate POMDP solvers in the past two decades. However, computing robust\nsolutions for problems with continuous observation spaces remains challenging.\nMost on-line solvers rely on discretising the observation space or artificially\nlimiting the number of observations that are considered during planning to\ncompute tractable policies. In this paper we propose a new on-line POMDP\nsolver, called Lazy Belief Extraction for Continuous POMDPs (LABECOP), that\ncombines methods from Monte-Carlo-Tree-Search and particle filtering to\nconstruct a policy reprentation which doesn't require discretised observation\nspaces and avoids limiting the number of observations considered during\nplanning. Experiments on three different problems involving continuous\nobservation spaces indicate that LABECOP performs similar or better than\nstate-of-the-art POMDP solvers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:16:08 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hoerger", "Marcus", ""], ["Kurniawati", "Hanna", ""]]}, {"id": "2011.02082", "submitter": "Somil Bansal", "authors": "Somil Bansal, Claire Tomlin", "title": "DeepReach: A Deep Learning Approach to High-Dimensional Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamilton-Jacobi (HJ) reachability analysis is an important formal\nverification method for guaranteeing performance and safety properties of\ndynamical control systems. Its advantages include compatibility with general\nnonlinear system dynamics, formal treatment of bounded disturbances, and the\nability to deal with state and input constraints. However, it involves solving\na PDE, whose computational and memory complexity scales exponentially with\nrespect to the number of state variables, limiting its direct use to\nsmall-scale systems. We propose DeepReach, a method that leverages new\ndevelopments in sinusoidal networks to develop a neural PDE solver for\nhigh-dimensional reachability problems. The computational requirements of\nDeepReach do not scale directly with the state dimension, but rather with the\ncomplexity of the underlying reachable tube. DeepReach achieves comparable\nresults to the state-of-the-art reachability methods, does not require any\nexplicit supervision for the PDE solution, can easily handle external\ndisturbances, adversarial inputs, and system constraints, and also provides a\nsafety controller for the system. We demonstrate DeepReach on a 9D\nmulti-vehicle collision problem, and a 10D narrow passage problem, motivated by\nautonomous driving applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:47:59 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Bansal", "Somil", ""], ["Tomlin", "Claire", ""]]}, {"id": "2011.02143", "submitter": "Alice Coucke", "authors": "St\\'ephane d'Ascoli, Alice Coucke, Francesco Caltagirone, Alexandre\n  Caulier, Marc Lelarge", "title": "Conditioned Text Generation with Transfer for Closed-Domain Dialogue\n  Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.03698", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of training data for task-oriented dialogue systems is a well known\nproblem that is usually tackled with costly and time-consuming manual data\nannotation. An alternative solution is to rely on automatic text generation\nwhich, although less accurate than human supervision, has the advantage of\nbeing cheap and fast. Our contribution is twofold. First we show how to\noptimally train and control the generation of intent-specific sentences using a\nconditional variational autoencoder. Then we introduce a new protocol called\nquery transfer that allows to leverage a large unlabelled dataset, possibly\ncontaining irrelevant queries, to extract relevant information. Comparison with\ntwo different baselines shows that this method, in the appropriate regime,\nconsistently improves the diversity of the generated queries without\ncompromising their quality. We also demonstrate the effectiveness of our\ngeneration method as a data augmentation technique for language modelling\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:06:10 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["d'Ascoli", "St\u00e9phane", ""], ["Coucke", "Alice", ""], ["Caltagirone", "Francesco", ""], ["Caulier", "Alexandre", ""], ["Lelarge", "Marc", ""]]}, {"id": "2011.02149", "submitter": "Chuan Wang", "authors": "Chuan Wang and Kwan-Liu Ma", "title": "HypperSteer: Hypothetical Steering and Data Perturbation in Sequence\n  Prediction with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Networks (RNN) continues to find success in predictive\ndecision-making with temporal event sequences. Recent studies have shown the\nimportance and practicality of visual analytics in interpreting deep learning\nmodels for real-world applications. However, very limited work enables\ninteractions with deep learning models and guides practitioners to form\nhypotheticals towards the desired prediction outcomes, especially for sequence\nprediction. Specifically, no existing work has addressed the what-if analysis\nand value perturbation along different time-steps for sequence outcome\nprediction. We present a model-agnostic visual analytics tool, HypperSteer,\nthat steers hypothetical testing and allows users to perturb data for sequence\npredictions interactively. We showcase how HypperSteer helps in steering\npatient data to achieve desired treatment outcomes and discuss how HypperSteer\ncan serve as a comprehensive solution for other practical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:26:58 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 01:55:39 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Chuan", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2011.02151", "submitter": "Kwadwo Opong-Mensah", "authors": "Kwadwo Opong-Mensah", "title": "Simulation of Human and Artificial Emotion (SHArE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework for Simulation of Human and Artificial Emotion (SHArE)\ndescribes the architecture of emotion in terms of parameters transferable\nbetween psychology, neuroscience, and artificial intelligence. These parameters\ncan be defined as abstract concepts or granularized down to the voltage levels\nof individual neurons. This model enables emotional trajectory design for\nhumans which may lead to novel therapeutic solutions for various mental health\nconcerns. For artificial intelligence, this work provides a compact notation\nwhich can be applied to neural networks as a means to observe the emotions and\nmotivations of machines.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:45:30 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Opong-Mensah", "Kwadwo", ""]]}, {"id": "2011.02223", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "New Ideas for Brain Modelling 7", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper updates the cognitive model, firstly by creating two systems and\nthen unifying them over the same structure. It represents information at the\nsemantic level only, where labelled patterns are aggregated into a\n'type-set-match' form. It is described that the aggregations can be used to\nmatch across regions with potentially different functionality and therefore\ngive the structure a required amount of flexibility. The theory is that if the\nmodel stores information which can be transposed in consistent ways, then that\nwill result in knowledge and some level of intelligence. As part of the design,\npatterns have to become distinct and that is realised by unique paths through\nshared aggregated structures. An ensemble-hierarchy relation also helps to\ndefine uniqueness through local feedback that may even be an action potential.\nThe earlier models are still consistent in terms of their proposed\nfunctionality, but some of the architecture boundaries have been moved to match\nthem up more closely. After pattern optimisation and tree-like aggregations,\nthe two main models differ only in their upper, more intelligent level. One\nprovides a propositional logic for mutually inclusive or exclusive pattern\ngroups and sequences, while the other provides a behaviour script that is\nconstructed from node types. It can be seen that these two views are\ncomplimentary and would allow some control over behaviours, as well as\nmemories, that might get selected.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 10:59:01 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 23:43:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "2011.02248", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen and Lina Yao and Aixin Sun and Xianzhi Wang and Xiwei Xu\n  and Liming Zhu", "title": "Generative Inverse Deep Reinforcement Learning for Online Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning enables an agent to capture user's interest\nthrough interactions with the environment dynamically. It has attracted great\ninterest in the recommendation research. Deep reinforcement learning uses a\nreward function to learn user's interest and to control the learning process.\nHowever, most reward functions are manually designed; they are either\nunrealistic or imprecise to reflect the high variety, dimensionality, and\nnon-linearity properties of the recommendation problem. That makes it difficult\nfor the agent to learn an optimal policy to generate the most satisfactory\nrecommendations. To address the above issue, we propose a novel generative\ninverse reinforcement learning approach, namely InvRec, which extracts the\nreward function from user's behaviors automatically, for online recommendation.\nWe conduct experiments on an online platform, VirtualTB, and compare with\nseveral state-of-the-art methods to demonstrate the feasibility and\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 12:12:25 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Sun", "Aixin", ""], ["Wang", "Xianzhi", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2011.02291", "submitter": "Thibault Lechien", "authors": "Thibault Lechien, Jorik Jooken, Patrick De Causmaecker", "title": "Evolving test instances of the Hamiltonian completion problem", "comments": "12 pages, 12 figures, minor revisions in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and comparing algorithm performance on graph instances is\nchallenging for multiple reasons. First, there is usually no standard set of\ninstances to benchmark performance. Second, using existing graph generators\nresults in a restricted spectrum of difficulty and the resulting graphs are\nusually not diverse enough to draw sound conclusions. That is why recent work\nproposes a new methodology to generate a diverse set of instances by using an\nevolutionary algorithm. We can then analyze the resulting graphs and get key\ninsights into which attributes are most related to algorithm performance. We\ncan also fill observed gaps in the instance space in order to generate graphs\nwith previously unseen combinations of features. This methodology is applied to\nthe instance space of the Hamiltonian completion problem using two different\nsolvers, namely the Concorde TSP Solver and a multi-start local search\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:04:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 19:28:04 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lechien", "Thibault", ""], ["Jooken", "Jorik", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "2011.02340", "submitter": "Khouloud Hwerbi", "authors": "Khouloud Hwerbi", "title": "An ontology-based chatbot for crises management: use case coronavirus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today is the era of intelligence in machines. With the advances in Artificial\nIntelligence, machines have started to impersonate different human traits, a\nchatbot is the next big thing in the domain of conversational services. A\nchatbot is a virtual person who is capable to carry out a natural conversation\nwith people. They can include skills that enable them to converse with the\nhumans in audio, visual, or textual formats. Artificial intelligence\nconversational entities, also called chatbots, conversational agents, or\ndialogue system, are an excellent example of such machines. Obtaining the right\ninformation at the right time and place is the key to effective disaster\nmanagement. The term \"disaster management\" encompasses both natural and\nhuman-caused disasters. To assist citizens, our project is to create a COVID\nAssistant to provide the need of up to date information to be available 24\nhours. With the growth in the World Wide Web, it is quite intelligible that\nusers are interested in the swift and relatedly correct information for their\nhunt. A chatbot can be seen as a question-and-answer system in which experts\nprovide knowledge to solicit users. This master thesis is dedicated to discuss\nCOVID Assistant chatbot and explain each component in detail. The design of the\nproposed chatbot is introduced by its seven components: Ontology, Web Scraping\nmodule, DB, State Machine, keyword Extractor, Trained chatbot, and User\nInterface.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:30:51 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hwerbi", "Khouloud", ""]]}, {"id": "2011.02403", "submitter": "Xiaosong Jia", "authors": "Xiaosong Jia, Liting Sun, Masayoshi Tomizuka, Wei Zhan", "title": "IDE-Net: Interactive Driving Event and Pattern Extraction from Human\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) need to share the road with multiple, heterogeneous\nroad users in a variety of driving scenarios. It is overwhelming and\nunnecessary to carefully interact with all observed agents, and AVs need to\ndetermine whether and when to interact with each surrounding agent. In order to\nfacilitate the design and testing of prediction and planning modules of AVs,\nin-depth understanding of interactive behavior is expected with proper\nrepresentation, and events in behavior data need to be extracted and\ncategorized automatically. Answers to what are the essential patterns of\ninteractions are also crucial for these motivations in addition to answering\nwhether and when. Thus, learning to extract interactive driving events and\npatterns from human data for tackling the whether-when-what tasks is of\ncritical importance for AVs. There is, however, no clear definition and\ntaxonomy of interactive behavior, and most of the existing works are based on\neither manual labelling or hand-crafted rules and features. In this paper, we\npropose the Interactive Driving event and pattern Extraction Network (IDE-Net),\nwhich is a deep learning framework to automatically extract interaction events\nand patterns directly from vehicle trajectories. In IDE-Net, we leverage the\npower of multi-task learning and proposed three auxiliary tasks to assist the\npattern extraction in an unsupervised fashion. We also design a unique\nspatial-temporal block to encode the trajectory data. Experimental results on\nthe INTERACTION dataset verified the effectiveness of such designs in terms of\nbetter generalizability and effective pattern extraction. We find three\ninterpretable patterns of interactions, bringing insights for driver behavior\nrepresentation, modeling and comprehension. Both objective and subjective\nevaluation metrics are adopted in our analysis of the learned patterns.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:56:12 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Jia", "Xiaosong", ""], ["Sun", "Liting", ""], ["Tomizuka", "Masayoshi", ""], ["Zhan", "Wei", ""]]}, {"id": "2011.02414", "submitter": "AnneMarie Borg", "authors": "AnneMarie Borg and Floris Bex", "title": "Necessary and Sufficient Explanations in Abstract Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss necessary and sufficient explanations for formal\nargumentation - the question whether and why a certain argument can be accepted\n(or not) under various extension-based semantics. Given a framework with which\nexplanations for argumentation-based conclusions can be derived, we study\nnecessity and sufficiency: what (sets of) arguments are necessary or sufficient\nfor the (non-)acceptance of an argument?\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:12:12 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Borg", "AnneMarie", ""], ["Bex", "Floris", ""]]}, {"id": "2011.02415", "submitter": "Maysum Panju", "authors": "Maysum Panju, Ali Ghodsi", "title": "A Neuro-Symbolic Method for Solving Differential and Functional\n  Equations", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When neural networks are used to solve differential equations, they usually\nproduce solutions in the form of black-box functions that are not directly\nmathematically interpretable. We introduce a method for generating symbolic\nexpressions to solve differential equations while leveraging deep learning\ntraining methods. Unlike existing methods, our system does not require learning\na language model over symbolic mathematics, making it scalable, compact, and\neasily adaptable for a variety of tasks and configurations. As part of the\nmethod, we propose a novel neural architecture for learning mathematical\nexpressions to optimize a customizable objective. The system is designed to\nalways return a valid symbolic formula, generating a useful approximation when\nan exact analytic solution to a differential equation is not or cannot be\nfound. We demonstrate through examples how our method can be applied on a\nnumber of differential equations, often obtaining symbolic approximations that\nare useful or insightful. Furthermore, we show how the system can be\neffortlessly generalized to find symbolic solutions to other mathematical\ntasks, including integration and functional equations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:13:25 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Panju", "Maysum", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2011.02442", "submitter": "Mohammad Izadikhah", "authors": "Mohammad Izadikhah", "title": "Modeling bank performance: A novel fuzzy two-stage DEA approach", "comments": "30 pages with 3 figures, to be published in Fuzzy Information and\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the banks' performance has always been of interest due to their\ncrucial role in the economic development of each country. Data envelopment\nanalysis (DEA) has been widely used for measuring the performance of bank\nbranches. In the conventional DEA approach, decision making units (DMUs) are\nregarded as black boxes that transform sets of inputs into sets of outputs\nwithout considering the internal interactions taking place within each DMU.\nTwo-stage DEA models are designed to overcome this shortfall. Thus, this paper\npresented a new two-stage DEA model based on a modification on Enhanced Russell\nModel. On the other hand, in many situations, such as in a manufacturing\nsystem, a production process or a service system, inputs, intermediates and\noutputs can be given as a fuzzy variable. The main aim of this paper is to\nbuild and present a new fuzzy two-stage DEA model for measuring the efficiency\nof 15 branches of Melli bank in Hamedan province.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:47:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Izadikhah", "Mohammad", ""]]}, {"id": "2011.02462", "submitter": "Jialiang Zhao", "authors": "Jialiang Zhao, Daniel Troniak, Oliver Kroemer", "title": "Towards Robotic Assembly by Predicting Robust, Precise and Task-oriented\n  Grasps", "comments": "Submitted and accepted to 4th Conference on Robot Learning (CoRL2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust task-oriented grasp planning is vital for autonomous robotic precision\nassembly tasks. Knowledge of the objects' geometry and preconditions of the\ntarget task should be incorporated when determining the proper grasp to\nexecute. However, several factors contribute to the challenges of realizing\nthese grasps such as noise when controlling the robot, unknown object\nproperties, and difficulties modeling complex object-object interactions. We\npropose a method that decomposes this problem and optimizes for grasp\nrobustness, precision, and task performance by learning three cascaded\nnetworks. We evaluate our method in simulation on three common assembly tasks:\ninserting gears onto pegs, aligning brackets into corners, and inserting shapes\ninto slots. Our policies are trained using a curriculum based on large-scale\nself-supervised grasp simulations with procedurally generated objects. Finally,\nwe evaluate the performance of the first two tasks with a real robot where our\nmethod achieves 4.28mm error for bracket insertion and 1.44mm error for gear\ninsertion.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 18:29:01 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zhao", "Jialiang", ""], ["Troniak", "Daniel", ""], ["Kroemer", "Oliver", ""]]}, {"id": "2011.02544", "submitter": "Kshitij Kulkarni", "authors": "Kshitij Kulkarni, Sven Neth", "title": "Social Choice with Changing Preferences: Representation Theorems and\n  Long-Run Policies", "comments": "Accepted to the Workshop on Consequential Decision Making in Dynamic\n  Environments, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study group decision making with changing preferences as a Markov Decision\nProcess. We are motivated by the increasing prevalence of automated\ndecision-making systems when making choices for groups of people over time. Our\nmain contribution is to show how classic representation theorems from social\nchoice theory can be adapted to characterize optimal policies in this dynamic\nsetting. We provide an axiomatic characterization of MDP reward functions that\nagree with the Utilitarianism social welfare functionals of social choice\ntheory. We also provide discussion of cases when the implementation of social\nchoice-theoretic axioms may fail to lead to long-run optimal outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:21:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Kulkarni", "Kshitij", ""], ["Neth", "Sven", ""]]}, {"id": "2011.02552", "submitter": "Fabrizio Sebastiani", "authors": "Alejandro Moreo and Fabrizio Sebastiani", "title": "Re-Assessing the \"Classify and Count\" Quantification Method", "comments": "This is the final version of the paper, identical to the one that is\n  going to appear on the Proceedings of the 43rd European Conference on\n  Information Retrieval (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learning to quantify (a.k.a.\\ quantification) is a task concerned with\ntraining unbiased estimators of class prevalence via supervised learning. This\ntask originated with the observation that \"Classify and Count\" (CC), the\ntrivial method of obtaining class prevalence estimates, is often a biased\nestimator, and thus delivers suboptimal quantification accuracy; following this\nobservation, several methods for learning to quantify have been proposed that\nhave been shown to outperform CC. In this work we contend that previous works\nhave failed to use properly optimised versions of CC. We thus reassess the real\nmerits of CC (and its variants), and argue that, while still inferior to some\ncutting-edge methods, they deliver near-state-of-the-art accuracy once (a)\nhyperparameter optimisation is performed, and (b) this optimisation is\nperformed by using a true quantification loss instead of a standard\nclassification-based loss. Experiments on three publicly available binary\nsentiment classification datasets support these conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:47:39 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 17:32:23 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "2011.02559", "submitter": "Robert Moss", "authors": "Robert J. Moss, Ritchie Lee, Nicholas Visser, Joachim Hochwarth, James\n  G. Lopez, and Mykel J. Kochenderfer", "title": "Adaptive Stress Testing of Trajectory Predictions in Flight Management\n  Systems", "comments": "10 pages, 10 figures, 6 algorithms. Digital Avionics Systems\n  Conference (DASC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find failure events and their likelihoods in flight-critical systems, we\ninvestigate the use of an advanced black-box stress testing approach called\nadaptive stress testing. We analyze a trajectory predictor from a developmental\ncommercial flight management system which takes as input a collection of\nlateral waypoints and en-route environmental conditions. Our aim is to search\nfor failure events relating to inconsistencies in the predicted lateral\ntrajectories. The intention of this work is to find likely failures and report\nthem back to the developers so they can address and potentially resolve\nshortcomings of the system before deployment. To improve search performance,\nthis work extends the adaptive stress testing formulation to be applied more\ngenerally to sequential decision-making problems with episodic reward by\ncollecting the state transitions during the search and evaluating at the end of\nthe simulated rollout. We use a modified Monte Carlo tree search algorithm with\nprogressive widening as our adversarial reinforcement learner. The performance\nis compared to direct Monte Carlo simulations and to the cross-entropy method\nas an alternative importance sampling baseline. The goal is to find potential\nproblems otherwise not found by traditional requirements-based testing. Results\nindicate that our adaptive stress testing approach finds more failures and\nfinds failures with higher likelihood relative to the baseline approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:05:43 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Moss", "Robert J.", ""], ["Lee", "Ritchie", ""], ["Visser", "Nicholas", ""], ["Hochwarth", "Joachim", ""], ["Lopez", "James G.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2011.02565", "submitter": "Anand Kamat", "authors": "Anand Kamat and Doina Precup", "title": "Diversity-Enriched Option-Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal abstraction allows reinforcement learning agents to represent\nknowledge and develop strategies over different temporal scales. The\noption-critic framework has been demonstrated to learn temporally extended\nactions, represented as options, end-to-end in a model-free setting. However,\nfeasibility of option-critic remains limited due to two major challenges,\nmultiple options adopting very similar behavior, or a shrinking set of task\nrelevant options. These occurrences not only void the need for temporal\nabstraction, they also affect performance. In this paper, we tackle these\nproblems by learning a diverse set of options. We introduce an\ninformation-theoretic intrinsic reward, which augments the task reward, as well\nas a novel termination objective, in order to encourage behavioral diversity in\nthe option set. We show empirically that our proposed method is capable of\nlearning options end-to-end on several discrete and continuous control tasks,\noutperforms option-critic by a wide margin. Furthermore, we show that our\napproach sustainably generates robust, reusable, reliable and interpretable\noptions, in contrast to option-critic.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:12:54 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Kamat", "Anand", ""], ["Precup", "Doina", ""]]}, {"id": "2011.02573", "submitter": "Suman Ojha", "authors": "Suman Ojha, Jonathan Vitale and Mary-Anne Williams", "title": "EEGS: A Transparent Model of Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the computational details of our emotion model, EEGS, and\nalso provides an overview of a three-stage validation methodology used for the\nevaluation of our model, which can also be applicable for other computational\nmodels of emotion. A major gap in existing emotion modelling literature has\nbeen the lack of computational/technical details of the implemented models,\nwhich not only makes it difficult for early-stage researchers to understand the\narea but also prevents benchmarking of the developed models for expert\nresearchers. We partly addressed these issues by presenting technical details\nfor the computation of appraisal variables in our previous work. In this paper,\nwe present mathematical formulas for the calculation of emotion intensities\nbased on the theoretical premises of appraisal theory. Moreover, we will\ndiscuss how we enable our emotion model to reach to a regulated emotional state\nfor social acceptability of autonomous agents. We hope this paper will allow a\nbetter transparency of knowledge, accurate benchmarking and further evolution\nof the field of emotion modelling.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 23:18:20 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ojha", "Suman", ""], ["Vitale", "Jonathan", ""], ["Williams", "Mary-Anne", ""]]}, {"id": "2011.02593", "submitter": "Chunting Zhou", "authors": "Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke\n  Zettlemoyer, Marjan Ghazvininejad", "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation", "comments": "Accepted by ACL-Finding 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence models can generate highly fluent sentences, but recent\nstudies have also shown that they are also prone to hallucinate additional\ncontent not supported by the input. These variety of fluent but wrong outputs\nare particularly problematic, as it will not be possible for users to tell they\nare being presented incorrect content. To detect these errors, we propose a\ntask to predict whether each token in the output sequence is hallucinated (not\ncontained in the input) and collect new manually annotated evaluation sets for\nthis task. We also introduce a method for learning to detect hallucinations\nusing pretrained language models fine tuned on synthetic data that includes\nautomatically inserted hallucinations Experiments on machine translation (MT)\nand abstractive summarization demonstrate that our proposed approach\nconsistently outperforms strong baselines on all benchmark datasets. We further\ndemonstrate how to use the token-level hallucination labels to define a\nfine-grained loss over the target sequence in low-resource MT and achieve\nsignificant improvements over strong baseline methods. We also apply our method\nto word-level quality estimation for MT and show its effectiveness in both\nsupervised and unsupervised settings. Codes and data available at\nhttps://github.com/violet-zct/fairseq-detect-hallucination.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 00:18:53 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 21:05:03 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 20:26:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhou", "Chunting", ""], ["Neubig", "Graham", ""], ["Gu", "Jiatao", ""], ["Diab", "Mona", ""], ["Guzman", "Paco", ""], ["Zettlemoyer", "Luke", ""], ["Ghazvininejad", "Marjan", ""]]}, {"id": "2011.02594", "submitter": "Yueming Yin", "authors": "Yueming Yin, Zhen Yang, Haifeng Hu, and Xiaofu Wu", "title": "Universal Multi-Source Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation enables intelligent models to transfer\nknowledge from a labeled source domain to a similar but unlabeled target\ndomain. Recent study reveals that knowledge can be transferred from one source\ndomain to another unknown target domain, called Universal Domain Adaptation\n(UDA). However, in the real-world application, there are often more than one\nsource domain to be exploited for domain adaptation. In this paper, we formally\npropose a more general domain adaptation setting, universal multi-source domain\nadaptation (UMDA), where the label sets of multiple source domains can be\ndifferent and the label set of target domain is completely unknown. The main\nchallenges in UMDA are to identify the common label set between each source\ndomain and target domain, and to keep the model scalable as the number of\nsource domains increases. To address these challenges, we propose a universal\nmulti-source adaptation network (UMAN) to solve the domain adaptation problem\nwithout increasing the complexity of the model in various UMDA settings. In\nUMAN, we estimate the reliability of each known class in the common label set\nvia the prediction margin, which helps adversarial training to better align the\ndistributions of multiple source domains and target domain in the common label\nset. Moreover, the theoretical guarantee for UMAN is also provided. Massive\nexperimental results show that existing UDA and multi-source DA (MDA) methods\ncannot be directly applied to UMDA and the proposed UMAN achieves the\nstate-of-the-art performance in various UMDA settings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 00:20:38 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yin", "Yueming", ""], ["Yang", "Zhen", ""], ["Hu", "Haifeng", ""], ["Wu", "Xiaofu", ""]]}, {"id": "2011.02604", "submitter": "Ethan Gordon", "authors": "Ethan K. Gordon, Sumegh Roychowdhury, Tapomayukh Bhattacharjee, Kevin\n  Jamieson, Siddhartha S. Srinivasa", "title": "Leveraging Post Hoc Context for Faster Learning in Bandit Settings with\n  Applications in Robot-Assisted Feeding", "comments": "6 pages + references, 5 figures, to appear in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robot-assisted feeding requires the ability to acquire a wide\nvariety of food items. However, it is impossible for such a system to be\ntrained on all types of food in existence. Therefore, a key challenge is\nchoosing a manipulation strategy for a previously unseen food item. Previous\nwork showed that the problem can be represented as a linear bandit with visual\ncontext. However, food has a wide variety of multi-modal properties relevant to\nmanipulation that can be hard to distinguish visually. Our key insight is that\nwe can leverage the haptic context we collect during and after manipulation\n(i.e., \"post hoc\") to learn some of these properties and more quickly adapt our\nvisual model to previously unseen food. In general, we propose a modified\nlinear contextual bandit framework augmented with post hoc context observed\nafter action selection to empirically increase learning speed and reduce\ncumulative regret. Experiments on synthetic data demonstrate that this effect\nis more pronounced when the dimensionality of the context is large relative to\nthe post hoc context or when the post hoc context model is particularly easy to\nlearn. Finally, we apply this framework to the bite acquisition problem and\ndemonstrate the acquisition of 8 previously unseen types of food with 21% fewer\nfailures across 64 attempts.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:28:25 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 22:04:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Gordon", "Ethan K.", ""], ["Roychowdhury", "Sumegh", ""], ["Bhattacharjee", "Tapomayukh", ""], ["Jamieson", "Kevin", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "2011.02606", "submitter": "Palakorn Achananuparp", "authors": "V N S Rama Krishna Pinnimty, Matt Zhao, Palakorn Achananuparp, and\n  Ee-Peng Lim", "title": "Transforming Facial Weight of Real Images by Editing Latent Space of\n  StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an invert-and-edit framework to automatically transform facial\nweight of an input face image to look thinner or heavier by leveraging semantic\nfacial attributes encoded in the latent space of Generative Adversarial\nNetworks (GANs). Using a pre-trained StyleGAN as the underlying generator, we\nfirst employ an optimization-based embedding method to invert the input image\ninto the StyleGAN latent space. Then, we identify the facial-weight attribute\ndirection in the latent space via supervised learning and edit the inverted\nlatent code by moving it positively or negatively along the extracted feature\naxis. Our framework is empirically shown to produce high-quality and realistic\nfacial-weight transformations without requiring training GANs with a large\namount of labeled face images from scratch. Ultimately, our framework can be\nutilized as part of an intervention to motivate individuals to make healthier\nfood choices by visualizing the future impacts of their behavior on appearance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:45:18 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pinnimty", "V N S Rama Krishna", ""], ["Zhao", "Matt", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2011.02608", "submitter": "Jingxi Xu", "authors": "Huy Ha, Jingxi Xu, Shuran Song", "title": "Learning a Decentralized Multi-arm Motion Planner", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a closed-loop multi-arm motion planner that is scalable and\nflexible with team size. Traditional multi-arm robot systems have relied on\ncentralized motion planners, whose runtimes often scale exponentially with team\nsize, and thus, fail to handle dynamic environments with open-loop control. In\nthis paper, we tackle this problem with multi-agent reinforcement learning,\nwhere a decentralized policy is trained to control one robot arm in the\nmulti-arm system to reach its target end-effector pose given observations of\nits workspace state and target end-effector pose. The policy is trained using\nSoft Actor-Critic with expert demonstrations from a sampling-based motion\nplanning algorithm (i.e., BiRRT). By leveraging classical planning algorithms,\nwe can improve the learning efficiency of the reinforcement learning algorithm\nwhile retaining the fast inference time of neural networks. The resulting\npolicy scales sub-linearly and can be deployed on multi-arm systems with\nvariable team sizes. Thanks to the closed-loop and decentralized formulation,\nour approach generalizes to 5-10 multi-arm systems and dynamic moving targets\n(>90% success rate for a 10-arm system), despite being trained on only 1-4 arm\nplanning tasks with static targets. Code and data links can be found at\nhttps://multiarm.cs.columbia.edu.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:47:23 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ha", "Huy", ""], ["Xu", "Jingxi", ""], ["Song", "Shuran", ""]]}, {"id": "2011.02610", "submitter": "Zonglin Yang", "authors": "Zonglin Yang, Xinya Du, Alexander Rush, Claire Cardie", "title": "Improving Event Duration Prediction via Time-aware Pre-training", "comments": "to be published in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models in NLP rarely encode external world knowledge about length\nof time. We introduce two effective models for duration prediction, which\nincorporate external knowledge by reading temporal-related news sentences\n(time-aware pre-training). Specifically, one model predicts the range/unit\nwhere the duration value falls in (R-pred); and the other predicts the exact\nduration value E-pred. Our best model -- E-pred, substantially outperforms\nprevious work, and captures duration information more accurately than R-pred.\nWe also demonstrate our models are capable of duration prediction in the\nunsupervised setting, outperforming the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:52:11 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yang", "Zonglin", ""], ["Du", "Xinya", ""], ["Rush", "Alexander", ""], ["Cardie", "Claire", ""]]}, {"id": "2011.02669", "submitter": "Weixun Wang", "authors": "Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen,\n  Jianye Hao, Feng Wu, Changjie Fan", "title": "Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping", "comments": "Accepted by NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward shaping is an effective technique for incorporating domain knowledge\ninto reinforcement learning (RL). Existing approaches such as potential-based\nreward shaping normally make full use of a given shaping reward function.\nHowever, since the transformation of human knowledge into numeric reward values\nis often imperfect due to reasons such as human cognitive bias, completely\nutilizing the shaping reward function may fail to improve the performance of RL\nalgorithms. In this paper, we consider the problem of adaptively utilizing a\ngiven shaping reward function. We formulate the utilization of shaping rewards\nas a bi-level optimization problem, where the lower level is to optimize policy\nusing the shaping rewards and the upper level is to optimize a parameterized\nshaping weight function for true reward maximization. We formally derive the\ngradient of the expected true reward with respect to the shaping weight\nfunction parameters and accordingly propose three learning algorithms based on\ndifferent assumptions. Experiments in sparse-reward cartpole and MuJoCo\nenvironments show that our algorithms can fully exploit beneficial shaping\nrewards, and meanwhile ignore unbeneficial shaping rewards or even transform\nthem into beneficial ones.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 05:34:14 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Hu", "Yujing", ""], ["Wang", "Weixun", ""], ["Jia", "Hangtian", ""], ["Wang", "Yixiang", ""], ["Chen", "Yingfeng", ""], ["Hao", "Jianye", ""], ["Wu", "Feng", ""], ["Fan", "Changjie", ""]]}, {"id": "2011.02692", "submitter": "Zhilin Lu", "authors": "Zhilin Lu, Jintao Wang, Jian Song", "title": "Binary Neural Network Aided CSI Feedback in Massive MIMO System", "comments": "6 pages, 5 figures, 4 tables. This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive multiple-input multiple-output (MIMO) system, channel state\ninformation (CSI) is essential for the base station to achieve high performance\ngain. Recently, deep learning is widely used in CSI compression to fight\nagainst the growing feedback overhead brought by massive MIMO in frequency\ndivision duplexing system. However, applying neural network brings extra memory\nand computation cost, which is non-negligible especially for the resource\nlimited user equipment (UE). In this paper, a novel binarization aided feedback\nnetwork named BCsiNet is introduced. Moreover, BCsiNet variants are designed to\nboost the performance under customized training and inference schemes.\nExperiments shows that BCsiNet offers over 30$\\times$ memory saving and around\n2$\\times$ inference acceleration for encoder at UE compared with CsiNet.\nFurthermore, the feedback performance of BCsiNet is comparable with original\nCsiNet. The key results can be reproduced with\nhttps://github.com/Kylin9511/BCsiNet.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 07:41:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Lu", "Zhilin", ""], ["Wang", "Jintao", ""], ["Song", "Jian", ""]]}, {"id": "2011.02700", "submitter": "Guangyan Zhou", "authors": "Jun Liu, Ke Xu, Guangyan Zhou", "title": "Exact Phase Transitions of Model RB with Slower-Growing Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second moment method has always been an effective tool to lower bound the\nsatisfiability threshold of many random constraint satisfaction problems.\nHowever, the calculation is usually hard to carry out and as a result, only\nsome loose results can be obtained. In this paper, based on a delicate analysis\nwhich fully exploit the power of the second moment method, we prove that random\nRB instances can exhibit exact phase transition under more relaxed conditions,\nespecially slower-growing domain size. These results are the best by using the\nsecond moment method, and new tools should be introduced for any better\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 08:40:24 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Liu", "Jun", ""], ["Xu", "Ke", ""], ["Zhou", "Guangyan", ""]]}, {"id": "2011.02705", "submitter": "Feng Ji", "authors": "Qianglong Chen, Feng Ji, Haiqing Chen and Yin Zhang", "title": "Improving Commonsense Question Answering by Graph-based Iterative\n  Retrieval over Multiple Knowledge Sources", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to facilitate natural language understanding, the key is to engage\ncommonsense or background knowledge. However, how to engage commonsense\neffectively in question answering systems is still under exploration in both\nresearch academia and industry. In this paper, we propose a novel\nquestion-answering method by integrating multiple knowledge sources, i.e.\nConceptNet, Wikipedia, and the Cambridge Dictionary, to boost the performance.\nMore concretely, we first introduce a novel graph-based iterative knowledge\nretrieval module, which iteratively retrieves concepts and entities related to\nthe given question and its choices from multiple knowledge sources. Afterward,\nwe use a pre-trained language model to encode the question, retrieved knowledge\nand choices, and propose an answer choice-aware attention mechanism to fuse all\nhidden representations of the previous modules. Finally, the linear classifier\nfor specific tasks is used to predict the answer. Experimental results on the\nCommonsenseQA dataset show that our method significantly outperforms other\ncompetitive methods and achieves the new state-of-the-art. In addition, further\nablation studies demonstrate the effectiveness of our graph-based iterative\nknowledge retrieval module and the answer choice-aware attention module in\nretrieving and synthesizing background knowledge from multiple knowledge\nsources.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 08:50:43 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Chen", "Qianglong", ""], ["Ji", "Feng", ""], ["Chen", "Haiqing", ""], ["Zhang", "Yin", ""]]}, {"id": "2011.02712", "submitter": "Sabera Talukder", "authors": "Sabera Talukder, Guruprasad Raghavan, Yisong Yue", "title": "Architecture Agnostic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore an alternate method for synthesizing neural network\narchitectures, inspired by the brain's stochastic synaptic pruning. During a\nperson's lifetime, numerous distinct neuronal architectures are responsible for\nperforming the same tasks. This indicates that biological neural networks are,\nto some degree, architecture agnostic. However, artificial networks rely on\ntheir fine-tuned weights and hand-crafted architectures for their remarkable\nperformance. This contrast begs the question: Can we build artificial\narchitecture agnostic neural networks? To ground this study we utilize sparse,\nbinary neural networks that parallel the brain's circuits. Within this sparse,\nbinary paradigm we sample many binary architectures to create families of\narchitecture agnostic neural networks not trained via backpropagation. These\nhigh-performing network families share the same sparsity, distribution of\nbinary weights, and succeed in both static and dynamic tasks. In summation, we\ncreate an architecture manifold search procedure to discover families or\narchitecture agnostic neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 09:04:07 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 05:06:51 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Talukder", "Sabera", ""], ["Raghavan", "Guruprasad", ""], ["Yue", "Yisong", ""]]}, {"id": "2011.02714", "submitter": "Francesco Flammini Ph.D.", "authors": "S. Amirhossein Mousavi, Donya Azizi Babani, Francesco Flammini", "title": "Obstacles in Fully Automatic Program Repair: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current article is an interdisciplinary attempt to decipher automatic\nprogram repair processes. The review is done by the manner typical to human\nscience known as diffraction. We attempt to spot a gap in the literature of\nself-healing and self-repair operations and further investigate the approaches\nthat would enable us to tackle the problems we face. As a conclusion, we\nsuggest a shift in the current approach to automatic program repair operations\nin order to attain our goals. The emphasis of this review is to achieve full\nautomation. Several obstacles are shortly mentioned in the current essay but\nthe main shortage that is covered is the overfitting obstacle, and this\nparticular problem is investigated in the stream that is related to full\nautomation of the repair process.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 09:15:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Mousavi", "S. Amirhossein", ""], ["Babani", "Donya Azizi", ""], ["Flammini", "Francesco", ""]]}, {"id": "2011.02744", "submitter": "Janek Gr\\\"ohl", "authors": "Janek Gr\\\"ohl, Melanie Schellenberg, Kris Dreher, Lena Maier-Hein", "title": "Deep learning for biomedical photoacoustic imaging: A review", "comments": "31 pages, 8 figures, 3 tables, 169 references", "journal-ref": "Photoacoustics Volume 22, June 2021, 100241", "doi": "10.1016/j.pacs.2021.100241", "report-no": null, "categories": "physics.med-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic imaging (PAI) is a promising emerging imaging modality that\nenables spatially resolved imaging of optical tissue properties up to several\ncentimeters deep in tissue, creating the potential for numerous exciting\nclinical applications. However, extraction of relevant tissue parameters from\nthe raw data requires the solving of inverse image reconstruction problems,\nwhich have proven extremely difficult to solve. The application of deep\nlearning methods has recently exploded in popularity, leading to impressive\nsuccesses in the context of medical imaging and also finding first use in the\nfield of PAI. Deep learning methods possess unique advantages that can\nfacilitate the clinical translation of PAI, such as extremely fast computation\ntimes and the fact that they can be adapted to any given problem. In this\nreview, we examine the current state of the art regarding deep learning in PAI\nand identify potential directions of research that will help to reach the goal\nof clinical applicability\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:33:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Gr\u00f6hl", "Janek", ""], ["Schellenberg", "Melanie", ""], ["Dreher", "Kris", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2011.02780", "submitter": "Bo Jiang", "authors": "Yue Shi, Bo Jiang, Zhengping Che, Jian Tang", "title": "Fast Object Detection with Latticed Multi-Scale Feature Fusion", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale variance is one of the crucial challenges in multi-scale object\ndetection. Early approaches address this problem by exploiting the image and\nfeature pyramid, which raises suboptimal results with computation burden and\nconstrains from inherent network structures. Pioneering works also propose\nmulti-scale (i.e., multi-level and multi-branch) feature fusions to remedy the\nissue and have achieved encouraging progress. However, existing fusions still\nhave certain limitations such as feature scale inconsistency, ignorance of\nlevel-wise semantic transformation, and coarse granularity. In this work, we\npresent a novel module, the Fluff block, to alleviate drawbacks of current\nmulti-scale fusion methods and facilitate multi-scale object detection.\nSpecifically, Fluff leverages both multi-level and multi-branch schemes with\ndilated convolutions to have rapid, effective and finer-grained feature\nfusions. Furthermore, we integrate Fluff to SSD as FluffNet, a powerful\nreal-time single-stage detector for multi-scale object detection. Empirical\nresults on MS COCO and PASCAL VOC have demonstrated that FluffNet obtains\nremarkable efficiency with state-of-the-art accuracy. Additionally, we indicate\nthe great generality of the Fluff block by showing how to embed it to other\nwidely-used detectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 12:16:30 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Shi", "Yue", ""], ["Jiang", "Bo", ""], ["Che", "Zhengping", ""], ["Tang", "Jian", ""]]}, {"id": "2011.02787", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta (1 and 2), Alexandrine Royer (1 and 3), Victoria Heath\n  (1 and 4), Connor Wright (1 and 5), Camylle Lanteigne (1, 6, and 7), Allison\n  Cohen (1, 8, and 9), Marianna Bergamaschi Ganapini (1 and 10), Muriam Fancy\n  (1, 11, and 12), Erick Galinkin (1 and 13), Ryan Khurana (1), Mo Akif (1),\n  Renjie Butalid (1), Falaah Arif Khan (1, 14, and 15), Masa Sweidan (1 and\n  16), Audrey Balogh (1 and 16) ((1) Montreal AI Ethics Institute, (2)\n  Microsoft, (3) University of Cambridge, (4) Creative Commons, (5) University\n  of Exeter, (6) Concordia University, (7) Algora Lab, (8) AI Global, (9) Mila,\n  (10) Union College, (11) University of Toronto, (12) University of Ottawa,\n  (13) Rapid7, (14) NYU Center for Responsible AI, (15) IIIT Hyderabad, (16)\n  McGill University)", "title": "The State of AI Ethics Report (October 2020)", "comments": "158 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics\ncaptures the most relevant developments in the field of AI Ethics since July\n2020. This report aims to help anyone, from machine learning experts to human\nrights activists and policymakers, quickly digest and understand the\never-changing developments in the field. Through research and article\nsummaries, as well as expert commentary, this report distills the research and\nreporting surrounding various domains related to the ethics of AI, including:\nAI and society, bias and algorithmic justice, disinformation, humans and AI,\nlabor impacts, privacy, risk, and future of AI ethics.\n  In addition, The State of AI Ethics includes exclusive content written by\nworld-class AI Ethics experts from universities, research institutes,\nconsulting firms, and governments. These experts include: Danit Gal (Tech\nAdvisor, United Nations), Amba Kak (Director of Global Policy and Programs,\nNYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI,\nAccenture), Brent Barron (Director of Strategic Projects and Knowledge\nManagement, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of\nthe OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of\nManagement), and Katya Klinova (AI and Economy Program Lead, Partnership on\nAI).\n  This report should be used not only as a point of reference and insight on\nthe latest thinking in the field of AI Ethics, but should also be used as a\ntool for introspection as we aim to foster a more nuanced conversation\nregarding the impacts of AI on the world.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 12:36:16 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Gupta", "Abhishek", "", "1 and 2"], ["Royer", "Alexandrine", "", "1 and 3"], ["Heath", "Victoria", "", "1 and 4"], ["Wright", "Connor", "", "1 and 5"], ["Lanteigne", "Camylle", "", "1, 6, and 7"], ["Cohen", "Allison", "", "1, 8, and 9"], ["Ganapini", "Marianna Bergamaschi", "", "1 and 10"], ["Fancy", "Muriam", "", "1, 11, and 12"], ["Galinkin", "Erick", "", "1 and 13"], ["Khurana", "Ryan", "", "1, 14, and 15"], ["Akif", "Mo", "", "1, 14, and 15"], ["Butalid", "Renjie", "", "1, 14, and 15"], ["Khan", "Falaah Arif", "", "1, 14, and 15"], ["Sweidan", "Masa", "", "1 and\n  16"], ["Balogh", "Audrey", "", "1 and 16"]]}, {"id": "2011.02803", "submitter": "Ting Chen", "authors": "Ting Chen and Calvin Luo and Lala Li", "title": "Intriguing Properties of Contrastive Losses", "comments": "Technical report. Code at\n  https://github.com/google-research/simclr/tree/master/colabs/intriguing_properties", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive loss and its variants have become very popular recently for\nlearning visual representations without supervision. In this work, we study\nthree intriguing properties of contrastive learning. We first generalize the\nstandard contrastive loss to a broader family of losses, and we find that\nvarious instantiations of the generalized loss perform similarly under the\npresence of a multi-layer non-linear projection head. We then study if\ninstance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,\nwhich are based on global image representation) can learn well on images with\nmultiple objects present. We find that meaningful hierarchical local features\ncan be learned despite the fact that these objectives operate on global\ninstance-level features.\n  Finally, we study an intriguing phenomenon of feature suppression among\ncompeting features shared across augmented views, such as \"color distribution\"\nvs \"object class\". We construct datasets with explicit and controllable\ncompeting features, and show that, for contrastive learning, a few bits of\neasy-to-learn shared features can suppress, and even fully prevent, the\nlearning of other sets of competing features. In scenarios where there are\nmultiple objects in an image, the dominant object would suppress the learning\nof smaller objects. Existing contrastive learning methods critically rely on\ndata augmentation to favor certain sets of features over others, and face\npotential limitation for scenarios where existing augmentations cannot fully\naddress the feature suppression. This poses open challenges to existing\ncontrastive learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 13:19:48 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 01:17:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Ting", ""], ["Luo", "Calvin", ""], ["Li", "Lala", ""]]}, {"id": "2011.02863", "submitter": "Meike Nauta", "authors": "Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert", "title": "This Looks Like That, Because ... Explaining Prototypes for\n  Interpretable Image Recognition", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition with prototypes is considered an interpretable alternative\nfor black box deep learning models. Classification depends on the extent to\nwhich a test image \"looks like\" a prototype. However, perceptual similarity for\nhumans can be different from the similarity learned by the classification\nmodel. Hence, only visualising prototypes can be insufficient for a user to\nunderstand what a prototype exactly represents, and why the model considers a\nprototype and an image to be similar. We address this ambiguity and argue that\nprototypes should be explained. We improve interpretability by automatically\nenhancing visual prototypes with textual quantitative information about visual\ncharacteristics deemed important by the classification model. Specifically, our\nmethod clarifies the meaning of a prototype by quantifying the influence of\ncolour hue, shape, texture, contrast and saturation and can generate both\nglobal and local explanations. Because of the generality of our approach, it\ncan improve the interpretability of any similarity-based method for\nprototypical image recognition. In our experiments, we apply our method to the\nexisting Prototypical Part Network (ProtoPNet). Our analysis confirms that the\nglobal explanations are generalisable, and often correspond to the visually\nperceptible properties of a prototype. Our explanations are especially relevant\nfor prototypes which might have been interpreted incorrectly otherwise. By\nexplaining such 'misleading' prototypes, we improve the interpretability and\nsimulatability of a prototype-based classification model. We also use our\nmethod to check whether visually similar prototypes have similar explanations,\nand are able to discover redundancy. Code is available at\nhttps://github.com/M-Nauta/Explaining_Prototypes .\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:43:07 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:13:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nauta", "Meike", ""], ["Jutte", "Annemarie", ""], ["Provoost", "Jesper", ""], ["Seifert", "Christin", ""]]}, {"id": "2011.02883", "submitter": "Yan Huang Dr.", "authors": "Junjie Pang, Jianbo Li, Zhenzhen Xie, Yan Huang, Zhipeng Cai", "title": "Collaborative City Digital Twin For Covid-19 Pandemic: A Federated\n  Learning Solution", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a collaborative city digital twin based on FL, a\nnovel paradigm that allowing multiple city DT to share the local strategy and\nstatus in a timely manner. In particular, an FL central server manages the\nlocal updates of multiple collaborators (city DT), provides a global model\nwhich is trained in multiple iterations at different city DT systems, until the\nmodel gains the correlations between various response plan and infection trend.\nThat means, a collaborative city DT paradigm based on FL techniques can obtain\nknowledge and patterns from multiple DTs, and eventually establish a `global\nview' for city crisis management. Meanwhile, it also helps to improve each city\ndigital twin selves by consolidating other DT's respective data without\nviolating privacy rules. To validate the proposed solution, we take COVID-19\npandemic as a case study. The experimental results on the real dataset with\nvarious response plan validate our proposed solution and demonstrate the\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:20:31 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pang", "Junjie", ""], ["Li", "Jianbo", ""], ["Xie", "Zhenzhen", ""], ["Huang", "Yan", ""], ["Cai", "Zhipeng", ""]]}, {"id": "2011.02909", "submitter": "Luca Pasqualini", "authors": "Luca Pasqualini and Maurizio Parton", "title": "Pseudo Random Number Generation through Reinforcement Learning and\n  Recurrent Neural Networks", "comments": "14 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1912.11531", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Pseudo-Random Number Generator (PRNG) is any algorithm generating a\nsequence of numbers approximating properties of random numbers. These numbers\nare widely employed in mid-level cryptography and in software applications.\nTest suites are used to evaluate PRNGs quality by checking statistical\nproperties of the generated sequences. These sequences are commonly represented\nbit by bit. This paper proposes a Reinforcement Learning (RL) approach to the\ntask of generating PRNGs from scratch by learning a policy to solve a partially\nobservable Markov Decision Process (MDP), where the full state is the period of\nthe generated sequence and the observation at each time step is the last\nsequence of bits appended to such state. We use a Long-Short Term Memory (LSTM)\narchitecture to model the temporal relationship between observations at\ndifferent time steps, by tasking the LSTM memory with the extraction of\nsignificant features of the hidden portion of the MDP's states. We show that\nmodeling a PRNG with a partially observable MDP and a LSTM architecture largely\nimproves the results of the fully observable feedforward RL approach introduced\nin previous work.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 10:53:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 14:55:48 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Pasqualini", "Luca", ""], ["Parton", "Maurizio", ""]]}, {"id": "2011.02912", "submitter": "Alessandro Antonucci", "authors": "Marco Zaffalon and Alessandro Antonucci and Rafael Caba\\~nas", "title": "EM Based Bounding of Unidentifiable Queries in Structural Causal Models", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.00463", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A structural causal model is made of endogenous (manifest) and exogenous\n(latent) variables. In a recent paper, it has been shown that endogenous\nobservations induce linear constraints on the probabilities of the exogenous\nvariables. This allows to exactly map a causal model into a \\emph{credal\nnetwork}. Causal inferences, such as interventions and counterfactuals, can\nconsequently be obtained by standard credal network algorithms. These natively\nreturn sharp values in the identifiable case, while intervals corresponding to\nthe exact bounds are produced for unidentifiable queries. In this paper we\npresent an approximate characterization of the constraints on the exogenous\nprobabilities. This is based on a specialization of the EM algorithm to the\ntreatment of the missing values in the exogenous observations. Multiple EM runs\ncan be consequently used to describe the causal model as a set of Bayesian\nnetworks and, hence, a credal network to be queried for the bounding of\nunidentifiable queries. Preliminary empirical tests show how this approach\nmight provide good inner bounds with relatively few runs. This is a promising\ndirection for causal analysis in models whose topology prevents a\nstraightforward specification of the credal mapping.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 10:25:13 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zaffalon", "Marco", ""], ["Antonucci", "Alessandro", ""], ["Caba\u00f1as", "Rafael", ""]]}, {"id": "2011.02918", "submitter": "Daniel Borrajo", "authors": "Daniel Borrajo, Manuela Veloso", "title": "Domain-independent generation and classification of behavior traces", "comments": "A version of this paper appears in the Pre-prints of the Workshop in\n  Planning for Financial Services (FinPlan) at ICAPS'20. arXiv admin note: text\n  overlap with arXiv:2011.01826", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial institutions mostly deal with people. Therefore, characterizing\ndifferent kinds of human behavior can greatly help institutions for improving\ntheir relation with customers and with regulatory offices. In many of such\ninteractions, humans have some internal goals, and execute some actions within\nthe financial system that lead them to achieve their goals. In this paper, we\ntackle these tasks as a behavior-traces classification task. An observer agent\ntries to learn characterizing other agents by observing their behavior when\ntaking actions in a given environment. The other agents can be of several types\nand the goal of the observer is to identify the type of the other agent given a\ntrace of observations. We present CABBOT, a learning technique that allows the\nagent to perform on-line classification of the type of planning agent whose\nbehavior is observing. In this work, the observer agent has partial and noisy\nobservability of the environment (state and actions of the other agents). In\norder to evaluate the performance of the learning technique, we have generated\na domain-independent goal-based simulator of agents. We present experiments in\nseveral (both financial and non-financial) domains with promising results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:58:54 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Borrajo", "Daniel", ""], ["Veloso", "Manuela", ""]]}, {"id": "2011.02962", "submitter": "Ramesha Karunasena", "authors": "Ramesha Karunasena, Mohammad Sarparajul Ambiya, Arunesh Sinha, Ruchit\n  Nagar, Saachi Dalal, Divy Thakkar, Dhyanesh Narayanan, Milind Tambe", "title": "Measuring Data Collection Diligence for Community Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics has tremendous potential to provide targeted benefit in\nlow-resource communities, however the availability of high-quality public\nhealth data is a significant challenge in developing countries primarily due to\nnon-diligent data collection by community health workers (CHWs). In this work,\nwe define and test a data collection diligence score. This challenging\nunlabeled data problem is handled by building upon domain expert's guidance to\ndesign a useful data representation of the raw data, using which we design a\nsimple and natural score. An important aspect of the score is relative scoring\nof the CHWs, which implicitly takes into account the context of the local area.\nThe data is also clustered and interpreting these clusters provides a natural\nexplanation of the past behavior of each data collector. We further predict the\ndiligence score for future time steps. Our framework has been validated on the\nground using observations by the field monitors of our partner NGO in India.\nBeyond the successful field test, our work is in the final stages of deployment\nin the state of Rajasthan, India.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 16:45:03 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:51:09 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 10:20:28 GMT"}, {"version": "v4", "created": "Fri, 13 Nov 2020 02:33:28 GMT"}, {"version": "v5", "created": "Wed, 7 Apr 2021 15:16:09 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Karunasena", "Ramesha", ""], ["Ambiya", "Mohammad Sarparajul", ""], ["Sinha", "Arunesh", ""], ["Nagar", "Ruchit", ""], ["Dalal", "Saachi", ""], ["Thakkar", "Divy", ""], ["Narayanan", "Dhyanesh", ""], ["Tambe", "Milind", ""]]}, {"id": "2011.02987", "submitter": "Guanghui Lan", "authors": "Georgios Kotsalis, Guanghui Lan, Tianjiao Li", "title": "Simple and optimal methods for stochastic variational inequalities, I:\n  operator extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first present a novel operator extrapolation (OE) method for\nsolving deterministic variational inequality (VI) problems. Similar to the\ngradient (operator) projection method, OE updates one single search sequence by\nsolving a single projection subproblem in each iteration. We show that OE can\nachieve the optimal rate of convergence for solving a variety of VI problems in\na much simpler way than existing approaches. We then introduce the stochastic\noperator extrapolation (SOE) method and establish its optimal convergence\nbehavior for solving different stochastic VI problems. In particular, SOE\nachieves the optimal complexity for solving a fundamental problem, i.e.,\nstochastic smooth and strongly monotone VI, for the first time in the\nliterature. We also present a stochastic block operator extrapolations (SBOE)\nmethod to further reduce the iteration cost for the OE method applied to\nlarge-scale deterministic VIs with a certain block structure. Numerical\nexperiments have been conducted to demonstrate the potential advantages of the\nproposed algorithms. In fact, all these algorithms are applied to solve\ngeneralized monotone variational inequality (GMVI) problems whose operator is\nnot necessarily monotone. We will also discuss optimal OE-based policy\nevaluation methods for reinforcement learning in a companion paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:20:19 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 04:12:37 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 00:26:01 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kotsalis", "Georgios", ""], ["Lan", "Guanghui", ""], ["Li", "Tianjiao", ""]]}, {"id": "2011.03023", "submitter": "Mahdi Namazifar", "authors": "Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur, Dilek\n  Hakkani-T\\\"ur", "title": "Language Model is All You Need: Natural Language Understanding as\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different flavors of transfer learning have shown tremendous impact in\nadvancing research and applications of machine learning. In this work we study\nthe use of a specific family of transfer learning, where the target domain is\nmapped to the source domain. Specifically we map Natural Language Understanding\n(NLU) problems to QuestionAnswering (QA) problems and we show that in low data\nregimes this approach offers significant improvements compared to other\napproaches to NLU. Moreover we show that these gains could be increased through\nsequential transfer learning across NLU problems from different domains. We\nshow that our approach could reduce the amount of required data for the same\nperformance by up to a factor of 10.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:31:22 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Namazifar", "Mahdi", ""], ["Papangelis", "Alexandros", ""], ["Tur", "Gokhan", ""], ["Hakkani-T\u00fcr", "Dilek", ""]]}, {"id": "2011.03028", "submitter": "Razvan Bunescu", "authors": "Patrick Gray and Razvan Bunescu", "title": "From Note-Level to Chord-Level Neural Network Models for Voice\n  Separation in Symbolic Music", "comments": "Paper submitted for publication in August 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is often experienced as a progression of concurrent streams of notes,\nor voices. The degree to which this happens depends on the position along a\nvoice-leading continuum, ranging from monophonic, to homophonic, to polyphonic,\nwhich complicates the design of automatic voice separation models. We address\nthis continuum by defining voice separation as the task of decomposing music\ninto streams that exhibit both a high degree of external perceptual separation\nfrom the other streams and a high degree of internal perceptual consistency.\nThe proposed voice separation task allows for a voice to diverge to multiple\nvoices and also for multiple voices to converge to the same voice. Equipped\nwith this flexible task definition, we manually annotated a corpus of popular\nmusic and used it to train neural networks that assign notes to voices either\nseparately for each note in a chord (note-level), or jointly to all notes in a\nchord (chord-level). The trained neural models greedily assign notes to voices\nin a left to right traversal of the input chord sequence, using a diverse set\nof perceptually informed input features. When evaluated on the extraction of\nconsecutive within voice note pairs, both models surpass a strong baseline\nbased on an iterative application of an envelope extraction function, with the\nchord-level model consistently edging out the note-level model. The two models\nare also shown to outperform previous approaches on separating the voices in\nBach music.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:39:42 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Gray", "Patrick", ""], ["Bunescu", "Razvan", ""]]}, {"id": "2011.03043", "submitter": "Nolan Dey", "authors": "Nolan S. Dey and J. Eric Taylor and Bryan P. Tripp and Alexander Wong\n  and Graham W. Taylor", "title": "Identifying and interpreting tuning dimensions in deep networks", "comments": "15 pages, 12 figures, Camera-ready for Shared Visual Representations\n  in Human & Machine Intelligence NeurIPS Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, a tuning dimension is a stimulus attribute that accounts for\nmuch of the activation variance of a group of neurons. These are commonly used\nto decipher the responses of such groups. While researchers have attempted to\nmanually identify an analogue to these tuning dimensions in deep neural\nnetworks, we are unaware of an automatic way to discover them. This work\ncontributes an unsupervised framework for identifying and interpreting \"tuning\ndimensions\" in deep networks. Our method correctly identifies the tuning\ndimensions of a synthetic Gabor filter bank and tuning dimensions of the first\ntwo layers of InceptionV1 trained on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:26:03 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 00:01:04 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Dey", "Nolan S.", ""], ["Taylor", "J. Eric", ""], ["Tripp", "Bryan P.", ""], ["Wong", "Alexander", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2011.03080", "submitter": "Momchil Hardalov", "authors": "Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov,\n  Ivan Koychev, Preslav Nakov", "title": "EXAMS: A Multi-Subject High School Examinations Dataset for\n  Cross-Lingual and Multilingual Question Answering", "comments": "EMNLP 2020, 17 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose EXAMS -- a new benchmark dataset for cross-lingual and\nmultilingual question answering for high school examinations. We collected more\nthan 24,000 high-quality high school exam questions in 16 languages, covering 8\nlanguage families and 24 school subjects from Natural Sciences and Social\nSciences, among others.\n  EXAMS offers a fine-grained evaluation framework across multiple languages\nand subjects, which allows precise analysis and comparison of various models.\nWe perform various experiments with existing top-performing multilingual\npre-trained models and we show that EXAMS offers multiple challenges that\nrequire multilingual knowledge and reasoning in multiple domains. We hope that\nEXAMS will enable researchers to explore challenging reasoning and knowledge\ntransfer methods and pre-trained models for school question answering in\nvarious languages which was not possible before. The data, code, pre-trained\nmodels, and evaluation are available at https://github.com/mhardalov/exams-qa.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:06:50 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Hardalov", "Momchil", ""], ["Mihaylov", "Todor", ""], ["Zlatkova", "Dimitrina", ""], ["Dinkov", "Yoan", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "2011.03085", "submitter": "Rinu Boney", "authors": "Rinu Boney, Jussi Sainio, Mikko Kaivola, Arno Solin, Juho Kannala", "title": "RealAnt: An Open-Source Low-Cost Quadruped for Research in Real-World\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current robot platforms available for research are either very expensive or\nunable to handle the abuse of exploratory controls in reinforcement learning.\nWe develop RealAnt, a minimal low-cost physical version of the popular 'Ant'\nbenchmark used in reinforcement learning. RealAnt costs only $410 in materials\nand can be assembled in less than an hour. We validate the platform with\nreinforcement learning experiments and provide baseline results on a set of\nbenchmark tasks. We demonstrate that the TD3 algorithm can learn to walk the\nRealAnt from less than 45 minutes of experience. We also provide simulator\nversions of the robot (with the same dimensions, state-action spaces, and\ndelayed noisy observations) in the MuJoCo and PyBullet simulators. We\nopen-source hardware designs, supporting software, and baseline results for\nease of reproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:26:22 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Boney", "Rinu", ""], ["Sainio", "Jussi", ""], ["Kaivola", "Mikko", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "2011.03088", "submitter": "Yichen Jiang", "authors": "Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh\n  Singh, Mohit Bansal", "title": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification", "comments": "Findings of EMNLP 2020 (20 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HoVer (HOppy VERification), a dataset for many-hop evidence\nextraction and fact verification. It challenges models to extract facts from\nseveral Wikipedia articles that are relevant to a claim and classify whether\nthe claim is Supported or Not-Supported by the facts. In HoVer, the claims\nrequire evidence to be extracted from as many as four English Wikipedia\narticles and embody reasoning graphs of diverse shapes. Moreover, most of the\n3/4-hop claims are written in multiple sentences, which adds to the complexity\nof understanding long-range dependency relations such as coreference. We show\nthat the performance of an existing state-of-the-art semantic-matching model\ndegrades significantly on our dataset as the number of reasoning hops\nincreases, hence demonstrating the necessity of many-hop reasoning to achieve\nstrong results. We hope that the introduction of this challenging dataset and\nthe accompanying evaluation task will encourage research in many-hop fact\nretrieval and information verification. We make the HoVer dataset publicly\navailable at https://hover-nlp.github.io\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:33:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 01:57:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jiang", "Yichen", ""], ["Bordia", "Shikha", ""], ["Zhong", "Zheng", ""], ["Dognin", "Charles", ""], ["Singh", "Maneesh", ""], ["Bansal", "Mohit", ""]]}, {"id": "2011.03125", "submitter": "Payam Nikdel", "authors": "Payam Nikdel, Richard Vaughan, Mo Chen", "title": "LBGP: Learning Based Goal Planning for Autonomous Following in Front", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a hybrid solution which combines deep reinforcement\nlearning (RL) and classical trajectory planning for the following in front\napplication. Here, an autonomous robot aims to stay ahead of a person as the\nperson freely walks around. Following in front is a challenging problem as the\nuser's intended trajectory is unknown and needs to be estimated, explicitly or\nimplicitly, by the robot. In addition, the robot needs to find a feasible way\nto safely navigate ahead of human trajectory. Our deep RL module implicitly\nestimates human trajectory and produces short-term navigational goals to guide\nthe robot. These goals are used by a trajectory planner to smoothly navigate\nthe robot to the short-term goals, and eventually in front of the user. We\nemploy curriculum learning in the deep RL module to efficiently achieve a high\nreturn. Our system outperforms the state-of-the-art in following ahead and is\nmore reliable compared to end-to-end alternatives in both the simulation and\nreal world experiments. In contrast to a pure deep RL approach, we demonstrate\nzero-shot transfer of the trained policy from simulation to the real world.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:29:30 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Nikdel", "Payam", ""], ["Vaughan", "Richard", ""], ["Chen", "Mo", ""]]}, {"id": "2011.03168", "submitter": "Hiroyasu Tsukamoto", "authors": "Hiroyasu Tsukamoto and Soon-Jo Chung and Jean-Jacques E. Slotine", "title": "Neural Stochastic Contraction Metrics for Learning-based Control and\n  Estimation", "comments": "IEEE CONTROL SYSTEMS LETTERS (L-CSS), preprint version, accepted Dec.\n  2020 (DOI: 10.1109/LCSYS.2020.3046529).\n  https://ieeexplore.ieee.org/document/9302618", "journal-ref": null, "doi": "10.1109/LCSYS.2020.3046529", "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Stochastic Contraction Metrics (NSCM), a new design\nframework for provably-stable robust control and estimation for a class of\nstochastic nonlinear systems. It uses a spectrally-normalized deep neural\nnetwork to construct a contraction metric, sampled via simplified convex\noptimization in the stochastic setting. Spectral normalization constrains the\nstate-derivatives of the metric to be Lipschitz continuous, thereby ensuring\nexponential boundedness of the mean squared distance of system trajectories\nunder stochastic disturbances. The NSCM framework allows autonomous agents to\napproximate optimal stable control and estimation policies in real-time, and\noutperforms existing nonlinear control and estimation techniques including the\nstate-dependent Riccati equation, iterative LQR, EKF, and the deterministic\nneural contraction metric, as illustrated in simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 03:04:42 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 17:39:43 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 23:43:24 GMT"}, {"version": "v4", "created": "Sun, 3 Jan 2021 14:12:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tsukamoto", "Hiroyasu", ""], ["Chung", "Soon-Jo", ""], ["Slotine", "Jean-Jacques E.", ""]]}, {"id": "2011.03183", "submitter": "Yilun Du", "authors": "Yilun Du, Joshua Tenenbaum, Tomas Lozano-Perez, Leslie Kaelbling", "title": "Learning an Object-Based Memory System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot operating in a household makes observations of multiple objects as it\nmoves around over the course of days or weeks. The objects may be moved by\ninhabitants, but not completely at random. The robot may be called upon later\nto retrieve objects and will need a long-term object-based memory in order to\nknow how to find them. In this paper, we combine some aspects of classic\ntechniques for data-association filtering with modern attention-based neural\nnetworks to construct object-based memory systems that consume and produce\nhigh-dimensional observations and hypotheses. We perform end-to-end learning on\nlabeled observation trajectories to learn both necessary internal transition\nand observation models. We demonstrate the system's effectiveness on a sequence\nof problem classes of increasing difficulty and show that it outperforms\nclustering-based methods, classic filters, and unstructured neural approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:18:52 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 06:07:34 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Du", "Yilun", ""], ["Tenenbaum", "Joshua", ""], ["Lozano-Perez", "Tomas", ""], ["Kaelbling", "Leslie", ""]]}, {"id": "2011.03189", "submitter": "Lihui Liu", "authors": "Lihui Liu, Boxin Du, Heng Ji, Hanghang Tong", "title": "KompaRe: A Knowledge Graph Comparative Reasoning System", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning is a fundamental capability for harnessing valuable insight,\nknowledge and patterns from knowledge graphs. Existing work has primarily been\nfocusing on point-wise reasoning, including search, link predication, entity\nprediction, subgraph matching and so on. This paper introduces comparative\nreasoning over knowledge graphs, which aims to infer the commonality and\ninconsistency with respect to multiple pieces of clues. We envision that the\ncomparative reasoning will complement and expand the existing point-wise\nreasoning over knowledge graphs. In detail, we develop KompaRe, the first of\nits kind prototype system that provides comparative reasoning capability over\nlarge knowledge graphs. We present both the system architecture and its core\nalgorithms, including knowledge segment extraction, pairwise reasoning and\ncollective reasoning. Empirical evaluations demonstrate the efficacy of the\nproposed KompaRe.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:57:37 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Liu", "Lihui", ""], ["Du", "Boxin", ""], ["Ji", "Heng", ""], ["Tong", "Hanghang", ""]]}, {"id": "2011.03195", "submitter": "Het Naik", "authors": "Devam Dave, Het Naik, Smiti Singhal, Pankesh Patel", "title": "Explainable AI meets Healthcare: A Study on Heart Disease Dataset", "comments": "23", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the increasing availability of structured and unstructured data and the\nswift progress of analytical techniques, Artificial Intelligence (AI) is\nbringing a revolution to the healthcare industry. With the increasingly\nindispensable role of AI in healthcare, there are growing concerns over the\nlack of transparency and explainability in addition to potential bias\nencountered by predictions of the model. This is where Explainable Artificial\nIntelligence (XAI) comes into the picture. XAI increases the trust placed in an\nAI system by medical practitioners as well as AI researchers, and thus,\neventually, leads to an increasingly widespread deployment of AI in healthcare.\n  In this paper, we present different interpretability techniques. The aim is\nto enlighten practitioners on the understandability and interpretability of\nexplainable AI systems using a variety of techniques available which can be\nvery advantageous in the health-care domain. Medical diagnosis model is\nresponsible for human life and we need to be confident enough to treat a\npatient as instructed by a black-box model. Our paper contains examples based\non the heart disease dataset and elucidates on how the explainability\ntechniques should be preferred to create trustworthiness while using AI systems\nin healthcare.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 05:18:43 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Dave", "Devam", ""], ["Naik", "Het", ""], ["Singhal", "Smiti", ""], ["Patel", "Pankesh", ""]]}, {"id": "2011.03252", "submitter": "Matteo Iovino", "authors": "Matteo Iovino, Jonathan Styrud, Pietro Falco and Christian Smith", "title": "Learning Behavior Trees with Genetic Programming in Unpredictable\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern industrial applications require robots to be able to operate in\nunpredictable environments, and programs to be created with a minimal effort,\nas there may be frequent changes to the task. In this paper, we show that\ngenetic programming can be effectively used to learn the structure of a\nbehavior tree (BT) to solve a robotic task in an unpredictable environment.\nMoreover, we propose to use a simple simulator for the learning and demonstrate\nthat the learned BTs can solve the same task in a realistic simulator, reaching\nconvergence without the need for task specific heuristics. The learned solution\nis tolerant to faults, making our method appealing for real robotic\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 09:28:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Iovino", "Matteo", ""], ["Styrud", "Jonathan", ""], ["Falco", "Pietro", ""], ["Smith", "Christian", ""]]}, {"id": "2011.03274", "submitter": "Dennis Ulmer", "authors": "Dennis Ulmer, Lotta Meijerink and Giovanni Cin\\`a", "title": "Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD\n  Detection On Medical Tabular Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When deploying machine learning models in high-stakes real-world environments\nsuch as health care, it is crucial to accurately assess the uncertainty\nconcerning a model's prediction on abnormal inputs. However, there is a\nscarcity of literature analyzing this problem on medical data, especially on\nmixed-type tabular data such as Electronic Health Records. We close this gap by\npresenting a series of tests including a large variety of contemporary\nuncertainty estimation techniques, in order to determine whether they are able\nto identify out-of-distribution (OOD) patients. In contrast to previous work,\nwe design tests on realistic and clinically relevant OOD groups, and run\nexperiments on real-world medical data. We find that almost all techniques fail\nto achieve convincing results, partly disagreeing with earlier findings.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:41:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ulmer", "Dennis", ""], ["Meijerink", "Lotta", ""], ["Cin\u00e0", "Giovanni", ""]]}, {"id": "2011.03275", "submitter": "Jonas Tebbe", "authors": "Jonas Tebbe, Lukas Krauch, Yapeng Gao, Andreas Zell", "title": "Sample-efficient Reinforcement Learning in Robotic Table Tennis", "comments": "accepted at ICRA 2021 (Xian, China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has achieved some impressive recent successes in\nvarious computer games and simulations. Most of these successes are based on\nhaving large numbers of episodes from which the agent can learn. In typical\nrobotic applications, however, the number of feasible attempts is very limited.\nIn this paper we present a sample-efficient RL algorithm applied to the example\nof a table tennis robot. In table tennis every stroke is different, with\nvarying placement, speed and spin. An accurate return therefore has to be found\ndepending on a high-dimensional continuous state space. To make learning in few\ntrials possible the method is embedded into our robot system. In this way we\ncan use a one-step environment. The state space depends on the ball at hitting\ntime (position, velocity, spin) and the action is the racket state\n(orientation, velocity) at hitting. An actor-critic based deterministic policy\ngradient algorithm was developed for accelerated learning. Our approach\nperforms competitively both in a simulation and on the real robot in a number\nof challenging scenarios. Accurate results are obtained without pre-training in\nunder $200$ episodes of training. The video presenting our experiments is\navailable at https://youtu.be/uRAtdoL6Wpw.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:42:41 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 10:25:12 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 16:02:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tebbe", "Jonas", ""], ["Krauch", "Lukas", ""], ["Gao", "Yapeng", ""], ["Zell", "Andreas", ""]]}, {"id": "2011.03359", "submitter": "Hao Nie", "authors": "Hao Nie and Qin Zhang", "title": "A New Inference algorithm of Dynamic Uncertain Causality Graph based on\n  Conditional Sampling Method for Complex Cases", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2021.3093205", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Uncertain Causality Graph(DUCG) is a recently proposed model for\ndiagnoses of complex systems. It performs well for industry system such as\nnuclear power plants, chemical system and spacecrafts. However, the variable\nstate combination explosion in some cases is still a problem that may result in\ninefficiency or even disability in DUCG inference. In the situation of clinical\ndiagnoses, when a lot of intermediate causes are unknown while the downstream\nresults are known in a DUCG graph, the combination explosion may appear during\nthe inference computation. Monte Carlo sampling is a typical algorithm to solve\nthis problem. However, we are facing the case that the occurrence rate of the\ncase is very small, e.g. $10^{-20}$, which means a huge number of samplings are\nneeded. This paper proposes a new scheme based on conditional stochastic\nsimulation which obtains the final result from the expectation of the\nconditional probability in sampling loops instead of counting the sampling\nfrequency, and thus overcomes the problem. As a result, the proposed algorithm\nrequires much less time than the DUCG recursive inference algorithm presented\nearlier. Moreover, a simple analysis of convergence rate based on a designed\nexample is given to show the advantage of the proposed method. % In addition,\nsupports for logic gate, logic cycles, and parallelization, which exist in\nDUCG, are also addressed in this paper. The new algorithm reduces the time\nconsumption a lot and performs 3 times faster than old one with 2.7% error\nratio in a practical graph for Viral Hepatitis B.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 13:55:12 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 02:29:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nie", "Hao", ""], ["Zhang", "Qin", ""]]}, {"id": "2011.03372", "submitter": "Lei Cheng", "authors": "Chunhui Zhang, Yongyuan Liang, Xiaoming Yuan, and Lei Cheng", "title": "FDNAS: Improving Data Privacy and Model Diversity in AutoML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To prevent the leakage of private information while enabling automated\nmachine intelligence, there is an emerging trend to integrate federated\nlearning and Neural Architecture Search (NAS). Although promising as it may\nseem, the coupling of difficulties from both two tenets makes the algorithm\ndevelopment quite challenging. In particular, how to efficiently search the\noptimal neural architecture directly from massive non-iid data of clients in a\nfederated manner remains to be a hard nut to crack. To tackle this challenge,\nin this paper, by leveraging the advances in proxy-less NAS, we propose a\nFederated Direct Neural Architecture Search (FDNAS) framework that allows\nhardware-aware NAS from decentralized non-iid data of clients. To further adapt\nfor various data distributions of clients, inspired by meta-learning, a cluster\nFederated Direct Neural Architecture Search (CFDNAS) framework is proposed to\nachieve client-aware NAS, in the sense that each client can learn a tailored\ndeep learning model for its particular data distribution. Extensive experiments\non real-world non-iid datasets show state-of-the-art accuracy-efficiency\ntrade-offs for various hardware and data distributions of clients. Our codes\nwill be released publicly upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:13:42 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zhang", "Chunhui", ""], ["Liang", "Yongyuan", ""], ["Yuan", "Xiaoming", ""], ["Cheng", "Lei", ""]]}, {"id": "2011.03435", "submitter": "Revanth Reddy", "authors": "Revanth Gangi Reddy, Md Arafat Sultan, Efsun Sarioglu Kayi, Rong\n  Zhang, Vittorio Castelli, Avirup Sil", "title": "Answer Span Correction in Machine Reading Comprehension", "comments": "Accepted in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer validation in machine reading comprehension (MRC) consists of\nverifying an extracted answer against an input context and question pair.\nPrevious work has looked at re-assessing the \"answerability\" of the question\ngiven the extracted answer. Here we address a different problem: the tendency\nof existing MRC systems to produce partially correct answers when presented\nwith answerable questions. We explore the nature of such errors and propose a\npost-processing correction method that yields statistically significant\nperformance improvements over state-of-the-art MRC systems in both monolingual\nand multilingual evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:31:07 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Reddy", "Revanth Gangi", ""], ["Sultan", "Md Arafat", ""], ["Kayi", "Efsun Sarioglu", ""], ["Zhang", "Rong", ""], ["Castelli", "Vittorio", ""], ["Sil", "Avirup", ""]]}, {"id": "2011.03459", "submitter": "Pasquale Minervini", "authors": "Erik Arakelyan, Daniel Daza, Pasquale Minervini, Michael Cochez", "title": "Complex Query Answering with Neural Link Predictors", "comments": "Proceedings of the Ninth International Conference on Learning\n  Representations (ICLR 2021, oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural link predictors are immensely useful for identifying missing edges in\nlarge scale Knowledge Graphs. However, it is still not clear how to use these\nmodels for answering more complex queries that arise in a number of domains,\nsuch as queries using logical conjunctions ($\\land$), disjunctions ($\\lor$) and\nexistential quantifiers ($\\exists$), while accounting for missing edges. In\nthis work, we propose a framework for efficiently answering complex queries on\nincomplete Knowledge Graphs. We translate each query into an end-to-end\ndifferentiable objective, where the truth value of each atom is computed by a\npre-trained neural link predictor. We then analyse two solutions to the\noptimisation problem, including gradient-based and combinatorial search. In our\nexperiments, the proposed approach produces more accurate results than\nstate-of-the-art methods -- black-box neural models trained on millions of\ngenerated queries -- without the need of training on a large and diverse set of\ncomplex queries. Using orders of magnitude less training data, we obtain\nrelative improvements ranging from 8% up to 40% in Hits@3 across different\nknowledge graphs containing factual information. Finally, we demonstrate that\nit is possible to explain the outcome of our model in terms of the intermediate\nsolutions identified for each of the complex query atoms. All our source code\nand datasets are available online, at https://github.com/uclnlp/cqd.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:20:49 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 21:19:37 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 18:08:30 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 09:42:49 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Arakelyan", "Erik", ""], ["Daza", "Daniel", ""], ["Minervini", "Pasquale", ""], ["Cochez", "Michael", ""]]}, {"id": "2011.03471", "submitter": "Yulin Zhang", "authors": "Yulin Zhang, Hazhar Rahmani, Dylan A. Shell, Jason M. O'Kane", "title": "Accelerating combinatorial filter reduction through constraints", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduction of combinatorial filters involves compressing state representations\nthat robots use. Such optimization arises in automating the construction of\nminimalist robots. But exact combinatorial filter reduction is an NP-complete\nproblem and all current techniques are either inexact or formalized with\nexponentially many constraints. This paper proposes a new formalization needing\nonly a polynomial number of constraints, and characterizes these constraints in\nthree different forms: nonlinear, linear, and conjunctive normal form.\nEmpirical results show that constraints in conjunctive normal form capture the\nproblem most effectively, leading to a method that outperforms the others.\nFurther examination indicates that a substantial proportion of constraints\nremain inactive during iterative filter reduction. To leverage this\nobservation, we introduce just-in-time generation of such constraints, which\nyields improvements in efficiency and has the potential to minimize large\nfilters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:52:06 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zhang", "Yulin", ""], ["Rahmani", "Hazhar", ""], ["Shell", "Dylan A.", ""], ["O'Kane", "Jason M.", ""]]}, {"id": "2011.03488", "submitter": "Gustav Sourek", "authors": "Gustav Sourek, Filip Zelezny, Ondrej Kuzelka", "title": "Learning with Molecules beyond Graph Neural Networks", "comments": "accepted to Machine Learning for Molecules Workshop @ NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a deep learning framework which is inherently based in the\nhighly expressive language of relational logic, enabling to, among other\nthings, capture arbitrarily complex graph structures. We show how Graph Neural\nNetworks and similar models can be easily covered in the framework by\nspecifying the underlying propagation rules in the relational logic. The\ndeclarative nature of the used language then allows to easily modify and extend\nthe propagation schemes into complex structures, such as the molecular rings\nwhich we choose for a short demonstration in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:42:42 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sourek", "Gustav", ""], ["Zelezny", "Filip", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "2011.03506", "submitter": "Christopher Grimm", "authors": "Christopher Grimm, Andr\\'e Barreto, Satinder Singh, David Silver", "title": "The Value Equivalence Principle for Model-Based Reinforcement Learning", "comments": "NeurIPS-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning models of the environment from data is often viewed as an essential\ncomponent to building intelligent reinforcement learning (RL) agents. The\ncommon practice is to separate the learning of the model from its use, by\nconstructing a model of the environment's dynamics that correctly predicts the\nobserved state transitions. In this paper we argue that the limited\nrepresentational resources of model-based RL agents are better used to build\nmodels that are directly useful for value-based planning. As our main\ncontribution, we introduce the principle of value equivalence: two models are\nvalue equivalent with respect to a set of functions and policies if they yield\nthe same Bellman updates. We propose a formulation of the model learning\nproblem based on the value equivalence principle and analyze how the set of\nfeasible solutions is impacted by the choice of policies and functions.\nSpecifically, we show that, as we augment the set of policies and functions\nconsidered, the class of value equivalent models shrinks, until eventually\ncollapsing to a single point corresponding to a model that perfectly describes\nthe environment. In many problems, directly modelling state-to-state\ntransitions may be both difficult and unnecessary. By leveraging the\nvalue-equivalence principle one may find simpler models without compromising\nperformance, saving computation and memory. We illustrate the benefits of\nvalue-equivalent model learning with experiments comparing it against more\ntraditional counterparts like maximum likelihood estimation. More generally, we\nargue that the principle of value equivalence underlies a number of recent\nempirical successes in RL, such as Value Iteration Networks, the Predictron,\nValue Prediction Networks, TreeQN, and MuZero, and provides a first theoretical\nunderpinning of those results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:25:54 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Grimm", "Christopher", ""], ["Barreto", "Andr\u00e9", ""], ["Singh", "Satinder", ""], ["Silver", "David", ""]]}, {"id": "2011.03574", "submitter": "Uri Alon", "authors": "Ben Finkelshtein, Chaim Baskin, Evgenii Zheltonozhskii, Uri Alon", "title": "Single-Node Attack for Fooling Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have shown broad applicability in a variety of\ndomains. Some of these domains, such as social networks and product\nrecommendations, are fertile ground for malicious users and behavior. In this\npaper, we show that GNNs are vulnerable to the extremely limited scenario of a\nsingle-node adversarial example, where the node cannot be picked by the\nattacker. That is, an attacker can force the GNN to classify any target node to\na chosen label by only slightly perturbing another single arbitrary node in the\ngraph, even when not being able to pick that specific attacker node. When the\nadversary is allowed to pick a specific attacker node, the attack is even more\neffective. We show that this attack is effective across various GNN types, such\nas GraphSAGE, GCN, GAT, and GIN, across a variety of real-world datasets, and\nas a targeted and a non-targeted attack. Our code is available at\nhttps://github.com/benfinkelshtein/SINGLE .\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:59:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Finkelshtein", "Ben", ""], ["Baskin", "Chaim", ""], ["Zheltonozhskii", "Evgenii", ""], ["Alon", "Uri", ""]]}, {"id": "2011.03591", "submitter": "Aleksandra \\'Ciprijanovi\\'c", "authors": "A. \\'Ciprijanovi\\'c and D. Kafkes and S. Jenkins and K. Downey and G.\n  N. Perdue and S. Madireddy and T. Johnston and B. Nord", "title": "Domain adaptation techniques for improved cross-domain study of galaxy\n  mergers", "comments": "Accepted in: Machine Learning and the Physical Sciences - Workshop at\n  the 34th Conference on Neural Information Processing Systems (NeurIPS); final\n  version", "journal-ref": null, "doi": null, "report-no": "FERMILAB-CONF-20-582-SCD", "categories": "astro-ph.IM astro-ph.GA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In astronomy, neural networks are often trained on simulated data with the\nprospect of being applied to real observations. Unfortunately, simply training\na deep neural network on images from one domain does not guarantee satisfactory\nperformance on new images from a different domain. The ability to share\ncross-domain knowledge is the main advantage of modern deep domain adaptation\ntechniques. Here we demonstrate the use of two techniques - Maximum Mean\nDiscrepancy (MMD) and adversarial training with Domain Adversarial Neural\nNetworks (DANN) - for the classification of distant galaxy mergers from the\nIllustris-1 simulation, where the two domains presented differ only due to\ninclusion of observational noise. We show how the addition of either MMD or\nadversarial training greatly improves the performance of the classifier on the\ntarget domain when compared to conventional machine learning algorithms,\nthereby demonstrating great promise for their use in astronomy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 20:42:32 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 15:48:22 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 23:36:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["\u0106iprijanovi\u0107", "A.", ""], ["Kafkes", "D.", ""], ["Jenkins", "S.", ""], ["Downey", "K.", ""], ["Perdue", "G. N.", ""], ["Madireddy", "S.", ""], ["Johnston", "T.", ""], ["Nord", "B.", ""]]}, {"id": "2011.03609", "submitter": "Qian Luo", "authors": "Qian Luo, Maks Sorokin, Sehoon Ha", "title": "A Few Shot Adaptation of Visual Navigation Skills to New Observations\n  using Meta-Learning", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-driven visual navigation is a challenging problem that requires a\nrobot to find the goal using only visual inputs. Many researchers have\ndemonstrated promising results using deep reinforcement learning (deep RL) on\nvarious robotic platforms, but typical end-to-end learning is known for its\npoor extrapolation capability to new scenarios. Therefore, learning a\nnavigation policy for a new robot with a new sensor configuration or a new\ntarget still remains a challenging problem. In this paper, we introduce a\nlearning algorithm that enables rapid adaptation to new sensor configurations\nor target objects with a few shots. We design a policy architecture with latent\nfeatures between perception and inference networks and quickly adapt the\nperception network via meta-learning while freezing the inference network. Our\nexperiments show that our algorithm adapts the learned navigation policy with\nonly three shots for unseen situations with different sensor configurations or\ndifferent target colors. We also analyze the proposed algorithm by\ninvestigating various hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:53:52 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:07:26 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 18:30:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Luo", "Qian", ""], ["Sorokin", "Maks", ""], ["Ha", "Sehoon", ""]]}, {"id": "2011.03616", "submitter": "Niranjan Hasabnis", "authors": "Niranjan Hasabnis and Justin Gottschlich", "title": "ControlFlag: A Self-Supervised Idiosyncratic Pattern Detection System\n  for Software Control Structures", "comments": "To appear in Proceedings of the 5th ACM SIGPLAN International\n  Symposium on Machine Programming (MAPS '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software debugging has been shown to utilize upwards of half of developers'\ntime. Yet, machine programming (MP), the field concerned with the automation of\nsoftware (and hardware) development, has recently made strides in both research\nand production-quality automated debugging systems. In this paper we present\nControlFlag, a self-supervised MP system that aims to improve debugging by\nattempting to detect idiosyncratic pattern violations in software control\nstructures. ControlFlag also suggests possible corrections in the event an\nanomalous pattern is detected. We present ControlFlag's design and provide an\nexperimental evaluation and analysis of its efficacy in identifying potential\nprogramming errors in production-quality software. As a first concrete evidence\ntowards improving software quality, ControlFlag has already found an anomaly in\nCURL that has been acknowledged and fixed by its developers. We also discuss\nfuture extensions of ControlFlag.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:19:05 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 06:55:50 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 00:44:27 GMT"}, {"version": "v4", "created": "Thu, 13 May 2021 21:02:03 GMT"}, {"version": "v5", "created": "Mon, 17 May 2021 16:22:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hasabnis", "Niranjan", ""], ["Gottschlich", "Justin", ""]]}, {"id": "2011.03639", "submitter": "Hunter Lang", "authors": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "title": "Graph cuts always find a global optimum for Potts models (with a catch)", "comments": "Published at ICML 2021. 18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the $\\alpha$-expansion algorithm for MAP inference always\nreturns a globally optimal assignment for Markov Random Fields with Potts\npairwise potentials, with a catch: the returned assignment is only guaranteed\nto be optimal for an instance within a small perturbation of the original\nproblem instance. In other words, all local minima with respect to expansion\nmoves are global minima to slightly perturbed versions of the problem. On\n\"real-world\" instances, MAP assignments of small perturbations of the problem\nshould be very similar to the MAP assignment(s) of the original problem\ninstance. We design an algorithm that can certify whether this is the case in\npractice. On several MAP inference problem instances from computer vision, this\nalgorithm certifies that MAP solutions to all of these perturbations are very\nclose to solutions of the original instance. These results taken together give\na cohesive explanation for the good performance of \"graph cuts\" algorithms in\npractice. Every local expansion minimum is a global minimum in a small\nperturbation of the problem, and all of these global minima are close to the\noriginal solution.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:01:06 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 01:33:06 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lang", "Hunter", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "2011.03646", "submitter": "Akshat Gupta", "authors": "Akshat Gupta, Xinjian Li, Sai Krishna Rallabandi, Alan W Black", "title": "Acoustics Based Intent Recognition Using Discovered Phonetic Units for\n  Low Resource Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advancements in language technologies, humans are now speaking to\ndevices. Increasing the reach of spoken language technologies requires building\nsystems in local languages. A major bottleneck here are the underlying\ndata-intensive parts that make up such systems, including automatic speech\nrecognition (ASR) systems that require large amounts of labelled data. With the\naim of aiding development of spoken dialog systems in low resourced languages,\nwe propose a novel acoustics based intent recognition system that uses\ndiscovered phonetic units for intent classification. The system is made up of\ntwo blocks - the first block is a universal phone recognition system that\ngenerates a transcript of discovered phonetic units for the input audio, and\nthe second block performs intent classification from the generated phonetic\ntranscripts. We propose a CNN+LSTM based architecture and present results for\ntwo languages families - Indic languages and Romance languages, for two\ndifferent intent recognition tasks. We also perform multilingual training of\nour intent classifier and show improved cross-lingual transfer and zero-shot\nperformance on an unknown language within the same language family.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:35:31 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 20:59:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gupta", "Akshat", ""], ["Li", "Xinjian", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "2011.03649", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager, Kotagiri Ramamohanarao, Rajkumar Buyya", "title": "Thermal Prediction for Efficient Energy Management of Clouds using\n  Machine Learning", "comments": "Under submission at IEEE Transactions on Parallel and Distributed\n  Systems (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal management in the hyper-scale cloud data centers is a critical\nproblem. Increased host temperature creates hotspots which significantly\nincreases cooling cost and affects reliability. Accurate prediction of host\ntemperature is crucial for managing the resources effectively. Temperature\nestimation is a non-trivial problem due to thermal variations in the data\ncenter. Existing solutions for temperature estimation are inefficient due to\ntheir computational complexity and lack of accurate prediction. However,\ndata-driven machine learning methods for temperature prediction is a promising\napproach. In this regard, we collect and study data from a private cloud and\nshow the presence of thermal variations. We investigate several machine\nlearning models to accurately predict the host temperature. Specifically, we\npropose a gradient boosting machine learning model for temperature prediction.\nThe experiment results show that our model accurately predicts the temperature\nwith the average RMSE value of 0.05 or an average prediction error of 2.38\ndegree Celsius, which is 6 degree Celsius less as compared to an existing\ntheoretical model. In addition, we propose a dynamic scheduling algorithm to\nminimize the peak temperature of hosts. The results show that our algorithm\nreduces the peak temperature by 6.5 degree Celsius and consumes 34.5% less\nenergy as compared to the baseline algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:55:47 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 03:57:01 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 01:14:59 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ilager", "Shashikant", ""], ["Ramamohanarao", "Kotagiri", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2011.03701", "submitter": "Guoqing Bao", "authors": "Guoqing Bao, Manuel B. Graeber and Xiuying Wang", "title": "Depthwise Multiception Convolution for Reducing Network Parameters\n  without Sacrificing Accuracy", "comments": "This paper was accepted by ICARCV 2020", "journal-ref": null, "doi": "10.1109/ICARCV50220.2020.9305369", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been proven successful in multiple\nbenchmark challenges in recent years. However, the performance improvements are\nheavily reliant on increasingly complex network architecture and a high number\nof parameters, which require ever increasing amounts of storage and memory\ncapacity. Depthwise separable convolution (DSConv) can effectively reduce the\nnumber of required parameters through decoupling standard convolution into\nspatial and cross-channel convolution steps. However, the method causes a\ndegradation of accuracy. To address this problem, we present depthwise\nmultiception convolution, termed Multiception, which introduces layer-wise\nmultiscale kernels to learn multiscale representations of all individual input\nchannels simultaneously. We have carried out the experiment on four benchmark\ndatasets, i.e. Cifar-10, Cifar-100, STL-10 and ImageNet32x32, using five\npopular CNN models, Multiception achieved accuracy promotion in all models and\ndemonstrated higher accuracy performance compared to related works. Meanwhile,\nMultiception significantly reduces the number of parameters of standard\nconvolution-based models by 32.48% on average while still preserving accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 05:33:54 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bao", "Guoqing", ""], ["Graeber", "Manuel B.", ""], ["Wang", "Xiuying", ""]]}, {"id": "2011.03704", "submitter": "Matthew Ferland", "authors": "Kyle Burke, Matthew Ferland, Shang-Hua Teng", "title": "Quantum Combinatorial Games: Structures and Computational Complexity", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a standardized framework was proposed for introducing\nquantum-inspired moves in mathematical games with perfect information and no\nchance. The beauty of quantum games-succinct in representation, rich in\nstructures, explosive in complexity, dazzling for visualization, and\nsophisticated for strategic reasoning-has drawn us to play concrete games full\nof subtleties and to characterize abstract properties pertinent to complexity\nconsequence. Going beyond individual games, we explore the tractability of\nquantum combinatorial games as whole, and address fundamental questions\nincluding:\n  Quantum Leap in Complexity: Are there polynomial-time solvable games whose\nquantum extensions are intractable?\n  Quantum Collapses in Complexity: Are there PSPACE-complete games whose\nquantum extensions fall to the lower levels of the polynomial-time hierarchy?\n  Quantumness Matters: How do outcome classes and strategies change under\nquantum moves? Under what conditions doesn't quantumness matter?\n  PSPACE Barrier for Quantum Leap: Can quantum moves launch PSPACE games into\nouter polynomial space\n  We show that quantum moves not only enrich the game structure, but also\nimpact their computational complexity. In settling some of these basic\nquestions, we characterize both the powers and limitations of quantum moves as\nwell as the superposition of game configurations that they create. Our\nconstructive proofs-both on the leap of complexity in concrete Quantum Nim and\nQuantum Undirected Geography and on the continuous collapses, in the quantum\nsetting, of complexity in abstract PSPACE-complete games to each level of the\npolynomial-time hierarchy-illustrate the striking computational landscape over\nquantum games and highlight surprising turns with unexpected quantum impact.\nOur studies also enable us to identify several elegant open questions\nfundamental to quantum combinatorial game theory (QCGT).\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 06:09:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Burke", "Kyle", ""], ["Ferland", "Matthew", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "2011.03721", "submitter": "Liangzi Rong", "authors": "Liangzi Rong, Chunping Li", "title": "Coarse- and Fine-grained Attention Network with Background-aware Loss\n  for Crowd Density Map Estimation", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method Coarse- and Fine-grained Attention\nNetwork (CFANet) for generating high-quality crowd density maps and people\ncount estimation by incorporating attention maps to better focus on the crowd\narea. We devise a from-coarse-to-fine progressive attention mechanism by\nintegrating Crowd Region Recognizer (CRR) and Density Level Estimator (DLE)\nbranch, which can suppress the influence of irrelevant background and assign\nattention weights according to the crowd density levels, because generating\naccurate fine-grained attention maps directly is normally difficult. We also\nemploy a multi-level supervision mechanism to assist the backpropagation of\ngradient and reduce overfitting. Besides, we propose a Background-aware\nStructural Loss (BSL) to reduce the false recognition ratio while improving the\nstructural similarity to groundtruth. Extensive experiments on commonly used\ndatasets show that our method can not only outperform previous state-of-the-art\nmethods in terms of count accuracy but also improve the image quality of\ndensity maps as well as reduce the false recognition ratio.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 08:05:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Rong", "Liangzi", ""], ["Li", "Chunping", ""]]}, {"id": "2011.03722", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Abhijit Mishra, Md Faisal Mahbub Chowdhury, Sagar Manohar, Dan\n  Gutfreund and Karthik Sankaranarayanan", "title": "Template Controllable keywords-to-text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel neural model for the understudied task of\ngenerating text from keywords. The model takes as input a set of un-ordered\nkeywords, and part-of-speech (POS) based template instructions. This makes it\nideal for surface realization in any NLG setup. The framework is based on the\nencode-attend-decode paradigm, where keywords and templates are encoded first,\nand the decoder judiciously attends over the contexts derived from the encoded\nkeywords and templates to generate the sentences. Training exploits weak\nsupervision, as the model trains on a large amount of labeled data with\nkeywords and POS based templates prepared through completely automatic means.\nQualitative and quantitative performance analyses on publicly available\ntest-data in various domains reveal our system's superiority over baselines,\nbuilt using state-of-the-art neural machine translation and controllable\ntransfer techniques. Our approach is indifferent to the order of input\nkeywords.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 08:05:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mishra", "Abhijit", ""], ["Chowdhury", "Md Faisal Mahbub", ""], ["Manohar", "Sagar", ""], ["Gutfreund", "Dan", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "2011.03751", "submitter": "Anh Nguyen Duc", "authors": "Elizamary Nascimento, Anh Nguyen-Duc, Ingrid Sundb{\\o} and Tayana\n  Conte", "title": "Software engineering for artificial intelligence and machine learning\n  software: A systematic literature review", "comments": "The paper is under submission to the Journal of System and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) or Machine Learning (ML) systems have been\nwidely adopted as value propositions by companies in all industries in order to\ncreate or extend the services and products they offer. However, developing\nAI/ML systems has presented several engineering problems that are different\nfrom those that arise in, non-AI/ML software development. This study aims to\ninvestigate how software engineering (SE) has been applied in the development\nof AI/ML systems and identify challenges and practices that are applicable and\ndetermine whether they meet the needs of professionals. Also, we assessed\nwhether these SE practices apply to different contexts, and in which areas they\nmay be applicable. We conducted a systematic review of literature from 1990 to\n2019 to (i) understand and summarize the current state of the art in this field\nand (ii) analyze its limitations and open challenges that will drive future\nresearch. Our results show these systems are developed on a lab context or a\nlarge company and followed a research-driven development process. The main\nchallenges faced by professionals are in areas of testing, AI software quality,\nand data management. The contribution types of most of the proposed SE\npractices are guidelines, lessons learned, and tools.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 11:06:28 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Nascimento", "Elizamary", ""], ["Nguyen-Duc", "Anh", ""], ["Sundb\u00f8", "Ingrid", ""], ["Conte", "Tayana", ""]]}, {"id": "2011.03755", "submitter": "Jason Angel", "authors": "Jason Angel, Carlos A. Rodriguez-Diaz, Alexander Gelbukh, Sergio\n  Jimenez", "title": "NLP-CIC @ DIACR-Ita: POS and Neighbor Based Distributional Models for\n  Lexical Semantic Change in Diachronic Italian Corpora", "comments": "Accepted at EVALITA 2020: Proceedings of Seventh Evaluation Campaign\n  of Natural Language Processing and Speech Tools for Italian. Final Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our systems and findings on unsupervised lexical semantic change\nfor the Italian language in the DIACR-Ita shared-task at EVALITA 2020. The task\nis to determine whether a target word has evolved its meaning with time, only\nrelying on raw-text from two time-specific datasets. We propose two models\nrepresenting the target words across the periods to predict the changing words\nusing threshold and voting schemes. Our first model solely relies on\npart-of-speech usage and an ensemble of distance measures. The second model\nuses word embedding representation to extract the neighbor's relative distances\nacross spaces and propose \"the average of absolute differences\" to estimate\nlexical semantic change. Our models achieved competent results, ranking third\nin the DIACR-Ita competition. Furthermore, we experiment with the k_neighbor\nparameter of our second model to compare the impact of using \"the average of\nabsolute differences\" versus the cosine distance used in Hamilton et al.\n(2016).\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 11:27:18 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Angel", "Jason", ""], ["Rodriguez-Diaz", "Carlos A.", ""], ["Gelbukh", "Alexander", ""], ["Jimenez", "Sergio", ""]]}, {"id": "2011.03760", "submitter": "Jason Angel", "authors": "Jason Angel, Segun Taofeek Aroyehun, Alexander Gelbukh", "title": "NLP-CIC @ PRELEARN: Mastering prerequisites relations, from handcrafted\n  features to embeddings", "comments": "Accepted at EVALITA 2020: Proceedings of Seventh Evaluation Campaign\n  of Natural Language Processing and Speech Tools for Italian. Final Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our systems and findings for the prerequisite relation learning\ntask (PRELEARN) at EVALITA 2020. The task aims to classify whether a pair of\nconcepts hold a prerequisite relation or not. We model the problem using\nhandcrafted features and embedding representations for in-domain and\ncross-domain scenarios. Our submissions ranked first place in both scenarios\nwith average F1 score of 0.887 and 0.690 respectively across domains on the\ntest sets. We made our code is freely available.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 12:13:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Angel", "Jason", ""], ["Aroyehun", "Segun Taofeek", ""], ["Gelbukh", "Alexander", ""]]}, {"id": "2011.03775", "submitter": "Jing Yu Koh", "authors": "Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang", "title": "Text-to-Image Generation Grounded by Fine-Grained User Attention", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localized Narratives is a dataset with detailed natural language descriptions\nof images paired with mouse traces that provide a sparse, fine-grained visual\ngrounding for phrases. We propose TReCS, a sequential model that exploits this\ngrounding to generate images. TReCS uses descriptions to retrieve segmentation\nmasks and predict object labels aligned with mouse traces. These alignments are\nused to select and position masks to generate a fully covered segmentation\ncanvas; the final image is produced by a segmentation-to-image generator using\nthis canvas. This multi-step, retrieval-based approach outperforms existing\ndirect text-to-image generation models on both automatic metrics and human\nevaluations: overall, its generated images are more photo-realistic and better\nmatch descriptions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 13:23:31 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:52:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Koh", "Jing Yu", ""], ["Baldridge", "Jason", ""], ["Lee", "Honglak", ""], ["Yang", "Yinfei", ""]]}, {"id": "2011.03783", "submitter": "Lifeng Han", "authors": "Lifeng Han, Gareth Jones, Alan Smeaton", "title": "AlphaMWE: Construction of Multilingual Parallel Corpora with MWE\n  Annotations", "comments": "Accepted to Proceedings of MWE-LEX2020@COLING, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present the construction of multilingual parallel corpora\nwith annotation of multiword expressions (MWEs). MWEs include verbal MWEs\n(vMWEs) defined in the PARSEME shared task that have a verb as the head of the\nstudied terms. The annotated vMWEs are also bilingually and multilingually\naligned manually. The languages covered include English, Chinese, Polish, and\nGerman. Our original English corpus is taken from the PARSEME shared task in\n2018. We performed machine translation of this source corpus followed by human\npost editing and annotation of target MWEs. Strict quality control was applied\nfor error limitation, i.e., each MT output sentence received first manual post\nediting and annotation plus second manual quality rechecking. One of our\nfindings during corpora preparation is that accurate translation of MWEs\npresents challenges to MT systems. To facilitate further MT research, we\npresent a categorisation of the error types encountered by MT systems in\nperforming MWE related translation. To acquire a broader view of MT issues, we\nselected four popular state-of-the-art MT models for comparisons namely:\nMicrosoft Bing Translator, GoogleMT, Baidu Fanyi and DeepL MT. Because of the\nnoise removal, translation post editing and MWE annotation by human\nprofessionals, we believe our AlphaMWE dataset will be an asset for\ncross-lingual and multilingual research, such as MT and information extraction.\nOur multilingual corpora are available as open access at\ngithub.com/poethan/AlphaMWE.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 14:28:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Han", "Lifeng", ""], ["Jones", "Gareth", ""], ["Smeaton", "Alan", ""]]}, {"id": "2011.03798", "submitter": "Linlin Chao", "authors": "Linlin Chao, Jianshan He, Taifeng Wang, Wei Chu", "title": "PairRE: Knowledge Graph Embeddings via Paired Relation Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance based knowledge graph embedding methods show promising results on\nlink prediction task, on which two topics have been widely studied: one is the\nability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the\nother is to encode various relation patterns, such as symmetry/antisymmetry.\nHowever, the existing methods fail to solve these two problems at the same\ntime, which leads to unsatisfactory results. To mitigate this problem, we\npropose PairRE, a model with paired vectors for each relation representation.\nThe paired vectors enable an adaptive adjustment of the margin in loss function\nto fit for complex relations. Besides, PairRE is capable of encoding three\nimportant relation patterns, symmetry/antisymmetry, inverse and composition.\nGiven simple constraints on relation representations, PairRE can encode\nsubrelation further. Experiments on link prediction benchmarks demonstrate the\nproposed key capabilities of PairRE. Moreover, We set a new state-of-the-art on\ntwo knowledge graph datasets of the challenging Open Graph Benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 16:09:03 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 09:14:46 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 13:06:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chao", "Linlin", ""], ["He", "Jianshan", ""], ["Wang", "Taifeng", ""], ["Chu", "Wei", ""]]}, {"id": "2011.03813", "submitter": "Yiyuan Lee", "authors": "Yiyuan Lee, Panpan Cai, David Hsu", "title": "MAGIC: Learning Macro-Actions for Online POMDP Planning", "comments": "9 pages (+ 2 page references, + 2 page appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partially observable Markov decision process (POMDP) is a principled\ngeneral framework for robot decision making under uncertainty, but POMDP\nplanning suffers from high computational complexity, when long-term planning is\nrequired. While temporally-extended macro-actions help to cut down the\neffective planning horizon and significantly improve computational efficiency,\nhow do we acquire good macro-actions? This paper proposes Macro-Action\nGenerator-Critic (MAGIC), which performs offline learning of macro-actions\noptimized for online POMDP planning. Specifically, MAGIC learns a macro-action\ngenerator end-to-end, using an online planner's performance as the feedback.\nDuring online planning, the generator generates on the fly situation-aware\nmacro-actions conditioned on the robot's belief and the environment context. We\nevaluated MAGIC on several long-horizon planning tasks both in simulation and\non a real robot. The experimental results show that the learned macro-actions\noffer significant benefits in online planning performance, compared with\nprimitive actions and handcrafted macro-actions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 17:18:45 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 12:45:54 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 16:20:56 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 06:04:09 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lee", "Yiyuan", ""], ["Cai", "Panpan", ""], ["Hsu", "David", ""]]}, {"id": "2011.03822", "submitter": "Chen Chen", "authors": "Weiping Yu and Taojiannan Yang and Chen Chen", "title": "Towards Resolving the Challenge of Long-tail Distribution in UAV Images\n  for Object Detection", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for object detection in UAV images ignored an important\nchallenge - imbalanced class distribution in UAV images - which leads to poor\nperformance on tail classes. We systematically investigate existing solutions\nto long-tail problems and unveil that re-balancing methods that are effective\non natural image datasets cannot be trivially applied to UAV datasets. To this\nend, we rethink long-tailed object detection in UAV images and propose the Dual\nSampler and Head detection Network (DSHNet), which is the first work that aims\nto resolve long-tail distribution in UAV images. The key components in DSHNet\ninclude Class-Biased Samplers (CBS) and Bilateral Box Heads (BBH), which are\ndeveloped to cope with tail classes and head classes in a dual-path manner.\nWithout bells and whistles, DSHNet significantly boosts the performance of tail\nclasses on different detection frameworks. Moreover, DSHNet significantly\noutperforms base detectors and generic approaches for long-tail problems on\nVisDrone and UAVDT datasets. It achieves new state-of-the-art performance when\ncombining with image cropping methods. Code is available at\nhttps://github.com/we1pingyu/DSHNet\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 17:53:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yu", "Weiping", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2011.03835", "submitter": "Thibaud De Souza", "authors": "Thibaud de Souza", "title": "Implementing Behavior Trees using Three-Valued Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With consideration to behavior trees and their relevance to planning and\ncontrol, within and without game development, the distinction between stateful\nand stateless models is discussed; a three-valued logic bridging traditional\ncontrol flow with behavior trees is introduced, and a C# implementation is\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:12:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["de Souza", "Thibaud", ""]]}, {"id": "2011.03836", "submitter": "Ning Li", "authors": "Ning Li, Bethany Keller, Mark Butler, Daniel Cer", "title": "SeqGenSQL -- A Robust Sequence Generation Model for Structured Query\n  Language", "comments": "6 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore using T5 (Raffel et al. (2019)) to directly translate natural\nlanguage questions into SQL statements. General purpose natural language that\ninterfaces to information stored within databases requires flexibly translating\nnatural language questions into database queries. The best performing\ntext-to-SQL systems approach this task by first converting questions into an\nintermediate logical form (LF) (Lyu et al. (2020)). While LFs provide a\nconvenient intermediate representation and simplify query generation, they\nintroduce an additional layer of complexity and annotation requirements.\nHowever, weakly supervised modeling that directly converts questions to SQL\nstatements has proven more difficult without the scaffolding provided by LFs\n(Min et al. (2019)). We approach direct conversion of questions to SQL\nstatements using T5 (Raffel et al. (2019)), a pre-trained textto-text\ngeneration model, modified to support pointer-generator style decoding (See et\nal. (2017)). We explore using question augmentation with table schema\ninformation and the use of automatically generated silver training data. The\nresulting model achieves 90.5% execution accuracy on the WikiSQL (Zhong et al.\n(2017)) test data set, a new state-of-the-art on weakly supervised SQL\ngeneration. The performance improvement is 6.6% absolute over the prior\nstate-of-the-art (Min et al. (2019)) and approaches the performance of\nstate-ofthe-art systems making use of LFs.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:22:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Ning", ""], ["Keller", "Bethany", ""], ["Butler", "Mark", ""], ["Cer", "Daniel", ""]]}, {"id": "2011.03863", "submitter": "Kaixin Ma", "authors": "Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric\n  Nyberg, Alessandro Oltramari", "title": "Knowledge-driven Data Construction for Zero-shot Evaluation in\n  Commonsense Question Answering", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in pre-trained neural language modeling have led to leaps\nin accuracy on commonsense question-answering benchmarks. However, there is\nincreasing concern that models overfit to specific tasks, without learning to\nutilize external knowledge or perform general semantic reasoning. In contrast,\nzero-shot evaluations have shown promise as a more robust measure of a model's\ngeneral reasoning abilities. In this paper, we propose a novel neuro-symbolic\nframework for zero-shot question answering across commonsense tasks. Guided by\na set of hypotheses, the framework studies how to transform various\npre-existing knowledge resources into a form that is most effective for\npre-training models. We vary the set of language models, training regimes,\nknowledge sources, and data generation strategies, and measure their impact\nacross tasks. Extending on prior work, we devise and compare four constrained\ndistractor-sampling strategies. We provide empirical results across five\ncommonsense question-answering tasks with data generated from five external\nknowledge resources. We show that, while an individual knowledge graph is\nbetter suited for specific tasks, a global knowledge graph brings consistent\ngains across different tasks. In addition, both preserving the structure of the\ntask as well as generating fair and informative questions help language models\nlearn more effectively.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 22:52:21 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 22:27:10 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ma", "Kaixin", ""], ["Ilievski", "Filip", ""], ["Francis", "Jonathan", ""], ["Bisk", "Yonatan", ""], ["Nyberg", "Eric", ""], ["Oltramari", "Alessandro", ""]]}, {"id": "2011.03870", "submitter": "Neema Kotonya", "authors": "Neema Kotonya and Francesca Toni", "title": "Explainable Automated Fact-Checking: A Survey", "comments": "Accepted to COLING 2020. Further resources available at\n  https://github.com/neemakot/Fact-Checking-Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A number of exciting advances have been made in automated fact-checking\nthanks to increasingly larger datasets and more powerful systems, leading to\nimprovements in the complexity of claims which can be accurately fact-checked.\nHowever, despite these advances, there are still desirable functionalities\nmissing from the fact-checking pipeline. In this survey, we focus on the\nexplanation functionality -- that is fact-checking systems providing reasons\nfor their predictions. We summarize existing methods for explaining the\npredictions of fact-checking systems and we explore trends in this topic.\nFurther, we consider what makes for good explanations in this specific domain\nthrough a comparative analysis of existing fact-checking explanations against\nsome desirable properties. Finally, we propose further research directions for\ngenerating fact-checking explanations, and describe how these may lead to\nimprovements in the research area.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 23:56:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kotonya", "Neema", ""], ["Toni", "Francesca", ""]]}, {"id": "2011.03882", "submitter": "Sarah Bechtle", "authors": "Sarah Bechtle, Neha Das and Franziska Meier", "title": "Multi-Modal Learning of Keypoint Predictive Models for Visual Object\n  Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have impressive generalization capabilities when it comes to\nmanipulating objects and tools in completely novel environments. These\ncapabilities are, at least partially, a result of humans having internal models\nof their bodies and any grasped object. How to learn such body schemas for\nrobots remains an open problem. In this work, we develop an self-supervised\napproach that can extend a robot's kinematic model when grasping an object from\nvisual latent representations. Our framework comprises two components: (1) we\npresent a multi-modal keypoint detector: an autoencoder architecture trained by\nfusing proprioception and vision to predict visual key points on an object; (2)\nwe show how we can use our learned keypoint detector to learn an extension of\nthe kinematic chain by regressing virtual joints from the predicted visual\nkeypoints. Our evaluation shows that our approach learns to consistently\npredict visual keypoints on objects in the manipulator's hand, and thus can\neasily facilitate learning an extended kinematic chain to include the object\ngrasped in various configurations, from a few seconds of visual data. Finally\nwe show that this extended kinematic chain lends itself for object manipulation\ntasks such as placing a grasped object and present experiments in simulation\nand on hardware.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 01:04:59 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 16:52:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bechtle", "Sarah", ""], ["Das", "Neha", ""], ["Meier", "Franziska", ""]]}, {"id": "2011.03890", "submitter": "Roman Levin", "authors": "Emil Noordeh, Roman Levin, Ruochen Jiang, Harris Shadmany", "title": "Echo Chambers in Collaborative Filtering Based Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems underpin the serving of nearly all online content in\nthe modern age. From Youtube and Netflix recommendations, to Facebook feeds and\nGoogle searches, these systems are designed to filter content to the predicted\npreferences of users. Recently, these systems have faced growing criticism with\nrespect to their impact on content diversity, social polarization, and the\nhealth of public discourse. In this work we simulate the recommendations given\nby collaborative filtering algorithms on users in the MovieLens data set. We\nfind that prolonged exposure to system-generated recommendations substantially\ndecreases content diversity, moving individual users into \"echo-chambers\"\ncharacterized by a narrow range of content. Furthermore, our work suggests that\nonce these echo-chambers have been established, it is difficult for an\nindividual user to break out by manipulating solely their own rating vector.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:35:47 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Noordeh", "Emil", ""], ["Levin", "Roman", ""], ["Jiang", "Ruochen", ""], ["Shadmany", "Harris", ""]]}, {"id": "2011.03891", "submitter": "Weiwei Fang", "authors": "Mengran Liu and Weiwei Fang and Xiaodong Ma and Wenyuan Xu and Naixue\n  Xiong and Yi Ding", "title": "Channel Pruning Guided by Spatial and Channel Attention for DNNs in\n  Intelligent Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved remarkable success in many computer\nvision tasks recently, but the huge number of parameters and the high\ncomputation overhead hinder their deployments on resource-constrained edge\ndevices. It is worth noting that channel pruning is an effective approach for\ncompressing DNN models. A critical challenge is to determine which channels are\nto be removed, so that the model accuracy will not be negatively affected. In\nthis paper, we first propose Spatial and Channel Attention (SCA), a new\nattention module combining both spatial and channel attention that respectively\nfocuses on \"where\" and \"what\" are the most informative parts. Guided by the\nscale values generated by SCA for measuring channel importance, we further\npropose a new channel pruning approach called Channel Pruning guided by Spatial\nand Channel Attention (CPSCA). Experimental results indicate that SCA achieves\nthe best inference accuracy, while incurring negligibly extra resource\nconsumption, compared to other state-of-the-art attention modules. Our\nevaluation on two benchmark datasets shows that, with the guidance of SCA, our\nCPSCA approach achieves higher inference accuracy than other state-of-the-art\npruning methods under the same pruning ratios.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:40:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 12:48:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Mengran", ""], ["Fang", "Weiwei", ""], ["Ma", "Xiaodong", ""], ["Xu", "Wenyuan", ""], ["Xiong", "Naixue", ""], ["Ding", "Yi", ""]]}, {"id": "2011.03894", "submitter": "Christoforos Mavrogiannis", "authors": "Junha Roh, Christoforos Mavrogiannis, Rishabh Madan, Dieter Fox,\n  Siddhartha S. Srinivasa", "title": "Multimodal Trajectory Prediction via Topological Invariance for\n  Navigation at Uncontrolled Intersections", "comments": "Preprint of a paper with the same title, accepted to the Conference\n  on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on decentralized navigation among multiple non-communicating\nrational agents at \\emph{uncontrolled} intersections, i.e., street\nintersections without traffic signs or signals. Avoiding collisions in such\ndomains relies on the ability of agents to predict each others' intentions\nreliably, and react quickly. Multiagent trajectory prediction is NP-hard\nwhereas the sample complexity of existing data-driven approaches limits their\napplicability. Our key insight is that the geometric structure of the\nintersection and the incentive of agents to move efficiently and avoid\ncollisions (rationality) reduces the space of likely behaviors, effectively\nrelaxing the problem of trajectory prediction. In this paper, we collapse the\nspace of multiagent trajectories at an intersection into a set of modes\nrepresenting different classes of multiagent behavior, formalized using a\nnotion of topological invariance. Based on this formalism, we design Multiple\nTopologies Prediction (MTP), a data-driven trajectory-prediction mechanism that\nreconstructs trajectory representations of high-likelihood modes in multiagent\nintersection scenes. We show that MTP outperforms a state-of-the-art multimodal\ntrajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24%\non a challenging simulated dataset. Finally, we show that MTP enables our\noptimization-based planner, MTPnav, to achieve collision-free and\ntime-efficient navigation across a variety of challenging intersection\nscenarios on the CARLA simulator.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:56:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Roh", "Junha", ""], ["Mavrogiannis", "Christoforos", ""], ["Madan", "Rishabh", ""], ["Fox", "Dieter", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "2011.03901", "submitter": "Shreya Khare Ms", "authors": "Alex Mathai, Shreya Khare, Srikanth Tamilselvam, Senthil Mani", "title": "Adversarial Black-Box Attacks On Text Classifiers Using Multi-Objective\n  Genetic Optimization Guided By Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel genetic-algorithm technique that generates black-box\nadversarial examples which successfully fool neural network based text\nclassifiers. We perform a genetic search with multi-objective optimization\nguided by deep learning based inferences and Seq2Seq mutation to generate\nsemantically similar but imperceptible adversaries. We compare our approach\nwith DeepWordBug (DWB) on SST and IMDB sentiment datasets by attacking three\ntrained models viz. char-LSTM, word-LSTM and elmo-LSTM. On an average, we\nachieve an attack success rate of 65.67% for SST and 36.45% for IMDB across the\nthree models showing an improvement of 49.48% and 101% respectively.\nFurthermore, our qualitative study indicates that 94% of the time, the users\nwere not able to distinguish between an original and adversarial sample.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 04:30:14 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 04:40:01 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mathai", "Alex", ""], ["Khare", "Shreya", ""], ["Tamilselvam", "Srikanth", ""], ["Mani", "Senthil", ""]]}, {"id": "2011.03909", "submitter": "Filipp Skomorokhov", "authors": "Filipp Skomorokhov (1 and 2) and George Ovchinnikov (2) ((1) Moscow\n  Institute of Physics and Technology, (2) Skolkovo Institute of Science and\n  Technology)", "title": "Reinforcement Learning for Assignment problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is dedicated to the application of reinforcement learning combined\nwith neural networks to the general formulation of user scheduling problem. Our\nsimulator resembles real world problems by means of stochastic changes in\nenvironment. We applied Q-learning based method to the number of dynamic\nsimulations and outperformed analytical greedy-based solution in terms of total\nreward, the aim of which is to get the lowest possible penalty throughout\nsimulation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 06:25:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Skomorokhov", "Filipp", "", "1 and 2"], ["Ovchinnikov", "George", ""]]}, {"id": "2011.03969", "submitter": "Marcos Baez", "authors": "Mla{\\dj}an Jovanovi\\'c, Marcos Baez, Fabio Casati", "title": "Chatbots as conversational healthcare services", "comments": null, "journal-ref": null, "doi": "10.1109/MIC.2020.3037151", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are emerging as a promising platform for accessing and delivering\nhealthcare services. The evidence is in the growing number of publicly\navailable chatbots aiming at taking an active role in the provision of\nprevention, diagnosis, and treatment services. This article takes a closer look\nat how these emerging chatbots address design aspects relevant to healthcare\nservice provision, emphasizing the Human-AI interaction aspects and the\ntransparency in AI automation and decision making.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 12:35:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Jovanovi\u0107", "Mla\u0111an", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""]]}, {"id": "2011.03972", "submitter": "Yunjie Tian", "authors": "Chang Liu and Yunjie Tian and Jianbin Jiao and Qixiang Ye", "title": "Adaptive Linear Span Network for Object Skeleton Detection", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3078079", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional networks for object skeleton detection are usually hand-crafted.\nAlthough effective, they require intensive priori knowledge to configure\nrepresentative features for objects in different scale granularity.In this\npaper, we propose adaptive linear span network (AdaLSN), driven by neural\narchitecture search (NAS), to automatically configure and integrate scale-aware\nfeatures for object skeleton detection. AdaLSN is formulated with the theory of\nlinear span, which provides one of the earliest explanations for multi-scale\ndeep feature fusion. AdaLSN is materialized by defining a mixed unit-pyramid\nsearch space, which goes beyond many existing search spaces using unit-level or\npyramid-level features.Within the mixed space, we apply genetic architecture\nsearch to jointly optimize unit-level operations and pyramid-level connections\nfor adaptive feature space expansion. AdaLSN substantiates its versatility by\nachieving significantly higher accuracy and latency trade-off compared with\nstate-of-the-arts. It also demonstrates general applicability to image-to-mask\ntasks such as edge detection and road extraction. Code is available at\n\\href{https://github.com/sunsmarterjie/SDL-Skeleton}{\\color{magenta}github.com/sunsmarterjie/SDL-Skeleton}.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 12:51:14 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Chang", ""], ["Tian", "Yunjie", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "2011.03974", "submitter": "Kai Chen", "authors": "Kai Chen, Twan van Laarhoven, Elena Marchiori", "title": "Skewed Laplace Spectral Mixture kernels for long-term forecasting in\n  Gaussian process", "comments": "13 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term forecasting involves predicting a horizon that is far ahead of the\nlast observation. It is a problem of highly practical relevance, for instance\nfor companies in order to decide upon expensive long-term investments. Despite\nrecent progress and success of Gaussian Processes (GPs) based on Spectral\nMixture kernels, long-term forecasting remains a challenging problem for these\nkernels because they decay exponentially at large horizons. This is mainly due\ntheir use of a mixture of Gaussians to model spectral densities. The challenges\nunderlying long-term forecasting become evident by investigating the\ndistribution of the Fourier coefficients of (the training part of) the signal,\nwhich is non-smooth, heavy-tailed, sparse and skewed. Notably the heavy tail\nand skewness characteristics of such distribution in spectral domain allow to\ncapture long range covariance of the signal in the time domain. Motivated by\nthese observations, we propose to model spectral densities using a Skewed\nLaplace Spectral Mixture (SLSM) due to the skewness of its peaks, sparsity,\nnon-smoothness, and heavy tail characteristics. By applying the inverse Fourier\nTransform to this spectral density we obtain a new GP kernel for long-term\nforecasting. Results of extensive experiments, including a multivariate time\nseries, show the beneficial effect of the proposed SLSM kernel for long-term\nextrapolation and robustness to the choice of the number of mixture components.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 13:03:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Kai", ""], ["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "2011.03984", "submitter": "Zhen Han", "authors": "Zhen Han, Yunpu Ma, Peng Chen, Volker Tresp", "title": "DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for\n  Temporal Knowledge Graph Completion", "comments": "16 pages, accepted as long paper at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been increasing interest in learning representations of\ntemporal knowledge graphs (KGs), which record the dynamic relationships between\nentities over time. Temporal KGs often exhibit multiple simultaneous\nnon-Euclidean structures, such as hierarchical and cyclic structures. However,\nexisting embedding approaches for temporal KGs typically learn entity\nrepresentations and their dynamic evolution in the Euclidean space, which might\nnot capture such intrinsic structures very well. To this end, we propose Dy-\nERNIE, a non-Euclidean embedding approach that learns evolving entity\nrepresentations in a product of Riemannian manifolds, where the composed spaces\nare estimated from the sectional curvatures of underlying data. Product\nmanifolds enable our approach to better reflect a wide variety of geometric\nstructures on temporal KGs. Besides, to capture the evolutionary dynamics of\ntemporal KGs, we let the entity representations evolve according to a velocity\nvector defined in the tangent space at each timestamp. We analyze in detail the\ncontribution of geometric spaces to representation learning of temporal KGs and\nevaluate our model on temporal knowledge graph completion tasks. Extensive\nexperiments on three real-world datasets demonstrate significantly improved\nperformance, indicating that the dynamics of multi-relational graph data can be\nmore properly modeled by the evolution of embeddings on Riemannian manifolds.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 14:04:16 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 13:20:02 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Han", "Zhen", ""], ["Ma", "Yunpu", ""], ["Chen", "Peng", ""], ["Tresp", "Volker", ""]]}, {"id": "2011.04000", "submitter": "Ashutosh Modi", "authors": "Ishika Singh and Ahsan Barkati and Tushar Goswamy and Ashutosh Modi", "title": "Adapting a Language Model for Controlled Affective Text Generation", "comments": "15 Pages (9 + 2 (references) + 4 (appendix)), accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human use language not just to convey information but also to express their\ninner feelings and mental states. In this work, we adapt the state-of-the-art\nlanguage generation models to generate affective (emotional) text. We posit a\nmodel capable of generating affect-driven and topic-focused sentences without\nlosing grammatical correctness as the affect intensity increases. We propose to\nincorporate emotion as prior for the probabilistic state-of-the-art text\ngeneration model such as GPT-2. The model gives a user the flexibility to\ncontrol the category and intensity of emotion as well as the topic of the\ngenerated text. Previous attempts at modelling fine-grained emotions fall out\non grammatical correctness at extreme intensities, but our model is resilient\nto this and delivers robust results at all intensities. We conduct automated\nevaluations and human studies to test the performance of our model and provide\na detailed comparison of the results with other models. In all evaluations, our\nmodel outperforms existing affective text generation models.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:24:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Singh", "Ishika", ""], ["Barkati", "Ahsan", ""], ["Goswamy", "Tushar", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2011.04006", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri,\n  Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:53:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Abnar", "Samira", ""], ["Shen", "Yikang", ""], ["Bahri", "Dara", ""], ["Pham", "Philip", ""], ["Rao", "Jinfeng", ""], ["Yang", "Liu", ""], ["Ruder", "Sebastian", ""], ["Metzler", "Donald", ""]]}, {"id": "2011.04010", "submitter": "Anand Natrajan", "authors": "Anand Natrajan, Mallige Anand", "title": "Scout Algorithm For Fast Substring Matching", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exact substring matching is a common task in many software applications.\nDespite the existence of several algorithms for finding whether or not a\npattern string is present in a target string, the most common implementation is\na na\\\"ive, brute force approach. Alternative approaches either do not provide\nenough of a benefit for the added complexity, or are impractical for modern\ncharacter sets, e.g., Unicode. We present a new algorithm, Scout, that is\nstraightforward, quick and appropriate for all applications. We also compare\nthe performance characteristics of the Scout algorithm with several others.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:09:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Natrajan", "Anand", ""], ["Anand", "Mallige", ""]]}, {"id": "2011.04016", "submitter": "Scott Friedman", "authors": "Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen,\n  Kyle Tunis", "title": "Provenance-Based Interpretation of Multi-Agent Information Analysis", "comments": "6 pages, 5 figures, appears in Proceedings of TaPP 2020", "journal-ref": "Proceedings of TaPP 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic software tools and workflows are increasing in capability,\ncomplexity, number, and scale, and the integrity of our workflows is as\nimportant as ever. Specifically, we must be able to inspect the process of\nanalytic workflows to assess (1) confidence of the conclusions, (2) risks and\nbiases of the operations involved, (3) sensitivity of the conclusions to\nsources and agents, (4) impact and pertinence of various sources and agents,\nand (5) diversity of the sources that support the conclusions. We present an\napproach that tracks agents' provenance with PROV-O in conjunction with agents'\nappraisals and evidence links (expressed in our novel DIVE ontology). Together,\nPROV-O and DIVE enable dynamic propagation of confidence and counter-factual\nrefutation to improve human-machine trust and analytic integrity. We\ndemonstrate representative software developed for user interaction with that\nprovenance, and discuss key needs for organizations adopting such approaches.\nWe demonstrate all of these assessments in a multi-agent analysis scenario,\nusing an interactive web-based information validation UI.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:43:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Friedman", "Scott", ""], ["Rye", "Jeff", ""], ["LaVergne", "David", ""], ["Thomsen", "Dan", ""], ["Allen", "Matthew", ""], ["Tunis", "Kyle", ""]]}, {"id": "2011.04021", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez,\n  Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar\n  Veli\\v{c}kovi\\'c, Th\\'eophane Weber", "title": "On the role of planning in model-based deep reinforcement learning", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based planning is often thought to be necessary for deep, careful\nreasoning and generalization in artificial agents. While recent successes of\nmodel-based reinforcement learning (MBRL) with deep function approximation have\nstrengthened this hypothesis, the resulting diversity of model-based methods\nhas also made it difficult to track which components drive success and why. In\nthis paper, we seek to disentangle the contributions of recent methods by\nfocusing on three questions: (1) How does planning benefit MBRL agents? (2)\nWithin planning, what choices drive performance? (3) To what extent does\nplanning improve generalization? To answer these questions, we study the\nperformance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL\nalgorithm with strong connections and overlapping components with many other\nMBRL algorithms. We perform a number of interventions and ablations of MuZero\nacross a wide range of environments, including control tasks, Atari, and 9x9\nGo. Our results suggest the following: (1) Planning is most useful in the\nlearning process, both for policy updates and for providing a more useful data\ndistribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as\nperformant as more complex methods, except in the most difficult reasoning\ntasks. (3) Planning alone is insufficient to drive strong generalization. These\nresults indicate where and how to utilize planning in reinforcement learning\nsettings, and highlight a number of open questions for future MBRL research.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:55:16 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 11:36:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Friesen", "Abram L.", ""], ["Behbahani", "Feryal", ""], ["Guez", "Arthur", ""], ["Viola", "Fabio", ""], ["Witherspoon", "Sims", ""], ["Anthony", "Thomas", ""], ["Buesing", "Lars", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Weber", "Th\u00e9ophane", ""]]}, {"id": "2011.04041", "submitter": "Aijun Zhang", "authors": "Agus Sudjianto, William Knauth, Rahul Singh, Zebin Yang, Aijun Zhang", "title": "Unwrapping The Black Box of Deep ReLU Networks: Interpretability,\n  Diagnostics, and Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep neural networks (DNNs) have achieved great success in learning\ncomplex patterns with strong predictive power, but they are often thought of as\n\"black box\" models without a sufficient level of transparency and\ninterpretability. It is important to demystify the DNNs with rigorous\nmathematics and practical tools, especially when they are used for\nmission-critical applications. This paper aims to unwrap the black box of deep\nReLU networks through local linear representation, which utilizes the\nactivation pattern and disentangles the complex network into an equivalent set\nof local linear models (LLMs). We develop a convenient LLM-based toolkit for\ninterpretability, diagnostics, and simplification of a pre-trained deep ReLU\nnetwork. We propose the local linear profile plot and other visualization\nmethods for interpretation and diagnostics, and an effective merging strategy\nfor network simplification. The proposed methods are demonstrated by simulation\nexamples, benchmark datasets, and a real case study in home lending credit risk\nassessment.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:09:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Sudjianto", "Agus", ""], ["Knauth", "William", ""], ["Singh", "Rahul", ""], ["Yang", "Zebin", ""], ["Zhang", "Aijun", ""]]}, {"id": "2011.04044", "submitter": "Yufei Feng", "authors": "Yufei Feng, Zi'ou Zheng, Quan Liu, Michael Greenspan, Xiaodan Zhu", "title": "Exploring End-to-End Differentiable Natural Logic Modeling", "comments": "10 pages", "journal-ref": "COLING 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore end-to-end trained differentiable models that integrate natural\nlogic with neural networks, aiming to keep the backbone of natural language\nreasoning based on the natural logic formalism while introducing subsymbolic\nvector representations and neural components. The proposed model adapts module\nnetworks to model natural logic operations, which is enhanced with a memory\ncomponent to model contextual information. Experiments show that the proposed\nframework can effectively model monotonicity-based reasoning, compared to the\nbaseline neural network models without built-in inductive bias for\nmonotonicity-based reasoning. Our proposed model shows to be robust when\ntransferred from upward to downward inference. We perform further analyses on\nthe performance of the proposed model on aggregation, showing the effectiveness\nof the proposed subcomponents on helping achieve better intermediate\naggregation performance.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:18:15 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Feng", "Yufei", ""], ["Zheng", "Zi'ou", ""], ["Liu", "Quan", ""], ["Greenspan", "Michael", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2011.04049", "submitter": "Cecilia Panigutti", "authors": "Cecilia Panigutti, Alan Perotti, Andr\\`e Panisson, Paolo Bajardi and\n  Dino Pedreschi", "title": "FairLens: Auditing Black-box Clinical Decision Support Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive application of algorithmic decision-making is raising concerns\non the risk of unintended bias in AI systems deployed in critical settings such\nas healthcare. The detection and mitigation of biased models is a very delicate\ntask which should be tackled with care and involving domain experts in the\nloop. In this paper we introduce FairLens, a methodology for discovering and\nexplaining biases. We show how our tool can be used to audit a fictional\ncommercial black-box model acting as a clinical decision support system. In\nthis scenario, the healthcare facility experts can use FairLens on their own\nhistorical data to discover the model's biases before incorporating it into the\nclinical decision flow. FairLens first stratifies the available patient data\naccording to attributes such as age, ethnicity, gender and insurance; it then\nassesses the model performance on such subgroups of patients identifying those\nin need of expert evaluation. Finally, building on recent state-of-the-art XAI\n(eXplainable Artificial Intelligence) techniques, FairLens explains which\nelements in patients' clinical history drive the model error in the selected\nsubgroup. Therefore, FairLens allows experts to investigate whether to trust\nthe model and to spotlight group-specific biases that might constitute\npotential fairness issues.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:40:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Panigutti", "Cecilia", ""], ["Perotti", "Alan", ""], ["Panisson", "Andr\u00e8", ""], ["Bajardi", "Paolo", ""], ["Pedreschi", "Dino", ""]]}, {"id": "2011.04052", "submitter": "Shreyas Labhsetwar", "authors": "Shreyas Rajesh Labhsetwar, Raj Sunil Salvi, Piyush Arvind Kolte,\n  Veerasai Subramaniam venkatesh, Alistair Michael Baretto", "title": "Predictive Analysis of Diabetic Retinopathy with Transfer Learning", "comments": "ICNTE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of Diabetes, the Diabetes Mellitus Retinopathy (DR) is\nbecoming a major health problem across the world. The long-term medical\ncomplications arising due to DR have a significant impact on the patient as\nwell as the society, as the disease mostly affects individuals in their most\nproductive years. Early detection and treatment can help reduce the extent of\ndamage to the patients. The rise of Convolutional Neural Networks for\npredictive analysis in the medical field paves the way for a robust solution to\nDR detection. This paper studies the performance of several highly efficient\nand scalable CNN architectures for Diabetic Retinopathy Classification with the\nhelp of Transfer Learning. The research focuses on VGG16, Resnet50 V2 and\nEfficientNet B0 models. The classification performance is analyzed using\nseveral performance metrics including True Positive Rate, False Positive Rate,\nAccuracy, etc. Also, several performance graphs are plotted for visualizing the\narchitecture performance including Confusion Matrix, ROC Curve, etc. The\nresults indicate that Transfer Learning with ImageNet weights using VGG 16\nmodel demonstrates the best classification performance with the best Accuracy\nof 95%. It is closely followed by ResNet50 V2 architecture with the best\nAccuracy of 93%. This paper shows that predictive analysis of DR from retinal\nimages is achieved with Transfer Learning on Convolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:54:57 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 05:40:46 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Labhsetwar", "Shreyas Rajesh", ""], ["Salvi", "Raj Sunil", ""], ["Kolte", "Piyush Arvind", ""], ["venkatesh", "Veerasai Subramaniam", ""], ["Baretto", "Alistair Michael", ""]]}, {"id": "2011.04064", "submitter": "Peri Akiva", "authors": "Peri Akiva and Benjamin Planche and Aditi Roy and Kristin Dana and\n  Peter Oudemans and Michael Mars", "title": "AI on the Bog: Monitoring and Evaluating Cranberry Crop Risk", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision for precision agriculture has attracted considerable research\ninterest in recent years. The goal of this paper is to develop an end-to-end\ncranberry health monitoring system to enable and support real time cranberry\nover-heating assessment to facilitate informed decisions that may sustain the\neconomic viability of the farm. Toward this goal, we propose two main deep\nlearning-based modules for: 1) cranberry fruit segmentation to delineate the\nexact fruit regions in the cranberry field image that are exposed to sun, 2)\nprediction of cloud coverage conditions and sun irradiance to estimate the\ninner temperature of exposed cranberries. We develop drone-based field data and\nground-based sky data collection systems to collect video imagery at multiple\ntime points for use in crop health analysis. Extensive evaluation on the data\nset shows that it is possible to predict exposed fruit's inner temperature with\nhigh accuracy (0.02% MAPE). The sun irradiance prediction error was found to be\n8.41-20.36% MAPE in the 5-20 minutes time horizon. With 62.54% mIoU for\nsegmentation and 13.46 MAE for counting accuracies in exposed fruit\nidentification, this system is capable of giving informed feedback to growers\nto take precautionary action (e.g. irrigation) in identified crop field regions\nwith higher risk of sunburn in the near future. Though this novel system is\napplied for cranberry health monitoring, it represents a pioneering step\nforward for efficient farming and is useful in precision agriculture beyond the\nproblem of cranberry overheating.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:03:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Akiva", "Peri", ""], ["Planche", "Benjamin", ""], ["Roy", "Aditi", ""], ["Dana", "Kristin", ""], ["Oudemans", "Peter", ""], ["Mars", "Michael", ""]]}, {"id": "2011.04079", "submitter": "Alexander Kell Mr", "authors": "Alexander J. M. Kell, Matthew Forshaw, A. Stephen McGough", "title": "Exploring market power using deep reinforcement learning for intelligent\n  bidding strategies", "comments": "Accepted at The 4th IEEE International Workshop on Big Data for\n  Financial News and Data at 2020 IEEE International Conference on Big Data\n  (IEEE BigData 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized electricity markets are often dominated by a small set of\ngenerator companies who control the majority of the capacity. In this paper, we\nexplore the effect of the total controlled electricity capacity by a single, or\ngroup, of generator companies can have on the average electricity price. We\ndemonstrate this through the use of ElecSim, a simulation of a country-wide\nenergy market. We develop a strategic agent, representing a generation company,\nwhich uses a deep deterministic policy gradient reinforcement learning\nalgorithm to bid in a uniform pricing electricity market. A uniform pricing\nmarket is one where all players are paid the highest accepted price. ElecSim is\nparameterized to the United Kingdom for the year 2018. This work can help\ninform policy on how to best regulate a market to ensure that the price of\nelectricity remains competitive.\n  We find that capacity has an impact on the average electricity price in a\nsingle year. If any single generator company, or a collaborating group of\ngenerator companies, control more than ${\\sim}$11$\\%$ of generation capacity\nand bid strategically, prices begin to increase by ${\\sim}$25$\\%$. The value of\n${\\sim}$25\\% and ${\\sim}$11\\% may vary between market structures and countries.\nFor instance, different load profiles may favour a particular type of generator\nor a different distribution of generation capacity. Once the capacity\ncontrolled by a generator company, which bids strategically, is higher than\n${\\sim}$35\\%, prices increase exponentially. We observe that the use of a\nmarket cap of approximately double the average market price has the effect of\nsignificantly decreasing this effect and maintaining a competitive market. A\nfair and competitive electricity market provides value to consumers and enables\na more competitive economy through the utilisation of electricity by both\nindustry and consumers.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 21:07:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kell", "Alexander J. M.", ""], ["Forshaw", "Matthew", ""], ["McGough", "A. Stephen", ""]]}, {"id": "2011.04085", "submitter": "Henrique Santos", "authors": "H. Santos, A. Mulvehill, J. S. Erickson, J. P. McCusker, M. Gordon, O.\n  Xie, S. Stouffer, G. Capraro, A. Pidwerbetsky, J. Burgess, A. Berlinsky, K.\n  Turck, J. Ashdown, D. L. McGuinness", "title": "A Semantic Framework for Enabling Radio Spectrum Policy Management and\n  Evaluation", "comments": null, "journal-ref": "The Semantic Web - ISWC 2020. ISWC 2020. Lecture Notes in Computer\n  Science, vol 12507", "doi": "10.1007/978-3-030-62466-8_30", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because radio spectrum is a finite resource, its usage and sharing is\nregulated by government agencies. These agencies define policies to manage\nspectrum allocation and assignment across multiple organizations, systems, and\ndevices. With more portions of the radio spectrum being licensed for commercial\nuse, the importance of providing an increased level of automation when\nevaluating such policies becomes crucial for the efficiency and efficacy of\nspectrum management. We introduce our Dynamic Spectrum Access Policy Framework\nfor supporting the United States government's mission to enable both federal\nand non-federal entities to compatibly utilize available spectrum. The DSA\nPolicy Framework acts as a machine-readable policy repository providing policy\nmanagement features and spectrum access request evaluation. The framework\nutilizes a novel policy representation using OWL and PROV-O along with a\ndomain-specific reasoning implementation that mixes GeoSPARQL, OWL reasoning,\nand knowledge graph traversal to evaluate incoming spectrum access requests and\nexplain how applicable policies were used. The framework is currently being\nused to support live, over-the-air field exercises involving a diverse set of\nfederal and commercial radios, as a component of a prototype spectrum\nmanagement system.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 21:29:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Santos", "H.", ""], ["Mulvehill", "A.", ""], ["Erickson", "J. S.", ""], ["McCusker", "J. P.", ""], ["Gordon", "M.", ""], ["Xie", "O.", ""], ["Stouffer", "S.", ""], ["Capraro", "G.", ""], ["Pidwerbetsky", "A.", ""], ["Burgess", "J.", ""], ["Berlinsky", "A.", ""], ["Turck", "K.", ""], ["Ashdown", "J.", ""], ["McGuinness", "D. L.", ""]]}, {"id": "2011.04096", "submitter": "Manik Bhandari", "authors": "Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu", "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization\n  Evaluation Metrics", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In text summarization, evaluating the efficacy of automatic metrics without\nhuman judgments has become recently popular. One exemplar work concludes that\nautomatic metrics strongly disagree when ranking high-scoring summaries. In\nthis paper, we revisit their experiments and find that their observations stem\nfrom the fact that metrics disagree in ranking summaries from any narrow\nscoring range. We hypothesize that this may be because summaries are similar to\neach other in a narrow scoring range and are thus, difficult to rank. Apart\nfrom the width of the scoring range of summaries, we analyze three other\nproperties that impact inter-metric agreement - Ease of Summarization,\nAbstractiveness, and Coverage. To encourage reproducible research, we make all\nour analysis code and data publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 22:26:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bhandari", "Manik", ""], ["Gour", "Pranav", ""], ["Ashfaq", "Atabak", ""], ["Liu", "Pengfei", ""]]}, {"id": "2011.04105", "submitter": "Puneet Kumar", "authors": "Puneet Kumar", "title": "Evolution of Artificial Intelligent Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of the internet, it is becoming hard to manage, configure and\nmonitor networks. Recent trends to control and operate them is artificial\nintelligence based automation to minimize human intervention. Albeit this\nconcept has been introduced since a decade with several different names, but\nthe underlying goal remains the same, which is to make network intelligent\nenough to assemble, reassemble if configuration changes, and detect a problem\non its own and fix it. As a result, in addition to Data Plane, Control Plane\nand Management Plane, a new plane called Artificial Intelligence (AI) Plane is\nintroduced. Our main objective is to analyze all major AI plane techniques,\nframeworks and algorithms proposed in various types of networks. We propose a\ncomprehensive and network independent framework to cover all aspects of AI\nplane, in particular we provide a systematically means of comparison. In\nconjunction to make AI plane understand simpler, this framework highlights\nrelevant challenges and design considerations for future research. To the best\nof our knowledge this is the first survey report which represents a complete\ncomparison of AI planes with their investigation issues in several types of\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 23:33:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kumar", "Puneet", ""]]}, {"id": "2011.04121", "submitter": "Tareq Tayeh", "authors": "Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, and Abdallah Shami", "title": "Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet\n  Networks", "comments": "6 pages, 8 figures, 2020 IEEE 11th Annual Information Technology,\n  Electronics and Mobile Communication Conference (Best Paper Award in the\n  category of Image Processing and Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface anomaly detection plays an important quality control role in many\nmanufacturing industries to reduce scrap production. Machine-based visual\ninspections have been utilized in recent years to conduct this task instead of\nhuman experts. In particular, deep learning Convolutional Neural Networks\n(CNNs) have been at the forefront of these image processing-based solutions due\nto their predictive accuracy and efficiency. Training a CNN on a classification\nobjective requires a sufficiently large amount of defective data, which is\noften not available. In this paper, we address that challenge by training the\nCNN on surface texture patches with a distance-based anomaly detection\nobjective instead. A deep residual-based triplet network model is utilized, and\ndefective training samples are synthesized exclusively from non-defective\nsamples via random erasing techniques to directly learn a similarity metric\nbetween the same-class samples and out-of-class samples. Evaluation results\ndemonstrate the approach's strength in detecting different types of anomalies,\nsuch as bent, broken, or cracked surfaces, for known surfaces that are part of\nthe training data and unseen novel surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:35:21 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 04:20:49 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tayeh", "Tareq", ""], ["Aburakhia", "Sulaiman", ""], ["Myers", "Ryan", ""], ["Shami", "Abdallah", ""]]}, {"id": "2011.04166", "submitter": "Pengcheng Zou", "authors": "Zhao Li, Donghui Ding, Pengcheng Zou, Yu Gong, Xi Chen, Ji Zhang,\n  Jianliang Gao, Youxi Wu and Yucong Duan", "title": "Distant Supervision for E-commerce Query Segmentation via Attention\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The booming online e-commerce platforms demand highly accurate approaches to\nsegment queries that carry the product requirements of consumers. Recent works\nhave shown that the supervised methods, especially those based on deep\nlearning, are attractive for achieving better performance on the problem of\nquery segmentation. However, the lack of labeled data is still a big challenge\nfor training a deep segmentation network, and the problem of Out-of-Vocabulary\n(OOV) also adversely impacts the performance of query segmentation. Different\nfrom query segmentation task in an open domain, e-commerce scenario can provide\nexternal documents that are closely related to these queries. Thus, to deal\nwith the two challenges, we employ the idea of distant supervision and design a\nnovel method to find contexts in external documents and extract features from\nthese contexts. In this work, we propose a BiLSTM-CRF based model with an\nattention module to encode external features, such that external contexts\ninformation, which can be utilized naturally and effectively to help query\nsegmentation. Experiments on two datasets show the effectiveness of our\napproach compared with several kinds of baselines.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:00:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Zhao", ""], ["Ding", "Donghui", ""], ["Zou", "Pengcheng", ""], ["Gong", "Yu", ""], ["Chen", "Xi", ""], ["Zhang", "Ji", ""], ["Gao", "Jianliang", ""], ["Wu", "Youxi", ""], ["Duan", "Yucong", ""]]}, {"id": "2011.04176", "submitter": "Xingyu Wu", "authors": "Xingyu Wu, Bingbing Jiang, Yan Zhong, Huanhuan Chen", "title": "Multi-label Causal Variable Discovery: Learning Common Causal Variables\n  and Label-specific Causal Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal variables in Markov boundary (MB) have been widely applied in\nextensive single-label tasks. While few researches focus on the causal variable\ndiscovery in multi-label data due to the complex causal relationships. Since\nsome variables in multi-label scenario might contain causal information about\nmultiple labels, this paper investigates the problem of multi-label causal\nvariable discovery as well as the distinguishing between common causal\nvariables shared by multiple labels and label-specific causal variables\nassociated with some single labels. Considering the multiple MBs under the\nnon-positive joint probability distribution, we explore the relationships\nbetween common causal variables and equivalent information phenomenon, and find\nthat the solutions are influenced by equivalent information following different\nmechanisms with or without existence of label causality. Analyzing these\nmechanisms, we provide the theoretical property of common causal variables,\nbased on which the discovery and distinguishing algorithm is designed to\nidentify these two types of variables. Similar to single-label problem, causal\nvariables for multiple labels also have extensive application prospects. To\ndemonstrate this, we apply the proposed causal mechanism to multi-label feature\nselection and present an interpretable algorithm, which is proved to achieve\nthe minimal redundancy and the maximum relevance. Extensive experiments\ndemonstrate the efficacy of these contributions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:01:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wu", "Xingyu", ""], ["Jiang", "Bingbing", ""], ["Zhong", "Yan", ""], ["Chen", "Huanhuan", ""]]}, {"id": "2011.04184", "submitter": "Takumi Aoki", "authors": "Takumi Aoki and Shunsuke Kitada and Hitoshi Iyatomi", "title": "Text Classification through Glyph-aware Disentangled Character Embedding\n  and Semantic Sub-character Augmentation", "comments": "6 pages, 3 figures, Accepted at AACL-IJCNLP 2020: Student Research\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new character-based text classification framework for\nnon-alphabetic languages, such as Chinese and Japanese. Our framework consists\nof a variational character encoder (VCE) and character-level text classifier.\nThe VCE is composed of a $\\beta$-variational auto-encoder ($\\beta$-VAE) that\nlearns the proposed glyph-aware disentangled character embedding (GDCE). Since\nour GDCE provides zero-mean unit-variance character embeddings that are\ndimensionally independent, it is applicable for our interpretable data\naugmentation, namely, semantic sub-character augmentation (SSA). In this paper,\nwe evaluated our framework using Japanese text classification tasks at the\ndocument- and sentence-level. We confirmed that our GDCE and SSA not only\nprovided embedding interpretability but also improved the classification\nperformance. Our proposal achieved a competitive result to the state-of-the-art\nmodel while also providing model interpretability. Our code is available on\nhttps://github.com/IyatomiLab/GDCE-SSA\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:38:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Aoki", "Takumi", ""], ["Kitada", "Shunsuke", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2011.04194", "submitter": "Yoon-Yeong Kim", "authors": "Yoon-Yeong Kim, Kyungwoo Song, JoonHo Jang, Il-Chul Moon", "title": "LADA: Look-Ahead Data Acquisition via Augmentation for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning effectively collects data instances for training deep\nlearning models when the labeled dataset is limited and the annotation cost is\nhigh. Besides active learning, data augmentation is also an effective technique\nto enlarge the limited amount of labeled instances. However, the potential gain\nfrom virtual instances generated by data augmentation has not been considered\nin the acquisition process of active learning yet. Looking ahead the effect of\ndata augmentation in the process of acquisition would select and generate the\ndata instances that are informative for training the model. Hence, this paper\nproposes Look-Ahead Data Acquisition via augmentation, or LADA, to integrate\ndata acquisition and data augmentation. LADA considers both 1) unlabeled data\ninstance to be selected and 2) virtual data instance to be generated by data\naugmentation, in advance of the acquisition process. Moreover, to enhance the\ninformativeness of the virtual data instances, LADA optimizes the data\naugmentation policy to maximize the predictive acquisition score, resulting in\nthe proposal of InfoMixup and InfoSTN. As LADA is a generalizable framework, we\nexperiment with the various combinations of acquisition and augmentation\nmethods. The performance of LADA shows a significant improvement over the\nrecent augmentation and acquisition baselines which were independently applied\nto the benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 05:21:14 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:32:39 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:41:16 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kim", "Yoon-Yeong", ""], ["Song", "Kyungwoo", ""], ["Jang", "JoonHo", ""], ["Moon", "Il-Chul", ""]]}, {"id": "2011.04216", "submitter": "Amit Sharma", "authors": "Amit Sharma, Emre Kiciman", "title": "DoWhy: An End-to-End Library for Causal Inference", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.MS econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to efficient statistical estimators of a treatment's effect,\nsuccessful application of causal inference requires specifying assumptions\nabout the mechanisms underlying observed data and testing whether they are\nvalid, and to what extent. However, most libraries for causal inference focus\nonly on the task of providing powerful statistical estimators. We describe\nDoWhy, an open-source Python library that is built with causal assumptions as\nits first-class citizens, based on the formal framework of causal graphs to\nspecify and test causal assumptions. DoWhy presents an API for the four steps\ncommon to any causal analysis---1) modeling the data using a causal graph and\nstructural assumptions, 2) identifying whether the desired effect is estimable\nunder the causal model, 3) estimating the effect using statistical estimators,\nand finally 4) refuting the obtained estimate through robustness checks and\nsensitivity analyses. In particular, DoWhy implements a number of robustness\nchecks including placebo tests, bootstrap tests, and tests for unoberved\nconfounding. DoWhy is an extensible library that supports interoperability with\nother implementations, such as EconML and CausalML for the the estimation step.\nThe library is available at https://github.com/microsoft/dowhy\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:22:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Sharma", "Amit", ""], ["Kiciman", "Emre", ""]]}, {"id": "2011.04218", "submitter": "Fengli Xu", "authors": "Fengli Xu, Quanming Yao, Pan Hui, Yong Li", "title": "Graph Neural Network with Automorphic Equivalence Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural network (GNN) has recently been established as an effective\nrepresentation learning framework on graph data. However, the popular message\npassing models rely on local permutation invariant aggregate functions, which\ngives rise to the concerns about their representational power. Here, we\nintroduce the concept of automorphic equivalence to theoretically analyze GNN's\nexpressiveness in differentiating node's structural role. We show that the\nexisting message passing GNNs have limitations in learning expressive\nrepresentations. Moreover, we design a novel GNN class that leverages learnable\nautomorphic equivalence filters to explicitly differentiate the structural\nroles of each node's neighbors, and uses a squeeze-and-excitation module to\nfuse various structural information. We theoretically prove that the proposed\nmodel is expressive in terms of generating distinct representations for nodes\nwith different structural feature. Besides, we empirically validate our model\non eight real-world graph data, including social network, e-commerce\nco-purchase network and citation network, and show that it consistently\noutperforms strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:36:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Xu", "Fengli", ""], ["Yao", "Quanming", ""], ["Hui", "Pan", ""], ["Li", "Yong", ""]]}, {"id": "2011.04222", "submitter": "Sushmita Bhattacharya", "authors": "Sushmita Bhattacharya, Siva Kailas, Sahil Badyal, Stephanie Gil,\n  Dimitri Bertsekas", "title": "Multiagent Rollout and Policy Iteration for POMDP with Application to\n  Multi-Robot Repair Problems", "comments": "8 pages + 3 pages appendix + 9 figures + 3 tables, accepted in\n  Conference on Robot Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider infinite horizon discounted dynamic programming\nproblems with finite state and control spaces, partial state observations, and\na multiagent structure. We discuss and compare algorithms that simultaneously\nor sequentially optimize the agents' controls by using multistep lookahead,\ntruncated rollout with a known base policy, and a terminal cost function\napproximation. Our methods specifically address the computational challenges of\npartially observable multiagent problems. In particular: 1) We consider rollout\nalgorithms that dramatically reduce required computation while preserving the\nkey cost improvement property of the standard rollout method. The per-step\ncomputational requirements for our methods are on the order of $O(Cm)$ as\ncompared with $O(C^m)$ for standard rollout, where $C$ is the maximum\ncardinality of the constraint set for the control component of each agent, and\n$m$ is the number of agents. 2) We show that our methods can be applied to\nchallenging problems with a graph structure, including a class of robot repair\nproblems whereby multiple robots collaboratively inspect and repair a system\nunder partial information. 3) We provide a simulation study that compares our\nmethods with existing methods, and demonstrate that our methods can handle\nlarger and more complex partially observable multiagent problems (state space\nsize $10^{37}$ and control space size $10^{7}$, respectively). Finally, we\nincorporate our multiagent rollout algorithms as building blocks in an\napproximate policy iteration scheme, where successive rollout policies are\napproximated by using neural network classifiers. While this scheme requires a\nstrictly off-line implementation, it works well in our computational\nexperiments and produces additional significant performance improvement over\nthe single online rollout iteration method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:51:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bhattacharya", "Sushmita", ""], ["Kailas", "Siva", ""], ["Badyal", "Sahil", ""], ["Gil", "Stephanie", ""], ["Bertsekas", "Dimitri", ""]]}, {"id": "2011.04233", "submitter": "Ruijin Liu", "authors": "Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong", "title": "End-to-end Lane Shape Prediction with Transformers", "comments": "9 pages, 7 figures, accepted by WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection, the process of identifying lane markings as approximated\ncurves, is widely used for lane departure warning and adaptive cruise control\nin autonomous vehicles. The popular pipeline that solves it in two steps --\nfeature extraction plus post-processing, while useful, is too inefficient and\nflawed in learning the global context and lanes' long and thin structures. To\ntackle these issues, we propose an end-to-end method that directly outputs\nparameters of a lane shape model, using a network built with a transformer to\nlearn richer structures and context. The lane shape model is formulated based\non road structures and camera pose, providing physical interpretation for\nparameters of network output. The transformer models non-local interactions\nwith a self-attention mechanism to capture slender structures and global\ncontext. The proposed method is validated on the TuSimple benchmark and shows\nstate-of-the-art accuracy with the most lightweight model size and fastest\nspeed. Additionally, our method shows excellent adaptability to a challenging\nself-collected lane detection dataset, showing its powerful deployment\npotential in real applications. Codes are available at\nhttps://github.com/liuruijin17/LSTR.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 07:42:55 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 10:55:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Ruijin", ""], ["Yuan", "Zejian", ""], ["Liu", "Tie", ""], ["Xiong", "Zhiliang", ""]]}, {"id": "2011.04242", "submitter": "Ben Burtenshaw", "authors": "Ben Burtenshaw", "title": "AI Stories: An Interactive Narrative System for Children", "comments": "Originally submitted to the ICCC 2017 Doctoral Consortium\n  [https://computationalcreativity.net/iccc2017/doctoralconsortium/]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI Stories is a proposed interactive dialogue system, that lets children\nco-create narrative worlds through conversation. Over the next three years this\nsystem will be developed and tested within pediatric wards, where it offers a\nuseful resource between the gap of education and play. Telling and making\nstories is a fundamental part of language play, and its chatty and nonsensical\nqualities are important; therefore, the prologued usage an automated system\noffers is a benefit to children. In this paper I will present the current state\nof this project, in its more experimental and general guise. Conceptually\nstory-telling through dialogue relates to the preprint interpretation of story,\nbeyond the static and linear medium, where stories were performative, temporal,\nand social.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 08:17:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Burtenshaw", "Ben", ""]]}, {"id": "2011.04244", "submitter": "Zicong Jiang", "authors": "Zicong Jiang, Liquan Zhao, Shuaiyang Li, Yanfei Jia", "title": "Real-time object detection method based on improved YOLOv4-tiny", "comments": "14pages,7figures,2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"You only look once v4\"(YOLOv4) is one type of object detection methods\nin deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network\nstructure and reduce parameters, which makes it be suitable for developing on\nthe mobile and embedded devices. To improve the real-time of object detection,\na fast object detection method is proposed based on YOLOv4-tiny. It firstly\nuses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules\nin Yolov4-tiny, which reduces the computation complexity. Secondly, it designs\nan auxiliary residual network block to extract more feature information of\nobject to reduce detection error. In the design of auxiliary network, two\nconsecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract\nglobal features, and channel attention and spatial attention are also used to\nextract more effective information. In the end, it merges the auxiliary network\nand backbone network to construct the whole network structure of improved\nYOLOv4-tiny. Simulation results show that the proposed method has faster object\ndetection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of\naverage precision as the YOLOv4-tiny. It is more suitable for real-time object\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 08:26:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 09:19:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Jiang", "Zicong", ""], ["Zhao", "Liquan", ""], ["Li", "Shuaiyang", ""], ["Jia", "Yanfei", ""]]}, {"id": "2011.04282", "submitter": "Mete Tuluhan Akbulut", "authors": "M.Tuluhan Akbulut, Utku Bozdogan, Ahmet Tekden and Emre Ugur", "title": "Reward Conditioned Neural Movement Primitives for Population Based\n  Variational Policy Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to study the reward based policy exploration problem\nin a supervised learning approach and enable robots to form complex movement\ntrajectories in challenging reward settings and search spaces. For this, the\nexperience of the robot, which can be bootstrapped from demonstrated\ntrajectories, is used to train a novel Neural Processes-based deep network that\nsamples from its latent space and generates the required trajectories given\ndesired rewards. Our framework can generate progressively improved trajectories\nby sampling them from high reward landscapes, increasing the reward gradually.\nVariational inference is used to create a stochastic latent space to sample\nvarying trajectories in generating population of trajectories given target\nrewards. We benefit from Evolutionary Strategies and propose a novel crossover\noperation, which is applied in the self-organized latent space of the\nindividual policies, allowing blending of the individuals that might address\ndifferent factors in the reward function. Using a number of tasks that require\nsequential reaching to multiple points or passing through gaps between objects,\nwe showed that our method provides stable learning progress and significant\nsample efficiency compared to a number of state-of-the-art robotic\nreinforcement learning methods. Finally, we show the real-world suitability of\nour method through real robot execution involving obstacle avoidance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:53:37 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Akbulut", "M. Tuluhan", ""], ["Bozdogan", "Utku", ""], ["Tekden", "Ahmet", ""], ["Ugur", "Emre", ""]]}, {"id": "2011.04328", "submitter": "Paul Schwerdtner", "authors": "Paul Schwerdtner, Florens Gre{\\ss}ner, Nikhil Kapoor, Felix Assion,\n  Ren\\'e Sass, Wiebke G\\\"unther, Fabian H\\\"uger, and Peter Schlicht", "title": "Risk Assessment for Machine Learning Models", "comments": "8 pages, 5 figures, conference workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for assessing the risk associated with\ndeploying a machine learning model in a specified environment. For that we\ncarry over the risk definition from decision theory to machine learning. We\ndevelop and implement a method that allows to define deployment scenarios, test\nthe machine learning model under the conditions specified in each scenario, and\nestimate the damage associated with the output of the machine learning model\nunder test. Using the likelihood of each scenario together with the estimated\ndamage we define \\emph{key risk indicators} of a machine learning model.\n  The definition of scenarios and weighting by their likelihood allows for\nstandardized risk assessment in machine learning throughout multiple domains of\napplication. In particular, in our framework, the robustness of a machine\nlearning model to random input corruptions, distributional shifts caused by a\nchanging environment, and adversarial perturbations can be assessed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:50:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Schwerdtner", "Paul", ""], ["Gre\u00dfner", "Florens", ""], ["Kapoor", "Nikhil", ""], ["Assion", "Felix", ""], ["Sass", "Ren\u00e9", ""], ["G\u00fcnther", "Wiebke", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""]]}, {"id": "2011.04333", "submitter": "Nathan Grinsztajn", "authors": "Nathan Grinsztajn, Olivier Beaumont, Emmanuel Jeannot, Philippe Preux", "title": "Geometric Deep Reinforcement Learning for Dynamic DAG Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, it is quite common to face combinatorial optimization problems\nwhich contain uncertainty along with non-determinism and dynamicity. These\nthree properties call for appropriate algorithms; reinforcement learning (RL)\nis dealing with them in a very natural way. Today, despite some efforts, most\nreal-life combinatorial optimization problems remain out of the reach of\nreinforcement learning algorithms.\n  In this paper, we propose a reinforcement learning approach to solve a\nrealistic scheduling problem, and apply it to an algorithm commonly executed in\nthe high performance computing community, the Cholesky factorization. On the\ncontrary to static scheduling, where tasks are assigned to processors in a\npredetermined ordering before the beginning of the parallel execution, our\nmethod is dynamic: task allocations and their execution ordering are decided at\nruntime, based on the system state and unexpected events, which allows much\nmore flexibility. To do so, our algorithm uses graph neural networks in\ncombination with an actor-critic algorithm (A2C) to build an adaptive\nrepresentation of the problem on the fly.\n  We show that this approach is competitive with state-of-the-art heuristics\nused in high-performance computing runtime systems. Moreover, our algorithm\ndoes not require an explicit model of the environment, but we demonstrate that\nextra knowledge can easily be incorporated and improves performance. We also\nexhibit key properties provided by this RL approach, and study its transfer\nabilities to other instances.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:57:21 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Grinsztajn", "Nathan", ""], ["Beaumont", "Olivier", ""], ["Jeannot", "Emmanuel", ""], ["Preux", "Philippe", ""]]}, {"id": "2011.04349", "submitter": "Hieu Phung", "authors": "Hieu Trong Phung (1 and 2), Anh Tuan Vu (1), Tung Dinh Nguyen (1), Lam\n  Thanh Do (1 and 2), Giang Nam Ngo (1), Trung Thanh Tran (1) and Ngoc C. L\\^e\n  (1 and 2) ((1) PIXTA Vietnam, Hanoi, Vietnam. (2) Hanoi University of Science\n  and Technology, Ha Noi, Viet Nam.)", "title": "MAGNeto: An Efficient Deep Learning Method for the Extractive Tags\n  Summarization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study a new image annotation task named Extractive Tags\nSummarization (ETS). The goal is to extract important tags from the context\nlying in an image and its corresponding tags. We adjust some state-of-the-art\ndeep learning models to utilize both visual and textual information. Our\nproposed solution consists of different widely used blocks like convolutional\nand self-attention layers, together with a novel idea of combining auxiliary\nloss functions and the gating mechanism to glue and elevate these fundamental\ncomponents and form a unified architecture. Besides, we introduce a loss\nfunction that aims to reduce the imbalance of the training data and a simple\nbut effective data augmentation technique dedicated to alleviates the effect of\noutliers on the final results. Last but not least, we explore an unsupervised\npre-training strategy to further boost the performance of the model by making\nuse of the abundant amount of available unlabeled data. Our model shows the\ngood results as 90% $F_\\text{1}$ score on the public NUS-WIDE benchmark, and\n50% $F_\\text{1}$ score on a noisy large-scale real-world private dataset.\nSource code for reproducing the experiments is publicly available at:\nhttps://github.com/pixta-dev/labteam\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:34:21 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Phung", "Hieu Trong", "", "1 and 2"], ["Vu", "Anh Tuan", "", "1 and 2"], ["Nguyen", "Tung Dinh", "", "1 and 2"], ["Do", "Lam Thanh", "", "1 and 2"], ["Ngo", "Giang Nam", "", "1 and 2"], ["Tran", "Trung Thanh", "", "1 and 2"], ["L\u00ea", "Ngoc C.", "", "1 and 2"]]}, {"id": "2011.04378", "submitter": "Marta Arias", "authors": "Christian Lezcano, Marta Arias", "title": "Characterizing Transactional Databases for Frequent Itemset Mining", "comments": "Workshop on Evaluation and Experimental Design in Data Mining and\n  Machine Learning (EDML@SDM 2019), May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study of the characteristics of transactional databases\nused in frequent itemset mining. Such characterizations have typically been\nused to benchmark and understand the data mining algorithms working on these\ndatabases. The aim of our study is to give a picture of how diverse and\nrepresentative these benchmarking databases are, both in general but also in\nthe context of particular empirical studies found in the literature. Our\nproposed list of metrics contains many of the existing metrics found in the\nliterature, as well as new ones. Our study shows that our list of metrics is\nable to capture much of the datasets' inner complexity and thus provides a good\nbasis for the characterization of transactional datasets. Finally, we provide a\nset of representative datasets based on our characterization that may be used\nas a benchmark safely.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:26:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lezcano", "Christian", ""], ["Arias", "Marta", ""]]}, {"id": "2011.04395", "submitter": "Hao Wang", "authors": "Hao Wang, Bing Ruan", "title": "MatRec: Matrix Factorization for Highly Skewed Dataset", "comments": null, "journal-ref": null, "doi": "10.1145/3422713.3422735", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems is one of the most successful AI technologies applied in\nthe internet cooperations. Popular internet products such as TikTok, Amazon,\nand YouTube have all integrated recommender systems as their core product\nfeature. Although recommender systems have received great success, it is well\nknown for highly skewed datasets, engineers and researchers need to adjust\ntheir methods to tackle the specific problem to yield good results. Inability\nto deal with highly skewed dataset usually generates hard computational\nproblems for big data clusters and unsatisfactory results for customers. In\nthis paper, we propose a new algorithm solving the problem in the framework of\nmatrix factorization. We model the data skewness factors in the theoretic\nmodeling of the approach with easy to interpret and easy to implement formulas.\nWe prove in experiments our method generates comparably favorite results with\npopular recommender system algorithms such as Learning to Rank , Alternating\nLeast Squares and Deep Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:55:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Hao", ""], ["Ruan", "Bing", ""]]}, {"id": "2011.04405", "submitter": "Kushagra Chandak", "authors": "Jiajing Ling, Kushagra Chandak, Akshat Kumar", "title": "Combining Propositional Logic Based Decision Diagrams with Decision\n  Making in Urban Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving multiagent problems can be an uphill task due to uncertainty in the\nenvironment, partial observability, and scalability of the problem at hand.\nEspecially in an urban setting, there are more challenges since we also need to\nmaintain safety for all users while minimizing congestion of the agents as well\nas their travel times. To this end, we tackle the problem of multiagent\npathfinding under uncertainty and partial observability where the agents are\ntasked to move from their starting points to ending points while also\nsatisfying some constraints, e.g., low congestion, and model it as a multiagent\nreinforcement learning problem. We compile the domain constraints using\npropositional logic and integrate them with the RL algorithms to enable fast\nsimulation for RL.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 13:13:54 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 05:46:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ling", "Jiajing", ""], ["Chandak", "Kushagra", ""], ["Kumar", "Akshat", ""]]}, {"id": "2011.04419", "submitter": "Vikas Verma", "authors": "Vikas Verma, Minh-Thang Luong, Kenji Kawaguchi, Hieu Pham, Quoc V. Le", "title": "Towards Domain-Agnostic Contrastive Learning", "comments": "Published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent success, most contrastive self-supervised learning methods are\ndomain-specific, relying heavily on data augmentation techniques that require\nknowledge about a particular domain, such as image cropping and rotation. To\novercome such limitation, we propose a novel domain-agnostic approach to\ncontrastive learning, named DACL, that is applicable to domains where\ninvariances, and thus, data augmentation techniques, are not readily available.\nKey to our approach is the use of Mixup noise to create similar and dissimilar\nexamples by mixing data samples differently either at the input or hidden-state\nlevels. To demonstrate the effectiveness of DACL, we conduct experiments across\nvarious domains such as tabular data, images, and graphs. Our results show that\nDACL not only outperforms other domain-agnostic noising methods, such as\nGaussian-noise, but also combines well with domain-specific methods, such as\nSimCLR, to improve self-supervised visual representation learning. Finally, we\ntheoretically analyze our method and show advantages over the Gaussian-noise\nbased contrastive learning approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 13:41:56 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 20:59:14 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Verma", "Vikas", ""], ["Luong", "Minh-Thang", ""], ["Kawaguchi", "Kenji", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""]]}, {"id": "2011.04422", "submitter": "Alvin Lim", "authors": "Jiefeng Xu and Evren Gul and Alvin Lim", "title": "Maximizing Store Revenues using Tabu Search for Floor Space Optimization", "comments": null, "journal-ref": "International Journal of Revenue Management (IJRM), Vol. 12, No.\n  1/2, 2021", "doi": "10.1504/IJRM.2021.114969", "report-no": null, "categories": "cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floor space optimization is a critical revenue management problem commonly\nencountered by retailers. It maximizes store revenue by optimally allocating\nfloor space to product categories which are assigned to their most appropriate\nplanograms. We formulate the problem as a connected multi-choice knapsack\nproblem with an additional global constraint and propose a tabu search based\nmeta-heuristic that exploits the multiple special neighborhood structures. We\nalso incorporate a mechanism to determine how to combine the multiple\nneighborhood moves. A candidate list strategy based on learning from prior\nsearch history is also employed to improve the search quality. The results of\ncomputational testing with a set of test problems show that our tabu search\nheuristic can solve all problems within a reasonable amount of time. Analyses\nof individual contributions of relevant components of the algorithm were\nconducted with computational experiments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:42:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Xu", "Jiefeng", ""], ["Gul", "Evren", ""], ["Lim", "Alvin", ""]]}, {"id": "2011.04424", "submitter": "Matthew Praeger", "authors": "Matthew Praeger, Yunhui Xie, James A. Grant-Jacob, Robert W. Eason and\n  Ben Mills", "title": "Playing optical tweezers with deep reinforcement learning: in virtual,\n  physical and augmented environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning was carried out in a simulated environment to learn\ncontinuous velocity control over multiple motor axes. This was then applied to\na real-world optical tweezers experiment with the objective of moving a\nlaser-trapped microsphere to a target location whilst avoiding collisions with\nother free-moving microspheres. The concept of training a neural network in a\nvirtual environment has significant potential in the application of machine\nlearning for experimental optimization and control, as the neural network can\ndiscover optimal methods for problem solving without the risk of damage to\nequipment, and at a speed not limited by movement in the physical environment.\nAs the neural network treats both virtual and physical environments\nequivalently, we show that the network can also be applied to an augmented\nenvironment, where a virtual environment is combined with the physical\nenvironment. This technique may have the potential to unlock capabilities\nassociated with mixed and augmented reality, such as enforcing safety limits\nfor machine motion or as a method of inputting observations from additional\nsensors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 13:49:55 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 13:07:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Praeger", "Matthew", ""], ["Xie", "Yunhui", ""], ["Grant-Jacob", "James A.", ""], ["Eason", "Robert W.", ""], ["Mills", "Ben", ""]]}, {"id": "2011.04428", "submitter": "Sofia Maria Nikolakaki", "authors": "Sofia Maria Nikolakaki, Mingxiang Cai, Evimaria Terzi", "title": "Finding teams that balance expert load and task coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of online labor markets (e.g., Freelancer, Guru and Upwork) has\nignited a lot of research on team formation, where experts acquiring different\nskills form teams to complete tasks. The core idea in this line of work has\nbeen the strict requirement that the team of experts assigned to complete a\ngiven task should contain a superset of the skills required by the task.\nHowever, in many applications the required skills are often a wishlist of the\nentity that posts the task and not all of the skills are absolutely necessary.\nThus, in our setting we relax the complete coverage requirement and we allow\nfor tasks to be partially covered by the formed teams, assuming that the\nquality of task completion is proportional to the fraction of covered skills\nper task. At the same time, we assume that when multiple tasks need to be\nperformed, the less the load of an expert the better the performance. We\ncombine these two high-level objectives into one and define the BalancedTA\nproblem. We also consider a generalization of this problem where each task\nconsists of required and optional skills. In this setting, our objective is the\nsame under the constraint that all required skills should be covered. From the\ntechnical point of view, we show that the BalancedTA problem (and its variant)\nis NP-hard and design efficient heuristics for solving it in practice. Using\nreal datasets from three online market places, Freelancer, Guru and Upwork we\ndemonstrate the efficiency of our methods and the practical utility of our\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:04:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Nikolakaki", "Sofia Maria", ""], ["Cai", "Mingxiang", ""], ["Terzi", "Evimaria", ""]]}, {"id": "2011.04452", "submitter": "Eranga De Saa", "authors": "Eranga De Saa and Lochandaka Ranathunga", "title": "Comparison between ARIMA and Deep Learning Models for Temperature\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weather forecasting benefits us in various ways from farmers in cultivation\nand harvesting their crops to airlines to schedule their flights. Weather\nforecasting is a challenging task due to the chaotic nature of the atmosphere.\nTherefore lot of research attention has drawn to obtain the benefits and to\novercome the challenges of weather forecasting. This paper compares ARIMA (Auto\nRegressive Integrated Moving Average) model and deep learning models to\nforecast temperature. The deep learning model consists of one dimensional\nconvolutional layers to extract spatial features and LSTM layers to extract\ntemporal features. Both of these models are applied to hourly temperature data\nset from Szeged, Hungry. According to the experimental results deep learning\nmodel was able to perform better than the traditional ARIMA methodology.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:21:46 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["De Saa", "Eranga", ""], ["Ranathunga", "Lochandaka", ""]]}, {"id": "2011.04463", "submitter": "Susana Lai-Yuen", "authors": "Maria Baldeon Calisto and Susana Lai-Yuen", "title": "Neural Architecture Search with an Efficient Multiobjective Evolutionary\n  Framework", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have become very successful at solving many complex\ntasks such as image classification and segmentation, speech recognition and\nmachine translation. Nevertheless, manually designing a neural network for a\nspecific problem is very difficult and time-consuming due to the massive\nhyperparameter search space, long training times, and lack of technical\nguidelines for the hyperparameter selection. Moreover, most networks are highly\ncomplex, task specific and over-parametrized. Recently, multiobjective neural\narchitecture search (NAS) methods have been proposed to automate the design of\naccurate and efficient architectures. However, they only optimize either the\nmacro- or micro-structure of the architecture requiring the unset\nhyperparameters to be manually defined, and do not use the information produced\nduring the optimization process to increase the efficiency of the search. In\nthis work, we propose EMONAS, an Efficient MultiObjective Neural Architecture\nSearch framework for the automatic design of neural architectures while\noptimizing the network's accuracy and size. EMONAS is composed of a search\nspace that considers both the macro- and micro-structure of the architecture,\nand a surrogate-assisted multiobjective evolutionary based algorithm that\nefficiently searches for the best hyperparameters using a Random Forest\nsurrogate and guiding selection probabilities. EMONAS is evaluated on the task\nof 3D cardiac segmentation from the MICCAI ACDC challenge, which is crucial for\ndisease diagnosis, risk evaluation, and therapy decision. The architecture\nfound with EMONAS is ranked within the top 10 submissions of the challenge in\nall evaluation metrics, performing better or comparable to other approaches\nwhile reducing the search time by more than 50% and having considerably fewer\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:41:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Calisto", "Maria Baldeon", ""], ["Lai-Yuen", "Susana", ""]]}, {"id": "2011.04485", "submitter": "Pablo Lanillos", "authors": "Matej Hoffmann, Shengzhi Wang, Vojtech Outrata, Elisabet Alzueta,\n  Pablo Lanillos", "title": "Robot in the mirror: toward an embodied computational model of mirror\n  self-recognition", "comments": "To appear in KI - K\\\"unstliche Intelligenz - German Journal of\n  Artificial Intelligence - Springer", "journal-ref": null, "doi": "10.1007/s13218-020-00701-7", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-recognition or self-awareness is a capacity attributed typically only to\nhumans and few other species. The definitions of these concepts vary and little\nis known about the mechanisms behind them. However, there is a Turing test-like\nbenchmark: the mirror self-recognition, which consists in covertly putting a\nmark on the face of the tested subject, placing her in front of a mirror, and\nobserving the reactions. In this work, first, we provide a mechanistic\ndecomposition, or process model, of what components are required to pass this\ntest. Based on these, we provide suggestions for empirical research. In\nparticular, in our view, the way the infants or animals reach for the mark\nshould be studied in detail. Second, we develop a model to enable the humanoid\nrobot Nao to pass the test. The core of our technical contribution is learning\nthe appearance representation and visual novelty detection by means of learning\nthe generative model of the face with deep auto-encoders and exploiting the\nprediction error. The mark is identified as a salient region on the face and\nreaching action is triggered, relying on a previously learned mapping to arm\njoint angles. The architecture is tested on two robots with a completely\ndifferent face.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:11:31 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Hoffmann", "Matej", ""], ["Wang", "Shengzhi", ""], ["Outrata", "Vojtech", ""], ["Alzueta", "Elisabet", ""], ["Lanillos", "Pablo", ""]]}, {"id": "2011.04501", "submitter": "Yitao Chen", "authors": "Yitao Chen and Deepanshu Vasal", "title": "Multi-Agent Decentralized Belief Propagation on Graphs", "comments": "16 pages. arXiv admin note: text overlap with arXiv:1109.2135,\n  arXiv:1209.1695, arXiv:1802.08757 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of interactive partially observable Markov decision\nprocesses (I-POMDPs), where the agents are located at the nodes of a\ncommunication network. Specifically, we assume a certain message type for all\nmessages. Moreover, each agent makes individual decisions based on the\ninteractive belief states, the information observed locally and the messages\nreceived from its neighbors over the network. Within this setting, the\ncollective goal of the agents is to maximize the globally averaged return over\nthe network through exchanging information with their neighbors. We propose a\ndecentralized belief propagation algorithm for the problem, and prove the\nconvergence of our algorithm. Finally we show multiple applications of our\nframework. Our work appears to be the first study of decentralized belief\npropagation algorithm for networked multi-agent I-POMDPs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:16:26 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:25:35 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chen", "Yitao", ""], ["Vasal", "Deepanshu", ""]]}, {"id": "2011.04527", "submitter": "Alun Preece", "authors": "Frank Stein, Alun Preece", "title": "AAAI FSS-20: Artificial Intelligence in Government and Public Sector\n  Proceedings", "comments": "Pre-symposium proceedings including 12 pre-submitted papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in\nGovernment and Public Sector, Washington, DC, USA, November 13-14, 2020\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:08:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Stein", "Frank", ""], ["Preece", "Alun", ""]]}, {"id": "2011.04548", "submitter": "Ivan Girardi", "authors": "Chiara Marchiori, Douglas Dykeman, Ivan Girardi, Adam Ivankay, Kevin\n  Thandiackal, Mario Zusag, Andrea Giovannini, Daniel Karpati, Henri Saenz", "title": "Artificial Intelligence Decision Support for Medical Triage", "comments": "10 pages, 5 figures, accepted to AMIA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying state-of-the-art machine learning and natural language processing on\napproximately one million of teleconsultation records, we developed a triage\nsystem, now certified and in use at the largest European telemedicine provider.\nThe system evaluates care alternatives through interactions with patients via a\nmobile application. Reasoning on an initial set of provided symptoms, the\ntriage application generates AI-powered, personalized questions to better\ncharacterize the problem and recommends the most appropriate point of care and\ntime frame for a consultation. The underlying technology was developed to meet\nthe needs for performance, transparency, user acceptance and ease of use,\ncentral aspects to the adoption of AI-based decision support systems. Providing\nsuch remote guidance at the beginning of the chain of care has significant\npotential for improving cost efficiency, patient experience and outcomes. Being\nremote, always available and highly scalable, this service is fundamental in\nhigh demand situations, such as the current COVID-19 outbreak.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:45:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Marchiori", "Chiara", ""], ["Dykeman", "Douglas", ""], ["Girardi", "Ivan", ""], ["Ivankay", "Adam", ""], ["Thandiackal", "Kevin", ""], ["Zusag", "Mario", ""], ["Giovannini", "Andrea", ""], ["Karpati", "Daniel", ""], ["Saenz", "Henri", ""]]}, {"id": "2011.04569", "submitter": "Wolfgang Mack", "authors": "Wolfgang Mack, Mohamed Elminshawi, and Emanu\\\"el A. P. Habets", "title": "Signal-Guided Source Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art separation of desired signal components (DSCs) from a\nmixture is achieved using time-frequency masks or filters estimated by a deep\nneural network (DNN). The DSCs are typically defined at the time of training,\nor alternatively during inference via a reference signal (RS). In the latter\ncase, typically, an auxiliary DNN extracts signal characteristics (SCs) from\nthe RS and estimates a set of adaptive weights (AWs) of the first DNN. In both\ncases, the information of DSCs is stored in the DNN weights. Current methods\nusing audio RSs estimate time-invariant AWs. Applications where the RS and DSCs\nexhibit time-variant SCs, i.e., they cannot be assigned to a specific class\nlike speech, require time-variant AWs. An example is acoustic echo cancellation\nwith the loudspeaker signal as RS. We propose a method to extract time-variant\nAWs from a RS and additionally show that current time-invariant AWs methods can\nbe employed for universal source separation. To avoid strong scaling between\nthe estimate and the mixture, we propose to train with the dual scale-invariant\nsignal-to-distortion ratio in a TASNET inspired DNN. We evaluate the proposed\nAWs systems under various acoustic conditions and show the scenario-dependent\nadvantages of time-variant over time-invariant AWs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:13:23 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 12:30:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mack", "Wolfgang", ""], ["Elminshawi", "Mohamed", ""], ["Habets", "Emanu\u00ebl A. P.", ""]]}, {"id": "2011.04573", "submitter": "Dongsheng Luo", "authors": "Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng\n  Chen, Xiang Zhang", "title": "Parameterized Explainer for Graph Neural Network", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent progress in Graph Neural Networks (GNNs), explaining\npredictions made by GNNs remains a challenging open problem. The leading method\nindependently addresses the local explanations (i.e., important subgraph\nstructure and node features) to interpret why a GNN model makes the prediction\nfor a single instance, e.g. a node or a graph. As a result, the explanation\ngenerated is painstakingly customized for each instance. The unique explanation\ninterpreting each instance independently is not sufficient to provide a global\nunderstanding of the learned GNN model, leading to a lack of generalizability\nand hindering it from being used in the inductive setting. Besides, as it is\ndesigned for explaining a single instance, it is challenging to explain a set\nof instances naturally (e.g., graphs of a given class). In this study, we\naddress these key challenges and propose PGExplainer, a parameterized explainer\nfor GNNs. PGExplainer adopts a deep neural network to parameterize the\ngeneration process of explanations, which enables PGExplainer a natural\napproach to explaining multiple instances collectively. Compared to the\nexisting work, PGExplainer has better generalization ability and can be\nutilized in an inductive setting easily. Experiments on both synthetic and\nreal-life datasets show highly competitive performance with up to 24.7\\%\nrelative improvement in AUC on explaining graph classification over the leading\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:15:03 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Luo", "Dongsheng", ""], ["Cheng", "Wei", ""], ["Xu", "Dongkuan", ""], ["Yu", "Wenchao", ""], ["Zong", "Bo", ""], ["Chen", "Haifeng", ""], ["Zhang", "Xiang", ""]]}, {"id": "2011.04590", "submitter": "Banafsheh Rafiee", "authors": "Banafsheh Rafiee, Zaheer Abbas, Sina Ghiassian, Raksha Kumaraswamy,\n  Richard Sutton, Elliot Ludvig, Adam White", "title": "From Eye-blinks to State Construction: Diagnostic Benchmarks for Online\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments in classical conditioning show that animals such as rabbits,\npigeons, and dogs can make long temporal associations that enable multi-step\nprediction. To replicate this remarkable ability, an agent must construct an\ninternal state representation that summarizes its interaction history.\nRecurrent neural networks can automatically construct state and learn temporal\nassociations. But the current training methods are prohibitively expensive for\nonline prediction -- continual learning on every time step -- which is the\nfocus of this paper. To facilitate research in online prediction, we present\nthree new diagnostic prediction problems inspired by classical-conditioning\nexperiments. The proposed problems test the learning capabilities that animals\nreadily exhibit and highlight the current recurrent learning methods'\nlimitations. While the proposed problems are nontrivial, they are still\namenable to extensive testing and analysis in the small-compute regime, thereby\nenabling researchers to study issues in isolation carefully, ultimately\naccelerating progress towards scalable online representation learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:41:13 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 17:25:23 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 01:52:57 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Rafiee", "Banafsheh", ""], ["Abbas", "Zaheer", ""], ["Ghiassian", "Sina", ""], ["Kumaraswamy", "Raksha", ""], ["Sutton", "Richard", ""], ["Ludvig", "Elliot", ""], ["White", "Adam", ""]]}, {"id": "2011.04593", "submitter": "Andr\\'e Schidler", "authors": "Johannes K. Fichte, Markus Hecher, Andre Schidler", "title": "Solving the Steiner Tree Problem with few Terminals", "comments": "Authors' version of a paper which is to appear in the proceedings of\n  ICTAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner tree problem is a well-known problem in network design, routing,\nand VLSI design. Given a graph, edge costs, and a set of dedicated vertices\n(terminals), the Steiner tree problem asks to output a sub-graph that connects\nall terminals at minimum cost. A state-of-the-art algorithm to solve the\nSteiner tree problem by means of dynamic programming is the Dijkstra-Steiner\nalgorithm. The algorithm builds a Steiner tree of the entire instance by\nsystematically searching for smaller instances, based on subsets of the\nterminals, and combining Steiner trees for these smaller instances. The search\nheavily relies on a guiding heuristic function in order to prune the search\nspace. However, to ensure correctness, this algorithm allows only for limited\nheuristic functions, namely, those that satisfy a so-called consistency\ncondition. In this paper, we enhance the Dijkstra-Steiner algorithm and\nestablish a revisited algorithm, called DS*. The DS* algorithm allows for\narbitrary lower bounds as heuristics relaxing the previous condition on the\nheuristic function. Notably, we can now use linear programming based lower\nbounds. Further, we capture new requirements for a heuristic function in a\ncondition, which we call admissibility. We show that admissibility is indeed\nweaker than consistency and establish correctness of the DS* algorithm when\nusing an admissible heuristic function. We implement DS* and combine it with\nmodern preprocessing, resulting in an open-source solver (DS* Solve). Finally,\nwe compare its performance on standard benchmarks and observe a competitive\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:46:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Schidler", "Andre", ""]]}, {"id": "2011.04601", "submitter": "Dimitris Spathis", "authors": "Dimitris Spathis, Ignacio Perez-Pozuelo, Soren Brage, Nicholas J.\n  Wareham and Cecilia Mascolo", "title": "Learning Generalizable Physiological Representations from Large-scale\n  Wearable Data", "comments": "Accepted to the Machine Learning for Mobile Health workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, research on sensor-equipped mobile devices has primarily focused on\nthe purely supervised task of human activity recognition (walking, running,\netc), demonstrating limited success in inferring high-level health outcomes\nfrom low-level signals, such as acceleration. Here, we present a novel\nself-supervised representation learning method using activity and heart rate\n(HR) signals without semantic labels. With a deep neural network, we set HR\nresponses as the supervisory signal for the activity data, leveraging their\nunderlying physiological relationship.\n  We evaluate our model in the largest free-living combined-sensing dataset\n(comprising more than 280,000 hours of wrist accelerometer & wearable ECG data)\nand show that the resulting embeddings can generalize in various downstream\ntasks through transfer learning with linear classifiers, capturing\nphysiologically meaningful, personalized information. For instance, they can be\nused to predict (higher than 70 AUC) variables associated with individuals'\nhealth, fitness and demographic characteristics, outperforming unsupervised\nautoencoders and common bio-markers. Overall, we propose the first multimodal\nself-supervised method for behavioral and physiological data with implications\nfor large-scale health and lifestyle monitoring.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:56:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Spathis", "Dimitris", ""], ["Perez-Pozuelo", "Ignacio", ""], ["Brage", "Soren", ""], ["Wareham", "Nicholas J.", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "2011.04605", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto", "title": "Causality-aware counterfactual confounding adjustment as an alternative\n  to linear residualization in anticausal prediction tasks based on linear\n  learners", "comments": "This paper draws some material from arXiv:2001.03998", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear residualization is a common practice for confounding adjustment in\nmachine learning (ML) applications. Recently, causality-aware predictive\nmodeling has been proposed as an alternative causality-inspired approach for\nadjusting for confounders. The basic idea is to simulate counterfactual data\nthat is free from the spurious associations generated by the observed\nconfounders. In this paper, we compare the linear residualization approach\nagainst the causality-aware confounding adjustment in anticausal prediction\ntasks, and show that the causality-aware approach tends to (asymptotically)\noutperform the residualization adjustment in terms of predictive performance in\nlinear learners. Importantly, our results still holds even when the true model\nis not linear. We illustrate our results in both regression and classification\ntasks, where we compared the causality-aware and residualization approaches\nusing mean squared errors and classification accuracy in synthetic data\nexperiments where the linear regression model is mispecified, as well as, when\nthe linear model is correctly specified. Furthermore, we illustrate how the\ncausality-aware approach is more stable than residualization with respect to\ndataset shifts in the joint distribution of the confounders and outcome\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:59:57 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Neto", "Elias Chaibub", ""]]}, {"id": "2011.04607", "submitter": "Yu Chen", "authors": "Yu Chen, Jie Chen, Ganesh Krishnamurthi, Huijing Yang, Huahui Wang,\n  Wenjie Zhao", "title": "Deep reinforcement learning for RAN optimization and control", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high variability of the traffic in the radio access network (RAN),\nfixed network configurations are not flexible enough to achieve optimal\nperformance. Our vendors provide several settings of the eNodeB to optimize the\nRAN performance, such as media access control scheduler, loading balance, etc.\nBut the detailed mechanisms of the eNodeB configurations are usually very\ncomplicated and not disclosed, not to mention the large key performance\nindicators (KPIs) space needed to be considered. These make constructing a\nsimulator, offline tuning, or rule-based solutions difficult. We aim to build\nan intelligent controller without strong assumption or domain knowledge about\nthe RAN and can run 24/7 without supervision. To achieve this goal, we first\nbuild a closed-loop control testbed RAN in a lab environment with one eNodeB\nprovided by one of the largest wireless vendors and four smartphones. Next, we\nbuild a double Q network agent trained with the live feedback of the key\nperformance indicators from the RAN. Our work proved the effectiveness of\napplying deep reinforcement learning to improve network performance in a real\nRAN network environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:02:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 16:47:01 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "Yu", ""], ["Chen", "Jie", ""], ["Krishnamurthi", "Ganesh", ""], ["Yang", "Huijing", ""], ["Wang", "Huahui", ""], ["Zhao", "Wenjie", ""]]}, {"id": "2011.04622", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael I. Jordan", "title": "On Function Approximation in Reinforcement Learning: Optimism in the\n  Face of Large State Spaces", "comments": "76 pages. The short version of this work appears in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical theory of reinforcement learning (RL) has focused on tabular\nand linear representations of value functions. Further progress hinges on\ncombining RL with modern function approximators such as kernel functions and\ndeep neural networks, and indeed there have been many empirical successes that\nhave exploited such combinations in large-scale applications. There are\nprofound challenges, however, in developing a theory to support this\nenterprise, most notably the need to take into consideration the\nexploration-exploitation tradeoff at the core of RL in conjunction with the\ncomputational and statistical tradeoffs that arise in modern\nfunction-approximation-based learning systems. We approach these challenges by\nstudying an optimistic modification of the least-squares value iteration\nalgorithm, in the context of the action-value function\n  represented by a kernel function or an overparameterized neural network. We\nestablish both polynomial runtime complexity and polynomial sample complexity\nfor this algorithm, without additional assumptions on the data-generating\nmodel. In particular, we prove that the algorithm incurs an\n$\\tilde{\\mathcal{O}}(\\delta_{\\mathcal{F}} H^2 \\sqrt{T})$ regret, where\n$\\delta_{\\mathcal{F}}$ characterizes the intrinsic complexity of the function\nclass $\\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total\nnumber of episodes. Our regret bounds are independent of the number of states,\na result which exhibits clearly the benefit of function approximation in RL.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:32:22 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 17:24:48 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yang", "Zhuoran", ""], ["Jin", "Chi", ""], ["Wang", "Zhaoran", ""], ["Wang", "Mengdi", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.04627", "submitter": "Jacky Liang", "authors": "Mohit Sharma, Jacky Liang, Jialiang Zhao, Alex LaGrassa, Oliver\n  Kroemer", "title": "Learning to Compose Hierarchical Object-Centric Controllers for Robotic\n  Manipulation", "comments": "Accepted as Plenary Talk at CoRL'20. First two authors contributed\n  equally. For results see\n  https://sites.google.com/view/compositional-object-control/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation tasks can often be decomposed into multiple subtasks performed\nin parallel, e.g., sliding an object to a goal pose while maintaining contact\nwith a table. Individual subtasks can be achieved by task-axis controllers\ndefined relative to the objects being manipulated, and a set of object-centric\ncontrollers can be combined in an hierarchy. In prior works, such combinations\nare defined manually or learned from demonstrations. By contrast, we propose\nusing reinforcement learning to dynamically compose hierarchical object-centric\ncontrollers for manipulation tasks. Experiments in both simulation and real\nworld show how the proposed approach leads to improved sample efficiency,\nzero-shot generalization to novel test environments, and simulation-to-reality\ntransfer without fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:38:29 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 20:27:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sharma", "Mohit", ""], ["Liang", "Jacky", ""], ["Zhao", "Jialiang", ""], ["LaGrassa", "Alex", ""], ["Kroemer", "Oliver", ""]]}, {"id": "2011.04692", "submitter": "Baichuan Huang", "authors": "Baichuan Huang, Shuai D. Han, Abdeslam Boularias, and Jingjin Yu", "title": "DIPN: Deep Interaction Prediction Network with Application to Clutter\n  Removal", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Deep Interaction Prediction Network (DIPN) for learning to\npredict complex interactions that ensue as a robot end-effector pushes multiple\nobjects, whose physical properties, including size, shape, mass, and friction\ncoefficients may be unknown a priori. DIPN \"imagines\" the effect of a push\naction and generates an accurate synthetic image of the predicted outcome. DIPN\nis shown to be sample efficient when trained in simulation or with a real\nrobotic system. The high accuracy of DIPN allows direct integration with a\ngrasp network, yielding a robotic manipulation system capable of executing\nchallenging clutter removal tasks while being trained in a fully\nself-supervised manner. The overall network demonstrates intelligent behavior\nin selecting proper actions between push and grasp for completing clutter\nremoval tasks and significantly outperforms the previous state-of-the-art.\nRemarkably, DIPN achieves even better performance on the real robotic hardware\nsystem than in simulation. Videos, code, and experiments log are available at\nhttps://github.com/rutgers-arc-lab/dipn.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:16:26 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 04:56:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Huang", "Baichuan", ""], ["Han", "Shuai D.", ""], ["Boularias", "Abdeslam", ""], ["Yu", "Jingjin", ""]]}, {"id": "2011.04697", "submitter": "Zhiqian Qiao", "authors": "Zhiqian Qiao, Jeff Schneider and John M. Dolan", "title": "Behavior Planning at Urban Intersections through Hierarchical\n  Reinforcement Learning", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous vehicles, effective behavior planning is crucial to ensure\nsafety of the ego car. In many urban scenarios, it is hard to create\nsufficiently general heuristic rules, especially for challenging scenarios that\nsome new human drivers find difficult. In this work, we propose a behavior\nplanning structure based on reinforcement learning (RL) which is capable of\nperforming autonomous vehicle behavior planning with a hierarchical structure\nin simulated urban environments. Application of the hierarchical structure\nallows the various layers of the behavior planning system to be satisfied. Our\nalgorithms can perform better than heuristic-rule-based methods for elective\ndecisions such as when to turn left between vehicles approaching from the\nopposite direction or possible lane-change when approaching an intersection due\nto lane blockage or delay in front of the ego car. Such behavior is hard to\nevaluate as correct or incorrect, but for some aggressive expert human drivers\nhandle such scenarios effectively and quickly. On the other hand, compared to\ntraditional RL methods, our algorithm is more sample-efficient, due to the use\nof a hybrid reward mechanism and heuristic exploration during the training\nprocess. The results also show that the proposed method converges to an optimal\npolicy faster than traditional RL methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:23:26 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Qiao", "Zhiqian", ""], ["Schneider", "Jeff", ""], ["Dolan", "John M.", ""]]}, {"id": "2011.04702", "submitter": "Zhiqian Qiao", "authors": "Josiah Coad, Zhiqian Qiao, John M. Dolan", "title": "Safe Trajectory Planning Using Reinforcement Learning for Self Driving", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles must be able to act intelligently in diverse and\ndifficult environments, marked by high-dimensional state spaces, a myriad of\noptimization objectives and complex behaviors. Traditionally, classical\noptimization and search techniques have been applied to the problem of\nself-driving; but they do not fully address operations in environments with\nhigh-dimensional states and complex behaviors. Recently, imitation learning has\nbeen proposed for the task of self-driving; but it is labor-intensive to obtain\nenough training data. Reinforcement learning has been proposed as a way to\ndirectly control the car, but this has safety and comfort concerns. We propose\nusing model-free reinforcement learning for the trajectory planning stage of\nself-driving and show that this approach allows us to operate the car in a more\nsafe, general and comfortable manner, required for the task of self driving.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:29:14 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Coad", "Josiah", ""], ["Qiao", "Zhiqian", ""], ["Dolan", "John M.", ""]]}, {"id": "2011.04748", "submitter": "Pragaash Ponnusamy", "authors": "Alireza Roshan-Ghias, Clint Solomon Mathialagan, Pragaash Ponnusamy,\n  Lambert Mathias, Chenlei Guo", "title": "Personalized Query Rewriting in Conversational AI Agents", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) systems in conversational AI agents often\nexperience errors in the form of misrecognitions by automatic speech\nrecognition (ASR) or semantic gaps in natural language understanding (NLU).\nThese errors easily translate to user frustrations, particularly so in\nrecurrent events e.g. regularly toggling an appliance, calling a frequent\ncontact, etc. In this work, we propose a query rewriting approach by leveraging\nusers' historically successful interactions as a form of memory. We present a\nneural retrieval model and a pointer-generator network with hierarchical\nattention and show that they perform significantly better at the query\nrewriting task with the aforementioned user memories than without. We also\nhighlight how our approach with the proposed models leverages the structural\nand semantic diversity in ASR's output towards recovering users' intents.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:45:39 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Roshan-Ghias", "Alireza", ""], ["Mathialagan", "Clint Solomon", ""], ["Ponnusamy", "Pragaash", ""], ["Mathias", "Lambert", ""], ["Guo", "Chenlei", ""]]}, {"id": "2011.04750", "submitter": "Mohamed Bakhouya", "authors": "Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya", "title": "Predicting the Future is like Completing a Painting!", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an introductory work towards a larger research framework\nrelative to Scientific Prediction. It is a mixed between science and philosophy\nof science, therefore we can talk about Experimental Philosophy of Science. As\na first result, we introduce a new forecasting method based on image\ncompletion, named Forecasting Method by Image Inpainting (FM2I). In fact, time\nseries forecasting is transformed into fully images- and signal-based\nprocessing procedures. After transforming a time series data into its\ncorresponding image, the problem of data forecasting becomes essentially a\nproblem of image inpainting problem, i.e., completing missing data in the\nimage. An extensive experimental evaluation is conducted using a large dataset\nproposed by the well-known M3-competition. Results show that FM2I represents an\nefficient and robust tool for time series forecasting. It has achieved\nprominent results in terms of accuracy and outperforms the best M3 forecasting\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:48:06 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Maaroufi", "Nadir", ""], ["Najib", "Mehdi", ""], ["Bakhouya", "Mohamed", ""]]}, {"id": "2011.04752", "submitter": "Zhiqian Qiao", "authors": "Kaleb Ben Naveed, Zhiqian Qiao and John M. Dolan", "title": "Trajectory Planning for Autonomous Vehicles Using Hierarchical\n  Reinforcement Learning", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning safe trajectories under uncertain and dynamic conditions makes the\nautonomous driving problem significantly complex. Current sampling-based\nmethods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this\nproblem because of the high computational cost. Supervised learning methods\nsuch as Imitation Learning lack generalization and safety guarantees. To\naddress these problems and in order to ensure a robust framework, we propose a\nHierarchical Reinforcement Learning (HRL) structure combined with a\nProportional-Integral-Derivative (PID) controller for trajectory planning. HRL\nhelps divide the task of autonomous vehicle driving into sub-goals and supports\nthe network to learn policies for both high-level options and low-level\ntrajectory planner choices. The introduction of sub-goals decreases convergence\ntime and enables the policies learned to be reused for other scenarios. In\naddition, the proposed planner is made robust by guaranteeing smooth\ntrajectories and by handling the noisy perception system of the ego-car. The\nPID controller is used for tracking the waypoints, which ensures smooth\ntrajectories and reduces jerk. The problem of incomplete observations is\nhandled by using a Long-Short-Term-Memory (LSTM) layer in the network. Results\nfrom the high-fidelity CARLA simulator indicate that the proposed method\nreduces convergence time, generates smoother trajectories, and is able to\nhandle dynamic surroundings and noisy observations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:49:54 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Naveed", "Kaleb Ben", ""], ["Qiao", "Zhiqian", ""], ["Dolan", "John M.", ""]]}, {"id": "2011.04767", "submitter": "Ali Emami Mr.", "authors": "Ali Emami, Adam Trischler, Kaheer Suleman and Jackie Chi Kit Cheung", "title": "An Analysis of Dataset Overlap on Winograd-Style Tasks", "comments": "11 pages with references, accepted at COLING 2020", "journal-ref": "Coling2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Winograd Schema Challenge (WSC) and variants inspired by it have become\nimportant benchmarks for common-sense reasoning (CSR). Model performance on the\nWSC has quickly progressed from chance-level to near-human using neural\nlanguage models trained on massive corpora. In this paper, we analyze the\neffects of varying degrees of overlap between these training corpora and the\ntest instances in WSC-style tasks. We find that a large number of test\ninstances overlap considerably with the corpora on which state-of-the-art\nmodels are (pre)trained, and that a significant drop in classification accuracy\noccurs when we evaluate models on instances with minimal overlap. Based on\nthese results, we develop the KnowRef-60K dataset, which consists of over 60k\npronoun disambiguation problems scraped from web data. KnowRef-60K is the\nlargest corpus to date for WSC-style common-sense reasoning and exhibits a\nsignificantly lower proportion of overlaps with current pretraining corpora.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:11:17 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Emami", "Ali", ""], ["Trischler", "Adam", ""], ["Suleman", "Kaheer", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "2011.04783", "submitter": "Amanda Sofie Rios", "authors": "Amanda Rios and Laurent Itti", "title": "Lifelong Learning Without a Task Oracle", "comments": "Proceedings of the IEEE 32nd International Conference on Tools with\n  Artificial Intelligence (ICTAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep neural networks are known to undergo a sharp decline in the\naccuracy of older tasks when new tasks are learned, termed \"catastrophic\nforgetting\". Many state-of-the-art solutions to continual learning rely on\nbiasing and/or partitioning a model to accommodate successive tasks\nincrementally. However, these methods largely depend on the availability of a\ntask-oracle to confer task identities to each test sample, without which the\nmodels are entirely unable to perform. To address this shortcoming, we propose\nand compare several candidate task-assigning mappers which require very little\nmemory overhead: (1) Incremental unsupervised prototype assignment using either\nnearest means, Gaussian Mixture Models or fuzzy ART backbones; (2) Supervised\nincremental prototype assignment with fast fuzzy ARTMAP; (3) Shallow perceptron\ntrained via a dynamic coreset. Our proposed model variants are trained either\nfrom pre-trained feature extractors or task-dependent feature embeddings of the\nmain classifier network. We apply these pipeline variants to continual learning\nbenchmarks, comprised of either sequences of several datasets or within one\nsingle dataset. Overall, these methods, despite their simplicity and\ncompactness, perform very close to a ground truth oracle, especially in\nexperiments of inter-dataset task assignment. Moreover, best-performing\nvariants only impose an average cost of 1.7% parameter memory increase.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:30:31 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Rios", "Amanda", ""], ["Itti", "Laurent", ""]]}, {"id": "2011.04796", "submitter": "Zahra Rezaei Khavas", "authors": "Zahra Rezaei Khavas, Reza Ahmadzadeh, Paul Robinette", "title": "Modeling Trust in Human-Robot Interaction: A Survey", "comments": "12 pages, 1 table, conference ICSR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As the autonomy and capabilities of robotic systems increase, they are\nexpected to play the role of teammates rather than tools and interact with\nhuman collaborators in a more realistic manner, creating a more human-like\nrelationship. Given the impact of trust observed in human-robot interaction\n(HRI), appropriate trust in robotic collaborators is one of the leading factors\ninfluencing the performance of human-robot interaction. Team performance can be\ndiminished if people do not trust robots appropriately by disusing or misusing\nthem based on limited experience. Therefore, trust in HRI needs to be\ncalibrated properly, rather than maximized, to let the formation of an\nappropriate level of trust in human collaborators. For trust calibration in\nHRI, trust needs to be modeled first. There are many reviews on factors\naffecting trust in HRI, however, as there are no reviews concentrated on\ndifferent trust models, in this paper, we review different techniques and\nmethods for trust modeling in HRI. We also present a list of potential\ndirections for further research and some challenges that need to be addressed\nin future work on human-robot trust modeling.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:56:34 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Khavas", "Zahra Rezaei", ""], ["Ahmadzadeh", "Reza", ""], ["Robinette", "Paul", ""]]}, {"id": "2011.04797", "submitter": "Dongsheng Luo", "authors": "Dongsheng Luo, Yuchen Bian, Xiang Zhang, Jun Huan", "title": "Attentive Social Recommendation: Towards User And Item Diversities", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommendation system is to predict unobserved user-item rating values\nby taking advantage of user-user social relation and user-item ratings.\nHowever, user/item diversities in social recommendations are not well utilized\nin the literature. Especially, inter-factor (social and rating factors)\nrelations and distinct rating values need taking into more consideration. In\nthis paper, we propose an attentive social recommendation system (ASR) to\naddress this issue from two aspects. First, in ASR, Rec-conv graph network\nlayers are proposed to extract the social factor, user-rating and item-rated\nfactors and then automatically assign contribution weights to aggregate these\nfactors into the user/item embedding vectors. Second, a disentangling strategy\nis applied for diverse rating values. Extensive experiments on benchmarks\ndemonstrate the effectiveness and advantages of our ASR.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:57:45 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 00:27:52 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Luo", "Dongsheng", ""], ["Bian", "Yuchen", ""], ["Zhang", "Xiang", ""], ["Huan", "Jun", ""]]}, {"id": "2011.04802", "submitter": "Kamran Kowsari", "authors": "Jinghe Zhang, Kamran Kowsari, Mehdi Boukhechba, James Harrison,\n  Jennifer Lobo, Laura Barnes", "title": "Sparse Longitudinal Representations of Electronic Health Record Data for\n  the Early Detection of Chronic Kidney Disease in Diabetic Patients", "comments": "Accepted in IEEE BIBM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.PE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic kidney disease (CKD) is a gradual loss of renal function over time,\nand it increases the risk of mortality, decreased quality of life, as well as\nserious complications. The prevalence of CKD has been increasing in the last\ncouple of decades, which is partly due to the increased prevalence of diabetes\nand hypertension. To accurately detect CKD in diabetic patients, we propose a\nnovel framework to learn sparse longitudinal representations of patients'\nmedical records. The proposed method is also compared with widely used\nbaselines such as Aggregated Frequency Vector and Bag-of-Pattern in Sequences\non real EHR data, and the experimental results indicate that the proposed model\nachieves higher predictive performance. Additionally, the learned\nrepresentations are interpreted and visualized to bring clinical insights.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 22:07:25 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:33:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Jinghe", ""], ["Kowsari", "Kamran", ""], ["Boukhechba", "Mehdi", ""], ["Harrison", "James", ""], ["Lobo", "Jennifer", ""], ["Barnes", "Laura", ""]]}, {"id": "2011.04820", "submitter": "Shuijing Liu", "authors": "Shuijing Liu, Peixin Chang, Weihang Liang, Neeloy Chakraborty,\n  Katherine Driggs-Campbell", "title": "Decentralized Structural-RNN for Robot Crowd Navigation with Deep\n  Reinforcement Learning", "comments": "Published as a conference paper in IEEE International Conference on\n  Robotics and Automation (ICRA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe and efficient navigation through human crowds is an essential capability\nfor mobile robots. Previous work on robot crowd navigation assumes that the\ndynamics of all agents are known and well-defined. In addition, the performance\nof previous methods deteriorates in partially observable environments and\nenvironments with dense crowds. To tackle these problems, we propose\ndecentralized structural-Recurrent Neural Network (DS-RNN), a novel network\nthat reasons about spatial and temporal relationships for robot decision making\nin crowd navigation. We train our network with model-free deep reinforcement\nlearning without any expert supervision. We demonstrate that our model\noutperforms previous methods in challenging crowd navigation scenarios. We\nsuccessfully transfer the policy learned in the simulator to a real-world\nTurtleBot 2i.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:15:31 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 20:53:01 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 15:51:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Shuijing", ""], ["Chang", "Peixin", ""], ["Liang", "Weihang", ""], ["Chakraborty", "Neeloy", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "2011.04825", "submitter": "Ramina Ghods", "authors": "Ramina Ghods, William J. Durkin, Jeff Schneider", "title": "Multi-Agent Active Search using Realistic Depth-Aware Noise Model", "comments": "To appear at the 2021 IEEE International Conference on Robotics and\n  Automation (ICRA), extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The active search for objects of interest in an unknown environment has many\nrobotics applications including search and rescue, detecting gas leaks or\nlocating animal poachers. Existing algorithms often prioritize the location\naccuracy of objects of interest while other practical issues such as the\nreliability of object detection as a function of distance and lines of sight\nremain largely ignored. Additionally, in many active search scenarios,\ncommunication infrastructure may be unreliable or unestablished, making\ncentralized control of multiple agents impractical. We present an algorithm\ncalled Noise-Aware Thompson Sampling (NATS) that addresses these issues for\nmultiple ground-based robots performing active search considering two sources\nof sensory information from monocular optical imagery and depth maps. By\nutilizing Thompson Sampling, NATS allows for decentralized coordination among\nmultiple agents. NATS also considers object detection uncertainty from depth as\nwell as environmental occlusions and operates while remaining agnostic of the\nnumber of objects of interest. Using simulation results, we show that NATS\nsignificantly outperforms existing methods such as information-greedy policies\nor exhaustive search. We demonstrate the real-world viability of NATS using a\npseudo-realistic environment created in the Unreal Engine 4 game development\nplatform with the AirSim plugin.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:20:55 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:39:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ghods", "Ramina", ""], ["Durkin", "William J.", ""], ["Schneider", "Jeff", ""]]}, {"id": "2011.04840", "submitter": "Harry Zhang Mr.", "authors": "Harry Zhang, Jeffrey Ichnowski, Daniel Seita, Jonathan Wang, Huang\n  Huang, and Ken Goldberg", "title": "Robots of the Lost Arc: Self-Supervised Learning to Dynamically\n  Manipulate Fixed-Endpoint Cables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how high-speed robot arm motions can dynamically manipulate cables\nto vault over obstacles, knock objects from pedestals, and weave between\nobstacles. In this paper, we propose a self-supervised learning framework that\nenables a UR5 robot to perform these three tasks. The framework finds a 3D apex\npoint for the robot arm, which, together with a task-specific trajectory\nfunction, defines an arcing motion that dynamically manipulates the cable to\nperform tasks with varying obstacle and target locations. The trajectory\nfunction computes minimum-jerk motions that are constrained to remain within\njoint limits and to travel through the 3D apex point by repeatedly solving\nquadratic programs to find the shortest and fastest feasible motion. We\nexperiment with 5 physical cables with different thickness and mass and compare\nperformance against two baselines in which a human chooses the apex point.\nResults suggest that a baseline with a fixed apex across the three tasks\nachieves respective success rates of 51.7%, 36.7%, and 15.0%, and a baseline\nwith human-specified, task-specific apex points achieves 66.7%, 56.7%, and\n15.0% success rate respectively, while the robot using the learned apex point\ncan achieve success rates of 81.7% in vaulting, 65.0% in knocking, and 60.0% in\nweaving. Code, data, and supplementary materials are available at https:\n//sites.google.com/berkeley.edu/dynrope/home.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 00:19:50 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:17:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Harry", ""], ["Ichnowski", "Jeffrey", ""], ["Seita", "Daniel", ""], ["Wang", "Jonathan", ""], ["Huang", "Huang", ""], ["Goldberg", "Ken", ""]]}, {"id": "2011.04853", "submitter": "Srikanth Malla", "authors": "Srikanth Malla, Chiho Choi, Behzad Dariush", "title": "Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers the problem of multi-modal future trajectory forecast\nwith ranking. Here, multi-modality and ranking refer to the multiple plausible\npath predictions and the confidence in those predictions, respectively. We\npropose Social-STAGE, Social interaction-aware Spatio-Temporal multi-Attention\nGraph convolution network with novel Evaluation for multi-modality. Our main\ncontributions include analysis and formulation of multi-modality with ranking\nusing interaction and multi-attention, and introduction of new metrics to\nevaluate the diversity and associated confidence of multi-modal predictions. We\nevaluate our approach on existing public datasets ETH and UCY and show that the\nproposed algorithm outperforms the state of the arts on these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 01:18:57 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:32:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Malla", "Srikanth", ""], ["Choi", "Chiho", ""], ["Dariush", "Behzad", ""]]}, {"id": "2011.04883", "submitter": "Rachel Gardner", "authors": "Rachel Gardner, Maya Varma, Clare Zhu, Ranjay Krishna", "title": "Determining Question-Answer Plausibility in Crowdsourced Datasets Using\n  Multi-Task Learning", "comments": "Published at the 6th Workshop on Noisy User-generated Text (W-NUT)\n  2020 at EMNLP (6 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets extracted from social networks and online forums are often prone to\nthe pitfalls of natural language, namely the presence of unstructured and noisy\ndata. In this work, we seek to enable the collection of high-quality\nquestion-answer datasets from social media by proposing a novel task for\nautomated quality analysis and data cleaning: question-answer (QA)\nplausibility. Given a machine or user-generated question and a crowd-sourced\nresponse from a social media user, we determine if the question and response\nare valid; if so, we identify the answer within the free-form response. We\ndesign BERT-based models to perform the QA plausibility task, and we evaluate\nthe ability of our models to generate a clean, usable question-answer dataset.\nOur highest-performing approach consists of a single-task model which\ndetermines the plausibility of the question, followed by a multi-task model\nwhich evaluates the plausibility of the response as well as extracts answers\n(Question Plausibility AUROC=0.75, Response Plausibility AUROC=0.78, Answer\nExtraction F1=0.665).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 04:11:44 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gardner", "Rachel", ""], ["Varma", "Maya", ""], ["Zhu", "Clare", ""], ["Krishna", "Ranjay", ""]]}, {"id": "2011.04896", "submitter": "Soroosh Tayebi Arasteh", "authors": "Soroosh Tayebi Arasteh", "title": "Generalized LSTM-based End-to-End Text-Independent Speaker Verification", "comments": "7 pages, 7 tables, 6 figures. Research Internship project at the\n  Pattern Recognition Lab at FAU Erlangen-Nuremberg, as a part of Master's\n  curriculum. Re-implementation of the paper arXiv:1710.10467 by Wan et al", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing amount of available data and more affordable hardware\nsolutions have opened a gate to the realm of Deep Learning (DL). Due to the\nrapid advancements and ever-growing popularity of DL, it has begun to invade\nalmost every field, where machine learning is applicable, by altering the\ntraditional state-of-the-art methods. While many researchers in the speaker\nrecognition area have also started to replace the former state-of-the-art\nmethods with DL techniques, some of the traditional i-vector-based methods are\nstill state-of-the-art in the context of text-independent speaker verification\n(TI-SV). In this paper, we discuss the most recent generalized end-to-end\n(GE2E) DL technique based on Long Short-term Memory (LSTM) units for TI-SV by\nGoogle and compare different scenarios and aspects including utterance\nduration, training time, and accuracy to prove that our method outperforms the\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:17:06 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 15:50:00 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 16:36:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Arasteh", "Soroosh Tayebi", ""]]}, {"id": "2011.04910", "submitter": "Kun Wang", "authors": "Kun Wang, Mridul Aanjaneya and Kostas Bekris", "title": "Spring-Rod System Identification via Differentiable Physics Engine", "comments": "Workshop on Differentiable Vision, Graphics, and Physics in Machine\n  Learning at NeurIPS 2020. arXiv admin note: substantial text overlap with\n  arXiv:2004.13859", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel differentiable physics engine for system identification of\ncomplex spring-rod assemblies. Unlike black-box data-driven methods for\nlearning the evolution of a dynamical system \\emph{and} its parameters, we\nmodularize the design of our engine using a discrete form of the governing\nequations of motion, similar to a traditional physics engine. We further reduce\nthe dimension from 3D to 1D for each module, which allows efficient learning of\nsystem parameters using linear regression. The regression parameters correspond\nto physical quantities, such as spring stiffness or the mass of the rod, making\nthe pipeline explainable. The approach significantly reduces the amount of\ntraining data required, and also avoids iterative identification of data\nsampling and model training. We compare the performance of the proposed engine\nwith previous solutions, and demonstrate its efficacy on tensegrity systems,\nsuch as NASA's icosahedron.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:36:22 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Kun", ""], ["Aanjaneya", "Mridul", ""], ["Bekris", "Kostas", ""]]}, {"id": "2011.04923", "submitter": "Steve Dias Da Cruz", "authors": "Hans-Peter Beise, Steve Dias Da Cruz", "title": "Expressiveness of Neural Networks Having Width Equal or Below the Input\n  Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding about the minimum width of deep neural networks needed to\nensure universal approximation for different activation functions has\nprogressively been extended (Park et al., 2020). In particular, with respect to\napproximation on general compact sets in the input space, a network width less\nthan or equal to the input dimension excludes universal approximation. In this\nwork, we focus on network functions of width less than or equal to the latter\ncritical bound. We prove that in this regime, the exact fit of partially\nconstant functions on disjoint compact sets is still possible for ReLU network\nfunctions under some conditions on the mutual location of these components. We\nshow that with cosine as activation function, a three layer network of width\none is sufficient to approximate any function on arbitrary finite sets.\nConversely, we prove a maximum principle from which we conclude that for all\ncontinuous and monotonic activation functions, universal approximation of\narbitrary continuous functions is impossible on sets that coincide with the\nboundary of an open set plus an inner point.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:06:02 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:55:09 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 09:18:18 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Beise", "Hans-Peter", ""], ["Da Cruz", "Steve Dias", ""]]}, {"id": "2011.04926", "submitter": "Ruoyu Sun", "authors": "Ruoyu Sun, Tiantian Fang, Alex Schwing", "title": "Towards a Better Global Loss Landscape of GANs", "comments": "Accepted to NeurIPS 2020 (oral). 43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding of GAN training is still very limited. One major challenge is\nits non-convex-non-concave min-max objective, which may lead to sub-optimal\nlocal minima. In this work, we perform a global landscape analysis of the\nempirical loss of GANs. We prove that a class of separable-GAN, including the\noriginal JS-GAN, has exponentially many bad basins which are perceived as\nmode-collapse. We also study the relativistic pairing GAN (RpGAN) loss which\ncouples the generated samples and the true samples. We prove that RpGAN has no\nbad basins. Experiments on synthetic data show that the predicted bad basin can\nindeed appear in training. We also perform experiments to support our theory\nthat RpGAN has a better landscape than separable-GAN. For instance, we\nempirically show that RpGAN performs better than separable-GAN with relatively\nnarrow neural nets. The code is available at https://github.com/AilsaF/RS-GAN.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:10:52 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sun", "Ruoyu", ""], ["Fang", "Tiantian", ""], ["Schwing", "Alex", ""]]}, {"id": "2011.04929", "submitter": "Kun Wang", "authors": "Kun Wang, Mridul Aanjaneya and Kostas Bekris", "title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics\n  Engine for Tensegrity Robots", "comments": "Accepted to IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning policies in simulation is promising for reducing human effort when\ntraining robot controllers. This is especially true for soft robots that are\nmore adaptive and safe but also more difficult to accurately model and control.\nThe sim2real gap is the main barrier to successfully transfer policies from\nsimulation to a real robot. System identification can be applied to reduce this\ngap but traditional identification methods require a lot of manual tuning.\nData-driven alternatives can tune dynamical models directly from data but are\noften data hungry, which also incorporates human effort in collecting data.\nThis work proposes a data-driven, end-to-end differentiable simulator focused\non the exciting but challenging domain of tensegrity robots. To the best of the\nauthors' knowledge, this is the first differentiable physics engine for\ntensegrity robots that supports cable, contact, and actuation modeling. The aim\nis to develop a reasonably simplified, data-driven simulation, which can learn\napproximate dynamics with limited ground truth data. The dynamics must be\naccurate enough to generate policies that can be transferred back to the\nground-truth system. As a first step in this direction, the current work\ndemonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts\nas a ground truth system. Two different tensegrity robots are used for\nevaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity.\nThe results indicate that only 0.25\\% of ground truth data are needed to train\na policy that works on the ground truth system when the differentiable engine\nis used for training against training the policy directly on the ground truth\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:19:54 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 23:08:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Kun", ""], ["Aanjaneya", "Mridul", ""], ["Bekris", "Kostas", ""]]}, {"id": "2011.04950", "submitter": "Anand Balakrishnan", "authors": "Parv Kapoor, Anand Balakrishnan, Jyotirmoy V. Deshmukh", "title": "Model-based Reinforcement Learning from Signal Temporal Logic\n  Specifications", "comments": "Submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Techniques based on Reinforcement Learning (RL) are increasingly being used\nto design control policies for robotic systems. RL fundamentally relies on\nstate-based reward functions to encode desired behavior of the robot and bad\nreward functions are prone to exploitation by the learning agent, leading to\nbehavior that is undesirable in the best case and critically dangerous in the\nworst. On the other hand, designing good reward functions for complex tasks is\na challenging problem. In this paper, we propose expressing desired high-level\nrobot behavior using a formal specification language known as Signal Temporal\nLogic (STL) as an alternative to reward/cost functions. We use STL\nspecifications in conjunction with model-based learning to design model\npredictive controllers that try to optimize the satisfaction of the STL\nspecification over a finite time horizon. The proposed algorithm is empirically\nevaluated on simulations of robotic system such as a pick-and-place robotic\narm, and adaptive cruise control for autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 07:31:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kapoor", "Parv", ""], ["Balakrishnan", "Anand", ""], ["Deshmukh", "Jyotirmoy V.", ""]]}, {"id": "2011.04999", "submitter": "Jennifer Grannen", "authors": "Jennifer Grannen, Priya Sundaresan, Brijen Thananjeyan, Jeffrey\n  Ichnowski, Ashwin Balakrishna, Minho Hwang, Vainavi Viswanath, Michael\n  Laskey, Joseph E. Gonzalez, Ken Goldberg", "title": "Untangling Dense Knots by Learning Task-Relevant Keypoints", "comments": "Conference on Robot Learning (CoRL) 2020 Oral. First two authors\n  contributed equally", "journal-ref": "4th Conference on Robot Learning (CoRL 2020)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Untangling ropes, wires, and cables is a challenging task for robots due to\nthe high-dimensional configuration space, visual homogeneity, self-occlusions,\nand complex dynamics. We consider dense (tight) knots that lack space between\nself-intersections and present an iterative approach that uses learned\ngeometric structure in configurations. We instantiate this into an algorithm,\nHULK: Hierarchical Untangling from Learned Keypoints, which combines\nlearning-based perception with a geometric planner into a policy that guides a\nbilateral robot to untangle knots. To evaluate the policy, we perform\nexperiments both in a novel simulation environment modelling cables with varied\nknot types and textures and in a physical system using the da Vinci surgical\nrobot. We find that HULK is able to untangle cables with dense figure-eight and\noverhand knots and generalize to varied textures and appearances. We compare\ntwo variants of HULK to three baselines and observe that HULK achieves 43.3%\nhigher success rates on a physical system compared to the next best baseline.\nHULK successfully untangles a cable from a dense initial configuration\ncontaining up to two overhand and figure-eight knots in 97.9% of 378 simulation\nexperiments with an average of 12.1 actions per trial. In physical experiments,\nHULK achieves 61.7% untangling success, averaging 8.48 actions per trial.\nSupplementary material, code, and videos can be found at\nhttps://tinyurl.com/y3a88ycu.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:29:01 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Grannen", "Jennifer", ""], ["Sundaresan", "Priya", ""], ["Thananjeyan", "Brijen", ""], ["Ichnowski", "Jeffrey", ""], ["Balakrishna", "Ashwin", ""], ["Hwang", "Minho", ""], ["Viswanath", "Vainavi", ""], ["Laskey", "Michael", ""], ["Gonzalez", "Joseph E.", ""], ["Goldberg", "Ken", ""]]}, {"id": "2011.05007", "submitter": "Quynh Ngoc Thi Do", "authors": "Quynh Do, Judith Gaspers, Tobias Roding, Melanie Bradford", "title": "To What Degree Can Language Borders Be Blurred In BERT-based\n  Multilingual Spoken Language Understanding?", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the question as to what degree a BERT-based multilingual\nSpoken Language Understanding (SLU) model can transfer knowledge across\nlanguages. Through experiments we will show that, although it works\nsubstantially well even on distant language groups, there is still a gap to the\nideal multilingual performance. In addition, we propose a novel BERT-based\nadversarial model architecture to learn language-shared and language-specific\nrepresentations for multilingual SLU. Our experimental results prove that the\nproposed model is capable of narrowing the gap to the ideal multilingual\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:59:24 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Do", "Quynh", ""], ["Gaspers", "Judith", ""], ["Roding", "Tobias", ""], ["Bradford", "Melanie", ""]]}, {"id": "2011.05047", "submitter": "Sayan Chakraborty", "authors": "Sayan Chakraborty, Smit Shah, Kiumars Soltani, Anna Swigart, Luyao\n  Yang, Kyle Buckingham", "title": "Building an Automated and Self-Aware Anomaly Detection System", "comments": "11 pages, 5 figures, Accepted to 2020 IEEE International Conference\n  on Big Data (IEEE BigData 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations rely heavily on time series metrics to measure and model key\naspects of operational and business performance. The ability to reliably detect\nissues with these metrics is imperative to identifying early indicators of\nmajor problems before they become pervasive. It can be very challenging to\nproactively monitor a large number of diverse and constantly changing time\nseries for anomalies, so there are often gaps in monitoring coverage, disabled\nor ignored monitors due to false positive alarms, and teams resorting to manual\ninspection of charts to catch problems. Traditionally, variations in the data\ngeneration processes and patterns have required strong modeling expertise to\ncreate models that accurately flag anomalies. In this paper, we describe an\nanomaly detection system that overcomes this common challenge by keeping track\nof its own performance and making changes as necessary to each model without\nrequiring manual intervention. We demonstrate that this novel approach\noutperforms available alternatives on benchmark datasets in many scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:19:07 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chakraborty", "Sayan", ""], ["Shah", "Smit", ""], ["Soltani", "Kiumars", ""], ["Swigart", "Anna", ""], ["Yang", "Luyao", ""], ["Buckingham", "Kyle", ""]]}, {"id": "2011.05049", "submitter": "Zongheng Tang", "authors": "Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu\n  Jiang, Qian Yu, Dong Xu", "title": "Human-centric Spatio-Temporal Video Grounding With Visual Transformers", "comments": "Accept at TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel task - Humancentric Spatio-Temporal Video\nGrounding (HC-STVG). Unlike the existing referring expression tasks in images\nor videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal\ntube of the target person from an untrimmed video based on a given textural\ndescription. This task is useful, especially for healthcare and\nsecurity-related applications, where the surveillance videos can be extremely\nlong but only a specific person during a specific period of time is concerned.\nHC-STVG is a video grounding task that requires both spatial (where) and\ntemporal (when) localization. Unfortunately, the existing grounding methods\ncannot handle this task well. We tackle this task by proposing an effective\nbaseline method named Spatio-Temporal Grounding with Visual Transformers\n(STGVT), which utilizes Visual Transformers to extract cross-modal\nrepresentations for video-sentence matching and temporal localization. To\nfacilitate this task, we also contribute an HC-STVG dataset consisting of 5,660\nvideo-sentence pairs on complex multi-person scenes. Specifically, each video\nlasts for 20 seconds, pairing with a natural query sentence with an average of\n17.25 words. Extensive experiments are conducted on this dataset, demonstrating\nthe newly-proposed method outperforms the existing baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:23:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 06:51:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tang", "Zongheng", ""], ["Liao", "Yue", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Jin", "Xiaojie", ""], ["Jiang", "Hongxu", ""], ["Yu", "Qian", ""], ["Xu", "Dong", ""]]}, {"id": "2011.05064", "submitter": "Herman Ho-Man Yau", "authors": "Herman Yau, Chris Russell, Simon Hadfield,", "title": "What Did You Think Would Happen? Explaining Agent Behaviour Through\n  Intended Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel form of explanation for Reinforcement Learning, based\naround the notion of intended outcome. These explanations describe the outcome\nan agent is trying to achieve by its actions. We provide a simple proof that\ngeneral methods for post-hoc explanations of this nature are impossible in\ntraditional reinforcement learning. Rather, the information needed for the\nexplanations must be collected in conjunction with training the agent. We\nderive approaches designed to extract local explanations based on intention for\nseveral variants of Q-function approximation and prove consistency between the\nexplanations and the Q-values learned. We demonstrate our method on multiple\nreinforcement learning problems, and provide code to help researchers\nintrospecting their RL environments and algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:05:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Yau", "Herman", ""], ["Russell", "Chris", ""], ["Hadfield", "Simon", ""]]}, {"id": "2011.05110", "submitter": "Jan-Hinrich N\\\"olke", "authors": "Jan-Hinrich N\\\"olke, Tim Adler, Janek Gr\\\"ohl, Thomas Kirchner, Lynton\n  Ardizzone, Carsten Rother, Ullrich K\\\"othe, Lena Maier-Hein", "title": "Invertible Neural Networks for Uncertainty Quantification in\n  Photoacoustic Imaging", "comments": "7 pages, 4 figures, submitted to \"Bildverarbeitung f\\\"ur die Medizin\n  (BVM) 2021\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral photoacoustic imaging (PAI) is an emerging imaging modality\nwhich enables the recovery of functional tissue parameters such as blood\noxygenation. However, the underlying inverse problems are potentially\nill-posed, meaning that radically different tissue properties may - in theory -\nyield comparable measurements. In this work, we present a new approach for\nhandling this specific type of uncertainty by leveraging the concept of\nconditional invertible neural networks (cINNs). Specifically, we propose going\nbeyond commonly used point estimates for tissue oxygenation and converting\nsingle-pixel initial pressure spectra to the full posterior probability\ndensity. This way, the inherent ambiguity of a problem can be encoded with\nmultiple modes in the output. Based on the presented architecture, we\ndemonstrate two use cases which leverage this information to not only detect\nand quantify but also to compensate for uncertainties: (1) photoacoustic device\ndesign and (2) optimization of photoacoustic image acquisition. Our in silico\nstudies demonstrate the potential of the proposed methodology to become an\nimportant building block for uncertainty-aware reconstruction of physiological\nparameters with PAI.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:17:18 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 18:11:01 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["N\u00f6lke", "Jan-Hinrich", ""], ["Adler", "Tim", ""], ["Gr\u00f6hl", "Janek", ""], ["Kirchner", "Thomas", ""], ["Ardizzone", "Lynton", ""], ["Rother", "Carsten", ""], ["K\u00f6the", "Ullrich", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2011.05126", "submitter": "Feihu Che", "authors": "Feihu Che, Guohua Yang, Dawei Zhang, Jianhua Tao, Pengpeng Shao, Tong\n  Liu", "title": "Self-supervised Graph Representation Learning via Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks~(GNNs) apply deep learning techniques to\ngraph-structured data and have achieved promising performance in graph\nrepresentation learning. However, existing GNNs rely heavily on enough labels\nor well-designed negative samples. To address these issues, we propose a new\nself-supervised graph representation method: deep graph bootstrapping~(DGB).\nDGB consists of two neural networks: online and target networks, and the input\nof them are different augmented views of the initial graph. The online network\nis trained to predict the target network while the target network is updated\nwith a slow-moving average of the online network, which means the online and\ntarget networks can learn from each other. As a result, the proposed DGB can\nlearn graph representation without negative examples in an unsupervised manner.\nIn addition, we summarize three kinds of augmentation methods for\ngraph-structured data and apply them to the DGB. Experiments on the benchmark\ndatasets show the DGB performs better than the current state-of-the-art methods\nand how the augmentation methods affect the performances.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:47:29 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 01:14:40 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Che", "Feihu", ""], ["Yang", "Guohua", ""], ["Zhang", "Dawei", ""], ["Tao", "Jianhua", ""], ["Shao", "Pengpeng", ""], ["Liu", "Tong", ""]]}, {"id": "2011.05138", "submitter": "Srivamshi Pittala", "authors": "Srivamshi Pittala, William Koehler, Jonathan Deans, Daniel Salinas,\n  Martin Bringmann, Katharina Sophia Volz, Berk Kapicioglu", "title": "Relation-weighted Link Prediction for Disease Gene Identification", "comments": "4th Knowledge Representation and Reasoning Meets Machine Learning\n  Workshop (KR2ML), NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of disease genes, which are a set of genes associated with a\ndisease, plays an important role in understanding and curing diseases. In this\npaper, we present a biomedical knowledge graph designed specifically for this\nproblem, propose a novel machine learning method that identifies disease genes\non such graphs by leveraging recent advances in network biology and graph\nrepresentation learning, study the effects of various relation types on\nprediction performance, and empirically demonstrate that our algorithms\noutperform its closest state-of-the-art competitor in disease gene\nidentification by 24.1%. We also show that we achieve higher precision than\nOpen Targets, the leading initiative for target identification, with respect to\npredicting drug targets in clinical trials for Parkinson's disease.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:09:33 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:20:31 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 14:48:00 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Pittala", "Srivamshi", ""], ["Koehler", "William", ""], ["Deans", "Jonathan", ""], ["Salinas", "Daniel", ""], ["Bringmann", "Martin", ""], ["Volz", "Katharina Sophia", ""], ["Kapicioglu", "Berk", ""]]}, {"id": "2011.05142", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Tiarnan D. L. Keenan, Alexis Allot, Yifan Peng, Elvira\n  Agr\\'on, Amitha Domalpally, Caroline C. W. Klaver, Daniel T. Luttikhuizen,\n  Marcus H. Colyer, Catherine A. Cukras, Henry E. Wiley, M. Teresa Magone,\n  Chantal Cousineau-Krieger, Wai T. Wong, Yingying Zhu, Emily Y. Chew, Zhiyong\n  Lu (for the AREDS2 Deep Learning Research Group)", "title": "Multi-modal, multi-task, multi-attention (M3) deep learning detection of\n  reticular pseudodrusen: towards automated and accessible classification of\n  age-related macular degeneration", "comments": "5 figures and 4 tables, To appear in Journal of the American Medical\n  Informatics Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective Reticular pseudodrusen (RPD), a key feature of age-related macular\ndegeneration (AMD), are poorly detected by human experts on standard color\nfundus photography (CFP) and typically require advanced imaging modalities such\nas fundus autofluorescence (FAF). The objective was to develop and evaluate the\nperformance of a novel 'M3' deep learning framework on RPD detection. Materials\nand Methods A deep learning framework M3 was developed to detect RPD presence\naccurately using CFP alone, FAF alone, or both, employing >8000 CFP-FAF image\npairs obtained prospectively (Age-Related Eye Disease Study 2). The M3\nframework includes multi-modal (detection from single or multiple image\nmodalities), multi-task (training different tasks simultaneously to improve\ngeneralizability), and multi-attention (improving ensembled feature\nrepresentation) operation. Performance on RPD detection was compared with\nstate-of-the-art deep learning models and 13 ophthalmologists; performance on\ndetection of two other AMD features (geographic atrophy and pigmentary\nabnormalities) was also evaluated. Results For RPD detection, M3 achieved area\nunder receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP\nalone, FAF alone, and both, respectively. M3 performance on CFP was very\nsubstantially superior to human retinal specialists (median F1-score 0.644\nversus 0.350). External validation (on Rotterdam Study, Netherlands)\ndemonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also\naccurately detected geographic atrophy and pigmentary abnormalities (AUROC\n0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion\nThis study demonstrates the successful development, robust evaluation, and\nexternal validation of a novel deep learning framework that enables accessible,\naccurate, and automated AMD diagnosis and prognosis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:26:38 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 13:26:39 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Qingyu", "", "for the AREDS2 Deep Learning Research Group"], ["Keenan", "Tiarnan D. L.", "", "for the AREDS2 Deep Learning Research Group"], ["Allot", "Alexis", "", "for the AREDS2 Deep Learning Research Group"], ["Peng", "Yifan", "", "for the AREDS2 Deep Learning Research Group"], ["Agr\u00f3n", "Elvira", "", "for the AREDS2 Deep Learning Research Group"], ["Domalpally", "Amitha", "", "for the AREDS2 Deep Learning Research Group"], ["Klaver", "Caroline C. W.", "", "for the AREDS2 Deep Learning Research Group"], ["Luttikhuizen", "Daniel T.", "", "for the AREDS2 Deep Learning Research Group"], ["Colyer", "Marcus H.", "", "for the AREDS2 Deep Learning Research Group"], ["Cukras", "Catherine A.", "", "for the AREDS2 Deep Learning Research Group"], ["Wiley", "Henry E.", "", "for the AREDS2 Deep Learning Research Group"], ["Magone", "M. Teresa", "", "for the AREDS2 Deep Learning Research Group"], ["Cousineau-Krieger", "Chantal", "", "for the AREDS2 Deep Learning Research Group"], ["Wong", "Wai T.", "", "for the AREDS2 Deep Learning Research Group"], ["Zhu", "Yingying", "", "for the AREDS2 Deep Learning Research Group"], ["Chew", "Emily Y.", "", "for the AREDS2 Deep Learning Research Group"], ["Lu", "Zhiyong", "", "for the AREDS2 Deep Learning Research Group"]]}, {"id": "2011.05149", "submitter": "Eva van Weenen", "authors": "Eva van Weenen and Stefan Feuerriegel", "title": "Estimating Risk-Adjusted Hospital Performance", "comments": "Accepted at the 2020 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of healthcare provided by hospitals is subject to considerable\nvariability. Consequently, accurate measurements of hospital performance are\nessential for various decision-makers, including patients, hospital managers\nand health insurers. Hospital performance is assessed via the health outcomes\nof their patients. However, as the risk profiles of patients between hospitals\nvary, measuring hospital performance requires adjustment for patient risk. This\ntask is formalized in the state-of-the-art procedure through a hierarchical\ngeneralized linear model, that isolates hospital fixed-effects from the effect\nof patient risk on health outcomes. Due to the linear nature of this approach,\nany non-linear relations or interaction terms between risk variables are\nneglected.\n  In this work, we propose a novel method for measuring hospital performance\nadjusted for patient risk. This method captures non-linear relationships as\nwell as interactions among patient risk variables, specifically the effect of\nco-occurring health conditions on health outcomes. For this purpose, we develop\na tailored neural network architecture that is partially interpretable: a\nnon-linear part is used to encode risk factors, while a linear structure models\nhospital fixed-effects, such that the risk-adjusted hospital performance can be\nestimated. We base our evaluation on more than 13 million patient admissions\nacross almost 1,900 US hospitals as provided by the Nationwide Readmissions\nDatabase. Our model improves the ROC-AUC over the state-of-the-art by 4.1\npercent. These findings demonstrate that a large portion of the variance in\nhealth outcomes can be attributed to non-linear relationships between patient\nrisk variables and implicate that the current approach of measuring hospital\nperformance should be expanded.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:14:51 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 10:43:05 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["van Weenen", "Eva", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2011.05151", "submitter": "M. F. Mridha", "authors": "Muhammad Mohsin Kabir, Abu Quwsar Ohi, M. F. Mridha", "title": "A Multi-Plant Disease Diagnosis Method using Convolutional Neural\n  Network", "comments": "Accepted in book chapter \"CVML in Agriculture\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disease that limits a plant from its maximal capacity is defined as plant\ndisease. From the perspective of agriculture, diagnosing plant disease is\ncrucial, as diseases often limit plants' production capacity. However, manual\napproaches to recognize plant diseases are often temporal, challenging, and\ntime-consuming. Therefore, computerized recognition of plant diseases is highly\ndesired in the field of agricultural automation. Due to the recent improvement\nof computer vision, identifying diseases using leaf images of a particular\nplant has already been introduced. Nevertheless, the most introduced model can\nonly diagnose diseases of a specific plant. Hence, in this chapter, we\ninvestigate an optimal plant disease identification model combining the\ndiagnosis of multiple plants. Despite relying on multi-class classification,\nthe model inherits a multilabel classification method to identify the plant and\nthe type of disease in parallel. For the experiment and evaluation, we\ncollected data from various online sources that included leaf images of six\nplants, including tomato, potato, rice, corn, grape, and apple. In our\ninvestigation, we implement numerous popular convolutional neural network (CNN)\narchitectures. The experimental results validate that the Xception and DenseNet\narchitectures perform better in multi-label plant disease classification tasks.\nThrough architectural investigation, we imply that skip connections, spatial\nconvolutions, and shorter hidden layer connectivity cause better results in\nplant disease classification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:18:52 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kabir", "Muhammad Mohsin", ""], ["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""]]}, {"id": "2011.05158", "submitter": "Pablo Samuel Castro", "authors": "Pablo Samuel Castro", "title": "GANterpretations", "comments": "In 4th Workshop on Machine Learning for Creativity and Design at\n  NeurIPS 2020, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of Generative Adversarial Networks (GANs) [Goodfellow\net al., 2014] there has been a regular stream of both technical advances (e.g.,\nArjovsky et al. [2017]) and creative uses of these generative models (e.g.,\n[Karras et al., 2019, Zhu et al., 2017, Jin et al., 2017]). In this work we\npropose an approach for using the power of GANs to automatically generate\nvideos to accompany audio recordings by aligning to spectral properties of the\nrecording. This allows musicians to explore new forms of multi-modal creative\nexpression, where musical performance can induce an AI-generated musical video\nthat is guided by said performance, as well as a medium for creating a visual\nnarrative to follow a storyline (similar to what was proposed by Frosst and\nKereliuk [2019]).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:08:40 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Castro", "Pablo Samuel", ""]]}, {"id": "2011.05174", "submitter": "Claire Pagetti", "authors": "Arthur Clavi\\`ere, Eric Asselin, Christophe Garion (ISAE-SUPAERO),\n  Claire Pagetti (ANITI)", "title": "Safety Verification of Neural Network Controlled Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a system-level approach for verifying the safety of\nneural network controlled systems, combining a continuous-time physical system\nwith a discrete-time neural network based controller. We assume a generic model\nfor the controller that can capture both simple and complex behaviours\ninvolving neural networks. Based on this model, we perform a reachability\nanalysis that soundly approximates the reachable states of the overall system,\nallowing to achieve a formal proof of safety. To this end, we leverage both\nvalidated simulation to approximate the behaviour of the physical system and\nabstract interpretation to approximate the behaviour of the controller. We\nevaluate the applicability of our approach using a real-world use case.\nMoreover, we show that our approach can provide valuable information when the\nsystem cannot be proved totally safe.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:26:38 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Clavi\u00e8re", "Arthur", "", "ISAE-SUPAERO"], ["Asselin", "Eric", "", "ISAE-SUPAERO"], ["Garion", "Christophe", "", "ISAE-SUPAERO"], ["Pagetti", "Claire", "", "ANITI"]]}, {"id": "2011.05180", "submitter": "Daniel Rodriguez Criado", "authors": "Daniel Rodriguez-Criado and Pilar Bachiller and Luis J. Manso", "title": "Generation of Human-aware Navigation Maps using Graph Neural Networks", "comments": "6 pages, 4 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimising the discomfort caused by robots when navigating in social\nsituations is crucial for them to be accepted. The paper presents a machine\nlearning-based framework that bootstraps existing one-dimensional datasets to\ngenerate a cost map dataset and a model combining Graph Neural Network and\nConvolutional Neural Network layers to produce cost maps for human-aware\nnavigation in real-time. The proposed framework is evaluated against the\noriginal one-dimensional dataset and in simulated navigation tasks. The results\noutperform similar state-of-the-art-methods considering the accuracy on the\ndataset and the navigation metrics used. The applications of the proposed\nframework are not limited to human-aware navigation, it could be applied to\nother fields where map generation is needed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:32:14 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Rodriguez-Criado", "Daniel", ""], ["Bachiller", "Pilar", ""], ["Manso", "Luis J.", ""]]}, {"id": "2011.05280", "submitter": "Hanle Zheng", "authors": "Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu and Guoqi Li", "title": "Going Deeper With Directly-Trained Larger Spiking Neural Networks", "comments": "12 pages, 6 figures, conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) are promising in a bio-plausible coding for\nspatio-temporal information and event-driven signal processing, which is very\nsuited for energy-efficient implementation in neuromorphic hardware. However,\nthe unique working mode of SNNs makes them more difficult to train than\ntraditional networks. Currently, there are two main routes to explore the\ntraining of deep SNNs with high performance. The first is to convert a\npre-trained ANN model to its SNN version, which usually requires a long coding\nwindow for convergence and cannot exploit the spatio-temporal features during\ntraining for solving temporal tasks. The other is to directly train SNNs in the\nspatio-temporal domain. But due to the binary spike activity of the firing\nfunction and the problem of gradient vanishing or explosion, current methods\nare restricted to shallow architectures and thereby difficult in harnessing\nlarge-scale datasets (e.g. ImageNet). To this end, we propose a\nthreshold-dependent batch normalization (tdBN) method based on the emerging\nspatio-temporal backpropagation, termed \"STBP-tdBN\", enabling direct training\nof a very deep SNN and the efficient implementation of its inference on\nneuromorphic hardware. With the proposed method and elaborated shortcut\nconnection, we significantly extend directly-trained SNNs from a shallow\nstructure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we\ntheoretically analyze the effectiveness of our method based on \"Block Dynamical\nIsometry\" theory. Finally, we report superior accuracy results including 93.15\n% on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few\ntimesteps. To our best knowledge, it's the first time to explore the\ndirectly-trained deep SNNs with high performance on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 07:15:52 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 06:50:45 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Zheng", "Hanle", ""], ["Wu", "Yujie", ""], ["Deng", "Lei", ""], ["Hu", "Yifan", ""], ["Li", "Guoqi", ""]]}, {"id": "2011.05281", "submitter": "Sidhdharth Sikka", "authors": "Sidhdharth Sikka, Harshvardhan Sikka", "title": "A Genetic Algorithm Based Approach for Satellite Autonomy", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17934.18247/2", "report-no": null, "categories": "cs.NE cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Autonomous spacecraft maneuver planning using an evolutionary algorithmic\napproach is investigated. Simulated spacecraft were placed into four different\ninitial orbits. Each was allowed a string of thirty delta-v impulse maneuvers\nin six cartesian directions, the positive and negative x, y and z directions.\nThe goal of the spacecraft maneuver string was to, starting from some non-polar\nstarting orbit, place the spacecraft into a polar, low eccentricity orbit. A\ngenetic algorithm was implemented, using a mating, fitness, mutation and\ncrossover scheme for impulse strings. The genetic algorithm was successfully\nable to produce this result for all the starting orbits. Performance and future\nwork is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:41:30 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 21:47:37 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sikka", "Sidhdharth", ""], ["Sikka", "Harshvardhan", ""]]}, {"id": "2011.05286", "submitter": "Kelvin Xu", "authors": "Kelvin Xu, Siddharth Verma, Chelsea Finn, Sergey Levine", "title": "Continual Learning of Control Primitives: Skill Discovery via\n  Reset-Games", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning has the potential to automate the acquisition of\nbehavior in complex settings, but in order for it to be successfully deployed,\na number of practical challenges must be addressed. First, in real world\nsettings, when an agent attempts a task and fails, the environment must somehow\n\"reset\" so that the agent can attempt the task again. While easy in simulation,\nthis could require considerable human effort in the real world, especially if\nthe number of trials is very large. Second, real world learning often involves\ncomplex, temporally extended behavior that is often difficult to acquire with\nrandom exploration. While these two problems may at first appear unrelated, in\nthis work, we show how a single method can allow an agent to acquire skills\nwith minimal supervision while removing the need for resets. We do this by\nexploiting the insight that the need to \"reset\" an agent to a broad set of\ninitial states for a learning task provides a natural setting to learn a\ndiverse set of \"reset-skills\". We propose a general-sum game formulation that\nbalances the objectives of resetting and learning skills, and demonstrate that\nthis approach improves performance on reset-free tasks, and additionally show\nthat the skills we obtain can be used to significantly accelerate downstream\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 18:07:44 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Xu", "Kelvin", ""], ["Verma", "Siddharth", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "2011.05287", "submitter": "Sayan Sinha", "authors": "Aadi Swadipto Mondal and Rakesh Bal and Sayan Sinha, Gourab K Patro", "title": "Two-Sided Fairness in Non-Personalised Recommendations", "comments": "Accepted in AAAI 2021 Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are one of the most widely used services on several\nonline platforms to suggest potential items to the end-users. These services\noften use different machine learning techniques for which fairness is a\nconcerning factor, especially when the downstream services have the ability to\ncause social ramifications. Thus, focusing on the non-personalised (global)\nrecommendations in news media platforms (e.g., top-k trending topics on\nTwitter, top-k news on a news platform, etc.), we discuss on two specific\nfairness concerns together (traditionally studied separately)---user fairness\nand organisational fairness. While user fairness captures the idea of\nrepresenting the choices of all the individual users in the case of global\nrecommendations, organisational fairness tries to ensure\npolitically/ideologically balanced recommendation sets. This makes user\nfairness a user-side requirement and organisational fairness a platform-side\nrequirement. For user fairness, we test with methods from social choice theory,\ni.e., various voting rules known to better represent user choices in their\nresults. Even in our application of voting rules to the recommendation setup,\nwe observe high user satisfaction scores. Now for organisational fairness, we\npropose a bias metric which measures the aggregate ideological bias of a\nrecommended set of items (articles). Analysing the results obtained from voting\nrule-based recommendation, we find that while the well-known voting rules are\nbetter from the user side, they show high bias values and clearly not suitable\nfor organisational requirements of the platforms. Thus, there is a need to\nbuild an encompassing mechanism by cohesively bridging ideas of user fairness\nand organisational fairness. In this abstract paper, we intend to frame the\nelementary ideas along with the clear motivation behind the requirement of such\na mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 18:11:37 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mondal", "Aadi Swadipto", ""], ["Bal", "Rakesh", ""], ["Sinha", "Sayan", ""], ["Patro", "Gourab K", ""]]}, {"id": "2011.05302", "submitter": "Stefanos Antaris", "authors": "Stefanos Antaris, Dimitrios Rafailidis, Mohammad Aliannejadi", "title": "On Estimating the Training Cost of Conversational Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational recommendation systems have recently gain a lot of attention,\nas users can continuously interact with the system over multiple conversational\nturns. However, conversational recommendation systems are based on complex\nneural architectures, thus the training cost of such models is high. To shed\nlight on the high computational training time of state-of-the art\nconversational models, we examine five representative strategies and\ndemonstrate this issue. Furthermore, we discuss possible ways to cope with the\nhigh training cost following knowledge distillation strategies, where we detail\nthe key challenges to reduce the online inference time of the high number of\nmodel parameters in conversational recommendation systems\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 18:37:10 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Antaris", "Stefanos", ""], ["Rafailidis", "Dimitrios", ""], ["Aliannejadi", "Mohammad", ""]]}, {"id": "2011.05354", "submitter": "Maxime Mulamba Ke Tchomba", "authors": "Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele\n  Lombardi, Victor Bucarey, Tias Guns", "title": "Contrastive Losses and Solution Caching for Predict-and-Optimize", "comments": "Accepted at IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many decision-making processes involve solving a combinatorial optimization\nproblem with uncertain input that can be estimated from historic data.\nRecently, problems in this class have been successfully addressed via\nend-to-end learning approaches, which rely on solving one optimization problem\nfor each training instance at every epoch. In this context, we provide two\ndistinct contributions. First, we use a Noise Contrastive approach to motivate\na family of surrogate loss functions, based on viewing non-optimal solutions as\nnegative examples. Second, we address a major bottleneck of all\npredict-and-optimize approaches, i.e. the need to frequently recompute optimal\nsolutions at training time. This is done via a solver-agnostic solution caching\nscheme, and by replacing optimization calls with a lookup in the solution\ncache. The method is formally based on an inner approximation of the feasible\nspace and, combined with a cache lookup strategy, provides a controllable\ntrade-off between training time and accuracy of the loss approximation. We\nempirically show that even a very slow growth rate is enough to match the\nquality of state-of-the-art methods, at a fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:09:12 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 10:39:33 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mulamba", "Maxime", ""], ["Mandi", "Jayanta", ""], ["Diligenti", "Michelangelo", ""], ["Lombardi", "Michele", ""], ["Bucarey", "Victor", ""], ["Guns", "Tias", ""]]}, {"id": "2011.05367", "submitter": "Emilio Ferrara", "authors": "Samar Haider, Luca Luceri, Ashok Deb, Adam Badawy, Nanyun Peng, Emilio\n  Ferrara", "title": "Detecting Social Media Manipulation in Low-Resource Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social media have been deliberately used for malicious purposes, including\npolitical manipulation and disinformation. Most research focuses on\nhigh-resource languages. However, malicious actors share content across\ncountries and languages, including low-resource ones. Here, we investigate\nwhether and to what extent malicious actors can be detected in low-resource\nlanguage settings. We discovered that a high number of accounts posting in\nTagalog were suspended as part of Twitter's crackdown on interference\noperations after the 2016 US Presidential election. By combining text embedding\nand transfer learning, our framework can detect, with promising accuracy,\nmalicious users posting in Tagalog without any prior knowledge or training on\nmalicious content in that language. We first learn an embedding model for each\nlanguage, namely a high-resource language (English) and a low-resource one\n(Tagalog), independently. Then, we learn a mapping between the two latent\nspaces to transfer the detection model. We demonstrate that the proposed\napproach significantly outperforms state-of-the-art models, including BERT, and\nyields marked advantages in settings with very limited training data-the norm\nwhen dealing with detecting malicious activity in online platforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:38:03 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Haider", "Samar", ""], ["Luceri", "Luca", ""], ["Deb", "Ashok", ""], ["Badawy", "Adam", ""], ["Peng", "Nanyun", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2011.05373", "submitter": "Bowen Baker", "authors": "Bowen Baker", "title": "Emergent Reciprocity and Team Formation from Randomized Uncertain Social\n  Preferences", "comments": "to be published in NeurIPS 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) has shown recent success in\nincreasingly complex fixed-team zero-sum environments. However, the real world\nis not zero-sum nor does it have fixed teams; humans face numerous social\ndilemmas and must learn when to cooperate and when to compete. To successfully\ndeploy agents into the human world, it may be important that they be able to\nunderstand and help in our conflicts. Unfortunately, selfish MARL agents\ntypically fail when faced with social dilemmas. In this work, we show evidence\nof emergent direct reciprocity, indirect reciprocity and reputation, and team\nformation when training agents with randomized uncertain social preferences\n(RUSP), a novel environment augmentation that expands the distribution of\nenvironments agents play in. RUSP is generic and scalable; it can be applied to\nany multi-agent environment without changing the original underlying game\ndynamics or objectives. In particular, we show that with RUSP these behaviors\ncan emerge and lead to higher social welfare equilibria in both classic\nabstract social dilemmas like Iterated Prisoner's Dilemma as well in more\ncomplex intertemporal environments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 20:06:19 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Baker", "Bowen", ""]]}, {"id": "2011.05411", "submitter": "Nguyen Truong", "authors": "Nguyen Truong, Kai Sun, Siyao Wang, Florian Guitton, Yike Guo", "title": "Privacy Preservation in Federated Learning: An insightful survey from\n  the GDPR Perspective", "comments": "21 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Along with the blooming of AI and Machine Learning-based applications and\nservices, data privacy and security have become a critical challenge.\nConventionally, data is collected and aggregated in a data centre on which\nmachine learning models are trained. This centralised approach has induced\nsevere privacy risks to personal data leakage, misuse, and abuse. Furthermore,\nin the era of the Internet of Things and big data in which data is essentially\ndistributed, transferring a vast amount of data to a data centre for processing\nseems to be a cumbersome solution. This is not only because of the difficulties\nin transferring and sharing data across data sources but also the challenges on\ncomplying with rigorous data protection regulations and complicated\nadministrative procedures such as the EU General Data Protection Regulation\n(GDPR). In this respect, Federated learning (FL) emerges as a prospective\nsolution that facilitates distributed collaborative learning without disclosing\noriginal training data whilst naturally complying with the GDPR. Recent\nresearch has demonstrated that retaining data and computation on-device in FL\nis not sufficient enough for privacy-guarantee. This is because ML model\nparameters exchanged between parties in an FL system still conceal sensitive\ninformation, which can be exploited in some privacy attacks. Therefore, FL\nsystems shall be empowered by efficient privacy-preserving techniques to comply\nwith the GDPR. This article is dedicated to surveying on the state-of-the-art\nprivacy-preserving techniques which can be employed in FL in a systematic\nfashion, as well as how these techniques mitigate data security and privacy\nrisks. Furthermore, we provide insights into the challenges along with\nprospective approaches following the GDPR regulatory guidelines that an FL\nsystem shall implement to comply with the GDPR.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:41:25 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 18:06:17 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 02:07:36 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2021 13:15:01 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 12:32:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Truong", "Nguyen", ""], ["Sun", "Kai", ""], ["Wang", "Siyao", ""], ["Guitton", "Florian", ""], ["Guo", "Yike", ""]]}, {"id": "2011.05435", "submitter": "Yuxiang Wu", "authors": "Yuxiang Wu, Sebastian Riedel, Pasquale Minervini, Pontus Stenetorp", "title": "Don't Read Too Much into It: Adaptive Computation for Open-Domain\n  Question Answering", "comments": "11 pages, 9 figures, presented in EMNLP 2020 main conference and\n  SustaiNLP 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches to Open-Domain Question Answering consist of a light-weight\nretriever that selects a set of candidate passages, and a computationally\nexpensive reader that examines the passages to identify the correct answer.\nPrevious works have shown that as the number of retrieved passages increases,\nso does the performance of the reader. However, they assume all retrieved\npassages are of equal importance and allocate the same amount of computation to\nthem, leading to a substantial increase in computational cost. To reduce this\ncost, we propose the use of adaptive computation to control the computational\nbudget allocated for the passages to be read. We first introduce a technique\noperating on individual passages in isolation which relies on anytime\nprediction and a per-layer estimation of an early exit probability. We then\nintroduce SkylineBuilder, an approach for dynamically deciding on which passage\nto allocate computation at each step, based on a resource allocation policy\ntrained via reinforcement learning. Our results on SQuAD-Open show that\nadaptive computation with global prioritisation improves over several strong\nstatic and adaptive methods, leading to a 4.3x reduction in computation while\nretaining 95% performance of the full model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:37:56 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Wu", "Yuxiang", ""], ["Riedel", "Sebastian", ""], ["Minervini", "Pasquale", ""], ["Stenetorp", "Pontus", ""]]}, {"id": "2011.05440", "submitter": "Yasas Senarath", "authors": "Yasas Senarath, Saideep Nannapaneni, Hemant Purohit, Abhishek Dubey", "title": "Emergency Incident Detection from Crowdsourced Waze Data using Bayesian\n  Information Fusion", "comments": "8 pages, The 2020 IEEE/WIC/ACM International Joint Conference On Web\n  Intelligence And Intelligent Agent Technology (WI-IAT '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number of emergencies have increased over the years with the growth in\nurbanization. This pattern has overwhelmed the emergency services with limited\nresources and demands the optimization of response processes. It is partly due\nto traditional `reactive' approach of emergency services to collect data about\nincidents, where a source initiates a call to the emergency number (e.g., 911\nin U.S.), delaying and limiting the potentially optimal response. Crowdsourcing\nplatforms such as Waze provides an opportunity to develop a rapid, `proactive'\napproach to collect data about incidents through crowd-generated observational\nreports. However, the reliability of reporting sources and spatio-temporal\nuncertainty of the reported incidents challenge the design of such a proactive\napproach. Thus, this paper presents a novel method for emergency incident\ndetection using noisy crowdsourced Waze data. We propose a principled\ncomputational framework based on Bayesian theory to model the uncertainty in\nthe reliability of crowd-generated reports and their integration across space\nand time to detect incidents. Extensive experiments using data collected from\nWaze and the official reported incidents in Nashville, Tenessee in the U.S.\nshow our method can outperform strong baselines for both F1-score and AUC. The\napplication of this work provides an extensible framework to incorporate\ndifferent noisy data sources for proactive incident detection to improve and\noptimize emergency response operations in our communities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:45:03 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Senarath", "Yasas", ""], ["Nannapaneni", "Saideep", ""], ["Purohit", "Hemant", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2011.05446", "submitter": "Sneha Aenugu", "authors": "Sneha Aenugu", "title": "Perturbation-based exploration methods in deep reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on structured exploration placed emphasis on identifying\nnovel states in the state space and incentivizing the agent to revisit them\nthrough intrinsic reward bonuses. In this study, we question whether the\nperformance boost demonstrated through these methods is indeed due to the\ndiscovery of structure in exploratory schedule of the agent or is the benefit\nlargely attributed to the perturbations in the policy and reward space\nmanifested in pursuit of structured exploration. In this study we investigate\nthe effect of perturbations in policy and reward spaces on the exploratory\nbehavior of the agent. We proceed to show that simple acts of perturbing the\npolicy just before the softmax layer and introduction of sporadic reward\nbonuses into the domain can greatly enhance exploration in several domains of\nthe arcade learning environment. In light of these findings, we recommend\nbenchmarking any enhancements to structured exploration research against the\nbackdrop of noisy exploration.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:57:51 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Aenugu", "Sneha", ""]]}, {"id": "2011.05457", "submitter": "Zhenpeng Zhou", "authors": "Zhenpeng Zhou, Ahmad Beirami, Paul Crook, Pararth Shah, Rajen Subba,\n  and Alborz Geramifard", "title": "Resource Constrained Dialog Policy Learning via Differentiable Inductive\n  Logic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the needs of resource constrained dialog policy learning, we\nintroduce dialog policy via differentiable inductive logic (DILOG). We explore\nthe tasks of one-shot learning and zero-shot domain transfer with DILOG on\nSimDial and MultiWoZ. Using a single representative dialog from the restaurant\ndomain, we train DILOG on the SimDial dataset and obtain 99+% in-domain test\naccuracy. We also show that the trained DILOG zero-shot transfers to all other\ndomains with 99+% accuracy, proving the suitability of DILOG to slot-filling\ndialogs. We further extend our study to the MultiWoZ dataset achieving 90+%\ninform and success metrics. We also observe that these metrics are not\ncapturing some of the shortcomings of DILOG in terms of false positives,\nprompting us to measure an auxiliary Action F1 score. We show that DILOG is\n100x more data efficient than state-of-the-art neural approaches on MultiWoZ\nwhile achieving similar performance metrics. We conclude with a discussion on\nthe strengths and weaknesses of DILOG.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 23:28:00 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zhou", "Zhenpeng", ""], ["Beirami", "Ahmad", ""], ["Crook", "Paul", ""], ["Shah", "Pararth", ""], ["Subba", "Rajen", ""], ["Geramifard", "Alborz", ""]]}, {"id": "2011.05519", "submitter": "Dilusha Weeraddana Dr", "authors": "Dilusha Weeraddana, Nguyen Lu Dang Khoa, Lachlan O Neil, Weihong Wang,\n  and Chen Cai", "title": "Energy consumption forecasting using a stacked nonparametric Bayesian\n  approach", "comments": "Conference: ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the process of forecasting household energy consumption is\nstudied within the framework of the nonparametric Gaussian Process (GP), using\nmultiple short time series data. As we begin to use smart meter data to paint a\nclearer picture of residential electricity use, it becomes increasingly\napparent that we must also construct a detailed picture and understanding of\nconsumer's complex relationship with gas consumption. Both electricity and gas\nconsumption patterns are highly dependent on various factors, and the intricate\ninterplay of these factors is sophisticated. Moreover, since typical gas\nconsumption data is low granularity with very few time points, naive\napplication of conventional time-series forecasting techniques can lead to\nsevere over-fitting. Given these considerations, we construct a stacked GP\nmethod where the predictive posteriors of each GP applied to each task are used\nin the prior and likelihood of the next level GP. We apply our model to a\nreal-world dataset to forecast energy consumption in Australian households\nacross several states. We compare intuitively appealing results against other\ncommonly used machine learning techniques. Overall, the results indicate that\nthe proposed stacked GP model outperforms other forecasting techniques that we\ntested, especially when we have a multiple short time-series instances.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 02:27:00 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Weeraddana", "Dilusha", ""], ["Khoa", "Nguyen Lu Dang", ""], ["Neil", "Lachlan O", ""], ["Wang", "Weihong", ""], ["Cai", "Chen", ""]]}, {"id": "2011.05537", "submitter": "Lucas Rosenblatt", "authors": "Lucas Rosenblatt, Xiaoyan Liu, Samira Pouyanfar, Eduardo de Leon, Anuj\n  Desai, Joshua Allen", "title": "Differentially Private Synthetic Data: Applied Evaluations and\n  Enhancements", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning practitioners frequently seek to leverage the most\ninformative available data, without violating the data owner's privacy, when\nbuilding predictive models. Differentially private data synthesis protects\npersonal details from exposure, and allows for the training of differentially\nprivate machine learning models on privately generated datasets. But how can we\neffectively assess the efficacy of differentially private synthetic data? In\nthis paper, we survey four differentially private generative adversarial\nnetworks for data synthesis. We evaluate each of them at scale on five standard\ntabular datasets, and in two applied industry scenarios. We benchmark with\nnovel metrics from recent literature and other standard machine learning tools.\nOur results suggest some synthesizers are more applicable for different privacy\nbudgets, and we further demonstrate complicating domain-based tradeoffs in\nselecting an approach. We offer experimental learning on applied machine\nlearning scenarios with private internal data to researchers and practioners\nalike. In addition, we propose QUAIL, an ensemble-based modeling approach to\ngenerating synthetic data. We examine QUAIL's tradeoffs, and note circumstances\nin which it outperforms baseline differentially private supervised learning\nmodels under the same budget constraint.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 04:03:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Rosenblatt", "Lucas", ""], ["Liu", "Xiaoyan", ""], ["Pouyanfar", "Samira", ""], ["de Leon", "Eduardo", ""], ["Desai", "Anuj", ""], ["Allen", "Joshua", ""]]}, {"id": "2011.05554", "submitter": "Hao Xue", "authors": "Hao Xue and Flora D Salim", "title": "TERMCast: Temporal Relation Modeling for Effective Urban Flow\n  Forecasting", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-75762-5\\_58", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban flow forecasting is a challenging task, given the inherent periodic\ncharacteristics of urban flow patterns. To capture the periodicity, existing\nurban flow prediction approaches are often designed with closeness, period, and\ntrend components extracted from the urban flow sequence. However, these three\ncomponents are often considered separately in the prediction model. These\ncomponents have not been fully explored together and simultaneously\nincorporated in urban flow forecasting models. We introduce a novel urban flow\nforecasting architecture, TERMCast. A Transformer based long-term relation\nprediction module is explicitly designed to discover the periodicity and enable\nthe three components to be jointly modeled This module predicts the periodic\nrelation which is then used to yield the predicted urban flow tensor. To\nmeasure the consistency of the predicted periodic relation vector and the\nrelation vector inferred from the predicted urban flow tensor, we propose a\nconsistency module. A consistency loss is introduced in the training process to\nfurther improve the prediction performance. Through extensive experiments on\nthree real-world datasets, we demonstrate that TERMCast outperforms multiple\nstate-of-the-art methods. The effectiveness of each module in TERMCast has also\nbeen investigated.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:33:12 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 01:53:36 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xue", "Hao", ""], ["Salim", "Flora D", ""]]}, {"id": "2011.05570", "submitter": "Hamed Khorasgani", "authors": "Hamed Khorasgani, Haiyan Wang, Chetan Gupta", "title": "Challenges of Applying Deep Reinforcement Learning in Dynamic\n  Dispatching", "comments": "arXiv admin note: text overlap with arXiv:2008.10713", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic dispatching aims to smartly allocate the right resources to the right\nplace at the right time. Dynamic dispatching is one of the core problems for\noperations optimization in the mining industry. Theoretically, deep\nreinforcement learning (RL) should be a natural fit to solve this problem.\nHowever, the industry relies on heuristics or even human intuitions, which are\noften short-sighted and sub-optimal solutions. In this paper, we review the\nmain challenges in using deep RL to address the dynamic dispatching problem in\nthe mining industry.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 22:26:45 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Khorasgani", "Hamed", ""], ["Wang", "Haiyan", ""], ["Gupta", "Chetan", ""]]}, {"id": "2011.05588", "submitter": "Sergey Yarushev A", "authors": "Alexey Averkin and Sergey Yarushev", "title": "Deep Neural Networks and Neuro-Fuzzy Networks for Intellectual Analysis\n  of Economic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In tis paper we consider approaches for time series forecasting based on deep\nneural networks and neuro-fuzzy nets. Also, we make short review of researches\nin forecasting based on various models of ANFIS models. Deep Learning has\nproven to be an effective method for making highly accurate predictions from\ncomplex data sources. Also, we propose our models of DL and Neuro-Fuzzy\nNetworks for this task. Finally, we show possibility of using these models for\ndata science tasks. This paper presents also an overview of approaches for\nincorporating rule-based methodology into deep learning neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 06:21:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Averkin", "Alexey", ""], ["Yarushev", "Sergey", ""]]}, {"id": "2011.05596", "submitter": "Lawrence Chan", "authors": "Harry Giles, Lawrence Chan", "title": "Accounting for Human Learning when Inferring Human Preferences", "comments": "Accepted to the 2020 NeurIPS HAMLETS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inverse reinforcement learning (IRL) is a common technique for inferring\nhuman preferences from data. Standard IRL techniques tend to assume that the\nhuman demonstrator is stationary, that is that their policy $\\pi$ doesn't\nchange over time. In practice, humans interacting with a novel environment or\nperforming well on a novel task will change their demonstrations as they learn\nmore about the environment or task. We investigate the consequences of relaxing\nthis assumption of stationarity, in particular by modelling the human as\nlearning. Surprisingly, we find in some small examples that this can lead to\nbetter inference than if the human was stationary. That is, by observing a\ndemonstrator who is themselves learning, a machine can infer more than by\nobserving a demonstrator who is noisily rational. In addition, we find evidence\nthat misspecification can lead to poor inference, suggesting that modelling\nhuman learning is important, especially when the human is facing an unfamiliar\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 06:50:24 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 07:22:07 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Giles", "Harry", ""], ["Chan", "Lawrence", ""]]}, {"id": "2011.05602", "submitter": "Zheng Zhu", "authors": "Jintao Ke, Siyuan Feng, Zheng Zhu, Hai Yang, Jieping Ye", "title": "Joint predictions of multi-modal ride-hailing demands: a deep multi-task\n  multigraph learning-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ride-hailing platforms generally provide various service options to\ncustomers, such as solo ride services, shared ride services, etc. It is\ngenerally expected that demands for different service modes are correlated, and\nthe prediction of demand for one service mode can benefit from historical\nobservations of demands for other service modes. Moreover, an accurate joint\nprediction of demands for multiple service modes can help the platforms better\nallocate and dispatch vehicle resources. Although there is a large stream of\nliterature on ride-hailing demand predictions for one specific service mode,\nlittle efforts have been paid towards joint predictions of ride-hailing demands\nfor multiple service modes. To address this issue, we propose a deep multi-task\nmulti-graph learning approach, which combines two components: (1) multiple\nmulti-graph convolutional (MGC) networks for predicting demands for different\nservice modes, and (2) multi-task learning modules that enable knowledge\nsharing across multiple MGC networks. More specifically, two multi-task\nlearning structures are established. The first one is the regularized\ncross-task learning, which builds cross-task connections among the inputs and\noutputs of multiple MGC networks. The second one is the multi-linear\nrelationship learning, which imposes a prior tensor normal distribution on the\nweights of various MGC networks. Although there are no concrete bridges between\ndifferent MGC networks, the weights of these networks are constrained by each\nother and subject to a common prior distribution. Evaluated with the\nfor-hire-vehicle datasets in Manhattan, we show that our propose approach\noutperforms the benchmark algorithms in prediction accuracy for different\nride-hailing modes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:10:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Ke", "Jintao", ""], ["Feng", "Siyuan", ""], ["Zhu", "Zheng", ""], ["Yang", "Hai", ""], ["Ye", "Jieping", ""]]}, {"id": "2011.05605", "submitter": "Tanmay Samak", "authors": "Sivanathan Kandhasamy, Vinayagam Babu Kuppusamy, Tanmay Vilas Samak,\n  Chinmay Vilas Samak", "title": "Decentralized Motion Planning for Multi-Robot Navigation using Deep\n  Reinforcement Learning", "comments": "Accepted at IEEE International Conference on Intelligent Sustainable\n  Systems (ICISS) 2020", "journal-ref": "2020 3rd International Conference on Intelligent Sustainable\n  Systems (ICISS), Thoothukudi, India, 2020, pp. 709-716", "doi": "10.1109/ICISS49785.2020.9316033", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a decentralized motion planning framework for addressing\nthe task of multi-robot navigation using deep reinforcement learning. A custom\nsimulator was developed in order to experimentally investigate the navigation\nproblem of 4 cooperative non-holonomic robots sharing limited state information\nwith each other in 3 different settings. The notion of decentralized motion\nplanning with common and shared policy learning was adopted, which allowed\nrobust training and testing of this approach in a stochastic environment since\nthe agents were mutually independent and exhibited asynchronous motion\nbehavior. The task was further aggravated by providing the agents with a sparse\nobservation space and requiring them to generate continuous action commands so\nas to efficiently, yet safely navigate to their respective goal locations,\nwhile avoiding collisions with other dynamic peers and static obstacles at all\ntimes. The experimental results are reported in terms of quantitative measures\nand qualitative remarks for both training and deployment phases.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:35:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 18:19:32 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kandhasamy", "Sivanathan", ""], ["Kuppusamy", "Vinayagam Babu", ""], ["Samak", "Tanmay Vilas", ""], ["Samak", "Chinmay Vilas", ""]]}, {"id": "2011.05617", "submitter": "Yeong-Jia Roger Chu", "authors": "Yeong-Jia Roger Chu, Ting-Han Wei, Jin-Bo Huang, Yuan-Hao Chen, I-Chen\n  Wu", "title": "Sim-To-Real Transfer for Miniature Autonomous Car Racing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sim-to-real, a term that describes where a model is trained in a simulator\nthen transferred to the real world, is a technique that enables faster deep\nreinforcement learning (DRL) training. However, differences between the\nsimulator and the real world often cause the model to perform poorly in the\nreal world. Domain randomization is a way to bridge the sim-to-real gap by\nexposing the model to a wide range of scenarios so that it can generalize to\nreal-world situations. However, following domain randomization to train an\nautonomous car racing model with DRL can lead to undesirable outcomes. Namely,\na model trained with randomization tends to run slower; a higher completion\nrate on the testing track comes at the expense of longer lap times. This paper\naims to boost the robustness of a trained race car model without compromising\nracing lap times. For a training track and a testing track having the same\nshape (and same optimal paths), but with different lighting, background, etc.,\nwe first train a model (teacher model) that overfits the training track, moving\nalong a near optimal path. We then use this model to teach a student model the\ncorrect actions along with randomization. With our method, a model with 18.4\\%\ncompletion rate on the testing track is able to help teach a student model with\n52\\% completion. Moreover, over an average of 50 trials, the student is able to\nfinish a lap 0.23 seconds faster than the teacher. This 0.23 second gap is\nsignificant in tight races, with lap times of about 10 to 12 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:17:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chu", "Yeong-Jia Roger", ""], ["Wei", "Ting-Han", ""], ["Huang", "Jin-Bo", ""], ["Chen", "Yuan-Hao", ""], ["Wu", "I-Chen", ""]]}, {"id": "2011.05622", "submitter": "Chengpeng Hu", "authors": "Chengpeng Hu, Ziqi Wang, Tianye Shu, Yang Tao, Hao Tong, Julian\n  Togelius, Xin Yao and Jialin Liu", "title": "Robust Reinforcement Learning for General Video Game Playing", "comments": "10 pages, 4 figures.This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has successfully learned to play challenging board and\nvideo games. However, its generalization ability remains under-explored. The\nGeneral Video Game AI Learning Competition aims at designing agents that are\ncapable of learning to play different games levels that were unseen during\ntraining. This paper presents the games, entries and results of the 2020\nGeneral Video Game AI Learning Competition, held at the Sixteenth International\nConference on Parallel Problem Solving from Nature and the 2020 IEEE Conference\non Games. Three new games with sparse, periodic and dense rewards,\nrespectively, were designed for this competition and the test levels were\ngenerated by adding minor perturbations to training levels or combining\ntraining levels. In this paper, we also design a reinforcement learning agent,\ncalled Arcane, for general video game playing. We assume that it is more likely\nto observe similar local information in different levels rather than global\ninformation. Therefore, instead of directly inputting a single, raw pixel-based\nscreenshot of current game screen, Arcane takes the encoded, transformed global\nand local observations of the game screen as two simultaneous inputs, aiming at\nlearning local information for playing new levels. Two versions of Arcane,\nusing a stochastic or deterministic policy for decision-making during test,\nboth show robust performance on the game set of the 2020 General Video Game AI\nLearning Competition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:28:20 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Hu", "Chengpeng", ""], ["Wang", "Ziqi", ""], ["Shu", "Tianye", ""], ["Tao", "Yang", ""], ["Tong", "Hao", ""], ["Togelius", "Julian", ""], ["Yao", "Xin", ""], ["Liu", "Jialin", ""]]}, {"id": "2011.05632", "submitter": "Ashwin Balakrishna", "authors": "Michael Danielczuk, Ashwin Balakrishna, Daniel S. Brown, Shivin\n  Devgon, Ken Goldberg", "title": "Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping\n  Challenging Polyhedral Objects", "comments": "Conference on Robot Learning (CoRL) 2020. First two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been significant recent work on data-driven algorithms for learning\ngeneral-purpose grasping policies. However, these policies can consistently\nfail to grasp challenging objects which are significantly out of the\ndistribution of objects in the training data or which have very few high\nquality grasps. Motivated by such objects, we propose a novel problem setting,\nExploratory Grasping, for efficiently discovering reliable grasps on an unknown\npolyhedral object via sequential grasping, releasing, and toppling. We\nformalize Exploratory Grasping as a Markov Decision Process, study the\ntheoretical complexity of Exploratory Grasping in the context of reinforcement\nlearning and present an efficient bandit-style algorithm, Bandits for Online\nRapid Grasp Exploration Strategy (BORGES), which leverages the structure of the\nproblem to efficiently discover high performing grasps for each object stable\npose. BORGES can be used to complement any general-purpose grasping algorithm\nwith any grasp modality (parallel-jaw, suction, multi-fingered, etc) to learn\npolicies for objects in which they exhibit persistent failures. Simulation\nexperiments suggest that BORGES can significantly outperform both\ngeneral-purpose grasping pipelines and two other online learning algorithms and\nachieves performance within 5% of the optimal policy within 1000 and 8000\ntimesteps on average across 46 challenging objects from the Dex-Net adversarial\nand EGAD! object datasets, respectively. Initial physical experiments suggest\nthat BORGES can improve grasp success rate by 45% over a Dex-Net baseline with\njust 200 grasp attempts in the real world. See https://tinyurl.com/exp-grasping\nfor supplementary material and videos.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:42:30 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 01:21:35 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Danielczuk", "Michael", ""], ["Balakrishna", "Ashwin", ""], ["Brown", "Daniel S.", ""], ["Devgon", "Shivin", ""], ["Goldberg", "Ken", ""]]}, {"id": "2011.05661", "submitter": "Ashwin Balakrishna", "authors": "Han Yu Li, Michael Danielczuk, Ashwin Balakrishna, Vishal Satish, Ken\n  Goldberg", "title": "Accelerating Grasp Exploration by Leveraging Learned Priors", "comments": "Conference on Automation Science and Engineering (CASE) 2020. First\n  three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability of robots to grasp novel objects has industry applications in\ne-commerce order fulfillment and home service. Data-driven grasping policies\nhave achieved success in learning general strategies for grasping arbitrary\nobjects. However, these approaches can fail to grasp objects which have complex\ngeometry or are significantly outside of the training distribution. We present\na Thompson sampling algorithm that learns to grasp a given object with unknown\ngeometry using online experience. The algorithm leverages learned priors from\nthe Dexterity Network robot grasp planner to guide grasp exploration and\nprovide probabilistic estimates of grasp success for each stable pose of the\nnovel object. We find that seeding the policy with the Dex-Net prior allows it\nto more efficiently find robust grasps on these objects. Experiments suggest\nthat the best learned policy attains an average total reward 64.5% higher than\na greedy baseline and achieves within 5.7% of an oracle baseline when evaluated\nover 300,000 training runs across a set of 3000 object poses.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:42:56 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Han Yu", ""], ["Danielczuk", "Michael", ""], ["Balakrishna", "Ashwin", ""], ["Satish", "Vishal", ""], ["Goldberg", "Ken", ""]]}, {"id": "2011.05664", "submitter": "Stefanos Antaris", "authors": "Stefanos Antaris, Dimitrios Rafailidis", "title": "Distill2Vec: Dynamic Graph Representation Learning with Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic graph representation learning strategies are based on different\nneural architectures to capture the graph evolution over time. However, the\nunderlying neural architectures require a large amount of parameters to train\nand suffer from high online inference latency, that is several model parameters\nhave to be updated when new data arrive online. In this study we propose\nDistill2Vec, a knowledge distillation strategy to train a compact model with a\nlow number of trainable parameters, so as to reduce the latency of online\ninference and maintain the model accuracy high. We design a distillation loss\nfunction based on Kullback-Leibler divergence to transfer the acquired\nknowledge from a teacher model trained on offline data, to a small-size student\nmodel for online data. Our experiments with publicly available datasets show\nthe superiority of our proposed model over several state-of-the-art approaches\nwith relative gains up to 5% in the link prediction task. In addition, we\ndemonstrate the effectiveness of our knowledge distillation strategy, in terms\nof number of required parameters, where Distill2Vec achieves a compression\nratio up to 7:100 when compared with baseline approaches. For reproduction\npurposes, our implementation is publicly available at\nhttps://stefanosantaris.github.io/Distill2Vec.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:49:24 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Antaris", "Stefanos", ""], ["Rafailidis", "Dimitrios", ""]]}, {"id": "2011.05671", "submitter": "Stefanos Antaris", "authors": "Stefanos Antaris, Dimitrios Rafailidis", "title": "VStreamDRLS: Dynamic Graph Representation Learning with Self-Attention\n  for Enterprise Distributed Video Streaming Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Live video streaming has become a mainstay as a standard communication\nsolution for several enterprises worldwide. To efficiently stream high-quality\nlive video content to a large amount of offices, companies employ distributed\nvideo streaming solutions which rely on prior knowledge of the underlying\nevolving enterprise network. However, such networks are highly complex and\ndynamic. Hence, to optimally coordinate the live video distribution, the\navailable network capacity between viewers has to be accurately predicted. In\nthis paper we propose a graph representation learning technique on weighted and\ndynamic graphs to predict the network capacity, that is the weights of\nconnections/links between viewers/nodes. We propose VStreamDRLS, a graph neural\nnetwork architecture with a self-attention mechanism to capture the evolution\nof the graph structure of live video streaming events. VStreamDRLS employs the\ngraph convolutional network (GCN) model over the duration of a live video\nstreaming event and introduces a self-attention mechanism to evolve the GCN\nparameters. In doing so, our model focuses on the GCN weights that are relevant\nto the evolution of the graph and generate the node representation,\naccordingly. We evaluate our proposed approach on the link prediction task on\ntwo real-world datasets, generated by enterprise live video streaming events.\nThe duration of each event lasted an hour. The experimental results demonstrate\nthe effectiveness of VStreamDRLS when compared with state-of-the-art\nstrategies. Our evaluation datasets and implementation are publicly available\nat https://github.com/stefanosantaris/vstreamdrls\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 10:00:12 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Antaris", "Stefanos", ""], ["Rafailidis", "Dimitrios", ""]]}, {"id": "2011.05705", "submitter": "Stefanos Antaris", "authors": "Stefanos Antaris, Dimitrios Rafailidis, Sarunas Girdzijauskas", "title": "EGAD: Evolving Graph Representation Learning with Self-Attention and\n  Knowledge Distillation for Live Video Streaming Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we present a dynamic graph representation learning model on\nweighted graphs to accurately predict the network capacity of connections\nbetween viewers in a live video streaming event. We propose EGAD, a neural\nnetwork architecture to capture the graph evolution by introducing a\nself-attention mechanism on the weights between consecutive graph convolutional\nnetworks. In addition, we account for the fact that neural architectures\nrequire a huge amount of parameters to train, thus increasing the online\ninference latency and negatively influencing the user experience in a live\nvideo streaming event. To address the problem of the high online inference of a\nvast number of parameters, we propose a knowledge distillation strategy. In\nparticular, we design a distillation loss function, aiming to first pretrain a\nteacher model on offline data, and then transfer the knowledge from the teacher\nto a smaller student model with less parameters. We evaluate our proposed model\non the link prediction task on three real-world datasets, generated by live\nvideo streaming events. The events lasted 80 minutes and each viewer exploited\nthe distribution solution provided by the company Hive Streaming AB. The\nexperiments demonstrate the effectiveness of the proposed model in terms of\nlink prediction accuracy and number of required parameters, when evaluated\nagainst state-of-the-art approaches. In addition, we study the distillation\nperformance of the proposed model in terms of compression ratio for different\ndistillation strategies, where we show that the proposed model can achieve a\ncompression ratio up to 15:100, preserving high link prediction accuracy. For\nreproduction purposes, our evaluation datasets and implementation are publicly\navailable at https://stefanosantaris.github.io/EGAD.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:16:52 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Antaris", "Stefanos", ""], ["Rafailidis", "Dimitrios", ""], ["Girdzijauskas", "Sarunas", ""]]}, {"id": "2011.05715", "submitter": "Thilo Fryen", "authors": "Thilo Fryen, Manfred Eppe, Phuong D.H. Nguyen, Timo Gerkmann, Stefan\n  Wermter", "title": "Reinforcement Learning with Time-dependent Goals for Robotic Musicians", "comments": "Preprint, submitted to IEEE Robotics and Automation Letters (RA-L)\n  2021 with International Conference on Robotics and Automation Conference\n  Option (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a promising method to accomplish robotic control\ntasks. The task of playing musical instruments is, however, largely unexplored\nbecause it involves the challenge of achieving sequential goals - melodies -\nthat have a temporal dimension. In this paper, we address robotic musicianship\nby introducing a temporal extension to goal-conditioned reinforcement learning:\nTime-dependent goals. We demonstrate that these can be used to train a robotic\nmusician to play the theremin instrument. We train the robotic agent in\nsimulation and transfer the acquired policy to a real-world robotic\nthereminist. Supplemental video: https://youtu.be/jvC9mPzdQN4\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:37:36 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Fryen", "Thilo", ""], ["Eppe", "Manfred", ""], ["Nguyen", "Phuong D. H.", ""], ["Gerkmann", "Timo", ""], ["Wermter", "Stefan", ""]]}, {"id": "2011.05782", "submitter": "Pierre Aumjaud", "authors": "Pierre Aumjaud, David McAuliffe, Francisco Javier Rodr\\'iguez Lera,\n  Philip Cardiff", "title": "Reinforcement Learning Experiments and Benchmark for Solving Robotic\n  Reaching Tasks", "comments": null, "journal-ref": "Advances in Intelligent Systems and Computing, 1285 (2021),\n  318-331", "doi": "10.1007/978-3-030-62579-5_22", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning has shown great promise in robotics thanks to its\nability to develop efficient robotic control procedures through self-training.\nIn particular, reinforcement learning has been successfully applied to solving\nthe reaching task with robotic arms. In this paper, we define a robust,\nreproducible and systematic experimental procedure to compare the performance\nof various model-free algorithms at solving this task. The policies are trained\nin simulation and are then transferred to a physical robotic manipulator. It is\nshown that augmenting the reward signal with the Hindsight Experience Replay\nexploration technique increases the average return of off-policy agents between\n7 and 9 folds when the target position is initialised randomly at the beginning\nof each episode.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:00:49 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Aumjaud", "Pierre", ""], ["McAuliffe", "David", ""], ["Lera", "Francisco Javier Rodr\u00edguez", ""], ["Cardiff", "Philip", ""]]}, {"id": "2011.05805", "submitter": "Mashnoon Islam", "authors": "Mashnoon Islam, Redwanul Karim, Kalyan Roy, Saif Mahmood, Sadat\n  Hossain, M. Rashedur Rahman", "title": "Crime Prediction Using Multiple-ANFIS Architecture and Spatiotemporal\n  Data", "comments": "Accepted Version, 2018 IEEE International Conference on Intelligent\n  Systems (IS) September 25-27, Funchal - Madeira, Portugal", "journal-ref": null, "doi": "10.1109/IS.2018.8710564", "report-no": null, "categories": "cs.CY cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical values alone cannot bring the whole scenario of crime occurrences\nin the city of Dhaka. We need a better way to use these statistical values to\npredict crime occurrences and make the city a safer place to live. Proper\ndecision-making for the future is key in reducing the rate of criminal offenses\nin an area or a city. If the law enforcement bodies can allocate their\nresources efficiently for the future, the rate of crime in Dhaka can be brought\ndown to a minimum. In this work, we have made an initiative to provide an\neffective tool with which law enforcement officials and detectives can predict\ncrime occurrences ahead of time and take better decisions easily and quickly.\nWe have used several Fuzzy Inference Systems (FIS) and Adaptive Neuro-Fuzzy\nInference Systems (ANFIS) to predict the type of crime that is highly likely to\noccur at a certain place and time.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:57:30 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Islam", "Mashnoon", ""], ["Karim", "Redwanul", ""], ["Roy", "Kalyan", ""], ["Mahmood", "Saif", ""], ["Hossain", "Sadat", ""], ["Rahman", "M. Rashedur", ""]]}, {"id": "2011.05807", "submitter": "Jason Pittman", "authors": "Jason M. Pittman, Ashlyn Hanks", "title": "Detecting Synthetic Phenomenology in a Contained Artificial General\n  Intelligence", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-like intelligence in a machine is a contentious subject. Whether\nmankind should or should not pursue the creation of artificial general\nintelligence is hotly debated. As well, researchers have aligned in opposing\nfactions according to whether mankind can create it. For our purposes, we\nassume mankind can and will do so. Thus, it becomes necessary to contemplate\nhow to do so in a safe and trusted manner -- enter the idea of boxing or\ncontainment. As part of such thinking, we wonder how a phenomenology might be\ndetected given the operational constraints imposed by any potential containment\nsystem. Accordingly, this work provides an analysis of existing measures of\nphenomenology through qualia and extends those ideas into the context of a\ncontained artificial general intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:10:38 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Pittman", "Jason M.", ""], ["Hanks", "Ashlyn", ""]]}, {"id": "2011.05824", "submitter": "Philipp Kopper", "authors": "Philipp Kopper, Sebastian P\\\"olsterl, Christian Wachinger, Bernd\n  Bischl, Andreas Bender, David R\\\"ugamer", "title": "Semi-Structured Deep Piecewise Exponential Models", "comments": "8 pages, 3 figures, Accepted at the AAAI spring symposium: Survival\n  Prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a versatile framework for survival analysis that combines advanced\nconcepts from statistics with deep learning. The presented framework is based\non piecewise exponential models and thereby supports various survival tasks,\nsuch as competing risks and multi-state modeling, and further allows for\nestimation of time-varying effects and time-varying features. To also include\nmultiple data sources and higher-order interaction effects into the model, we\nembed the model class in a neural network and thereby enable the simultaneous\nestimation of both inherently interpretable structured regression inputs as\nwell as deep neural network components which can potentially process additional\nunstructured data sources. A proof of concept is provided by using the\nframework to predict Alzheimer's disease progression based on tabular and 3D\npoint cloud data and applying it to synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:41:19 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 20:28:54 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 13:32:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kopper", "Philipp", ""], ["P\u00f6lsterl", "Sebastian", ""], ["Wachinger", "Christian", ""], ["Bischl", "Bernd", ""], ["Bender", "Andreas", ""], ["R\u00fcgamer", "David", ""]]}, {"id": "2011.05857", "submitter": "Jun Jin", "authors": "Jun Jin, Daniel Graves, Cameron Haigh, Jun Luo and Martin Jagersand", "title": "Offline Learning of Counterfactual Perception as Prediction for\n  Real-World Robotic Reinforcement Learning", "comments": "submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for offline learning of counterfactual predictions to\naddress real world robotic reinforcement learning challenges. The proposed\nmethod encodes action-oriented visual observations as several \"what if\"\nquestions learned offline from prior experience using reinforcement learning\nmethods. These \"what if\" questions counterfactually predict how\naction-conditioned observation would evolve on multiple temporal scales if the\nagent were to stick to its current action. We show that combining these offline\ncounterfactual predictions along with online in-situ observations (e.g. force\nfeedback) allows efficient policy learning with only a sparse terminal\n(success/failure) reward. We argue that the learned predictions form an\neffective representation of the visual task, and guide the online exploration\ntowards high-potential success interactions (e.g. contact-rich regions).\nExperiments were conducted in both simulation and real-world scenarios for\nevaluation. Our results demonstrate that it is practical to train a\nreinforcement learning agent to perform real-world fine manipulation in about\nhalf a day, without hand engineered perception systems or calibrated\ninstrumentation. Recordings of the real robot training can be found via\nhttps://sites.google.com/view/realrl.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 15:45:17 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Jin", "Jun", ""], ["Graves", "Daniel", ""], ["Haigh", "Cameron", ""], ["Luo", "Jun", ""], ["Jagersand", "Martin", ""]]}, {"id": "2011.05910", "submitter": "Chung Hoon Hong", "authors": "Chung Hoon Hong, Yuan Liang, Sagnik Sinha Roy, Arushi Jain, Vihang\n  Agarwal, Ryan Draves, Zhizhuo Zhou, William Chen, Yujian Liu, Martha Miracky,\n  Lily Ge, Nikola Banovic, David Jurgens", "title": "Audrey: A Personalized Open-Domain Conversational Bot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational Intelligence requires that a person engage on informational,\npersonal and relational levels. Advances in Natural Language Understanding have\nhelped recent chatbots succeed at dialog on the informational level. However,\ncurrent techniques still lag for conversing with humans on a personal level and\nfully relating to them. The University of Michigan's submission to the Alexa\nPrize Grand Challenge 3, Audrey, is an open-domain conversational chat-bot that\naims to engage customers on these levels through interest driven conversations\nguided by customers' personalities and emotions. Audrey is built from\nsocially-aware models such as Emotion Detection and a Personal Understanding\nModule to grasp a deeper understanding of users' interests and desires. Our\narchitecture interacts with customers using a hybrid approach balanced between\nknowledge-driven response generators and context-driven neural response\ngenerators to cater to all three levels of conversations. During the\nsemi-finals period, we achieved an average cumulative rating of 3.25 on a 1-5\nLikert scale.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:02:01 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Hong", "Chung Hoon", ""], ["Liang", "Yuan", ""], ["Roy", "Sagnik Sinha", ""], ["Jain", "Arushi", ""], ["Agarwal", "Vihang", ""], ["Draves", "Ryan", ""], ["Zhou", "Zhizhuo", ""], ["Chen", "William", ""], ["Liu", "Yujian", ""], ["Miracky", "Martha", ""], ["Ge", "Lily", ""], ["Banovic", "Nikola", ""], ["Jurgens", "David", ""]]}, {"id": "2011.05928", "submitter": "Namyong Park", "authors": "Namyong Park, Andrey Kan, Christos Faloutsos, Xin Luna Dong", "title": "J-Recs: Principled and Scalable Recommendation Justification", "comments": "ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommendation is an essential functionality across a variety of\nservices, including e-commerce and video streaming, where items to buy, watch,\nor read are suggested to users. Justifying recommendations, i.e., explaining\nwhy a user might like the recommended item, has been shown to improve user\nsatisfaction and persuasiveness of the recommendation. In this paper, we\ndevelop a method for generating post-hoc justifications that can be applied to\nthe output of any recommendation algorithm. Existing post-hoc methods are often\nlimited in providing diverse justifications, as they either use only one of\nmany available types of input data, or rely on the predefined templates. We\naddress these limitations of earlier approaches by developing J-Recs, a method\nfor producing concise and diverse justifications. J-Recs is a recommendation\nmodel-agnostic method that generates diverse justifications based on various\ntypes of product and user data (e.g., purchase history and product attributes).\nThe challenge of jointly processing multiple types of data is addressed by\ndesigning a principled graph-based approach for justification generation. In\naddition to theoretical analysis, we present an extensive evaluation on\nsynthetic and real-world data. Our results show that J-Recs satisfies desirable\nproperties of justifications, and efficiently produces effective\njustifications, matching user preferences up to 20% more accurately than\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:37:52 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Park", "Namyong", ""], ["Kan", "Andrey", ""], ["Faloutsos", "Christos", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2011.06023", "submitter": "Pierre Monnin", "authors": "Pierre Monnin and Chedy Ra\\\"issi and Amedeo Napoli and Adrien Coulet", "title": "Rediscovering alignment relations with Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are concurrently published and edited in the Web of data.\nHence they may overlap, which makes key the task that consists in matching\ntheir content. This task encompasses the identification, within and across\nknowledge graphs, of nodes that are equivalent, more specific, or weakly\nrelated. In this article, we propose to match nodes of a knowledge graph by (i)\nlearning node embeddings with Graph Convolutional Networks such that similar\nnodes have low distances in the embedding space, and (ii) clustering nodes\nbased on their embeddings. We experimented this approach on a biomedical\nknowledge graph and particularly investigated the interplay between formal\nsemantics and GCN models with the two following main focuses. Firstly, we\napplied various inference rules associated with domain knowledge, independently\nor combined, before learning node embeddings, and we measured the improvements\nin matching results. Secondly, while our GCN model is agnostic to the exact\nalignment relations (e.g., equivalence, weak similarity), we observed that\ndistances in the embedding space are coherent with the \"strength\" of these\ndifferent relations (e.g., smaller distances for equivalences), somehow\ncorresponding to their rediscovery by the model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:19:20 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Monnin", "Pierre", ""], ["Ra\u00efssi", "Chedy", ""], ["Napoli", "Amedeo", ""], ["Coulet", "Adrien", ""]]}, {"id": "2011.06102", "submitter": "Aya Abdelsalam Ismail", "authors": "Aya Abdelsalam Ismail, Mahmudul Hasan, Faisal Ishtiaq", "title": "Improving Multimodal Accuracy Through Modality Pre-training and\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a multimodal network is challenging and it requires complex\narchitectures to achieve reasonable performance. We show that one reason for\nthis phenomena is the difference between the convergence rate of various\nmodalities. We address this by pre-training modality-specific sub-networks in\nmultimodal architectures independently before end-to-end training of the entire\nnetwork. Furthermore, we show that the addition of an attention mechanism\nbetween sub-networks after pre-training helps identify the most important\nmodality during ambiguous scenarios boosting the performance. We demonstrate\nthat by performing these two tricks a simple network can achieve similar\nperformance to a complicated architecture that is significantly more expensive\nto train on multiple tasks including sentiment analysis, emotion recognition,\nand speaker trait recognition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 22:31:27 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ismail", "Aya Abdelsalam", ""], ["Hasan", "Mahmudul", ""], ["Ishtiaq", "Faisal", ""]]}, {"id": "2011.06123", "submitter": "Loris Nanni", "authors": "Loris Nanni, Eugenio De Luca, Marco Ludovico Facin, Gianluca Maguolo", "title": "Deep learning and hand-crafted features for virus image classification", "comments": null, "journal-ref": "Journal of Imaging 2020, 6(12), 143", "doi": "10.3390/jimaging6120143", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present an ensemble of descriptors for the classification of\ntransmission electron microscopy images of viruses. We propose to combine\nhandcrafted and deep learning approaches for virus image classification. The\nset of handcrafted is mainly based on Local Binary Pattern variants, for each\ndescriptor a different Support Vector Machine is trained, then the set of\nclassifiers is combined by sum rule. The deep learning approach is a\ndensenet201 pretrained on ImageNet and then tuned in the virus dataset, the net\nis used as features extractor for feeding another Support Vector Machine, in\nparticular the last average pooling layer is used as feature extractor.\nFinally, classifiers trained on handcrafted features and classifier trained on\ndeep learning features are combined by sum rule. The proposed fusion strongly\nboosts the performance obtained by each stand-alone approach, obtaining state\nof the art performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:46:16 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 00:29:22 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Nanni", "Loris", ""], ["De Luca", "Eugenio", ""], ["Facin", "Marco Ludovico", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2011.06125", "submitter": "L\\'eonard Boussioux", "authors": "L\\'eonard Boussioux, Cynthia Zeng, Th\\'eo Gu\\'enais, Dimitris\n  Bertsimas", "title": "Hurricane Forecasting: A Novel Multimodal Machine Learning Framework", "comments": "Under revision by the AMS' Weather and Forecasting journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a machine learning (ML) framework for tropical cyclone\nintensity and track forecasting, combining multiple distinct ML techniques and\nutilizing diverse data sources. Our framework, which we refer to as Hurricast\n(HURR), is built upon the combination of distinct data processing techniques\nusing gradient-boosted trees and novel encoder-decoder architectures, including\nCNN, GRU and Transformers components. We propose a deep-feature extractor\nmethodology to mix spatial-temporal data with statistical data efficiently. Our\nmultimodal framework unleashes the potential of making forecasts based on a\nwide range of data sources, including historical storm data, and visual data\nsuch as reanalysis atmospheric images. We evaluate our models with current\noperational forecasts in North Atlantic and Eastern Pacific basins on 2016-2019\nfor 24-hour lead time, and show our models consistently outperform\nstatistical-dynamical models and compete with the best dynamical models, while\ncomputing forecasts in seconds. Furthermore, the inclusion of Hurricast into an\noperational forecast consensus model leads to a significant improvement of 5% -\n15% over NHC's official forecast, thus highlighting the complementary\nproperties with existing approaches. In summary, our work demonstrates that\ncombining different data sources and distinct machine learning methodologies\ncan lead to superior tropical cyclone forecasting. We hope that this work opens\nthe door for further use of machine learning in meteorological forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:55:33 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:52:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Boussioux", "L\u00e9onard", ""], ["Zeng", "Cynthia", ""], ["Gu\u00e9nais", "Th\u00e9o", ""], ["Bertsimas", "Dimitris", ""]]}, {"id": "2011.06146", "submitter": "Himabindu Lakkaraju", "authors": "Alexis Ross, Himabindu Lakkaraju, Osbert Bastani", "title": "Ensuring Actionable Recourse via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning models are increasingly deployed in high-stakes domains\nsuch as legal and financial decision-making, there has been growing interest in\npost-hoc methods for generating counterfactual explanations. Such explanations\nprovide individuals adversely impacted by predicted outcomes (e.g., an\napplicant denied a loan) with \"recourse\" ---i.e., a description of how they can\nchange their features to obtain a positive outcome. We propose a novel\nalgorithm that leverages adversarial training and PAC confidence sets to learn\nmodels that theoretically guarantee recourse to affected individuals with high\nprobability without sacrificing accuracy. To the best of our knowledge, our\napproach is the first to learn models for which recourses are guaranteed with\nhigh probability. Extensive experimentation with real world datasets spanning\nvarious applications including recidivism prediction, bail outcomes, and\nlending demonstrate the efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 01:15:18 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ross", "Alexis", ""], ["Lakkaraju", "Himabindu", ""], ["Bastani", "Osbert", ""]]}, {"id": "2011.06156", "submitter": "Baogang Hu", "authors": "Bao-Gang Hu and Han-Bing Qu", "title": "Generalized Constraints as A New Mathematical Problem in Artificial\n  Intelligence: A Review and Perspective", "comments": "20 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this comprehensive review, we describe a new mathematical problem in\nartificial intelligence (AI) from a mathematical modeling perspective,\nfollowing the philosophy stated by Rudolf E. Kalman that \"Once you get the\nphysics right, the rest is mathematics\". The new problem is called \"Generalized\nConstraints (GCs)\", and we adopt GCs as a general term to describe any type of\nprior information in modelings. To understand better about GCs to be a general\nproblem, we compare them with the conventional constraints (CCs) and list their\nextra challenges over CCs. In the construction of AI machines, we basically\nencounter more often GCs for modeling, rather than CCs with well-defined forms.\nFurthermore, we discuss the ultimate goals of AI and redefine transparent,\ninterpretable, and explainable AI in terms of comprehension levels about\nmachines. We review the studies in relation to the GC problems although most of\nthem do not take the notion of GCs. We demonstrate that if AI machines are\nsimplified by a coupling with both knowledge-driven submodel and data-driven\nsubmodel, GCs will play a critical role in a knowledge-driven submodel as well\nas in the coupling form between the two submodels. Examples are given to show\nthat the studies in view of a generalized constraint problem will help us\nperceive and explore novel subjects in AI, or even in mathematics, such as\ngeneralized constraint learning (GCL).\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 01:47:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Hu", "Bao-Gang", ""], ["Qu", "Han-Bing", ""]]}, {"id": "2011.06176", "submitter": "Yihan Lin", "authors": "Zhenzhi Wu, Hehui Zhang, Yihan Lin, Guoqi Li, Meng Wang, Ye Tang", "title": "LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and\n  Efficient Spatiotemporal Information Processing", "comments": "14 pages, 9 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking neural networks (SNNs) based on Leaky Integrate and Fire (LIF) model\nhave been applied to energy-efficient temporal and spatiotemporal processing\ntasks. Thanks to the bio-plausible neuronal dynamics and simplicity, LIF-SNN\nbenefits from event-driven processing, however, usually faces the embarrassment\nof reduced performance. This may because in LIF-SNN the neurons transmit\ninformation via spikes. To address this issue, in this work, we propose a Leaky\nIntegrate and Analog Fire (LIAF) neuron model, so that analog values can be\ntransmitted among neurons, and a deep network termed as LIAF-Net is built on it\nfor efficient spatiotemporal processing. In the temporal domain, LIAF follows\nthe traditional LIF dynamics to maintain its temporal processing capability. In\nthe spatial domain, LIAF is able to integrate spatial information through\nconvolutional integration or fully-connected integration. As a spatiotemporal\nlayer, LIAF can also be used with traditional artificial neural network (ANN)\nlayers jointly. Experiment results indicate that LIAF-Net achieves comparable\nperformance to Gated Recurrent Unit (GRU) and Long short-term memory (LSTM) on\nbAbI Question Answering (QA) tasks, and achieves state-of-the-art performance\non spatiotemporal Dynamic Vision Sensor (DVS) datasets, including MNIST-DVS,\nCIFAR10-DVS and DVS128 Gesture, with much less number of synaptic weights and\ncomputational overhead compared with traditional networks built by LSTM, GRU,\nConvolutional LSTM (ConvLSTM) or 3D convolution (Conv3D). Compared with\ntraditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these\nexperiments. In conclusion, LIAF-Net provides a framework combining the\nadvantages of both ANNs and SNNs for lightweight and efficient spatiotemporal\ninformation processing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 03:04:21 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Wu", "Zhenzhi", ""], ["Zhang", "Hehui", ""], ["Lin", "Yihan", ""], ["Li", "Guoqi", ""], ["Wang", "Meng", ""], ["Tang", "Ye", ""]]}, {"id": "2011.06195", "submitter": "Cheng-I Lai", "authors": "Cheng-I Lai, Jin Cao, Sravan Bodapati, Shang-Wen Li", "title": "Towards Semi-Supervised Semantics Understanding from Speech", "comments": "arXiv admin note: text overlap with arXiv:2010.13826", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much recent work on Spoken Language Understanding (SLU) falls short in at\nleast one of three ways: models were trained on oracle text input and neglected\nthe Automatics Speech Recognition (ASR) outputs, models were trained to predict\nonly intents without the slot values, or models were trained on a large amount\nof in-house data. We proposed a clean and general framework to learn semantics\ndirectly from speech with semi-supervision from transcribed speech to address\nthese. Our framework is built upon pretrained end-to-end (E2E) ASR and\nself-supervised language models, such as BERT, and fine-tuned on a limited\namount of target SLU corpus. In parallel, we identified two inadequate settings\nunder which SLU models have been tested: noise-robustness and E2E semantics\nevaluation. We tested the proposed framework under realistic environmental\nnoises and with a new metric, the slots edit F1 score, on two public SLU\ncorpora. Experiments show that our SLU framework with speech as input can\nperform on par with those with oracle text as input in semantics understanding,\nwhile environmental noises are present, and a limited amount of labeled\nsemantics data is available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:48:09 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lai", "Cheng-I", ""], ["Cao", "Jin", ""], ["Bodapati", "Sravan", ""], ["Li", "Shang-Wen", ""]]}, {"id": "2011.06210", "submitter": "Sulaiman Aburakhia", "authors": "Sulaiman Aburakhia, Tareq Tayeh, Ryan Myers, Abdallah Shami", "title": "A Transfer Learning Framework for Anomaly Detection Using Model of\n  Normality", "comments": "7 pages, 4 figures, 2 tables, conference: The 11th Annual IEEE\n  Information Technology, Electronics and Mobile Communication Conference \"IEEE\n  IEMCON\", Vancouver, Canada, November 2020. IEEE IEMCON'20' best paper award\n  in the category of Industrial Automation and Control Systems Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Network (CNN) techniques have proven to be very useful\nin image-based anomaly detection applications. CNN can be used as deep features\nextractor where other anomaly detection techniques are applied on these\nfeatures. For this scenario, using transfer learning is common since pretrained\nmodels provide deep feature representations that are useful for anomaly\ndetection tasks. Consequentially, anomaly can be detected by applying similarly\nmeasure between extracted features and a defined model of normality. A key\nfactor in such approaches is the decision threshold used for detecting anomaly.\nWhile most of the proposed methods focus on the approach itself, slight\nattention has been paid to address decision threshold settings. In this paper,\nwe tackle this problem and propose a welldefined method to set the\nworking-point decision threshold that improves detection accuracy. We introduce\na transfer learning framework for anomaly detection based on similarity measure\nwith a Model of Normality (MoN) and show that with the proposed threshold\nsettings, a significant performance improvement can be achieved. Moreover, the\nframework has low complexity with relaxed computational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:26:32 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Aburakhia", "Sulaiman", ""], ["Tayeh", "Tareq", ""], ["Myers", "Ryan", ""], ["Shami", "Abdallah", ""]]}, {"id": "2011.06225", "submitter": "Moloud Abdar", "authors": "Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li\n  Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U\n  Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi", "title": "A Review of Uncertainty Quantification in Deep Learning: Techniques,\n  Applications and Challenges", "comments": null, "journal-ref": "2021", "doi": "10.1016/j.inffus.2021.05.008", "report-no": "INFFUS_1411]", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification (UQ) plays a pivotal role in reduction of\nuncertainties during both optimization and decision making processes. It can be\napplied to solve a variety of real-world applications in science and\nengineering. Bayesian approximation and ensemble learning techniques are two\nmost widely-used UQ methods in the literature. In this regard, researchers have\nproposed different UQ methods and examined their performance in a variety of\napplications such as computer vision (e.g., self-driving cars and object\ndetection), image processing (e.g., image restoration), medical image analysis\n(e.g., medical image classification and segmentation), natural language\nprocessing (e.g., text classification, social media texts and recidivism\nrisk-scoring), bioinformatics, etc. This study reviews recent advances in UQ\nmethods used in deep learning. Moreover, we also investigate the application of\nthese methods in reinforcement learning (RL). Then, we outline a few important\napplications of UQ methods. Finally, we briefly highlight the fundamental\nresearch challenges faced by UQ methods and discuss the future research\ndirections in this field.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:41:05 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 13:07:02 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:58:51 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 01:58:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abdar", "Moloud", ""], ["Pourpanah", "Farhad", ""], ["Hussain", "Sadiq", ""], ["Rezazadegan", "Dana", ""], ["Liu", "Li", ""], ["Ghavamzadeh", "Mohammad", ""], ["Fieguth", "Paul", ""], ["Cao", "Xiaochun", ""], ["Khosravi", "Abbas", ""], ["Acharya", "U Rajendra", ""], ["Makarenkov", "Vladimir", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2011.06231", "submitter": "Wenting Tang", "authors": "Wenting Tang, Xingxing Wei, Bo Li", "title": "Automated Model Compression by Jointly Applied Pruning and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the traditional deep compression framework, iteratively performing network\npruning and quantization can reduce the model size and computation cost to meet\nthe deployment requirements. However, such a step-wise application of pruning\nand quantization may lead to suboptimal solutions and unnecessary time\nconsumption. In this paper, we tackle this issue by integrating network pruning\nand quantization as a unified joint compression problem and then use AutoML to\nautomatically solve it. We find the pruning process can be regarded as the\nchannel-wise quantization with 0 bit. Thus, the separate two-step pruning and\nquantization can be simplified as the one-step quantization with mixed\nprecision. This unification not only simplifies the compression pipeline but\nalso avoids the compression divergence. To implement this idea, we propose the\nautomated model compression by jointly applied pruning and quantization (AJPQ).\nAJPQ is designed with a hierarchical architecture: the layer controller\ncontrols the layer sparsity, and the channel controller decides the bit-width\nfor each kernel. Following the same importance criterion, the layer controller\nand the channel controller collaboratively decide the compression strategy.\nWith the help of reinforcement learning, our one-step compression is\nautomatically achieved. Compared with the state-of-the-art automated\ncompression methods, our method obtains a better accuracy while reducing the\nstorage considerably. For fixed precision quantization, AJPQ can reduce more\nthan five times model size and two times computation with a slight performance\nincrease for Skynet in remote sensing object detection. When mixed-precision is\nallowed, AJPQ can reduce five times model size with only 1.06% top-5 accuracy\ndecline for MobileNet in the classification task.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:06:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tang", "Wenting", ""], ["Wei", "Xingxing", ""], ["Li", "Bo", ""]]}, {"id": "2011.06275", "submitter": "Jakub Tetek", "authors": "Jakub T\\v{e}tek, Marek Sklenka, Tom\\'a\\v{s} Gaven\\v{c}iak", "title": "Performance of Bounded-Rational Agents With the Ability to Self-Modify", "comments": "Fixed minor problems; To appear in SafeAI @ AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-modification of agents embedded in complex environments is hard to\navoid, whether it happens via direct means (e.g. own code modification) or\nindirectly (e.g. influencing the operator, exploiting bugs or the environment).\nIt has been argued that intelligent agents have an incentive to avoid modifying\ntheir utility function so that their future instances work towards the same\ngoals.\n  Everitt et al. (2016) formally show that providing an option to self-modify\nis harmless for perfectly rational agents. We show that this result is no\nlonger true for agents with bounded rationality. In such agents,\nself-modification may cause exponential deterioration in performance and\ngradual misalignment of a previously aligned agent. We investigate how the size\nof this effect depends on the type and magnitude of imperfections in the\nagent's rationality (1-4 below). We also discuss model assumptions and the\nwider problem and framing space.\n  We examine four ways in which an agent can be bounded-rational: it either (1)\ndoesn't always choose the optimal action, (2) is not perfectly aligned with\nhuman values, (3) has an inaccurate model of the environment, or (4) uses the\nwrong temporal discounting factor. We show that while in the cases (2)-(4) the\nmisalignment caused by the agent's imperfection does not increase over time,\nwith (1) the misalignment may grow exponentially.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:25:08 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 09:55:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["T\u011btek", "Jakub", ""], ["Sklenka", "Marek", ""], ["Gaven\u010diak", "Tom\u00e1\u0161", ""]]}, {"id": "2011.06288", "submitter": "Pankaj Mishra", "authors": "Pankaj Mishra, Claudio Piciarelli, Gian Luca Foresti", "title": "Image Anomaly Detection by Aggregating Deep Pyramidal Representations", "comments": "Published in First International Conference of Industrial Machine\n  Learning ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomaly detection consists in identifying, within a dataset, those samples\nthat significantly differ from the majority of the data, representing the\nnormal class. It has many practical applications, e.g. ranging from defective\nproduct detection in industrial systems to medical imaging. This paper focuses\non image anomaly detection using a deep neural network with multiple pyramid\nlevels to analyze the image features at different scales. We propose a network\nbased on encoding-decoding scheme, using a standard convolutional autoencoders,\ntrained on normal data only in order to build a model of normality. Anomalies\ncan be detected by the inability of the network to reconstruct its input.\nExperimental results show a good accuracy on MNIST, FMNIST and the recent MVTec\nAnomaly Detection dataset\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:58:27 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Mishra", "Pankaj", ""], ["Piciarelli", "Claudio", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "2011.06300", "submitter": "Bahadorreza Ofoghi", "authors": "Bahadorreza Ofoghi, Vicky Mak, John Yearwood", "title": "A Knowledge Representation Approach to Automated Mathematical Modelling", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new mixed-integer linear programming (MILP) model\nontology and a novel constraint typology of MILP formulations. MILP is a\ncommonly used mathematical programming technique for modelling and solving\nreal-life scheduling, routing, planning, resource allocation, and timetabling\noptimization problems providing optimized business solutions for industry\nsectors such as manufacturing, agriculture, defence, healthcare, medicine,\nenergy, finance, and transportation. Despite the numerous real-life\nCombinatorial Optimization Problems found and solved and millions yet to be\ndiscovered and formulated, the number of types of constraints (the building\nblocks of a MILP) is relatively small. In the search for a suitable\nmachine-readable knowledge representation structure for MILPs, we propose an\noptimization modelling tree built based upon an MILP model ontology that can be\nused as a guide for automated systems to elicit an MILP model from end-users on\ntheir combinatorial business optimization problems. Our ultimate aim is to\ndevelop a machine-readable knowledge representation for MILP that allows us to\nmap an end-user's natural language description of the business optimization\nproblem to an MILP formal specification as a first step towards automated\nmathematical modelling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 10:29:57 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 07:48:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ofoghi", "Bahadorreza", ""], ["Mak", "Vicky", ""], ["Yearwood", "John", ""]]}, {"id": "2011.06306", "submitter": "Youmna Farag", "authors": "Youmna Farag, Josef Valvoda, Helen Yannakoudakis and Ted Briscoe", "title": "Analyzing Neural Discourse Coherence Models", "comments": null, "journal-ref": "CODI workshop in EMNLP2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we systematically investigate how well current models of\ncoherence can capture aspects of text implicated in discourse organisation. We\ndevise two datasets of various linguistic alterations that undermine coherence\nand test model sensitivity to changes in syntax and semantics. We furthermore\nprobe discourse embedding space and examine the knowledge that is encoded in\nrepresentations of coherence. We hope this study shall provide further insight\ninto how to frame the task and improve models of coherence assessment further.\nFinally, we make our datasets publicly available as a resource for researchers\nto use to test discourse coherence models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 10:44:41 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Farag", "Youmna", ""], ["Valvoda", "Josef", ""], ["Yannakoudakis", "Helen", ""], ["Briscoe", "Ted", ""]]}, {"id": "2011.06315", "submitter": "Veysel Kocaman Vk", "authors": "Veysel Kocaman and David Talby", "title": "Biomedical Named Entity Recognition at Scale", "comments": "Accepted for presentation and inclusion in CADL 2020 (International\n  Workshop on Computational Aspects of Deep Learning) , organized in\n  conjunction with ICPR 2020, the 25th International Conference on Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a widely applicable natural language\nprocessing task and building block of question answering, topic modeling,\ninformation retrieval, etc. In the medical domain, NER plays a crucial role by\nextracting meaningful chunks from clinical notes and reports, which are then\nfed to downstream tasks like assertion status detection, entity resolution,\nrelation extraction, and de-identification. Reimplementing a Bi-LSTM-CNN-Char\ndeep learning architecture on top of Apache Spark, we present a single\ntrainable NER model that obtains new state-of-the-art results on seven public\nbiomedical benchmarks without using heavy contextual embeddings like BERT. This\nincludes improving BC4CHEMD to 93.72% (4.1% gain), Species800 to 80.91% (4.6%\ngain), and JNLPBA to 81.29% (5.2% gain). In addition, this model is freely\navailable within a production-grade code base as part of the open-source Spark\nNLP library; can scale up for training and inference in any Spark cluster; has\nGPU support and libraries for popular programming languages such as Python, R,\nScala and Java; and can be extended to support other human languages with no\ncode changes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 11:10:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kocaman", "Veysel", ""], ["Talby", "David", ""]]}, {"id": "2011.06335", "submitter": "Lorenzo Steccanella", "authors": "Lorenzo Steccanella, Simone Totaro, Damien Allonsius, Anders Jonsson", "title": "Hierarchical reinforcement learning for efficient exploration and\n  transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sparse-reward domains are challenging for reinforcement learning algorithms\nsince significant exploration is needed before encountering reward for the\nfirst time. Hierarchical reinforcement learning can facilitate exploration by\nreducing the number of decisions necessary before obtaining a reward. In this\npaper, we present a novel hierarchical reinforcement learning framework based\non the compression of an invariant state space that is common to a range of\ntasks. The algorithm introduces subtasks which consist of moving between the\nstate partitions induced by the compression. Results indicate that the\nalgorithm can successfully solve complex sparse-reward domains, and transfer\nknowledge to solve new, previously unseen tasks more quickly.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:09:13 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Steccanella", "Lorenzo", ""], ["Totaro", "Simone", ""], ["Allonsius", "Damien", ""], ["Jonsson", "Anders", ""]]}, {"id": "2011.06346", "submitter": "Zhenghao Zhang", "authors": "Zhenghao Zhang, Jianbin Huang and Qinglin Tan", "title": "Multi-View Dynamic Heterogeneous Information Network Embedding", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing Heterogeneous Information Network (HIN) embedding methods focus\non static environments while neglecting the evolving characteristic of\nrealworld networks. Although several dynamic embedding methods have been\nproposed, they are merely designed for homogeneous networks and cannot be\ndirectly applied in heterogeneous environment. To tackle above challenges, we\npropose a novel framework for incorporating temporal information into HIN\nembedding, denoted as Multi-View Dynamic HIN Embedding (MDHNE), which can\nefficiently preserve evolution patterns of implicit relationships from\ndifferent views in updating node representations over time. We first transform\nHIN to a series of homogeneous networks corresponding to different views. Then\nour proposed MDHNE applies Recurrent Neural Network (RNN) to incorporate\nevolving pattern of complex network structure and semantic relationships\nbetween nodes into latent embedding spaces, and thus the node representations\nfrom multiple views can be learned and updated when HIN evolves over time.\nMoreover, we come up with an attention based fusion mechanism, which can\nautomatically infer weights of latent representations corresponding to\ndifferent views by minimizing the objective function specific for different\nmining tasks. Extensive experiments clearly demonstrate that our MDHNE model\noutperforms state-of-the-art baselines on three real-world dynamic datasets for\ndifferent network mining tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:33:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhang", "Zhenghao", ""], ["Huang", "Jianbin", ""], ["Tan", "Qinglin", ""]]}, {"id": "2011.06363", "submitter": "Shengyi Huang", "authors": "Chris Bamford, Shengyi Huang, Simon Lucas", "title": "Griddly: A platform for AI research in games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there have been immense breakthroughs in Game AI research,\nparticularly with Reinforcement Learning (RL). Despite their success, the\nunderlying games are usually implemented with their own preset environments and\ngame mechanics, thus making it difficult for researchers to prototype different\ngame environments. However, testing the RL agents against a variety of game\nenvironments is critical for recent effort to study generalization in RL and\navoid the problem of overfitting that may otherwise occur. In this paper, we\npresent Griddly as a new platform for Game AI research that provides a unique\ncombination of highly configurable games, different observer types and an\nefficient C++ core engine. Additionally, we present a series of baseline\nexperiments to study the effect of different observation configurations and\ngeneralization ability of RL agents.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 13:23:31 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 17:35:33 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bamford", "Chris", ""], ["Huang", "Shengyi", ""], ["Lucas", "Simon", ""]]}, {"id": "2011.06382", "submitter": "Arbi Haza Nasution", "authors": "Dian Indriani, Arbi Haza Nasution, Winda Monika and Salhazan Nasution", "title": "Towards A Sentiment Analyzer for Low-Resource Languages", "comments": "Accepted to be published in Proceedings of International Conference\n  on Smart Computing and Cyber Security (SMARTCYBER 2020)", "journal-ref": "Proceedings of International Conference on Smart Computing and\n  Cyber Security: Strategic Foresight, Security Challenges and Innovation\n  (SMARTCYBER 2020)", "doi": "10.1007/978-981-15-7990-5_10", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Twitter is one of the top influenced social media which has a million number\nof active users. It is commonly used for microblogging that allows users to\nshare messages, ideas, thoughts and many more. Thus, millions interaction such\nas short messages or tweets are flowing around among the twitter users\ndiscussing various topics that has been happening world-wide. This research\naims to analyse a sentiment of the users towards a particular trending topic\nthat has been actively and massively discussed at that time. We chose a hashtag\n\\textit{\\#kpujangancurang} that was the trending topic during the Indonesia\npresidential election in 2019. We use the hashtag to obtain a set of data from\nTwitter to analyse and investigate further the positive or the negative\nsentiment of the users from their tweets. This research utilizes rapid miner\ntool to generate the twitter data and comparing Naive Bayes, K-Nearest\nNeighbor, Decision Tree, and Multi-Layer Perceptron classification methods to\nclassify the sentiment of the twitter data. There are overall 200 labeled data\nin this experiment. Overall, Naive Bayes and Multi-Layer Perceptron\nclassification outperformed the other two methods on 11 experiments with\ndifferent size of training-testing data split. The two classifiers are\npotential to be used in creating sentiment analyzer for low-resource languages\nwith small corpus.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 13:50:00 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Indriani", "Dian", ""], ["Nasution", "Arbi Haza", ""], ["Monika", "Winda", ""], ["Nasution", "Salhazan", ""]]}, {"id": "2011.06393", "submitter": "Lixuan Yang", "authors": "Lixuan Yang, Cedric Beliard, Dario Rossi", "title": "Heterogeneous Data-Aware Federated Learning", "comments": "IJCAI 2020 Federated learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an appealing concept to perform distributed\ntraining of Neural Networks (NN) while keeping data private. With the\nindustrialization of the FL framework, we identify several problems hampering\nits successful deployment, such as presence of non i.i.d data, disjoint\nclasses, signal multi-modality across datasets. In this work, we address these\nproblems by proposing a novel method that not only (1) aggregates generic model\nparameters (e.g. a common set of task generic NN layers) on server (e.g. in\ntraditional FL), but also (2) keeps a set of parameters (e.g, a set of task\nspecific NN layer) specific to each client. We validate our method on the\ntraditionally used public benchmarks (e.g., Femnist) as well as on our\nproprietary collected dataset (i.e., traffic classification). Results show the\nbenefit of our method, with significant advantage on extreme cases.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:07:09 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Yang", "Lixuan", ""], ["Beliard", "Cedric", ""], ["Rossi", "Dario", ""]]}, {"id": "2011.06423", "submitter": "Mario Scrocca", "authors": "Mario Scrocca, Marco Comerio, Alessio Carenini and Irene Celino", "title": "Turning Transport Data to Comply with EU Standards while Enabling a\n  Multimodal Transport Knowledge Graph", "comments": "International Semantic Web Conference (ISWC 2020) - In Use Track", "journal-ref": null, "doi": "10.1007/978-3-030-62466-8_26", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complying with the EU Regulation on multimodal transportation services\nrequires sharing data on the National Access Points in one of the standards\n(e.g., NeTEx and SIRI) indicated by the European Commission. These standards\nare complex and of limited practical adoption. This means that datasets are\nnatively expressed in other formats and require a data translation process for\nfull compliance.\n  This paper describes the solution to turn the authoritative data of three\ndifferent transport stakeholders from Italy and Spain into a format compliant\nwith EU standards by means of Semantic Web technologies. Our solution addresses\nthe challenge and also contributes to build a multi-modal transport Knowledge\nGraph of interlinked and interoperable information that enables intelligent\nquerying and exploration, as well as facilitates the design of added-value\nservices.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:56:15 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Scrocca", "Mario", ""], ["Comerio", "Marco", ""], ["Carenini", "Alessio", ""], ["Celino", "Irene", ""]]}, {"id": "2011.06427", "submitter": "Sadra Rahimi Kari", "authors": "S. Rahimi Kari", "title": "Realization of Stochastic Neural Networks and Its Potential Applications", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.NI eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successive Cancellation Decoders have come a long way since the\nimplementation of traditional SC decoders, but there still is a potential for\nimprovement. The main struggle over the years was to find an optimal algorithm\nto implement them. Most of the proposed algorithms are not practical enough to\nbe implemented in real-life. In this research, we aim to introduce the\nEfficiency of stochastic neural networks as an SC decoder and Find the possible\nways of improving its performance and practicality. In this paper, after a\nbrief introduction to stochastic neurons and SNNs, we introduce methods to\nrealize Stochastic NNs on both deterministic and stochastic platforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:01:07 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kari", "S. Rahimi", ""]]}, {"id": "2011.06450", "submitter": "Manish Bhattarai", "authors": "Manish Bhattarai and Manel Martinez-Ramon", "title": "A deep Q-Learning based Path Planning and Navigation System for\n  Firefighting Environments", "comments": "Accepted to ICAART2021", "journal-ref": null, "doi": "10.5220/0010267102670277", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live fire creates a dynamic, rapidly changing environment that presents a\nworthy challenge for deep learning and artificial intelligence methodologies to\nassist firefighters with scene comprehension in maintaining their situational\nawareness, tracking and relay of important features necessary for key decisions\nas they tackle these catastrophic events. We propose a deep Q-learning based\nagent who is immune to stress induced disorientation and anxiety and thus able\nto make clear decisions for navigation based on the observed and stored facts\nin live fire environments. As a proof of concept, we imitate structural fire in\na gaming engine called Unreal Engine which enables the interaction of the agent\nwith the environment. The agent is trained with a deep Q-learning algorithm\nbased on a set of rewards and penalties as per its actions on the environment.\nWe exploit experience replay to accelerate the learning process and augment the\nlearning of the agent with human-derived experiences. The agent trained under\nthis deep Q-learning approach outperforms agents trained through alternative\npath planning systems and demonstrates this methodology as a promising\nfoundation on which to build a path planning navigation assistant capable of\nsafely guiding fire fighters through live fire environments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:43:17 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattarai", "Manish", ""], ["Martinez-Ramon", "Manel", ""]]}, {"id": "2011.06464", "submitter": "Xian Zhou", "authors": "Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, Katerina\n  Fragkiadaki", "title": "3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an action-conditioned dynamics model that predicts scene changes\ncaused by object and agent interactions in a viewpoint-invariant 3D neural\nscene representation space, inferred from RGB-D videos. In this 3D feature\nspace, objects do not interfere with one another and their appearance persists\nover time and across viewpoints. This permits our model to predict future\nscenes long in the future by simply \"moving\" 3D object features based on\ncumulative object motion predictions. Object motion predictions are computed by\na graph neural network that operates over the object features extracted from\nthe 3D neural scene representation. Our model's simulations can be decoded by a\nneural renderer into2D image views from any desired viewpoint, which aids the\ninterpretability of our latent 3D simulation space. We show our model\ngeneralizes well its predictions across varying number and appearances of\ninteracting objects as well as across camera viewpoints, outperforming existing\n2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the\nlearnt dynamics by applying our model trained solely in simulation to\nmodel-based control for pushing objects to desired locations under clutter on a\nreal robotic setup\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:15:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Xian", "Zhou", ""], ["Prabhudesai", "Mihir", ""], ["Lal", "Shamit", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2011.06475", "submitter": "Alessandro Luongo", "authors": "Alessandro Luongo, Changpeng Shao", "title": "Quantum algorithms for spectral sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose and analyze new quantum algorithms for estimating the most common\nspectral sums of symmetric positive definite (SPD) matrices. For a function $f$\nand a matrix $A \\in \\mathbb{R}^{n\\times n}$, the spectral sum is defined as\n$S_f(A) :=\\text{Tr}[f(A)] = \\sum_j f(\\lambda_j)$, where $\\lambda_j$ are the\neigenvalues. Examples of spectral sums are the von Neumann entropy, the trace\nof inverse, the log-determinant, and the Schatten-$p$ norm, where the latter\ndoes not require the matrix to be SPD. The fastest classical randomized\nalgorithms estimate these quantities have a runtime that depends at least\nlinearly on the number of nonzero components of the matrix. Assuming quantum\naccess to the matrix, our algorithms are sub-linear in the matrix size, and\ndepend at most quadratically on other quantities, like the condition number and\nthe approximation error, and thus can compete with most of the randomized and\ndistributed classical algorithms proposed in recent literature. These\nalgorithms can be used as subroutines for solving many practical problems, for\nwhich the estimation of a spectral sum often represents a computational\nbottleneck.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:29:45 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Luongo", "Alessandro", ""], ["Shao", "Changpeng", ""]]}, {"id": "2011.06485", "submitter": "Robert Adragna", "authors": "Robert Adragna, Elliot Creager, David Madras, Richard Zemel", "title": "Fairness and Robustness in Invariant Learning: A Case Study in Toxicity\n  Classification", "comments": "12 pages, 5 figures. Appears in the NeurIPS 2020 Workshop on\n  Algorithmic Fairness through the Lens of Causality and Interpretability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robustness is of central importance in machine learning and has given rise to\nthe fields of domain generalization and invariant learning, which are concerned\nwith improving performance on a test distribution distinct from but related to\nthe training distribution. In light of recent work suggesting an intimate\nconnection between fairness and robustness, we investigate whether algorithms\nfrom robust ML can be used to improve the fairness of classifiers that are\ntrained on biased data and tested on unbiased data. We apply Invariant Risk\nMinimization (IRM), a domain generalization algorithm that employs a causal\ndiscovery inspired method to find robust predictors, to the task of fairly\npredicting the toxicity of internet comments. We show that IRM achieves better\nout-of-distribution accuracy and fairness than Empirical Risk Minimization\n(ERM) methods, and analyze both the difficulties that arise when applying IRM\nin practice and the conditions under which IRM will likely be effective in this\nscenario. We hope that this work will inspire further studies of how robust\nmachine learning methods relate to algorithmic fairness.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:42:14 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 02:21:12 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Adragna", "Robert", ""], ["Creager", "Elliot", ""], ["Madras", "David", ""], ["Zemel", "Richard", ""]]}, {"id": "2011.06498", "submitter": "Shubham Agrawal", "authors": "Huy Ha, Shubham Agrawal, Shuran Song", "title": "Fit2Form: 3D Generative Model for Robot Gripper Form Design", "comments": "Conference on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D shape of a robot's end-effector plays a critical role in determining\nit's functionality and overall performance. Many industrial applications rely\non task-specific gripper designs to ensure the system's robustness and\naccuracy. However, the process of manual hardware design is both costly and\ntime-consuming, and the quality of the resulting design is dependent on the\nengineer's experience and domain expertise, which can easily be out-dated or\ninaccurate. The goal of this work is to use machine learning algorithms to\nautomate the design of task-specific gripper fingers. We propose Fit2Form, a 3D\ngenerative design framework that generates pairs of finger shapes to maximize\ndesign objectives (i.e., grasp success, stability, and robustness) for target\ngrasp objects. We model the design objectives by training a Fitness network to\npredict their values for pairs of gripper fingers and their corresponding grasp\nobjects. This Fitness network then provides supervision to a 3D Generative\nnetwork that produces a pair of 3D finger geometries for the target grasp\nobject. Our experiments demonstrate that the proposed 3D generative design\nframework generates parallel jaw gripper finger shapes that achieve more stable\nand robust grasps compared to other general-purpose and task-specific gripper\ndesign algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:09:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ha", "Huy", ""], ["Agrawal", "Shubham", ""], ["Song", "Shuran", ""]]}, {"id": "2011.06502", "submitter": "Jens Brandenburger", "authors": "Jens Brandenburger, Christoph Schirm, Josef Melcher, Edgar Hancke,\n  Marco Vannucci, Valentina Colla, Silvia Cateni, Rami Sellami, S\\'ebastien\n  Dupont, Annick Majchrowski, Asier Arteaga", "title": "Quality4.0 -- Transparent product quality supervision in the age of\n  Industry 4.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Progressive digitalization is changing the game of many industrial sectors.\nFocus-ing on product quality the main profitability driver of this so-called\nIndustry 4.0 will be the horizontal integration of information over the\ncomplete supply chain. Therefore, the European RFCS project 'Quality4.0' aims\nin developing an adap-tive platform, which releases decisions on product\nquality and provides tailored information of high reliability that can be\nindividually exchanged with customers. In this context Machine Learning will be\nused to detect outliers in the quality data. This paper discusses the\nintermediate project results and the concepts developed so far for this\nhorizontal integration of quality information.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:12:21 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Brandenburger", "Jens", ""], ["Schirm", "Christoph", ""], ["Melcher", "Josef", ""], ["Hancke", "Edgar", ""], ["Vannucci", "Marco", ""], ["Colla", "Valentina", ""], ["Cateni", "Silvia", ""], ["Sellami", "Rami", ""], ["Dupont", "S\u00e9bastien", ""], ["Majchrowski", "Annick", ""], ["Arteaga", "Asier", ""]]}, {"id": "2011.06507", "submitter": "Karl Schmeckpeper", "authors": "Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine,\n  Chelsea Finn", "title": "Reinforcement Learning with Videos: Combining Offline Observations with\n  Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful framework for robots to acquire skills\nfrom experience, but often requires a substantial amount of online data\ncollection. As a result, it is difficult to collect sufficiently diverse\nexperiences that are needed for robots to generalize broadly. Videos of humans,\non the other hand, are a readily available source of broad and interesting\nexperiences. In this paper, we consider the question: can we perform\nreinforcement learning directly on experience collected by humans? This problem\nis particularly difficult, as such videos are not annotated with actions and\nexhibit substantial visual domain shift relative to the robot's embodiment. To\naddress these challenges, we propose a framework for reinforcement learning\nwith videos (RLV). RLV learns a policy and value function using experience\ncollected by humans in combination with data collected by robots. In our\nexperiments, we find that RLV is able to leverage such videos to learn\nchallenging vision-based skills with less than half as many samples as RL\nmethods that learn from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:15:48 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Schmeckpeper", "Karl", ""], ["Rybkin", "Oleh", ""], ["Daniilidis", "Kostas", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "2011.06544", "submitter": "Jonas Gonzalez", "authors": "Jonas Gonzalez-Billandon, Lukas Grasse, Matthew Tata, Alessandra\n  Sciutti, Francesco Rea", "title": "Self-supervised reinforcement learning for speaker localisation with the\n  iCub humanoid robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the future robots will interact more and more with humans and will have to\ncommunicate naturally and efficiently. Automatic speech recognition systems\n(ASR) will play an important role in creating natural interactions and making\nrobots better companions. Humans excel in speech recognition in noisy\nenvironments and are able to filter out noise. Looking at a person's face is\none of the mechanisms that humans rely on when it comes to filtering speech in\nsuch noisy environments. Having a robot that can look toward a speaker could\nbenefit ASR performance in challenging environments. To this aims, we propose a\nself-supervised reinforcement learning-based framework inspired by the early\ndevelopment of humans to allow the robot to autonomously create a dataset that\nis later used to learn to localize speakers with a deep learning network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:02:15 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Gonzalez-Billandon", "Jonas", ""], ["Grasse", "Lukas", ""], ["Tata", "Matthew", ""], ["Sciutti", "Alessandra", ""], ["Rea", "Francesco", ""]]}, {"id": "2011.06549", "submitter": "Maxime Chaveroche", "authors": "Maxime Chaveroche, Franck Davoine, V\\'eronique Cherfaoui", "title": "Focal points and their implications for M\\\"obius Transforms and\n  Dempster-Shafer Theory", "comments": "Accepted for publication in Elsevier Information Sciences Journal", "journal-ref": "Information Sciences 555 (2021) 215-235", "doi": "10.1016/j.ins.2020.10.060", "report-no": null, "categories": "cs.DM cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dempster-Shafer Theory (DST) generalizes Bayesian probability theory,\noffering useful additional information, but suffers from a much higher\ncomputational burden. A lot of work has been done to reduce the time complexity\nof information fusion with Dempster's rule, which is a pointwise multiplication\nof two zeta transforms, and optimal general algorithms have been found to get\nthe complete definition of these transforms. Yet, it is shown in this paper\nthat the zeta transform and its inverse, the M\\\"obius transform, can be exactly\nsimplified, fitting the quantity of information contained in belief functions.\nBeyond that, this simplification actually works for any function on any\npartially ordered set. It relies on a new notion that we call focal point and\nthat constitutes the smallest domain on which both the zeta and M\\\"obius\ntransforms can be defined. We demonstrate the interest of these general results\nfor DST, not only for the reduction in complexity of most transformations\nbetween belief representations and their fusion, but also for theoretical\npurposes. Indeed, we provide a new generalization of the conjunctive\ndecomposition of evidence and formulas uncovering how each decomposition weight\nis tied to the corresponding mass function.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:08:23 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 02:50:35 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 17:33:51 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chaveroche", "Maxime", ""], ["Davoine", "Franck", ""], ["Cherfaoui", "V\u00e9ronique", ""]]}, {"id": "2011.06619", "submitter": "Annie Xie", "authors": "Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh", "title": "Learning Latent Representations to Influence Multi-Agent Interaction", "comments": "Conference on Robot Learning (CoRL) 2020. Supplementary website at\n  https://sites.google.com/view/latent-strategies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seamlessly interacting with humans or robots is hard because these agents are\nnon-stationary. They update their policy in response to the ego agent's\nbehavior, and the ego agent must anticipate these changes to co-adapt. Inspired\nby humans, we recognize that robots do not need to explicitly model every\nlow-level action another agent will make; instead, we can capture the latent\nstrategy of other agents through high-level representations. We propose a\nreinforcement learning-based framework for learning latent representations of\nan agent's policy, where the ego agent identifies the relationship between its\nbehavior and the other agent's future strategy. The ego agent then leverages\nthese latent dynamics to influence the other agent, purposely guiding them\ntowards policies suitable for co-adaptation. Across several simulated domains\nand a real-world air hockey game, our approach outperforms the alternatives and\nlearns to influence the other agent.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 19:04:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Xie", "Annie", ""], ["Losey", "Dylan P.", ""], ["Tolsma", "Ryan", ""], ["Finn", "Chelsea", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2011.06631", "submitter": "Bojun Huang", "authors": "Huang Bojun", "title": "Steady State Analysis of Episodic Reinforcement Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves that the episodic learning environment of every\nfinite-horizon decision task has a unique steady state under any behavior\npolicy, and that the marginal distribution of the agent's input indeed\nconverges to the steady-state distribution in essentially all episodic learning\nprocesses. This observation supports an interestingly reversed mindset against\nconventional wisdom: While the existence of unique steady states was often\npresumed in continual learning but considered less relevant in episodic\nlearning, it turns out their existence is guaranteed for the latter. Based on\nthis insight, the paper unifies episodic and continual RL around several\nimportant concepts that have been separately treated in these two RL\nformalisms. Practically, the existence of unique and approachable steady state\nenables a general way to collect data in episodic RL tasks, which the paper\napplies to policy gradient algorithms as a demonstration, based on a new\nsteady-state policy gradient theorem. Finally, the paper also proposes and\nexperimentally validates a perturbation method that facilitates rapid\nsteady-state convergence in real-world RL tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 19:34:59 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 17:40:34 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bojun", "Huang", ""]]}, {"id": "2011.06639", "submitter": "Sangeeta Satish Rao", "authors": "Sangeeta Satish Rao, Nikunj Phutela, V R Badri Prasad", "title": "Empirical Performance Analysis of Conventional Deep Learning Models for\n  Recognition of Objects in 2-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial Neural Networks, an essential part of Deep Learning, are derived\nfrom the structure and functionality of the human brain. It has a broad range\nof applications ranging from medical analysis to automated driving. Over the\npast few years, deep learning techniques have improved drastically - models can\nnow be customized to a much greater extent by varying the network architecture,\nnetwork parameters, among others. We have varied parameters like learning rate,\nfilter size, the number of hidden layers, stride size and the activation\nfunction among others to analyze the performance of the model and thus produce\na model with the highest performance. The model classifies images into 3\ncategories, namely, cars, faces and aeroplanes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 20:14:03 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Rao", "Sangeeta Satish", ""], ["Phutela", "Nikunj", ""], ["Prasad", "V R Badri", ""]]}, {"id": "2011.06665", "submitter": "Ole Meyer", "authors": "Jonas Andrulis, Ole Meyer, Gr\\'egory Schott, Samuel Weinbach and\n  Volker Gruhn", "title": "Domain-Level Explainability -- A Challenge for Creating Trust in\n  Superhuman AI Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For strategic problems, intelligent systems based on Deep Reinforcement\nLearning (DRL) have demonstrated an impressive ability to learn advanced\nsolutions that can go far beyond human capabilities, especially when dealing\nwith complex scenarios. While this creates new opportunities for the\ndevelopment of intelligent assistance systems with groundbreaking\nfunctionalities, applying this technology to real-world problems carries\nsignificant risks and therefore requires trust in their transparency and\nreliability. With superhuman strategies being non-intuitive and complex by\ndefinition and real-world scenarios prohibiting a reliable performance\nevaluation, the key components for trust in these systems are difficult to\nachieve. Explainable AI (XAI) has successfully increased transparency for\nmodern AI systems through a variety of measures, however, XAI research has not\nyet provided approaches enabling domain level insights for expert users in\nstrategic situations. In this paper, we discuss the existence of superhuman\nDRL-based strategies, their properties, the requirements and challenges for\ntransforming them into real-world environments, and the implications for trust\nthrough explainability as a key technology.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:42:02 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Andrulis", "Jonas", ""], ["Meyer", "Ole", ""], ["Schott", "Gr\u00e9gory", ""], ["Weinbach", "Samuel", ""], ["Gruhn", "Volker", ""]]}, {"id": "2011.06666", "submitter": "Azarakhsh Keipour", "authors": "Azarakhsh Keipour, Mohammadreza Mousaei, Andrew T Ashley, Sebastian\n  Scherer", "title": "Integration of Fully-Actuated Multirotors into Real-World Applications", "comments": "Submitted to the 2021 International Conference on Intelligent Robots\n  and Systems (IROS 2021) -- Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of fully-actuated multirotors has opened the door to new\npossibilities and more efficient solutions to many real-world applications.\nHowever, their integration had been slower than expected, partly due to the\nneed for new tools to take full advantage of these robots.\n  As far as we know, all the groups currently working on the fully-actuated\nmultirotors develop new full-pose (6-D) tools and methods to use their robots,\nwhich is inefficient, time-consuming, and requires many resources.\n  We propose a way of bridging the gap between the tools already available for\nunderactuated robots and the new fully-actuated vehicles. The approach can\nextend the existing underactuated flight controllers to support the\nfully-actuated robots, or enhance the existing fully-actuated controllers to\nsupport existing underactuated flight stacks. We introduce attitude strategies\nthat work with the underactuated controllers, tools, planners and remote\ncontrol interfaces, all while allowing taking advantage of the full actuation.\nMoreover, new methods are proposed that can properly handle the limited lateral\nthrust suffered by many fully-actuated UAV designs. The strategies are\nlightweight, simple, and allow rapid integration of the available tools with\nthese new vehicles for the fast development of new real-world applications.\n  The real experiments on our robots and simulations on several UAV\narchitectures with different underlying controller methods show how these\nstrategies can be utilized to extend existing flight controllers for\nfully-actuated applications. We have provided the source code for the PX4\nfirmware enhanced with our proposed methods to showcase an example flight\ncontroller for underactuated multirotors that can be modified to seamlessly\nsupport fully-actuated vehicles while retaining the rest of the flight stack\nunchanged.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:42:29 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 04:22:13 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Keipour", "Azarakhsh", ""], ["Mousaei", "Mohammadreza", ""], ["Ashley", "Andrew T", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2011.06709", "submitter": "David Krueger", "authors": "David Krueger, Jan Leike, Owain Evans, John Salvatier", "title": "Active Reinforcement Learning: Observing Rewards at a Cost", "comments": "Originally appeared at the NeurIPS 2016 \"Future of Interactive\n  Learning Machines (FILM)\" workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active reinforcement learning (ARL) is a variant on reinforcement learning\nwhere the agent does not observe the reward unless it chooses to pay a query\ncost c > 0. The central question of ARL is how to quantify the long-term value\nof reward information. Even in multi-armed bandits, computing the value of this\ninformation is intractable and we have to rely on heuristics. We propose and\nevaluate several heuristic approaches for ARL in multi-armed bandits and\n(tabular) Markov decision processes, and discuss and illustrate some\nchallenging aspects of the ARL problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 01:01:13 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 21:47:29 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Krueger", "David", ""], ["Leike", "Jan", ""], ["Evans", "Owain", ""], ["Salvatier", "John", ""]]}, {"id": "2011.06716", "submitter": "Sha Lu", "authors": "Sha Lu, Lin Liu, Jiuyong Li, Thuc Duy Le, Jixue Liu", "title": "Dependency-based Anomaly Detection: Framework, Methods and Benchmark", "comments": "39 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection is an important research problem because anomalies often\ncontain critical insights for understanding the unusual behavior in data. One\ntype of anomaly detection approach is dependency-based, which identifies\nanomalies by examining the violations of the normal dependency among variables.\nThese methods can discover subtle and meaningful anomalies with better\ninterpretation. Existing dependency-based methods adopt different\nimplementations and show different strengths and weaknesses. However, the\ntheoretical fundamentals and the general process behind them have not been well\nstudied. This paper proposes a general framework, DepAD, to provide a unified\nprocess for dependency-based anomaly detection. DepAD decomposes unsupervised\nanomaly detection tasks into feature selection and prediction problems.\nUtilizing off-the-shelf techniques, the DepAD framework can have various\ninstantiations to suit different application domains. Comprehensive experiments\nhave been conducted over one hundred instantiated DepAD methods with 32\nreal-world datasets to evaluate the performance of representative techniques in\nDepAD. To show the effectiveness of DepAD, we compare two DepAD methods with\nnine state-of-the-art anomaly detection methods, and the results show that\nDepAD methods outperform comparison methods in most cases. Through the DepAD\nframework, this paper gives guidance and inspiration for future research of\ndependency-based anomaly detection and provides a benchmark for its evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 01:39:44 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Lu", "Sha", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""], ["Le", "Thuc Duy", ""], ["Liu", "Jixue", ""]]}, {"id": "2011.06727", "submitter": "Zanbo Wang", "authors": "Zhiyong He, Zanbo Wang, Wei Wei, Shanshan Feng, Xianling Mao, and\n  Sheng Jiang", "title": "A Survey on Recent Advances in Sequence Labeling from Deep Learning\n  Models", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling (SL) is a fundamental research problem encompassing a\nvariety of tasks, e.g., part-of-speech (POS) tagging, named entity recognition\n(NER), text chunking, etc. Though prevalent and effective in many downstream\napplications (e.g., information retrieval, question answering, and knowledge\ngraph embedding), conventional sequence labeling approaches heavily rely on\nhand-crafted or language-specific features. Recently, deep learning has been\nemployed for sequence labeling tasks due to its powerful capability in\nautomatically learning complex features of instances and effectively yielding\nthe stat-of-the-art performances. In this paper, we aim to present a\ncomprehensive review of existing deep learning-based sequence labeling models,\nwhich consists of three related tasks, e.g., part-of-speech tagging, named\nentity recognition, and text chunking. Then, we systematically present the\nexisting approaches base on a scientific taxonomy, as well as the widely-used\nexperimental datasets and popularly-adopted evaluation metrics in the SL\ndomain. Furthermore, we also present an in-depth analysis of different SL\nmodels on the factors that may affect the performance and future directions in\nthe SL domain.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 02:29:50 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["He", "Zhiyong", ""], ["Wang", "Zanbo", ""], ["Wei", "Wei", ""], ["Feng", "Shanshan", ""], ["Mao", "Xianling", ""], ["Jiang", "Sheng", ""]]}, {"id": "2011.06775", "submitter": "Peide Cai", "authors": "Peide Cai, Hengli Wang, Yuxiang Sun, Ming Liu", "title": "DiGNet: Learning Scalable Self-Driving Policies for Generic Traffic\n  Scenarios with Graph Neural Networks", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional modular self-driving frameworks scale poorly in new scenarios,\nwhich usually require tedious hand-tuning of rules and parameters to maintain\nacceptable performance in all foreseeable occasions. Therefore, robust and safe\nself-driving using traditional frameworks is still challenging, especially in\ncomplex and dynamic environments. Recently, deep-learning based self-driving\nmethods have shown promising results with better generalization capability but\nless hand engineering effort. However, most of the previous learning-based\nmethods are trained and evaluated in limited driving scenarios with scattered\ntasks, such as lane-following, autonomous braking, and conditional driving. In\nthis paper, we propose a graph-based deep network to achieve scalable\nself-driving that can handle massive traffic scenarios. Specifically, more than\n7,000 km of evaluation is conducted in a high-fidelity driving simulator, in\nwhich our method can obey the traffic rules and safely navigate the vehicle in\na large variety of urban, rural, and highway environments, including\nunprotected left turns, narrow roads, roundabouts, and pedestrian-rich\nintersections. The results also show that our method achieves better\nperformance over the baselines in terms of success rate. This work is\naccompanied with some demonstration videos which are available at\nhttps://sites.google.com/view/dignet-self-driving/video-clips/\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 06:13:28 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 07:27:30 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cai", "Peide", ""], ["Wang", "Hengli", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2011.06780", "submitter": "Mingcheng Zuo", "authors": "Mingcheng Zuo, Guangming Dai, Lei Peng, Zhe Tang", "title": "A differential evolution-based optimization tool for interplanetary\n  transfer trajectory design", "comments": "The algorithm has been developed, and the results need a change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremely sensitive and highly nonlinear search space of interplanetary\ntransfer trajectory design bring about big challenges on global optimization.\nAs a representative, the current known best solution of the global trajectory\noptimization problem (GTOP) designed by the European space agency (ESA) is very\nhard to be found. To deal with this difficulty, a powerful differential\nevolution-based optimization tool named COoperative Differential Evolution\n(CODE) is proposed in this paper. CODE employs a two-stage evolutionary\nprocess, which concentrates on learning global structure in the earlier\nprocess, and tends to self-adaptively learn the structures of different local\nspaces. Besides, considering the spatial distribution of global optimum on\ndifferent problems and the gradient information on different variables, a\nmultiple boundary check technique has been employed. Also, Covariance Matrix\nAdaptation Evolutionary Strategies (CMA-ES) is used as a local optimizer. The\nprevious studies have shown that a specific swarm intelligent optimization\nalgorithm usually can solve only one or two GTOP problems. However, the\nexperimental test results show that CODE can find the current known best\nsolutions of Cassini1 and Sagas directly, and the cooperation with CMA-ES can\nsolve Cassini2, GTOC1, Messenger (reduced) and Rosetta. For the most\ncomplicated Messenger (full) problem, even though CODE cannot find the current\nknown best solution, the found best solution with objective function equaling\nto 3.38 km/s is still a level that other swarm intelligent algorithms cannot\neasily reach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 06:35:17 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 07:30:15 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 13:55:31 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zuo", "Mingcheng", ""], ["Dai", "Guangming", ""], ["Peng", "Lei", ""], ["Tang", "Zhe", ""]]}, {"id": "2011.06791", "submitter": "Giulia Cisotto", "authors": "Giulia Bressan, Selina C. Wriessnegger, Giulia Cisotto", "title": "Deep learning-based classification of fine hand movements from low\n  frequency EEG", "comments": null, "journal-ref": null, "doi": "10.3390/fi13050103", "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of different fine hand movements from EEG signals\nrepresents a relevant research challenge, e.g., in brain-computer interface\napplications for motor rehabilitation. Here, we analyzed two different datasets\nwhere fine hand movements (touch, grasp, palmar and lateral grasp) were\nperformed in a self-paced modality. We trained and tested a newly proposed\nconvolutional neural network (CNN), and we compared its classification\nperformance into respect to two well-established machine learning models,\nnamely, a shrinked-LDA and a Random Forest. Compared to previous literature, we\ntook advantage of the knowledge of the neuroscience field, and we trained our\nCNN model on the so-called Movement Related Cortical Potentials (MRCPs)s. They\nare EEG amplitude modulations at low frequencies, i.e., (0.3, 3) Hz, that have\nbeen proved to encode several properties of the movements, e.g., type of grasp,\nforce level and speed. We showed that CNN achieved good performance in both\ndatasets and they were similar or superior to the baseline models. Also,\ncompared to the baseline, our CNN requires a lighter and faster pre-processing\nprocedure, paving the way for its possible use in an online modality, e.g., for\nmany brain-computer interface applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:16:06 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 08:45:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bressan", "Giulia", ""], ["Wriessnegger", "Selina C.", ""], ["Cisotto", "Giulia", ""]]}, {"id": "2011.06794", "submitter": "Gilles Blanchard", "authors": "Hannah Marienwald (TUB), Jean-Baptiste Fermanian (ENS Rennes), Gilles\n  Blanchard (DATASHAPE, LMO, CNRS)", "title": "High-Dimensional Multi-Task Averaging and Application to Kernel Mean\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved estimator for the multi-task averaging problem, whose\ngoal is the joint estimation of the means of multiple distributions using\nseparate, independent data sets. The naive approach is to take the empirical\nmean of each data set individually, whereas the proposed method exploits\nsimilarities between tasks, without any related information being known in\nadvance. First, for each data set, similar or neighboring means are determined\nfrom the data by multiple testing. Then each naive estimator is shrunk towards\nthe local average of its neighbors. We prove theoretically that this approach\nprovides a reduction in mean squared error. This improvement can be significant\nwhen the dimension of the input space is large, demonstrating a \"blessing of\ndimensionality\" phenomenon. An application of this approach is the estimation\nof multiple kernel mean embeddings, which plays an important role in many\nmodern applications. The theoretical results are verified on artificial and\nreal world data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:31:30 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Marienwald", "Hannah", "", "TUB"], ["Fermanian", "Jean-Baptiste", "", "ENS Rennes"], ["Blanchard", "Gilles", "", "DATASHAPE, LMO, CNRS"]]}, {"id": "2011.06796", "submitter": "Lijing Wang", "authors": "Lijing Wang, Dipanjan Ghosh, Maria Teresa Gonzalez Diaz, Ahmed\n  Farahat, Mahbubul Alam, Chetan Gupta, Jiangzhuo Chen, Madhav Marathe", "title": "Wisdom of the Ensemble: Improving Consistency of Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers are assisting humans in making decisions and hence\nthe user's trust in these models is of paramount importance. Trust is often a\nfunction of constant behavior. From an AI model perspective it means given the\nsame input the user would expect the same output, especially for correct\noutputs, or in other words consistently correct outputs. This paper studies a\nmodel behavior in the context of periodic retraining of deployed models where\nthe outputs from successive generations of the models might not agree on the\ncorrect labels assigned to the same input. We formally define consistency and\ncorrect-consistency of a learning model. We prove that consistency and\ncorrect-consistency of an ensemble learner is not less than the average\nconsistency and correct-consistency of individual learners and\ncorrect-consistency can be improved with a probability by combining learners\nwith accuracy not less than the average accuracy of ensemble component\nlearners. To validate the theory using three datasets and two state-of-the-art\ndeep learning classifiers we also propose an efficient dynamic snapshot\nensemble method and demonstrate its value.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:47:01 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Wang", "Lijing", ""], ["Ghosh", "Dipanjan", ""], ["Diaz", "Maria Teresa Gonzalez", ""], ["Farahat", "Ahmed", ""], ["Alam", "Mahbubul", ""], ["Gupta", "Chetan", ""], ["Chen", "Jiangzhuo", ""], ["Marathe", "Madhav", ""]]}, {"id": "2011.06798", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang, Pengyuan Ren and Jianmin Li", "title": "Deep Template Matching for Pedestrian Attribute Recognition with the\n  Auxiliary Supervision of Attribute-wise Keypoints", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian Attribute Recognition (PAR) has aroused extensive attention due to\nits important role in video surveillance scenarios. In most cases, the\nexistence of a particular attribute is strongly related to a partial region.\nRecent works design complicated modules, e.g., attention mechanism and proposal\nof body parts to localize the attribute corresponding region. These works\nfurther prove that localization of attribute specific regions precisely will\nhelp in improving performance. However, these part-information-based methods\nare still not accurate as well as increasing model complexity which makes it\nhard to deploy on realistic applications. In this paper, we propose a Deep\nTemplate Matching based method to capture body parts features with less\ncomputation. Further, we also proposed an auxiliary supervision method that use\nhuman pose keypoints to guide the learning toward discriminative local cues.\nExtensive experiments show that the proposed method outperforms and has lower\ncomputational complexity, compared with the state-of-the-art approaches on\nlarge-scale pedestrian attribute datasets, including PETA, PA-100K, RAP, and\nRAPv2 zs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:52:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zhang", "Jiajun", ""], ["Ren", "Pengyuan", ""], ["Li", "Jianmin", ""]]}, {"id": "2011.06807", "submitter": "Zekun Li", "authors": "Zekun Li, Yujia Zheng, Shu Wu, Xiaoyu Zhang, Liang Wang", "title": "Heterogeneous Graph Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based collaborative filtering (CF) algorithms have gained increasing\nattention. Existing work in this literature usually models the user-item\ninteractions as a bipartite graph, where users and items are two isolated node\nsets and edges between them indicate their interactions. Then, the unobserved\npreference of users can be exploited by modeling high-order connectivity on the\nbipartite graph. In this work, we propose to model user-item interactions as a\nheterogeneous graph which consists of not only user-item edges indicating their\ninteraction but also user-user edges indicating their similarity. We develop\nheterogeneous graph collaborative filtering (HGCF), a GCN-based framework which\ncan explicitly capture both the interaction signal and similarity signal\nthrough embedding propagation on the heterogeneous graph. Since the\nheterogeneous graph is more connected than the bipartite graph, the sparsity\nissue can be alleviated and the demand for expensive high-order connectivity\nmodeling can be lowered. Extensive experiments conducted on three public\nbenchmarks demonstrate its superiority over the state-of-the-arts. Further\nanalysis verifies the importance of user-user edges in the graph, justifying\nthe rationality and effectiveness of HGCF.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 08:34:53 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 07:34:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Li", "Zekun", ""], ["Zheng", "Yujia", ""], ["Wu", "Shu", ""], ["Zhang", "Xiaoyu", ""], ["Wang", "Liang", ""]]}, {"id": "2011.06835", "submitter": "Otmane Sakhi", "authors": "Otmane Sakhi, Louis Faury, Flavian Vasile", "title": "Improving Offline Contextual Bandits with Distributional Robustness", "comments": "In Proceedings of the ACM RecSys Workshop on Reinforcement Learning\n  and Robust Estimators for Recommendation Systems (REVEAL 20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper extends the Distributionally Robust Optimization (DRO) approach\nfor offline contextual bandits. Specifically, we leverage this framework to\nintroduce a convex reformulation of the Counterfactual Risk Minimization\nprinciple. Besides relying on convex programs, our approach is compatible with\nstochastic optimization, and can therefore be readily adapted tothe large data\nregime. Our approach relies on the construction of asymptotic confidence\nintervals for offline contextual bandits through the DRO framework. By\nleveraging known asymptotic results of robust estimators, we also show how to\nautomatically calibrate such confidence intervals, which in turn removes the\nburden of hyper-parameter selection for policy optimization. We present\npreliminary empirical results supporting the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:52:16 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Sakhi", "Otmane", ""], ["Faury", "Louis", ""], ["Vasile", "Flavian", ""]]}, {"id": "2011.06850", "submitter": "Patrick Bordes Mr", "authors": "Patrick Bordes, Eloi Zablocki, Benjamin Piwowarski, Patrick Gallinari", "title": "Transductive Zero-Shot Learning using Cross-Modal CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Computer Vision, Zero-Shot Learning (ZSL) aims at classifying unseen\nclasses -- classes for which no matching training image exists. Most of ZSL\nworks learn a cross-modal mapping between images and class labels for seen\nclasses. However, the data distribution of seen and unseen classes might\ndiffer, causing a domain shift problem. Following this observation,\ntransductive ZSL (T-ZSL) assumes that unseen classes and their associated\nimages are known during training, but not their correspondence. As current\nT-ZSL approaches do not scale efficiently when the number of seen classes is\nhigh, we tackle this problem with a new model for T-ZSL based upon CycleGAN.\nOur model jointly (i) projects images on their seen class labels with a\nsupervised objective and (ii) aligns unseen class labels and visual exemplars\nwith adversarial and cycle-consistency objectives. We show the efficiency of\nour Cross-Modal CycleGAN model (CM-GAN) on the ImageNet T-ZSL task where we\nobtain state-of-the-art results. We further validate CM-GAN on a language\ngrounding task, and on a new task that we propose: zero-shot sentence-to-image\nmatching on MS COCO.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 10:37:29 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Bordes", "Patrick", ""], ["Zablocki", "Eloi", ""], ["Piwowarski", "Benjamin", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2011.06933", "submitter": "Mustafa Abdallah", "authors": "Mustafa Abdallah, Daniel Woods, Parinaz Naghizadeh, Issa Khalil,\n  Timothy Cason, Shreyas Sundaram, Saurabh Bagchi", "title": "Morshed: Guiding Behavioral Decision-Makers towards Better Security\n  Investment in Interdependent Systems", "comments": "Accepted to appear at the 16th ACM Asia Conference on Computer and\n  Communications Security (ASIACCS), 2021. arXiv admin note: text overlap with\n  arXiv:2004.01958", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the behavioral biases of human decision-making in securing\ninterdependent systems and show that such behavioral decision-making leads to a\nsuboptimal pattern of resource allocation compared to non-behavioral (rational)\ndecision-making. We provide empirical evidence for the existence of such\nbehavioral bias model through a controlled subject study with 145 participants.\nWe then propose three learning techniques for enhancing decision-making in\nmulti-round setups. We illustrate the benefits of our decision-making model\nthrough multiple interdependent real-world systems and quantify the level of\ngain compared to the case in which the defenders are behavioral. We also show\nthe benefit of our learning techniques against different attack models. We\nidentify the effects of different system parameters on the degree of\nsuboptimality of security outcomes due to behavioral decision-making.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:23:55 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 18:51:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Abdallah", "Mustafa", ""], ["Woods", "Daniel", ""], ["Naghizadeh", "Parinaz", ""], ["Khalil", "Issa", ""], ["Cason", "Timothy", ""], ["Sundaram", "Shreyas", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "2011.06958", "submitter": "Guillaume Vaudaux-Ruth", "authors": "Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard", "title": "SALAD: Self-Assessment Learning for Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature on self-assessment in machine learning mainly focuses on the\nproduction of well-calibrated algorithms through consensus frameworks i.e.\ncalibration is seen as a problem. Yet, we observe that learning to be properly\nconfident could behave like a powerful regularization and thus, could be an\nopportunity to improve performance.Precisely, we show that used within a\nframework of action detection, the learning of a self-assessment score is able\nto improve the whole action localization process.Experimental results show that\nour approach outperforms the state-of-the-art on two action detection\nbenchmarks. On THUMOS14 dataset, the mAP at tIoU@0.5 is improved from 42.8\\% to\n44.6\\%, and from 50.4\\% to 51.7\\% on ActivityNet1.3 dataset. For lower tIoU\nvalues, we achieve even more significant improvements on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:10:40 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Vaudaux-Ruth", "Guillaume", ""], ["Chan-Hon-Tong", "Adrien", ""], ["Achard", "Catherine", ""]]}, {"id": "2011.06985", "submitter": "Manfred Eppe", "authors": "Phuong D.H. Nguyen, Manfred Eppe and Stefan Wermter", "title": "Robotic self-representation improves manipulation skills and transfer\n  learning", "comments": "Submitted to IEEE Robotics and Automation Letters (RA-L) 2021 with\n  International Conference on Robotics and Automation Conference Option (ICRA)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive science suggests that the self-representation is critical for\nlearning and problem-solving. However, there is a lack of computational methods\nthat relate this claim to cognitively plausible robots and reinforcement\nlearning. In this paper, we bridge this gap by developing a model that learns\nbidirectional action-effect associations to encode the representations of body\nschema and the peripersonal space from multisensory information, which is named\nmultimodal BidAL. Through three different robotic experiments, we demonstrate\nthat this approach significantly stabilizes the learning-based problem-solving\nunder noisy conditions and that it improves transfer learning of robotic\nmanipulation skills.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:04:58 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Nguyen", "Phuong D. H.", ""], ["Eppe", "Manfred", ""], ["Wermter", "Stefan", ""]]}, {"id": "2011.07005", "submitter": "Geoffrey Clark", "authors": "Geoffrey Clark, Joseph Campbell, and Heni Ben Amor", "title": "Learning Predictive Models for Ergonomic Control of Prosthetic Devices", "comments": "Accepted to CoRL 2020. Accompanying video presentation:\n  https://www.youtube.com/watch?v=DxQPF3VwuoA&feature=youtu.be", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Model-Predictive Interaction Primitives -- a robot learning\nframework for assistive motion in human-machine collaboration tasks which\nexplicitly accounts for biomechanical impact on the human musculoskeletal\nsystem. First, we extend Interaction Primitives to enable predictive\nbiomechanics: the prediction of future biomechanical states of a human partner\nconditioned on current observations and intended robot control signals. In\nturn, we leverage this capability within a model-predictive control strategy to\nidentify the future ergonomic and biomechanical ramifications of potential\nrobot actions. Optimal control trajectories are selected so as to minimize\nfuture physical impact on the human musculoskeletal system. We empirically\ndemonstrate that our approach minimizes knee or muscle forces via generated\ncontrol actions selected according to biomechanical cost functions. Experiments\nare performed in synthetic and real-world experiments involving powered\nprosthetic devices.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:39:01 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Clark", "Geoffrey", ""], ["Campbell", "Joseph", ""], ["Amor", "Heni Ben", ""]]}, {"id": "2011.07010", "submitter": "Pasquale Antonante", "authors": "Pasquale Antonante, David I. Spivak, Luca Carlone", "title": "Monitoring and Diagnosability of Perception Systems", "comments": "Updated version of arXiv:2005.11816", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Perception is a critical component of high-integrity applications of robotics\nand autonomous systems, such as self-driving vehicles. In these applications,\nfailure of perception systems may put human life at risk, and a broad adoption\nof these technologies requires the development of methodologies to guarantee\nand monitor safe operation. Despite the paramount importance of perception\nsystems, currently there is no formal approach for system-level monitoring. In\nthis work, we propose a mathematical model for runtime monitoring and fault\ndetection and identification in perception systems. Towards this goal, we draw\nconnections with the literature on diagnosability in multiprocessor systems,\nand generalize it to account for modules with heterogeneous outputs that\ninteract over time. The resulting temporal diagnostic graphs (i) provide a\nframework to reason over the consistency of perception outputs -- across\nmodules and over time -- thus enabling fault detection, (ii) allow us to\nestablish formal guarantees on the maximum number of faults that can be\nuniquely identified in a given perception system, and (iii) enable the design\nof efficient algorithms for fault identification. We demonstrate our monitoring\nsystem, dubbed PerSyS, in realistic simulations using the LGSVL self-driving\nsimulator and the Apollo Auto autonomy software stack, and show that PerSyS is\nable to detect failures in challenging scenarios (including scenarios that have\ncaused self-driving car accidents in recent years), and is able to correctly\nidentify faults while entailing a minimal computation overhead (< 5 ms on a\nsingle-core CPU).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:03:14 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 18:30:29 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 16:51:50 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 22:23:17 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Antonante", "Pasquale", ""], ["Spivak", "David I.", ""], ["Carlone", "Luca", ""]]}, {"id": "2011.07013", "submitter": "Kian Kenyon-Dean", "authors": "Kian Kenyon-Dean, Edward Newell, Jackie Chi Kit Cheung", "title": "Deconstructing word embedding algorithms", "comments": "EMNLP 2020, 6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1911.13280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embeddings are reliable feature representations of words used to obtain\nhigh quality results for various NLP applications. Uncontextualized word\nembeddings are used in many NLP tasks today, especially in resource-limited\nsettings where high memory capacity and GPUs are not available. Given the\nhistorical success of word embeddings in NLP, we propose a retrospective on\nsome of the most well-known word embedding algorithms. In this work, we\ndeconstruct Word2vec, GloVe, and others, into a common form, unveiling some of\nthe common conditions that seem to be required for making performant word\nembeddings. We believe that the theoretical findings in this paper can provide\na basis for more informed development of future models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:23:35 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Kenyon-Dean", "Kian", ""], ["Newell", "Edward", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "2011.07026", "submitter": "Ali AlQallaf", "authors": "Ali AlQallaf, Gerardo Aragon-Camarasa", "title": "Enabling the Sense of Self in a Dual-Arm Robot", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While humans are aware of their body and capabilities, robots are not. To\naddress this, we present in this paper a neural network architecture that\nenables a dual-arm robot to get a sense of itself in an environment. Our\napproach is inspired by human self-awareness developmental levels and serves as\nthe underlying building block for a robot to achieve awareness of itself while\ncarrying out tasks in an environment. We assume that a robot has to know itself\nbefore interacting with the environment in order to be able to support\ndifferent robotic tasks. Hence, we implemented a neural network architecture to\nenable a robot to differentiate its limbs from the environment using visual and\nproprioception sensory inputs. We demonstrate experimentally that a robot can\ndistinguish itself with an accuracy of 88.7% on average in cluttered\nenvironmental settings and under confounding input signals.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:25:07 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["AlQallaf", "Ali", ""], ["Aragon-Camarasa", "Gerardo", ""]]}, {"id": "2011.07027", "submitter": "Joel Leibo", "authors": "Charles Beattie, Thomas K\\\"oppe, Edgar A. Du\\'e\\~nez-Guzm\\'an, Joel Z.\n  Leibo", "title": "DeepMind Lab2D", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepMind Lab2D, a scalable environment simulator for artificial\nintelligence research that facilitates researcher-led experimentation with\nenvironment design. DeepMind Lab2D was built with the specific needs of\nmulti-agent deep reinforcement learning researchers in mind, but it may also be\nuseful beyond that particular subfield.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:29:26 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 20:56:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Beattie", "Charles", ""], ["K\u00f6ppe", "Thomas", ""], ["Du\u00e9\u00f1ez-Guzm\u00e1n", "Edgar A.", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "2011.07035", "submitter": "Blake Camp", "authors": "Blake Camp, Jaya Krishna Mandivarapu, Rolando Estrada", "title": "Continual Learning with Deep Artificial Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neurons in real brains are enormously complex computational units. Among\nother things, they're responsible for transforming inbound electro-chemical\nvectors into outbound action potentials, updating the strengths of intermediate\nsynapses, regulating their own internal states, and modulating the behavior of\nother nearby neurons. One could argue that these cells are the only things\nexhibiting any semblance of real intelligence. It is odd, therefore, that the\nmachine learning community has, for so long, relied upon the assumption that\nthis complexity can be reduced to a simple sum and fire operation. We ask,\nmight there be some benefit to substantially increasing the computational power\nof individual neurons in artificial systems? To answer this question, we\nintroduce Deep Artificial Neurons (DANs), which are themselves realized as deep\nneural networks. Conceptually, we embed DANs inside each node of a traditional\nneural network, and we connect these neurons at multiple synaptic sites,\nthereby vectorizing the connections between pairs of cells. We demonstrate that\nit is possible to meta-learn a single parameter vector, which we dub a neuronal\nphenotype, shared by all DANs in the network, which facilitates a\nmeta-objective during deployment. Here, we isolate continual learning as our\nmeta-objective, and we show that a suitable neuronal phenotype can endow a\nsingle network with an innate ability to update its synapses with minimal\nforgetting, using standard backpropagation, without experience replay, nor\nseparate wake/sleep phases. We demonstrate this ability on sequential\nnon-linear regression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:50:10 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Camp", "Blake", ""], ["Mandivarapu", "Jaya Krishna", ""], ["Estrada", "Rolando", ""]]}, {"id": "2011.07057", "submitter": "Dongsheng Luo", "authors": "Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng\n  Chen, Xiang Zhang", "title": "Learning to Drop: Robust Graph Neural Network via Topological Denoising", "comments": "WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have shown to be powerful tools for graph\nanalytics. The key idea is to recursively propagate and aggregate information\nalong edges of the given graph. Despite their success, however, the existing\nGNNs are usually sensitive to the quality of the input graph. Real-world graphs\nare often noisy and contain task-irrelevant edges, which may lead to suboptimal\ngeneralization performance in the learned GNN models. In this paper, we propose\nPTDNet, a parameterized topological denoising network, to improve the\nrobustness and generalization performance of GNNs by learning to drop\ntask-irrelevant edges. PTDNet prunes task-irrelevant edges by penalizing the\nnumber of edges in the sparsified graph with parameterized networks. To take\ninto consideration of the topology of the entire graph, the nuclear norm\nregularization is applied to impose the low-rank constraint on the resulting\nsparsified graph for better generalization. PTDNet can be used as a key\ncomponent in GNN models to improve their performances on various tasks, such as\nnode classification and link prediction. Experimental studies on both synthetic\nand benchmark datasets show that PTDNet can improve the performance of GNNs\nsignificantly and the performance gain becomes larger for more noisy datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:53:21 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Luo", "Dongsheng", ""], ["Cheng", "Wei", ""], ["Yu", "Wenchao", ""], ["Zong", "Bo", ""], ["Ni", "Jingchao", ""], ["Chen", "Haifeng", ""], ["Zhang", "Xiang", ""]]}, {"id": "2011.07130", "submitter": "Adam Johs", "authors": "Adam J. Johs, Denise E. Agosto, Rosina O. Weber", "title": "Qualitative Investigation in Explainable Artificial Intelligence: A Bit\n  More Insight from Social Science", "comments": "Accepted to the AAAI 2021 Explainable Agency in Artificial\n  Intelligence Workshop, 10 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a focused analysis of user studies in explainable artificial\nintelligence (XAI) entailing qualitative investigation. We draw on social\nscience corpora to suggest ways for improving the rigor of studies where XAI\nresearchers use observations, interviews, focus groups, and/or questionnaires\nto capture qualitative data. We contextualize the presentation of the XAI\npapers included in our analysis according to the components of rigor described\nin the qualitative research literature: 1) underlying theories or frameworks,\n2) methodological approaches, 3) data collection methods, and 4) data analysis\nprocesses. The results of our analysis support calls from others in the XAI\ncommunity advocating for collaboration with experts from social disciplines to\nbolster rigor and effectiveness in user studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:02:16 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 22:22:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Johs", "Adam J.", ""], ["Agosto", "Denise E.", ""], ["Weber", "Rosina O.", ""]]}, {"id": "2011.07137", "submitter": "Harald Stromfelt Mr", "authors": "Harald Str\\\"omfelt, Luke Dickens, Artur d'Avila Garcez, Alessandra\n  Russo", "title": "On the Transferability of VAE Embeddings using Relational Knowledge with\n  Semi-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new model for relational VAE semi-supervision capable of\nbalancing disentanglement and low complexity modelling of relations with\ndifferent symbolic properties. We compare the relative benefits of\nrelation-decoder complexity and latent space structure on both inductive and\ntransductive transfer learning. Our results depict a complex picture where\nenforcing structure on semi-supervised representations can greatly improve\nzero-shot transductive transfer, but may be less favourable or even impact\nnegatively the capacity for inductive transfer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:40:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Str\u00f6mfelt", "Harald", ""], ["Dickens", "Luke", ""], ["Garcez", "Artur d'Avila", ""], ["Russo", "Alessandra", ""]]}, {"id": "2011.07158", "submitter": "Nicolas Schreuder", "authors": "Evgenii Chzhen and Nicolas Schreuder", "title": "An example of prediction which complies with Demographic Parity and\n  equalizes group-wise risks in the context of regression", "comments": "Presented at the NeurIPS 2020 Workshop on Algorithmic Fairness\n  through the Lens of Causality and Interpretability", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X, S, Y) \\in \\mathbb{R}^p \\times \\{1, 2\\} \\times \\mathbb{R}$ be a\ntriplet following some joint distribution $\\mathbb{P}$ with feature vector $X$,\nsensitive attribute $S$ , and target variable $Y$. The Bayes optimal prediction\n$f^*$ which does not produce Disparate Treatment is defined as $f^*(x) =\n\\mathbb{E}[Y | X = x]$. We provide a non-trivial example of a prediction $x \\to\nf(x)$ which satisfies two common group-fairness notions: Demographic Parity\n\\begin{align} (f(X) | S = 1) &\\stackrel{d}{=} (f(X) | S = 2) \\end{align} and\nEqual Group-Wise Risks \\begin{align}\n  \\mathbb{E}[(f^*(X) - f(X))^2 | S = 1] = \\mathbb{E}[(f^*(X) - f(X))^2 | S =\n2]. \\end{align} To the best of our knowledge this is the first explicit\nconstruction of a non-constant predictor satisfying the above. We discuss\nseveral implications of this result on better understanding of mathematical\nnotions of algorithmic fairness.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 22:46:05 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Schreuder", "Nicolas", ""]]}, {"id": "2011.07160", "submitter": "Nan Wu", "authors": "Nan Wu, Pengcheng Li", "title": "Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for\n  Emerging Storage Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.LG cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With data durability, high access speed, low power efficiency and byte\naddressability, NVMe and SSD, which are acknowledged representatives of\nemerging storage technologies, have been applied broadly in many areas.\nHowever, one key issue with high-performance adoption of these technologies is\nhow to properly define intelligent cache layers such that the performance gap\nbetween emerging technologies and main memory can be well bridged. To this end,\nwe propose Phoebe, a reuse-aware reinforcement learning framework for the\noptimal online caching that is applicable for a wide range of emerging storage\nmodels. By continuous interacting with the cache environment and the data\nstream, Phoebe is capable to extract critical temporal data dependency and\nrelative positional information from a single trace, becoming ever smarter over\ntime. To reduce training overhead during online learning, we utilize periodical\ntraining to amortize costs. Phoebe is evaluated on a set of Microsoft cloud\nstorage workloads. Experiment results show that Phoebe is able to close the gap\nof cache miss rate from LRU and a state-of-the-art online learning based cache\npolicy to the Belady's optimal policy by 70.3% and 52.6%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 22:55:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wu", "Nan", ""], ["Li", "Pengcheng", ""]]}, {"id": "2011.07177", "submitter": "Maria Florina Balcan", "authors": "Maria-Florina Balcan", "title": "Data-driven Algorithm Design", "comments": "Chapter 29 of the book Beyond the Worst-Case Analysis of Algorithms,\n  edited by Tim Roughgarden and published by Cambridge University Press (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven algorithm design is an important aspect of modern data science\nand algorithm design. Rather than using off the shelf algorithms that only have\nworst case performance guarantees, practitioners often optimize over large\nfamilies of parametrized algorithms and tune the parameters of these algorithms\nusing a training set of problem instances from their domain to determine a\nconfiguration with high expected performance over future instances. However,\nmost of this work comes with no performance guarantees. The challenge is that\nfor many combinatorial problems of significant importance including\npartitioning, subset selection, and alignment problems, a small tweak to the\nparameters can cause a cascade of changes in the algorithm's behavior, so the\nalgorithm's performance is a discontinuous function of its parameters.\n  In this chapter, we survey recent work that helps put data-driven\ncombinatorial algorithm design on firm foundations. We provide strong\ncomputational and statistical performance guarantees, both for the batch and\nonline scenarios where a collection of typical problem instances from the given\napplication are presented either all at once or in an online fashion,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 00:51:57 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Balcan", "Maria-Florina", ""]]}, {"id": "2011.07191", "submitter": "Sabera Talukder", "authors": "George Barnum, Sabera Talukder, Yisong Yue", "title": "On the Benefits of Early Fusion in Multimodal Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligently reasoning about the world often requires integrating data from\nmultiple modalities, as any individual modality may contain unreliable or\nincomplete information. Prior work in multimodal learning fuses input\nmodalities only after significant independent processing. On the other hand,\nthe brain performs multimodal processing almost immediately. This divide\nbetween conventional multimodal learning and neuroscience suggests that a\ndetailed study of early multimodal fusion could improve artificial multimodal\nrepresentations. To facilitate the study of early multimodal fusion, we create\na convolutional LSTM network architecture that simultaneously processes both\naudio and visual inputs, and allows us to select the layer at which audio and\nvisual information combines. Our results demonstrate that immediate fusion of\naudio and visual inputs in the initial C-LSTM layer results in higher\nperforming networks that are more robust to the addition of white noise in both\naudio and visual inputs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 01:58:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Barnum", "George", ""], ["Talukder", "Sabera", ""], ["Yue", "Yisong", ""]]}, {"id": "2011.07193", "submitter": "Kei Ota", "authors": "Kei Ota, Devesh K. Jha, Diego Romeres, Jeroen van Baar, Kevin A.\n  Smith, Takayuki Semitsu, Tomoaki Oiki, Alan Sullivan, Daniel Nikovski, and\n  Joshua B. Tenenbaum", "title": "Data-Efficient Learning for Complex and Real-Time Physical Problem\n  Solving using Augmented Simulation", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans quickly solve tasks in novel systems with complex dynamics, without\nrequiring much interaction. While deep reinforcement learning algorithms have\nachieved tremendous success in many complex tasks, these algorithms need a\nlarge number of samples to learn meaningful policies. In this paper, we present\na task for navigating a marble to the center of a circular maze. While this\nsystem is very intuitive and easy for humans to solve, it can be very difficult\nand inefficient for standard reinforcement learning algorithms to learn\nmeaningful policies. We present a model that learns to move a marble in the\ncomplex environment within minutes of interacting with the real system.\nLearning consists of initializing a physics engine with parameters estimated\nusing data from the real system. The error in the physics engine is then\ncorrected using Gaussian process regression, which is used to model the\nresidual between real observations and physics engine simulations. The physics\nengine augmented with the residual model is then used to control the marble in\nthe maze environment using a model-predictive feedback over a receding horizon.\nTo the best of our knowledge, this is the first time that a hybrid model\nconsisting of a full physics engine along with a statistical function\napproximator has been used to control a complex physical system in real-time\nusing nonlinear model-predictive control (NMPC).\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 02:03:08 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 02:30:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ota", "Kei", ""], ["Jha", "Devesh K.", ""], ["Romeres", "Diego", ""], ["van Baar", "Jeroen", ""], ["Smith", "Kevin A.", ""], ["Semitsu", "Takayuki", ""], ["Oiki", "Tomoaki", ""], ["Sullivan", "Alan", ""], ["Nikovski", "Daniel", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2011.07213", "submitter": "Wenxuan Zhou", "authors": "Wenxuan Zhou, Sujay Bajracharya, David Held", "title": "PLAS: Latent Action Space for Offline Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of offline reinforcement learning is to learn a policy from a fixed\ndataset, without further interactions with the environment. This setting will\nbe an increasingly more important paradigm for real-world applications of\nreinforcement learning such as robotics, in which data collection is slow and\npotentially dangerous. Existing off-policy algorithms have limited performance\non static datasets due to extrapolation errors from out-of-distribution\nactions. This leads to the challenge of constraining the policy to select\nactions within the support of the dataset during training. We propose to simply\nlearn the Policy in the Latent Action Space (PLAS) such that this requirement\nis naturally satisfied. We evaluate our method on continuous control benchmarks\nin simulation and a deformable object manipulation task with a physical robot.\nWe demonstrate that our method provides competitive performance consistently\nacross various continuous control tasks and different types of datasets,\noutperforming existing offline reinforcement learning methods with explicit\nconstraints. Videos and code are available at\nhttps://sites.google.com/view/latent-policy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 03:38:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Bajracharya", "Sujay", ""], ["Held", "David", ""]]}, {"id": "2011.07227", "submitter": "Hao Sheng", "authors": "Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher\n  Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara,\n  Ritesh Gautam, Robert B. Jackson, Andrew Y. Ng", "title": "OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep\n  Learning on Remotely Sensed Imagery", "comments": "Tackling Climate Change with Machine Learning at NeurIPS 2020\n  (Spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At least a quarter of the warming that the Earth is experiencing today is due\nto anthropogenic methane emissions. There are multiple satellites in orbit and\nplanned for launch in the next few years which can detect and quantify these\nemissions; however, to attribute methane emissions to their sources on the\nground, a comprehensive database of the locations and characteristics of\nemission sources worldwide is essential. In this work, we develop deep learning\nalgorithms that leverage freely available high-resolution aerial imagery to\nautomatically detect oil and gas infrastructure, one of the largest\ncontributors to global methane emissions. We use the best algorithm, which we\ncall OGNet, together with expert review to identify the locations of oil\nrefineries and petroleum terminals in the U.S. We show that OGNet detects many\nfacilities which are not present in four standard public datasets of oil and\ngas infrastructure. All detected facilities are associated with characteristics\nknown to contribute to methane emissions, including the infrastructure type and\nthe number of storage tanks. The data curated and produced in this study is\nfreely available at http://stanfordmlgroup.github.io/projects/ognet .\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 06:20:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sheng", "Hao", ""], ["Irvin", "Jeremy", ""], ["Munukutla", "Sasankh", ""], ["Zhang", "Shawn", ""], ["Cross", "Christopher", ""], ["Story", "Kyle", ""], ["Rustowicz", "Rose", ""], ["Elsworth", "Cooper", ""], ["Yang", "Zutao", ""], ["Omara", "Mark", ""], ["Gautam", "Ritesh", ""], ["Jackson", "Robert B.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "2011.07274", "submitter": "Serkan Sulun", "authors": "Serkan Sulun, Matthew E. P. Davies", "title": "On Filter Generalization for Music Bandwidth Extension Using Deep Neural\n  Networks", "comments": "Qualitative examples on https://serkansulun.com/bwe. Source code on\n  https://github.com/serkansulun/deep-music-enhancer", "journal-ref": null, "doi": "10.1109/JSTSP.2020.3037485", "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a sub-topic of the broad domain of audio\nenhancement, namely musical audio bandwidth extension. We formulate the\nbandwidth extension problem using deep neural networks, where a band-limited\nsignal is provided as input to the network, with the goal of reconstructing a\nfull-bandwidth output. Our main contribution centers on the impact of the\nchoice of low pass filter when training and subsequently testing the network.\nFor two different state of the art deep architectures, ResNet and U-Net, we\ndemonstrate that when the training and testing filters are matched,\nimprovements in signal-to-noise ratio (SNR) of up to 7dB can be obtained.\nHowever, when these filters differ, the improvement falls considerably and\nunder some training conditions results in a lower SNR than the band-limited\ninput. To circumvent this apparent overfitting to filter shape, we propose a\ndata augmentation strategy which utilizes multiple low pass filters during\ntraining and leads to improved generalization to unseen filtering conditions at\ntest time.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:41:28 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 08:45:20 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Sulun", "Serkan", ""], ["Davies", "Matthew E. P.", ""]]}, {"id": "2011.07290", "submitter": "Roxana R\\u{a}dulescu", "authors": "Roxana R\\u{a}dulescu, Timothy Verstraeten, Yijie Zhang, Patrick\n  Mannion, Diederik M. Roijers, Ann Now\\'e", "title": "Opponent Learning Awareness and Modelling in Multi-Objective Normal Form\n  Games", "comments": "Under review since 14 November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world multi-agent interactions consider multiple distinct criteria,\ni.e. the payoffs are multi-objective in nature. However, the same\nmulti-objective payoff vector may lead to different utilities for each\nparticipant. Therefore, it is essential for an agent to learn about the\nbehaviour of other agents in the system. In this work, we present the first\nstudy of the effects of such opponent modelling on multi-objective multi-agent\ninteractions with non-linear utilities. Specifically, we consider two-player\nmulti-objective normal form games with non-linear utility functions under the\nscalarised expected returns optimisation criterion. We contribute novel\nactor-critic and policy gradient formulations to allow reinforcement learning\nof mixed strategies in this setting, along with extensions that incorporate\nopponent policy reconstruction and learning with opponent learning awareness\n(i.e., learning while considering the impact of one's policy when anticipating\nthe opponent's learning step). Empirical results in five different MONFGs\ndemonstrate that opponent learning awareness and modelling can drastically\nalter the learning dynamics in this setting. When equilibria are present,\nopponent modelling can confer significant benefits on agents that implement it.\nWhen there are no Nash equilibria, opponent learning awareness and modelling\nallows agents to still converge to meaningful solutions that approximate\nequilibria.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 12:35:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["R\u0103dulescu", "Roxana", ""], ["Verstraeten", "Timothy", ""], ["Zhang", "Yijie", ""], ["Mannion", "Patrick", ""], ["Roijers", "Diederik M.", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "2011.07307", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao and Rui Yan", "title": "Meaningful Answer Generation of E-Commerce Question-Answering", "comments": "Accepted By TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In e-commerce portals, generating answers for product-related questions has\nbecome a crucial task. In this paper, we focus on the task of product-aware\nanswer generation, which learns to generate an accurate and complete answer\nfrom large-scale unlabeled e-commerce reviews and product attributes. However,\nsafe answer problems pose significant challenges to text generation tasks, and\ne-commerce question-answering task is no exception. To generate more meaningful\nanswers, in this paper, we propose a novel generative neural model, called the\nMeaningful Product Answer Generator (MPAG), which alleviates the safe answer\nproblem by taking product reviews, product attributes, and a prototype answer\ninto consideration. Product reviews and product attributes are used to provide\nmeaningful content, while the prototype answer can yield a more diverse answer\npattern. To this end, we propose a novel answer generator with a review\nreasoning module and a prototype answer reader. Our key idea is to obtain the\ncorrect question-aware information from a large scale collection of reviews and\nlearn how to write a coherent and meaningful answer from an existing prototype\nanswer. To be more specific, we propose a read-and-write memory consisting of\nselective writing units to conduct reasoning among these reviews. We then\nemploy a prototype reader consisting of comprehensive matching to extract the\nanswer skeleton from the prototype answer. Finally, we propose an answer editor\nto generate the final answer by taking the question and the above parts as\ninput. Conducted on a real-world dataset collected from an e-commerce platform,\nextensive experimental results show that our model achieves state-of-the-art\nperformance in terms of both automatic metrics and human evaluations. Human\nevaluation also demonstrates that our model can consistently generate specific\nand proper answers.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 14:05:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Ren", "Zhaochun", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2011.07312", "submitter": "Fabian Beigang", "authors": "Fabian Beigang", "title": "Shortcomings of Counterfactual Fairness and a Proposed Modification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I argue that counterfactual fairness does not constitute a\nnecessary condition for an algorithm to be fair, and subsequently suggest how\nthe constraint can be modified in order to remedy this shortcoming. To this\nend, I discuss a hypothetical scenario in which counterfactual fairness and an\nintuitive judgment of fairness come apart. Then, I turn to the question how the\nconcept of discrimination can be explicated in order to examine the\nshortcomings of counterfactual fairness as a necessary condition of algorithmic\nfairness in more detail. I then incorporate the insights of this analysis into\na novel fairness constraint, causal relevance fairness, which is a modification\nof the counterfactual fairness constraint that seems to circumvent its\nshortcomings.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 14:49:51 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Beigang", "Fabian", ""]]}, {"id": "2011.07318", "submitter": "Cristian Bodnar", "authors": "Cristian Bodnar, Karol Hausman, Gabriel Dulac-Arnold, Rico\n  Jonschkowski", "title": "A Geometric Perspective on Self-Supervised Policy Adaptation", "comments": "Contains 17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging aspects of real-world reinforcement learning (RL)\nis the multitude of unpredictable and ever-changing distractions that could\ndivert an agent from what was tasked to do in its training environment. While\nan agent could learn from reward signals to ignore them, the complexity of the\nreal-world can make rewards hard to acquire, or, at best, extremely sparse. A\nrecent class of self-supervised methods have shown promise that reward-free\nadaptation under challenging distractions is possible. However, previous work\nfocused on a short one-episode adaptation setting. In this paper, we consider a\nlong-term adaptation setup that is more akin to the specifics of the real-world\nand propose a geometric perspective on self-supervised adaptation. We\nempirically describe the processes that take place in the embedding space\nduring this adaptation process, reveal some of its undesirable effects on\nperformance and show how they can be eliminated. Moreover, we theoretically\nstudy how actor-based and actor-free agents can further generalise to the\ntarget environment by manipulating the geometry of the manifolds described by\nthe actor and critic functions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 15:16:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bodnar", "Cristian", ""], ["Hausman", "Karol", ""], ["Dulac-Arnold", "Gabriel", ""], ["Jonschkowski", "Rico", ""]]}, {"id": "2011.07357", "submitter": "Andrew Melnik", "authors": "Augustin Harter, Andrew Melnik, Gaurav Kumar, Dhruv Agarwal, Animesh\n  Garg, Helge Ritter", "title": "Solving Physics Puzzles by Reasoning about Paths", "comments": "1st NeurIPS workshop on Interpretable Inductive Biases and Physically\n  Structured Learning (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new deep learning model for goal-driven tasks that require\nintuitive physical reasoning and intervention in the scene to achieve a desired\nend goal. Its modular structure is motivated by hypothesizing a sequence of\nintuitive steps that humans apply when trying to solve such a task. The model\nfirst predicts the path the target object would follow without intervention and\nthe path the target object should follow in order to solve the task. Next, it\npredicts the desired path of the action object and generates the placement of\nthe action object. All components of the model are trained jointly in a\nsupervised way; each component receives its own learning signal but learning\nsignals are also backpropagated through the entire architecture. To evaluate\nthe model we use PHYRE - a benchmark test for goal-driven physical reasoning in\n2D mechanics puzzles.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 18:21:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Harter", "Augustin", ""], ["Melnik", "Andrew", ""], ["Kumar", "Gaurav", ""], ["Agarwal", "Dhruv", ""], ["Garg", "Animesh", ""], ["Ritter", "Helge", ""]]}, {"id": "2011.07368", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani and Nick Craswell", "title": "Conformer-Kernel with Query Term Independence at TREC 2020 Deep Learning\n  Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We benchmark Conformer-Kernel models under the strict blind evaluation\nsetting of the TREC 2020 Deep Learning track. In particular, we study the\nimpact of incorporating: (i) Explicit term matching to complement matching\nbased on learned representations (i.e., the \"Duet principle\"), (ii) query term\nindependence (i.e., the \"QTI assumption\") to scale the model to the full\nretrieval setting, and (iii) the ORCAS click data as an additional document\ndescription field. We find evidence which supports that all three\naforementioned strategies can lead to improved retrieval quality.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:03:24 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 23:57:45 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Hofstatter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "2011.07374", "submitter": "Jixuan Zhi", "authors": "Jixuan Zhi, Lap-Fai Yu and Jyh-Ming Lien", "title": "Designing Human-Robot Coexistence Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When the human-robot interactions become ubiquitous, the environment\nsurrounding these interactions will have significant impact on the safety and\ncomfort of the human and the effectiveness and efficiency of the robot.\nAlthough most robots are designed to work in the spaces created for humans,\nmany environments, such as living rooms and offices, can be and should be\nredesigned to enhance and improve human-robot collaboration and interactions.\nThis work uses autonomous wheelchair as an example and investigates the\ncomputational design in the human-robot coexistence spaces. Given the room size\nand the objects $O$ in the room, the proposed framework computes the optimal\nlayouts of $O$ that satisfy both human preferences and navigation constraints\nof the wheelchair. The key enabling technique is a motion planner that can\nefficiently evaluate hundreds of similar motion planning problems. Our\nimplementation shows that the proposed framework can produce a design around\nthree to five minutes on average comparing to 10 to 20 minutes without the\nproposed motion planner. Our results also show that the proposed method\nproduces reasonable designs even for tight spaces and for users with different\npreferences.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:32:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhi", "Jixuan", ""], ["Yu", "Lap-Fai", ""], ["Lien", "Jyh-Ming", ""]]}, {"id": "2011.07384", "submitter": "Valts Blukis", "authors": "Valts Blukis, Ross A. Knepper, Yoav Artzi", "title": "Few-shot Object Grounding and Mapping for Natural Language Robot\n  Instruction Following", "comments": "4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a robot policy to follow natural language\ninstructions that can be easily extended to reason about new objects. We\nintroduce a few-shot language-conditioned object grounding method trained from\naugmented reality data that uses exemplars to identify objects and align them\nto their mentions in instructions. We present a learned map representation that\nencodes object locations and their instructed use, and construct it from our\nfew-shot grounding output. We integrate this mapping approach into an\ninstruction-following policy, thereby allowing it to reason about previously\nunseen objects at test-time by simply adding exemplars. We evaluate on the task\nof learning to map raw observations and instructions to continuous control of a\nphysical quadcopter. Our approach significantly outperforms the prior state of\nthe art in the presence of new objects, even when the prior approach observes\nall objects during training.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 20:35:20 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Blukis", "Valts", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "2011.07394", "submitter": "Robert Henderson", "authors": "Robert D. E. Henderson, Xin Yi, Scott J. Adams and Paul Babyn", "title": "Automatic classification of multiple catheters in neonatal radiographs\n  with deep learning", "comments": "10 pages, 5 figures (+1 suppl.), 2 tables (+2 suppl.). Submitted to\n  Journal of Digital Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate a deep learning algorithm to classify multiple\ncatheters on neonatal chest and abdominal radiographs. A convolutional neural\nnetwork (CNN) was trained using a dataset of 777 neonatal chest and abdominal\nradiographs, with a split of 81%-9%-10% for training-validation-testing,\nrespectively. We employed ResNet-50 (a CNN), pre-trained on ImageNet. Ground\ntruth labelling was limited to tagging each image to indicate the presence or\nabsence of endotracheal tubes (ETTs), nasogastric tubes (NGTs), and umbilical\narterial and venous catheters (UACs, UVCs). The data set included 561 images\ncontaining 2 or more catheters, 167 images with only one, and 49 with none.\nPerformance was measured with average precision (AP), calculated from the area\nunder the precision-recall curve. On our test data, the algorithm achieved an\noverall AP (95% confidence interval) of 0.977 (0.679-0.999) for NGTs, 0.989\n(0.751-1.000) for ETTs, 0.979 (0.873-0.997) for UACs, and 0.937 (0.785-0.984)\nfor UVCs. Performance was similar for the set of 58 test images consisting of 2\nor more catheters, with an AP of 0.975 (0.255-1.000) for NGTs, 0.997\n(0.009-1.000) for ETTs, 0.981 (0.797-0.998) for UACs, and 0.937 (0.689-0.990)\nfor UVCs. Our network thus achieves strong performance in the simultaneous\ndetection of these four catheter types. Radiologists may use such an algorithm\nas a time-saving mechanism to automate reporting of catheters on radiographs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:27:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Henderson", "Robert D. E.", ""], ["Yi", "Xin", ""], ["Adams", "Scott J.", ""], ["Babyn", "Paul", ""]]}, {"id": "2011.07423", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Declarative Approaches to Counterfactual Explanations for Classification", "comments": "Revised and considerably extended version of journal submission after\n  reviews, by invitation. Based on RuleML-RR'20 paper [arXiv:2004.13237]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose answer-set programs that specify and compute counterfactual\ninterventions on entities that are input on a classification model. In relation\nto the outcome of the model, the resulting counterfactual entities serve as a\nbasis for the definition and computation of causality-based explanation scores\nfor the feature values in the entity under classification, namely\n\"responsibility scores\". The approach and the programs can be applied with\nblack-box models, and also with models that can be specified as logic programs,\nsuch as rule-based classifiers. The main focus of this work is on the\nspecification and computation of \"best\" counterfactual entities, i.e. those\nthat lead to maximum responsibility scores. From them one can read off the\nexplanations as maximum responsibility feature values in the original entity.\nWe also extend the programs to bring into the picture semantic or domain\nknowledge. We show how the approach could be extended by means of probabilistic\nmethods, and how the underlying probability distributions could be modified\nthrough the use of constraints.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:44:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:33:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "2011.07424", "submitter": "Zheng Wang", "authors": "Zhanhong Yan, Kaiming Yang, Zheng Wang, Bo Yang, Tsutomu Kaizuka,\n  Kimihiko Nakano", "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic guidance in a shared steering assistance system has drawn significant\nattention in intelligent vehicle fields, owing to its mutual communication\nability for vehicle control. By exerting continuous torque on the steering\nwheel, both the driver and support system can share lateral control of the\nvehicle. However, current haptic guidance steering systems demonstrate some\ndeficiencies in assisting lane changing. This study explored a new steering\ninteraction method, including the design and evaluation of an intention-based\nhaptic shared steering system. Such an intention-based method can support both\nlane keeping and lane changing assistance, by detecting a driver lane change\nintention. By using a deep learning-based method to model a driver decision\ntiming regarding lane crossing, an adaptive gain control method was proposed\nfor realizing a steering control system. An intention consistency method was\nproposed to detect whether the driver and the system were acting towards the\nsame target trajectories and to accurately capture the driver intention. A\ndriving simulator experiment was conducted to test the system performance.\nParticipants were required to perform six trials with assistive methods and one\ntrial without assistance. The results demonstrated that the supporting system\ndecreased the lane departure risk in the lane keeping tasks and could support a\nfast and stable lane changing maneuver.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:55:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yan", "Zhanhong", ""], ["Yang", "Kaiming", ""], ["Wang", "Zheng", ""], ["Yang", "Bo", ""], ["Kaizuka", "Tsutomu", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2011.07432", "submitter": "Jiayi Liu", "authors": "Wei Wei, Jiayi Liu, Xianling Mao, Guibin Guo, Feida Zhu, Pan Zhou,\n  Yuchong Hu and Shanshan Feng", "title": "Target Guided Emotion Aware Chat Machine", "comments": "To appear on TOIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency of a response to a given post at semantic-level and\nemotional-level is essential for a dialogue system to deliver human-like\ninteractions. However, this challenge is not well addressed in the literature,\nsince most of the approaches neglect the emotional information conveyed by a\npost while generating responses. This article addresses this problem by\nproposing a unifed end-to-end neural architecture, which is capable of\nsimultaneously encoding the semantics and the emotions in a post and leverage\ntarget information for generating more intelligent responses with appropriately\nexpressed emotions. Extensive experiments on real-world data demonstrate that\nthe proposed method outperforms the state-of-the-art methods in terms of both\ncontent coherence and emotion appropriateness.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:55:37 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 07:36:47 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wei", "Wei", ""], ["Liu", "Jiayi", ""], ["Mao", "Xianling", ""], ["Guo", "Guibin", ""], ["Zhu", "Feida", ""], ["Zhou", "Pan", ""], ["Hu", "Yuchong", ""], ["Feng", "Shanshan", ""]]}, {"id": "2011.07453", "submitter": "Kurtis Evan David", "authors": "Kurtis Evan David, Qiang Liu, Ruth Fong", "title": "Debiasing Convolutional Neural Networks via Meta Orthogonalization", "comments": "Accepted to NeuRIPS 2020 Workshop on Algorithmic Fairness through the\n  Lens of Causality and Interpretability (AFCI). Supplemental materials\n  provided at:\n  https://drive.google.com/drive/folders/1klIAqZDgg3sCVmzFjLw5Y_T-GTc2E3oh?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep learning models often achieve strong task performance, their\nsuccesses are hampered by their inability to disentangle spurious correlations\nfrom causative factors, such as when they use protected attributes (e.g., race,\ngender, etc.) to make decisions. In this work, we tackle the problem of\ndebiasing convolutional neural networks (CNNs) in such instances. Building off\nof existing work on debiasing word embeddings and model interpretability, our\nMeta Orthogonalization method encourages the CNN representations of different\nconcepts (e.g., gender and class labels) to be orthogonal to one another in\nactivation space while maintaining strong downstream task performance. Through\na variety of experiments, we systematically test our method and demonstrate\nthat it significantly mitigates model bias and is competitive against current\nadversarial debiasing methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 05:13:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["David", "Kurtis Evan", ""], ["Liu", "Qiang", ""], ["Fong", "Ruth", ""]]}, {"id": "2011.07460", "submitter": "Jacob Ouyang", "authors": "Jacob Ouyang, Isaac R Galatzer-Levy, Vidya Koesmahargyo, Li Zhang", "title": "Direct Classification of Emotional Intensity", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a model that can directly predict emotion intensity\nscore from video inputs, instead of deriving from action units. Using a 3d DNN\nincorporated with dynamic emotion information, we train a model using videos of\ndifferent people smiling that outputs an intensity score from 0-10. Each video\nis labeled framewise using a normalized action-unit based intensity score. Our\nmodel then employs an adaptive learning technique to improve performance when\ndealing with new subjects. Compared to other models, our model excels in\ngeneralization between different people as well as provides a new framework to\ndirectly classify emotional intensity.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 06:32:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ouyang", "Jacob", ""], ["Galatzer-Levy", "Isaac R", ""], ["Koesmahargyo", "Vidya", ""], ["Zhang", "Li", ""]]}, {"id": "2011.07495", "submitter": "Andrija Petrovic", "authors": "Andrija Petrovi\\'c, Mladen Nikoli\\'c, Sandro Radovanovi\\'c, Boris\n  Deliba\\v{s}i\\'c, Milo\\v{s} Jovanovi\\'c", "title": "FAIR: Fair Adversarial Instance Re-weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing awareness of societal impact of artificial intelligence,\nfairness has become an important aspect of machine learning algorithms. The\nissue is that human biases towards certain groups of population, defined by\nsensitive features like race and gender, are introduced to the training data\nthrough data collection and labeling. Two important directions of fairness\nensuring research have focused on (i) instance weighting in order to decrease\nthe impact of more biased instances and (ii) adversarial training in order to\nconstruct data representations informative of the target variable, but\nuninformative of the sensitive attributes. In this paper we propose a Fair\nAdversarial Instance Re-weighting (FAIR) method, which uses adversarial\ntraining to learn instance weighting function that ensures fair predictions.\nMerging the two paradigms, it inherits desirable properties from both --\ninterpretability of reweighting and end-to-end trainability of adversarial\ntraining. We propose four different variants of the method and, among other\nthings, demonstrate how the method can be cast in a fully probabilistic\nframework. Additionally, theoretical analysis of FAIR models' properties have\nbeen studied extensively. We compare FAIR models to 7 other related and\nstate-of-the-art models and demonstrate that FAIR is able to achieve a better\ntrade-off between accuracy and unfairness. To the best of our knowledge, this\nis the first model that merges reweighting and adversarial approaches by means\nof a weighting function that can provide interpretable information about\nfairness of individual instances.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 10:48:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Petrovi\u0107", "Andrija", ""], ["Nikoli\u0107", "Mladen", ""], ["Radovanovi\u0107", "Sandro", ""], ["Deliba\u0161i\u0107", "Boris", ""], ["Jovanovi\u0107", "Milo\u0161", ""]]}, {"id": "2011.07497", "submitter": "Tara Safavi", "authors": "Tara Safavi, Danai Koutra", "title": "Generating Negative Commonsense Knowledge", "comments": "Preprint, ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acquisition of commonsense knowledge is an important open challenge in\nartificial intelligence. In this work-in-progress paper, we study the task of\nautomatically augmenting commonsense knowledge bases (KBs) with novel\nstatements. We show empirically that obtaining meaningful negative samples for\nthe completion task is nontrivial, and propose NegatER, a framework for\ngenerating negative commonsense knowledge, to address this challenge. In our\nevaluation we demonstrate the intrinsic value and extrinsic utility of the\nknowledge generated by NegatER, opening up new avenues for future research in\nthis direction.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 10:55:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Safavi", "Tara", ""], ["Koutra", "Danai", ""]]}, {"id": "2011.07507", "submitter": "Md. Mushfiqur Rahman", "authors": "Md. Mushfiqur Rahman, Sabah Binte Noor, Fazlul Hasan Siddiqui", "title": "Automated Large-scale Class Scheduling in MiniZinc", "comments": null, "journal-ref": null, "doi": "10.1109/STI50764.2020.9350485", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class Scheduling is a highly constrained task. Educational institutes spend a\nlot of resources, in the form of time and manual computation, to find a\nsatisficing schedule that fulfills all the requirements. A satisficing class\nschedule accommodates all the students to all their desired courses at\nconvenient timing. The scheduler also needs to take into account the\navailability of course teachers on the given slots. With the added limitation\nof available classrooms, the number of solutions satisfying all constraints in\nthis huge search-space, further decreases.\n  This paper proposes an efficient system to generate class schedules that can\nfulfill every possible need of a typical university. Though it is primarily a\nfixed-credit scheduler, it can be adjusted for open-credit systems as well. The\nmodel is designed in MiniZinc and solved using various off-the-shelf solvers.\nThe proposed scheduling system can find a balanced schedule for a\nmoderate-sized educational institute in less than a minute.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 12:02:52 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Rahman", "Md. Mushfiqur", ""], ["Noor", "Sabah Binte", ""], ["Siddiqui", "Fazlul Hasan", ""]]}, {"id": "2011.07509", "submitter": "Md. Mushfiqur Rahman", "authors": "Md. Mushfiqur Rahman, Nahian Muhtasim Zahin, Kazi Raiyan Mahmud, Md.\n  Azmaeen Bin Ansar", "title": "Automated Intersection Management with MiniZinc", "comments": null, "journal-ref": null, "doi": "10.1109/STI50764.2020.9350408", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ill-managed intersections are the primary reasons behind the increasing\ntraffic problem in urban areas, leading to nonoptimal traffic-flow and\nunnecessary deadlocks. In this paper, we propose an automated intersection\nmanagement system that extracts data from a well-defined grid of sensors and\noptimizes traffic flow by controlling traffic signals. The data extraction\nmechanism is independent of the optimization algorithm and this paper primarily\nemphasizes the later one. We have used MiniZinc modeling language to define our\nsystem as a constraint satisfaction problem which can be solved using any\noff-the-shelf solver. The proposed system performs much better than the systems\ncurrently in use. Our system reduces the mean waiting time and standard\ndeviation of the waiting time of vehicles and avoids deadlocks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 12:11:05 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Rahman", "Md. Mushfiqur", ""], ["Zahin", "Nahian Muhtasim", ""], ["Mahmud", "Kazi Raiyan", ""], ["Ansar", "Md. Azmaeen Bin", ""]]}, {"id": "2011.07577", "submitter": "Harshit Rampal", "authors": "Dhruv Vashisht, Harshit Rampal, Haiguang Liao, Yang Lu, Devika\n  Shanbhag, Elias Fallon, Levent Burak Kara", "title": "Placement in Integrated Circuits using Cyclic Reinforcement Learning and\n  Simulated Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical design and production of Integrated Circuits (IC) is becoming\nincreasingly more challenging as the sophistication in IC technology is\nsteadily increasing. Placement has been one of the most critical steps in IC\nphysical design. Through decades of research, partition-based, analytical-based\nand annealing-based placers have been enriching the placement solution toolbox.\nHowever, open challenges including long run time and lack of ability to\ngeneralize continue to restrict wider applications of existing placement tools.\nWe devise a learning-based placement tool based on cyclic application of\nReinforcement Learning (RL) and Simulated Annealing (SA) by leveraging the\nadvancement of RL. Results show that the RL module is able to provide a better\ninitialization for SA and thus leads to a better final placement design.\nCompared to other recent learning-based placers, our method is majorly\ndifferent with its combination of RL and SA. It leverages the RL model's\nability to quickly get a good rough solution after training and the heuristic's\nability to realize greedy improvements in the solution.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:48:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Vashisht", "Dhruv", ""], ["Rampal", "Harshit", ""], ["Liao", "Haiguang", ""], ["Lu", "Yang", ""], ["Shanbhag", "Devika", ""], ["Fallon", "Elias", ""], ["Kara", "Levent Burak", ""]]}, {"id": "2011.07613", "submitter": "Swapnil Daga", "authors": "Swapnil Daga, Gokul B. Nair, Anirudha Ramesh, Rahul Sajnani, Junaid\n  Ahmed Ansari and K. Madhava Krishna", "title": "BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View", "comments": "Accepted in VISIGRAPP (VISAPP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present BirdSLAM, a novel simultaneous localization and\nmapping (SLAM) system for the challenging scenario of autonomous driving\nplatforms equipped with only a monocular camera. BirdSLAM tackles challenges\nfaced by other monocular SLAM systems (such as scale ambiguity in monocular\nreconstruction, dynamic object localization, and uncertainty in feature\nrepresentation) by using an orthographic (bird's-eye) view as the configuration\nspace in which localization and mapping are performed. By assuming only the\nheight of the ego-camera above the ground, BirdSLAM leverages single-view\nmetrology cues to accurately localize the ego-vehicle and all other traffic\nparticipants in bird's-eye view. We demonstrate that our system outperforms\nprior work that uses strictly greater information, and highlight the relevance\nof each design decision via an ablation analysis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 19:37:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Daga", "Swapnil", ""], ["Nair", "Gokul B.", ""], ["Ramesh", "Anirudha", ""], ["Sajnani", "Rahul", ""], ["Ansari", "Junaid Ahmed", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2011.07635", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Han Guo, Mohit Bansal", "title": "DORB: Dynamically Optimizing Multiple Rewards with Bandits", "comments": "EMNLP 2020 (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradients-based reinforcement learning has proven to be a promising\napproach for directly optimizing non-differentiable evaluation metrics for\nlanguage generation tasks. However, optimizing for a specific metric reward\nleads to improvements in mostly that metric only, suggesting that the model is\ngaming the formulation of that metric in a particular way without often\nachieving real qualitative improvements. Hence, it is more beneficial to make\nthe model optimize multiple diverse metric rewards jointly. While appealing,\nthis is challenging because one needs to manually decide the importance and\nscaling weights of these metric rewards. Further, it is important to consider\nusing a dynamic combination and curriculum of metric rewards that flexibly\nchanges over time. Considering the above aspects, in our work, we automate the\noptimization of multiple metric rewards simultaneously via a multi-armed bandit\napproach (DORB), where at each round, the bandit chooses which metric reward to\noptimize next, based on expected arm gains. We use the Exp3 algorithm for\nbandits and formulate two approaches for bandit rewards: (1) Single\nMulti-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit\n(HM-Bandit). We empirically show the effectiveness of our approaches via\nvarious automatic metrics and human evaluation on two important NLG tasks:\nquestion generation and data-to-text generation, including on an unseen-test\ntransfer setup. Finally, we present interpretable analyses of the learned\nbandit curriculum over the optimized rewards.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 21:57:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Guo", "Han", ""], ["Bansal", "Mohit", ""]]}, {"id": "2011.07636", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Niranjan Balasubramanian", "title": "Open4Business(O4B): An Open Access Dataset for Summarizing Business\n  Documents", "comments": "7 pages, 3 figures, accepted in Workshop on Dataset Curation and\n  Security-NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge in fine-tuning deep learning models for automatic\nsummarization is the need for large domain specific datasets. One of the\nbarriers to curating such data from resources like online publications is\nnavigating the license regulations applicable to their re-use, especially for\ncommercial purposes. As a result, despite the availability of several business\njournals there are no large scale datasets for summarizing business documents.\nIn this work, we introduce Open4Business(O4B),a dataset of 17,458 open access\nbusiness articles and their reference summaries. The dataset introduces a new\nchallenge for summarization in the business domain, requiring highly\nabstractive and more concise summaries as compared to other existing datasets.\nAdditionally, we evaluate existing models on it and consequently show that\nmodels trained on O4B and a 7x larger non-open access dataset achieve\ncomparable performance on summarization. We release the dataset, along with the\ncode which can be leveraged to similarly gather data for multiple domains.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 22:00:07 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 21:50:18 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 21:19:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Singh", "Amanpreet", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2011.07641", "submitter": "Alexander Lambert", "authors": "Alexander Lambert, Adam Fishman, Dieter Fox, Byron Boots, Fabio Ramos", "title": "Stein Variational Model Predictive Control", "comments": "Accepted to Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision making under uncertainty is critical to real-world, autonomous\nsystems. Model Predictive Control (MPC) methods have demonstrated favorable\nperformance in practice, but remain limited when dealing with complex\nprobability distributions. In this paper, we propose a generalization of MPC\nthat represents a multitude of solutions as posterior distributions. By casting\nMPC as a Bayesian inference problem, we employ variational methods for\nposterior computation, naturally encoding the complexity and multi-modality of\nthe decision making problem. We present a Stein variational gradient descent\nmethod to estimate the posterior directly over control parameters, given a cost\nfunction and observed state trajectories. We show that this framework leads to\nsuccessful planning in challenging, non-convex optimal control problems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 22:36:59 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 15:46:44 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 03:10:19 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 16:20:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lambert", "Alexander", ""], ["Fishman", "Adam", ""], ["Fox", "Dieter", ""], ["Boots", "Byron", ""], ["Ramos", "Fabio", ""]]}, {"id": "2011.07647", "submitter": "Tim Miller", "authors": "Simon Coghlan and Tim Miller and Jeannie Paterson", "title": "Good proctor or \"Big Brother\"? AI Ethics and Online Exam Supervision\n  Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article philosophically analyzes online exam supervision technologies,\nwhich have been thrust into the public spotlight due to campus lockdowns during\nthe COVID-19 pandemic and the growing demand for online courses. Online exam\nproctoring technologies purport to provide effective oversight of students\nsitting online exams, using artificial intelligence (AI) systems and human\ninvigilators to supplement and review those systems. Such technologies have\nalarmed some students who see them as `Big Brother-like', yet some universities\ndefend their judicious use. Critical ethical appraisal of online proctoring\ntechnologies is overdue. This article philosophically analyzes these\ntechnologies, focusing on the ethical concepts of academic integrity, fairness,\nnon-maleficence, transparency, privacy, respect for autonomy, liberty, and\ntrust. Most of these concepts are prominent in the new field of AI ethics and\nall are relevant to the education context. The essay provides ethical\nconsiderations that educational institutions will need to carefully review\nbefore electing to deploy and govern specific online proctoring technologies.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 22:53:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Coghlan", "Simon", ""], ["Miller", "Tim", ""], ["Paterson", "Jeannie", ""]]}, {"id": "2011.07660", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Abhay Zala, Graham Burri, Hao Tan, Mohit Bansal", "title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in\n  Dynamic Environments", "comments": "EMNLP Findings 2020 (18 pages; extended to Hindi)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For embodied agents, navigation is an important ability but not an isolated\ngoal. Agents are also expected to perform specific tasks after reaching the\ntarget location, such as picking up objects and assembling them into a\nparticular arrangement. We combine Vision-and-Language Navigation, assembling\nof collected objects, and object referring expression comprehension, to create\na novel joint navigation-and-assembly task, named ArraMon. During this task,\nthe agent (similar to a PokeMON GO player) is asked to find and collect\ndifferent target objects one-by-one by navigating based on natural language\ninstructions in a complex, realistic outdoor environment, but then also ARRAnge\nthe collected objects part-by-part in an egocentric grid-layout environment. To\nsupport this task, we implement a 3D dynamic environment simulator and collect\na dataset (in English; and also extended to Hindi) with human-written\nnavigation and assembling instructions, and the corresponding ground truth\ntrajectories. We also filter the collected instructions via a verification\nstage, leading to a total of 7.7K task instances (30.8K instructions and\npaths). We present results for several baseline models (integrated and biased)\nand metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance\ngap demonstrates that our task is challenging and presents a wide scope for\nfuture work. Our dataset, simulator, and code are publicly available at:\nhttps://arramonunc.github.io\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:30:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Hyounghun", ""], ["Zala", "Abhay", ""], ["Burri", "Graham", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2011.07661", "submitter": "Luca Parisi", "authors": "Luca Parisi, Renfei Ma, Narrendar RaviChandran and Matteo Lanzillotta", "title": "hyper-sinh: An Accurate and Reliable Function from Shallow to Deep\n  Learning in TensorFlow and Keras", "comments": "19 pages, 6 listings/Python code snippets, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation\nfunction suitable for Deep Learning (DL)-based algorithms for supervised\nlearning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in\nthe open source Python libraries TensorFlow and Keras, is thus described and\nvalidated as an accurate and reliable activation function for both shallow and\ndeep neural networks. Improvements in accuracy and reliability in image and\ntext classification tasks on five (N = 5) benchmark data sets available from\nKeras are discussed. Experimental results demonstrate the overall competitive\nclassification performance of both shallow and deep neural networks, obtained\nvia this novel function. This function is evaluated with respect to gold\nstandard activation functions, demonstrating its overall competitive accuracy\nand reliability for both image and text classification.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:38:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Parisi", "Luca", ""], ["Ma", "Renfei", ""], ["RaviChandran", "Narrendar", ""], ["Lanzillotta", "Matteo", ""]]}, {"id": "2011.07680", "submitter": "Wenting Xu", "authors": "Wenting Xu, Chang Qi, Zhenghua Xu and Thomas Lukasiewicz", "title": "Reinforced Medical Report Generation with X-Linear Attention and\n  Repetition Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To reduce doctors' workload, deep-learning-based automatic medical report\ngeneration has recently attracted more and more research efforts, where\nattention mechanisms and reinforcement learning are integrated with the classic\nencoder-decoder architecture to enhance the performance of deep models.\nHowever, these state-of-the-art solutions mainly suffer from two shortcomings:\n(i) their attention mechanisms cannot utilize high-order feature interactions,\nand (ii) due to the use of TF-IDF-based reward functions, these methods are\nfragile with generating repeated terms. Therefore, in this work, we propose a\nreinforced medical report generation solution with x-linear attention and\nrepetition penalty mechanisms (ReMRG-XR) to overcome these problems.\nSpecifically, x-linear attention modules are used to explore high-order feature\ninteractions and achieve multi-modal reasoning, while repetition penalty is\nused to apply penalties to repeated terms during the model's training process.\nExtensive experimental studies have been conducted on two public datasets, and\nthe results show that ReMRG-XR greatly outperforms the state-of-the-art\nbaselines in terms of all metrics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 01:44:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xu", "Wenting", ""], ["Qi", "Chang", ""], ["Xu", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2011.07682", "submitter": "Scott Freitas", "authors": "Scott Freitas, Yuxiao Dong, Joshua Neil, Duen Horng Chau", "title": "A Large-Scale Database for Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid emergence of graph representation learning, the construction\nof new large-scale datasets are necessary to distinguish model capabilities and\naccurately assess the strengths and weaknesses of each technique. By carefully\nanalyzing existing graph databases, we identify 3 critical components important\nfor advancing the field of graph representation learning: (1) large graphs, (2)\nmany graphs, and (3) class diversity. To date, no single graph database offers\nall of these desired properties. We introduce MalNet, the largest public graph\ndatabase ever constructed, representing a large-scale ontology of software\nfunction call graphs. MalNet contains over 1.2 million graphs, averaging over\n17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696\nfamilies. Compared to the popular REDDIT-12K database, MalNet offers 105x more\ngraphs, 44x larger graphs on average, and 63x the classes. We provide a\ndetailed analysis of MalNet, discussing its properties and provenance. The\nunprecedented scale and diversity of MalNet offers exciting opportunities to\nadvance the frontiers of graph representation learning---enabling new\ndiscoveries and research into imbalanced classification, explainability and the\nimpact of class hardness. The database is publically available at\nwww.mal-net.org.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 01:50:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Freitas", "Scott", ""], ["Dong", "Yuxiao", ""], ["Neil", "Joshua", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2011.07689", "submitter": "Yi Luo", "authors": "Yi Luo (1), Siyi Chen (2), X.-G. Ma (2) ((1) School of Energy and\n  Environment, Southeast University, Nanjing, China (2) International Institute\n  for Urban Systems Engineering, Southeast University, Nanjing, China)", "title": "Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and\n  Many Scenarios", "comments": "8 pages, 6 figures,conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presented a new drone-based face detection dataset Drone LAMS in\norder to solve issues of low performance of drone-based face detection in\nscenarios such as large angles which was a predominant working condition when a\ndrone flies high. The proposed dataset captured images from 261 videos with\nover 43k annotations and 4.0k images with pitch or yaw angle in the range of\n-90{\\deg} to 90{\\deg}. Drone LAMS showed significant improvement over currently\navailable drone-based face detection datasets in terms of detection\nperformance, especially with large pitch and yaw angle. Detailed analysis of\nhow key factors, such as duplication rate, annotation method, etc., impact\ndataset performance was also provided to facilitate further usage of a drone on\nface detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:26:05 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Luo", "Yi", ""], ["Chen", "Siyi", ""], ["Ma", "X. -G.", ""]]}, {"id": "2011.07693", "submitter": "Uwe Aickelin", "authors": "J Navrro, C Wagner, Uwe Aickelin, L Green, R Ashford", "title": "Measuring agreement on linguistic expressions in medical treatment\n  scenarios", "comments": "IEEE Symposium on Computational Intelligence, 6-9 Dec 2016, Athens,\n  Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality of life assessment represents a key process of deciding treatment\nsuccess and viability. As such, patients' perceptions of their functional\nstatus and well-being are important inputs for impairment assessment. Given\nthat patient completed questionnaires are often used to assess patient status\nand determine future treatment options, it is important to know the level of\nagreement of the words used by patients and different groups of medical\nprofessionals. In this paper, we propose a measure called the Agreement Ratio\nwhich provides a ratio of overall agreement when modelling words through Fuzzy\nSets (FSs). The measure has been specifically designed for assessing this\nagreement in fuzzy sets which are generated from data such as patient\nresponses. The measure relies on using the Jaccard Similarity Measure for\ncomparing the different levels of agreement in the FSs generated.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:36:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Navrro", "J", ""], ["Wagner", "C", ""], ["Aickelin", "Uwe", ""], ["Green", "L", ""], ["Ashford", "R", ""]]}, {"id": "2011.07699", "submitter": "Dhanoop Karunakaran", "authors": "Dhanoop Karunakaran, Stewart Worrall, Eduardo Nebot", "title": "Efficient falsification approach for autonomous vehicle validation using\n  a parameter optimisation technique based on reinforcement learning", "comments": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widescale deployment of Autonomous Vehicles (AV) appears to be imminent\ndespite many safety challenges that are yet to be resolved. It is well-known\nthat there are no universally agreed Verification and Validation (VV)\nmethodologies guarantee absolute safety, which is crucial for the acceptance of\nthis technology. The uncertainties in the behaviour of the traffic participants\nand the dynamic world cause stochastic reactions in advanced autonomous\nsystems. The addition of ML algorithms and probabilistic techniques adds\nsignificant complexity to the process for real-world testing when compared to\ntraditional methods. Most research in this area focuses on generating\nchallenging concrete scenarios or test cases to evaluate the system performance\nby looking at the frequency distribution of extracted parameters as collected\nfrom the real-world data. These approaches generally employ Monte-Carlo\nsimulation and importance sampling to generate critical cases. This paper\npresents an efficient falsification method to evaluate the System Under Test.\nThe approach is based on a parameter optimisation problem to search for\nchallenging scenarios. The optimisation process aims at finding the challenging\ncase that has maximum return. The method applies policy-gradient reinforcement\nlearning algorithm to enable the learning. The riskiness of the scenario is\nmeasured by the well established RSS safety metric, euclidean distance, and\ninstance of a collision. We demonstrate that by using the proposed method, we\ncan more efficiently search for challenging scenarios which could cause the\nsystem to fail in order to satisfy the safety requirements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:56:13 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Karunakaran", "Dhanoop", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2011.07713", "submitter": "Shalabh Gupta", "authors": "Jing Yang and James P. Wilson and Shalabh Gupta", "title": "DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs\n  for AUV Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of sensing, control and robotic technologies, autonomous\nunderwater vehicles (AUVs) have become useful assistants to human divers for\nperforming various underwater operations. In the current practice, the divers\nare required to carry expensive, bulky, and waterproof keyboards or\njoystick-based controllers for supervision and control of AUVs. Therefore,\ndiver action-based supervision is becoming increasingly popular because it is\nconvenient, easier to use, faster, and cost effective. However, the various\nenvironmental, diver and sensing uncertainties present underwater makes it\nchallenging to train a robust and reliable diver action recognition system. In\nthis regard, this paper presents DARE, a diver action recognition system, that\nis trained based on Cognitive Autonomous Driving Buddy (CADDY) dataset, which\nis a rich set of data containing images of different diver gestures and poses\nin several different and realistic underwater environments. DARE is based on\nfusion of stereo-pairs of camera images using a multi-channel convolutional\nneural network supported with a systematically trained tree-topological deep\nneural network classifier to enhance the classification performance. DARE is\nfast and requires only a few milliseconds to classify one stereo-pair, thus\nmaking it suitable for real-time underwater implementation. DARE is\ncomparatively evaluated against several existing classifier architectures and\nthe results show that DARE supersedes the performance of all classifiers for\ndiver action recognition in terms of overall as well as individual class\naccuracies and F1-scores.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 04:05:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Jing", ""], ["Wilson", "James P.", ""], ["Gupta", "Shalabh", ""]]}, {"id": "2011.07735", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Gurneet Arora, Navpreet Kaloty", "title": "iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video\n  Captioning and Video Question Answering", "comments": "13 pages, 6 figures, 4 tables, Project Page:\n  https://iperceive.amanchadha.com", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior art in visual understanding relies solely on analyzing the \"what\"\n(e.g., event recognition) and \"where\" (e.g., event localization), which in some\ncases, fails to describe correct contextual relationships between events or\nleads to incorrect underlying visual attention. Part of what defines us as\nhuman and fundamentally different from machines is our instinct to seek\ncausality behind any association, say an event Y that happened as a direct\nresult of event X. To this end, we propose iPerceive, a framework capable of\nunderstanding the \"why\" between events in a video by building a common-sense\nknowledge base using contextual cues to infer causal relationships between\nobjects in the video. We demonstrate the effectiveness of our technique using\nthe dense video captioning (DVC) and video question answering (VideoQA) tasks.\nFurthermore, while most prior work in DVC and VideoQA relies solely on visual\ninformation, other modalities such as audio and speech are vital for a human\nobserver's perception of an environment. We formulate DVC and VideoQA tasks as\nmachine translation problems that utilize multiple modalities. By evaluating\nthe performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet\nCaptions and TVQA datasets respectively, we show that our approach furthers the\nstate-of-the-art. Code and samples are available at: iperceive.amanchadha.com.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:44:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chadha", "Aman", ""], ["Arora", "Gurneet", ""], ["Kaloty", "Navpreet", ""]]}, {"id": "2011.07743", "submitter": "Yu Gu", "authors": "Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng\n  Yan, Yu Su", "title": "Beyond I.I.D.: Three Levels of Generalization for Question Answering on\n  Knowledge Bases", "comments": "Accepted to TheWebConf 2021 (previously WWW)", "journal-ref": null, "doi": "10.1145/3442381.3449992", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing studies on question answering on knowledge bases (KBQA) mainly\noperate with the standard i.i.d assumption, i.e., training distribution over\nquestions is the same as the test distribution. However, i.i.d may be neither\nreasonably achievable nor desirable on large-scale KBs because 1) true user\ndistribution is hard to capture and 2) randomly sample training examples from\nthe enormous space would be highly data-inefficient. Instead, we suggest that\nKBQA models should have three levels of built-in generalization: i.i.d,\ncompositional, and zero-shot. To facilitate the development of KBQA models with\nstronger generalization, we construct and release a new large-scale,\nhigh-quality dataset with 64,331 questions, GrailQA, and provide evaluation\nsettings for all three levels of generalization. In addition, we propose a\nnovel BERT-based KBQA model. The combination of our dataset and model enables\nus to thoroughly examine and demonstrate, for the first time, the key role of\npre-trained contextual embeddings like BERT in the generalization of KBQA.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:36:26 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 03:36:38 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 03:13:37 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 18:48:38 GMT"}, {"version": "v5", "created": "Fri, 19 Feb 2021 04:11:23 GMT"}, {"version": "v6", "created": "Mon, 22 Feb 2021 19:04:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gu", "Yu", ""], ["Kase", "Sue", ""], ["Vanni", "Michelle", ""], ["Sadler", "Brian", ""], ["Liang", "Percy", ""], ["Yan", "Xifeng", ""], ["Su", "Yu", ""]]}, {"id": "2011.07748", "submitter": "Guanya Shi", "authors": "Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Fabio\n  Ramos, Animashree Anandkumar, Yuke Zhu", "title": "Fast Uncertainty Quantification for Deep Object Pose Estimation", "comments": "Video and code are available at https://sites.google.com/view/fastuq", "journal-ref": "International Conferenceon Robotics and Automation (ICRA), 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based object pose estimators are often unreliable and\noverconfident especially when the input image is outside the training domain,\nfor instance, with sim2real transfer. Efficient and robust uncertainty\nquantification (UQ) in pose estimators is critically needed in many robotic\ntasks. In this work, we propose a simple, efficient, and plug-and-play UQ\nmethod for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models\nwith different neural network architectures and/or training data sources, and\ncompute their average pairwise disagreement against one another to obtain the\nuncertainty quantification. We propose four disagreement metrics, including a\nlearned metric, and show that the average distance (ADD) is the best\nlearning-free metric and it is only slightly worse than the learned metric,\nwhich requires labeled target data. Our method has several advantages compared\nto the prior art: 1) our method does not require any modification of the\ntraining process or the model inputs; and 2) it needs only one forward pass for\neach model. We evaluate the proposed UQ method on three tasks where our\nuncertainty quantification yields much stronger correlations with pose\nestimation errors than the baselines. Moreover, in a real robot grasping task,\nour method increases the grasping success rate from 35% to 90%.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:51:55 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 20:38:01 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 05:13:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Shi", "Guanya", ""], ["Zhu", "Yifeng", ""], ["Tremblay", "Jonathan", ""], ["Birchfield", "Stan", ""], ["Ramos", "Fabio", ""], ["Anandkumar", "Animashree", ""], ["Zhu", "Yuke", ""]]}, {"id": "2011.07751", "submitter": "Pengpeng Shao", "authors": "Pengpeng Shao, Guohua Yang, Dawei Zhang, Jianhua Tao, Feihu Che, Tong\n  Liu", "title": "Tucker decomposition-based Temporal Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": "3467828", "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs have been demonstrated to be an effective tool for numerous\nintelligent applications. However, a large amount of valuable knowledge still\nexists implicitly in the knowledge graphs. To enrich the existing knowledge\ngraphs, recent years witness that many algorithms for link prediction and\nknowledge graphs embedding have been designed to infer new facts. But most of\nthese studies focus on the static knowledge graphs and ignore the temporal\ninformation that reflects the validity of knowledge. Developing the model for\ntemporal knowledge graphs completion is an increasingly important task. In this\npaper, we build a new tensor decomposition model for temporal knowledge graphs\ncompletion inspired by the Tucker decomposition of order 4 tensor. We\ndemonstrate that the proposed model is fully expressive and report\nstate-of-the-art results for several public benchmarks. Additionally, we\npresent several regularization schemes to improve the strategy and study their\nimpact on the proposed model. Experimental studies on three temporal datasets\n(i.e. ICEWS2014, ICEWS2005-15, GDELT) justify our design and demonstrate that\nour model outperforms baselines with an explicit margin on link prediction\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:05:52 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shao", "Pengpeng", ""], ["Yang", "Guohua", ""], ["Zhang", "Dawei", ""], ["Tao", "Jianhua", ""], ["Che", "Feihu", ""], ["Liu", "Tong", ""]]}, {"id": "2011.07759", "submitter": "Yuan Chang", "authors": "Yuan Chang, Chao Yan, Xingyu Liu, Xiangke Wang, Han Zhou, Xiaojia\n  Xiang, Dengqing Tang", "title": "Time-Efficient Mars Exploration of Simultaneous Coverage and Charging\n  with Multiple Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a time-efficient scheme for Mars exploration by the\ncooperation of multiple drones and a rover. To maximize effective coverage of\nthe Mars surface in the long run, a comprehensive framework has been developed\nwith joint consideration for limited energy, sensor model, communication range\nand safety radius, which we call TIME-SC2 (TIme-efficient Mars Exploration of\nSimultaneous Coverage and Charging). First, we propose a multi-drone coverage\ncontrol algorithm by leveraging emerging deep reinforcement learning and design\na novel information map to represent dynamic system states. Second, we propose\na near-optimal charging scheduling algorithm to navigate each drone to an\nindividual charging slot, and we have proven that there always exists feasible\nsolutions. The attractiveness of this framework not only resides on its ability\nto maximize exploration efficiency, but also on its high autonomy that has\ngreatly reduced the non-exploring time. Extensive simulations have been\nconducted to demonstrate the remarkable performance of TIME-SC2 in terms of\ntime-efficiency, adaptivity and flexibility.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:28:37 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chang", "Yuan", ""], ["Yan", "Chao", ""], ["Liu", "Xingyu", ""], ["Wang", "Xiangke", ""], ["Zhou", "Han", ""], ["Xiang", "Xiaojia", ""], ["Tang", "Dengqing", ""]]}, {"id": "2011.07785", "submitter": "Ji Woong Kim", "authors": "Ji Woong Kim, Changyan He, Muller Urias, Peter Gehlbach, Gregory D.\n  Hager, Iulian Iordachita, Marin Kobilarov", "title": "Autonomously Navigating a Surgical Tool Inside the Eye by Learning from\n  Demonstration", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental challenge in retinal surgery is safely navigating a surgical\ntool to a desired goal position on the retinal surface while avoiding damage to\nsurrounding tissues, a procedure that typically requires tens-of-microns\naccuracy. In practice, the surgeon relies on depth-estimation skills to\nlocalize the tool-tip with respect to the retina in order to perform the\ntool-navigation task, which can be prone to human error. To alleviate such\nuncertainty, prior work has introduced ways to assist the surgeon by estimating\nthe tool-tip distance to the retina and providing haptic or auditory feedback.\nHowever, automating the tool-navigation task itself remains unsolved and\nlargely unexplored. Such a capability, if reliably automated, could serve as a\nbuilding block to streamline complex procedures and reduce the chance for\ntissue damage. Towards this end, we propose to automate the tool-navigation\ntask by learning to mimic expert demonstrations of the task. Specifically, a\ndeep network is trained to imitate expert trajectories toward various locations\non the retina based on recorded visual servoing to a given goal specified by\nthe user. The proposed autonomous navigation system is evaluated in simulation\nand in physical experiments using a silicone eye phantom. We show that the\nnetwork can reliably navigate a needle surgical tool to various desired\nlocations within 137 microns accuracy in physical experiments and 94 microns in\nsimulation on average, and generalizes well to unseen situations such as in the\npresence of auxiliary surgical tools, variable eye backgrounds, and brightness\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:30:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Ji Woong", ""], ["He", "Changyan", ""], ["Urias", "Muller", ""], ["Gehlbach", "Peter", ""], ["Hager", "Gregory D.", ""], ["Iordachita", "Iulian", ""], ["Kobilarov", "Marin", ""]]}, {"id": "2011.07798", "submitter": "Miao Cheng", "authors": "Miao Cheng, Xinge You", "title": "Adaptive Matching of Kernel Means", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a promising step, the performance of data analysis and feature learning\nare able to be improved if certain pattern matching mechanism is available. One\nof the feasible solutions can refer to the importance estimation of instances,\nand consequently, kernel mean matching (KMM) has become an important method for\nknowledge discovery and novelty detection in kernel machines. Furthermore, the\nexisting KMM methods have focused on concrete learning frameworks. In this\nwork, a novel approach to adaptive matching of kernel means is proposed, and\nselected data with high importance are adopted to achieve calculation\nefficiency with optimization. In addition, scalable learning can be conducted\nin proposed method as a generalized solution to matching of appended data. The\nexperimental results on a wide variety of real-world data sets demonstrate the\nproposed method is able to give outstanding performance compared with several\nstate-of-the-art methods, while calculation efficiency can be preserved.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:00:14 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cheng", "Miao", ""], ["You", "Xinge", ""]]}, {"id": "2011.07801", "submitter": "Guannan Hu", "authors": "Guannan Hu, Wu Zhang, Hu Ding, Wenhao Zhu", "title": "Gradient Episodic Memory with a Soft Constraint for Continual Learning", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting in continual learning is a common destructive\nphenomenon in gradient-based neural networks that learn sequential tasks, and\nit is much different from forgetting in humans, who can learn and accumulate\nknowledge throughout their whole lives. Catastrophic forgetting is the fatal\nshortcoming of a large decrease in performance on previous tasks when the model\nis learning a novel task. To alleviate this problem, the model should have the\ncapacity to learn new knowledge and preserve learned knowledge. We propose an\naverage gradient episodic memory (A-GEM) with a soft constraint $\\epsilon \\in\n[0, 1]$, which is a balance factor between learning new knowledge and\npreserving learned knowledge; our method is called gradient episodic memory\nwith a soft constraint $\\epsilon$ ($\\epsilon$-SOFT-GEM). $\\epsilon$-SOFT-GEM\noutperforms A-GEM and several continual learning benchmarks in a single\ntraining epoch; additionally, it has state-of-the-art average accuracy and\nefficiency for computation and memory, like A-GEM, and provides a better\ntrade-off between the stability of preserving learned knowledge and the\nplasticity of learning new knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:06:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hu", "Guannan", ""], ["Zhang", "Wu", ""], ["Ding", "Hu", ""], ["Zhu", "Wenhao", ""]]}, {"id": "2011.07846", "submitter": "Na Young Ahn", "authors": "N. Y. Ahn, D.H. Lee", "title": "Secure Vehicle Communications Using Proof-of-Nonce Blockchain", "comments": "6 pages, 4 figures, IEEE Communication Magazine (Submitted on Nov.\n  16, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an autonomous driving that achieves physical layer\nsecurity. Proposed vehicle communication is implemented based on Proof-of-Nonce\n(PoN) blockchain algorithm. PoN blockchain algorithm is a consensus algorithm\nthat can be implemented in light weight. We propose a more secure vehicle\ncommunication scheme while achieving physical layer security by defecting PoN\nalgorithm and secrecy capacity. By generating a block only when secrecy\ncapacity is greater than or equal to the reference value, traffic information\ncan be provided only to vehicles with physical layer security. This vehicle\ncommunication scheme can secure sufficient safety even from hackers based on\nquantum computing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 10:31:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ahn", "N. Y.", ""], ["Lee", "D. H.", ""]]}, {"id": "2011.07876", "submitter": "Marco Huber", "authors": "Nadia Burkart and Marco F. Huber", "title": "A Survey on the Explainability of Supervised Machine Learning", "comments": "Accepted for publication at the Journal of Artificial Intelligence\n  Research (JAIR)", "journal-ref": "Journal of Artificial Intelligence Research (JAIR), 70:245-317,\n  2021", "doi": "10.1613/jair.1.12228", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions obtained by, e.g., artificial neural networks have a high\naccuracy but humans often perceive the models as black boxes. Insights about\nthe decision making are mostly opaque for humans. Particularly understanding\nthe decision making in highly sensitive areas such as healthcare or fifinance,\nis of paramount importance. The decision-making behind the black boxes requires\nit to be more transparent, accountable, and understandable for humans. This\nsurvey paper provides essential definitions, an overview of the different\nprinciples and methodologies of explainable Supervised Machine Learning (SML).\nWe conduct a state-of-the-art survey that reviews past and recent explainable\nSML approaches and classifies them according to the introduced definitions.\nFinally, we illustrate principles by means of an explanatory case study and\ndiscuss important future directions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:25:39 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Burkart", "Nadia", ""], ["Huber", "Marco F.", ""]]}, {"id": "2011.07901", "submitter": "Mla{\\dj}an Jovanovi\\'c Dr", "authors": "Jasna Petrovic, Mladjan Jovanovic", "title": "Conversational agents for learning foreign languages -- a survey", "comments": "Sinteza 2020 - International Scientific Conference on Information\n  Technology and Data Related Research, Belgrade, Singidunum University, Serbia", "journal-ref": null, "doi": "10.15308/Sinteza-2020-14-22", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational practice, while crucial for all language learners, can be\nchallenging to get enough of and very expensive. Chatbots are computer programs\ndeveloped to engage in conversations with humans. They are designed as software\navatars with limited, but growing conversational capability. The most natural\nand potentially powerful application of chatbots is in line with their\nfundamental nature - language practice. However, their role and outcomes within\n(in)formal language learning are currently tangential at best. Existing\nresearch in the area has generally focused on chatbots' comprehensibility and\nthe motivation they inspire in their users. In this paper, we provide an\noverview of the chatbots for learning languages, critically analyze existing\napproaches, and discuss the major challenges for future work.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 12:27:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Petrovic", "Jasna", ""], ["Jovanovic", "Mladjan", ""]]}, {"id": "2011.07916", "submitter": "Jingjing Gong", "authors": "Jingjing Gong, Hang Yan, Yining Zheng, Xipeng Qiu and Xuanjing Huang", "title": "Text Information Aggregation with Centrality Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of natural language processing problems need to encode the text\nsequence as a fix-length vector, which usually involves aggregation process of\ncombining the representations of all the words, such as pooling or\nself-attention. However, these widely used aggregation approaches did not take\nhigher-order relationship among the words into consideration. Hence we propose\na new way of obtaining aggregation weights, called eigen-centrality\nself-attention. More specifically, we build a fully-connected graph for all the\nwords in a sentence, then compute the eigen-centrality as the attention score\nof each word.\n  The explicit modeling of relationships as a graph is able to capture some\nhigher-order dependency among words, which helps us achieve better results in 5\ntext classification tasks and one SNLI task than baseline models such as\npooling, self-attention and dynamic routing. Besides, in order to compute the\ndominant eigenvector of the graph, we adopt power method algorithm to get the\neigen-centrality measure. Moreover, we also derive an iterative approach to get\nthe gradient for the power method process to reduce both memory consumption and\ncomputation requirement.}\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:08:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gong", "Jingjing", ""], ["Yan", "Hang", ""], ["Zheng", "Yining", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2011.07934", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Aleksei Triastcyn, Boi Faltings", "title": "Differential Privacy Meets Maximum-weight Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to large-scale multi-agent systems with a diverse set of\nagents, traditional differential privacy (DP) mechanisms are ill-matched\nbecause they consider a very broad class of adversaries, and they protect all\nusers, independent of their characteristics, by the same guarantee. Achieving a\nmeaningful privacy leads to pronounced reduction in solution quality. Such\nassumptions are unnecessary in many real-world applications for three key\nreasons: (i) users might be willing to disclose less sensitive information\n(e.g., city of residence, but not exact location), (ii) the attacker might\nposses auxiliary information (e.g., city of residence in a mobility-on-demand\nsystem, or reviewer expertise in a paper assignment problem), and (iii) domain\ncharacteristics might exclude a subset of solutions (an expert on auctions\nwould not be assigned to review a robotics paper, thus there is no need for\nindistinguishably between reviewers on different fields).\n  We introduce Piecewise Local Differential Privacy (PLDP), a privacy model\ndesigned to protect the utility function in applications where the attacker\npossesses additional information on the characteristics of the utility space.\nPLDP enables a high degree of privacy, while being applicable to real-world,\nunboundedly large settings. Moreover, we propose PALMA, a privacy-preserving\nheuristic for maximum-weight matching. We evaluate PALMA in a vehicle-passenger\nmatching scenario using real data and demonstrate that it provides strong\nprivacy, $\\varepsilon \\leq 3$ and a median of $\\varepsilon = 0.44$, and high\nquality matchings ($10.8\\%$ worse than the non-private optimal).\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:33:04 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Triastcyn", "Aleksei", ""], ["Faltings", "Boi", ""]]}, {"id": "2011.07952", "submitter": "Roberto Souza", "authors": "Youssef Beauferris, Jonas Teuwen, Dimitrios Karkalousos, Nikita\n  Moriakov, Mattha Caan, L\\'ivia Rodrigues, Alexandre Lopes, H\\'elio Pedrini,\n  Let\\'icia Rittner, Maik Dannecker, Viktor Studenyak, Fabian Gr\\\"oger,\n  Devendra Vyas, Shahrooz Faghih-Roohi, Amrit Kumar Jethi, Jaya Chandra Raju,\n  Mohanasankar Sivaprakasam, Wallace Loos, Richard Frayne, Roberto Souza", "title": "Multi-channel MR Reconstruction (MC-MRRec) Challenge -- Comparing\n  Accelerated MR Reconstruction Models and Assessing Their Genereralizability\n  to Datasets Collected with Different Coils", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2020 Multi-channel Magnetic Resonance Reconstruction (MC-MRRec) Challenge\nhad two primary goals: 1) compare different MR image reconstruction models on a\nlarge dataset and 2) assess the generalizability of these models to datasets\nacquired with a different number of receiver coils (i.e., multiple channels).\nThe challenge had two tracks: Track 01 focused on assessing models trained and\ntested with 12-channel data. Track 02 focused on assessing models trained with\n12-channel data and tested on both 12-channel and 32-channel data. While the\nchallenge is ongoing, here we describe the first edition of the challenge and\nsummarise submissions received prior to 5 September 2020. Track 01 had five\nbaseline models and received four independent submissions. Track 02 had two\nbaseline models and received two independent submissions. This manuscript\nprovides relevant comparative information on the current state-of-the-art of MR\nreconstruction and highlights the challenges of obtaining generalizable models\nthat are required prior to clinical adoption. Both challenge tracks remain open\nand will provide an objective performance assessment for future submissions.\nSubsequent editions of the challenge are proposed to investigate new concepts\nand strategies, such as the integration of potentially available longitudinal\ninformation during the MR reconstruction process. An outline of the proposed\nsecond edition of the challenge is presented in this manuscript.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 04:11:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Beauferris", "Youssef", ""], ["Teuwen", "Jonas", ""], ["Karkalousos", "Dimitrios", ""], ["Moriakov", "Nikita", ""], ["Caan", "Mattha", ""], ["Rodrigues", "L\u00edvia", ""], ["Lopes", "Alexandre", ""], ["Pedrini", "H\u00e9lio", ""], ["Rittner", "Let\u00edcia", ""], ["Dannecker", "Maik", ""], ["Studenyak", "Viktor", ""], ["Gr\u00f6ger", "Fabian", ""], ["Vyas", "Devendra", ""], ["Faghih-Roohi", "Shahrooz", ""], ["Jethi", "Amrit Kumar", ""], ["Raju", "Jaya Chandra", ""], ["Sivaprakasam", "Mohanasankar", ""], ["Loos", "Wallace", ""], ["Frayne", "Richard", ""], ["Souza", "Roberto", ""]]}, {"id": "2011.07956", "submitter": "Dong-Ho Lee", "authors": "Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill\n  Yuchen Lin, Xiang Ren", "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "comments": "15 pages, 4 figures. Code and Data: https://github.com/INK-USC/CALM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (PTLM) have achieved impressive results in a\nrange of natural language understanding (NLU) and generation (NLG) tasks.\nHowever, current pre-training objectives such as masked token prediction (for\nBERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not\nexplicitly model the relational commonsense knowledge about everyday concepts,\nwhich is crucial to many downstream tasks that need common sense to understand\nor generate. To augment PTLMs with concept-centric commonsense knowledge, in\nthis paper, we propose both generative and contrastive objectives for learning\ncommon sense from the text, and use them as intermediate self-supervised\nlearning tasks for incrementally pre-training PTLMs (before task-specific\nfine-tuning on downstream datasets). Furthermore, we develop a joint\npre-training framework to unify generative and contrastive objectives so that\nthey can mutually reinforce each other. Extensive experimental results show\nthat our method, concept-aware language model (CALM), can pack more commonsense\nknowledge into the parameters of a pre-trained text-to-text transformer without\nrelying on external knowledge graphs, yielding better performance on both NLU\nand NLG tasks. We show that while only incrementally pre-trained on a\nrelatively small corpus for a few steps, CALM outperforms baseline methods by a\nconsistent margin and even comparable with some larger PTLMs, which suggests\nthat CALM can serve as a general, plug-and-play method for improving the\ncommonsense reasoning ability of a PTLM.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:00:37 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 04:53:38 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Lee", "Dong-Ho", ""], ["Selvam", "Ravi Kiran", ""], ["Lee", "Seyeon", ""], ["Lin", "Bill Yuchen", ""], ["Ren", "Xiang", ""]]}, {"id": "2011.07967", "submitter": "Michele Focchi", "authors": "Abdelrahman Abdalla1, Michele Focchi, Romeo Orsolino and Claudio\n  Semini", "title": "An Efficient Paradigm for Feasibility Guarantees in Legged Locomotion", "comments": "17 pages, 13 figures, submitted to Transaction on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing feasible body trajectories for legged systems on arbitrary\nterrains is a challenging task. Given some contact points, the trajectories for\nthe Center of Mass (CoM) and body orientation, designed to move the robot, must\nsatisfy crucial constraints to maintain balance, and to avoid violating\nphysical actuation and kinematic limits. In this paper, we present a paradigm\nthat allows to design feasible trajectories in an efficient manner. In\ncontinuation to our previous work, we extend the notion of the 2D feasible\nregion, where static balance and the satisfaction of actuation limits were\nguaranteed, whenever the projection of the CoM lies inside the proposed\nadmissible region. We here develop a general formulation of the improved\nfeasible region to guarantee dynamic balance alongside the satisfaction of both\nactuation and kinematic limits for arbitrary terrains in an efficient manner.\nTo incorporate the feasibility of the kinematic limits, we introduce an\nalgorithm that computes the reachable region of the CoM. Furthermore, we\npropose an efficient planning strategy that utilizes the improved feasible\nregion to design feasible CoM and body orientation trajectories. Finally, we\nvalidate the capabilities of the improved feasible region and the effectiveness\nof the proposed planning strategy, using simulations and experiments on the HyQ\nrobot and comparing them to a previously developed heuristic approach. Various\nscenarios and terrains that mimic confined and challenging environments are\nused for the validation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:58:33 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Abdalla1", "Abdelrahman", ""], ["Focchi", "Michele", ""], ["Orsolino", "Romeo", ""], ["Semini", "Claudio", ""]]}, {"id": "2011.07981", "submitter": "Mohammad Jafarian", "authors": "Mohammad Jafarian, Alireza Soroudi, Andrew Keane", "title": "Resilient Identification of Distribution Network Topology", "comments": null, "journal-ref": null, "doi": "10.1109/TPWRD.2020.3037639", "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network topology identification (TI) is an essential function for distributed\nenergy resources management systems (DERMS) to organize and operate widespread\ndistributed energy resources (DERs). In this paper, discriminant analysis (DA)\nis deployed to develop a network TI function that relies only on the\nmeasurements available to DERMS. The propounded method is able to identify the\nnetwork switching configuration, as well as the status of protective devices.\nFollowing, to improve the TI resiliency against the interruption of\ncommunication channels, a quadratic programming optimization approach is\nproposed to recover the missing signals. By deploying the propounded data\nrecovery approach and Bayes' theorem together, a benchmark is developed\nafterward to identify anomalous measurements. This benchmark can make the TI\nfunction resilient against cyber-attacks. Having a low computational burden,\nthis approach is fast-track and can be applied in real-time applications.\nSensitivity analysis is performed to assess the contribution of different\nmeasurements and the impact of the system load type and loading level on the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:23:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jafarian", "Mohammad", ""], ["Soroudi", "Alireza", ""], ["Keane", "Andrew", ""]]}, {"id": "2011.08004", "submitter": "Gonzague Henri", "authors": "Gonzague Henri, Tanguy Levent, Avishai Halev, Reda Alami, Philippe\n  Cordier", "title": "pymgrid: An Open-Source Python Microgrid Simulator for Applied\n  Artificial Intelligence Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microgrids, self contained electrical grids that are capable of disconnecting\nfrom the main grid, hold potential in both tackling climate change mitigation\nvia reducing CO2 emissions and adaptation by increasing infrastructure\nresiliency. Due to their distributed nature, microgrids are often\nidiosyncratic; as a result, control of these systems is nontrivial. While\nmicrogrid simulators exist, many are limited in scope and in the variety of\nmicrogrids they can simulate. We propose pymgrid, an open-source Python package\nto generate and simulate a large number of microgrids, and the first\nopen-source tool that can generate more than 600 different microgrids. pymgrid\nabstracts most of the domain expertise, allowing users to focus on control\nalgorithms. In particular, pymgrid is built to be a reinforcement learning (RL)\nplatform, and includes the ability to model microgrids as Markov decision\nprocesses. pymgrid also introduces two pre-computed list of microgrids,\nintended to allow for research reproducibility in the microgrid setting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:05:12 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Henri", "Gonzague", ""], ["Levent", "Tanguy", ""], ["Halev", "Avishai", ""], ["Alami", "Reda", ""], ["Cordier", "Philippe", ""]]}, {"id": "2011.08008", "submitter": "Laura von Rueden", "authors": "Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider,\n  Christian Bauckhage", "title": "Towards Map-Based Validation of Semantic Segmentation Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence for autonomous driving must meet strict requirements\non safety and robustness. We propose to validate machine learning models for\nself-driving vehicles not only with given ground truth labels, but also with\nadditional a-priori knowledge. In particular, we suggest to validate the\ndrivable area in semantic segmentation masks using given street map data. We\npresent first results, which indicate that prediction errors can be uncovered\nby map-based validation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:07:22 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 15:28:58 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["von Rueden", "Laura", ""], ["Wirtz", "Tim", ""], ["Hueger", "Fabian", ""], ["Schneider", "Jan David", ""], ["Bauckhage", "Christian", ""]]}, {"id": "2011.08027", "submitter": "Boyao Li", "authors": "Boyao Li, Tao Lu, Jiayi Li, Ning Lu, Yinghao Cai, Shuo Wang", "title": "ACDER: Augmented Curiosity-Driven Experience Replay", "comments": null, "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA2020)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration in environments with sparse feedback remains a challenging\nresearch problem in reinforcement learning (RL). When the RL agent explores the\nenvironment randomly, it results in low exploration efficiency, especially in\nrobotic manipulation tasks with high dimensional continuous state and action\nspace. In this paper, we propose a novel method, called Augmented\nCuriosity-Driven Experience Replay (ACDER), which leverages (i) a new\ngoal-oriented curiosity-driven exploration to encourage the agent to pursue\nnovel and task-relevant states more purposefully and (ii) the dynamic initial\nstates selection as an automatic exploratory curriculum to further improve the\nsample-efficiency. Our approach complements Hindsight Experience Replay (HER)\nby introducing a new way to pursue valuable states. Experiments conducted on\nfour challenging robotic manipulation tasks with binary rewards, including\nReach, Push, Pick&Place and Multi-step Push. The empirical results show that\nour proposed method significantly outperforms existing methods in the first\nthree basic tasks and also achieves satisfactory performance in multi-step\nrobotic task learning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:27:15 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Li", "Boyao", ""], ["Lu", "Tao", ""], ["Li", "Jiayi", ""], ["Lu", "Ning", ""], ["Cai", "Yinghao", ""], ["Wang", "Shuo", ""]]}, {"id": "2011.08028", "submitter": "Giuseppe Pirr\\`o", "authors": "Giuseppe Pirr\\`o", "title": "Fact Checking via Path Embedding and Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs (KGs) are a useful source of background knowledge to\n(dis)prove facts of the form (s, p, o). Finding paths between s and o is the\ncornerstone of several fact-checking approaches. While paths are useful to\n(visually) explain why a given fact is true or false, it is not completely\nclear how to identify paths that are most relevant to a fact, encode them and\nweigh their importance. The goal of this paper is to present the Fact Checking\nvia path Embedding and Aggregation (FEA) system. FEA starts by carefully\ncollecting the paths between s and o that are most semantically related to the\ndomain of p. However, instead of directly working with this subset of all\npaths, it learns vectorized path representations, aggregates them according to\ndifferent strategies, and use them to finally (dis)prove a fact. We conducted a\nlarge set of experiments on a variety of KGs and found that our hybrid solution\nbrings some benefits in terms of performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:27:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Pirr\u00f2", "Giuseppe", ""]]}, {"id": "2011.08076", "submitter": "Sebastian Niehaus", "authors": "Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian\n  Wagner, Ingo Roeder, Nico Scherf", "title": "A comparative study of semi- and self-supervised semantic segmentation\n  of biomedical microscopy data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have become the\nstate-of-the-art method for biomedical image analysis. However, these networks\nare usually trained in a supervised manner, requiring large amounts of labelled\ntraining data. These labelled data sets are often difficult to acquire in the\nbiomedical domain. In this work, we validate alternative ways to train CNNs\nwith fewer labels for biomedical image segmentation using. We adapt two semi-\nand self-supervised image classification methods and analyse their performance\nfor semantic segmentation of biomedical microscopy images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:57:10 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 13:03:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Horlava", "Nastassya", ""], ["Mironenko", "Alisa", ""], ["Niehaus", "Sebastian", ""], ["Wagner", "Sebastian", ""], ["Roeder", "Ingo", ""], ["Scherf", "Nico", ""]]}, {"id": "2011.08090", "submitter": "Arash Shaban-Nejad", "authors": "Nariman Ammar, Arash Shaban-Nejad", "title": "Explainable Artificial Intelligence Recommendation System by Leveraging\n  the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype\n  Development", "comments": "15 Pages, 7 Figures", "journal-ref": "JMIR Med Inform. 2020 Nov 4;8(11):e18752", "doi": "10.2196/18752", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of adverse childhood experiences and their consequences has emerged\nover the past 20 years. In this study, we aimed to leverage explainable\nartificial intelligence, and propose a proof-of-concept prototype for a\nknowledge-driven evidence-based recommendation system to improve surveillance\nof adverse childhood experiences. We used concepts from an ontology that we\nhave developed to build and train a question-answering agent using the Google\nDialogFlow engine. In addition to the question-answering agent, the initial\nprototype includes knowledge graph generation and recommendation components\nthat leverage third-party graph technology. To showcase the framework\nfunctionalities, we here present a prototype design and demonstrate the main\nfeatures through four use case scenarios motivated by an initiative currently\nimplemented at a children hospital in Memphis, Tennessee. Ongoing development\nof the prototype requires implementing an optimization algorithm of the\nrecommendations, incorporating a privacy layer through a personal health\nlibrary, and conducting a clinical trial to assess both usability and\nusefulness of the implementation. This semantic-driven explainable artificial\nintelligence prototype can enhance health care practitioners ability to provide\nexplanations for the decisions they make.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:45:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ammar", "Nariman", ""], ["Shaban-Nejad", "Arash", ""]]}, {"id": "2011.08091", "submitter": "Fabrizio Sebastiani", "authors": "Alejandro Moreo and Fabrizio Sebastiani", "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:41:34 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 10:49:24 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "2011.08129", "submitter": "Mou-Cheng Xu", "authors": "Mou-Cheng Xu", "title": "Tissue characterization based on the analysis on i3DUS data for\n  diagnosis support in neurosurgery", "comments": "MRes thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain shift makes the pre-operative MRI navigation highly inaccurate hence\nthe intraoperative modalities are adopted in surgical theatre. Due to the\nexcellent economic and portability merits, the Ultrasound imaging is used at\nour collaborating hospital, Charing Cross Hospital, Imperial College London,\nUK. However, it is found that intraoperative diagnosis on Ultrasound images is\nnot straightforward and consistent, even for very experienced clinical experts.\nHence, there is a demand to design a Computer-aided-diagnosis system to provide\na robust second opinion to help the surgeons. The proposed CAD system based on\n\"Mixed-Attention Res-U-net with asymmetric loss function\" achieves the\nstate-of-the-art results comparing to the ground truth by classification at\npixel-level directly, it also outperforms all the current main stream\npixel-level classification methods (e.g. U-net, FCN) in all the evaluation\nmetrices.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:44:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xu", "Mou-Cheng", ""]]}, {"id": "2011.08182", "submitter": "Uwe Aickelin", "authors": "Bahram Farhadinia, Uwe Aickelin, Hadi Akbarzadeh Khorshidi", "title": "Uncertainty measures for probabilistic hesitant fuzzy sets in multiple\n  criteria decision making", "comments": "International Journal of Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This contribution reviews critically the existing entropy measures for\nprobabilistic hesitant fuzzy sets (PHFSs), and demonstrates that these entropy\nmeasures fail to effectively distinguish a variety of different PHFSs in some\ncases. In the sequel, we develop a new axiomatic framework of entropy measures\nfor probabilistic hesitant fuzzy elements (PHFEs) by considering two facets of\nuncertainty associated with PHFEs which are known as fuzziness and\nnonspecificity. Respect to each kind of uncertainty, a number of formulae are\nderived to permit flexible selection of PHFE entropy measures. Moreover, based\non the proposed PHFE entropy measures, we introduce some entropy-based distance\nmeasures which are used in the portion of comparative analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:25:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Farhadinia", "Bahram", ""], ["Aickelin", "Uwe", ""], ["Khorshidi", "Hadi Akbarzadeh", ""]]}, {"id": "2011.08183", "submitter": "Uwe Aickelin", "authors": "B Farhadinia, Uwe Aickelin, HA Khorshidi", "title": "Higher order hesitant fuzzy Choquet integral operator and its\n  application to multiple criteria decision making", "comments": "Iranian Journal of Fuzzy Systems, Volume, 2002, Issue 5687", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generally, the criteria involved in a decision making problem are interactive\nor inter-dependent, and therefore aggregating them by the use of traditional\noperators which are based on additive measures is not logical. This verifies\nthat we have to implement fuzzy measures for modelling the interaction\nphenomena among the criteria.On the other hand, based on the recent extension\nof hesitant fuzzy set, called higher order hesitant fuzzy set (HOHFS) which\nallows the membership of a given element to be defined in forms of several\npossible generalized types of fuzzy set, we encourage to propose the higher\norder hesitant fuzzy (HOHF) Choquet integral operator. This concept not only\nconsiders the importance of the higher order hesitant fuzzy arguments, but also\nit can reflect the correlations among those arguments. Then,a detailed\ndiscussion on the aggregation properties of the HOHF Choquet integral operator\nwill be presented.To enhance the application of HOHF Choquet integral operator\nin decision making, we first assess the appropriate energy policy for the\nsocio-economic development. Then, the efficiency of the proposed HOHF Choquet\nintegral operator-based technique over a number of exiting techniques is\nfurther verified by employing another decision making problem associated with\nthe technique of TODIM (an acronym in Portuguese of Interactive and\nMulticriteria Decision Making).\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:52:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Farhadinia", "B", ""], ["Aickelin", "Uwe", ""], ["Khorshidi", "HA", ""]]}, {"id": "2011.08191", "submitter": "Johann Brehmer Mr", "authors": "Johann Brehmer, Sebastian Macaluso, Duccio Pappadopulo, Kyle Cranmer", "title": "Hierarchical clustering in particle physics through reinforcement\n  learning", "comments": "Accepted at the Machine Learning and the Physical Sciences workshop\n  at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle physics experiments often require the reconstruction of decay\npatterns through a hierarchical clustering of the observed final-state\nparticles. We show that this task can be phrased as a Markov Decision Process\nand adapt reinforcement learning algorithms to solve it. In particular, we show\nthat Monte-Carlo Tree Search guided by a neural policy can construct\nhigh-quality hierarchical clusterings and outperform established greedy and\nbeam search baselines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 19:00:01 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 09:39:44 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Brehmer", "Johann", ""], ["Macaluso", "Sebastian", ""], ["Pappadopulo", "Duccio", ""], ["Cranmer", "Kyle", ""]]}, {"id": "2011.08243", "submitter": "Chien-Wei Lin", "authors": "Chien-Wei Lin, Vincent Auvray, Daniel Elkind, Arijit Biswas, Maryam\n  Fazel-Zarandi, Nehal Belgamwar, Shubhra Chandra, Matt Zhao, Angeliki\n  Metallinou, Tagyoung Chung, Charlie Shucheng Zhu, Suranjit Adhikari, Dilek\n  Hakkani-Tur", "title": "Dialog Simulation with Realistic Variations for Training Goal-Oriented\n  Conversational Systems", "comments": "To be presented at Human in the Loop Dialogue Systems Workshop,\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-oriented dialog systems enable users to complete specific goals like\nrequesting information about a movie or booking a ticket. Typically the dialog\nsystem pipeline contains multiple ML models, including natural language\nunderstanding, state tracking and action prediction (policy learning). These\nmodels are trained through a combination of supervised or reinforcement\nlearning methods and therefore require collection of labeled domain specific\ndatasets. However, collecting annotated datasets with language and dialog-flow\nvariations is expensive, time-consuming and scales poorly due to human\ninvolvement. In this paper, we propose an approach for automatically creating a\nlarge corpus of annotated dialogs from a few thoroughly annotated sample\ndialogs and the dialog schema. Our approach includes a novel goal-sampling\ntechnique for sampling plausible user goals and a dialog simulation technique\nthat uses heuristic interplay between the user and the system (Alexa), where\nthe user tries to achieve the sampled goal. We validate our approach by\ngenerating data and training three different downstream conversational ML\nmodels. We achieve 18 ? 50% relative accuracy improvements on a held-out test\nset compared to a baseline dialog generation approach that only samples natural\nlanguage and entity value variations from existing catalogs but does not\ngenerate any novel dialog flow variations. We also qualitatively establish that\nthe proposed approach is better than the baseline. Moreover, several different\nconversational experiences have been built using this method, which enables\ncustomers to have a wide variety of conversations with Alexa.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 19:39:15 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lin", "Chien-Wei", ""], ["Auvray", "Vincent", ""], ["Elkind", "Daniel", ""], ["Biswas", "Arijit", ""], ["Fazel-Zarandi", "Maryam", ""], ["Belgamwar", "Nehal", ""], ["Chandra", "Shubhra", ""], ["Zhao", "Matt", ""], ["Metallinou", "Angeliki", ""], ["Chung", "Tagyoung", ""], ["Zhu", "Charlie Shucheng", ""], ["Adhikari", "Suranjit", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2011.08272", "submitter": "Rajkumar Ramamurthy", "authors": "Rajkumar Ramamurthy, Rafet Sifa and Christian Bauckhage", "title": "NLPGym -- A toolkit for evaluating RL agents on Natural Language\n  Processing Tasks", "comments": "Accepted at Wordplay: When Language Meets Games Workshop @ NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has recently shown impressive performance in\ncomplex game AI and robotics tasks. To a large extent, this is thanks to the\navailability of simulated environments such as OpenAI Gym, Atari Learning\nEnvironment, or Malmo which allow agents to learn complex tasks through\ninteraction with virtual environments. While RL is also increasingly applied to\nnatural language processing (NLP), there are no simulated textual environments\navailable for researchers to apply and consistently benchmark RL on NLP tasks.\nWith the work reported here, we therefore release NLPGym, an open-source Python\ntoolkit that provides interactive textual environments for standard NLP tasks\nsuch as sequence tagging, multi-label classification, and question answering.\nWe also present experimental results for 6 tasks using different RL algorithms\nwhich serve as baselines for further research. The toolkit is published at\nhttps://github.com/rajcscw/nlp-gym\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:58:35 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ramamurthy", "Rajkumar", ""], ["Sifa", "Rafet", ""], ["Bauckhage", "Christian", ""]]}, {"id": "2011.08315", "submitter": "Omid Hajihassani", "authors": "Omid Hajihassani, Omid Ardakanian, Hamzeh Khazaei", "title": "Privacy-preserving Data Analysis through Representation Learning and\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of data from the sensors embedded in mobile and Internet of\nThings (IoT) devices and the remarkable success of deep neural networks in\nuncovering hidden patterns in time series data have led to mounting privacy\nconcerns in recent years. In this paper, we aim to navigate the trade-off\nbetween data utility and privacy by learning low-dimensional representations\nthat are useful for data anonymization. We propose probabilistic\ntransformations in the latent space of a variational autoencoder to synthesize\ntime series data such that intrusive inferences are prevented while desired\ninferences can still be made with a satisfactory level of accuracy. We compare\nour technique with state-of-the-art autoencoder-based anonymization techniques\nand additionally show that it can anonymize data in real time on\nresource-constrained edge devices.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:32:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hajihassani", "Omid", ""], ["Ardakanian", "Omid", ""], ["Khazaei", "Hamzeh", ""]]}, {"id": "2011.08317", "submitter": "Ehsan Emad Marvasti", "authors": "Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser\n  P.Fallah, Rui Guo, Hongsheng Lu", "title": "Feature Sharing and Integration for Cooperative Cognition and Perception\n  with Volumetric Sensors", "comments": "12 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent advancement in computational and communication systems has led to\nthe introduction of high-performing neural networks and high-speed wireless\nvehicular communication networks. As a result, new technologies such as\ncooperative perception and cognition have emerged, addressing the inherent\nlimitations of sensory devices by providing solutions for the detection of\npartially occluded targets and expanding the sensing range. However, designing\na reliable cooperative cognition or perception system requires addressing the\nchallenges caused by limited network resources and discrepancies between the\ndata shared by different sources. In this paper, we examine the requirements,\nlimitations, and performance of different cooperative perception techniques,\nand present an in-depth analysis of the notion of Deep Feature Sharing (DFS).\nWe explore different cooperative object detection designs and evaluate their\nperformance in terms of average precision. We use the Volony dataset for our\nexperimental study. The results confirm that the DFS methods are significantly\nless sensitive to the localization error caused by GPS noise. Furthermore, the\nresults attest that detection gain of DFS methods caused by adding more\ncooperative participants in the scenes is comparable to raw information sharing\ntechnique while DFS enables flexibility in design toward satisfying\ncommunication requirements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:43:44 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 21:18:33 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 17:58:14 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Marvasti", "Ehsan Emad", ""], ["Raftari", "Arash", ""], ["Marvasti", "Amir Emad", ""], ["Fallah", "Yaser P.", ""], ["Guo", "Rui", ""], ["Lu", "Hongsheng", ""]]}, {"id": "2011.08334", "submitter": "Michael Wessel", "authors": "Michael Wessel, Edgar Kalns, Girish Acharya, Andreas Kathol", "title": "Widening the Dialogue Workflow Modeling Bottleneck in Ontology-Based\n  Personal Assistants", "comments": "7 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to dialogue specification for Virtual Personal\nAssistants (VPAs) based on so-called dialogue workflow graphs, with several\ndemonstrated advantages over current ontology-based methods. Our new dialogue\nspecification language (DSL) enables customers to more easily participate in\nthe VPA modeling process due to a user-friendly modeling framework. Resulting\nmodels are also significantly more compact. VPAs can be developed much more\nrapidly. The DSL is a new modeling layer on top of our ontology-based Dialogue\nManagement (DM) framework OntoVPA. We explain the rationale and benefits behind\nthe new language and support our claims with concrete reduced Level-of-Effort\n(LOE) numbers from two recent OntoVPA projects.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:32:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wessel", "Michael", ""], ["Kalns", "Edgar", ""], ["Acharya", "Girish", ""], ["Kathol", "Andreas", ""]]}, {"id": "2011.08341", "submitter": "Li Chen", "authors": "Li Chen, David Yang, Purvi Goel, Ilknur Kabul", "title": "Robust Deep Learning with Active Noise Cancellation for Spatial\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes CANC, a Co-teaching Active Noise Cancellation method,\napplied in spatial computing to address deep learning trained with extreme\nnoisy labels. Deep learning algorithms have been successful in spatial\ncomputing for land or building footprint recognition. However a lot of noise\nexists in ground truth labels due to how labels are collected in spatial\ncomputing and satellite imagery. Existing methods to deal with extreme label\nnoise conduct clean sample selection and do not utilize the remaining samples.\nSuch techniques can be wasteful due to the cost of data retrieval. Our proposed\nCANC algorithm not only conserves high-cost training samples but also provides\nactive label correction to better improve robust deep learning with extreme\nnoisy labels. We demonstrate the effectiveness of CANC for building footprint\nrecognition for spatial computing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:56:14 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chen", "Li", ""], ["Yang", "David", ""], ["Goel", "Purvi", ""], ["Kabul", "Ilknur", ""]]}, {"id": "2011.08356", "submitter": "Henrique Aguiar", "authors": "Henrique Aguiar, Mauro Santos, Peter Watkinson, Tingting Zhu", "title": "Phenotyping Clusters of Patient Trajectories suffering from Chronic\n  Complex Disease", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract. 6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an increased focus into the tasks of predicting\nhospital inpatient risk of deterioration and trajectory evolution due to the\navailability of electronic patient data. A common approach to these problems\ninvolves clustering patients time-series information such as vital sign\nobservations) to determine dissimilar subgroups of the patient population. Most\nclustering methods assume time-invariance of vital-signs and are unable to\nprovide interpretability in clusters that is clinically relevant, for instance,\nevent or outcome information. In this work, we evaluate three different\nclustering models on a large hospital dataset of vital-sign observations from\npatients suffering from Chronic Obstructive Pulmonary Disease. We further\npropose novel modifications to deal with unevenly sampled time-series data and\nunbalanced class distribution to improve phenotype separation. Lastly, we\ndiscuss further avenues of investigation for models to learn patient subgroups\nwith distinct behaviour and phenotype.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 01:18:33 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Aguiar", "Henrique", ""], ["Santos", "Mauro", ""], ["Watkinson", "Peter", ""], ["Zhu", "Tingting", ""]]}, {"id": "2011.08406", "submitter": "DongNyeong Heo", "authors": "DongNyeong Heo, Doyoung Lee, Hee-Gon Kim, Suhyun Park, Heeyoul Choi", "title": "Reinforcement Learning of Graph Neural Networks for Service Function\n  Chaining", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the management of computer network systems, the service function chaining\n(SFC) modules play an important role by generating efficient paths for network\ntraffic through physical servers with virtualized network functions (VNF). To\nprovide the highest quality of services, the SFC module should generate a valid\npath quickly even in various network topology situations including dynamic VNF\nresources, various requests, and changes of topologies. The previous supervised\nlearning method demonstrated that the network features can be represented by\ngraph neural networks (GNNs) for the SFC task. However, the performance was\nlimited to only the fixed topology with labeled data. In this paper, we apply\nreinforcement learning methods for training models on various network\ntopologies with unlabeled data. In the experiments, compared to the previous\nsupervised learning method, the proposed methods demonstrated remarkable\nflexibility in new topologies without re-designing and re-training, while\npreserving a similar level of performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:50:53 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Heo", "DongNyeong", ""], ["Lee", "Doyoung", ""], ["Kim", "Hee-Gon", ""], ["Park", "Suhyun", ""], ["Choi", "Heeyoul", ""]]}, {"id": "2011.08430", "submitter": "Yueyue Dai", "authors": "Yueyue Dai (Member, IEEE), Ke Zhang, Sabita Maharjan (Senior Member,\n  IEEE), and Yan Zhang (Fellow, IEEE)", "title": "Deep Reinforcement Learning for Stochastic Computation Offloading in\n  Digital Twin Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of Industrial Internet of Things (IIoT) requires\nindustrial production towards digitalization to improve network efficiency.\nDigital Twin is a promising technology to empower the digital transformation of\nIIoT by creating virtual models of physical objects. However, the provision of\nnetwork efficiency in IIoT is very challenging due to resource-constrained\ndevices, stochastic tasks, and resources heterogeneity. Distributed resources\nin IIoT networks can be efficiently exploited through computation offloading to\nreduce energy consumption while enhancing data processing efficiency. In this\npaper, we first propose a new paradigm Digital Twin Networks (DTN) to build\nnetwork topology and the stochastic task arrival model in IIoT systems. Then,\nwe formulate the stochastic computation offloading and resource allocation\nproblem to minimize the long-term energy efficiency. As the formulated problem\nis a stochastic programming problem, we leverage Lyapunov optimization\ntechnique to transform the original problem into a deterministic per-time slot\nproblem. Finally, we present Asynchronous Actor-Critic (AAC) algorithm to find\nthe optimal stochastic computation offloading policy. Illustrative results\ndemonstrate that our proposed scheme is able to significantly outperforms the\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 05:40:16 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 02:42:44 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Dai", "Yueyue", "", "Member, IEEE"], ["Zhang", "Ke", "", "Senior Member,\n  IEEE"], ["Maharjan", "Sabita", "", "Senior Member,\n  IEEE"], ["Zhang", "Yan", "", "Fellow, IEEE"]]}, {"id": "2011.08434", "submitter": "Georgios Kotsalis", "authors": "Georgios Kotsalis and Guanghui Lan and Tianjiao Li", "title": "Simple and optimal methods for stochastic variational inequalities, II:\n  Markovian noise and policy evaluation in reinforcement learning", "comments": "arXiv admin note: text overlap with arXiv:2011.02987", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is on stochastic variational inequalities (VI) under\nMarkovian noise. A prominent application of our algorithmic developments is the\nstochastic policy evaluation problem in reinforcement learning. Prior\ninvestigations in the literature focused on temporal difference (TD) learning\nby employing nonsmooth finite time analysis motivated by stochastic subgradient\ndescent leading to certain limitations. These encompass the requirement of\nanalyzing a modified TD algorithm that involves projection to an a-priori\ndefined Euclidean ball, achieving a non-optimal convergence rate and no clear\nway of deriving the beneficial effects of parallel implementation. Our approach\nremedies these shortcomings in the broader context of stochastic VIs and in\nparticular when it comes to stochastic policy evaluation. We developed a\nvariety of simple TD learning type algorithms motivated by its original version\nthat maintain its simplicity, while offering distinct advantages from a\nnon-asymptotic analysis point of view. We first provide an improved analysis of\nthe standard TD algorithm that can benefit from parallel implementation. Then\nwe present versions of a conditional TD algorithm (CTD), that involves periodic\nupdates of the stochastic iterates, which reduce the bias and therefore exhibit\nimproved iteration complexity. This brings us to the fast TD (FTD) algorithm\nwhich combines elements of CTD and the stochastic operator extrapolation method\nof the companion paper. For a novel index resetting policy FTD exhibits the\nbest known convergence rate. We also devised a robust version of the algorithm\nthat is particularly suitable for discounting factors close to 1.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 04:05:22 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 21:46:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 02:57:38 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kotsalis", "Georgios", ""], ["Lan", "Guanghui", ""], ["Li", "Tianjiao", ""]]}, {"id": "2011.08435", "submitter": "Guo-Jun Qi", "authors": "Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi", "title": "AdCo: Adversarial Contrast for Efficient Learning of Unsupervised\n  Representations from Self-Trained Negative Adversaries", "comments": "Appendices with more results on symmetric loss, different numbers of\n  negative samples, computing costs is presented. We also discuss \"whether we\n  still need negative examples\" in Appendix C, a question emerging from the\n  comparison with the BYOL. The source code is also available at\n  https://github.com/maple-research-lab/AdCo/", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), June 19th - June 25th, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning relies on constructing a collection of negative examples\nthat are sufficiently hard to discriminate against positive queries when their\nrepresentations are self-trained. Existing contrastive learning methods either\nmaintain a queue of negative samples over minibatches while only a small\nportion of them are updated in an iteration, or only use the other examples\nfrom the current minibatch as negatives. They could not closely track the\nchange of the learned representation over iterations by updating the entire\nqueue as a whole, or discard the useful information from the past minibatches.\nAlternatively, we present to directly learn a set of negative adversaries\nplaying against the self-trained representation. Two players, the\nrepresentation network and negative adversaries, are alternately updated to\nobtain the most challenging negative examples against which the representation\nof positive queries will be trained to discriminate. We further show that the\nnegative adversaries are updated towards a weighted combination of positive\nqueries by maximizing the adversarial contrastive loss, thereby allowing them\nto closely track the change of representations over time. Experiment results\ndemonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves\nsuperior performances (a top-1 accuracy of 73.2\\% over 200 epochs and 75.7\\%\nover 800 epochs with linear evaluation on ImageNet), but also can be\npre-trained more efficiently with fewer epochs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 05:45:46 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 23:56:08 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 03:01:28 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 08:44:44 GMT"}, {"version": "v5", "created": "Fri, 5 Mar 2021 07:01:19 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hu", "Qianjiang", ""], ["Wang", "Xiao", ""], ["Hu", "Wei", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2011.08436", "submitter": "Chiho Choi", "authors": "Chiho Choi, Joon Hee Choi, Jiachen Li, Srikanth Malla", "title": "Shared Cross-Modal Trajectory Prediction for Autonomous Driving", "comments": "CVPR 2021 [Oral]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predicting future trajectories of traffic agents in highly interactive\nenvironments is an essential and challenging problem for the safe operation of\nautonomous driving systems. On the basis of the fact that self-driving vehicles\nare equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,\nradar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit\nfrom the use of multiple input modalities. At training time, our model learns\nto embed a set of complementary features in a shared latent space by jointly\noptimizing the objective functions across different types of input data. At\ntest time, a single input modality (e.g., LiDAR data) is required to generate\npredictions from the input perspective (i.e., in the LiDAR space), while taking\nadvantages from the model trained with multiple sensor modalities. An extensive\nevaluation is conducted to show the efficacy of the proposed framework using\ntwo benchmark driving datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 07:18:50 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 09:05:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Choi", "Chiho", ""], ["Choi", "Joon Hee", ""], ["Li", "Jiachen", ""], ["Malla", "Srikanth", ""]]}, {"id": "2011.08446", "submitter": "William McNally", "authors": "William McNally, Kanav Vats, Alexander Wong, John McPhee", "title": "EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using\n  Neuroevolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search has proven to be highly effective in the design of\ncomputationally efficient, task-specific convolutional neural networks across\nseveral areas of computer vision. In 2D human pose estimation, however, its\napplication has been limited by high computational demands. Hypothesizing that\nneural architecture search holds great potential for 2D human pose estimation,\nwe propose a new weight transfer scheme that relaxes function-preserving\nmutations, enabling us to accelerate neuroevolution in a flexible manner. Our\nmethod produces 2D human pose network designs that are more efficient and more\naccurate than state-of-the-art hand-designed networks. In fact, the generated\nnetworks can process images at higher resolutions using less computation than\nprevious networks at lower resolutions, permitting us to push the boundaries of\n2D human pose estimation. Our baseline network designed using neuroevolution,\nwhich we refer to as EvoPose2D-S, provides comparable accuracy to\nSimpleBaseline while using 4.9x fewer floating-point operations and 13.5x fewer\nparameters. Our largest network, EvoPose2D-L, achieves new state-of-the-art\naccuracy on the Microsoft COCO Keypoints benchmark while using 2.0x fewer\noperations and 4.3x fewer parameters than its nearest competitor.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 05:56:16 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["McNally", "William", ""], ["Vats", "Kanav", ""], ["Wong", "Alexander", ""], ["McPhee", "John", ""]]}, {"id": "2011.08463", "submitter": "R\\'emy Portelas", "authors": "R\\'emy Portelas, Cl\\'ement Romac, Katja Hofmann, Pierre-Yves Oudeyer", "title": "Meta Automatic Curriculum Learning", "comments": "This paper extends and generalizes work in arXiv:2004.03168", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in the Deep RL (DRL) community is to train agents able to\ngeneralize their control policy over situations never seen in training.\nTraining on diverse tasks has been identified as a key ingredient for good\ngeneralization, which pushed researchers towards using rich procedural task\ngeneration systems controlled through complex continuous parameter spaces. In\nsuch complex task spaces, it is essential to rely on some form of Automatic\nCurriculum Learning (ACL) to adapt the task sampling distribution to a given\nlearning agent, instead of randomly sampling tasks, as many could end up being\neither trivial or unfeasible. Since it is hard to get prior knowledge on such\ntask spaces, many ACL algorithms explore the task space to detect progress\nniches over time, a costly tabula-rasa process that needs to be performed for\neach new learning agents, although they might have similarities in their\ncapabilities profiles. To address this limitation, we introduce the concept of\nMeta-ACL, and formalize it in the context of black-box RL learners, i.e.\nalgorithms seeking to generalize curriculum generation to an (unknown)\ndistribution of learners. In this work, we present AGAIN, a first instantiation\nof Meta-ACL, and showcase its benefits for curriculum generation over classical\nACL in multiple simulated environments including procedurally generated parkour\nenvironments with learners of varying morphologies. Videos and code are\navailable at https://sites.google.com/view/meta-acl .\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:56:42 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:19:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Portelas", "R\u00e9my", ""], ["Romac", "Cl\u00e9ment", ""], ["Hofmann", "Katja", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2011.08484", "submitter": "Joseph Lubars", "authors": "Joseph Lubars, Harsh Gupta, Adnan Raja, R. Srikant, Liyun Li, and\n  Xinzhou Wu", "title": "Combining Reinforcement Learning with Model Predictive Control for\n  On-Ramp Merging", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing an algorithm to allow a car to\nautonomously merge on to a highway from an on-ramp. Two broad classes of\ntechniques have been proposed to solve motion planning problems in autonomous\ndriving: Model Predictive Control (MPC) and Reinforcement Learning (RL). In\nthis paper, we first establish the strengths and weaknesses of state-of-the-art\nMPC and RL-based techniques through simulations. We show that the performance\nof the RL agent is worse than that of the MPC solution from the perspective of\nsafety and robustness to out-of-distribution traffic patterns, i.e., traffic\npatterns which were not seen by the RL agent during training. On the other\nhand, the performance of the RL agent is better than that of the MPC solution\nwhen it comes to efficiency and passenger comfort. We subsequently present an\nalgorithm which blends the model-free RL agent with the MPC solution and show\nthat it provides better trade-offs between all metrics -- passenger comfort,\nefficiency, crash rate and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 07:42:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lubars", "Joseph", ""], ["Gupta", "Harsh", ""], ["Raja", "Adnan", ""], ["Srikant", "R.", ""], ["Li", "Liyun", ""], ["Wu", "Xinzhou", ""]]}, {"id": "2011.08492", "submitter": "Utku Ketenci", "authors": "Utku G\\\"orkem Ketenci and Tolga Kurt and Selim \\\"Onal and Cenk Erbil\n  and Sinan Akt\\\"urko\\u{g}lu and Hande \\c{S}erban \\.Ilhan", "title": "A Time-Frequency based Suspicious Activity Detection for Anti-Money\n  Laundering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Money laundering is the crucial mechanism utilized by criminals to inject\nproceeds of crime to the financial system. The primary responsibility of the\ndetection of suspicious activity related to money laundering is with the\nfinancial institutions. Most of the current systems in these institutions are\nrule-based and ineffective. The available data science-based anti-money\nlaundering (AML) models in order to replace the existing rule-based systems\nwork on customer relationship management (CRM) features and time\ncharacteristics of transaction behaviour. However, there is still a challenge\non accuracy and problems around feature engineering due to thousands of\npossible features.\n  Aiming to improve the detection performance of suspicious transaction\nmonitoring systems for AML systems, in this article, we introduce a novel\nfeature set based on time-frequency analysis, that makes use of 2-D\nrepresentations of financial transactions. Random forest is utilized as a\nmachine learning method, and simulated annealing is adopted for hyperparameter\ntuning. The designed algorithm is tested on real banking data, proving the\nefficacy of the results in practically relevant environments. It is shown that\nthe time-frequency characteristics of suspicious and non-suspicious entities\ndifferentiate significantly, which would substantially improve the precision of\ndata science-based transaction monitoring systems looking at only time-series\ntransaction and CRM features.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 08:01:50 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ketenci", "Utku G\u00f6rkem", ""], ["Kurt", "Tolga", ""], ["\u00d6nal", "Selim", ""], ["Erbil", "Cenk", ""], ["Akt\u00fcrko\u011flu", "Sinan", ""], ["\u0130lhan", "Hande \u015eerban", ""]]}, {"id": "2011.08518", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Michael Milford", "title": "DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and\n  Sequence-based Place Recognition", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": "NeurIPS 2020 Workshop on Machine Learning for Autonomous Driving\n  (ML4AD)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-based place recognition methods for all-weather navigation are\nwell-known for producing state-of-the-art results under challenging day-night\nor summer-winter transitions. These systems, however, rely on complex\nhandcrafted heuristics for sequential matching - which are applied on top of a\npre-computed pairwise similarity matrix between reference and query image\nsequences of a single route - to further reduce false-positive rates compared\nto single-frame retrieval methods. As a result, performing multi-frame place\nrecognition can be extremely slow for deployment on autonomous vehicles or\nevaluation on large datasets, and fail when using relatively short parameter\nvalues such as a sequence length of 2 frames. In this paper, we propose\nDeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and\npositional representations from a single monocular image sequence of a route.\nWe demonstrate our approach on two large benchmark datasets, Nordland and\nOxford RobotCar - recorded over 728 km and 10 km routes, respectively, each\nduring 1 year with multiple seasons, weather, and lighting conditions. On\nNordland, we compare our method to two state-of-the-art sequence-based methods\nacross the entire route under summer-winter changes using a sequence length of\n2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta\nDescriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment\ntime from around 1 hour to 1 minute against both. The framework code and video\nare available at https://mchancan.github.io/deepseqslam\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:14:02 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "2011.08523", "submitter": "Haoxiang Shi", "authors": "Haoxiang Shi and Cen Wang", "title": "Self-supervised Document Clustering Based on BERT with Data Augment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive learning is a good way to pursue discriminative unsupervised\nlearning, which can inherit advantages and experiences of well-studied deep\nmodels without complexly novel model designing. In this paper, we propose two\nlearning method for document clustering, the one is a partial contrastive\nlearning with unsupervised data augment, and the other is a self-supervised\ncontrastive learning. Both methods achieve state-of-the-art results in\nclustering accuracy when compared to recently proposed unsupervised clustering\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:18:47 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 18:46:28 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shi", "Haoxiang", ""], ["Wang", "Cen", ""]]}, {"id": "2011.08541", "submitter": "Sreejith Balakrishnan", "authors": "Sreejith Balakrishnan, Quoc Phong Nguyen, Bryan Kian Hsiang Low,\n  Harold Soh", "title": "Efficient Exploration of Reward Functions in Inverse Reinforcement\n  Learning via Bayesian Optimization", "comments": "Accepted to 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020). Includes Appendix. 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inverse reinforcement learning (IRL) is relevant to a variety\nof tasks including value alignment and robot learning from demonstration.\nDespite significant algorithmic contributions in recent years, IRL remains an\nill-posed problem at its core; multiple reward functions coincide with the\nobserved behavior and the actual reward function is not identifiable without\nprior knowledge or supplementary information. This paper presents an IRL\nframework called Bayesian optimization-IRL (BO-IRL) which identifies multiple\nsolutions that are consistent with the expert demonstrations by efficiently\nexploring the reward function space. BO-IRL achieves this by utilizing Bayesian\nOptimization along with our newly proposed kernel that (a) projects the\nparameters of policy invariant reward functions to a single point in a latent\nspace and (b) ensures nearby points in the latent space correspond to reward\nfunctions yielding similar likelihoods. This projection allows the use of\nstandard stationary kernels in the latent space to capture the correlations\npresent across the reward function space. Empirical results on synthetic and\nreal-world environments (model-free and model-based) show that BO-IRL discovers\nmultiple reward functions while minimizing the number of expensive exact policy\noptimizations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:17:45 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Balakrishnan", "Sreejith", ""], ["Nguyen", "Quoc Phong", ""], ["Low", "Bryan Kian Hsiang", ""], ["Soh", "Harold", ""]]}, {"id": "2011.08596", "submitter": "Jonathan Crabb\\'e", "authors": "Jonathan Crabb\\'e, Yao Zhang, William Zame, Mihaela van der Schaar", "title": "Learning outside the Black-Box: The pursuit of interpretable models", "comments": "presented in 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020)", "journal-ref": "Advances in Neural Information Processing Systems 33 (2020)\n  17838-17849", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning has proved its ability to produce accurate models but the\ndeployment of these models outside the machine learning community has been\nhindered by the difficulties of interpreting these models. This paper proposes\nan algorithm that produces a continuous global interpretation of any given\ncontinuous black-box function. Our algorithm employs a variation of projection\npursuit in which the ridge functions are chosen to be Meijer G-functions,\nrather than the usual polynomial splines. Because Meijer G-functions are\ndifferentiable in their parameters, we can tune the parameters of the\nrepresentation by gradient descent; as a consequence, our algorithm is\nefficient. Using five familiar data sets from the UCI repository and two\nfamiliar machine learning algorithms, we demonstrate that our algorithm\nproduces global interpretations that are both highly accurate and parsimonious\n(involve a small number of terms). Our interpretations permit easy\nunderstanding of the relative importance of features and feature interactions.\nOur interpretation algorithm represents a leap forward from the previous state\nof the art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 12:39:44 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Crabb\u00e9", "Jonathan", ""], ["Zhang", "Yao", ""], ["Zame", "William", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2011.08606", "submitter": "Deeksha Sinha", "authors": "Vivek F. Farias, Andrew A. Li, and Deeksha Sinha", "title": "Optimizing Offer Sets in Sub-Linear Time", "comments": "30 pages, 3 figures. Proceedings of the 21st ACM Conference on\n  Economics and Computation. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization and recommendations are now accepted as core competencies in\njust about every online setting, ranging from media platforms to e-commerce to\nsocial networks. While the challenge of estimating user preferences has\ngarnered significant attention, the operational problem of using such\npreferences to construct personalized offer sets to users is still a challenge,\nparticularly in modern settings where a massive number of items and a\nmillisecond response time requirement mean that even enumerating all of the\nitems is impossible. Faced with such settings, existing techniques are either\n(a) entirely heuristic with no principled justification, or (b) theoretically\nsound, but simply too slow to work.\n  Thus motivated, we propose an algorithm for personalized offer set\noptimization that runs in time sub-linear in the number of items while enjoying\na uniform performance guarantee. Our algorithm works for an extremely general\nclass of problems and models of user choice that includes the mixed multinomial\nlogit model as a special case. We achieve a sub-linear runtime by leveraging\nthe dimensionality reduction from learning an accurate latent factor model,\nalong with existing sub-linear time approximate near neighbor algorithms. Our\nalgorithm can be entirely data-driven, relying on samples of the user, where a\n`sample' refers to the user interaction data typically collected by firms. We\nevaluate our approach on a massive content discovery dataset from Outbrain that\nincludes millions of advertisements. Results show that our implementation\nindeed runs fast and with increased performance relative to existing fast\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:02:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Farias", "Vivek F.", ""], ["Li", "Andrew A.", ""], ["Sinha", "Deeksha", ""]]}, {"id": "2011.08612", "submitter": "Jing Zhang", "authors": "Jing Zhang and Dacheng Tao", "title": "Empowering Things with Intelligence: A Survey of the Progress,\n  Challenges, and Opportunities in Artificial Intelligence of Things", "comments": "Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things (IoT) era, billions of sensors and devices collect\nand process data from the environment, transmit them to cloud centers, and\nreceive feedback via the internet for connectivity and perception. However,\ntransmitting massive amounts of heterogeneous data, perceiving complex\nenvironments from these data, and then making smart decisions in a timely\nmanner are difficult. Artificial intelligence (AI), especially deep learning,\nis now a proven success in various areas including computer vision, speech\nrecognition, and natural language processing. AI introduced into the IoT\nheralds the era of artificial intelligence of things (AIoT). This paper\npresents a comprehensive survey on AIoT to show how AI can empower the IoT to\nmake it faster, smarter, greener, and safer. Specifically, we briefly present\nthe AIoT architecture in the context of cloud computing, fog computing, and\nedge computing. Then, we present progress in AI research for IoT from four\nperspectives: perceiving, learning, reasoning, and behaving. Next, we summarize\nsome promising applications of AIoT that are likely to profoundly reshape our\nworld. Finally, we highlight the challenges facing AIoT and some potential\nresearch opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:14:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.08649", "submitter": "Zerong Xi", "authors": "Zerong Xi, Gita Sukthankar", "title": "Leveraging the Variance of Return Sequences for Exploration Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a method for constructing an upper bound for\nexploration policy using either the weighted variance of return sequences or\nthe weighted temporal difference (TD) error. We demonstrate that the variance\nof the return sequence for a specific state-action pair is an important\ninformation source that can be leveraged to guide exploration in reinforcement\nlearning. The intuition is that fluctuation in the return sequence indicates\ngreater uncertainty in the near future returns. This divergence occurs because\nof the cyclic nature of value-based reinforcement learning; the evolving value\nfunction begets policy improvements which in turn modify the value function.\nAlthough both variance and TD errors capture different aspects of this\nuncertainty, our analysis shows that both can be valuable to guide exploration.\nWe propose a two-stream network architecture to estimate weighted variance/TD\nerrors within DQN agents for our exploration method and show that it\noutperforms the baseline on a wide range of Atari games.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:19:22 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Xi", "Zerong", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2011.08671", "submitter": "Dilusha Weeraddana Dr", "authors": "Dilusha Weeraddana, Sudaraka MallawaArachchi, Tharindu Warnakula,\n  Zhidong Li, and Yang Wang", "title": "Long-Term Pipeline Failure Prediction Using Nonparametric Survival\n  Analysis", "comments": "ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Australian water infrastructure is more than a hundred years old, thus has\nbegun to show its age through water main failures. Our work concerns\napproximately half a million pipelines across major Australian cities that\ndeliver water to houses and businesses, serving over five million customers.\nFailures on these buried assets cause damage to properties and water supply\ndisruptions. We applied Machine Learning techniques to find a cost-effective\nsolution to the pipe failure problem in these Australian cities, where on\naverage 1500 of water main failures occur each year. To achieve this objective,\nwe construct a detailed picture and understanding of the behaviour of the water\npipe network by developing a Machine Learning model to assess and predict the\nfailure likelihood of water main breaking using historical failure records,\ndescriptors of pipes and other environmental factors. Our results indicate that\nour system incorporating a nonparametric survival analysis technique called\n\"Random Survival Forest\" outperforms several popular algorithms and expert\nheuristics in long-term prediction. In addition, we construct a statistical\ninference technique to quantify the uncertainty associated with the long-term\npredictions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 02:31:31 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Weeraddana", "Dilusha", ""], ["MallawaArachchi", "Sudaraka", ""], ["Warnakula", "Tharindu", ""], ["Li", "Zhidong", ""], ["Wang", "Yang", ""]]}, {"id": "2011.08674", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "On Numerosity of Deep Neural Networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a provocative claim was published that number sense spontaneously\nemerges in a deep neural network trained merely for visual object recognition.\nThis has, if true, far reaching significance to the fields of machine learning\nand cognitive science alike. In this paper, we prove the above claim to be\nunfortunately incorrect. The statistical analysis to support the claim is\nflawed in that the sample set used to identify number-aware neurons is too\nsmall, compared to the huge number of neurons in the object recognition\nnetwork. By this flawed analysis one could mistakenly identify number-sensing\nneurons in any randomly initialized deep neural networks that are not trained\nat all. With the above critique we ask the question what if a deep\nconvolutional neural network is carefully trained for numerosity? Our findings\nare mixed. Even after being trained with number-depicting images, the deep\nlearning approach still has difficulties to acquire the abstract concept of\nnumbers, a cognitive task that preschoolers perform with ease. But on the other\nhand, we do find some encouraging evidences suggesting that deep neural\nnetworks are more robust to distribution shift for small numbers than for large\nnumbers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:30:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2011.08678", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yang Xiao, Jiang Guo, Xiangyu Yue, Jufeng Yang, Ravi\n  Krishna, Pengfei Xu, Kurt Keutzer", "title": "Curriculum CycleGAN for Textual Sentiment Domain Adaptation with\n  Multiple Sources", "comments": "Accepted by WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of user-generated reviews or comments on products and\nservices in social networks can help enterprises to analyze the feedback from\ncustomers and take corresponding actions for improvement. To mitigate\nlarge-scale annotations on the target domain, domain adaptation (DA) provides\nan alternate solution by learning a transferable model from other labeled\nsource domains. Existing multi-source domain adaptation (MDA) methods either\nfail to extract some discriminative features in the target domain that are\nrelated to sentiment, neglect the correlations of different sources and the\ndistribution difference among different sub-domains even in the same source, or\ncannot reflect the varying optimal weighting during different training stages.\nIn this paper, we propose a novel instance-level MDA framework, named\ncurriculum cycle-consistent generative adversarial network (C-CycleGAN), to\naddress the above issues. Specifically, C-CycleGAN consists of three\ncomponents: (1) pre-trained text encoder which encodes textual input from\ndifferent domains into a continuous representation space, (2) intermediate\ndomain generator with curriculum instance-level adaptation which bridges the\ngap across source and target domains, and (3) task classifier trained on the\nintermediate domain for final sentiment classification. C-CycleGAN transfers\nsource samples at instance-level to an intermediate domain that is closer to\nthe target domain with sentiment semantics preserved and without losing\ndiscriminative features. Further, our dynamic instance-level weighting\nmechanisms can assign the optimal weights to different source samples in each\ntraining stage. We conduct extensive experiments on three benchmark datasets\nand achieve substantial gains over state-of-the-art DA approaches. Our source\ncode is released at: https://github.com/WArushrush/Curriculum-CycleGAN.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:50:55 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 01:22:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Zhao", "Sicheng", ""], ["Xiao", "Yang", ""], ["Guo", "Jiang", ""], ["Yue", "Xiangyu", ""], ["Yang", "Jufeng", ""], ["Krishna", "Ravi", ""], ["Xu", "Pengfei", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2011.08692", "submitter": "Nina Varney", "authors": "Nina Varney, Vijayan K. Asari and Quinn Graehling", "title": "Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature\n  Layers", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method to learn a diverse group of object categories from an\nunordered point set. We propose our Pyramid Point network, which uses a dense\npyramid structure instead of the traditional 'U' shape, typically seen in\nsemantic segmentation networks. This pyramid structure gives a second look,\nallowing the network to revisit different layers simultaneously, increasing the\ncontextual information by creating additional layers with less noise. We\nintroduce a Focused Kernel Point convolution (FKP Conv), which expands on the\ntraditional point convolutions by adding an attention mechanism to the kernel\noutputs. This FKP Conv increases our feature quality and allows us to weigh the\nkernel outputs dynamically. These FKP Convs are the central part of our\nRecurrent FKP Bottleneck block, which makes up the backbone of our encoder.\nWith this distinct network, we demonstrate competitive performance on three\nbenchmark data sets. We also perform an ablation study to show the positive\neffects of each element in our FKP Conv.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:23:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:35:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Varney", "Nina", ""], ["Asari", "Vijayan K.", ""], ["Graehling", "Quinn", ""]]}, {"id": "2011.08733", "submitter": "Amruta Yelamanchili", "authors": "Jagriti Agrawal and Amruta Yelamanchili and Steve Chien", "title": "Using Explainable Scheduling for the Mars 2020 Rover Mission", "comments": "Submitted to the International Workshop of Explainable AI Planning\n  (XAIP) at the International Conference on Automated Planning and Scheduling\n  (ICAPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the reasoning behind the behavior of an automated scheduling\nsystem is essential to ensure that it will be trusted and consequently used to\nits full capabilities in critical applications. In cases where a scheduler\nschedules activities in an invalid location, it is usually easy for the user to\ninfer the missing constraint by inspecting the schedule with the invalid\nactivity to determine the missing constraint. If a scheduler fails to schedule\nactivities because constraints could not be satisfied, determining the cause\ncan be more challenging. In such cases it is important to understand which\nconstraints caused the activities to fail to be scheduled and how to alter\nconstraints to achieve the desired schedule. In this paper, we describe such a\nscheduling system for NASA's Mars 2020 Perseverance Rover, as well as\nCrosscheck, an explainable scheduling tool that explains the scheduler\nbehavior. The scheduling system and Crosscheck are the baseline for operational\nuse to schedule activities for the Mars 2020 rover. As we describe, the\nscheduler generates a schedule given a set of activities and their constraints\nand Crosscheck: (1) provides a visual representation of the generated schedule;\n(2) analyzes and explains why activities failed to schedule given the\nconstraints provided; and (3) provides guidance on potential constraint\nrelaxations to enable the activities to schedule in future scheduler runs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:10:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Agrawal", "Jagriti", ""], ["Yelamanchili", "Amruta", ""], ["Chien", "Steve", ""]]}, {"id": "2011.08743", "submitter": "Mohammed Sharafath Abdul Hameed", "authors": "Mohammed Sharafath Abdul Hameed, Md Muzahid Khan, Andreas Schwung", "title": "Curiosity Based Reinforcement Learning on Robot Manufacturing Cell", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces a novel combination of scheduling control on a flexible\nrobot manufacturing cell with curiosity based reinforcement learning.\nReinforcement learning has proved to be highly successful in solving tasks like\nrobotics and scheduling. But this requires hand tuning of rewards in problem\ndomains like robotics and scheduling even where the solution is not obvious. To\nthis end, we apply a curiosity based reinforcement learning, using intrinsic\nmotivation as a form of reward, on a flexible robot manufacturing cell to\nalleviate this problem. Further, the learning agents are embedded into the\ntransportation robots to enable a generalized learning solution that can be\napplied to a variety of environments. In the first approach, the curiosity\nbased reinforcement learning is applied to a simple structured robot\nmanufacturing cell. And in the second approach, the same algorithm is applied\nto a graph structured robot manufacturing cell. Results from the experiments\nshow that the agents are able to solve both the environments with the ability\nto transfer the curiosity module directly from one environment to another. We\nconclude that curiosity based learning on scheduling tasks provide a viable\nalternative to the reward shaped reinforcement learning traditionally used.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:19:47 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hameed", "Mohammed Sharafath Abdul", ""], ["Khan", "Md Muzahid", ""], ["Schwung", "Andreas", ""]]}, {"id": "2011.08755", "submitter": "Bimal Bhattarai", "authors": "Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao", "title": "Measuring the Novelty of Natural Language Text Using the Conjunctive\n  Clauses of a Tsetlin Machine Text Classifier", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most supervised text classification approaches assume a closed world,\ncounting on all classes being present in the data at training time. This\nassumption can lead to unpredictable behaviour during operation, whenever\nnovel, previously unseen, classes appear. Although deep learning-based methods\nhave recently been used for novelty detection, they are challenging to\ninterpret due to their black-box nature. This paper addresses\n\\emph{interpretable} open-world text classification, where the trained\nclassifier must deal with novel classes during operation. To this end, we\nextend the recently introduced Tsetlin machine (TM) with a novelty scoring\nmechanism. The mechanism uses the conjunctive clauses of the TM to measure to\nwhat degree a text matches the classes covered by the training data. We\ndemonstrate that the clauses provide a succinct interpretable description of\nknown topics, and that our scoring mechanism makes it possible to discern novel\ntopics from the known ones. Empirically, our TM-based approach outperforms\nseven other novelty detection schemes on three out of five datasets, and\nperforms second and third best on the remaining, with the added benefit of an\ninterpretable propositional logic-based representation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:35:21 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Bhattarai", "Bimal", ""], ["Granmo", "Ole-Christoffer", ""], ["Jiao", "Lei", ""]]}, {"id": "2011.08772", "submitter": "Zimo Zhou", "authors": "Hongru Wang, Min Li, Zimo Zhou, Gabriel Pui Cheong Fung, Kam-Fai Wong", "title": "KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant\n  Towards Customized Dialogue System", "comments": "8 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with CrossWOZ (Chinese) and MultiWOZ (English) dataset which have\ncoarse-grained information, there is no dataset which handle fine-grained and\nhierarchical level information properly. In this paper, we publish a first\nCantonese knowledge-driven Dialogue Dataset for REStaurant (KddRES) in Hong\nKong, which grounds the information in multi-turn conversations to one specific\nrestaurant. Our corpus contains 0.8k conversations which derive from 10\nrestaurants with various styles in different regions. In addition to that, we\ndesigned fine-grained slots and intents to better capture semantic information.\nThe benchmark experiments and data statistic analysis show the diversity and\nrich annotations of our dataset. We believe the publish of KddRES can be a\nnecessary supplement of current dialogue datasets and more suitable and\nvaluable for small and middle enterprises (SMEs) of society, such as build a\ncustomized dialogue system for each restaurant. The corpus and benchmark models\nare publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:57:41 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 11:38:24 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Wang", "Hongru", ""], ["Li", "Min", ""], ["Zhou", "Zimo", ""], ["Fung", "Gabriel Pui Cheong", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "2011.08774", "submitter": "David Piorkowski", "authors": "David Piorkowski, Daniel Gonz\\'alez, John Richards and Stephanie Houde", "title": "Towards evaluating and eliciting high-quality documentation for\n  intelligent systems", "comments": "15 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital component of trust and transparency in intelligent systems built on\nmachine learning and artificial intelligence is the development of clear,\nunderstandable documentation. However, such systems are notorious for their\ncomplexity and opaqueness making quality documentation a non-trivial task.\nFurthermore, little is known about what makes such documentation \"good.\" In\nthis paper, we propose and evaluate a set of quality dimensions to identify in\nwhat ways this type of documentation falls short. Then, using those dimensions,\nwe evaluate three different approaches for eliciting intelligent system\ndocumentation. We show how the dimensions identify shortcomings in such\ndocumentation and posit how such dimensions can be use to further enable users\nto provide documentation that is suitable to a given persona or use case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:01:47 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Piorkowski", "David", ""], ["Gonz\u00e1lez", "Daniel", ""], ["Richards", "John", ""], ["Houde", "Stephanie", ""]]}, {"id": "2011.08779", "submitter": "Cory Merkel", "authors": "Cory Merkel", "title": "Exploring Energy-Accuracy Tradeoffs in AI Hardware", "comments": "To be published in the proceedings of the 2020 International Green\n  and Sustainable Computing Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) is playing an increasingly significant role in\nour everyday lives. This trend is expected to continue, especially with recent\npushes to move more AI to the edge. However, one of the biggest challenges\nassociated with AI on edge devices (mobile phones, unmanned vehicles, sensors,\netc.) is their associated size, weight, and power constraints. In this work, we\nconsider the scenario where an AI system may need to operate at\nless-than-maximum accuracy in order to meet application-dependent energy\nrequirements. We propose a simple function that divides the cost of using an AI\nsystem into the cost of the decision making process and the cost of decision\nexecution. For simple binary decision problems with convolutional neural\nnetworks, it is shown that minimizing the cost corresponds to using fewer than\nthe maximum number of resources (e.g. convolutional neural network layers and\nfilters). Finally, it is shown that the cost associated with energy can be\nsignificantly reduced by leveraging high-confidence predictions made in\nlower-level layers of the network.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:14:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Merkel", "Cory", ""]]}, {"id": "2011.08816", "submitter": "Jean Christoph Jung", "authors": "V\\'ictor Guti\\'errez-Basulto and Yazm\\'in Ib\\'a\\~nez-Garc\\'ia and Jean\n  Christoph Jung", "title": "Answering Regular Path Queries Over SQ Ontologies", "comments": "Full Version of AAAI'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study query answering in the description logic $\\mathcal{SQ}$ supporting\nqualified number restrictions on both transitive and non-transitive roles. Our\nmain contributions are a tree-like model property for $\\mathcal{SQ}$ knowledge\nbases and, building upon this, an optimal automata-based algorithm for\nanswering positive existential regular path queries in 2ExpTime.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:27:20 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Guti\u00e9rrez-Basulto", "V\u00edctor", ""], ["Ib\u00e1\u00f1ez-Garc\u00eda", "Yazm\u00edn", ""], ["Jung", "Jean Christoph", ""]]}, {"id": "2011.08820", "submitter": "Jonathan Uesato", "authors": "Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria\n  Krakovna, Shane Legg", "title": "REALab: An Embedded Perspective on Tampering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes REALab, a platform for embedded agency research in\nreinforcement learning (RL). REALab is designed to model the structure of\ntampering problems that may arise in real-world deployments of RL. Standard\nMarkov Decision Process (MDP) formulations of RL and simulated environments\nmirroring the MDP structure assume secure access to feedback (e.g., rewards).\nThis may be unrealistic in settings where agents are embedded and can corrupt\nthe processes producing feedback (e.g., human supervisors, or an implemented\nreward function). We describe an alternative Corrupt Feedback MDP formulation\nand the REALab environment platform, which both avoid the secure feedback\nassumption. We hope the design of REALab provides a useful perspective on\ntampering problems, and that the platform may serve as a unit test for the\npresence of tampering incentives in RL agent designs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:37:20 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kumar", "Ramana", ""], ["Uesato", "Jonathan", ""], ["Ngo", "Richard", ""], ["Everitt", "Tom", ""], ["Krakovna", "Victoria", ""], ["Legg", "Shane", ""]]}, {"id": "2011.08822", "submitter": "Zack Dulberg", "authors": "Zachary Dulberg and Jonathan Cohen", "title": "Learning Canonical Transformations", "comments": "NeurIPS 2020 Workshop on BabyMind", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans understand a set of canonical geometric transformations (such as\ntranslation and rotation) that support generalization by being untethered to\nany specific object. We explore inductive biases that help a neural network\nmodel learn these transformations in pixel space in a way that can generalize\nout-of-domain. Specifically, we find that high training set diversity is\nsufficient for the extrapolation of translation to unseen shapes and scales,\nand that an iterative training scheme achieves significant extrapolation of\nrotation in time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:41:07 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dulberg", "Zachary", ""], ["Cohen", "Jonathan", ""]]}, {"id": "2011.08827", "submitter": "Jonathan Uesato", "authors": "Jonathan Uesato, Ramana Kumar, Victoria Krakovna, Tom Everitt, Richard\n  Ngo, Shane Legg", "title": "Avoiding Tampering Incentives in Deep RL via Decoupled Approval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design agents that pursue a given objective when all feedback\nmechanisms are influenceable by the agent? Standard RL algorithms assume a\nsecure reward function, and can thus perform poorly in settings where agents\ncan tamper with the reward-generating mechanism. We present a principled\nsolution to the problem of learning from influenceable feedback, which combines\napproval with a decoupled feedback collection procedure. For a natural class of\ncorruption functions, decoupled approval algorithms have aligned incentives\nboth at convergence and for their local updates. Empirically, they also scale\nto complex 3D environments where tampering is possible.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:48:59 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Uesato", "Jonathan", ""], ["Kumar", "Ramana", ""], ["Krakovna", "Victoria", ""], ["Everitt", "Tom", ""], ["Ngo", "Richard", ""], ["Legg", "Shane", ""]]}, {"id": "2011.08843", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Rex Ying, Jure Leskovec", "title": "Design Space for Graph Neural Networks", "comments": "NeurIPS 2020 (Spotlight). Typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid evolution of Graph Neural Networks (GNNs) has led to a growing\nnumber of new architectures as well as novel applications. However, current\nresearch focuses on proposing and evaluating specific architectural designs of\nGNNs, as opposed to studying the more general design space of GNNs that\nconsists of a Cartesian product of different design dimensions, such as the\nnumber of layers or the type of the aggregation function. Additionally, GNN\ndesigns are often specialized to a single task, yet few efforts have been made\nto understand how to quickly find the best GNN design for a novel task or a\nnovel dataset. Here we define and systematically study the architectural design\nspace for GNNs which consists of 315,000 different designs over 32 different\npredictive tasks. Our approach features three key innovations: (1) A general\nGNN design space; (2) a GNN task space with a similarity metric, so that for a\ngiven novel task/dataset, we can quickly identify/transfer the best performing\narchitecture; (3) an efficient and effective design space evaluation method\nwhich allows insights to be distilled from a huge number of model-task\ncombinations. Our key results include: (1) A comprehensive set of guidelines\nfor designing well-performing GNNs; (2) while best GNN designs for different\ntasks vary significantly, the GNN task space allows for transferring the best\ndesigns across different tasks; (3) models discovered using our design space\nachieve state-of-the-art performance. Overall, our work offers a principled and\nscalable approach to transition from studying individual GNN designs for\nspecific tasks, to systematically studying the GNN design space and the task\nspace. Finally, we release GraphGym, a powerful platform for exploring\ndifferent GNN designs and tasks. GraphGym features modularized GNN\nimplementation, standardized GNN evaluation, and reproducible and scalable\nexperiment management.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:59:27 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 20:37:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["You", "Jiaxuan", ""], ["Ying", "Rex", ""], ["Leskovec", "Jure", ""]]}, {"id": "2011.08881", "submitter": "Andrei Diaconu", "authors": "Andrei Diaconu", "title": "Learning functional programs with function invention and reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inductive programming (IP) is a field whose main goal is synthesising\nprograms that respect a set of examples, given some form of background\nknowledge. This paper is concerned with a subfield of IP, inductive functional\nprogramming (IFP). We explore the idea of generating modular functional\nprograms, and how those allow for function reuse, with the aim to reduce the\nsize of the programs. We introduce two algorithms that attempt to solve the\nproblem and explore type based pruning techniques in the context of modular\nprograms. By experimenting with the implementation of one of those algorithms,\nwe show reuse is important (if not crucial) for a variety of problems and\ndistinguished two broad classes of programs that will generally benefit from\nfunction reuse.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:11:00 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Diaconu", "Andrei", ""]]}, {"id": "2011.08906", "submitter": "Dian Yu", "authors": "Kaihui Liang, Austin Chau, Yu Li, Xueyuan Lu, Dian Yu, Mingyang Zhou,\n  Ishan Jain, Sam Davidson, Josh Arnold, Minh Nguyen, Zhou Yu", "title": "Gunrock 2.0: A User Adaptive Social Conversational System", "comments": "Published in 3rd Proceedings of Alexa Prize (Alexa Prize 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gunrock 2.0 is built on top of Gunrock with an emphasis on user adaptation.\nGunrock 2.0 combines various neural natural language understanding modules,\nincluding named entity detection, linking, and dialog act prediction, to\nimprove user understanding. Its dialog management is a hierarchical model that\nhandles various topics, such as movies, music, and sports. The system-level\ndialog manager can handle question detection, acknowledgment, error handling,\nand additional functions, making downstream modules much easier to design and\nimplement. The dialog manager also adapts its topic selection to accommodate\ndifferent users' profile information, such as inferred gender and personality.\nThe generation model is a mix of templates and neural generation models.\nGunrock 2.0 is able to achieve an average rating of 3.73 at its latest build\nfrom May 29th to June 4th.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:52:32 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 18:47:46 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liang", "Kaihui", ""], ["Chau", "Austin", ""], ["Li", "Yu", ""], ["Lu", "Xueyuan", ""], ["Yu", "Dian", ""], ["Zhou", "Mingyang", ""], ["Jain", "Ishan", ""], ["Davidson", "Sam", ""], ["Arnold", "Josh", ""], ["Nguyen", "Minh", ""], ["Yu", "Zhou", ""]]}, {"id": "2011.08909", "submitter": "Benjamin Eysenbach", "authors": "Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine", "title": "C-Learning: Learning to Achieve Goals via Recursive Classification", "comments": "Accepted at ICLR 2021. Project website with videos\n  (https://ben-eysenbach.github.io/c_learning/) and code\n  (https://github.com/google-research/google-research/tree/master/c_learning)\n  are online", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of predicting and controlling the future state\ndistribution of an autonomous agent. This problem, which can be viewed as a\nreframing of goal-conditioned reinforcement learning (RL), is centered around\nlearning a conditional probability density function over future states. Instead\nof directly estimating this density function, we indirectly estimate this\ndensity function by training a classifier to predict whether an observation\ncomes from the future. Via Bayes' rule, predictions from our classifier can be\ntransformed into predictions over future states. Importantly, an off-policy\nvariant of our algorithm allows us to predict the future state distribution of\na new policy, without collecting new experience. This variant allows us to\noptimize functionals of a policy's future state distribution, such as the\ndensity of reaching a particular goal state. While conceptually similar to\nQ-learning, our work lays a principled foundation for goal-conditioned RL as\ndensity estimation, providing justification for goal-conditioned methods used\nin prior work. This foundation makes hypotheses about Q-learning, including the\noptimal goal-sampling ratio, which we confirm experimentally. Moreover, our\nproposed method is competitive with prior goal-conditioned RL methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:58:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 18:33:47 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Eysenbach", "Benjamin", ""], ["Salakhutdinov", "Ruslan", ""], ["Levine", "Sergey", ""]]}, {"id": "2011.08916", "submitter": "Tanvirul Alam", "authors": "Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam and Umair Qazi", "title": "Deep Learning Benchmarks and Datasets for Social Media Image\n  Classification for Disaster Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a disaster event, images shared on social media helps crisis managers\ngain situational awareness and assess incurred damages, among other response\ntasks. Recent advances in computer vision and deep neural networks have enabled\nthe development of models for real-time image classification for a number of\ntasks, including detecting crisis incidents, filtering irrelevant images,\nclassifying images into specific humanitarian categories, and assessing the\nseverity of damage. Despite several efforts, past works mainly suffer from\nlimited resources (i.e., labeled images) available to train more robust deep\nlearning models. In this study, we propose new datasets for disaster type\ndetection, and informativeness classification, and damage severity assessment.\nMoreover, we relabel existing publicly available datasets for new tasks. We\nidentify exact- and near-duplicates to form non-overlapping data splits, and\nfinally consolidate them to create larger datasets. In our extensive\nexperiments, we benchmark several state-of-the-art deep learning models and\nachieve promising results. We release our datasets and models publicly, aiming\nto provide proper baselines as well as to spur further research in the crisis\ninformatics community.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:15:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Alam", "Firoj", ""], ["Ofli", "Ferda", ""], ["Imran", "Muhammad", ""], ["Alam", "Tanvirul", ""], ["Qazi", "Umair", ""]]}, {"id": "2011.08960", "submitter": "Ruixiang Tang", "authors": "Ruixiang Tang, Mengnan Du, Xia Hu", "title": "Deep Serial Number: Computational Watermarking for DNN Intellectual\n  Property Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DSN (Deep Serial Number), a new watermarking\napproach that can prevent the stolen model from being deployed by unauthorized\nparties. Recently, watermarking in DNNs has emerged as a new research direction\nfor owners to claim ownership of DNN models. However, the verification schemes\nof existing watermarking approaches are vulnerable to various watermark\nattacks. Different from existing work that embeds identification information\ninto DNNs, we explore a new DNN Intellectual Property Protection mechanism that\ncan prevent adversaries from deploying the stolen deep neural networks.\nMotivated by the success of serial number in protecting conventional software\nIP, we introduce the first attempt to embed a serial number into DNNs.\nSpecifically, the proposed DSN is implemented in the knowledge distillation\nframework, where a private teacher DNN is first trained, then its knowledge is\ndistilled and transferred to a series of customized student DNNs. During the\ndistillation process, each customer DNN is augmented with a unique serial\nnumber, i.e., an encrypted 0/1 bit trigger pattern. Customer DNN works properly\nonly when a potential customer enters the valid serial number. The embedded\nserial number could be used as a strong watermark for ownership verification.\nExperiments on various applications indicate that DSN is effective in terms of\npreventing unauthorized application while not sacrificing the original DNN\nperformance. The experimental analysis further shows that DSN is resistant to\ndifferent categories of attacks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 21:42:40 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tang", "Ruixiang", ""], ["Du", "Mengnan", ""], ["Hu", "Xia", ""]]}, {"id": "2011.08977", "submitter": "Nemath Ahmed", "authors": "Nemath Ahmed, Aashit Singh, Srivyshnav KS, Gulshan Kumar, Gaurav\n  Parchani, Vibhor Saran", "title": "Classification Of Sleep-Wake State In A Ballistocardiogram System Based\n  On Deep Learning", "comments": "11 Pages, 4 Figues, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep state classification is vital in managing and understanding sleep\npatterns and is generally the first step in identifying acute or chronic sleep\ndisorders. However, it is essential to do this without affecting the natural\nenvironment or conditions of the subject during their sleep. Techniques such as\nPolysomnography(PSG) are obtrusive and are not convenient for regular sleep\nmonitoring. Fortunately, The rise of novel technologies and advanced computing\nhas given a recent resurgence to monitoring sleep techniques. One such\ncontactless and unobtrusive monitoring technique is Ballistocradiography(BCG),\nin which vitals are monitored by measuring the body's reaction to the cardiac\nejection of blood. In this study, we propose a Multi-Head 1D-Convolution based\nDeep Neural Network to classify sleep-wake state and predict sleep-wake time\naccurately using the signals coming from a BCG sensor. Our method achieves a\nsleep-wake classification score of 95.5%, which is on par with researches based\non the PSG system. We further conducted two independent studies in a controlled\nand uncontrolled environment to test the sleep-wake prediction accuracy. We\nachieve a score of 94.16% in a controlled environment on 115 subjects and\n94.90% in an uncontrolled environment on 350 subjects. The high accuracy and\ncontactless nature of the proposed system make it a convenient method for long\nterm monitoring of sleep states.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 03:38:33 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ahmed", "Nemath", ""], ["Singh", "Aashit", ""], ["KS", "Srivyshnav", ""], ["Kumar", "Gulshan", ""], ["Parchani", "Gaurav", ""], ["Saran", "Vibhor", ""]]}, {"id": "2011.08981", "submitter": "Xiangyu Gao", "authors": "Xiangyu Gao, Guanbin Xing, Sumit Roy, and Hui Liu", "title": "RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object\n  Recognition", "comments": "15 pages", "journal-ref": "IEEE Sensor Journal, 2020", "doi": "10.1109/JSEN.2020.3036047", "report-no": null, "categories": "eess.SP cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter-wave (mmW) radars are being increasingly integrated into\ncommercial vehicles to support new advanced driver-assistance systems (ADAS) by\nenabling robust and high-performance object detection, localization, as well as\nrecognition - a key component of new environmental perception. In this paper,\nwe propose a novel radar multiple-perspectives convolutional neural network\n(RAMP-CNN) that extracts the location and class of objects based on further\nprocessing of the range-velocity-angle (RVA) heatmap sequences. To bypass the\ncomplexity of 4D convolutional neural networks (NN), we propose to combine\nseveral lower-dimension NN models within our RAMP-CNN model that nonetheless\napproaches the performance upper-bound with lower complexity. The extensive\nexperiments show that the proposed RAMP-CNN model achieves better average\nrecall (AR) and average precision (AP) than prior works in all testing\nscenarios (see Table. III). Besides, the RAMP-CNN model is validated to work\nrobustly under the nighttime, which enables low-cost radars as a potential\nsubstitute for pure optical sensing under severe conditions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:12:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gao", "Xiangyu", ""], ["Xing", "Guanbin", ""], ["Roy", "Sumit", ""], ["Liu", "Hui", ""]]}, {"id": "2011.08999", "submitter": "Kaushik Manchella", "authors": "Kaushik Manchella, Marina Haliem, Vaneet Aggarwal, and Bharat Bhargava", "title": "PassGoodPool: Joint Passengers and Goods Fleet Management with\n  Reinforcement Learning aided Pricing, Matching, and Route Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we present a dynamic, demand aware, and pricing-based matching\nand route planning framework that allows efficient pooling of multiple\npassengers and goods in each vehicle. This approach includes the flexibility\nfor transferring goods through multiple hops from source to destination as well\nas pooling of passengers. The key components of the proposed approach include\n(i) Pricing by the vehicles to passengers which is based on the insertion cost,\nwhich determines the matching based on passenger's acceptance/rejection, (ii)\nMatching of goods to vehicles, and the multi-hop routing of goods, (iii) Route\nplanning of the vehicles to pick. up and drop passengers and goods, (i)\nDispatching idle vehicles to areas of anticipated high passenger and goods\ndemand using Deep Reinforcement Learning, and (v) Allowing for distributed\ninference at each vehicle while collectively optimizing fleet objectives. Our\nproposed framework can be deployed independently within each vehicle as this\nminimizes computational costs associated with the gorwth of distributed systems\nand democratizes decision-making to each individual. The proposed framework is\nvalidated in a simulated environment, where we leverage realistic delivery\ndatasets such as the New York City Taxi public dataset and Google Maps traffic\ndata from delivery offering businesses.Simulations on a variety of vehicle\ntypes, goods, and passenger utility functions show the effectiveness of our\napproach as compared to baselines that do not consider combined load\ntransportation or dynamic multi-hop route planning. Our proposed method showed\nimprovements over the next best baseline in various aspects including a 15%\nincrease in fleet utilization and 20% increase in average vehicle profits.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:15:03 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Manchella", "Kaushik", ""], ["Haliem", "Marina", ""], ["Aggarwal", "Vaneet", ""], ["Bhargava", "Bharat", ""]]}, {"id": "2011.09004", "submitter": "Rebecca Russell", "authors": "Aastha Acharya, Rebecca Russell, Nisar R. Ahmed", "title": "Explaining Conditions for Reinforcement Learning Behaviors from Real and\n  Imagined Data", "comments": "Accepted to the Workshop on Challenges of Real-World RL at NeurIPS\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of reinforcement learning (RL) in the real world comes with\nchallenges in calibrating user trust and expectations. As a step toward\ndeveloping RL systems that are able to communicate their competencies, we\npresent a method of generating human-interpretable abstract behavior models\nthat identify the experiential conditions leading to different task execution\nstrategies and outcomes. Our approach consists of extracting experiential\nfeatures from state representations, abstracting strategy descriptors from\ntrajectories, and training an interpretable decision tree that identifies the\nconditions most predictive of different RL behaviors. We demonstrate our method\non trajectory data generated from interactions with the environment and on\nimagined trajectory data that comes from a trained probabilistic world model in\na model-based RL setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:40:47 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Acharya", "Aastha", ""], ["Russell", "Rebecca", ""], ["Ahmed", "Nisar R.", ""]]}, {"id": "2011.09006", "submitter": "Stylianos Loukas Vasileiou", "authors": "Stylianos Loukas Vasileiou, William Yeoh, Tran Cao Son", "title": "On the Relationship Between KR Approaches for Explainable Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon notions from knowledge representation and\nreasoning (KR) to expand a preliminary logic-based framework that characterizes\nthe model reconciliation problem for explainable planning. We also provide a\ndetailed exposition on the relationship between similar KR techniques, such as\nabductive explanations and belief change, and their applicability to\nexplainable planning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 23:57:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 01:37:43 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 18:57:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Vasileiou", "Stylianos Loukas", ""], ["Yeoh", "William", ""], ["Son", "Tran Cao", ""]]}, {"id": "2011.09020", "submitter": "Rong Zhu", "authors": "Ziniu Wu, Rong Zhu, Andreas Pfadler, Yuxing Han, Jiangneng Li,\n  Zhengping Qian, Kai Zeng, Jingren Zhou", "title": "FSPN: A New Class of Probabilistic Graphical Model", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce factorize sum split product networks (FSPNs), a new class of\nprobabilistic graphical models (PGMs). FSPNs are designed to overcome the\ndrawbacks of existing PGMs in terms of estimation accuracy and inference\nefficiency. Specifically, Bayesian networks (BNs) have low inference speed and\nperformance of tree structured sum product networks(SPNs) significantly\ndegrades in presence of highly correlated variables. FSPNs absorb their\nadvantages by adaptively modeling the joint distribution of variables according\nto their dependence degree, so that one can simultaneously attain the two\ndesirable goals: high estimation accuracy and fast inference speed. We present\nefficient probability inference and structure learning algorithms for FSPNs,\nalong with a theoretical analysis and extensive evaluation evidence. Our\nexperimental results on synthetic and benchmark datasets indicate the\nsuperiority of FSPN over other PGMs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:11:55 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 08:22:09 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wu", "Ziniu", ""], ["Zhu", "Rong", ""], ["Pfadler", "Andreas", ""], ["Han", "Yuxing", ""], ["Li", "Jiangneng", ""], ["Qian", "Zhengping", ""], ["Zeng", "Kai", ""], ["Zhou", "Jingren", ""]]}, {"id": "2011.09022", "submitter": "Rong Zhu", "authors": "Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping\n  Qian, Jingren Zhou, Bin Cui", "title": "FLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation", "comments": "Technical Report of the FLAT Paper in VLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query optimizers rely on accurate cardinality estimation (CardEst) to produce\ngood execution plans. The core problem of CardEst is how to model the rich\njoint distribution of attributes in an accurate and compact manner. Despite\ndecades of research, existing methods either over simplify the models only\nusing independent factorization which leads to inaccurate estimates, or over\ncomplicate them by lossless conditional factorization without any independent\nassumption which results in slow probability computation. In this paper, we\npropose FLAT, a CardEst method that is simultaneously fast in probability\ncomputation, lightweight in model size and accurate in estimation quality. The\nkey idea of FLAT is a novel unsupervised graphical model, called FSPN. It\nutilizes both independent and conditional factorization to adaptively model\ndifferent levels of attributes correlations, and thus dovetails their\nadvantages. FLAT supports efficient online probability computation in near\nliner time on the underlying FSPN model, provides effective offline model\nconstruction and enables incremental model updates. It can estimate cardinality\nfor both single table queries and multi table join queries. Extensive\nexperimental study demonstrates the superiority of FLAT over existing CardEst\nmethods on well known IMDB benchmarks: FLAT achieves 1 to 5 orders of magnitude\nbetter accuracy, 1 to 3 orders of magnitude faster probability computation\nspeed and 1 to 2 orders of magnitude lower storage cost. We also integrate FLAT\ninto Postgres to perform an end to end test. It improves the query execution\ntime by 12.9% on the benchmark workload, which is very close to the optimal\nresult 14.2% using the true cardinality.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:14:45 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 07:26:58 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 07:52:43 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 08:37:57 GMT"}, {"version": "v5", "created": "Wed, 19 May 2021 07:37:36 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhu", "Rong", ""], ["Wu", "Ziniu", ""], ["Han", "Yuxing", ""], ["Zeng", "Kai", ""], ["Pfadler", "Andreas", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""], ["Cui", "Bin", ""]]}, {"id": "2011.09034", "submitter": "Akshay Sharma", "authors": "Akshay Sharma, Piyush Rajesh Medikeri and Yu Zhang", "title": "Domain Concretization from Examples: Addressing Missing Domain Knowledge\n  via Robust Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of complete domain knowledge is not warranted for robot\nplanning and decision-making in the real world. It could be due to design flaws\nor arise from domain ramifications or qualifications. In such cases, existing\nplanning and learning algorithms could produce highly undesirable behaviors.\nThis problem is more challenging than partial observability in the sense that\nthe agent is unaware of certain knowledge, in contrast to it being partially\nobservable: the difference between known unknowns and unknown unknowns. In this\nwork, we formulate it as the problem of Domain Concretization, an inverse\nproblem to domain abstraction. Based on an incomplete domain model provided by\nthe designer and teacher traces from human users, our algorithm searches for a\ncandidate model set under a minimalistic model assumption. It then generates a\nrobust plan with the maximum probability of success under the set of candidate\nmodels. In addition to a standard search formulation in the model-space, we\npropose a sample-based search method and also an online version of it to\nimprove search time. We tested our approach on IPC domains and a simulated\nrobotics domain where incompleteness was introduced by removing domain features\nfrom the complete model. Results show that our planning algorithm increases the\nplan success rate without impacting the cost much.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:56:15 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sharma", "Akshay", ""], ["Medikeri", "Piyush Rajesh", ""], ["Zhang", "Yu", ""]]}, {"id": "2011.09159", "submitter": "Ke Shen", "authors": "Mayank Kejriwal and Ke Shen", "title": "Do Fine-tuned Commonsense Language Models Really Generalize?", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, transformer-based methods such as RoBERTa and GPT-3 have led to\nsignificant experimental advances in natural language processing tasks such as\nquestion answering and commonsense reasoning. The latter is typically evaluated\nthrough multiple benchmarks framed as multiple-choice instances of the former.\nAccording to influential leaderboards hosted by the Allen Institute (evaluating\nstate-of-the-art performance on commonsense reasoning benchmarks), models based\non such transformer methods are approaching human-like performance and have\naverage accuracy well over 80% on many benchmarks. Since these are commonsense\nbenchmarks, a model that generalizes on commonsense reasoning should not\nexperience much performance loss across multiple commonsense benchmarks. In\nthis paper, we study the generalization issue in detail by designing and\nconducting a rigorous scientific study. Using five common benchmarks, multiple\ncontrols and statistical analysis, we find clear evidence that fine-tuned\ncommonsense language models still do not generalize well, even with moderate\nchanges to the experimental setup, and may, in fact, be susceptible to dataset\nbias. We also perform selective studies, including qualitative and consistency\nanalyses, to gain deeper insight into the problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:52:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Shen", "Ke", ""]]}, {"id": "2011.09176", "submitter": "Leif Sabellek", "authors": "Carsten Lutz, Johannes Marti, Leif Sabellek", "title": "Query Expressibility and Verification in Ontology-Based Data Access", "comments": null, "journal-ref": "Principles of Knowledge Representation and Reasoning: Proceedings\n  of the Sixteenth International Conference, KR 2018, Tempe, Arizona, 30\n  October - 2 November 2018, pages 389--398, AAAI Press, 2018", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access, multiple data sources are integrated using an\nontology and mappings. In practice, this is often achieved by a bootstrapping\nprocess, that is, the ontology and mappings are first designed to support only\nthe most important queries over the sources and then gradually extended to\nenable additional queries. In this paper, we study two reasoning problems that\nsupport such an approach. The expressibility problem asks whether a given\nsource query $q_s$ is expressible as a target query (that is, over the\nontology's vocabulary) and the verification problem asks, additionally given a\ncandidate target query $q_t$, whether $q_t$ expresses $q_s$. We consider (U)CQs\nas source and target queries and GAV mappings, showing that both problems are\n$\\Pi^p_2$-complete in DL-Lite, coNExpTime-complete between EL and ELHI when\nsource queries are rooted, and 2ExpTime-complete for unrestricted source\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 09:50:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lutz", "Carsten", ""], ["Marti", "Johannes", ""], ["Sabellek", "Leif", ""]]}, {"id": "2011.09192", "submitter": "Karl Tuyls", "authors": "Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome\n  Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd\n  Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton,\n  Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet\n  Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De\n  Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Remi Munos, Trevor Back,\n  Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore\n  Graepel, Demis Hassabis", "title": "Game Plan: What AI can do for Football, and What Football can do for AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress in artificial intelligence (AI) and machine learning has\nopened unprecedented analytics possibilities in various team and individual\nsports, including baseball, basketball, and tennis. More recently, AI\ntechniques have been applied to football, due to a huge increase in data\ncollection by professional teams, increased computational power, and advances\nin machine learning, with the goal of better addressing new scientific\nchallenges involved in the analysis of both individual players' and coordinated\nteams' behaviors. The research challenges associated with predictive and\nprescriptive football analytics require new developments and progress at the\nintersection of statistical learning, game theory, and computer vision. In this\npaper, we provide an overarching perspective highlighting how the combination\nof these fields, in particular, forms a unique microcosm for AI research, while\noffering mutual benefits for professional teams, spectators, and broadcasters\nin the years to come. We illustrate that this duality makes football analytics\na game changer of tremendous value, in terms of not only changing the game of\nfootball itself, but also in terms of what this domain can mean for the field\nof AI. We review the state-of-the-art and exemplify the types of analysis\nenabled by combining the aforementioned fields, including illustrative examples\nof counterfactual analysis using predictive models, and the combination of\ngame-theoretic analysis of penalty kicks with statistical learning of player\nattributes. We conclude by highlighting envisioned downstream impacts,\nincluding possibilities for extensions to other sports (real and virtual).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:26:02 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tuyls", "Karl", ""], ["Omidshafiei", "Shayegan", ""], ["Muller", "Paul", ""], ["Wang", "Zhe", ""], ["Connor", "Jerome", ""], ["Hennes", "Daniel", ""], ["Graham", "Ian", ""], ["Spearman", "William", ""], ["Waskett", "Tim", ""], ["Steele", "Dafydd", ""], ["Luc", "Pauline", ""], ["Recasens", "Adria", ""], ["Galashov", "Alexandre", ""], ["Thornton", "Gregory", ""], ["Elie", "Romuald", ""], ["Sprechmann", "Pablo", ""], ["Moreno", "Pol", ""], ["Cao", "Kris", ""], ["Garnelo", "Marta", ""], ["Dutta", "Praneet", ""], ["Valko", "Michal", ""], ["Heess", "Nicolas", ""], ["Bridgland", "Alex", ""], ["Perolat", "Julien", ""], ["De Vylder", "Bart", ""], ["Eslami", "Ali", ""], ["Rowland", "Mark", ""], ["Jaegle", "Andrew", ""], ["Munos", "Remi", ""], ["Back", "Trevor", ""], ["Ahamed", "Razia", ""], ["Bouton", "Simon", ""], ["Beauguerlange", "Nathalie", ""], ["Broshear", "Jackson", ""], ["Graepel", "Thore", ""], ["Hassabis", "Demis", ""]]}, {"id": "2011.09241", "submitter": "Francesco Salvetti", "authors": "Enrico Sutera, Vittorio Mazzia, Francesco Salvetti, Giovanni Fantin\n  and Marcello Chiaberge", "title": "Indoor Point-to-Point Navigation with Deep Reinforcement Learning and\n  Ultra-wideband", "comments": "Accepted by ICAART 2021 - http://www.icaart.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Indoor autonomous navigation requires a precise and accurate localization\nsystem able to guide robots through cluttered, unstructured and dynamic\nenvironments. Ultra-wideband (UWB) technology, as an indoor positioning system,\noffers precise localization and tracking, but moving obstacles and\nnon-line-of-sight occurrences can generate noisy and unreliable signals. That,\ncombined with sensors noise, unmodeled dynamics and environment changes can\nresult in a failure of the guidance algorithm of the robot. We demonstrate how\na power-efficient and low computational cost point-to-point local planner,\nlearnt with deep reinforcement learning (RL), combined with UWB localization\ntechnology can constitute a robust and resilient to noise short-range guidance\nsystem complete solution. We trained the RL agent on a simulated environment\nthat encapsulates the robot dynamics and task constraints and then, we tested\nthe learnt point-to-point navigation policies in a real setting with more than\ntwo-hundred experimental evaluations using UWB localization. Our results show\nthat the computational efficient end-to-end policy learnt in plain simulation,\nthat directly maps low-range sensors signals to robot controls, deployed in\ncombination with ultra-wideband noisy localization in a real environment, can\nprovide a robust, scalable and at-the-edge low-cost navigation system solution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:30:36 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sutera", "Enrico", ""], ["Mazzia", "Vittorio", ""], ["Salvetti", "Francesco", ""], ["Fantin", "Giovanni", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2011.09257", "submitter": "Pablo Pino", "authors": "Pablo Pino, Denis Parra, Pablo Messina, Cecilia Besa, Sergio Uribe", "title": "Inspecting state of the art performance and NLP metrics in image-based\n  medical report generation", "comments": "3 pages, 1 figure, 1 table. Accepted in LatinX in AI workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 13:09:12 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 17:58:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pino", "Pablo", ""], ["Parra", "Denis", ""], ["Messina", "Pablo", ""], ["Besa", "Cecilia", ""], ["Uribe", "Sergio", ""]]}, {"id": "2011.09264", "submitter": "Luis Haug", "authors": "Luis Haug, Ivan Ovinnikov, Eugene Bykovets", "title": "Inverse Reinforcement Learning via Matching of Optimality Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of inverse reinforcement learning (IRL) is to infer a reward\nfunction that explains the behavior of an agent performing a task. The\nassumption that most approaches make is that the demonstrated behavior is\nnear-optimal. In many real-world scenarios, however, examples of truly optimal\nbehavior are scarce, and it is desirable to effectively leverage sets of\ndemonstrations of suboptimal or heterogeneous performance, which are easier to\nobtain. We propose an algorithm that learns a reward function from such\ndemonstrations together with a weak supervision signal in the form of a\ndistribution over rewards collected during the demonstrations (or, more\ngenerally, a distribution over cumulative discounted future rewards). We view\nsuch distributions, which we also refer to as optimality profiles, as summaries\nof the degree of optimality of the demonstrations that may, for example,\nreflect the opinion of a human expert. Given an optimality profile and a small\namount of additional supervision, our algorithm fits a reward function, modeled\nas a neural network, by essentially minimizing the Wasserstein distance between\nthe corresponding induced distribution and the optimality profile. We show that\nour method is capable of learning reward functions such that policies trained\nto optimize them outperform the demonstrations used for fitting the reward\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 13:23:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:55:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Haug", "Luis", ""], ["Ovinnikov", "Ivan", ""], ["Bykovets", "Eugene", ""]]}, {"id": "2011.09285", "submitter": "Reza Fotohi", "authors": "Maryam Faraji-Biregani and Reza Fotohi", "title": "Secure communication between UAVs using a method based on smart agents\n  in unmanned aerial vehicles", "comments": "25 pages, 10 figures, 14 tables, JCR (Q2). J Supercomput (2020)", "journal-ref": null, "doi": "10.1007/s11227-020-03462-0", "report-no": null, "categories": "cs.CR cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) can be deployed to monitor very large areas\nwithout the need for network infrastructure. UAVs communicate with each other\nduring flight and exchange information with each other. However, such\ncommunication poses security challenges due to its dynamic topology. To solve\nthese challenges, the proposed method uses two phases to counter malicious UAV\nattacks. In the first phase, we applied a number of rules and principles to\ndetect malicious UAVs. In this phase, we try to identify and remove malicious\nUAVs according to the behavior of UAVs in the network in order to prevent\nsending fake information to the investigating UAVs. In the second phase, a\nmobile agent based on a three-step negotiation process is used to eliminate\nmalicious UAVs. In this way, we use mobile agents to inform our normal neighbor\nUAVs so that they do not listen to the data generated by the malicious UAVs.\nTherefore, the mobile agent of each UAV uses reliable neighbors through a\nthree-step negotiation process so that they do not listen to the traffic\ngenerated by the malicious UAVs. The NS-3 simulator was used to demonstrate the\nefficiency of the SAUAV method. The proposed method is more efficient than\nCST-UAS, CS-AVN, HVCR, and BSUM-based methods in detection rate, false positive\nrate, false negative rate, packet delivery rate, and residual energy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:33:39 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Faraji-Biregani", "Maryam", ""], ["Fotohi", "Reza", ""]]}, {"id": "2011.09294", "submitter": "Tom Ward", "authors": "Tom Ward, Andrew Bolt, Nik Hemmings, Simon Carter, Manuel Sanchez,\n  Ricardo Barreira, Seb Noury, Keith Anderson, Jay Lemmon, Jonathan Coe, Piotr\n  Trochim, Tom Handley, Adrian Bolton", "title": "Using Unity to Help Solve Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the pursuit of artificial general intelligence, our most significant\nmeasurement of progress is an agent's ability to achieve goals in a wide range\nof environments. Existing platforms for constructing such environments are\ntypically constrained by the technologies they are founded on, and are\ntherefore only able to provide a subset of scenarios necessary to evaluate\nprogress. To overcome these shortcomings, we present our use of Unity, a widely\nrecognized and comprehensive game engine, to create more diverse, complex,\nvirtual simulations. We describe the concepts and components developed to\nsimplify the authoring of these environments, intended for use predominantly in\nthe field of reinforcement learning. We also introduce a practical approach to\npackaging and re-distributing environments in a way that attempts to improve\nthe robustness and reproducibility of experiment results. To illustrate the\nversatility of our use of Unity compared to other solutions, we highlight\nenvironments already created using our approach from published papers. We hope\nthat others can draw inspiration from how we adapted Unity to our needs, and\nanticipate increasingly varied and complex environments to emerge from our\napproach as familiarity grows.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:04:01 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ward", "Tom", ""], ["Bolt", "Andrew", ""], ["Hemmings", "Nik", ""], ["Carter", "Simon", ""], ["Sanchez", "Manuel", ""], ["Barreira", "Ricardo", ""], ["Noury", "Seb", ""], ["Anderson", "Keith", ""], ["Lemmon", "Jay", ""], ["Coe", "Jonathan", ""], ["Trochim", "Piotr", ""], ["Handley", "Tom", ""], ["Bolton", "Adrian", ""]]}, {"id": "2011.09314", "submitter": "Carsten Lutz", "authors": "Pablo Barcelo, Gerald Berger, Carsten Lutz, Andreas Pieris", "title": "First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded\nexistential rules and (unions of) conjunctive queries, and we investigate the\nproblem of FO-rewritability, i.e., whether an OMQ can be rewritten as a\nfirst-order query. We adopt two different approaches. The first approach\nemploys standard two-way alternating parity tree automata. Although it does not\nlead to a tight complexity bound, it provides a transparent solution based on\nwidely known tools. The second approach relies on a sophisticated automata\nmodel, known as cost automata. This allows us to show that our problem is\n2ExpTime-complete. In both approaches, we provide semantic characterizations of\nFO-rewritability that are of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:31:17 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Barcelo", "Pablo", ""], ["Berger", "Gerald", ""], ["Lutz", "Carsten", ""], ["Pieris", "Andreas", ""]]}, {"id": "2011.09351", "submitter": "Uwe Aickelin", "authors": "Chaofan Tu, Ruibin Bai, Zheng Lu, Uwe Aickelin, Peiming Ge, Jianshuang\n  Zhao", "title": "Learning Regular Expressions for Interpretable Medical Text\n  Classification Using a Pool-based Simulated Annealing and Word-vector Models", "comments": "9th Multidisciplinary International Conference on Scheduling : Theory\n  and Applications (MISTA 2019) 12-15 December 2019, Ningbo, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a rule-based engine composed of high quality and\ninterpretable regular expressions for medical text classification. The regular\nexpressions are auto generated by a constructive heuristic method and optimized\nusing a Pool-based Simulated Annealing (PSA) approach. Although existing Deep\nNeural Network (DNN) methods present high quality performance in most Natural\nLanguage Processing (NLP) applications, the solutions are regarded as\nuninterpretable black boxes to humans. Therefore, rule-based methods are often\nintroduced when interpretable solutions are needed, especially in the medical\nfield. However, the construction of regular expressions can be extremely\nlabor-intensive for large data sets. This research aims to reduce the manual\nefforts while maintaining high-quality solutions\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:20:02 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tu", "Chaofan", ""], ["Bai", "Ruibin", ""], ["Lu", "Zheng", ""], ["Aickelin", "Uwe", ""], ["Ge", "Peiming", ""], ["Zhao", "Jianshuang", ""]]}, {"id": "2011.09353", "submitter": "Mihai Codescu", "authors": "Bernd Krieg-Br\\\"uckner and Till Mossakowski and Mihai Codescu", "title": "Generic Ontology Design Patterns: Roles and Change over Time", "comments": "To appear in Advances in Pattern-based Ontology Engineering. Studies\n  on the Semantic Web, IOS Press, Amsterdam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this chapter we propose Generic Ontology Design Patterns, GODPs, as a\nmethodology for representing and instantiating ontology design patterns in a\nway that is adaptable, and allows domain experts (and other users) to safely\nuse them without cluttering their ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:40:13 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Krieg-Br\u00fcckner", "Bernd", ""], ["Mossakowski", "Till", ""], ["Codescu", "Mihai", ""]]}, {"id": "2011.09369", "submitter": "Feng Gao", "authors": "Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, Huazhong Yang", "title": "Attentional Separation-and-Aggregation Network for Self-supervised\n  Depth-Pose Learning in Dynamic Scenes", "comments": "accepted by CoRL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning depth and ego-motion from unlabeled videos via self-supervision from\nepipolar projection can improve the robustness and accuracy of the 3D\nperception and localization of vision-based robots. However, the rigid\nprojection computed by ego-motion cannot represent all scene points, such as\npoints on moving objects, leading to false guidance in these regions. To\naddress this problem, we propose an Attentional Separation-and-Aggregation\nNetwork (ASANet), which can learn to distinguish and extract the scene's static\nand dynamic characteristics via the attention mechanism. We further propose a\nnovel MotionNet with an ASANet as the encoder, followed by two separate\ndecoders, to estimate the camera's ego-motion and the scene's dynamic motion\nfield. Then, we introduce an auto-selecting approach to detect the moving\nobjects for dynamic-aware learning automatically. Empirical experiments\ndemonstrate that our method can achieve the state-of-the-art performance on the\nKITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:07:30 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gao", "Feng", ""], ["Yu", "Jincheng", ""], ["Shen", "Hao", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2011.09370", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Negash Begashaw", "title": "Cycle-to-Cycle Queue Length Estimation from Connected Vehicles with\n  Filtering on Primary Parameters", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.RO cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation models from connected vehicles often assume low level parameters\nsuch as arrival rates and market penetration rates as known or estimate them in\nreal-time. At low market penetration rates, such parameter estimators produce\nlarge errors making estimated queue lengths inefficient for control or\noperations applications. In order to improve accuracy of low level parameter\nestimations, this study investigates the impact of connected vehicles\ninformation filtering on queue length estimation models. Filters are used as\nmultilevel real-time estimators. Accuracy is tested against known arrival rate\nand market penetration rate scenarios using microsimulations. To understand the\neffectiveness for short-term or for dynamic processes, arrival rates, and\nmarket penetration rates are changed every 15 minutes. The results show that\nwith Kalman and Particle filters, parameter estimators are able to find the\ntrue values within 15 minutes and meet and surpass the accuracy of known\nparameter scenarios especially for low market penetration rates. In addition,\nusing last known estimated queue lengths when no connected vehicle is present\nperforms better than inputting average estimated values. Moreover, the study\nshows that both filtering algorithms are suitable for real-time applications\nthat require less than 0.1 second computational time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:09:45 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Begashaw", "Negash", ""]]}, {"id": "2011.09384", "submitter": "Dan Feldman PhD", "authors": "Dan Feldman", "title": "Introduction to Core-sets: an Updated Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In optimization or machine learning problems we are given a set of items,\nusually points in some metric space, and the goal is to minimize or maximize an\nobjective function over some space of candidate solutions. For example, in\nclustering problems, the input is a set of points in some metric space, and a\ncommon goal is to compute a set of centers in some other space (points, lines)\nthat will minimize the sum of distances to these points. In database queries,\nwe may need to compute such a some for a specific query set of $k$ centers.\n  However, traditional algorithms cannot handle modern systems that require\nparallel real-time computations of infinite distributed streams from sensors\nsuch as GPS, audio or video that arrive to a cloud, or networks of weaker\ndevices such as smartphones or robots.\n  Core-set is a \"small data\" summarization of the input \"big data\", where every\npossible query has approximately the same answer on both data sets. Generic\ntechniques enable efficient coreset \\changed{maintenance} of streaming,\ndistributed and dynamic data. Traditional algorithms can then be applied on\nthese coresets to maintain the approximated optimal solutions.\n  The challenge is to design coresets with provable tradeoff between their size\nand approximation error. This survey summarizes such constructions in a\nretrospective way, that aims to unified and simplify the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:31:34 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Feldman", "Dan", ""]]}, {"id": "2011.09393", "submitter": "Nurislam Tursynbek", "authors": "Nurislam Tursynbek, Ilya Vilkoviskiy, Maria Sindeeva, Ivan Oseledets", "title": "Adversarial Turing Patterns from Cellular Automata", "comments": "Published as a conference paper at AAAI 2021 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep classifiers are intriguingly vulnerable to universal\nadversarial perturbations: single disturbances of small magnitude that lead to\nmisclassification of most in-puts. This phenomena may potentially result in a\nserious security problem. Despite the extensive research in this area,there is\na lack of theoretical understanding of the structure of these perturbations. In\nimage domain, there is a certain visual similarity between patterns, that\nrepresent these perturbations, and classical Turing patterns, which appear as a\nsolution of non-linear partial differential equations and are underlying\nconcept of many processes in nature. In this paper,we provide a theoretical\nbridge between these two different theories, by mapping a simplified algorithm\nfor crafting universal perturbations to (inhomogeneous) cellular automata,the\nlatter is known to generate Turing patterns. Furthermore,we propose to use\nTuring patterns, generated by cellular automata, as universal perturbations,\nand experimentally show that they significantly degrade the performance of deep\nlearning models. We found this method to be a fast and efficient way to create\na data-agnostic quasi-imperceptible perturbation in the black-box scenario. The\nsource code is available at https://github.com/NurislamT/advTuring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:50:54 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 07:51:43 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 08:59:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Tursynbek", "Nurislam", ""], ["Vilkoviskiy", "Ilya", ""], ["Sindeeva", "Maria", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2011.09407", "submitter": "Devleena Das", "authors": "Devleena Das, Siddhartha Banerjee, Sonia Chernova", "title": "Explainable AI for System Failures: Generating Explanations that Improve\n  Human Assistance in Fault Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing capabilities of intelligent systems, the integration of\nartificial intelligence (AI) and robots in everyday life is increasing.\nHowever, when interacting in such complex human environments, the failure of\nintelligent systems, such as robots, can be inevitable, requiring recovery\nassistance from users. In this work, we develop automated, natural language\nexplanations for failures encountered during an AI agents' plan execution.\nThese explanations are developed with a focus of helping non-expert users\nunderstand different point of failures to better provide recovery assistance.\nSpecifically, we introduce a context-based information type for explanations\nthat can both help non-expert users understand the underlying cause of a system\nfailure, and select proper failure recoveries. Additionally, we extend an\nexisting sequence-to-sequence methodology to automatically generate our\ncontext-based explanations. By doing so, we are able develop a model that can\ngeneralize context-based explanations over both different failure types and\nfailure scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:08:50 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 13:35:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Das", "Devleena", ""], ["Banerjee", "Siddhartha", ""], ["Chernova", "Sonia", ""]]}, {"id": "2011.09410", "submitter": "Deokgun Park", "authors": "Deokgun Park", "title": "A Definition and a Test for Human-Level Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in many application-specific domains, we do not know\nhow to build a human-level artificial intelligence (HLAI). We conjecture that\nlearning from others' experience with the language is the essential\ncharacteristic that distinguishes human intelligence from the rest. Humans can\nupdate the action-value function with the verbal description as if they\nexperience states, actions, and corresponding rewards sequences firsthand. In\nthis paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:10:02 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 17:14:29 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 21:54:04 GMT"}, {"version": "v4", "created": "Sat, 17 Jul 2021 21:30:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Park", "Deokgun", ""]]}, {"id": "2011.09436", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Mashrur Chowdhury, David M. Nicol", "title": "Assessment of System-Level Cyber Attack Vulnerability for Connected and\n  Autonomous Vehicles Using Bayesian Networks", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a methodology to quantify vulnerability of cyber attacks\nand their impacts based on probabilistic graphical models for intelligent\ntransportation systems under connected and autonomous vehicles framework. Cyber\nattack vulnerabilities from various types and their impacts are calculated for\nintelligent signals and cooperative adaptive cruise control (CACC) applications\nbased on the selected performance measures. Numerical examples are given that\nshow impact of vulnerabilities in terms of average intersection queue lengths,\nnumber of stops, average speed, and delays. At a signalized network with and\nwithout redundant systems, vulnerability can increase average queues and delays\nby $3\\%$ and $15\\%$ and $4\\%$ and $17\\%$, respectively. For CACC application,\nimpact levels reach to $50\\%$ delay difference on average when low amount of\nspeed information is perturbed. When significantly different speed\ncharacteristics are inserted by an attacker, delay difference increases beyond\n$100\\%$ of normal traffic conditions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:13:57 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Chowdhury", "Mashrur", ""], ["Nicol", "David M.", ""]]}, {"id": "2011.09455", "submitter": "Ehsan Amiri", "authors": "Shahriar Sharifi Borojerdi, Mehdi Karimi, Ehsan Amiri", "title": "Investigation of Warrior Robots Behavior by Using Evolutionary\n  Algorithms", "comments": "3 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": "ICLCT'2012", "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this study, we review robots behavior especially warrior robots by using\nevolutionary algorithms. This kind of algorithms is inspired by nature that\ncauses robots behaviors get resemble to collective behavior. Collective\nbehavior of creatures such as bees was shown that do some functions which\ndepended on interaction and cooperation would need to a well-organized system\nso that all creatures within it carry out their duty, very well. For robots\nwhich do not have any intelligence, we can define an algorithm and show the\nresults by a simple simulation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:31:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Borojerdi", "Shahriar Sharifi", ""], ["Karimi", "Mehdi", ""], ["Amiri", "Ehsan", ""]]}, {"id": "2011.09469", "submitter": "Gurcan Comert", "authors": "Gurcan Comert, Negash Begashaw, Nathan Huynh", "title": "Improved Grey System Models for Predicting Traffic Parameters", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In transportation applications such as real-time route guidance, ramp\nmetering, congestion pricing and special events traffic management, accurate\nshort-term traffic flow prediction is needed. For this purpose, this paper\nproposes several novel \\textit{online} Grey system models (GM):\nGM(1,1$|cos(\\omega t)$), GM(1,1$|sin(\\omega t)$, $cos(\\omega t)$), and\nGM(1,1$|e^{-at}$,$sin(\\omega t)$,$cos(\\omega t)$). To evaluate the performance\nof the proposed models, they are compared against a set of benchmark models:\nGM(1,1) model, Grey Verhulst models with and without Fourier error corrections,\nlinear time series model, and nonlinear time series model. The evaluation is\nperformed using loop detector and probe vehicle data from California, Virginia,\nand Oregon. Among the benchmark models, the error corrected Grey Verhulst model\nwith Fourier outperformed the GM(1,1) model, linear time series, and non-linear\ntime series models. In turn, the three proposed models, GM(1,1$|cos(\\omega\nt)$), GM(1,1$|sin(\\omega t)$,$cos(\\omega t)$), and GM(1,1$|e^{-at}$,$sin(\\omega\nt)$,$cos(\\omega t)$), outperformed the Grey Verhulst model in prediction by at\nleast $65\\%$, $16\\%$, and $11\\%$, in terms of Root Mean Squared Error, and by\n$82\\%$, $58\\%$, and $42\\%$, in terms of Mean Absolute Percentage Error,\nrespectively. It is observed that the proposed Grey system models are more\nadaptive to location (e.g., perform well for all roadway types) and traffic\nparameters (e.g., speed, travel time, occupancy, and volume), and they do not\nrequire as many data points for training (4 observations are found to be\nsufficient).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:55:03 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Comert", "Gurcan", ""], ["Begashaw", "Negash", ""], ["Huynh", "Nathan", ""]]}, {"id": "2011.09471", "submitter": "Leslie Smith", "authors": "Helena E. Liu and Leslie N. Smith", "title": "FROST: Faster and more Robust One-shot Semi-supervised Training", "comments": "Withdrawn because the results reported were due to an error in our\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in one-shot semi-supervised learning have lowered the barrier\nfor deep learning of new applications. However, the state-of-the-art for\nsemi-supervised learning is slow to train and the performance is sensitive to\nthe choices of the labeled data and hyper-parameter values. In this paper, we\npresent a one-shot semi-supervised learning method that trains up to an order\nof magnitude faster and is more robust than state-of-the-art methods.\nSpecifically, we show that by combining semi-supervised learning with a\none-stage, single network version of self-training, our FROST methodology\ntrains faster and is more robust to choices for the labeled samples and changes\nin hyper-parameters. Our experiments demonstrate FROST's capability to perform\nwell when the composition of the unlabeled data is unknown; that is when the\nunlabeled data contain unequal numbers of each class and can contain\nout-of-distribution examples that don't belong to any of the training classes.\nHigh performance, speed of training, and insensitivity to hyper-parameters make\nFROST the most practical method for one-shot semi-supervised training. Our code\nis available at https://github.com/HelenaELiu/FROST.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:56:03 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 11:29:58 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 23:45:56 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 14:04:18 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liu", "Helena E.", ""], ["Smith", "Leslie N.", ""]]}, {"id": "2011.09517", "submitter": "Tanveer Syeda-Mahmood", "authors": "Tanveer Syeda-Mahmood, Ph.D, K.C.L Wong, Ph.D, Joy T. Wu, M.D., M.P.H,\n  Ashutosh Jadhav, Ph.D, Orest Boyko, M.D. Ph.D", "title": "Extracting and Learning Fine-Grained Labels from Chest Radiographs", "comments": "This paper won the Homer R. Warner Award at AMIA 2020 awarded to a\n  paper that best describes approaches to improving computerized information\n  acquisition, knowledge data acquisition and management, and experimental\n  results documenting the value of these approaches. The paper shows a\n  combination of textual and visual processing to automatically recognize\n  complex findings in chest X-rays", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are the most common diagnostic exam in emergency rooms and\nintensive care units today. Recently, a number of researchers have begun\nworking on large chest X-ray datasets to develop deep learning models for\nrecognition of a handful of coarse finding classes such as opacities, masses\nand nodules. In this paper, we focus on extracting and learning fine-grained\nlabels for chest X-ray images. Specifically we develop a new method of\nextracting fine-grained labels from radiology reports by combining\nvocabulary-driven concept extraction with phrasal grouping in dependency parse\ntrees for association of modifiers with findings. A total of 457 fine-grained\nlabels depicting the largest spectrum of findings to date were selected and\nsufficiently large datasets acquired to train a new deep learning model\ndesigned for fine-grained classification. We show results that indicate a\nhighly accurate label extraction process and a reliable learning of\nfine-grained labels. The resulting network, to our knowledge, is the first to\nrecognize fine-grained descriptions of findings in images covering over nine\nmodifiers including laterality, location, severity, size and appearance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 19:56:08 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Syeda-Mahmood", "Tanveer", ""], ["D", "Ph.", ""], ["Wong", "K. C. L", ""], ["D", "Ph.", ""], ["Wu", "Joy T.", ""], ["D.", "M.", ""], ["H", "M. P.", ""], ["Jadhav", "Ashutosh", ""], ["D", "Ph.", ""], ["Boyko", "Orest", ""], ["D", "M. D. Ph.", ""]]}, {"id": "2011.09530", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Hamid Palangi, Jianwei Yang, Sudha Rao, Asli\n  Celikyilmaz, Roland Fernandez, Paul Smolensky, Jianfeng Gao, Shih-Fu Chang", "title": "Neuro-Symbolic Representations for Video Captioning: A Case for\n  Leveraging Inductive Biases for Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuro-symbolic representations have proved effective in learning structure\ninformation in vision and language. In this paper, we propose a new model\narchitecture for learning multi-modal neuro-symbolic representations for video\ncaptioning. Our approach uses a dictionary learning-based method of learning\nrelations between videos and their paired text descriptions. We refer to these\nrelations as relative roles and leverage them to make each token role-aware\nusing attention. This results in a more structured and interpretable\narchitecture that incorporates modality-specific inductive biases for the\ncaptioning task. Intuitively, the model is able to learn spatial, temporal, and\ncross-modal relations in a given pair of video and text. The disentanglement\nachieved by our proposal gives the model more capacity to capture multi-modal\nstructures which result in captions with higher quality for videos. Our\nexperiments on two established video captioning datasets verifies the\neffectiveness of the proposed approach based on automatic metrics. We further\nconduct a human evaluation to measure the grounding and relevance of the\ngenerated captions and observe consistent improvement for the proposed model.\nThe codes and trained models can be found at\nhttps://github.com/hassanhub/R3Transformer\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:21:19 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Akbari", "Hassan", ""], ["Palangi", "Hamid", ""], ["Yang", "Jianwei", ""], ["Rao", "Sudha", ""], ["Celikyilmaz", "Asli", ""], ["Fernandez", "Roland", ""], ["Smolensky", "Paul", ""], ["Gao", "Jianfeng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2011.09533", "submitter": "Christian Schroeder de Witt", "authors": "Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor\n  Makoviychuk, Philip H.S. Torr, Mingfei Sun, Shimon Whiteson", "title": "Is Independent Learning All You Need in the StarCraft Multi-Agent\n  Challenge?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most recently developed approaches to cooperative multi-agent reinforcement\nlearning in the \\emph{centralized training with decentralized execution}\nsetting involve estimating a centralized, joint value function. In this paper,\nwe demonstrate that, despite its various theoretical shortcomings, Independent\nPPO (IPPO), a form of independent learning in which each agent simply estimates\nits local value function, can perform just as well as or better than\nstate-of-the-art joint learning approaches on popular multi-agent benchmark\nsuite SMAC with little hyperparameter tuning. We also compare IPPO to several\nvariants; the results suggest that IPPO's strong performance may be due to its\nrobustness to some forms of environment non-stationarity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:29:59 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["de Witt", "Christian Schroeder", ""], ["Gupta", "Tarun", ""], ["Makoviichuk", "Denys", ""], ["Makoviychuk", "Viktor", ""], ["Torr", "Philip H. S.", ""], ["Sun", "Mingfei", ""], ["Whiteson", "Shimon", ""]]}, {"id": "2011.09553", "submitter": "Yue Feng", "authors": "Yue Feng, Yang Wang, Hang Li", "title": "A Sequence-to-Sequence Approach to Dialogue State Tracking", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with dialogue state tracking (DST) in a task-oriented\ndialogue system. Building a DST module that is highly effective is still a\nchallenging issue, although significant progresses have been made recently.\nThis paper proposes a new approach to dialogue state tracking, referred to as\nSeq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU\nemploys two BERT-based encoders to respectively encode the utterances in the\ndialogue and the descriptions of schemas, an attender to calculate attentions\nbetween the utterance embeddings and the schema embeddings, and a decoder to\ngenerate pointers to represent the current state of dialogue. Seq2Seq-DU has\nthe following advantages. It can jointly model intents, slots, and slot values;\nit can leverage the rich representations of utterances and schemas based on\nBERT; it can effectively deal with categorical and non-categorical slots, and\nunseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural\nlanguage understanding) module of a dialogue system. Experimental results on\nbenchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1,\nWOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:42:44 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 02:48:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Feng", "Yue", ""], ["Wang", "Yang", ""], ["Li", "Hang", ""]]}, {"id": "2011.09554", "submitter": "Giuseppe Fenza", "authors": "Giuseppe Fenza, Mariacristina Gallo, Vincenzo Loia, Domenico Marino,\n  Francesco Orciuoli", "title": "A Cognitive Approach based on the Actionable Knowledge Graph for\n  supporting Maintenance Operations", "comments": null, "journal-ref": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems\n  (EAIS), Bari, Italy, 2020, pp. 1-7", "doi": "10.1109/EAIS48028.2020.9122759", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the era of Industry 4.0, cognitive computing and its enabling technologies\n(Artificial Intelligence, Machine Learning, etc.) allow to define systems able\nto support maintenance by providing relevant information, at the right time,\nretrieved from structured companies' databases, and unstructured documents,\nlike technical manuals, intervention reports, and so on. Moreover, contextual\ninformation plays a crucial role in tailoring the support both during the\nplanning and the execution of interventions. Contextual information can be\ndetected with the help of sensors, wearable devices, indoor and outdoor\npositioning systems, and object recognition capabilities (using fixed or\nwearable cameras), all of which can collect historical data for further\nanalysis. In this work, we propose a cognitive system that learns from past\ninterventions to generate contextual recommendations for improving maintenance\npractices in terms of time, budget, and scope. The system uses formal\nconceptual models, incremental learning, and ranking algorithms to accomplish\nthese objectives.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:53:00 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Fenza", "Giuseppe", ""], ["Gallo", "Mariacristina", ""], ["Loia", "Vincenzo", ""], ["Marino", "Domenico", ""], ["Orciuoli", "Francesco", ""]]}, {"id": "2011.09580", "submitter": "Luis Miguel Vaquero Gonzalez", "authors": "Kentaro Takiguchi, Niall Twomey, Luis M. Vaquero", "title": "Non-Linear Multiple Field Interactions Neural Document Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ranking tasks are usually based on the text of the main body of the page and\nthe actions (clicks) of users on the page. There are other elements that could\nbe leveraged to better contextualise the ranking experience (e.g. text in other\nfields, query made by the user, images, etc). We present one of the first\nin-depth analyses of field interaction for multiple field ranking in two\nseparate datasets. While some works have taken advantage of full document\nstructure, some aspects remain unexplored. In this work we build on previous\nanalyses to show how query-field interactions, non-linear field interactions,\nand the architecture of the underlying neural model affect performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 23:13:33 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Takiguchi", "Kentaro", ""], ["Twomey", "Niall", ""], ["Vaquero", "Luis M.", ""]]}, {"id": "2011.09608", "submitter": "Soopil Kim", "authors": "Soopil Kim, Sion An, Philip Chikontwe, Sang Hyun Park", "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image\n  Segmentation", "comments": "Submitted to AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of organs of interest in 3D medical images is necessary for\naccurate diagnosis and longitudinal studies. Though recent advances using deep\nlearning have shown success for many segmentation tasks, large datasets are\nrequired for high performance and the annotation process is both time consuming\nand labor intensive. In this paper, we propose a 3D few shot segmentation\nframework for accurate organ segmentation using limited training samples of the\ntarget organ annotation. To achieve this, a U-Net like network is designed to\npredict segmentation by learning the relationship between 2D slices of support\ndata and a query image, including a bidirectional gated recurrent unit (GRU)\nthat learns consistency of encoded features between adjacent slices. Also, we\nintroduce a transfer learning method to adapt the characteristics of the target\nimage and organ by updating the model before testing with arbitrary support and\nquery data sampled from the support data. We evaluate our proposed model using\nthree 3D CT datasets with annotations of different organs. Our model yielded\nsignificantly improved performance over state-of-the-art few shot segmentation\nmodels and was comparable to a fully supervised model trained with more target\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 01:44:55 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kim", "Soopil", ""], ["An", "Sion", ""], ["Chikontwe", "Philip", ""], ["Park", "Sang Hyun", ""]]}, {"id": "2011.09625", "submitter": "John Chen", "authors": "John Chen, Ian Berlot-Attwell, Safwan Hossain, Xindi Wang and Frank\n  Rudzicz", "title": "Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal\n  Clinical NLP", "comments": "Best paper award at 3rd Clinical Natural Language Processing Workshop\n  at EMNLP 2020", "journal-ref": "Proceedings of the 3rd Clinical Natural Language Processing\n  Workshop (2020), pages 301--312", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical machine learning is increasingly multimodal, collected in both\nstructured tabular formats and unstructured forms such as freetext. We propose\na novel task of exploring fairness on a multimodal clinical dataset, adopting\nequalized odds for the downstream medical prediction tasks. To this end, we\ninvestigate a modality-agnostic fairness algorithm - equalized odds post\nprocessing - and compare it to a text-specific fairness algorithm: debiased\nclinical word embeddings. Despite the fact that debiased word embeddings do not\nexplicitly address equalized odds of protected groups, we show that a\ntext-specific approach to fairness may simultaneously achieve a good balance of\nperformance and classical notions of fairness. We hope that our paper inspires\nfuture contributions at the critical intersection of clinical NLP and fairness.\nThe full source code is available here:\nhttps://github.com/johntiger1/multimodal_fairness\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 03:11:24 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:54:39 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chen", "John", ""], ["Berlot-Attwell", "Ian", ""], ["Hossain", "Safwan", ""], ["Wang", "Xindi", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2011.09643", "submitter": "Wei Jin", "authors": "Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu and Jiliang Tang", "title": "Node Similarity Preserving Graph Convolutional Networks", "comments": "WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have achieved tremendous success in various\nreal-world applications due to their strong ability in graph representation\nlearning. GNNs explore the graph structure and node features by aggregating and\ntransforming information within node neighborhoods. However, through\ntheoretical and empirical analysis, we reveal that the aggregation process of\nGNNs tends to destroy node similarity in the original feature space. There are\nmany scenarios where node similarity plays a crucial role. Thus, it has\nmotivated the proposed framework SimP-GCN that can effectively and efficiently\npreserve node similarity while exploiting graph structure. Specifically, to\nbalance information from graph structure and node features, we propose a\nfeature similarity preserving aggregation which adaptively integrates graph\nstructure and node features. Furthermore, we employ self-supervised learning to\nexplicitly capture the complex feature similarity and dissimilarity relations\nbetween nodes. We validate the effectiveness of SimP-GCN on seven benchmark\ndatasets including three assortative and four disassorative graphs. The results\ndemonstrate that SimP-GCN outperforms representative baselines. Further probe\nshows various advantages of the proposed framework. The implementation of\nSimP-GCN is available at \\url{https://github.com/ChandlerBang/SimP-GCN}.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:18:01 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 10:48:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jin", "Wei", ""], ["Derr", "Tyler", ""], ["Wang", "Yiqi", ""], ["Ma", "Yao", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "2011.09644", "submitter": "Karthik Valmeekam", "authors": "Karthik Valmeekam, Sarath Sreedharan, Sailik Sengupta, Subbarao\n  Kambhampati", "title": "RADAR-X: An Interactive Interface Pairing Contrastive Explanations with\n  Revised Plan Suggestions", "comments": "Accepted at ICAPS workshop on Explainable AI and Planning (XAIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowering decision support systems with automated planning has received\nsignificant recognition in the planning community. The central idea for such\nsystems is to augment the capabilities of the human-in-the-loop with automated\nplanning techniques and provide timely support to enhance the decision-making\nexperience. In addition to this, an effective decision support system must be\nable to provide intuitive explanations based on specific queries on proposed\ndecisions to its end users. This makes decision-support systems an ideal\ntest-bed to study the effectiveness of various XAIP techniques being developed\nin the community. To this end, we present our decision support system RADAR-X\nthat extends RADAR (Grover et al. 2020) by allowing the user to participate in\nan interactive explanatory dialogue with the system. Specifically, we allow the\nuser to ask for contrastive explanations, wherein the user can try to\nunderstand why a specific plan was chosen over an alternative (referred to as\nthe foil). Furthermore, we use the foil raised as evidence for unspecified user\npreferences and use it to further refine plan suggestions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:18:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Valmeekam", "Karthik", ""], ["Sreedharan", "Sarath", ""], ["Sengupta", "Sailik", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2011.09670", "submitter": "Xue Yang", "authors": "Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, Junchi Yan", "title": "Dense Label Encoding for Boundary Discontinuity Free Rotation Detection", "comments": "12 pages, 6 figures, 9 tables, accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation detection serves as a fundamental building block in many visual\napplications involving aerial image, scene text, and face etc. Differing from\nthe dominant regression-based approaches for orientation estimation, this paper\nexplores a relatively less-studied methodology based on classification. The\nhope is to inherently dismiss the boundary discontinuity issue as encountered\nby the regression-based detectors. We propose new techniques to push its\nfrontier in two aspects: i) new encoding mechanism: the design of two Densely\nCoded Labels (DCL) for angle classification, to replace the Sparsely Coded\nLabel (SCL) in existing classification-based detectors, leading to three times\ntraining speed increase as empirically observed across benchmarks, further with\nnotable improvement in detection accuracy; ii) loss re-weighting: we propose\nAngle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves\nthe detection accuracy especially for square-like objects, by making DCL-based\ndetectors sensitive to angular distance and object's aspect ratio. Extensive\nexperiments and visual analysis on large-scale public datasets for aerial\nimages i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015\nand MLT, show the effectiveness of our approach. The source code is available\nat https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also\nintegrated in our open source rotation detection benchmark:\nhttps://github.com/yangxue0827/RotationDetection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 05:42:02 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 03:31:54 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 02:50:43 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 08:54:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yang", "Xue", ""], ["Hou", "Liping", ""], ["Zhou", "Yue", ""], ["Wang", "Wentao", ""], ["Yan", "Junchi", ""]]}, {"id": "2011.09671", "submitter": "Qiang Shen", "authors": "Qiang Shen and Stefano Teso and Wanyi Zhang and Hao Xu and Fausto\n  Giunchiglia", "title": "Multi-Modal Subjective Context Modelling and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications like personal assistants need to be aware ofthe user's context,\ne.g., where they are, what they are doing, and with whom. Context information\nis usually inferred from sensor data, like GPS sensors and accelerometers on\nthe user's smartphone. This prediction task is known as context recognition. A\nwell-defined context model is fundamental for successful recognition. Existing\nmodels, however, have two major limitations. First, they focus on few aspects,\nlike location or activity, meaning that recognition methods based onthem can\nonly compute and leverage few inter-aspect correlations. Second, existing\nmodels typically assume that context is objective, whereas in most applications\ncontext is best viewed from the user's perspective. Neglecting these factors\nlimits the usefulness of the context model and hinders recognition. We present\na novel ontological context model that captures five dimensions, namely time,\nlocation, activity, social relations and object. Moreover, our model defines\nthree levels of description(objective context, machine context and subjective\ncontext) that naturally support subjective annotations and reasoning.An initial\ncontext recognition experiment on real-world data hints at the promise of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 05:42:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Shen", "Qiang", ""], ["Teso", "Stefano", ""], ["Zhang", "Wanyi", ""], ["Xu", "Hao", ""], ["Giunchiglia", "Fausto", ""]]}, {"id": "2011.09705", "submitter": "Rebecca Eifler", "authors": "Rebecca Eifler and J\\\"org Hoffmann", "title": "Iterative Planning with Plan-Space Explanations: A Tool and User Study", "comments": "Proceedings of the International Workshop of Explainable AI Planning\n  (XAIP'20), at ICAPS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a variety of application settings, the user preference for a planning task\n- the precise optimization objective - is difficult to elicit. One possible\nremedy is planning as an iterative process, allowing the user to iteratively\nrefine and modify example plans. A key step to support such a process are\nexplanations, answering user questions about the current plan. In particular, a\nrelevant kind of question is \"Why does the plan you suggest not satisfy $p$?\",\nwhere p is a plan property desirable to the user. Note that such a question\npertains to plan space, i.e., the set of possible alternative plans. Adopting\nthe recent approach to answer such questions in terms of plan-property\ndependencies, here we implement a tool and user interface for human-guided\niterative planning including plan-space explanations. The tool runs in standard\nWeb browsers, and provides simple user interfaces for both developers and\nusers. We conduct a first user study, whose outcome indicates the usefulness of\nplan-property dependency explanations in iterative planning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:15:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Eifler", "Rebecca", ""], ["Hoffmann", "J\u00f6rg", ""]]}, {"id": "2011.09719", "submitter": "Chawin Sitawarin", "authors": "Chawin Sitawarin, Evgenios M. Kornaropoulos, Dawn Song, David Wagner", "title": "Adversarial Examples for $k$-Nearest Neighbor Classifiers Based on\n  Higher-Order Voronoi Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples are a widely studied phenomenon in machine learning\nmodels. While most of the attention has been focused on neural networks, other\npractical models also suffer from this issue. In this work, we propose an\nalgorithm for evaluating the adversarial robustness of $k$-nearest neighbor\nclassification, i.e., finding a minimum-norm adversarial example. Diverging\nfrom previous proposals, we take a geometric approach by performing a search\nthat expands outwards from a given input point. On a high level, the search\nradius expands to the nearby Voronoi cells until we find a cell that classifies\ndifferently from the input point. To scale the algorithm to a large $k$, we\nintroduce approximation steps that find perturbations with smaller norm,\ncompared to the baselines, in a variety of datasets. Furthermore, we analyze\nthe structural properties of a dataset where our approach outperforms the\ncompetition.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:49:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Sitawarin", "Chawin", ""], ["Kornaropoulos", "Evgenios M.", ""], ["Song", "Dawn", ""], ["Wagner", "David", ""]]}, {"id": "2011.09722", "submitter": "Yuri Lavinas Mr", "authors": "Felipe Vaz, Yuri Lavinas, Claus Aranha and Marcelo Ladeira", "title": "Exploring Constraint Handling Techniques in Real-world Problems on\n  MOEA/D with Limited Budget of Evaluations", "comments": "Final version will be submitted to EMO-2021. This is only a preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding good solutions for Multi-objective Optimization (MOPs) Problems is\nconsidered a hard problem, especially when considering MOPs with constraints.\nThus, most of the works in the context of MOPs do not explore in-depth how\ndifferent constraints affect the performance of MOP solvers. Here, we focus on\nexploring the effects of different Constraint Handling Techniques (CHTs) on\nMOEA/D, a commonly used MOP solver when solving complex real-world MOPs.\nMoreover, we introduce a simple and effective CHT focusing on the exploration\nof the decision space, the Three Stage Penalty. We explore each of these CHTs\nin MOEA/D on two simulated MOPs and six analytic MOPs (eight in total). The\nresults of this work indicate that while the best CHT is problem-dependent, our\nnew proposed Three Stage Penalty achieves competitive results and remarkable\nperformance in terms of hypervolume values in the hard simulated car design\nMOP.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:51:53 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Vaz", "Felipe", ""], ["Lavinas", "Yuri", ""], ["Aranha", "Claus", ""], ["Ladeira", "Marcelo", ""]]}, {"id": "2011.09741", "submitter": "Rodrigo de Medrano", "authors": "Rodrigo de Medrano, V\\'ictor de Buen Remiro, Jos\\'e L. Aznarte", "title": "SOCAIRE: Forecasting and Monitoring Urban Air Quality in Madrid", "comments": "26 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Air quality has become one of the main issues in public health and urban\nplanning management, due to the proven adverse effects of high pollutant\nconcentrations. Considering the mitigation measures that cities all over the\nworld are taking in order to face frequent low air quality episodes, the\ncapability of foreseeing future pollutant concentrations is of great\nimportance. Through this paper, we present SOCAIRE, an operational tool based\non a Bayesian and spatiotemporal ensemble of neural and statistical nested\nmodels. SOCAIRE integrates endogenous and exogenous information in order to\npredict and monitor future distributions of the concentration for several\npollutants in the city of Madrid. It focuses on modeling each and every\navailable component which might play a role in air quality: past concentrations\nof pollutants, human activity, numerical pollution estimation, and numerical\nweather predictions. This tool is currently in operation in Madrid, producing\ndaily air quality predictions for the next 48 hours and anticipating the\nprobability of the activation of the measures included in the city's official\nair quality \\no protocols through probabilistic inferences about compound\nevents.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:39:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["de Medrano", "Rodrigo", ""], ["Remiro", "V\u00edctor de Buen", ""], ["Aznarte", "Jos\u00e9 L.", ""]]}, {"id": "2011.09756", "submitter": "Corrado Pezzato", "authors": "Corrado Pezzato, Carlos Hernandez, Stefan Bonhof, Martijn Wisse", "title": "Active Inference and Behavior Trees for Reactive Action Planning and\n  Execution in Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid combination of active inference and behavior trees (BTs)\nfor reactive action planning and execution in dynamic environments, showing how\nrobotic tasks can be formulated as a free-energy minimization problem. The\nproposed approach allows to handle partially observable initial states and\nimproves the robustness of classical BTs against unexpected contingencies while\nat the same time reducing the number of nodes in a tree. In this work, the\ngeneral nominal behavior is specified offline through BTs, where a new type of\nleaf node, the prior node, is introduced to specify the desired state to be\nachieved rather than an action to be executed as typically done in BTs. The\ndecision of which action to execute to reach the desired state is performed\nonline through active inference. This results in the combination of continual\nonline planning and hierarchical deliberation, that is an agent is able to\nfollow a predefined offline plan while still being able to locally adapt and\ntake autonomous decisions at runtime. The properties of our algorithm, such as\nconvergence and robustness, are thoroughly analyzed, and the theoretical\nresults are validated in two different mobile manipulators performing similar\ntasks, both in a simulated and real retail environment.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:24:41 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:14:31 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 10:07:30 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pezzato", "Corrado", ""], ["Hernandez", "Carlos", ""], ["Bonhof", "Stefan", ""], ["Wisse", "Martijn", ""]]}, {"id": "2011.09776", "submitter": "Yang Liu", "authors": "Yang Liu, Anthony C. Constantinou, ZhiGao Guo", "title": "Improving Bayesian Network Structure Learning in the Presence of\n  Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning algorithms that learn the graph of a Bayesian network from\nobservational data often do so by assuming the data correctly reflect the true\ndistribution of the variables. However, this assumption does not hold in the\npresence of measurement error, which can lead to spurious edges. This is one of\nthe reasons why the synthetic performance of these algorithms often\noverestimates real-world performance. This paper describes an algorithm that\ncan be added as an additional learning phase at the end of any structure\nlearning algorithm, and serves as a correction learning phase that removes\npotential false positive edges. The results show that the proposed correction\nalgorithm successfully improves the graphical score of four well-established\nstructure learning algorithms spanning different classes of learning in the\npresence of measurement error.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:27:47 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Yang", ""], ["Constantinou", "Anthony C.", ""], ["Guo", "ZhiGao", ""]]}, {"id": "2011.09784", "submitter": "Robert White Mr", "authors": "Robert White and Jens Krinke", "title": "ReAssert: Deep Learning for Assert Generation", "comments": "10 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated generation of test code can reduce the time and effort required\nto build software while increasing its correctness and robustness. In this\npaper, we present RE-ASSERT, an approach for the automated generation of JUnit\ntest asserts which produces more accurate asserts than previous work with fewer\nconstraints. This is achieved by targeting projects individually, using precise\ncode-to-test traceability for learning and by generating assert statements from\nthe method-under-test directly without the need to write an assert-less test\nfirst. We also utilise Reformer, a state-of-the-art deep learning model, along\nwith two models from previous work to evaluate ReAssert and an existing\napproach, known as ATLAS, using lexical accuracy,uniqueness, and dynamic\nanalysis. Our evaluation of ReAssert shows up to 44% of generated asserts for a\nsingle project match exactly with the ground truth, increasing to 51% for\ngenerated asserts that compile. We also improve on the ATLAS results through\nour use of Reformer with 28% of generated asserts matching exactly with the\nground truth. Reformer also produces the greatest proportion of unique asserts\n(71%), giving further evidence that Reformer produces the most useful asserts.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:55:59 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["White", "Robert", ""], ["Krinke", "Jens", ""]]}, {"id": "2011.09811", "submitter": "Bing Liu", "authors": "Bing Liu and Chuhe Mei", "title": "Lifelong Knowledge Learning in Rule-based Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main weaknesses of current chatbots or dialogue systems is that\nthey do not learn online during conversations after they are deployed. This is\na major loss of opportunity. Clearly, each human user has a great deal of\nknowledge about the world that may be useful to others. If a chatbot can learn\nfrom their users during chatting, it will greatly expand its knowledge base and\nserve its users better. This paper proposes to build such a learning capability\nin a rule-based chatbot so that it can continuously acquire new knowledge in\nits chatting with users. This work is useful because many real-life deployed\nchatbots are rule-based.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:33:12 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Bing", ""], ["Mei", "Chuhe", ""]]}, {"id": "2011.09821", "submitter": "Jerry Swan", "authors": "Jerry Swan, Steven Adriaensen, Alexander E. I. Brownlee, Kevin\n  Hammond, Colin G. Johnson, Ahmed Kheiri, Faustyna Krawiec, J. J. Merelo,\n  Leandro L. Minku, Ender \\\"Ozcan, Gisele L. Pappa, Pablo Garc\\'ia-S\\'anchez,\n  Kenneth S\\\"orensen, Stefan Vo{\\ss}, Markus Wagner, David R. White", "title": "Metaheuristics \"In the Large\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Following decades of sustained improvement, metaheuristics are one of the\ngreat success stories of optimization research. However, in order for research\nin metaheuristics to avoid fragmentation and a lack of reproducibility, there\nis a pressing need for stronger scientific and computational infrastructure to\nsupport the development, analysis and comparison of new approaches. We argue\nthat, via principled choice of infrastructure support, the field can pursue a\nhigher level of scientific enquiry. We describe our vision and report on\nprogress, showing how the adoption of common protocols for all metaheuristics\ncan help liberate the potential of the field, easing the exploration of the\ndesign space of metaheuristics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:49:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:24:40 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:52:28 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 11:04:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Swan", "Jerry", ""], ["Adriaensen", "Steven", ""], ["Brownlee", "Alexander E. I.", ""], ["Hammond", "Kevin", ""], ["Johnson", "Colin G.", ""], ["Kheiri", "Ahmed", ""], ["Krawiec", "Faustyna", ""], ["Merelo", "J. J.", ""], ["Minku", "Leandro L.", ""], ["\u00d6zcan", "Ender", ""], ["Pappa", "Gisele L.", ""], ["Garc\u00eda-S\u00e1nchez", "Pablo", ""], ["S\u00f6rensen", "Kenneth", ""], ["Vo\u00df", "Stefan", ""], ["Wagner", "Markus", ""], ["White", "David R.", ""]]}, {"id": "2011.09836", "submitter": "Carsten Lutz", "authors": "Meghyn Bienvenu, Peter Hansen, Carsten Lutz, Frank Wolter", "title": "First Order-Rewritability and Containment of Conjunctive Queries in Horn\n  Description Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study FO-rewritability of conjunctive queries in the presence of\nontologies formulated in a description logic between EL and Horn-SHIF, along\nwith related query containment problems. Apart from providing\ncharacterizations, we establish complexity results ranging from ExpTime via\nNExpTime to 2ExpTime, pointing out several interesting effects. In particular,\nFO-rewriting is more complex for conjunctive queries than for atomic queries\nwhen inverse roles are present, but not otherwise.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:24:02 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Hansen", "Peter", ""], ["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "2011.09839", "submitter": "Rajesh Ramachandran", "authors": "Rishabh Verma, R Rajesh and MS Easwaran", "title": "Modular Multi Target Tracking Using LSTM Networks", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The process of association and tracking of sensor detections is a key element\nin providing situational awareness. When the targets in the scenario are dense\nand exhibit high maneuverability, Multi-Target Tracking (MTT) becomes a\nchallenging task. The conventional techniques to solve such NP-hard\ncombinatorial optimization problem involves multiple complex models and\nrequires tedious tuning of parameters, failing to provide an acceptable\nperformance within the computational constraints. This paper proposes a model\nfree end-to-end approach for airborne target tracking system using sensor\nmeasurements, integrating all the key elements of multi target tracking --\nassociation, prediction and filtering using deep learning with memory. The\nchallenging task of association is performed using the Bi-Directional Long\nshort-term memory (LSTM) whereas filtering and prediction are done using LSTM\nmodels. The proposed modular blocks can be independently trained and used in\nmultitude of tracking applications including non co-operative (e.g., radar) and\nco-operative sensors (e.g., AIS, IFF, ADS-B). Such modular blocks also enhances\nthe interpretability of the deep learning application. It is shown that\nperformance of the proposed technique outperforms conventional state of the art\ntechnique Joint Probabilistic Data Association with Interacting Multiple Model\n(JPDA-IMM) filter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:58:49 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Verma", "Rishabh", ""], ["Rajesh", "R", ""], ["Easwaran", "MS", ""]]}, {"id": "2011.09843", "submitter": "Seshadhri Srinivasan", "authors": "S. Sairam, Seshadhri Srinivasan, G. Marafioti, B. Subathra, G.\n  Mathisen, and Korkut Bekiroglu", "title": "Explainable Incipient Fault Detection Systems for Photovoltaic Panels", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an eXplainable Fault Detection and Diagnosis System\n(XFDDS) for incipient faults in PV panels. The XFDDS is a hybrid approach that\ncombines the model-based and data-driven framework. Model-based FDD for PV\npanels lacks high fidelity models at low irradiance conditions for detecting\nincipient faults. To overcome this, a novel irradiance based three diode model\n(IB3DM) is proposed. It is a nine parameter model that provides higher accuracy\neven at low irradiance conditions, an important aspect for detecting incipient\nfaults from noise. To exploit PV data, extreme gradient boosting (XGBoost) is\nused due to its ability to detecting incipient faults. Lack of explainability,\nfeature variability for sample instances, and false alarms are challenges with\ndata-driven FDD methods. These shortcomings are overcome by hybridization of\nXGBoost and IB3DM, and using eXplainable Artificial Intelligence (XAI)\ntechniques. To combine the XGBoost and IB3DM, a fault-signature metric is\nproposed that helps reducing false alarms and also trigger an explanation on\ndetecting incipient faults. To provide explainability, an eXplainable\nArtificial Intelligence (XAI) application is developed. It uses the local\ninterpretable model-agnostic explanations (LIME) framework and provides\nexplanations on classifier outputs for data instances. These explanations help\nfield engineers/technicians for performing troubleshooting and maintenance\noperations. The proposed XFDDS is illustrated using experiments on different PV\ntechnologies and our results demonstrate the perceived benefits.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:26:29 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Sairam", "S.", ""], ["Srinivasan", "Seshadhri", ""], ["Marafioti", "G.", ""], ["Subathra", "B.", ""], ["Mathisen", "G.", ""], ["Bekiroglu", "Korkut", ""]]}, {"id": "2011.09845", "submitter": "Feng Li", "authors": "Youming Tao, Shuzhen Chen, Feng Li, Dongxiao Yu, Jiguo Yu, Hao Sheng", "title": "A Distributed Privacy-Preserving Learning Dynamics in General Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a distributed privacy-preserving learning problem in\ngeneral social networks. Specifically, we consider a very general problem\nsetting where the agents in a given multi-hop social network are required to\nmake sequential decisions to choose among a set of options featured by unknown\nstochastic quality signals. Each agent is allowed to interact with its peers\nthrough multi-hop communications but with its privacy preserved. To serve the\nabove goals, we propose a four-staged distributed social learning algorithm. In\na nutshell, our algorithm proceeds iteratively, and in every round, each agent\ni) randomly perturbs its adoption for privacy-preserving purpose, ii)\ndisseminates the perturbed adoption over the social network in a nearly uniform\nmanner through random walking, iii) selects an option by referring to its\npeers' perturbed latest adoptions, and iv) decides whether or not to adopt the\nselected option according to its latest quality signal. By our solid\ntheoretical analysis, we provide answers to two fundamental algorithmic\nquestions about the performance of our four-staged algorithm: on one hand, we\nillustrate the convergence of our algorithm when there are a sufficient number\nof agents in the social network, each of which are with incomplete and\nperturbed knowledge as input; on the other hand, we reveal the quantitative\ntrade-off between the privacy loss and the communication overhead towards the\nconvergence. We also perform extensive simulations to validate our theoretical\nanalysis and to verify the efficacy of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 04:00:45 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tao", "Youming", ""], ["Chen", "Shuzhen", ""], ["Li", "Feng", ""], ["Yu", "Dongxiao", ""], ["Yu", "Jiguo", ""], ["Sheng", "Hao", ""]]}, {"id": "2011.09850", "submitter": "Lenore Blum", "authors": "Manuel Blum and Lenore Blum", "title": "A Theoretical Computer Science Perspective on Consciousness", "comments": "33 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quest to understand consciousness, once the purview of philosophers and\ntheologians, is now actively pursued by scientists of many stripes. This paper\nstudies consciousness from the perspective of theoretical computer science. It\nformalizes the Global Workspace Theory (GWT) originated by cognitive\nneuroscientist Bernard Baars and further developed by him, Stanislas Dehaene,\nand others. Our major contribution lies in the precise formal definition of a\nConscious Turing Machine (CTM), also called a Conscious AI. We define the CTM\nin the spirit of Alan Turing's simple yet powerful definition of a computer,\nthe Turing Machine (TM). We are not looking for a complex model of the brain\nnor of cognition but for a simple model of (the admittedly complex concept of)\nconsciousness. After formally defining CTM, we give a formal definition of\nconsciousness in CTM. We then suggest why the CTM has the feeling of\nconsciousness. The reasonableness of the definitions and explanations can be\njudged by how well they agree with commonly accepted intuitive concepts of\nhuman consciousness, the breadth of related concepts that the model explains\neasily and naturally, and the extent of its agreement with scientific evidence.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 11:28:37 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 16:48:42 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 18:40:51 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Blum", "Manuel", ""], ["Blum", "Lenore", ""]]}, {"id": "2011.09852", "submitter": "Yusuke Sekikawa", "authors": "Yusuke Sekikawa, Teppei Suzuki", "title": "Irregularly Tabulated MLP for Fast Point Feature Embedding", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.00790", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aiming at drastic speedup for point-feature embeddings at test time, we\npropose a new framework that uses a pair of multi-layer perceptrons (MLP) and a\nlookup table (LUT) to transform point-coordinate inputs into high-dimensional\nfeatures. When compared with PointNet's feature embedding part realized by MLP\nthat requires millions of dot products, the proposed framework at test time\nrequires no such layers of matrix-vector products but requires only looking up\nthe nearest entities from the tabulated MLP followed by interpolation, defined\nover discrete inputs on a 3D lattice that is substantially arranged\nirregularly. We call this framework LUTI-MLP: LUT Interpolation ML that\nprovides a way to train end-to-end irregularly tabulated MLP coupled to a LUT\nin a specific manner without the need for any approximation at test time.\nLUTI-MLP also provides significant speedup for Jacobian computation of the\nembedding function wrt global pose coordinate on Lie algebra $\\mathfrak{se}(3)$\nat test time, which could be used for point-set registration problems. After\nextensive evaluation using the ModelNet40, we confirmed that the LUTI-MLP even\nwith a small (e.g., $4^3$) lattice yields performance comparable to that of the\nMLP while achieving significant speedup: $100\\times$ for the embedding,\n$12\\times$ for the approximate Jacobian, and $860\\times$ for the canonical\nJacobian.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 04:15:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Sekikawa", "Yusuke", ""], ["Suzuki", "Teppei", ""]]}, {"id": "2011.09854", "submitter": "Sirui Xie", "authors": "Sirui Xie and Feng Gao and Song-Chun Zhu", "title": "Generalized Inverse Planning: Learning Lifted non-Markovian Utility for\n  Generalizable Task Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In searching for a generalizable representation of temporally extended tasks,\nwe spot two necessary constituents: the utility needs to be non-Markovian to\ntransfer temporal relations invariant to a probability shift, the utility also\nneeds to be lifted to abstract out specific grounding objects. In this work, we\nstudy learning such utility from human demonstrations. While inverse\nreinforcement learning (IRL) has been accepted as a general framework of\nutility learning, its fundamental formulation is one concrete Markov Decision\nProcess. Thus the learned reward function does not specify the task\nindependently of the environment. Going beyond that, we define a domain of\ngeneralization that spans a set of planning problems following a schema. We\nhence propose a new quest, Generalized Inverse Planning, for utility learning\nin this domain. We further outline a computational framework, Maximum Entropy\nInverse Planning (MEIP), that learns non-Markovian utility and associated\nconcepts in a generative manner. The learned utility and concepts form a task\nrepresentation that generalizes regardless of probability shift or structural\nchange. Seeing that the proposed generalization problem has not been widely\nstudied yet, we carefully define an evaluation protocol, with which we\nillustrate the effectiveness of MEIP on two proof-of-concept domains and one\nchallenging task: learning to fold from demonstrations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:06:26 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Xie", "Sirui", ""], ["Gao", "Feng", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2011.09858", "submitter": "Carsten Lutz", "authors": "Jean Christoph Jung, Carsten Lutz, Mauricio Martel, Thomas Schneider", "title": "Conservative Extensions in Horn Description Logics with Inverse Roles", "comments": null, "journal-ref": "Journal of Artificial Intelligence Ressearch 68: 365-411 (2020)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the decidability and computational complexity of conservative\nextensions and the related notions of inseparability and entailment in Horn\ndescription logics (DLs) with inverse roles. We consider both query\nconservative extensions, defined by requiring that the answers to all\nconjunctive queries are left unchanged, and deductive conservative extensions,\nwhich require that the entailed concept inclusions, role inclusions, and\nfunctionality assertions do not change. Upper bounds for query conservative\nextensions are particularly challenging because characterizations in terms of\nunbounded homomorphisms between universal models, which are the foundation of\nthe standard approach to establishing decidability, fail in the presence of\ninverse roles. We resort to a characterization that carefully mixes unbounded\nand bounded homomorphisms and enables a decision procedure that combines tree\nautomata and a mosaic technique. Our main results are that query conservative\nextensions are 2ExpTime-complete in all DLs between ELI and Horn-ALCHIF and\nbetween Horn-ALC and Horn-ALCHIF, and that deductive conservative extensions\nare 2ExpTime-complete in all DLs between ELI and ELHIF_\\bot. The same results\nhold for inseparability and entailment.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:41:02 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jung", "Jean Christoph", ""], ["Lutz", "Carsten", ""], ["Martel", "Mauricio", ""], ["Schneider", "Thomas", ""]]}, {"id": "2011.09860", "submitter": "Victor Kolev", "authors": "Victor Kolev, Bogdan Georgiev, Svetlin Penkov", "title": "Neural Abstract Reasoner", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract reasoning and logic inference are difficult problems for neural\nnetworks, yet essential to their applicability in highly structured domains. In\nthis work we demonstrate that a well known technique such as spectral\nregularization can significantly boost the capabilities of a neural learner. We\nintroduce the Neural Abstract Reasoner (NAR), a memory augmented architecture\ncapable of learning and using abstract rules. We show that, when trained with\nspectral regularization, NAR achieves $78.8\\%$ accuracy on the Abstraction and\nReasoning Corpus, improving performance 4 times over the best known human\nhand-crafted symbolic solvers. We provide some intuition for the effects of\nspectral regularization in the domain of abstract reasoning based on\ntheoretical generalization bounds and Solomonoff's theory of inductive\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 20:30:33 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kolev", "Victor", ""], ["Georgiev", "Bogdan", ""], ["Penkov", "Svetlin", ""]]}, {"id": "2011.09867", "submitter": "Hanshuang Tong", "authors": "Hanshuang Tong, Yun Zhou and Zhen Wang", "title": "Exercise Hierarchical Feature Enhanced Knowledge Tracing", "comments": "5 pages, 4 figures, Accepted by AIED 2020. In the 21st International\n  Conference on Artificial Intelligence in Education (AIED 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-52240-7_59", "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing is a fundamental task in the computer-aid educational\nsystem. In this paper, we propose a hierarchical exercise feature enhanced\nknowledge tracing framework, which could enhance the ability of knowledge\ntracing by incorporating knowledge distribution, semantic features, and\ndifficulty features from exercise text. Extensive experiments show the high\nperformance of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:16:07 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tong", "Hanshuang", ""], ["Zhou", "Yun", ""], ["Wang", "Zhen", ""]]}, {"id": "2011.09884", "submitter": "Qing Guo", "authors": "Bing Yu and Hua Qi and Qing Guo and Felix Juefei-Xu and Xiaofei Xie\n  and Lei Ma and Jianjun Zhao", "title": "DeepRepair: Style-Guided Repairing for DNNs in the Real-world\n  Operational Environment", "comments": "14 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are being widely applied for various real-world\napplications across domains due to their high performance (e.g., high accuracy\non image classification). Nevertheless, a well-trained DNN after deployment\ncould oftentimes raise errors during practical use in the operational\nenvironment due to the mismatching between distributions of the training\ndataset and the potential unknown noise factors in the operational environment,\ne.g., weather, blur, noise etc. Hence, it poses a rather important problem for\nthe DNNs' real-world applications: how to repair the deployed DNNs for\ncorrecting the failure samples (i.e., incorrect prediction) under the deployed\noperational environment while not harming their capability of handling normal\nor clean data. The number of failure samples we can collect in practice, caused\nby the noise factors in the operational environment, is often limited.\nTherefore, It is rather challenging how to repair more similar failures based\non the limited failure samples we can collect.\n  In this paper, we propose a style-guided data augmentation for repairing DNN\nin the operational environment. We propose a style transfer method to learn and\nintroduce the unknown failure patterns within the failure data into the\ntraining data via data augmentation. Moreover, we further propose the\nclustering-based failure data generation for much more effective style-guided\ndata augmentation. We conduct a large-scale evaluation with fifteen degradation\nfactors that may happen in the real world and compare with four\nstate-of-the-art data augmentation methods and two DNN repairing methods,\ndemonstrating that our method can significantly enhance the deployed DNNs on\nthe corrupted data in the operational environment, and with even better\naccuracy on clean datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:09:44 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yu", "Bing", ""], ["Qi", "Hua", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Zhao", "Jianjun", ""]]}, {"id": "2011.09890", "submitter": "Uwe Aickelin", "authors": "Xiaoping Jiang, Ruibin Bai, Dario Landa-Silva, Uwe Aickelin", "title": "Fuzzy C-means-based scenario bundling for stochastic service network\n  design", "comments": "2017 IEEE Symposium on Computational Intelligence (IEEE-SSCI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic service network designs with uncertain demand represented by a set\nof scenarios can be modelled as a large-scale two-stage stochastic\nmixed-integer program (SMIP). The progressive hedging algorithm (PHA) is a\ndecomposition method for solving the resulting SMIP. The computational\nperformance of the PHA can be greatly enhanced by decomposing according to\nscenario bundles instead of individual scenarios. At the heart of bundle-based\ndecomposition is the method for grouping the scenarios into bundles. In this\npaper, we present a fuzzy c-means-based scenario bundling method to address\nthis problem. Rather than full membership of a bundle, which is typically the\ncase in existing scenario bundling strategies such as k-means, a scenario has\npartial membership in each of the bundles and can be assigned to more than one\nbundle in our method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:41:47 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jiang", "Xiaoping", ""], ["Bai", "Ruibin", ""], ["Landa-Silva", "Dario", ""], ["Aickelin", "Uwe", ""]]}, {"id": "2011.09891", "submitter": "Uwe Aickelin", "authors": "Uwe Aickelin, Jenna Marie Reps, Peer-Olaf Siebers, Peng Li", "title": "Using simulation to incorporate dynamic criteria into multiple criteria\n  decision-making", "comments": "Journal of the Operational Research Society, Volume 69, Issue 7,\n  Pages 1021-1032", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a case study demonstrating how dynamic and\nuncertain criteria can be incorporated into a multicriteria analysis with the\nhelp of discrete event simulation. The simulation guided multicriteria analysis\ncan include both monetary and non-monetary criteria that are static or dynamic,\nwhereas standard multi criteria analysis only deals with static criteria and\ncost benefit analysis only deals with static monetary criteria. The dynamic and\nuncertain criteria are incorporated by using simulation to explore how the\ndecision options perform. The results of the simulation are then fed into the\nmulticriteria analysis. By enabling the incorporation of dynamic and uncertain\ncriteria, the dynamic multiple criteria analysis was able to take a unique\nperspective of the problem. The highest ranked option returned by the dynamic\nmulticriteria analysis differed from the other decision aid techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:11:45 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Aickelin", "Uwe", ""], ["Reps", "Jenna Marie", ""], ["Siebers", "Peer-Olaf", ""], ["Li", "Peng", ""]]}, {"id": "2011.09892", "submitter": "Shideh Shams Amiri", "authors": "Shideh Shams Amiri, Rosina O. Weber, Prateek Goel, Owen Brooks, Archer\n  Gandley, Brian Kitchell, Aaron Zehm", "title": "Data Representing Ground-Truth Explanations to Evaluate XAI Methods", "comments": "Submitted to the AAAI 2021 Explainable Agency in Artificial\n  Intelligence Workshop, 6 pages, 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificial intelligence (XAI) methods are currently evaluated\nwith approaches mostly originated in interpretable machine learning (IML)\nresearch that focus on understanding models such as comparison against existing\nattribution approaches, sensitivity analyses, gold set of features, axioms, or\nthrough demonstration of images. There are problems with these methods such as\nthat they do not indicate where current XAI approaches fail to guide\ninvestigations towards consistent progress of the field. They do not measure\naccuracy in support of accountable decisions, and it is practically impossible\nto determine whether one XAI method is better than the other or what the\nweaknesses of existing models are, leaving researchers without guidance on\nwhich research questions will advance the field. Other fields usually utilize\nground-truth data and create benchmarks. Data representing ground-truth\nexplanations is not typically used in XAI or IML. One reason is that\nexplanations are subjective, in the sense that an explanation that satisfies\none user may not satisfy another. To overcome these problems, we propose to\nrepresent explanations with canonical equations that can be used to evaluate\nthe accuracy of XAI methods. The contributions of this paper include a\nmethodology to create synthetic data representing ground-truth explanations,\nthree data sets, an evaluation of LIME using these data sets, and a preliminary\nanalysis of the challenges and potential benefits in using these data to\nevaluate existing XAI approaches. Evaluation methods based on human-centric\nstudies are outside the scope of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:54:53 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Amiri", "Shideh Shams", ""], ["Weber", "Rosina O.", ""], ["Goel", "Prateek", ""], ["Brooks", "Owen", ""], ["Gandley", "Archer", ""], ["Kitchell", "Brian", ""], ["Zehm", "Aaron", ""]]}, {"id": "2011.09900", "submitter": "Tao Huang", "authors": "Tao Huang, Yihan Zhang, Jiajing Wu, Junyuan Fang, Zibin Zheng", "title": "MG-GCN: Fast and Effective Learning with Mix-grained Aggregators for\n  Training Large Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) have been employed as a kind of\nsignificant tool on many graph-based applications recently. Inspired by\nconvolutional neural networks (CNNs), GCNs generate the embeddings of nodes by\naggregating the information of their neighbors layer by layer. However, the\nhigh computational and memory cost of GCNs due to the recursive neighborhood\nexpansion across GCN layers makes it infeasible for training on large graphs.\nTo tackle this issue, several sampling methods during the process of\ninformation aggregation have been proposed to train GCNs in a mini-batch\nStochastic Gradient Descent (SGD) manner. Nevertheless, these sampling\nstrategies sometimes bring concerns about insufficient information collection,\nwhich may hinder the learning performance in terms of accuracy and convergence.\nTo tackle the dilemma between accuracy and efficiency, we propose to use\naggregators with different granularities to gather neighborhood information in\ndifferent layers. Then, a degree-based sampling strategy, which avoids the\nexponential complexity, is constructed for sampling a fixed number of nodes.\nCombining the above two mechanisms, the proposed model, named Mix-grained GCN\n(MG-GCN) achieves state-of-the-art performance in terms of accuracy, training\nspeed, convergence speed, and memory cost through a comprehensive set of\nexperiments on four commonly used benchmark datasets and a new Ethereum\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:51:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Huang", "Tao", ""], ["Zhang", "Yihan", ""], ["Wu", "Jiajing", ""], ["Fang", "Junyuan", ""], ["Zheng", "Zibin", ""]]}, {"id": "2011.09903", "submitter": "Brian Liu", "authors": "Brian Liu and Madeleine Udell", "title": "Impact of Accuracy on Model Interpretations", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretations are often used in practice to extract real world\ninsights from machine learning models. These interpretations have a wide range\nof applications; they can be presented as business recommendations or used to\nevaluate model bias. It is vital for a data scientist to choose trustworthy\ninterpretations to drive real world impact. Doing so requires an understanding\nof how the accuracy of a model impacts the quality of standard interpretation\ntools. In this paper, we will explore how a model's predictive accuracy affects\ninterpretation quality. We propose two metrics to quantify the quality of an\ninterpretation and design an experiment to test how these metrics vary with\nmodel accuracy. We find that for datasets that can be modeled accurately by a\nvariety of methods, simpler methods yield higher quality interpretations. We\nalso identify which interpretation method works the best for lower levels of\nmodel accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:02:59 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Brian", ""], ["Udell", "Madeleine", ""]]}, {"id": "2011.09907", "submitter": "Asan Agibetov", "authors": "Asan Agibetov", "title": "Graph embeddings via matrix factorization for link prediction: smoothing\n  or truncating negatives?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Link prediction -- the process of uncovering missing links in a complex\nnetwork -- is an important problem in information sciences, with applications\nranging from social sciences to molecular biology. Recent advances in neural\ngraph embeddings have proposed an end-to-end way of learning latent vector\nrepresentations of nodes, with successful application in link prediction tasks.\nYet, our understanding of the internal mechanisms of such approaches has been\nrather limited, and only very recently we have witnessed the development of a\nvery compelling connection to the mature matrix factorization theory. In this\nwork, we make an important contribution to our understanding of the interplay\nbetween the skip-gram powered neural graph embedding algorithms and the matrix\nfactorization via SVD. In particular, we show that the link prediction accuracy\nof graph embeddings strongly depends on the transformations of the original\ngraph co-occurrence matrix that they decompose, sometimes resulting in\nstaggering boosts of accuracy performance on link prediction tasks. Our\nimproved approach to learning low-rank factorization embeddings that\nincorporate information from unlikely pairs of nodes yields results on par with\nthe state-of-the-art link prediction performance achieved by a complex neural\ngraph embedding model\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:23:12 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Agibetov", "Asan", ""]]}, {"id": "2011.09928", "submitter": "Haoyu Dong", "authors": "Haoyu Dong, Ze Wang, Qiang Qiu, and Guillermo Sapiro", "title": "Using Text to Teach Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval relies heavily on the quality of the data modeling and the\ndistance measurement in the feature space. Building on the concept of image\nmanifold, we first propose to represent the feature space of images, learned\nvia neural networks, as a graph. Neighborhoods in the feature space are now\ndefined by the geodesic distance between images, represented as graph vertices\nor manifold samples. When limited images are available, this manifold is\nsparsely sampled, making the geodesic computation and the corresponding\nretrieval harder. To address this, we augment the manifold samples with\ngeometrically aligned text, thereby using a plethora of sentences to teach us\nabout images. In addition to extensive results on standard datasets\nillustrating the power of text to help in image retrieval, a new public dataset\nbased on CLEVR is introduced to quantify the semantic similarity between visual\ndata and text data. The experimental results show that the joint embedding\nmanifold is a robust representation, allowing it to be a better basis to\nperform image retrieval given only an image and a textual instruction on the\ndesired modifications over the image\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:09:14 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Dong", "Haoyu", ""], ["Wang", "Ze", ""], ["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "2011.09967", "submitter": "Bin Wang", "authors": "Wanshi Hong, Cong Zhang, Cy Chan, Bin Wang", "title": "Electric Vehicle Charging Infrastructure Planning: A Scalable\n  Computational Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal charging infrastructure planning problem over a large geospatial\narea is challenging due to the increasing network sizes of the transportation\nsystem and the electric grid. The coupling between the electric vehicle travel\nbehaviors and charging events is therefore complex. This paper focuses on the\ndemonstration of a scalable computational framework for the electric vehicle\ncharging infrastructure planning over the tightly integrated transportation and\nelectric grid networks. On the transportation side, a charging profile\ngeneration strategy is proposed leveraging the EV energy consumption model,\ntrip routing, and charger selection methods. On the grid side, a genetic\nalgorithm is utilized within the optimal power flow program to solve the\noptimal charger placement problem with integer variables by adaptively\nevaluating candidate solutions in the current iteration and generating new\nsolutions for the next iterations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:48:07 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Hong", "Wanshi", ""], ["Zhang", "Cong", ""], ["Chan", "Cy", ""], ["Wang", "Bin", ""]]}, {"id": "2011.09994", "submitter": "Mahdi Sani", "authors": "Reza Namazi, Arsham Zolanvari, Mahdi Sani, Seyed Amir Ali Ghafourian\n  Ghahramani", "title": "GL-Coarsener: A Graph representation learning framework to construct\n  coarse grid hierarchy for AMG solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AI cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many numerical schemes, the computational complexity scales non-linearly\nwith the problem size. Solving a linear system of equations using direct\nmethods or most iterative methods is a typical example. Algebraic multi-grid\n(AMG) methods are numerical methods used to solve large linear systems of\nequations efficiently. One of the main differences between AMG methods is how\nthe coarser grid is constructed from a given fine grid. There are two main\nclasses of AMG methods; graph and aggregation based coarsening methods. Here we\npropose an aggregation-based coarsening framework leveraging graph\nrepresentation learning and clustering algorithms. Our method introduces the\npower of machine learning into the AMG research field and opens a new\nperspective for future researches. The proposed method uses graph\nrepresentation learning techniques to learn latent features of the graph\nobtained from the underlying matrix of coefficients. Using these extracted\nfeatures, we generated a coarser grid from the fine grid. The proposed method\nis highly capable of parallel computations. Our experiments show that the\nproposed method's efficiency in solving large systems is closely comparable\nwith other aggregation-based methods, demonstrating the high capability of\ngraph representation learning in designing multi-grid solvers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:49:09 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Namazi", "Reza", ""], ["Zolanvari", "Arsham", ""], ["Sani", "Mahdi", ""], ["Ghahramani", "Seyed Amir Ali Ghafourian", ""]]}, {"id": "2011.10039", "submitter": "Songwei Ge", "authors": "Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick and Devi Parikh", "title": "Creative Sketch Generation", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching or doodling is a popular creative activity that people engage in.\nHowever, most existing work in automatic sketch understanding or generation has\nfocused on sketches that are quite mundane. In this work, we introduce two\ndatasets of creative sketches -- Creative Birds and Creative Creatures --\ncontaining 10k sketches each along with part annotations. We propose DoodlerGAN\n-- a part-based Generative Adversarial Network (GAN) -- to generate unseen\ncompositions of novel part appearances. Quantitative evaluations as well as\nhuman studies demonstrate that sketches generated by our approach are more\ncreative and of higher quality than existing approaches. In fact, in Creative\nBirds, subjects prefer sketches generated by DoodlerGAN over those drawn by\nhumans! Our code can be found at https://github.com/facebookresearch/DoodlerGAN\nand a demo can be found at http://doodlergan.cloudcv.org.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:57:00 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 20:01:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ge", "Songwei", ""], ["Goswami", "Vedanuj", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "2011.10076", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Guanghui Lan", "title": "Optimal Algorithms for Convex Nested Stochastic Composite Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, convex nested stochastic composite optimization (NSCO) has received\nconsiderable attention for its application in reinforcement learning and\nrisk-averse optimization. However, In the current literature, there exists a\nsignificant gap in the iteration complexities between these NSCO problems and\nother simpler stochastic composite optimization problems (e.g., sum of smooth\nand nonsmooth functions) without the nested structure. In this paper, we close\nthe gap by reformulating a class of convex NSCO problems as \"$\\min\\max\\ldots\n\\max$\" saddle point problems under mild assumptions and proposing two\nprimal-dual type algorithms with the optimal $\\mathcal{O}\\{1/\\epsilon^2\\}$\n(resp., $\\mathcal{O}\\{1/\\epsilon\\}$) complexity for solving nested (resp.,\nstrongly) convex problems. More specifically, for the often-considered\ntwo-layer smooth-nonsmooth problem, we introduce a simple vanilla stochastic\nsequential dual (SSD) algorithm which can be implemented purely in the primal\nform. For the multi-layer problem, we propose a general stochastic sequential\ndual framework. The framework consists of modular dual updates for different\ntypes of functions (smooth, smoothable, and non-smooth, etc.), so that it can\nhandle a more general composition of layer functions. Moreover, we present\nmodular convergence proofs to show that the complexity of the general SSD is\noptimal with respect to nearly all the problem parameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:22:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 04:01:19 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 04:40:57 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 03:44:32 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhang", "Zhe", ""], ["Lan", "Guanghui", ""]]}, {"id": "2011.10094", "submitter": "Anh Cat Le Ngo", "authors": "Anh-Cat Le-Ngo, Truyen Tran, Santu Rana, Sunil Gupta, Svetha Venkatesh", "title": "Logically Consistent Loss for Visual Question Answering", "comments": "10 pages, 6 figure, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image, a back-ground knowledge, and a set of questions about an\nobject, human learners answer the questions very consistently regardless of\nquestion forms and semantic tasks. The current advancement in neural-network\nbased Visual Question Answering (VQA), despite their impressive performance,\ncannot ensure such consistency due to identically distribution (i.i.d.)\nassumption. We propose a new model-agnostic logic constraint to tackle this\nissue by formulating a logically consistent loss in the multi-task learning\nframework as well as a data organisation called family-batch and hybrid-batch.\nTo demonstrate usefulness of this proposal, we train and evaluate MAC-net based\nVQA machines with and without the proposed logically consistent loss and the\nproposed data organization. The experiments confirm that the proposed loss\nformulae and introduction of hybrid-batch leads to more consistency as well as\nbetter performance. Though the proposed approach is tested with MAC-net, it can\nbe utilised in any other QA methods whenever the logical consistency between\nanswers exist.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 20:31:05 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Le-Ngo", "Anh-Cat", ""], ["Tran", "Truyen", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2011.10098", "submitter": "Odd Erik Gundersen", "authors": "Odd Erik Gundersen", "title": "The Fundamental Principles of Reproducibility", "comments": "Accepted for publication in Philosophical Transactions of the Royal\n  Society A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reproducibility is a confused terminology. In this paper, I take a\nfundamental view on reproducibility rooted in the scientific method. The\nscientific method is analysed and characterised in order to develop the\nterminology required to define reproducibility. Further, the literature on\nreproducibility and replication is surveyed, and experiments are modeled as\ntasks and problem solving methods. Machine learning is used to exemplify the\ndescribed approach. Based on the analysis, reproducibility is defined and three\ndifferent types of reproducibility as well as four degrees of reproducibility\nare specified.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 20:37:58 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:01:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gundersen", "Odd Erik", ""]]}, {"id": "2011.10118", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam and\n  Jessica Hodgins", "title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial vehicles are revolutionizing the way film-makers can capture shots of\nactors by composing novel aerial and dynamic viewpoints. However, despite great\nadvancements in autonomous flight technology, generating expressive camera\nbehaviors is still a challenge and requires non-technical users to edit a large\nnumber of unintuitive control parameters. In this work, we develop a\ndata-driven framework that enables editing of these complex camera positioning\nparameters in a semantic space (e.g. calm, enjoyable, establishing). First, we\ngenerate a database of video clips with a diverse range of shots in a\nphoto-realistic simulator, and use hundreds of participants in a crowd-sourcing\nframework to obtain scores for a set of semantic descriptors for each clip.\nNext, we analyze correlations between descriptors and build a semantic control\nspace based on cinematography guidelines and human perception studies. Finally,\nwe learn a generative model that can map a set of desired semantic video\ndescriptors into low-level camera trajectory parameters. We evaluate our system\nby demonstrating that our model successfully generates shots that are rated by\nparticipants as having the expected degrees of expression for each descriptor.\nWe also show that our models generalize to different scenes in both simulation\nand real-world experiments. Data and video found at:\nhttps://sites.google.com/view/robotcam.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:56:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Bucker", "Arthur", ""], ["Scherer", "Sebastian", ""], ["Mukadam", "Mustafa", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2011.10134", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou and Jiahao Chen and Quanquan Gu", "title": "Provable Multi-Objective Reinforcement Learning with Generative Models", "comments": "10 pages, Workshop on Real-World Reinforcement Learning at the 34th\n  Conference on Neural Information ProcessingSystems (NeurIPS 2020), Vancouver,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective reinforcement learning (MORL) is an extension of ordinary,\nsingle-objective reinforcement learning (RL) that is applicable to many\nreal-world tasks where multiple objectives exist without known relative costs.\nWe study the problem of single policy MORL, which learns an optimal policy\ngiven the preference of objectives. Existing methods require strong assumptions\nsuch as exact knowledge of the multi-objective Markov decision process, and are\nanalyzed in the limit of infinite data and time. We propose a new algorithm\ncalled model-based envelop value iteration (EVI), which generalizes the\nenveloped multi-objective $Q$-learning algorithm in Yang et al., 2019. Our\nmethod can learn a near-optimal value function with polynomial sample\ncomplexity and linear convergence speed. To the best of our knowledge, this is\nthe first finite-sample analysis of MORL algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 22:35:31 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 07:28:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhou", "Dongruo", ""], ["Chen", "Jiahao", ""], ["Gu", "Quanquan", ""]]}, {"id": "2011.10208", "submitter": "Eric Nichols", "authors": "Eric Nichols and Leo Gao and Randy Gomez", "title": "Collaborative Storytelling with Large-scale Neural Language Models", "comments": "To appear in Proceedings of the 13th Annual ACM SIGGRAPH Conference\n  on Motion, Interaction and Games (MIG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Storytelling plays a central role in human socializing and entertainment.\nHowever, much of the research on automatic storytelling generation assumes that\nstories will be generated by an agent without any human interaction. In this\npaper, we introduce the task of collaborative storytelling, where an artificial\nintelligence agent and a person collaborate to create a unique story by taking\nturns adding to it. We present a collaborative storytelling system which works\nwith a human storyteller to create a story by generating new utterances based\non the story so far. We constructed the storytelling system by tuning a\npublicly-available large scale language model on a dataset of writing prompts\nand their accompanying fictional works. We identify generating sufficiently\nhuman-like utterances to be an important technical issue and propose a\nsample-and-rank approach to improve utterance quality. Quantitative evaluation\nshows that our approach outperforms a baseline, and we present qualitative\nevaluation of our system's capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:36:54 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Nichols", "Eric", ""], ["Gao", "Leo", ""], ["Gomez", "Randy", ""]]}, {"id": "2011.10219", "submitter": "Xingchao Liu", "authors": "Xingchao Liu, Xing Han, Na Zhang, Qiang Liu", "title": "Certified Monotonic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning monotonic models with respect to a subset of the inputs is a\ndesirable feature to effectively address the fairness, interpretability, and\ngeneralization issues in practice. Existing methods for learning monotonic\nneural networks either require specifically designed model structures to ensure\nmonotonicity, which can be too restrictive/complicated, or enforce monotonicity\nby adjusting the learning process, which cannot provably guarantee the learned\nmodel is monotonic on selected features. In this work, we propose to certify\nthe monotonicity of the general piece-wise linear neural networks by solving a\nmixed integer linear programming problem.This provides a new general approach\nfor learning monotonic neural networks with arbitrary model structures. Our\nmethod allows us to train neural networks with heuristic monotonicity\nregularizations, and we can gradually increase the regularization magnitude\nuntil the learned network is certified monotonic. Compared to prior works, our\napproach does not require human-designed constraints on the weight space and\nalso yields more accurate approximation. Empirical studies on various datasets\ndemonstrate the efficiency of our approach over the state-of-the-art methods,\nsuch as Deep Lattice Networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:58:13 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Liu", "Xingchao", ""], ["Han", "Xing", ""], ["Zhang", "Na", ""], ["Liu", "Qiang", ""]]}, {"id": "2011.10254", "submitter": "Xiang Fang", "authors": "Xiang Fang, Yuchong Hu, Pan Zhou, and Dapeng Oliver Wu", "title": "Unbalanced Incomplete Multi-view Clustering via the Scheme of View\n  Evolution: Weak Views are Meat; Strong Views do Eat", "comments": "Accepted by IEEE Transactions on Emerging Topics in Computational\n  Intelligence", "journal-ref": "IEEE Transactions on Emerging Topics in Computational Intelligence\n  2021", "doi": "10.1109/TETCI.2021.3077909", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete multi-view clustering is an important technique to deal with\nreal-world incomplete multi-view data. Previous works assume that all views\nhave the same incompleteness, i.e., balanced incompleteness. However, different\nviews often have distinct incompleteness, i.e., unbalanced incompleteness,\nwhich results in strong views (low-incompleteness views) and weak views\n(high-incompleteness views). The unbalanced incompleteness prevents us from\ndirectly using the previous methods for clustering. In this paper, inspired by\nthe effective biological evolution theory, we design the novel scheme of view\nevolution to cluster strong and weak views. Moreover, we propose an Unbalanced\nIncomplete Multi-view Clustering method (UIMC), which is the first effective\nmethod based on view evolution for unbalanced incomplete multi-view clustering.\nCompared with previous methods, UIMC has two unique advantages: 1) it proposes\nweighted multi-view subspace clustering to integrate these unbalanced\nincomplete views, which effectively solves the unbalanced incomplete multi-view\nproblem; 2) it designs the low-rank and robust representation to recover the\ndata, which diminishes the impact of the incompleteness and noises. Extensive\nexperimental results demonstrate that UIMC improves the clustering performance\nby up to 40% on three evaluation metrics over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:00:25 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 08:51:46 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fang", "Xiang", ""], ["Hu", "Yuchong", ""], ["Zhou", "Pan", ""], ["Wu", "Dapeng Oliver", ""]]}, {"id": "2011.10274", "submitter": "Boris Chidlovskii", "authors": "Maxime Pietrantoni, Boris Chidlovskii, Tomi Silander", "title": "Learning Synthetic to Real Transfer for Localization and Navigational\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation consists in an agent being able to navigate without\nhuman intervention or supervision, it affects both high level planning and low\nlevel control. Navigation is at the crossroad of multiple disciplines, it\ncombines notions of computer vision, robotics and control. This work aimed at\ncreating, in a simulation, a navigation pipeline whose transfer to the real\nworld could be done with as few efforts as possible. Given the limited time and\nthe wide range of problematic to be tackled, absolute navigation performances\nwhile important was not the main objective. The emphasis was rather put on\nstudying the sim2real gap which is one the major bottlenecks of modern robotics\nand autonomous navigation. To design the navigation pipeline four main\nchallenges arise; environment, localization, navigation and planning. The\niGibson simulator is picked for its photo-realistic textures and physics\nengine. A topological approach to tackle space representation was picked over\nmetric approaches because they generalize better to new environments and are\nless sensitive to change of conditions. The navigation pipeline is decomposed\nas a localization module, a planning module and a local navigation module.\nThese modules utilize three different networks, an image representation\nextractor, a passage detector and a local policy. The laters are trained on\nspecifically tailored tasks with some associated datasets created for those\nspecific tasks. Localization is the ability for the agent to localize itself\nagainst a specific space representation. It must be reliable, repeatable and\nrobust to a wide variety of transformations. Localization is tackled as an\nimage retrieval task using a deep neural network trained on an auxiliary task\nas a feature descriptor extractor. The local policy is trained with behavioral\ncloning from expert trajectories gathered with ROS navigation stack.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:37:03 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:43:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pietrantoni", "Maxime", ""], ["Chidlovskii", "Boris", ""], ["Silander", "Tomi", ""]]}, {"id": "2011.10307", "submitter": "Margaux Nattaf", "authors": "Margaux Nattaf (G-SCOP), Arnaud Malapert", "title": "Filtering Rules for Flow Time Minimization in a Parallel Machine\n  Scheduling Problem", "comments": null, "journal-ref": "CP 2020: Principles and Practice of Constraint Programming,\n  pp.462-477, 2020", "doi": "10.1007/978-3-030-58475-7_27", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the scheduling of jobs of different families on parallel\nmachines with qualification constraints. Originating from semiconductor\nmanufacturing, this constraint imposes a time threshold between the execution\nof two jobs of the same family. Otherwise, the machine becomes disqualified for\nthis family. The goal is to minimize both the flow time and the number of\ndisqualifications. Recently, an efficient constraint programming model has been\nproposed. However, when priority is given to the flow time objective, the\nefficiency of the model can be improved. This paper uses a polynomial-time\nalgorithm which minimize the flow time for a single machine relaxation where\ndisqualifications are not considered. Using this algorithm one can derived\nfiltering rules on different variables of the model. Experimental results are\npresented showing the effectiveness of these rules. They improve the\ncompetitiveness with the mixed integer linear program of the literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:00:14 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Nattaf", "Margaux", "", "G-SCOP"], ["Malapert", "Arnaud", ""]]}, {"id": "2011.10341", "submitter": "Raca Todosijevic", "authors": "El-Ghazali Talbi and Raca Todosijevic", "title": "Recovery-to-Efficiency: A New Robustness Concept for Multi-objective\n  Optimization under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new robustness concept for uncertain multi-objective\noptimization problems. More precisely, in the paper the so-called\nrecovery-to-efficiency robustness concept is proposed and investigated. Several\napproaches for generating recovery-to-efficiency robust sets in the context of\nmulti-objective optimization are proposed as well. An extensive experimental\nanalysis is performed to disclose differences among robust sets obtained using\ndifferent concepts as well as to deduce some interesting observations. For\ntesting purposes, instances from the bi-objective knapsack problem are\nconsidered.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:10:39 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Talbi", "El-Ghazali", ""], ["Todosijevic", "Raca", ""]]}, {"id": "2011.10363", "submitter": "David Hanson", "authors": "David Hanson, Frankie Storm, Wenwei Huang, Vytas Krisciunas, Tiger\n  Darrow, Audrey Brown, Mengna Lei, Matthew Aylett, Adam Pickrell, Sophia the\n  Robot", "title": "SophiaPop: Experiments in Human-AI Collaboration on Popular Music", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A diverse team of engineers, artists, and algorithms, collaborated to create\nsongs for SophiaPop, via various neural networks, robotics technologies, and\nartistic tools, and animated the results on Sophia the Robot, a robotic\ncelebrity and animated character. Sophia is a platform for arts, research, and\nother uses. To advance the art and technology of Sophia, we combine various AI\nwith a fictional narrative of her burgeoning career as a popstar. Her actual\nAI-generated pop lyrics, music, and paintings, and animated conversations\nwherein she interacts with humans real-time in narratives that discuss her\nexperiences. To compose the music, SophiaPop team built corpora from human and\nAI-generated Sophia character personality content, along with pop music song\nforms, to train and provide seeds for a number of AI algorithms including\nexpert models, and custom-trained transformer neural networks, which then\ngenerated original pop-song lyrics and melodies. Our musicians including\nFrankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations\nof the AI-generated musical content, including singing and instrumentation. The\nhuman-performed singing data then was processed by a neural-network-based\nSophia voice, which was custom-trained from human performances by Cereproc.\nThis AI then generated the unique Sophia voice singing of the songs. Then we\nanimated Sophia to sing the songs in music videos, using a variety of animation\ngenerators and human-generated animations. Being algorithms and humans, working\ntogether, SophiaPop represents a human-AI collaboration, aspiring toward human\nAI symbiosis. We believe that such a creative convergence of multiple\ndisciplines with humans and AI working together, can make AI relevant to human\nculture in new and exciting ways, and lead to a hopeful vision for the future\nof human-AI relations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:05:50 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hanson", "David", ""], ["Storm", "Frankie", ""], ["Huang", "Wenwei", ""], ["Krisciunas", "Vytas", ""], ["Darrow", "Tiger", ""], ["Brown", "Audrey", ""], ["Lei", "Mengna", ""], ["Aylett", "Matthew", ""], ["Pickrell", "Adam", ""], ["Robot", "Sophia the", ""]]}, {"id": "2011.10364", "submitter": "Mohamadreza Faridghasemnia", "authors": "Mohamadreza Faridghasemnia, Daniele Nardi, Alessandro Saffiotti", "title": "Towards Abstract Relational Learning in Human Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have a rich representation of the entities in their environment.\nEntities are described by their attributes, and entities that share attributes\nare often semantically related. For example, if two books have \"Natural\nLanguage Processing\" as the value of their `title' attribute, we can expect\nthat their `topic' attribute will also be equal, namely, \"NLP\". Humans tend to\ngeneralize such observations, and infer sufficient conditions under which the\n`topic' attribute of any entity is \"NLP\". If robots need to interact\nsuccessfully with humans, they need to represent entities, attributes, and\ngeneralizations in a similar way. This ends in a contextualized cognitive agent\nthat can adapt its understanding, where context provides sufficient conditions\nfor a correct understanding. In this work, we address the problem of how to\nobtain these representations through human-robot interaction. We integrate\nvisual perception and natural language input to incrementally build a semantic\nmodel of the world, and then use inductive reasoning to infer logical rules\nthat capture generic semantic relations, true in this model. These relations\ncan be used to enrich the human-robot interaction, to populate a knowledge base\nwith inferred facts, or to remove uncertainty in the robot's sensory inputs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:06:46 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Faridghasemnia", "Mohamadreza", ""], ["Nardi", "Daniele", ""], ["Saffiotti", "Alessandro", ""]]}, {"id": "2011.10367", "submitter": "Neus Llop Torrent", "authors": "Neus Llop Torrent (1 and 2), Giorgio Visani (2 and 3), Enrico Bagli\n  (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A,\n  (3) University of Bologna School of Informatics and Engineering)", "title": "PSD2 Explainable AI Model for Credit Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to develop and test advanced analytical methods to\nimprove the prediction accuracy of Credit Risk Models, preserving at the same\ntime the model interpretability. In particular, the project focuses on applying\nan explainable machine learning model to PSD2-related databases. The input data\nwere obtained solely from synthetic account transactions generated from a pool\nof commercial banks from a pool of Italian commercial banks. Over the total\nproven models, CatBoost has shown the highest performance. The algorithm\nimplementation produces a GINI of 0.45 after tuning the hyper-parameters\ncombined with their inherent class-weight resampling method. SHAP package is\nused to provide a global and local interpretation of the model predictions to\nformulate a human-comprehensive approach to understanding the decision-maker\nalgorithm. The 20 most important features are selected using the Shapley values\nto present a full human-understandable model that reveals how the attributes of\nan individual are related to its model prediction.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:12:38 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 14:31:10 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Torrent", "Neus Llop", "", "1 and 2"], ["Visani", "Giorgio", "", "2 and 3"], ["Bagli", "Enrico", ""]]}, {"id": "2011.10381", "submitter": "Oh Kwanseok", "authors": "Kwanseok Oh, Jee Seok Yoon, Heung-Il Suk", "title": "Born Identity Network: Multi-way Counterfactual Map Generation to\n  Explain a Classifier's Decision", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists an apparent negative correlation between performance and\ninterpretability of deep learning models. In an effort to reduce this negative\ncorrelation, we propose a Born Identity Network (BIN), which is a post-hoc\napproach for producing multi-way counterfactual maps. A counterfactual map\ntransforms an input sample to be conditioned and classified as a target label,\nwhich is similar to how humans process knowledge through counterfactual\nthinking. For example, a counterfactual map can localize hypothetical\nabnormalities from a normal brain image that may cause it to be diagnosed with\na disease. Specifically, our proposed BIN consists of two core components:\nCounterfactual Map Generator and Target Attribution Network. The Counterfactual\nMap Generator is a variation of conditional GAN which can synthesize a\ncounterfactual map conditioned on an arbitrary target label. The Target\nAttribution Network provides adequate assistance for generating synthesized\nmaps by conditioning a target label into the Counterfactual Map Generator. We\nhave validated our proposed BIN in qualitative and quantitative analysis on\nMNIST, 3D Shapes, and ADNI datasets, and showed the comprehensibility and\nfidelity of our method from various ablation studies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:43:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:09:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 03:53:20 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 05:24:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Oh", "Kwanseok", ""], ["Yoon", "Jee Seok", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2011.10396", "submitter": "Xiang Fang", "authors": "Xiang Fang, Yuchong Hu", "title": "Double Self-weighted Multi-view Clustering via Adaptive View Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering has been applied in many real-world applications where\noriginal data often contain noises. Some graph-based multi-view clustering\nmethods have been proposed to try to reduce the negative influence of noises.\nHowever, previous graph-based multi-view clustering methods treat all features\nequally even if there are redundant features or noises, which is obviously\nunreasonable. In this paper, we propose a novel multi-view clustering framework\nDouble Self-weighted Multi-view Clustering (DSMC) to overcome the\naforementioned deficiency. DSMC performs double self-weighted operations to\nremove redundant features and noises from each graph, thereby obtaining robust\ngraphs. For the first self-weighted operation, it assigns different weights to\ndifferent features by introducing an adaptive weight matrix, which can\nreinforce the role of the important features in the joint representation and\nmake each graph robust. For the second self-weighting operation, it weights\ndifferent graphs by imposing an adaptive weight factor, which can assign larger\nweights to more robust graphs. Furthermore, by designing an adaptive multiple\ngraphs fusion, we can fuse the features in the different graphs to integrate\nthese graphs for clustering. Experiments on six real-world datasets demonstrate\nits advantages over other state-of-the-art multi-view clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:23:01 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Fang", "Xiang", ""], ["Hu", "Yuchong", ""]]}, {"id": "2011.10452", "submitter": "Zachary Ravichandran", "authors": "Zachary Ravichandran, J. Daniel Griffith, Benjamin Smith, and Costas\n  Frost", "title": "Bridging Scene Understanding and Task Execution with Flexible Simulation\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in scene understanding which seeks to\nbuild 3D, metric and object-oriented representations of the world.\nConcurrently, reinforcement learning has made impressive strides largely\nenabled by advances in simulation. Comparatively, there has been less focus in\nsimulation for perception algorithms. Simulation is becoming increasingly vital\nas sophisticated perception approaches such as metric-semantic mapping or 3D\ndynamic scene graph generation require precise 3D, 2D, and inertial information\nin an interactive environment. To that end, we present TESSE (Task Execution\nwith Semantic Segmentation Environments), an open source simulator for\ndeveloping scene understanding and task execution algorithms. TESSE has been\nused to develop state-of-the-art solutions for metric-semantic mapping and 3D\ndynamic scene graph generation. Additionally, TESSE served as the platform for\nthe GOSEEK Challenge at the International Conference of Robotics and Automation\n(ICRA) 2020, an object search competition with an emphasis on reinforcement\nlearning. Code for TESSE is available at https://github.com/MIT-TESSE.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:29:23 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ravichandran", "Zachary", ""], ["Griffith", "J. Daniel", ""], ["Smith", "Benjamin", ""], ["Frost", "Costas", ""]]}, {"id": "2011.10482", "submitter": "Giuseppe Fenza", "authors": "Alfonso Di Pace and Giuseppe Fenza and Mariacristina Gallo and\n  Vincenzo Loia and Aldo Meglio and Francesco Orciuoli", "title": "Implementing the Cognition Level for Industry 4.0 by integrating\n  Augmented Reality and Manufacturing Execution Systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-44041-1_83", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the current industrial practices, the exponential growth in terms of\navailability and affordability of sensors, data acquisition systems, and\ncomputer networks forces factories to move toward implementing high integrating\nCyber-Physical Systems (CPS) with production, logistics, and services. This\ntransforms today's factories into Industry 4.0 factories with significant\neconomic potential. Industry 4.0, also known as the fourth Industrial\nRevolution, levers on the integration of cyber technologies, the Internet of\nThings, and Services. This paper proposes an Augmented Reality (AR)-based\nsystem that creates a Cognition Level that integrates existent Manufacturing\nExecution Systems (MES) to CPS. The idea is to highlight the opportunities\noffered by AR technologies to CPS by describing an application scenario. The\nsystem, analyzed in a real factory, shows its capacity to integrate physical\nand digital worlds strongly. Furthermore, the conducted survey (based on the\nSituation Awareness Global Assessment Technique method) reveals significant\nadvantages in terms of production monitoring, progress, and workers' Situation\nAwareness in general.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:53:13 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Di Pace", "Alfonso", ""], ["Fenza", "Giuseppe", ""], ["Gallo", "Mariacristina", ""], ["Loia", "Vincenzo", ""], ["Meglio", "Aldo", ""], ["Orciuoli", "Francesco", ""]]}, {"id": "2011.10510", "submitter": "M Quamer Nasim", "authors": "M Quamer Nasim, Tannistha Maiti, Ayush Srivastava, Tarry Singh, and\n  Jie Mei", "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach", "comments": "18 pages, 10 figures, 5 tables, and supplementary material included\n  in the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:09:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:27:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nasim", "M Quamer", ""], ["Maiti", "Tannistha", ""], ["Srivastava", "Ayush", ""], ["Singh", "Tarry", ""], ["Mei", "Jie", ""]]}, {"id": "2011.10549", "submitter": "Ankith Mohan", "authors": "Ankith Mohan, Aiichiro Nakano, Emilio Ferrara", "title": "Graph Signal Recovery Using Restricted Boltzmann Machines", "comments": "Paper: 27 pages, 9 figures. Appendix: 5 pages, 12 figures. Submitted\n  to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a model-agnostic pipeline to recover graph signals from an expert\nsystem by exploiting the content addressable memory property of restricted\nBoltzmann machine and the representational ability of a neural network. The\nproposed pipeline requires the deep neural network that is trained on a\ndownward machine learning task with clean data, data which is free from any\nform of corruption or incompletion. We show that denoising the representations\nlearned by the deep neural networks is usually more effective than denoising\nthe data itself. Although this pipeline can deal with noise in any dataset, it\nis particularly effective for graph-structured datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 18:43:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mohan", "Ankith", ""], ["Nakano", "Aiichiro", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2011.10568", "submitter": "Nishant Sinha", "authors": "Azhar Shaikh, Nishant Sinha", "title": "Learn to Bind and Grow Neural Structures", "comments": "Accepted to 8th ACM IKDD CODS and 26th COMAD (CODS-COMAD '21)\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-incremental learning involves the challenging problem of learning new\ntasks continually, without forgetting past knowledge. Many approaches address\nthe problem by expanding the structure of a shared neural network as tasks\narrive, but struggle to grow optimally, without losing past knowledge. We\npresent a new framework, Learn to Bind and Grow, which learns a neural\narchitecture for a new task incrementally, either by binding with layers of a\nsimilar task or by expanding layers which are more likely to conflict between\ntasks. Central to our approach is a novel, interpretable, parameterization of\nthe shared, multi-task architecture space, which then enables computing\nglobally optimal architectures using Bayesian optimization. Experiments on\ncontinual learning benchmarks show that our framework performs comparably with\nearlier expansion based approaches and is able to flexibly compute multiple\noptimal solutions with performance-size trade-offs.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:40:26 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shaikh", "Azhar", ""], ["Sinha", "Nishant", ""]]}, {"id": "2011.10577", "submitter": "Luisa Lucie-Smith", "authors": "Luisa Lucie-Smith, Hiranya V. Peiris, Andrew Pontzen, Brian Nord,\n  Jeyan Thiyagalingam", "title": "Deep learning insights into cosmological structure formation", "comments": "15 pages, 6 figures, to be submitted to Nature Communications,\n  comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the evolution of linear initial conditions present in the early\nuniverse into extended halos of dark matter at late times can be computed using\ncosmological simulations, a theoretical understanding of this complex process\nremains elusive. Here, we build a deep learning framework to learn this\nnon-linear relationship, and develop techniques to physically interpret the\nlearnt mapping. A three-dimensional convolutional neural network (CNN) is\ntrained to predict the mass of dark matter halos from the initial conditions.\nWe find no change in the predictive accuracy of the model if we retrain the\nmodel removing anisotropic information from the inputs. This suggests that the\nfeatures learnt by the CNN are equivalent to spherical averages over the\ninitial conditions. Our results indicate that interpretable deep learning\nframeworks can provide a powerful tool for extracting insight into cosmological\nstructure formation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 19:00:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lucie-Smith", "Luisa", ""], ["Peiris", "Hiranya V.", ""], ["Pontzen", "Andrew", ""], ["Nord", "Brian", ""], ["Thiyagalingam", "Jeyan", ""]]}, {"id": "2011.10615", "submitter": "Tom Grimes", "authors": "Tom Grimes, Eric Church, William Pitts, Lynn Wood, Eva Brayfindley,\n  Luke Erikson, Mark Greaves", "title": "Adversarial Training for EM Classification Networks", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel variant of Domain Adversarial Networks with impactful\nimprovements to the loss functions, training paradigm, and hyperparameter\noptimization. New loss functions are defined for both forks of the DANN\nnetwork, the label predictor and domain classifier, in order to facilitate more\nrapid gradient descent, provide more seamless integration into modern neural\nnetworking frameworks, and allow previously unavailable inferences into network\nbehavior. Using these loss functions, it is possible to extend the concept of\n'domain' to include arbitrary user defined labels applicable to subsets of the\ntraining data, the test data, or both. As such, the network can be operated in\neither 'On the Fly' mode where features provided by the feature extractor\nindicative of differences between 'domain' labels in the training data are\nremoved or in 'Test Collection Informed' mode where features indicative of\ndifference between 'domain' labels in the combined training and test data are\nremoved (without needing to know or provide test activity labels to the\nnetwork). This work also draws heavily from previous works on Robust Training\nwhich draws training examples from a L_inf ball around the training data in\norder to remove fragile features induced by random fluctuations in the data. On\nthese networks we explore the process of hyperparameter optimization for both\nthe domain adversarial and robust hyperparameters. Finally, this network is\napplied to the construction of a binary classifier used to identify the\npresence of EM signal emitted by a turbopump. For this example, the effect of\nthe robust and domain adversarial training is to remove features indicative of\nthe difference in background between instances of operation of the device -\nproviding highly discriminative features on which to construct the classifier.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 20:11:58 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Grimes", "Tom", ""], ["Church", "Eric", ""], ["Pitts", "William", ""], ["Wood", "Lynn", ""], ["Brayfindley", "Eva", ""], ["Erikson", "Luke", ""], ["Greaves", "Mark", ""]]}, {"id": "2011.10640", "submitter": "Michael Gr. Voskoglou Prof. Dr.", "authors": "Michael Voskoglou", "title": "Assessment and Linear Programming under Fuzzy Conditions", "comments": "19 pages, 3 figures", "journal-ref": "Journal of Fuzzy Extension and Applications, 1(3), 198-216, 2020", "doi": "10.22105/jfea.2020.253436.1024", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers\nfor evaluating a group's mean performance, when qualitative grades instead of\nnumerical scores are used for assessing its members' individual performance.\nAlso, a new technique is developed for solving Linear Programming problems with\nfuzzy coefficients and everyday life applications are presented to illustrate\nour results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:13:36 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Voskoglou", "Michael", ""]]}, {"id": "2011.10647", "submitter": "Krunal Shah", "authors": "Krunal Shah, Nitish Gupta, Dan Roth", "title": "What do we expect from Multiple-choice QA Systems?", "comments": "Findings of EMNLP 2020", "journal-ref": "Findings of the Association for Computational Linguistics: EMNLP\n  2020 pg. 3547-3553", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of machine learning systems on various QA datasets could\nbe interpreted as a significant improvement in models' language understanding\nabilities. However, using various perturbations, multiple recent works have\nshown that good performance on a dataset might not indicate performance that\ncorrelates well with human's expectations from models that \"understand\"\nlanguage. In this work we consider a top performing model on several Multiple\nChoice Question Answering (MCQA) datasets, and evaluate it against a set of\nexpectations one might have from such a model, using a series of\nzero-information perturbations of the model's inputs. Our results show that the\nmodel clearly falls short of our expectations, and motivates a modified\ntraining approach that forces the model to better attend to the inputs. We show\nthat the new training paradigm leads to a model that performs on par with the\noriginal model while better satisfying our expectations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:27:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shah", "Krunal", ""], ["Gupta", "Nitish", ""], ["Roth", "Dan", ""]]}, {"id": "2011.10669", "submitter": "James Hare", "authors": "James Z. Hare, Cesar A. Uribe, Lance Kaplan, Ali Jadbabaie", "title": "A General Framework for Distributed Inference with Uncertain Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of distributed classification with a network\nof heterogeneous agents. The agents seek to jointly identify the underlying\ntarget class that best describes a sequence of observations. The problem is\nfirst abstracted to a hypothesis-testing framework, where we assume that the\nagents seek to agree on the hypothesis (target class) that best matches the\ndistribution of observations. Non-Bayesian social learning theory provides a\nframework that solves this problem in an efficient manner by allowing the\nagents to sequentially communicate and update their beliefs for each hypothesis\nover the network. Most existing approaches assume that agents have access to\nexact statistical models for each hypothesis. However, in many practical\napplications, agents learn the likelihood models based on limited data, which\ninduces uncertainty in the likelihood function parameters. In this work, we\nbuild upon the concept of uncertain models to incorporate the agents'\nuncertainty in the likelihoods by identifying a broad set of parametric\ndistribution that allows the agents' beliefs to converge to the same result as\na centralized approach. Furthermore, we empirically explore extensions to\nnon-parametric models to provide a generalized framework of uncertain models in\nnon-Bayesian social learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 22:17:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hare", "James Z.", ""], ["Uribe", "Cesar A.", ""], ["Kaplan", "Lance", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "2011.10672", "submitter": "Johannes Schneider", "authors": "Johannes Schneider and Rene Abraham and Christian Meske", "title": "AI Governance for Businesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) governance regulates the exercise of authority\nand control over the management of AI. It aims at leveraging AI through\neffective use of data and minimization of AI-related cost and risk. While\ntopics such as AI governance and AI ethics are thoroughly discussed on a\ntheoretical, philosophical, societal and regulatory level, there is limited\nwork on AI governance targeted to companies and corporations. This work views\nAI products as systems, where key functionality is delivered by machine\nlearning (ML) models leveraging (training) data. We derive a conceptual\nframework by synthesizing literature on AI and related fields such as ML. Our\nframework decomposes AI governance into governance of data, (ML) models and\n(AI) systems along four dimensions. It relates to existing IT and data\ngovernance frameworks and practices. It can be adopted by practitioners and\nacademics alike. For practitioners the synthesis of mainly research papers, but\nalso practitioner publications and publications of regulatory bodies provides a\nvaluable starting point to implement AI governance, while for academics the\npaper highlights a number of areas of AI governance that deserve more\nattention.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 22:31:37 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Schneider", "Johannes", ""], ["Abraham", "Rene", ""], ["Meske", "Christian", ""]]}, {"id": "2011.10678", "submitter": "Alireza Zareian", "authors": "Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang", "title": "Open-Vocabulary Object Detection Using Captions", "comments": "To be presented at CVPR 2021 (oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the remarkable accuracy of deep neural networks in object detection,\nthey are costly to train and scale due to supervision requirements.\nParticularly, learning more object categories typically requires proportionally\nmore bounding box annotations. Weakly supervised and zero-shot learning\ntechniques have been explored to scale object detectors to more categories with\nless supervision, but they have not been as successful and widely adopted as\nsupervised models. In this paper, we put forth a novel formulation of the\nobject detection problem, namely open-vocabulary object detection, which is\nmore general, more practical, and more effective than weakly supervised and\nzero-shot approaches. We propose a new method to train object detectors using\nbounding box annotations for a limited set of object categories, as well as\nimage-caption pairs that cover a larger variety of objects at a significantly\nlower cost. We show that the proposed method can detect and localize objects\nfor which no bounding box annotation is provided during training, at a\nsignificantly higher accuracy than zero-shot approaches. Meanwhile, objects\nwith bounding box annotation can be detected almost as accurately as supervised\nmethods, which is significantly better than weakly supervised baselines.\nAccordingly, we establish a new state of the art for scalable object detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 23:05:46 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 18:45:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zareian", "Alireza", ""], ["Rosa", "Kevin Dela", ""], ["Hu", "Derek Hao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2011.10687", "submitter": "Gowri Somanath", "authors": "Gowri Somanath and Daniel Kurz", "title": "HDR Environment Map Estimation for Real-Time Augmented Reality", "comments": "Supplementary video at\n  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4\n  Code at https://github.com/apple/ml-envmapnet Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:01:53 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 19:32:32 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:32:49 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 22:15:31 GMT"}, {"version": "v5", "created": "Tue, 27 Jul 2021 20:48:22 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Somanath", "Gowri", ""], ["Kurz", "Daniel", ""]]}, {"id": "2011.10698", "submitter": "Shihong Fang", "authors": "Shihong Fang, Anna Choromanska", "title": "Backdoor Attacks on the DNN Interpretation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is crucial to understand the inner workings of deep neural\nnetworks (DNNs) and many interpretation methods generate saliency maps that\nhighlight parts of the input image that contribute the most to the prediction\nmade by the DNN. In this paper we design a backdoor attack that alters the\nsaliency map produced by the network for an input image only with injected\ntrigger that is invisible to the naked eye while maintaining the prediction\naccuracy. The attack relies on injecting poisoned data with a trigger into the\ntraining data set. The saliency maps are incorporated in the penalty term of\nthe objective function that is used to train a deep model and its influence on\nmodel training is conditioned upon the presence of a trigger. We design two\ntypes of attacks: targeted attack that enforces a specific modification of the\nsaliency map and untargeted attack when the importance scores of the top pixels\nfrom the original saliency map are significantly reduced. We perform empirical\nevaluation of the proposed backdoor attacks on gradient-based and gradient-free\ninterpretation methods for a variety of deep learning architectures. We show\nthat our attacks constitute a serious security threat when deploying deep\nlearning models developed by untrusty sources. Finally, in the Supplement we\ndemonstrate that the proposed methodology can be used in an inverted setting,\nwhere the correct saliency map can be obtained only in the presence of a\ntrigger (key), effectively making the interpretation system available only to\nselected users.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:54:45 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:49:42 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fang", "Shihong", ""], ["Choromanska", "Anna", ""]]}, {"id": "2011.10704", "submitter": "Weixin Liang", "authors": "Weixin Liang, James Zou", "title": "Neural Group Testing to Accelerate Deep Learning", "comments": "ISIT 2021. Code & data available at\n  https://github.com/Weixin-Liang/NeuralGroupTesting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have made the use of large, deep neural\nnetworks with tens of millions of parameters. The sheer size of these networks\nimposes a challenging computational burden during inference. Existing work\nfocuses primarily on accelerating each forward pass of a neural network.\nInspired by the group testing strategy for efficient disease testing, we\npropose neural group testing, which accelerates by testing a group of samples\nin one forward pass. Groups of samples that test negative are ruled out. If a\ngroup tests positive, samples in that group are then retested adaptively. A key\nchallenge of neural group testing is to modify a deep neural network so that it\ncould test multiple samples in one forward pass. We propose three designs to\nachieve this without introducing any new parameters and evaluate their\nperformances. We applied neural group testing in an image moderation task to\ndetect rare but inappropriate images. We found that neural group testing can\ngroup up to 16 images in one forward pass and reduce the overall computation\ncost by over 73% while improving detection performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 02:23:54 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 23:03:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liang", "Weixin", ""], ["Zou", "James", ""]]}, {"id": "2011.10707", "submitter": "Sarath Sreedharan", "authors": "Sarath Sreedharan, Tathagata Chakraborti, Yara Rizk and Yasaman\n  Khazaeni", "title": "Explainable Composition of Aggregated Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new design of an AI assistant that has become increasingly popular is that\nof an \"aggregated assistant\" -- realized as an orchestrated composition of\nseveral individual skills or agents that can each perform atomic tasks. In this\npaper, we will talk about the role of planning in the automated composition of\nsuch assistants and explore how concepts in automated planning can help to\nestablish transparency of the inner workings of the assistant to the end-user.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 02:39:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sreedharan", "Sarath", ""], ["Chakraborti", "Tathagata", ""], ["Rizk", "Yara", ""], ["Khazaeni", "Yasaman", ""]]}, {"id": "2011.10714", "submitter": "Elahe Aghapour", "authors": "Elahe Aghapour, Nora Ayanian", "title": "Double Meta-Learning for Data Efficient Policy Optimization in\n  Non-Stationary Environments", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning models of non-stationary environments, which\ncan be framed as a multi-task learning problem. Model-free reinforcement\nlearning algorithms can achieve good asymptotic performance in multi-task\nlearning at a cost of extensive sampling, due to their approach, which requires\nlearning from scratch. While model-based approaches are among the most data\nefficient learning algorithms, they still struggle with complex tasks and model\nuncertainties. Meta-reinforcement learning addresses the efficiency and\ngeneralization challenges on multi task learning by quickly leveraging the\nmeta-prior policy for a new task. In this paper, we propose a\nmeta-reinforcement learning approach to learn the dynamic model of a\nnon-stationary environment to be used for meta-policy optimization later. Due\nto the sample efficiency of model-based learning methods, we are able to\nsimultaneously train both the meta-model of the non-stationary environment and\nthe meta-policy until dynamic model convergence. Then, the meta-learned dynamic\nmodel of the environment will generate simulated data for meta-policy\noptimization. Our experiment demonstrates that our proposed method can\nmeta-learn the policy in a non-stationary environment with the data efficiency\nof model-based learning approaches while achieving the high asymptotic\nperformance of model-free meta-reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 03:19:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Aghapour", "Elahe", ""], ["Ayanian", "Nora", ""]]}, {"id": "2011.10715", "submitter": "Kyungmin Kim", "authors": "Seungjae Jung, Kyung-Min Kim, Hanock Kwak and Young-Jin Park", "title": "A Worrying Analysis of Probabilistic Time-series Models for Sales\n  Forecasting", "comments": "NeurIPS 2020 workshop (I Can't Believe It's Not Better,\n  ICBINB@NeurIPS 2020). All authors contributed equally to this research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic time-series models become popular in the forecasting field as\nthey help to make optimal decisions under uncertainty. Despite the growing\ninterest, a lack of thorough analysis hinders choosing what is worth applying\nfor the desired task. In this paper, we analyze the performance of three\nprominent probabilistic time-series models for sales forecasting. To remove the\nrole of random chance in architecture's performance, we make two experimental\nprinciples; 1) Large-scale dataset with various cross-validation sets. 2) A\nstandardized training and hyperparameter selection. The experimental results\nshow that a simple Multi-layer Perceptron and Linear Regression outperform the\nprobabilistic models on RMSE without any feature engineering. Overall, the\nprobabilistic models fail to achieve better performance on point estimation,\nsuch as RMSE and MAPE, than comparably simple baselines. We analyze and discuss\nthe performances of probabilistic time-series models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 03:31:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jung", "Seungjae", ""], ["Kim", "Kyung-Min", ""], ["Kwak", "Hanock", ""], ["Park", "Young-Jin", ""]]}, {"id": "2011.10731", "submitter": "Weixin Liang", "authors": "Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan\n  Tur", "title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular\n  Supervision for Visual Question Answering", "comments": "NeurIPS KR2ML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant approach to visual question answering (VQA) relies on\nencoding the image and question with a \"black-box\" neural encoder and decoding\na single token as the answer like \"yes\" or \"no\". Despite this approach's strong\nquantitative results, it struggles to come up with intuitive, human-readable\nforms of justification for the prediction process. To address this\ninsufficiency, we reformulate VQA as a full answer generation task, which\nrequires the model to justify its predictions in natural language. We propose\nLRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning\nframework for visual question answering that solves the problem step-by-step\nlike humans and provides human-readable form of justification at each step.\nSpecifically, LRTA learns to first convert an image into a scene graph and\nparse a question into multiple reasoning instructions. It then executes the\nreasoning instructions one at a time by traversing the scene graph using a\nrecurrent neural-symbolic execution module. Finally, it generates a full answer\nto the given question with natural language justifications. Our experiments on\nGQA dataset show that LRTA outperforms the state-of-the-art model by a large\nmargin (43.1% v.s. 28.0%) on the full answer generation task. We also create a\nperturbed GQA test set by removing linguistic cues (attributes and relations)\nin the questions for analyzing whether a model is having a smart guess with\nsuperficial data correlations. We show that LRTA makes a step towards truly\nunderstanding the question while the state-of-the-art model tends to learn\nsuperficial correlations from the training data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 06:39:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liang", "Weixin", ""], ["Niu", "Feiyang", ""], ["Reganti", "Aishwarya", ""], ["Thattai", "Govind", ""], ["Tur", "Gokhan", ""]]}, {"id": "2011.10737", "submitter": "Jun Ma", "authors": "Zilong Cheng, Jun Ma, Xiaoxue Zhang, Frank L. Lewis, Tong Heng Lee", "title": "Neural Network iLQR: A New Reinforcement Learning Architecture", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a notable machine learning paradigm, the research efforts in the context\nof reinforcement learning have certainly progressed leaps and bounds. When\ncompared with reinforcement learning methods with the given system model, the\nmethodology of the reinforcement learning architecture based on the unknown\nmodel generally exhibits significantly broader universality and applicability.\nIn this work, a new reinforcement learning architecture is developed and\npresented without the requirement of any prior knowledge of the system model,\nwhich is termed as an approach of a \"neural network iterative linear quadratic\nregulator (NNiLQR)\". Depending solely on measurement data, this method yields a\ncompletely new non-parametric routine for the establishment of the optimal\npolicy (without the necessity of system modeling) through iterative refinements\nof the neural network system. Rather importantly, this approach significantly\noutperforms the classical iterative linear quadratic regulator (iLQR) method in\nterms of the given objective function because of the innovative utilization of\nfurther exploration in the methodology. As clearly indicated from the results\nattained in two illustrative examples, these significant merits of the NNiLQR\nmethod are demonstrated rather evidently.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 07:17:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cheng", "Zilong", ""], ["Ma", "Jun", ""], ["Zhang", "Xiaoxue", ""], ["Lewis", "Frank L.", ""], ["Lee", "Tong Heng", ""]]}, {"id": "2011.10753", "submitter": "Avik Pal", "authors": "Avik Pal, Jonah Philion, Yuan-Hong Liao and Sanja Fidler", "title": "Emergent Road Rules In Multi-Agent Driving Environments", "comments": "International Conference on Learning Representations (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For autonomous vehicles to safely share the road with human drivers,\nautonomous vehicles must abide by specific \"road rules\" that human drivers have\nagreed to follow. \"Road rules\" include rules that drivers are required to\nfollow by law -- such as the requirement that vehicles stop at red lights -- as\nwell as more subtle social rules -- such as the implicit designation of fast\nlanes on the highway. In this paper, we provide empirical evidence that\nsuggests that -- instead of hard-coding road rules into self-driving algorithms\n-- a scalable alternative may be to design multi-agent environments in which\nroad rules emerge as optimal solutions to the problem of maximizing traffic\nflow. We analyze what ingredients in driving environments cause the emergence\nof these road rules and find that two crucial factors are noisy perception and\nagents' spatial density. We provide qualitative and quantitative evidence of\nthe emergence of seven social driving behaviors, ranging from obeying traffic\nsignals to following lanes, all of which emerge from training agents to drive\nquickly to destinations without colliding. Our results add empirical support\nfor the social road rules that countries worldwide have agreed on for safe,\nefficient driving.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:43:50 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:29:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pal", "Avik", ""], ["Philion", "Jonah", ""], ["Liao", "Yuan-Hong", ""], ["Fidler", "Sanja", ""]]}, {"id": "2011.10754", "submitter": "Libu\\v{s}e Hannah Vep\\v{r}ek", "authors": "Libu\\v{s}e Hannah Vep\\v{r}ek, Patricia Seymour, Pietro Michelucci", "title": "Human computation requires and enables a new approach to ethical review", "comments": "8 pages, 4 figures. This is a pre-publication draft submitted to 34th\n  Conference on Neural Information Processing Systems (NeurIPS 2020),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With humans increasingly serving as computational elements in distributed\ninformation processing systems and in consideration of the profit-driven\nmotives and potential inequities that might accompany the emerging thinking\neconomy[1], we recognize the need for establishing a set of related ethics to\nensure the fair treatment and wellbeing of online cognitive laborers and the\nconscientious use of the capabilities to which they contribute. Toward this\nend, we first describe human-in-the-loop computing in context of the new\nconcerns it raises that are not addressed by traditional ethical research\nstandards. We then describe shortcomings in the traditional approach to ethical\nreview and introduce a dynamic approach for sustaining an ethical framework\nthat can continue to evolve within the rapidly shifting context of disruptive\nnew technologies.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:44:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vep\u0159ek", "Libu\u0161e Hannah", ""], ["Seymour", "Patricia", ""], ["Michelucci", "Pietro", ""]]}, {"id": "2011.10794", "submitter": "Nandish Chattopadhyay", "authors": "Nandish Chattopadhyay, Lionell Yip En Zhi, Bryan Tan Bing Xing and\n  Anupam Chattopadhyay", "title": "Spatially Correlated Patterns in Adversarial Images", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial attacks have proved to be the major impediment in the progress on\nresearch towards reliable machine learning solutions. Carefully crafted\nperturbations, imperceptible to human vision, can be added to images to force\nmisclassification by an otherwise high performing neural network. To have a\nbetter understanding of the key contributors of such structured attacks, we\nsearched for and studied spatially co-located patterns in the distribution of\npixels in the input space. In this paper, we propose a framework for\nsegregating and isolating regions within an input image which are particularly\ncritical towards either classification (during inference), or adversarial\nvulnerability or both. We assert that during inference, the trained model looks\nat a specific region in the image, which we call Region of Importance (RoI);\nand the attacker looks at a region to alter/modify, which we call Region of\nAttack (RoA). The success of this approach could also be used to design a\npost-hoc adversarial defence method, as illustrated by our observations. This\nuses the notion of blocking out (we call neutralizing) that region of the image\nwhich is highly vulnerable to adversarial attacks but is not important for the\ntask of classification. We establish the theoretical setup for formalising the\nprocess of segregation, isolation and neutralization and substantiate it\nthrough empirical analysis on standard benchmarking datasets. The findings\nstrongly indicate that mapping features into the input space preserves the\nsignificant patterns typically observed in the feature-space while adding major\ninterpretability and therefore simplifies potential defensive mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 14:06:59 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chattopadhyay", "Nandish", ""], ["Zhi", "Lionell Yip En", ""], ["Xing", "Bryan Tan Bing", ""], ["Chattopadhyay", "Anupam", ""]]}, {"id": "2011.10804", "submitter": "Tianchen Zhao", "authors": "Tianchen Zhao, Xuefei Ning, Xiangsheng Shi, Songyi Yang, Shuang Liang,\n  Peng Lei, Jianfei Chen, Huazhong Yang, Yu Wang", "title": "BARS: Joint Search of Cell Topology and Layout for Accurate and\n  Efficient Binary ARchitectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) have received significant attention due to\ntheir promising efficiency. Currently, most BNN studies directly adopt\nwidely-used CNN architectures, which can be suboptimal for BNNs. This paper\nproposes a novel Binary ARchitecture Search (BARS) flow to discover superior\nbinary architecture in a large design space. Specifically, we analyze the\ninformation bottlenecks that are related to both the topology and layout\narchitecture design choices. And we propose to automatically search for the\noptimal information flow. To achieve that, we design a two-level (Macro &\nMicro) search space tailored for BNNs and apply a differentiable neural\narchitecture search (NAS) to explore this search space efficiently. The\nmacro-level search space includes width and depth decisions, which is required\nfor better balancing the model performance and complexity. We also design the\nmicro-level search space to strengthen the information flow for BNN. %A notable\nchallenge of BNN architecture search lies in that binary operations exacerbate\nthe \"collapse\" problem of differentiable NAS, for which we incorporate various\nsearch and derive strategies to stabilize the search process. On CIFAR-10, BARS\nachieves 1.5% higher accuracy with 2/3 binary operations and 1/10\nfloating-point operations comparing with existing BNN NAS studies. On ImageNet,\nwith similar resource consumption, BARS-discovered architecture achieves a 6%\naccuracy gain than hand-crafted binary ResNet-18 architectures and outperforms\nother binary architectures while fully binarizing the architecture backbone.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 14:38:44 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 07:38:32 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 05:54:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Tianchen", ""], ["Ning", "Xuefei", ""], ["Shi", "Xiangsheng", ""], ["Yang", "Songyi", ""], ["Liang", "Shuang", ""], ["Lei", "Peng", ""], ["Chen", "Jianfei", ""], ["Yang", "Huazhong", ""], ["Wang", "Yu", ""]]}, {"id": "2011.10824", "submitter": "Adish Singla", "authors": "Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, Adish Singla", "title": "Policy Teaching in Reinforcement Learning via Environment Poisoning\n  Attacks", "comments": "Journal version of ICML'20 paper. New theoretical results for jointly\n  poisoning rewards and transitions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a security threat to reinforcement learning where an attacker\npoisons the learning environment to force the agent into executing a target\npolicy chosen by the attacker. As a victim, we consider RL agents whose\nobjective is to find a policy that maximizes reward in infinite-horizon problem\nsettings. The attacker can manipulate the rewards and the transition dynamics\nin the learning environment at training-time, and is interested in doing so in\na stealthy manner. We propose an optimization framework for finding an optimal\nstealthy attack for different measures of attack cost. We provide lower/upper\nbounds on the attack cost, and instantiate our attacks in two settings: (i) an\noffline setting where the agent is doing planning in the poisoned environment,\nand (ii) an online setting where the agent is learning a policy with poisoned\nfeedback. Our results show that the attacker can easily succeed in teaching any\ntarget policy to the victim under mild conditions and highlight a significant\nsecurity threat to reinforcement learning agents in practice.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 16:54:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rakhsha", "Amin", ""], ["Radanovic", "Goran", ""], ["Devidze", "Rati", ""], ["Zhu", "Xiaojin", ""], ["Singla", "Adish", ""]]}, {"id": "2011.10831", "submitter": "AbdElRahman ElSaid", "authors": "AbdElRahman ElSaid, Joshua Karns, Zimeng Lyu, Alexander Ororbia,\n  Travis Desell", "title": "Continuous Ant-Based Neural Topology Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel, nature-inspired neural architecture search\n(NAS) algorithm based on ant colony optimization, Continuous Ant-based Neural\nTopology Search (CANTS), which utilizes synthetic ants that move over a\ncontinuous search space based on the density and distribution of pheromones, is\nstrongly inspired by how ants move in the real world. The paths taken by the\nant agents through the search space are utilized to construct artificial neural\nnetworks (ANNs). This continuous search space allows CANTS to automate the\ndesign of ANNs of any size, removing a key limitation inherent to many current\nNAS algorithms that must operate within structures with a size predetermined by\nthe user. CANTS employs a distributed asynchronous strategy which allows it to\nscale to large-scale high performance computing resources, works with a variety\nof recurrent memory cell structures, and makes use of a communal weight sharing\nstrategy to reduce training time. The proposed procedure is evaluated on three\nreal-world, time series prediction problems in the field of power systems and\ncompared to two state-of-the-art algorithms. Results show that CANTS is able to\nprovide improved or competitive results on all of these problems, while also\nbeing easier to use, requiring half the number of user-specified\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 17:49:44 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["ElSaid", "AbdElRahman", ""], ["Karns", "Joshua", ""], ["Lyu", "Zimeng", ""], ["Ororbia", "Alexander", ""], ["Desell", "Travis", ""]]}, {"id": "2011.10890", "submitter": "Tianrong Chen", "authors": "Tianrong Chen, Ziyi Wang, Ioannis Exarchos, Evangelos A. Theodorou", "title": "Large-Scale Multi-Agent Deep FBSDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a scalable deep learning framework for finding\nMarkovian Nash Equilibria in multi-agent stochastic games using fictitious\nplay. The motivation is inspired by theoretical analysis of Forward Backward\nStochastic Differential Equations (FBSDE) and their implementation in a deep\nlearning setting, which is the source of our algorithm's sample efficiency\nimprovement. By taking advantage of the permutation-invariant property of\nagents in symmetric games, the scalability and performance is further enhanced\nsignificantly. We showcase superior performance of our framework over the\nstate-of-the-art deep fictitious play algorithm on an inter-bank\nlending/borrowing problem in terms of multiple metrics. More importantly, our\napproach scales up to 3000 agents in simulation, a scale which, to the best of\nour knowledge, represents a new state-of-the-art. We also demonstrate the\napplicability of our framework in robotics on a belief space autonomous racing\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 23:00:50 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 18:42:42 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 04:46:01 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Tianrong", ""], ["Wang", "Ziyi", ""], ["Exarchos", "Ioannis", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2011.10897", "submitter": "Hyungjun Park", "authors": "Hyungjun Park, Daiki Min, Jong-hyun Ryu, Dong Gu Choi", "title": "Reinforcement learning with distance-based incentive/penalty (DIP)\n  updates for highly constrained industrial control systems", "comments": "We request withdrawal of this article due to a definition error on\n  methodology and problem definition (Section 3-4; pages 2-5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical reinforcement learning (RL) methods show limited applicability for\nreal-world industrial control problems because industrial systems involve\nvarious constraints and simultaneously require continuous and discrete control.\nTo overcome these challenges, we devise a novel RL algorithm that enables an\nagent to handle a highly constrained action space. This algorithm has two main\nfeatures. First, we devise two distance-based Q-value update schemes, incentive\nupdate and penalty update, in a distance-based incentive/penalty update\ntechnique to enable the agent to decide discrete and continuous actions in the\nfeasible region and to update the value of these types of actions. Second, we\npropose a method for defining the penalty cost as a shadow price-weighted\npenalty. This approach affords two advantages compared to previous methods to\nefficiently induce the agent to not select an infeasible action. We apply our\nalgorithm to an industrial control problem, microgrid system operation, and the\nexperimental results demonstrate its superiority.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 00:26:50 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 08:16:29 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Park", "Hyungjun", ""], ["Min", "Daiki", ""], ["Ryu", "Jong-hyun", ""], ["Choi", "Dong Gu", ""]]}, {"id": "2011.10909", "submitter": "Prashanth Vijayaraghavan", "authors": "Prashanth Vijayaraghavan, Deb Roy", "title": "Video SemNet: Memory-Augmented Video Semantic Network", "comments": "6 pages, NIPS 2017 Workshop Visually-Grounded Interaction and\n  Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stories are a very compelling medium to convey ideas, experiences, social and\ncultural values. Narrative is a specific manifestation of the story that turns\nit into knowledge for the audience. In this paper, we propose a machine\nlearning approach to capture the narrative elements in movies by bridging the\ngap between the low-level data representations and semantic aspects of the\nvisual medium. We present a Memory-Augmented Video Semantic Network, called\nVideo SemNet, to encode the semantic descriptors and learn an embedding for the\nvideo. The model employs two main components: (i) a neural semantic learner\nthat learns latent embeddings of semantic descriptors and (ii) a memory module\nthat retains and memorizes specific semantic patterns from the video. We\nevaluate the video representations obtained from variants of our model on two\ntasks: (a) genre prediction and (b) IMDB Rating prediction. We demonstrate that\nour model is able to predict genres and IMDB ratings with a weighted F-1 score\nof 0.72 and 0.63 respectively. The results are indicative of the\nrepresentational power of our model and the ability of such representations to\nmeasure audience engagement.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 01:36:37 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "2011.10920", "submitter": "Sarath Sreedharan", "authors": "Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, David E.\n  Smith and Subbarao Kambhampati", "title": "A Bayesian Account of Measures of Interpretability in Human-AI\n  Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for the design of interpretable agent behavior consider\ndifferent measures of interpretability in isolation. In this paper we posit\nthat, in the design and deployment of human-aware agents in the real world,\nnotions of interpretability are just some among many considerations; and the\ntechniques developed in isolation lack two key properties to be useful when\nconsidered together: they need to be able to 1) deal with their mutually\ncompeting properties; and 2) an open world where the human is not just there to\ninterpret behavior in one specific form. To this end, we consider three\nwell-known instances of interpretable behavior studied in existing literature\n-- namely, explicability, legibility, and predictability -- and propose a\nrevised model where all these behaviors can be meaningfully modeled together.\nWe will highlight interesting consequences of this unified model and motivate,\nthrough results of a user study, why this revision is necessary.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 03:28:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sreedharan", "Sarath", ""], ["Kulkarni", "Anagha", ""], ["Chakraborti", "Tathagata", ""], ["Smith", "David E.", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2011.10970", "submitter": "El Moatez Billah Nagoudi", "authors": "Muhammad Abdul-Mageed, Shady Elbassuoni, Jad Doughman, AbdelRahim\n  Elmadany, El Moatez Billah Nagoudi, Yorgo Zoughby, Ahmad Shaher, Iskander\n  Gaba, Ahmed Helal, Mohammed El-Razzaz", "title": "DiaLex: A Benchmark for Evaluating Multidialectal Arabic Word Embeddings", "comments": "WANLP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Word embeddings are a core component of modern natural language processing\nsystems, making the ability to thoroughly evaluate them a vital task. We\ndescribe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word\nembedding. DiaLex covers five important Arabic dialects: Algerian, Egyptian,\nLebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a\ntestbank for six syntactic and semantic relations, namely male to female,\nsingular to dual, singular to plural, antonym, comparative, and genitive to\npast tense. DiaLex thus consists of a collection of word pairs representing\neach of the six relations in each of the five dialects. To demonstrate the\nutility of DiaLex, we use it to evaluate a set of existing and new Arabic word\nembeddings that we developed. Our benchmark, evaluation code, and new word\nembedding models will be publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 08:47:52 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 04:02:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Elbassuoni", "Shady", ""], ["Doughman", "Jad", ""], ["Elmadany", "AbdelRahim", ""], ["Nagoudi", "El Moatez Billah", ""], ["Zoughby", "Yorgo", ""], ["Shaher", "Ahmad", ""], ["Gaba", "Iskander", ""], ["Helal", "Ahmed", ""], ["El-Razzaz", "Mohammed", ""]]}, {"id": "2011.10972", "submitter": "Weixia Zhang", "authors": "Weixia Zhang, Chao Ma, Qi Wu and Xiaokang Yang", "title": "Language-guided Navigation via Cross-Modal Grounding and Alternate\n  Adversarial Learning", "comments": "Accepted to IEEE TCSVT", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3039522", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emerging vision-and-language navigation (VLN) problem aims at learning to\nnavigate an agent to the target location in unseen photo-realistic environments\naccording to the given language instruction. The main challenges of VLN arise\nmainly from two aspects: first, the agent needs to attend to the meaningful\nparagraphs of the language instruction corresponding to the dynamically-varying\nvisual environments; second, during the training process, the agent usually\nimitate the shortest-path to the target location. Due to the discrepancy of\naction selection between training and inference, the agent solely on the basis\nof imitation learning does not perform well. Sampling the next action from its\npredicted probability distribution during the training process allows the agent\nto explore diverse routes from the environments, yielding higher success rates.\nNevertheless, without being presented with the shortest navigation paths during\nthe training process, the agent may arrive at the target location through an\nunexpected longer route. To overcome these challenges, we design a cross-modal\ngrounding module, which is composed of two complementary attention mechanisms,\nto equip the agent with a better ability to track the correspondence between\nthe textual and visual modalities. We then propose to recursively alternate the\nlearning schemes of imitation and exploration to narrow the discrepancy between\ntraining and inference. We further exploit the advantages of both these two\nlearning schemes via adversarial learning. Extensive experimental results on\nthe Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning\nscheme is generalized and complementary to prior arts. Our method performs well\nagainst state-of-the-art approaches in terms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 09:13:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Chao", ""], ["Wu", "Qi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2011.10996", "submitter": "Antoine Guillaume", "authors": "Antoine Guillaume, Christel Vrain, Elloumi Wael", "title": "Time series classification for predictive maintenance on event logs", "comments": "19 pages, 9 figures, submitted to ECMLPKDD 2021 Journal Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Time series classification (TSC) gained a lot of attention in the past decade\nand number of methods for representing and classifying time series have been\nproposed. Nowadays, methods based on convolutional networks and ensemble\ntechniques represent the state of the art for time series classification.\nTechniques transforming time series to image or text also provide reliable ways\nto extract meaningful features or representations of time series. We compare\nthe state-of-the-art representation and classification methods on a specific\napplication, that is predictive maintenance from sequences of event logs. The\ncontributions of this paper are twofold: introducing a new data set for\npredictive maintenance on automated teller machines (ATMs) log data and\ncomparing the performance of different representation methods for predicting\nthe occurrence of a breakdown. The problem is difficult since unlike the\nclassic case of predictive maintenance via signals from sensors, we have\nsequences of discrete event logs occurring at any time and the lengths of the\nsequences, corresponding to life cycles, vary a lot.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 12:12:14 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 08:23:45 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 16:33:52 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Guillaume", "Antoine", ""], ["Vrain", "Christel", ""], ["Wael", "Elloumi", ""]]}, {"id": "2011.11058", "submitter": "Shashi Kant Gupta", "authors": "Shivi Gupta, Shashi Kant Gupta", "title": "Investigating Emotion-Color Association in Deep Neural Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been found that representations learned by Deep Neural Networks (DNNs)\ncorrelate very well to neural responses measured in primates' brains and\npsychological representations exhibited by human similarity judgment. On\nanother hand, past studies have shown that particular colors can be associated\nwith specific emotion arousal in humans. Do deep neural networks also learn\nthis behavior? In this study, we investigate if DNNs can learn implicit\nassociations in stimuli, particularly, an emotion-color association between\nimage stimuli. Our study was conducted in two parts. First, we collected human\nresponses on a forced-choice decision task in which subjects were asked to\nselect a color for a specified emotion-inducing image. Next, we modeled this\ndecision task on neural networks using the similarity between deep\nrepresentation (extracted using DNNs trained on object classification tasks) of\nthe images and images of colors used in the task. We found that our model\nshowed a fuzzy linear relationship between the two decision probabilities. This\nresults in two interesting findings, 1. The representations learned by deep\nneural networks can indeed show an emotion-color association 2. The\nemotion-color association is not just random but involves some cognitive\nphenomena. Finally, we also show that this method can help us in the emotion\nclassification task, specifically when there are very few examples to train the\nmodel. This analysis can be relevant to psychologists studying emotion-color\nassociations and artificial intelligence researchers modeling emotional\nintelligence in machines or studying representations learned by deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:48:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gupta", "Shivi", ""], ["Gupta", "Shashi Kant", ""]]}, {"id": "2011.11081", "submitter": "Limin Yu", "authors": "Junli Cao, B.S., Junyan Wu, M.S., Jing W. Zhang, M.D., Ph.D., Jay J.\n  Ye, M.D., Ph.D., Limin Yu, M.D., M.S", "title": "Deep learning model trained on mobile phone-acquired frozen section\n  images effectively detects basal cell carcinoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Margin assessment of basal cell carcinoma using the frozen\nsection is a common task of pathology intraoperative consultation. Although\nfrequently straight-forward, the determination of the presence or absence of\nbasal cell carcinoma on the tissue sections can sometimes be challenging. We\nexplore if a deep learning model trained on mobile phone-acquired frozen\nsection images can have adequate performance for future deployment. Materials\nand Methods: One thousand two hundred and forty-one (1241) images of frozen\nsections performed for basal cell carcinoma margin status were acquired using\nmobile phones. The photos were taken at 100x magnification (10x objective). The\nimages were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel\nresolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone\nwas used for model training. Results: The model uses an image as input and\nproduces a 2-dimensional black and white output of prediction of the same\ndimension; the areas determined to be basal cell carcinoma were displayed with\nwhite color, in a black background. Any output with the number of white pixels\nexceeding 0.5% of the total number of pixels is deemed positive for basal cell\ncarcinoma. On the test set, the model achieves area under curve of 0.99 for\nreceiver operator curve and 0.97 for precision-recall curve at the pixel level.\nThe accuracy of classification at the slide level is 96%. Conclusions: The deep\nlearning model trained with mobile phone images shows satisfactory performance\ncharacteristics, and thus demonstrates the potential for deploying as a mobile\nphone app to assist in frozen section interpretation in real time.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:30:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cao", "Junli", ""], ["S.", "B.", ""], ["Wu", "Junyan", ""], ["S.", "M.", ""], ["Zhang", "Jing W.", ""], ["D.", "M.", ""], ["D.", "Ph.", ""], ["Ye", "Jay J.", ""], ["D.", "M.", ""], ["D.", "Ph.", ""], ["Yu", "Limin", ""], ["D.", "M.", ""], ["S", "M.", ""]]}, {"id": "2011.11151", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers, Danniene Wete and Steffen Staab", "title": "LaHAR: Latent Human Activity Recognition using LDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing sequential multi-sensor data becomes important in many tasks due\nto the dramatic increase in the availability of sensors that can acquire\nsequential data over time. Human Activity Recognition (HAR) is one of the\nfields which are actively benefiting from this availability. Unlike most of the\napproaches addressing HAR by considering predefined activity classes, this\npaper proposes a novel approach to discover the latent HAR patterns in\nsequential data. To this end, we employed Latent Dirichlet Allocation (LDA),\nwhich is initially a topic modelling approach used in text analysis. To make\nthe data suitable for LDA, we extract the so-called \"sensory words\" from the\nsequential data. We carried out experiments on a challenging HAR dataset,\ndemonstrating that LDA is capable of uncovering underlying structures in\nsequential data, which provide a human-understandable representation of the\ndata. The extrinsic evaluations reveal that LDA is capable of accurately\nclustering HAR data sequences compared to the labelled activities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:33:01 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Wete", "Danniene", ""], ["Staab", "Steffen", ""]]}, {"id": "2011.11152", "submitter": "Zeke Xie", "authors": "Zeke Xie, Issei Sato, Masashi Sugiyama", "title": "Stable Weight Decay Regularization", "comments": "20 pages, 18 figures, Weight Decay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight decay is a popular regularization technique for training of deep\nneural networks. Modern deep learning libraries mainly use $L_{2}$\nregularization as the default implementation of weight decay.\n\\citet{loshchilov2018decoupled} demonstrated that $L_{2}$ regularization is not\nidentical to weight decay for adaptive gradient methods, such as Adaptive\nMomentum Estimation (Adam), and proposed Adam with Decoupled Weight Decay\n(AdamW). However, we found that the popular implementations of weight decay,\nincluding $L_{2}$ regularization and decoupled weight decay, in modern deep\nlearning libraries usually damage performance. First, the $L_{2}$\nregularization is unstable weight decay for all optimizers that use Momentum,\nsuch as stochastic gradient descent (SGD). Second, decoupled weight decay is\nhighly unstable for all adaptive gradient methods. We further propose the\nStable Weight Decay (SWD) method to fix the unstable weight decay problem from\na dynamical perspective. The proposed SWD method makes significant improvements\nover $L_{2}$ regularization and decoupled weight decay in our experiments.\nSimply fixing weight decay in Adam by SWD, with no extra hyperparameter, can\nusually outperform complex Adam variants, which have more hyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:39:49 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 05:09:37 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 04:47:42 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Xie", "Zeke", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2011.11201", "submitter": "Wei Yu", "authors": "Wei Yu, Wenxin Chen, Songhenh Yin, Steve Easterbrook, Animesh Garg", "title": "Concept Grounding with Modular Action-Capsules in Semantic Video\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works in video prediction have mainly focused on passive forecasting\nand low-level action-conditional prediction, which sidesteps the learning of\ninteraction between agents and objects. We introduce the task of semantic\naction-conditional video prediction, which uses semantic action labels to\ndescribe those interactions and can be regarded as an inverse problem of action\nrecognition. The challenge of this new task primarily lies in how to\neffectively inform the model of semantic action information. To bridge vision\nand language, we utilize the idea of capsule and propose a novel video\nprediction model, Modular Action Capsule Network (MAC). Our method is evaluated\non two newly designed synthetic datasets, CLEVR-Building-Blocks and\nSapien-Kitchen, and one real-world dataset called TowerCreation. Experiments\nshow that given different action labels, MAC can correctly condition on\ninstructions and generate corresponding future frames without need of bounding\nboxes. We further demonstrate that the trained model can make\nout-of-distribution generalization, be quickly adapted to new object categories\nand exploit its learnt features for object detection, showing the progression\ntowards higher-level cognitive abilities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 04:12:22 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:56:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yu", "Wei", ""], ["Chen", "Wenxin", ""], ["Yin", "Songhenh", ""], ["Easterbrook", "Steve", ""], ["Garg", "Animesh", ""]]}, {"id": "2011.11233", "submitter": "Xiangxiang Chu", "authors": "Xiaoxing Wang and Xiangxiang Chu and Yuda Fan and Zhexi Zhang and\n  Xiaolin Wei and Junchi Yan and Xiaokang Yang", "title": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and\n  Gradients Accumulation", "comments": "Observe new collapse in memory efficient NAS and address it using\n  ROME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-path based differentiable neural architecture search has great\nstrengths for its low computational cost and memory-friendly nature. However,\nwe surprisingly discover that it suffers from severe searching instability\nwhich has been primarily ignored, posing a potential weakness for a wider\napplication. In this paper, we delve into its performance collapse issue and\npropose a new algorithm called RObustifying Memory-Efficient NAS (ROME).\nSpecifically, 1) for consistent topology in the search and evaluation stage, we\ninvolve separate parameters to disentangle the topology from the operations of\nthe architecture. In such a way, we can independently sample connections and\noperations without interference; 2) to discount sampling unfairness and\nvariance, we enforce fair sampling for weight update and apply a gradient\naccumulation mechanism for architecture parameters. Extensive experiments\ndemonstrate that our proposed method has strong performance and robustness,\nwhere it mostly achieves state-of-the-art results on a large number of standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 06:34:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Xiaoxing", ""], ["Chu", "Xiangxiang", ""], ["Fan", "Yuda", ""], ["Zhang", "Zhexi", ""], ["Wei", "Xiaolin", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2011.11261", "submitter": "Zehua Zhang", "authors": "Zehua Zhang and David Crandall", "title": "Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised\n  Video Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel way for self-supervised video representation learning by:\n(a) decoupling the learning objective into two contrastive subtasks\nrespectively emphasizing spatial and temporal features, and (b) performing it\nhierarchically to encourage multi-scale understanding. Motivated by their\neffectiveness in supervised learning, we first introduce spatial-temporal\nfeature learning decoupling and hierarchical learning to the context of\nunsupervised video learning. In particular, our method directs the network to\nseparately capture spatial and temporal features on the basis of contrastive\nlearning via manipulating augmentations as regularization, and further solve\nsuch proxy tasks hierarchically by optimizing towards a compound contrastive\nloss. Experiments show that our proposed Hierarchically Decoupled\nSpatial-Temporal Contrast (HDC) achieves substantial gains over directly\nlearning spatial-temporal features as a whole and significantly outperforms\nother state-of-the-art unsupervised methods on downstream action recognition\nbenchmarks on UCF101 and HMDB51. We will release our code and pretrained\nweights.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:05:39 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Zehua", ""], ["Crandall", "David", ""]]}, {"id": "2011.11266", "submitter": "Miao Yang", "authors": "Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, Hua Qian", "title": "Federated learning with class imbalance reduction", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising technique that enables a large amount\nof edge computing devices to collaboratively train a global learning model. Due\nto privacy concerns, the raw data on devices could not be available for\ncentralized server. Constrained by the spectrum limitation and computation\ncapacity, only a subset of devices can be engaged to train and transmit the\ntrained model to centralized server for aggregation. Since the local data\ndistribution varies among all devices, class imbalance problem arises along\nwith the unfavorable client selection, resulting in a slow converge rate of the\nglobal model. In this paper, an estimation scheme is designed to reveal the\nclass distribution without the awareness of raw data. Based on the scheme, a\ndevice selection algorithm towards minimal class imbalance is proposed, thus\ncan improve the convergence performance of the global model. Simulation results\ndemonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:13:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yang", "Miao", ""], ["Wong", "Akitanoshou", ""], ["Zhu", "Hongbin", ""], ["Wang", "Haifeng", ""], ["Qian", "Hua", ""]]}, {"id": "2011.11278", "submitter": "Dianbo Liu Dr", "authors": "He Zhu and Dianbo Liu", "title": "FakeSafe: Human Level Data Protection by Disinformation Mapping using\n  Cycle-consistent Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of disinformation is to use fake messages to confuse people in\norder to protect the real information. This strategy can be adapted into data\nscience to protect valuable private and sensitive data. Huge amount of private\ndata are being generated from personal devices such as smart phone and wearable\nin recent years. Being able to utilize these personal data will bring big\nopportunities to design personalized products, conduct precision healthcare and\nmany other tasks that were impossible in the past. However, due to privacy,\nsafety and regulation reasons, it is often difficult to transfer or store data\nin its original form while keeping them safe. Building a secure data transfer\nand storage infrastructure to preserving privacy is costly in most cases and\nthere is always a concern of data security due to human errors. In this study,\nwe propose a method, named FakeSafe, to provide human level data protection\nusing generative adversarial network with cycle consistency and conducted\nexperiments using both benchmark and real world data sets to illustrate\npotential applications of FakeSafe.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:47:40 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 04:10:21 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhu", "He", ""], ["Liu", "Dianbo", ""]]}, {"id": "2011.11286", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan", "title": "MEG: Multi-Evidence GNN for Multimodal Semantic Forensics", "comments": "To be published at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news often involves semantic manipulations across modalities such as\nimage, text, location etc and requires the development of multimodal semantic\nforensics for its detection. Recent research has centered the problem around\nimages, calling it image repurposing -- where a digitally unmanipulated image\nis semantically misrepresented by means of its accompanying multimodal metadata\nsuch as captions, location, etc. The image and metadata together comprise a\nmultimedia package. The problem setup requires algorithms to perform multimodal\nsemantic forensics to authenticate a query multimedia package using a reference\ndataset of potentially related packages as evidences. Existing methods are\nlimited to using a single evidence (retrieved package), which ignores potential\nperformance improvement from the use of multiple evidences. In this work, we\nintroduce a novel graph neural network based model for multimodal semantic\nforensics, which effectively utilizes multiple retrieved packages as evidences\nand is scalable with the number of evidences. We compare the scalability and\nperformance of our model against existing methods. Experimental results show\nthat the proposed model outperforms existing state-of-the-art algorithms with\nan error reduction of up to 25%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 09:01:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sabir", "Ekraam", ""], ["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Prem", ""]]}, {"id": "2011.11288", "submitter": "Yaoman Li", "authors": "Yaoman Li and Irwin King", "title": "AutoGraph: Automated Graph Neural Network", "comments": "Accepted by ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphs play an important role in many applications. Recently, Graph Neural\nNetworks (GNNs) have achieved promising results in graph analysis tasks. Some\nstate-of-the-art GNN models have been proposed, e.g., Graph Convolutional\nNetworks (GCNs), Graph Attention Networks (GATs), etc. Despite these successes,\nmost of the GNNs only have shallow structure. This causes the low expressive\npower of the GNNs. To fully utilize the power of the deep neural network, some\ndeep GNNs have been proposed recently. However, the design of deep GNNs\nrequires significant architecture engineering. In this work, we propose a\nmethod to automate the deep GNNs design. In our proposed method, we add a new\ntype of skip connection to the GNNs search space to encourage feature reuse and\nalleviate the vanishing gradient problem. We also allow our evolutionary\nalgorithm to increase the layers of GNNs during the evolution to generate\ndeeper networks. We evaluate our method in the graph node classification task.\nThe experiments show that the GNNs generated by our method can obtain\nstate-of-the-art results in Cora, Citeseer, Pubmed and PPI datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 09:04:17 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Yaoman", ""], ["King", "Irwin", ""]]}, {"id": "2011.11311", "submitter": "Ines Rieger", "authors": "Jessica Deuschel, Bettina Finzel, Ines Rieger", "title": "Uncovering the Bias in Facial Expressions", "comments": "Accepted at the colloquium \"Forschende Frauen\", University of\n  Bamberg, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades the machine and deep learning community has celebrated\ngreat achievements in challenging tasks such as image classification. The deep\narchitecture of artificial neural networks together with the plenitude of\navailable data makes it possible to describe highly complex relations. Yet, it\nis still impossible to fully capture what the deep learning model has learned\nand to verify that it operates fairly and without creating bias, especially in\ncritical tasks, for instance those arising in the medical field. One example\nfor such a task is the detection of distinct facial expressions, called Action\nUnits, in facial images. Considering this specific task, our research aims to\nprovide transparency regarding bias, specifically in relation to gender and\nskin color. We train a neural network for Action Unit classification and\nanalyze its performance quantitatively based on its accuracy and qualitatively\nbased on heatmaps. A structured review of our results indicates that we are\nable to detect bias. Even though we cannot conclude from our results that lower\nclassification performance emerged solely from gender and skin color bias,\nthese biases must be addressed, which is why we end by giving suggestions on\nhow the detected bias can be avoided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:20:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Deuschel", "Jessica", ""], ["Finzel", "Bettina", ""], ["Rieger", "Ines", ""]]}, {"id": "2011.11313", "submitter": "Bipin Kumar Dr.", "authors": "Bipin Kumar, Rajib Chattopadhyay, Manmeet Singh, Niraj Chaudhari,\n  Karthik Kodari and Amit Barve", "title": "Deep-learning based down-scaling of summer monsoon rainfall data over\n  Indian region", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.AI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Downscaling is necessary to generate high-resolution observation data to\nvalidate the climate model forecast or monitor rainfall at the micro-regional\nlevel operationally. Dynamical and statistical downscaling models are often\nused to get information at high-resolution gridded data over larger domains. As\nrainfall variability is dependent on the complex Spatio-temporal process\nleading to non-linear or chaotic Spatio-temporal variations, no single\ndownscaling method can be considered efficient enough. In data with complex\ntopographies, quasi-periodicities, and non-linearities, deep Learning (DL)\nbased methods provide an efficient solution in downscaling rainfall data for\nregional climate forecasting and real-time rainfall observation data at high\nspatial resolutions. In this work, we employed three deep learning-based\nalgorithms derived from the super-resolution convolutional neural network\n(SRCNN) methods, to precipitation data, in particular, IMD and TRMM data to\nproduce 4x-times high-resolution downscaled rainfall data during the summer\nmonsoon season. Among the three algorithms, namely SRCNN, stacked SRCNN, and\nDeepSD, employed here, the best spatial distribution of rainfall amplitude and\nminimum root-mean-square error is produced by DeepSD based downscaling. Hence,\nthe use of the DeepSD algorithm is advocated for future use. We found that\nspatial discontinuity in amplitude and intensity rainfall patterns is the main\nobstacle in the downscaling of precipitation. Furthermore, we applied these\nmethods for model data postprocessing, in particular, ERA5 data. Downscaled\nERA5 rainfall data show a much better distribution of spatial covariance and\ntemporal variance when compared with observation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:24:17 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:00:11 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 09:24:20 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kumar", "Bipin", ""], ["Chattopadhyay", "Rajib", ""], ["Singh", "Manmeet", ""], ["Chaudhari", "Niraj", ""], ["Kodari", "Karthik", ""], ["Barve", "Amit", ""]]}, {"id": "2011.11344", "submitter": "Michael Mommert", "authors": "Michael Mommert, Mario Sigel, Marcel Neuhausler, Linus Scheibenreif,\n  Damian Borth", "title": "Characterization of Industrial Smoke Plumes from Remote Sensing Data", "comments": "To be presented at the \"Tackling Climate Change with Machine\n  Learning\" workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The major driver of global warming has been identified as the anthropogenic\nrelease of greenhouse gas (GHG) emissions from industrial activities. The\nquantitative monitoring of these emissions is mandatory to fully understand\ntheir effect on the Earth's climate and to enforce emission regulations on a\nlarge scale. In this work, we investigate the possibility to detect and\nquantify industrial smoke plumes from globally and freely available multi-band\nimage data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can\ndetect smoke plumes of different sizes with an accuracy of 94.3%. The model\ncorrectly ignores natural clouds and focuses on those imaging channels that are\nrelated to the spectral absorption from aerosols and water vapor, enabling the\nlocalization of smoke. We exploit this localization ability and train a U-Net\nsegmentation model on a labeled sub-sample of our data, resulting in an\nIntersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the\ndetection of any smoke plume of 94.0%; on average, our model can reproduce the\narea covered by smoke in an image to within 5.6%. The performance of our model\nis mostly limited by occasional confusion with surface objects, the inability\nto identify semi-transparent smoke, and human limitations to properly identify\nsmoke based on RGB-only images. Nevertheless, our results enable us to reliably\ndetect and qualitatively estimate the level of smoke activity in order to\nmonitor activity in industrial plants across the globe. Our data set and code\nbase are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 11:54:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mommert", "Michael", ""], ["Sigel", "Mario", ""], ["Neuhausler", "Marcel", ""], ["Scheibenreif", "Linus", ""], ["Borth", "Damian", ""]]}, {"id": "2011.11347", "submitter": "Chenguang Fang", "authors": "Chenguang Fang, Chen Wang", "title": "Time Series Data Imputation: A Survey on Deep Learning Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are all around in real-world applications. However, unexpected\naccidents for example broken sensors or missing of the signals will cause\nmissing values in time series, making the data hard to be utilized. It then\ndoes harm to the downstream applications such as traditional classification or\nregression, sequential data integration and forecasting tasks, thus raising the\ndemand for data imputation. Currently, time series data imputation is a\nwell-studied problem with different categories of methods. However, these works\nrarely take the temporal relations among the observations and treat the time\nseries as normal structured data, losing the information from the time data. In\nrecent, deep learning models have raised great attention. Time series methods\nbased on deep learning have made progress with the usage of models like RNN,\nsince it captures time information from data. In this paper, we mainly focus on\ntime series imputation technique with deep learning methods, which recently\nmade progress in this field. We will review and discuss their model\narchitectures, their pros and cons as well as their effects to show the\ndevelopment of the time series imputation methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 11:57:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fang", "Chenguang", ""], ["Wang", "Chen", ""]]}, {"id": "2011.11358", "submitter": "Alastair Finlinson Mr", "authors": "Alastair Finlinson, Sotiris Moschoyiannis", "title": "Synthesis and Pruning as a Dynamic Compression Strategy for Efficient\n  Deep Neural Networks", "comments": "29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE\n  MANAGEMENT, 9th International Symposium DATAMOD 2020 FROM DATA TO MODELS AND\n  BACK, 16 Pages, 7 Figures, 3 Tables, 2 Equations", "journal-ref": null, "doi": "10.1007/978-3-030-70650-0_1", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a highly reconfigurable machine capable of task-specific\nadaptations. The brain continually rewires itself for a more optimal\nconfiguration to solve problems. We propose a novel strategic synthesis\nalgorithm for feedforward networks that draws directly from the brain's\nbehaviours when learning. The proposed approach analyses the network and ranks\nweights based on their magnitude. Unlike existing approaches that advocate\nrandom selection, we select highly performing nodes as starting points for new\nedges and exploit the Gaussian distribution over the weights to select\ncorresponding endpoints. The strategy aims only to produce useful connections\nand result in a smaller residual network structure. The approach is\ncomplemented with pruning to further the compression. We demonstrate the\ntechniques to deep feedforward networks. The residual sub-networks that are\nformed from the synthesis approaches in this work form common sub-networks with\nsimilarities up to ~90%. Using pruning as a complement to the strategic\nsynthesis approach, we observe improvements in compression.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 12:30:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Finlinson", "Alastair", ""], ["Moschoyiannis", "Sotiris", ""]]}, {"id": "2011.11369", "submitter": "Yilun Lin", "authors": "Yilun Lin, Chaochao Chen, Cen Chen and Li Wang", "title": "Improving Federated Relational Data Modeling via Basis Alignment and\n  Weight Penalty", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) has attracted increasing attention in recent years.\nAs a privacy-preserving collaborative learning paradigm, it enables a broader\nrange of applications, especially for computer vision and natural language\nprocessing tasks. However, to date, there is limited research of federated\nlearning on relational data, namely Knowledge Graph (KG). In this work, we\npresent a modified version of the graph neural network algorithm that performs\nfederated modeling over KGs across different participants. Specifically, to\ntackle the inherent data heterogeneity issue and inefficiency in algorithm\nconvergence, we propose a novel optimization algorithm, named FedAlign, with 1)\noptimal transportation (OT) for on-client personalization and 2) weight\nconstraint to speed up the convergence. Extensive experiments have been\nconducted on several widely used datasets. Empirical results show that our\nproposed method outperforms the state-of-the-art FL methods, such as FedAVG and\nFedProx, with better convergence.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 12:52:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lin", "Yilun", ""], ["Chen", "Chaochao", ""], ["Chen", "Cen", ""], ["Wang", "Li", ""]]}, {"id": "2011.11376", "submitter": "Jacobo Ayensa-Jim\\'enez", "authors": "Jacobo Ayensa-Jim\\'enez, Mohamed H. Doweidar, Jose A. Sanz-Herrera,\n  Manuel Doblar\\'e", "title": "On the application of Physically-Guided Neural Networks with Internal\n  Variables to Continuum Problems", "comments": "40 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive Physics has been historically based upon the development of\nmathematical models that describe the evolution of a system under certain\nexternal stimuli and constraints. The structure of such mathematical models\nrelies on a set of hysical hypotheses that are assumed to be fulfilled by the\nsystem within a certain range of environmental conditions. A new perspective is\nnow raising that uses physical knowledge to inform the data prediction\ncapability of artificial neural networks. A particular extension of this\ndata-driven approach is Physically-Guided Neural Networks with Internal\nVariables (PGNNIV): universal physical laws are used as constraints in the\nneural network, in such a way that some neuron values can be interpreted as\ninternal state variables of the system. This endows the network with unraveling\ncapacity, as well as better predictive properties such as faster convergence,\nfewer data needs and additional noise filtering. Besides, only observable data\nare used to train the network, and the internal state equations may be\nextracted as a result of the training processes, so there is no need to make\nexplicit the particular structure of the internal state model. We extend this\nnew methodology to continuum physical problems, showing again its predictive\nand explanatory capacities when only using measurable values in the training\nset. We show that the mathematical operators developed for image analysis in\ndeep learning approaches can be used and extended to consider standard\nfunctional operators in continuum Physics, thus establishing a common framework\nfor both. The methodology presented demonstrates its ability to discover the\ninternal constitutive state equation for some problems, including heterogeneous\nand nonlinear features, while maintaining its predictive ability for the whole\ndataset coverage, with the cost of a single evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:06:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ayensa-Jim\u00e9nez", "Jacobo", ""], ["Doweidar", "Mohamed H.", ""], ["Sanz-Herrera", "Jose A.", ""], ["Doblar\u00e9", "Manuel", ""]]}, {"id": "2011.11381", "submitter": "Kelvin Kok Fung Li", "authors": "Kelvin K.F. Li, Stephen A. Jarvis, Fayyaz Minhas", "title": "Elementary Effects Analysis of factors controlling COVID-19 infections\n  in computational simulation reveals the importance of Social Distancing and\n  Mask Usage", "comments": "14 pages, 10 figures", "journal-ref": "Computers in Biology and Medicine 134 (2021) 104369", "doi": "10.1016/j.compbiomed.2021.104369", "report-no": null, "categories": "cs.AI cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 was declared a pandemic by the World Health Organization (WHO) on\nMarch 11th, 2020. With half of the world's countries in lockdown as of April\ndue to this pandemic, monitoring and understanding the spread of the virus and\ninfection rates and how these factors relate to behavioural and societal\nparameters is crucial for effective policy making. This paper aims to\ninvestigate the effectiveness of masks, social distancing, lockdown and\nself-isolation for reducing the spread of SARS-CoV-2 infections. Our findings\nbased on agent-based simulation modelling show that whilst requiring a lockdown\nis widely believed to be the most efficient method to quickly reduce infection\nnumbers, the practice of social distancing and the usage of surgical masks can\npotentially be more effective than requiring a lockdown. Our multivariate\nanalysis of simulation results using the Morris Elementary Effects Method\nsuggests that if a sufficient proportion of the population wore surgical masks\nand followed social distancing regulations, then SARS-CoV-2 infections can be\ncontrolled without requiring a lockdown.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:36:26 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 17:43:14 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 04:31:49 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Kelvin K. F.", ""], ["Jarvis", "Stephen A.", ""], ["Minhas", "Fayyaz", ""]]}, {"id": "2011.11395", "submitter": "Giuseppe Fenza", "authors": "Giuseppe Fenza and Mariacristina Gallo and Vincenzo Loia and Domenico\n  Marinoand Francesco Orciuoli and Alberto Volpe", "title": "Semantic CPPS in Industry 4.0", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-44041-1_91", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cyber-Physical Systems (CPS) play a crucial role in the era of the\n4thIndustrial Revolution. Recently, the application of the CPS to industrial\nmanufacturing leads to a specialization of them referred as Cyber-Physical\nProduction Systems (CPPS). Among other challenges, CPS and CPPS should be able\nto address interoperability issues, since one of their intrinsic requirement is\nthe capability to interface and cooperate with other systems. On the other\nhand, to fully realize theIndustry 4.0 vision, it is required to address\nhorizontal, vertical, and end-to-end integration enabling a complete awareness\nthrough the entire supply chain. In this context, Semantic Web standards and\ntechnologies may have a promising role to represent manufacturing knowledge in\na machine-interpretable way for enabling communications among heterogeneous\nIndustrial assets. This paper proposes an integration of Semantic Web models\navailable at state of the art for implementing a5C architecture mainly targeted\nto collect and process semantic data stream in a way that would unlock the\npotentiality of data yield in a smart manufacturing environment. The analysis\nof key industrial ontologies and semantic technologies allows us to instantiate\nan example scenario for monitoring Overall Equipment Effectiveness(OEE). The\nsolution uses the SOSA ontology for representing the semantic datastream. Then,\nC-SPARQL queries are defined for periodically carrying out useful KPIs to\naddress the proposed aim.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:53:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fenza", "Giuseppe", ""], ["Gallo", "Mariacristina", ""], ["Loia", "Vincenzo", ""], ["Orciuoli", "Domenico Marinoand Francesco", ""], ["Volpe", "Alberto", ""]]}, {"id": "2011.11400", "submitter": "Feng Qi", "authors": "Feng Qi", "title": "Language guided machine action", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Here we build a hierarchical modular network called Language guided machine\naction (LGMA), whose modules process information stream mimicking human\ncortical network that allows to achieve multiple general tasks such as language\nguided action, intention decomposition and mental simulation before action\nexecution etc. LGMA contains 3 main systems: (1) primary sensory system that\nmultimodal sensory information of vision, language and sensorimotor. (2)\nassociation system involves and Broca modules to comprehend and synthesize\nlanguage, BA14/40 module to translate between sensorimotor and language,\nmidTemporal module to convert between language and vision, and superior\nparietal lobe to integrate attended visual object and arm state into cognitive\nmap for future spatial actions. Pre-supplementary motor area (pre-SMA) can\nconverts high level intention into sequential atomic actions, while SMA can\nintegrate these atomic actions, current arm and attended object state into\nsensorimotor vector to apply corresponding torques on arm via pre-motor and\nprimary motor of arm to achieve the intention. The high-level executive system\ncontains PFC that does explicit inference and guide voluntary action based on\nlanguage, while BG is the habitual action control center.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:49:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Qi", "Feng", ""]]}, {"id": "2011.11440", "submitter": "Paolo Pagliuca", "authors": "Paolo Pagliuca and Stefano Nolfi", "title": "The Dynamic of Body and Brain Co-Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that permits to co-evolve the body and the control\nproperties of robots. It can be used to adapt the morphological traits of\nrobots with a hand-designed morphological bauplan or to evolve the\nmorphological bauplan as well. Our results indicate that robots with co-adapted\nbody and control traits outperform robots with fixed hand-designed\nmorphologies. Interestingly, the advantage is not due to the selection of\nbetter morphologies but rather to the mutual scaffolding process that results\nfrom the possibility to co-adapt the morphological traits to the control traits\nand vice versa. Our results also demonstrate that morphological variations do\nnot necessarily have destructive effects on robot skills.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 14:41:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pagliuca", "Paolo", ""], ["Nolfi", "Stefano", ""]]}, {"id": "2011.11442", "submitter": "Quoc Hung Ngo", "authors": "Quoc Hung Ngo, Tahar Kechadi, and Nhien-An Le-Khac", "title": "OAK: Ontology-Based Knowledge Map Model for Digital Agriculture", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-63924-2_14", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, a huge amount of knowledge has been amassed in digital agriculture.\nThis knowledge and know-how information are collected from various sources,\nhence the question is how to organise this knowledge so that it can be\nefficiently exploited. Although this knowledge about agriculture practices can\nbe represented using ontology, rule-based expert systems, or knowledge model\nbuilt from data mining processes, the scalability still remains an open issue.\nIn this study, we propose a knowledge representation model, called an\nontology-based knowledge map, which can collect knowledge from different\nsources, store it, and exploit either directly by stakeholders or as an input\nto the knowledge discovery process (Data Mining). The proposed model consists\nof two stages, 1) build an ontology as a knowledge base for a specific domain\nand data mining concepts, and 2) build the ontology-based knowledge map model\nfor representing and storing the knowledge mined on the crop datasets. A\nframework of the proposed model has been implemented in agriculture domain. It\nis an efficient and scalable model, and it can be used as knowledge repository\na digital agriculture.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:16:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ngo", "Quoc Hung", ""], ["Kechadi", "Tahar", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "2011.11484", "submitter": "Mingyong Zhou", "authors": "Mingyong Zhou", "title": "A Theory on AI Uncertainty Based on Rademacher Complexity and Shannon\n  Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a theoretical discussion on AI deep learning neural\nnetwork uncertainty investigation based on the classical Rademacher complexity\nand Shannon entropy. First it is shown that the classical Rademacher complexity\nand Shannon entropy is closely related by quantity by definitions. Secondly\nbased on the Shannon mathematical theory on communication [3], we derive a\ncriteria to ensure AI correctness and accuracy in classifications problems.\nLast but not the least based on Peter Barlette's work, we show both a relaxing\ncondition and a stricter condition to guarantee the correctness and accuracy in\nAI classification . By elucidating in this paper criteria condition in terms of\nShannon entropy based on Shannon theory, it becomes easier to explore other\ncriteria in terms of other complexity measurements such as Vapnik-Cheronenkis,\nGaussian complexity by taking advantage of the relations studies results in\nother references. A close to 0.5 criteria on Shannon entropy is derived in this\npaper for the theoretical investigation of AI accuracy and correctness for\nclassification problems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:34:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhou", "Mingyong", ""]]}, {"id": "2011.11517", "submitter": "Tyler Malloy", "authors": "Tyler Malloy, Tim Klinger, Miao Liu, Matthew Riemer, Gerald Tesauro,\n  Chris R. Sims", "title": "Consolidation via Policy Information Regularization in Deep RL for\n  Multi-Agent Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an information-theoretic constraint on learned policy\ncomplexity in the Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\nreinforcement learning algorithm. Previous research with a related approach in\ncontinuous control experiments suggests that this method favors learning\npolicies that are more robust to changing environment dynamics. The multi-agent\ngame setting naturally requires this type of robustness, as other agents'\npolicies change throughout learning, introducing a nonstationary environment.\nFor this reason, recent methods in continual learning are compared to our\napproach, termed Capacity-Limited MADDPG. Results from experimentation in\nmulti-agent cooperative and competitive tasks demonstrate that the\ncapacity-limited approach is a good candidate for improving learning\nperformance in these environments.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:28:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Malloy", "Tyler", ""], ["Klinger", "Tim", ""], ["Liu", "Miao", ""], ["Riemer", "Matthew", ""], ["Tesauro", "Gerald", ""], ["Sims", "Chris R.", ""]]}, {"id": "2011.11521", "submitter": "Yang Zhou", "authors": "Yang Zhou and Shiliang Sun", "title": "Manifold Partition Discriminant Analysis", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics 47.4 (2016): 830-840", "doi": "10.1109/TCYB.2016.2529299", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for supervised dimensionality reduction named\nManifold Partition Discriminant Analysis (MPDA). It aims to find a linear\nembedding space where the within-class similarity is achieved along the\ndirection that is consistent with the local variation of the data manifold,\nwhile nearby data belonging to different classes are well separated. By\npartitioning the data manifold into a number of linear subspaces and utilizing\nthe first-order Taylor expansion, MPDA explicitly parameterizes the connections\nof tangent spaces and represents the data manifold in a piecewise manner. While\ngraph Laplacian methods capture only the pairwise interaction between data\npoints, our method capture both pairwise and higher order interactions (using\nregional consistency) between data points. This manifold representation can\nhelp to improve the measure of within-class similarity, which further leads to\nimproved performance of dimensionality reduction. Experimental results on\nmultiple real-world data sets demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:33:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhou", "Yang", ""], ["Sun", "Shiliang", ""]]}, {"id": "2011.11545", "submitter": "Xuhong Wang", "authors": "Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang, Xinwen Wang,\n  Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, Zhenyu Guo", "title": "APAN: Asynchronous Propagation Attention Network for Real-time Temporal\n  Graph Embedding", "comments": "In Proceedings of the 2021 International Conference on Management of\n  Data (SIGMOD/PODS '21)", "journal-ref": null, "doi": "10.1145/3448016.3457564", "report-no": null, "categories": "cs.AI cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Limited by the time complexity of querying k-hop neighbors in a graph\ndatabase, most graph algorithms cannot be deployed online and execute\nmillisecond-level inference. This problem dramatically limits the potential of\napplying graph algorithms in certain areas, such as financial fraud detection.\nTherefore, we propose Asynchronous Propagation Attention Network, an\nasynchronous continuous time dynamic graph algorithm for real-time temporal\ngraph embedding. Traditional graph models usually execute two serial\noperations: first graph computation and then model inference. We decouple model\ninference and graph computation step so that the heavy graph query operations\nwill not damage the speed of model inference. Extensive experiments demonstrate\nthat the proposed method can achieve competitive performance and 8.7 times\ninference speed improvement in the meantime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:58:50 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 03:12:47 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 02:35:09 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 05:42:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Xuhong", ""], ["Lyu", "Ding", ""], ["Li", "Mengjian", ""], ["Xia", "Yang", ""], ["Yang", "Qi", ""], ["Wang", "Xinwen", ""], ["Wang", "Xinguang", ""], ["Cui", "Ping", ""], ["Yang", "Yupu", ""], ["Sun", "Bowen", ""], ["Guo", "Zhenyu", ""]]}, {"id": "2011.11600", "submitter": "Vitor Fortes Rey", "authors": "Vitor Fortes Rey, Kamalveer Kaur Garewal, Paul Lukowicz", "title": "Yet it moves: Learning from Generic Motions to Generate IMU data from\n  YouTube videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human activity recognition (HAR) using wearable sensors has benefited much\nless from recent advances in Machine Learning than fields such as computer\nvision and natural language processing. This is to a large extent due to the\nlack of large scale repositories of labeled training data. In our research we\naim to facilitate the use of online videos, which exists in ample quantity for\nmost activities and are much easier to label than sensor data, to simulate\nlabeled wearable motion sensor data. In previous work we already demonstrate\nsome preliminary results in this direction focusing on very simple, activity\nspecific simulation models and a single sensor modality (acceleration\nnorm)\\cite{10.1145/3341162.3345590}. In this paper we show how we can train a\nregression model on generic motions for both accelerometer and gyro signals and\nthen apply it to videos of the target activities to generate synthetic IMU data\n(acceleration and gyro norms) that can be used to train and/or improve HAR\nmodels. We demonstrate that systems trained on simulated data generated by our\nregression model can come to within around 10% of the mean F1 score of a system\ntrained on real sensor data. Furthermore we show that by either including a\nsmall amount of real sensor data for model calibration or simply leveraging the\nfact that (in general) we can easily generate much more simulated data from\nvideo than we can collect in terms of real sensor data the advantage of real\nsensor data can be eventually equalized.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:16:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rey", "Vitor Fortes", ""], ["Garewal", "Kamalveer Kaur", ""], ["Lukowicz", "Paul", ""]]}, {"id": "2011.11603", "submitter": "Zhonghao Wang", "authors": "Zhonghao Wang, Mo Yu, Kai Wang, Jinjun Xiong, Wen-mei Hwu, Mark\n  Hasegawa-Johnson, Humphrey Shi", "title": "Interpretable Visual Reasoning via Induced Symbolic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Experiments on the\nCLEVR dataset demonstrate: 1) our OCCAM achieves a new state of the art without\nhuman-annotated functional programs; 2) our induced concepts are both accurate\nand sufficient as OCCAM achieves an on-par performance on objects represented\neither in visual features or in the induced symbolic concept space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:21:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Zhonghao", ""], ["Yu", "Mo", ""], ["Wang", "Kai", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Hasegawa-Johnson", "Mark", ""], ["Shi", "Humphrey", ""]]}, {"id": "2011.11609", "submitter": "Joseph Vincent", "authors": "Joseph A. Vincent, Mac Schwager", "title": "Reachable Polyhedral Marching (RPM): A Safety Verification Algorithm for\n  Robotic Systems with Deep Neural Network Components", "comments": "accepted to International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for computing exact reachable sets for deep neural\nnetworks with rectified linear unit (ReLU) activation. Our method is\nwell-suited for use in rigorous safety analysis of robotic perception and\ncontrol systems with deep neural network components. Our algorithm can compute\nboth forward and backward reachable sets for a ReLU network iterated over\nmultiple time steps, as would be found in a perception-action loop in a robotic\nsystem. Our algorithm is unique in that it builds the reachable sets by\nincrementally enumerating polyhedral cells in the input space, rather than\niterating layer-by-layer through the network as in other methods. If an unsafe\ncell is found, our algorithm can return this result without completing the full\nreachability computation, thus giving an anytime property that accelerates\nsafety verification. In addition, our method requires less memory during\nexecution compared to existing methods where memory can be a limiting factor.\nWe demonstrate our algorithm on safety verification of the ACAS Xu aircraft\nadvisory system. We find unsafe actions many times faster than the fastest\nexisting method and certify no unsafe actions exist in about twice the time of\nthe existing method. We also compute forward and backward reachable sets for a\nlearned model of pendulum dynamics over a 50 time step horizon in 87s on a\nlaptop computer. Algorithm source code:\nhttps://github.com/StanfordMSL/Neural-Network-Reach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:35:53 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 04:17:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Vincent", "Joseph A.", ""], ["Schwager", "Mac", ""]]}, {"id": "2011.11637", "submitter": "Ilia Shumailov", "authors": "Yiren Zhao, Ilia Shumailov, Robert Mullins and Ross Anderson", "title": "Nudge Attacks on Point-Cloud DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The wide adaption of 3D point-cloud data in safety-critical applications such\nas autonomous driving makes adversarial samples a real threat. Existing\nadversarial attacks on point clouds achieve high success rates but modify a\nlarge number of points, which is usually difficult to do in real-life\nscenarios. In this paper, we explore a family of attacks that only perturb a\nfew points of an input point cloud, and name them nudge attacks. We demonstrate\nthat nudge attacks can successfully flip the results of modern point-cloud\nDNNs. We present two variants, gradient-based and decision-based, showing their\neffectiveness in white-box and grey-box scenarios. Our extensive experiments\nshow nudge attacks are effective at generating both targeted and untargeted\nadversarial point clouds, by changing a few points or even a single point from\nthe entire point-cloud input. We find that with a single point we can reliably\nthwart predictions in 12--80% of cases, whereas 10 points allow us to further\nincrease this to 37--95%. Finally, we discuss the possible defenses against\nsuch attacks, and explore their limitations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:04:02 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhao", "Yiren", ""], ["Shumailov", "Ilia", ""], ["Mullins", "Robert", ""], ["Anderson", "Ross", ""]]}, {"id": "2011.11713", "submitter": "Boyu Zhang", "authors": "Fuchang Gao and Boyu Zhang", "title": "A Use of Even Activation Functions in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite broad interest in applying deep learning techniques to scientific\ndiscovery, learning interpretable formulas that accurately describe scientific\ndata is very challenging because of the vast landscape of possible functions\nand the \"black box\" nature of deep neural networks. The key to success is to\neffectively integrate existing knowledge or hypotheses about the underlying\nstructure of the data into the architecture of deep learning models to guide\nmachine learning. Currently, such integration is commonly done through\ncustomization of the loss functions. Here we propose an alternative approach to\nintegrate existing knowledge or hypotheses of data structure by constructing\ncustom activation functions that reflect this structure. Specifically, we study\na common case when the multivariate target function $f$ to be learned from the\ndata is partially exchangeable, \\emph{i.e.} $f(u,v,w)=f(v,u,w)$ for $u,v\\in\n\\mathbb{R}^d$. For instance, these conditions are satisfied for the\nclassification of images that is invariant under left-right flipping. Through\ntheoretical proof and experimental verification, we show that using an even\nactivation function in one of the fully connected layers improves neural\nnetwork performance. In our experimental 9-dimensional regression problems,\nreplacing one of the non-symmetric activation functions with the designated\n\"Seagull\" activation function $\\log(1+x^2)$ results in substantial improvement\nin network performance. Surprisingly, even activation functions are seldom used\nin neural networks. Our results suggest that customized activation functions\nhave great potential in neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:33:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gao", "Fuchang", ""], ["Zhang", "Boyu", ""]]}, {"id": "2011.11715", "submitter": "C.-H. Huck Yang", "authors": "Chao-Han Huck Yang, Linda Liu, Ankur Gandhe, Yile Gu, Anirudh Raju,\n  Denis Filimonov, Ivan Bulyko", "title": "Multi-task Language Modeling for Improving Speech Recognition of Rare\n  Words", "comments": "Preprint. Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR).\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:40:44 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 03:12:54 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 20:31:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Liu", "Linda", ""], ["Gandhe", "Ankur", ""], ["Gu", "Yile", ""], ["Raju", "Anirudh", ""], ["Filimonov", "Denis", ""], ["Bulyko", "Ivan", ""]]}, {"id": "2011.11728", "submitter": "Yuhao Zhu", "authors": "Carlos Mauricio Villegas Burgos, Tianqi Yang, Nick Vamivakas, Yuhao\n  Zhu", "title": "End-to-End Framework for Efficient Deep Learning Using Metasurfaces\n  Optics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning using Convolutional Neural Networks (CNNs) has been shown to\nsignificantly out-performed many conventional vision algorithms. Despite\nefforts to increase the CNN efficiency both algorithmically and with\nspecialized hardware, deep learning remains difficult to deploy in\nresource-constrained environments. In this paper, we propose an end-to-end\nframework to explore optically compute the CNNs in free-space, much like a\ncomputational camera. Compared to existing free-space optics-based approaches\nwhich are limited to processing single-channel (i.e., grayscale) inputs, we\npropose the first general approach, based on nanoscale meta-surface optics,\nthat can process RGB data directly from the natural scenes. Our system achieves\nup to an order of magnitude energy saving, simplifies the sensor design, all\nthe while sacrificing little network accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:06:04 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:22:00 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Burgos", "Carlos Mauricio Villegas", ""], ["Yang", "Tianqi", ""], ["Vamivakas", "Nick", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2011.11735", "submitter": "Varnith Chordia", "authors": "Varnith Chordia, Vijay Kumar BG", "title": "Large Scale Multimodal Classification Using an Ensemble of Transformer\n  Models and Co-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient product classification is significant for E-commerce\napplications, as it enables various downstream tasks such as recommendation,\nretrieval, and pricing. Items often contain textual and visual information, and\nutilizing both modalities usually outperforms classification utilizing either\nmode alone. In this paper we describe our methodology and results for the SIGIR\neCom Rakuten Data Challenge. We employ a dual attention technique to model\nimage-text relationships using pretrained language and image embeddings. While\ndual attention has been widely used for Visual Question Answering(VQA) tasks,\nours is the first attempt to apply the concept for multimodal classification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:22:54 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chordia", "Varnith", ""], ["BG", "Vijay Kumar", ""]]}, {"id": "2011.11745", "submitter": "Ruikang Zhong", "authors": "Ruikang Zhong, Xiao Liu, Yuanwei Liu, Yue Chen and Xianbin Wang", "title": "Path Design and Resource Management for NOMA enhanced Indoor Intelligent\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A communication enabled indoor intelligent robots (IRs) service framework is\nproposed, where non-orthogonal multiple access (NOMA) technique is adopted to\nenable highly reliable communications. In cooperation with the ultramodern\nindoor channel model recently proposed by the International Telecommunication\nUnion (ITU), the Lego modeling method is proposed, which can deterministically\ndescribe the indoor layout and channel state in order to construct the radio\nmap. The investigated radio map is invoked as a virtual environment to train\nthe reinforcement learning agent, which can save training time and hardware\ncosts. Build on the proposed communication model, motions of IRs who need to\nreach designated mission destinations and their corresponding down-link power\nallocation policy are jointly optimized to maximize the mission efficiency and\ncommunication reliability of IRs. In an effort to solve this optimization\nproblem, a novel reinforcement learning approach named deep transfer\ndeterministic policy gradient (DT-DPG) algorithm is proposed. Our simulation\nresults demonstrate that 1) With the aid of NOMA techniques, the communication\nreliability of IRs is effectively improved; 2) The radio map is qualified to be\na virtual training environment, and its statistical channel state information\nimproves training efficiency by about 30%; 3) The proposed DT-DPG algorithm is\nsuperior to the conventional deep deterministic policy gradient (DDPG)\nalgorithm in terms of optimization performance, training time, and anti-local\noptimum ability.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:45:01 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 23:25:46 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhong", "Ruikang", ""], ["Liu", "Xiao", ""], ["Liu", "Yuanwei", ""], ["Chen", "Yue", ""], ["Wang", "Xianbin", ""]]}, {"id": "2011.11785", "submitter": "Carlos Henrique Caloete Pena", "authors": "Carlos H. C. Pena, Mateus G. Machado, Mariana S. Barros, Jos\\'e D. P.\n  Silva, Lucas D. Maciel, Tsang Ing Ren, Edna N. S. Barros, Pedro H. M. Braga,\n  Hansenclever F. Bassani", "title": "An analysis of Reinforcement Learning applied to Coach task in IEEE Very\n  Small Size Soccer", "comments": "6 pages, 9 figures, to be published in Latin American Robotics\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The IEEE Very Small Size Soccer (VSSS) is a robot soccer competition in which\ntwo teams of three small robots play against each other. Traditionally, a\ndeterministic coach agent will choose the most suitable strategy and formation\nfor each adversary's strategy. Therefore, the role of a coach is of great\nimportance to the game. In this sense, this paper proposes an end-to-end\napproach for the coaching task based on Reinforcement Learning (RL). The\nproposed system processes the information during the simulated matches to learn\nan optimal policy that chooses the current formation, depending on the opponent\nand game conditions. We trained two RL policies against three different teams\n(balanced, offensive, and heavily offensive) in a simulated environment. Our\nresults were assessed against one of the top teams of the VSSS league, showing\npromising results after achieving a win/loss ratio of approximately 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 23:10:06 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Pena", "Carlos H. C.", ""], ["Machado", "Mateus G.", ""], ["Barros", "Mariana S.", ""], ["Silva", "Jos\u00e9 D. P.", ""], ["Maciel", "Lucas D.", ""], ["Ren", "Tsang Ing", ""], ["Barros", "Edna N. S.", ""], ["Braga", "Pedro H. M.", ""], ["Bassani", "Hansenclever F.", ""]]}, {"id": "2011.11805", "submitter": "Andrew O'Brien", "authors": "Edward Kim, Connor Onweller, Andrew O'Brien, Kathleen McCoy", "title": "The Interpretable Dictionary in Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs), specifically deep learning networks, have\noften been labeled as black boxes due to the fact that the internal\nrepresentation of the data is not easily interpretable. In our work, we\nillustrate that an ANN, trained using sparse coding under specific sparsity\nconstraints, yields a more interpretable model than the standard deep learning\nmodel. The dictionary learned by sparse coding can be more easily understood\nand the activations of these elements creates a selective feature output. We\ncompare and contrast our sparse coding model with an equivalent feed forward\nconvolutional autoencoder trained on the same data. Our results show both\nqualitative and quantitative benefits in the interpretation of the learned\nsparse coding dictionary as well as the internal activation representations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:26:40 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kim", "Edward", ""], ["Onweller", "Connor", ""], ["O'Brien", "Andrew", ""], ["McCoy", "Kathleen", ""]]}, {"id": "2011.11819", "submitter": "Ming Ding Dr.", "authors": "Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai\n  Lin", "title": "When Machine Learning Meets Privacy: A Survey and Outlook", "comments": "This work is accepted by ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:52:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Liu", "Bo", ""], ["Ding", "Ming", ""], ["Shaham", "Sina", ""], ["Rahayu", "Wenny", ""], ["Farokhi", "Farhad", ""], ["Lin", "Zihuai", ""]]}, {"id": "2011.11827", "submitter": "Yunzhe Tao", "authors": "Yunzhe Tao, Sahika Genc, Jonathan Chung, Tao Sun, Sunil Mallya", "title": "REPAINT: Knowledge Transfer in Deep Reinforcement Learning", "comments": "Published at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating learning processes for complex tasks by leveraging previously\nlearned tasks has been one of the most challenging problems in reinforcement\nlearning, especially when the similarity between source and target tasks is\nlow. This work proposes REPresentation And INstance Transfer (REPAINT)\nalgorithm for knowledge transfer in deep reinforcement learning. REPAINT not\nonly transfers the representation of a pre-trained teacher policy in the\non-policy learning, but also uses an advantage-based experience selection\napproach to transfer useful samples collected following the teacher policy in\nthe off-policy learning. Our experimental results on several benchmark tasks\nshow that REPAINT significantly reduces the total training time in generic\ncases of task similarity. In particular, when the source tasks are dissimilar\nto, or sub-tasks of, the target tasks, REPAINT outperforms other baselines in\nboth training-time reduction and asymptotic performance of return scores.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 01:18:32 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 18:57:25 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 05:25:23 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Tao", "Yunzhe", ""], ["Genc", "Sahika", ""], ["Chung", "Jonathan", ""], ["Sun", "Tao", ""], ["Mallya", "Sunil", ""]]}, {"id": "2011.11834", "submitter": "Loris Nanni", "authors": "Loris Nanni, Alessandra Lumini, Stefano Ghidoni and Gianluca Maguolo", "title": "Comparisons among different stochastic selection of activation layers\n  for convolutional neural networks for healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of biological images is an important task with crucial\napplication in many fields, such as cell phenotypes recognition, detection of\ncell organelles and histopathological classification, and it might help in\nearly medical diagnosis, allowing automatic disease classification without the\nneed of a human expert. In this paper we classify biomedical images using\nensembles of neural networks. We create this ensemble using a ResNet50\narchitecture and modifying its activation layers by substituting ReLUs with\nother functions. We select our activations among the following ones: ReLU,\nleaky ReLU, Parametric ReLU, ELU, Adaptive Piecewice Linear Unit, S-Shaped\nReLU, Swish , Mish, Mexican Linear Unit, Gaussian Linear Unit, Parametric\nDeformable Linear Unit, Soft Root Sign (SRS) and others.\n  As a baseline, we used an ensemble of neural networks that only use ReLU\nactivations. We tested our networks on several small and medium sized\nbiomedical image datasets. Our results prove that our best ensemble obtains a\nbetter performance than the ones of the naive approaches. In order to encourage\nthe reproducibility of this work, the MATLAB code of all the experiments will\nbe shared at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 01:53:39 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nanni", "Loris", ""], ["Lumini", "Alessandra", ""], ["Ghidoni", "Stefano", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2011.11840", "submitter": "Omobayode Fagbohungbe", "authors": "Omobayode Fagbohungbe, Lijun Qian", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analog hardware implemented deep learning models are promising for\ncomputation and energy constrained systems such as edge computing devices.\nHowever, the analog nature of the device and the associated many noise sources\nwill cause changes to the value of the weights in the trained deep learning\nmodels deployed on such devices. In this study, systematic evaluation of the\ninference performance of trained popular deep learning models for image\nclassification deployed on analog devices has been carried out, where additive\nwhite Gaussian noise has been added to the weights of the trained models during\ninference. It is observed that deeper models and models with more redundancy in\ndesign such as VGG are more robust to the noise in general. However, the\nperformance is also affected by the design philosophy of the model, the\ndetailed structure of the model, the exact machine learning task, as well as\nthe datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:14:39 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:04:52 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Fagbohungbe", "Omobayode", ""], ["Qian", "Lijun", ""]]}, {"id": "2011.11851", "submitter": "Woohwan Jung", "authors": "Woohwan Jung and Kyuseok Shim", "title": "Dual Supervision Framework for Relation Extraction with Distant\n  Supervision and Human Annotation", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) has been extensively studied due to its importance\nin real-world applications such as knowledge base construction and question\nanswering. Most of the existing works train the models on either distantly\nsupervised data or human-annotated data. To take advantage of the high accuracy\nof human annotation and the cheap cost of distant supervision, we propose the\ndual supervision framework which effectively utilizes both types of data.\nHowever, simply combining the two types of data to train a RE model may\ndecrease the prediction accuracy since distant supervision has labeling bias.\nWe employ two separate prediction networks HA-Net and DS-Net to predict the\nlabels by human annotation and distant supervision, respectively, to prevent\nthe degradation of accuracy by the incorrect labeling of distant supervision.\nFurthermore, we propose an additional loss term called disagreement penalty to\nenable HA-Net to learn from distantly supervised labels. In addition, we\nexploit additional networks to adaptively assess the labeling bias by\nconsidering contextual information. Our performance study on sentence-level and\ndocument-level REs confirms the effectiveness of the dual supervision\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:35:24 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jung", "Woohwan", ""], ["Shim", "Kyuseok", ""]]}, {"id": "2011.11858", "submitter": "Hexin Bai", "authors": "Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, Haibin\n  Ling", "title": "GMOT-40: A Benchmark for Generic Multiple Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) has witnessed remarkable advances in recent\nyears. However, existing studies dominantly request prior knowledge of the\ntracking target, and hence may not generalize well to unseen categories. In\ncontrast, Generic Multiple Object Tracking (GMOT), which requires little prior\ninformation about the target, is largely under-explored. In this paper, we make\ncontributions to boost the study of GMOT in three aspects. First, we construct\nthe first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully\nannotated sequences evenly distributed among 10 object categories. In addition,\ntwo tracking protocols are adopted to evaluate different characteristics of\ntracking algorithms. Second, by noting the lack of devoted tracking algorithms,\nwe have designed a series of baseline GMOT algorithms. Third, we perform a\nthorough evaluation on GMOT-40, involving popular MOT algorithms (with\nnecessary modifications) and the proposed baselines. We will release the\nGMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to\nthe public upon the publication of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:51:46 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:40:03 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 19:13:00 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bai", "Hexin", ""], ["Cheng", "Wensheng", ""], ["Chu", "Peng", ""], ["Liu", "Juehuan", ""], ["Zhang", "Kai", ""], ["Ling", "Haibin", ""]]}, {"id": "2011.11866", "submitter": "Gurcan Comert", "authors": "Gurcan Comert", "title": "Gaussian Processes for Traffic Speed Prediction at Different Aggregation\n  Levels", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic behavior of traffic adversely affect the performance of the\nprediction models in intelligent transportation applications. This study\napplies Gaussian processes (GPs) to traffic speed prediction. Such predictions\ncan be used by various transportation applications, such as real-time route\nguidance, ramp metering, congestion pricing and special events traffic\nmanagement. One-step predictions with various aggregation levels (1 to\n60-minute) are tested for performance of the generated models. Univariate and\nmultivariate GPs are compared with several other linear, nonlinear time series,\nand Grey system models using loop and Inrix probe vehicle datasets from\nCalifornia, Portland, and Virginia freeways respectively. Based on the test\ndata samples, results are promising that GP models are able to consistently\noutperform compared models with similar computational times.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:05:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Comert", "Gurcan", ""]]}, {"id": "2011.11878", "submitter": "Hyemi Kim", "authors": "Hyemi Kim, Seungjae Shin, JoonHo Jang, Kyungwoo Song, Weonyoung Joo,\n  Wanmo Kang, Il-Chul Moon", "title": "Counterfactual Fairness with Disentangled Causal Effect Variational\n  Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of fair classification can be mollified if we develop a method to\nremove the embedded sensitive information from the classification features.\nThis line of separating the sensitive information is developed through the\ncausal inference, and the causal inference enables the counterfactual\ngenerations to contrast the what-if case of the opposite sensitive attribute.\nAlong with this separation with the causality, a frequent assumption in the\ndeep latent causal model defines a single latent variable to absorb the entire\nexogenous uncertainty of the causal graph. However, we claim that such\nstructure cannot distinguish the 1) information caused by the intervention\n(i.e., sensitive variable) and 2) information correlated with the intervention\nfrom the data. Therefore, this paper proposes Disentangled Causal Effect\nVariational Autoencoder (DCEVAE) to resolve this limitation by disentangling\nthe exogenous uncertainty into two latent variables: either 1) independent to\ninterventions or 2) correlated to interventions without causality.\nParticularly, our disentangling approach preserves the latent variable\ncorrelated to interventions in generating counterfactual examples. We show that\nour method estimates the total effect and the counterfactual effect without a\ncomplete causal graph. By adding a fairness regularization, DCEVAE generates a\ncounterfactual fair dataset while losing less original information. Also,\nDCEVAE generates natural counterfactual images by only flipping sensitive\ninformation. Additionally, we theoretically show the differences in the\ncovariance structures of DCEVAE and prior works from the perspective of the\nlatent disentanglement.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:43:59 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 09:46:14 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Kim", "Hyemi", ""], ["Shin", "Seungjae", ""], ["Jang", "JoonHo", ""], ["Song", "Kyungwoo", ""], ["Joo", "Weonyoung", ""], ["Kang", "Wanmo", ""], ["Moon", "Il-Chul", ""]]}, {"id": "2011.11913", "submitter": "Ahmadreza Ahmadi", "authors": "Ahmadreza Ahmadi, T{\\o}nnes Nygaard, Navinda Kottege, David Howard,\n  Nicolas Hudson", "title": "Semi-supervised Gated Recurrent Neural Networks for Robotic Terrain\n  Classification", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Legged robots are popular candidates for missions in challenging terrains due\nto the wide variety of locomotion strategies they can employ. Terrain\nclassification is a key enabling technology for autonomous legged robots, as it\nallows the robot to harness their innate flexibility to adapt their behaviour\nto the demands of their operating environment. In this paper, we show how\nhighly capable machine learning techniques, namely gated recurrent neural\nnetworks, allow our target legged robot to correctly classify the terrain it\ntraverses in both supervised and semi-supervised fashions. Tests on a benchmark\ndata set shows that our time-domain classifiers are well capable of dealing\nwith raw and variable-length data with small amount of labels and perform to a\nlevel far exceeding the frequency-domain classifiers. The classification\nresults on our own extended data set opens up a range of high-performance\nbehaviours that are specific to those environments. Furthermore, we show how\nraw unlabelled data is used to improve significantly the classification results\nin a semi-supervised model.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 06:25:19 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ahmadi", "Ahmadreza", ""], ["Nygaard", "T\u00f8nnes", ""], ["Kottege", "Navinda", ""], ["Howard", "David", ""], ["Hudson", "Nicolas", ""]]}, {"id": "2011.11922", "submitter": "Jiachen Sun", "authors": "Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen, Z. Morley Mao", "title": "On Adversarial Robustness of 3D Point Cloud Classification under\n  Adaptive Attacks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point clouds play pivotal roles in various safety-critical applications,\nsuch as autonomous driving, which desires the underlying deep neural networks\nto be robust to adversarial perturbations. Though a few defenses against\nadversarial point cloud classification have been proposed, it remains unknown\nwhether they are truly robust to adaptive attacks. To this end, we perform the\nfirst security analysis of state-of-the-art defenses and design adaptive\nevaluations on them. Our 100% adaptive attack success rates show that current\ncountermeasures are still vulnerable. Since adversarial training (AT) is\nbelieved as the most robust defense, we present the first in-depth study\nshowing how AT behaves in point cloud classification and identify that the\nrequired symmetric function (pooling operation) is paramount to the 3D model's\nrobustness under AT. Through our systematic analysis, we find that the\ndefault-used fixed pooling (e.g., MAX pooling) generally weakens AT's\neffectiveness in point cloud classification. Interestingly, we further discover\nthat sorting-based parametric pooling can significantly improve the models'\nrobustness. Based on above insights, we propose DeepSym, a deep symmetric\npooling operation, to architecturally advance the robustness to 47.0% under AT\nwithout sacrificing nominal accuracy, outperforming the original design and a\nstrong baseline by 28.5% ($\\sim 2.6 \\times$) and 6.5%, respectively, in\nPointNet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 06:46:38 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:36:44 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Sun", "Jiachen", ""], ["Koenig", "Karl", ""], ["Cao", "Yulong", ""], ["Chen", "Qi Alfred", ""], ["Mao", "Z. Morley", ""]]}, {"id": "2011.11933", "submitter": "Xiupeng Shi Dr", "authors": "Xiupeng Shi, Yiik Diew Wong, Chen Chai, Michael Zhi-Feng Li, Tianyi\n  Chen, Zeng Zeng", "title": "Automatic Clustering for Unsupervised Risk Diagnosis of Vehicle Driving\n  for Smart Road", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early risk diagnosis and driving anomaly detection from vehicle stream are of\ngreat benefits in a range of advanced solutions towards Smart Road and crash\nprevention, although there are intrinsic challenges, especially lack of ground\ntruth, definition of multiple risk exposures. This study proposes a\ndomain-specific automatic clustering (termed Autocluster) to self-learn the\noptimal models for unsupervised risk assessment, which integrates key steps of\nrisk clustering into an auto-optimisable pipeline, including feature and\nalgorithm selection, hyperparameter auto-tuning. Firstly, based on surrogate\nconflict measures, indicator-guided feature extraction is conducted to\nconstruct temporal-spatial and kinematical risk features. Then we develop an\nelimination-based model reliance importance (EMRI) method to\nunsupervised-select the useful features. Secondly, we propose balanced\nSilhouette Index (bSI) to evaluate the internal quality of imbalanced\nclustering. A loss function is designed that considers the clustering\nperformance in terms of internal quality, inter-cluster variation, and model\nstability. Thirdly, based on Bayesian optimisation, the algorithm selection and\nhyperparameter auto-tuning are self-learned to generate the best clustering\npartitions. Various algorithms are comprehensively investigated. Herein, NGSIM\nvehicle trajectory data is used for test-bedding. Findings show that\nAutocluster is reliable and promising to diagnose multiple distinct risk\nexposures inherent to generalised driving behaviour. Besides, we also delve\ninto risk clustering, such as, algorithms heterogeneity, Silhouette analysis,\nhierarchical clustering flows, etc. Meanwhile, the Autocluster is also a method\nfor unsupervised multi-risk data labelling and indicator threshold calibration.\nFurthermore, Autocluster is useful to tackle the challenges in imbalanced\nclustering without ground truth or priori knowledge\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:15:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Shi", "Xiupeng", ""], ["Wong", "Yiik Diew", ""], ["Chai", "Chen", ""], ["Li", "Michael Zhi-Feng", ""], ["Chen", "Tianyi", ""], ["Zeng", "Zeng", ""]]}, {"id": "2011.11938", "submitter": "Junyou He", "authors": "Junyou He, Guibao Mei, Feng Xing, Xiaorui Yang, Yongjun Bao, Weipeng\n  Yan", "title": "DADNN: Multi-Scene CTR Prediction via Domain-Aware Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click through rate(CTR) prediction is a core task in advertising systems. The\nbooming e-commerce business in our company, results in a growing number of\nscenes. Most of them are so-called long-tail scenes, which means that the\ntraffic of a single scene is limited, but the overall traffic is considerable.\nTypical studies mainly focus on serving a single scene with a well designed\nmodel. However, this method brings excessive resource consumption both on\noffline training and online serving. Besides, simply training a single model\nwith data from multiple scenes ignores the characteristics of their own. To\naddress these challenges, we propose a novel but practical model named\nDomain-Aware Deep Neural Network(DADNN) by serving multiple scenes with only\none model. Specifically, shared bottom block among all scenes is applied to\nlearn a common representation, while domain-specific heads maintain the\ncharacteristics of every scene. Besides, knowledge transfer is introduced to\nenhance the opportunity of knowledge sharing among different scenes. In this\npaper, we study two instances of DADNN where its shared bottom block is\nmultilayer perceptron(MLP) and Multi-gate Mixture-of-Experts(MMoE)\nrespectively, for which we denote as DADNN-MLP and DADNN-MMoE.Comprehensive\noffline experiments on a real production dataset from our company show that\nDADNN outperforms several state-of-the-art methods for multi-scene CTR\nprediction. Extensive online A/B tests reveal that DADNN-MLP contributes up to\n6.7% CTR and 3.0% CPM(Cost Per Mille) promotion compared with a well-engineered\nDCN model. Furthermore, DADNN-MMoE outperforms DADNN-MLP with a relative\nimprovement of 2.2% and 2.7% on CTR and CPM respectively. More importantly,\nDADNN utilizes a single model for multiple scenes which saves a lot of offline\ntraining and online serving resources.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:30:52 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["He", "Junyou", ""], ["Mei", "Guibao", ""], ["Xing", "Feng", ""], ["Yang", "Xiaorui", ""], ["Bao", "Yongjun", ""], ["Yan", "Weipeng", ""]]}, {"id": "2011.11950", "submitter": "Nikitha Rao", "authors": "Nikitha Rao, Chetan Bansal and Joe Guan", "title": "Search4Code: Code Search Intent Classification Using Weak Supervision", "comments": "Dataset for this paper is available here:\n  https://github.com/microsoft/Search4Code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers use search for various tasks such as finding code, documentation,\ndebugging information, etc. In particular, web search is heavily used by\ndevelopers for finding code examples and snippets during the coding process.\nRecently, natural language based code search has been an active area of\nresearch. However, the lack of real-world large-scale datasets is a significant\nbottleneck. In this work, we propose a weak supervision based approach for\ndetecting code search intent in search queries for C# and Java programming\nlanguages. We evaluate the approach against several baselines on a real-world\ndataset comprised of over 1 million queries mined from Bing web search engine\nand show that the CNN based model can achieve an accuracy of 77% and 76% for C#\nand Java respectively. Furthermore, we are also releasing Search4Code, the\nfirst large-scale real-world dataset of code search queries mined from Bing web\nsearch engine. We hope that the dataset will aid future research on code\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:06:53 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 01:54:17 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 15:01:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rao", "Nikitha", ""], ["Bansal", "Chetan", ""], ["Guan", "Joe", ""]]}, {"id": "2011.12024", "submitter": "Haoxi Ran", "authors": "Haoxi Ran, Guangfu Wang, Li Lu", "title": "RIN: Textured Human Model Recovery and Imitation with a Single Image", "comments": "The paper is not well-organized and have to be modified later. Hence\n  authors decide to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human imitation has become topical recently, driven by GAN's ability to\ndisentangle human pose and body content. However, the latest methods hardly\nfocus on 3D information, and to avoid self-occlusion, a massive amount of input\nimages are needed. In this paper, we propose RIN, a novel volume-based\nframework for reconstructing a textured 3D model from a single picture and\nimitating a subject with the generated model. Specifically, to estimate most of\nthe human texture, we propose a U-Net-like front-to-back translation network.\nWith both front and back images input, the textured volume recovery module\nallows us to color a volumetric human. A sequence of 3D poses then guides the\ncolored volume via Flowable Disentangle Networks as a volume-to-volume\ntranslation task. To project volumes to a 2D plane during training, we design a\ndifferentiable depth-aware renderer. Our experiments demonstrate that our\nvolume-based model is adequate for human imitation, and the back view can be\nestimated reliably using our network. While prior works based on either 2D pose\nor semantic map often fail for the unstable appearance of a human, our\nframework can still produce concrete results, which are competitive to those\nimagined from multi-view input.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:04:35 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 05:03:23 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 07:17:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ran", "Haoxi", ""], ["Wang", "Guangfu", ""], ["Lu", "Li", ""]]}, {"id": "2011.12026", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny", "title": "Adversarial Generation of Continuous Images", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:06:40 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 09:00:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Ignatyev", "Savva", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2011.12043", "submitter": "Lukas Mauch", "authors": "Lukas Mauch, Stephen Tiedemann, Javier Alonso Garcia, Bac Nguyen Cong,\n  Kazuki Yoshiyama, Fabien Cardinaux, Thomas Kemp", "title": "Efficient Sampling for Predictor-Based Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, predictor-based algorithms emerged as a promising approach for\nneural architecture search (NAS). For NAS, we typically have to calculate the\nvalidation accuracy of a large number of Deep Neural Networks (DNNs), what is\ncomputationally complex. Predictor-based NAS algorithms address this problem.\nThey train a proxy model that can infer the validation accuracy of DNNs\ndirectly from their network structure. During optimization, the proxy can be\nused to narrow down the number of architectures for which the true validation\naccuracy must be computed, what makes predictor-based algorithms sample\nefficient. Usually, we compute the proxy for all DNNs in the network search\nspace and pick those that maximize the proxy as candidates for optimization.\nHowever, that is intractable in practice, because the search spaces are often\nvery large and contain billions of network architectures. The contributions of\nthis paper are threefold: 1) We define a sample efficiency gain to compare\ndifferent predictor-based NAS algorithms. 2) We conduct experiments on the\nNASBench-101 dataset and show that the sample efficiency of predictor-based\nalgorithms decreases dramatically if the proxy is only computed for a subset of\nthe search space. 3) We show that if we choose the subset of the search space\non which the proxy is evaluated in a smart way, the sample efficiency of the\noriginal predictor-based algorithm that has access to the full search space can\nbe regained. This is an important step to make predictor-based NAS algorithms\nuseful, in practice.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:36:36 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Mauch", "Lukas", ""], ["Tiedemann", "Stephen", ""], ["Garcia", "Javier Alonso", ""], ["Cong", "Bac Nguyen", ""], ["Yoshiyama", "Kazuki", ""], ["Cardinaux", "Fabien", ""], ["Kemp", "Thomas", ""]]}, {"id": "2011.12075", "submitter": "Eduardo C\\'esar Garrido Merch\\'an", "authors": "Alejandro Sobrino and Eduardo C. Garrido-Merchan and Cristina Puente", "title": "Fuzzy Stochastic Timed Petri Nets for Causal properties representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagery is frequently used to model, represent and communicate knowledge. In\nparticular, graphs are one of the most powerful tools, being able to represent\nrelations between objects. Causal relations are frequently represented by\ndirected graphs, with nodes denoting causes and links denoting causal\ninfluence. A causal graph is a skeletal picture, showing causal associations\nand impact between entities. Common methods used for graphically representing\ncausal scenarios are neurons, truth tables, causal Bayesian networks, cognitive\nmaps and Petri Nets. Causality is often defined in terms of precedence (the\ncause precedes the effect), concurrency (often, an effect is provoked\nsimultaneously by two or more causes), circularity (a cause provokes the effect\nand the effect reinforces the cause) and imprecision (the presence of the cause\nfavors the effect, but not necessarily causes it). We will show that, even\nthough the traditional graphical models are able to represent separately some\nof the properties aforementioned, they fail trying to illustrate indistinctly\nall of them. To approach that gap, we will introduce Fuzzy Stochastic Timed\nPetri Nets as a graphical tool able to represent time, co-occurrence, looping\nand imprecision in causal flow.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:22:34 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Sobrino", "Alejandro", ""], ["Garrido-Merchan", "Eduardo C.", ""], ["Puente", "Cristina", ""]]}, {"id": "2011.12077", "submitter": "Muhammad Zaigham Zaheer", "authors": "Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee", "title": "CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy\n  Suppression for Anomalous Event Detection", "comments": "Presented in the European Conference on Computer Vision ECCV 2020.\n  (Changes from actual paper: 1) Recently published methods have been added in\n  ShanghaiTech and UCF Crime comparison tabs. 2) Due to some error in arxiv\n  compilation, few references are exceeding the paragraph. Also, word\n  'normalcy' in the title is misspelling despite being correct in the code.\n  (Contents are intact)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to detect real-world anomalous events through video-level labels is\na challenging task due to the rare occurrence of anomalies as well as noise in\nthe labels. In this work, we propose a weakly supervised anomaly detection\nmethod which has manifold contributions including1) a random batch based\ntraining procedure to reduce inter-batch correlation, 2) a normalcy suppression\nmechanism to minimize anomaly scores of the normal regions of a video by taking\ninto account the overall information available in one training batch, and 3) a\nclustering distance based loss to contribute towards mitigating the label noise\nand to produce better anomaly representations by encouraging our model to\ngenerate distinct normal and anomalous clusters. The proposed method\nobtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and\nShanghaiTech datasets respectively, demonstrating its superiority over the\nexisting state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:27:40 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 04:11:27 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 16:08:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zaheer", "Muhammad Zaigham", ""], ["Mahmood", "Arif", ""], ["Astrid", "Marcella", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2011.12081", "submitter": "Brandon Bennett Dr", "authors": "Suk Joon Hong, Brandon Bennett", "title": "Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning\n  and Machine Learning", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Winograd Schema Challenge (WSC) is a common-sense reasoning task that\nrequires background knowledge. In this paper, we contribute to tackling WSC in\nfour ways. Firstly, we suggest a keyword method to define a restricted domain\nwhere distinctive high-level semantic patterns can be found. A thanking domain\nwas defined by key-words, and the data set in this domain is used in our\nexperiments. Secondly, we develop a high-level knowledge-based reasoning method\nusing semantic roles which is based on the method of Sharma [2019]. Thirdly, we\npropose an ensemble method to combine knowledge-based reasoning and machine\nlearning which shows the best performance in our experiments. As a machine\nlearning method, we used Bidirectional Encoder Representations from\nTransformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we\nsuggest a \"robust\" accuracy measurement by modifying that of Trichelair et al.\n[2018]. As with their switching method, we evaluate a model by considering its\nperformance on trivial variants of each sentence in the test set.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:34:38 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Hong", "Suk Joon", ""], ["Bennett", "Brandon", ""]]}, {"id": "2011.12091", "submitter": "Xirong Li", "authors": "Xirong Li and Fangming Zhou and Chaoxi Xu and Jiaqi Ji and Gang Yang", "title": "SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries", "comments": "accepted for publication as a REGULAR paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search\n(AVS), is a core theme in multimedia data management and retrieval. The success\nof AVS counts on cross-modal representation learning that encodes both query\nsentences and videos into common spaces for semantic similarity computation.\nInspired by the initial success of previously few works in combining multiple\nsentence encoders, this paper takes a step forward by developing a new and\ngeneral method for effectively exploiting diverse sentence encoders. The\nnovelty of the proposed method, which we term Sentence Encoder Assembly (SEA),\nis two-fold. First, different from prior art that use only a single common\nspace, SEA supports text-video matching in multiple encoder-specific common\nspaces. Such a property prevents the matching from being dominated by a\nspecific encoder that produces an encoding vector much longer than other\nencoders. Second, in order to explore complementarities among the individual\ncommon spaces, we propose multi-space multi-loss learning. As extensive\nexperiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)\nshow, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to\nimplement. All this makes SEA an appealing solution for AVS and promising for\ncontinuously advancing the task by harvesting new sentence encoders.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Li", "Xirong", ""], ["Zhou", "Fangming", ""], ["Xu", "Chaoxi", ""], ["Ji", "Jiaqi", ""], ["Yang", "Gang", ""]]}, {"id": "2011.12143", "submitter": "Lukun Zheng", "authors": "Yuhang Jiang, Lukun Zheng", "title": "Deep learning for video game genre classification", "comments": "21 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2011.07658", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video game genre classification based on its cover and textual description\nwould be utterly beneficial to many modern identification, collocation, and\nretrieval systems. At the same time, it is also an extremely challenging task\ndue to the following reasons: First, there exists a wide variety of video game\ngenres, many of which are not concretely defined. Second, video game covers\nvary in many different ways such as colors, styles, textual information, etc,\neven for games of the same genre. Third, cover designs and textual descriptions\nmay vary due to many external factors such as country, culture, target reader\npopulations, etc. With the growing competitiveness in the video game industry,\nthe cover designers and typographers push the cover designs to its limit in the\nhope of attracting sales. The computer-based automatic video game genre\nclassification systems become a particularly exciting research topic in recent\nyears. In this paper, we propose a multi-modal deep learning framework to solve\nthis problem. The contribution of this paper is four-fold. First, we compiles a\nlarge dataset consisting of 50,000 video games from 21 genres made of cover\nimages, description text, and title text and the genre information. Second,\nimage-based and text-based, state-of-the-art models are evaluated thoroughly\nfor the task of genre classification for video games. Third, we developed an\nefficient and salable multi-modal framework based on both images and texts.\nFourth, a thorough analysis of the experimental results is given and future\nworks to improve the performance is suggested. The results show that the\nmulti-modal framework outperforms the current state-of-the-art image-based or\ntext-based models. Several challenges are outlined for this task. More efforts\nand resources are needed for this classification task in order to reach a\nsatisfactory level.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 22:31:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jiang", "Yuhang", ""], ["Zheng", "Lukun", ""]]}, {"id": "2011.12149", "submitter": "Sheng Ao", "authors": "Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, Yulan Guo", "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting robust and general 3D local features is key to downstream tasks\nsuch as point cloud registration and reconstruction. Existing learning-based\nlocal descriptors are either sensitive to rotation transformations, or rely on\nclassical handcrafted features which are neither general nor representative. In\nthis paper, we introduce a new, yet conceptually simple, neural architecture,\ntermed SpinNet, to extract local features which are rotationally invariant\nwhilst sufficiently informative to enable accurate registration. A Spatial\nPoint Transformer is first introduced to map the input local surface into a\ncarefully designed cylindrical space, enabling end-to-end optimization with\nSO(2) equivariant representation. A Neural Feature Extractor which leverages\nthe powerful point-based and 3D cylindrical convolutional neural layers is then\nutilized to derive a compact and representative descriptor for matching.\nExtensive experiments on both indoor and outdoor datasets demonstrate that\nSpinNet outperforms existing state-of-the-art techniques by a large margin.\nMore critically, it has the best generalization ability across unseen scenarios\nwith different sensor modalities. The code is available at\nhttps://github.com/QingyongHu/SpinNet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:00:56 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:42:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ao", "Sheng", ""], ["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Markham", "Andrew", ""], ["Guo", "Yulan", ""]]}, {"id": "2011.12172", "submitter": "Chen Chen", "authors": "Sijie Zhu and Taojiannan Yang and Chen Chen", "title": "VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image geo-localization aims to determine the locations of\nstreet-view query images by matching with GPS-tagged reference images from\naerial view. Recent works have achieved surprisingly high retrieval accuracy on\ncity-scale datasets. However, these results rely on the assumption that there\nexists a reference image exactly centered at the location of any query image,\nwhich is not applicable for practical scenarios. In this paper, we redefine\nthis problem with a more realistic assumption that the query image can be\narbitrary in the area of interest and the reference images are captured before\nthe queries emerge. This assumption breaks the one-to-one retrieval setting of\nexisting datasets as the queries and reference images are not perfectly aligned\npairs, and there may be multiple reference images covering one query location.\nTo bridge the gap between this realistic setting and existing datasets, we\npropose a new large-scale benchmark -- VIGOR -- for cross-View Image\nGeo-localization beyond One-to-one Retrieval. We benchmark existing\nstate-of-the-art methods and propose a novel end-to-end framework to localize\nthe query in a coarse-to-fine manner. Apart from the image-level retrieval\naccuracy, we also evaluate the localization accuracy in terms of the actual\ndistance (meters) using the raw GPS data. Extensive experiments are conducted\nunder different application scenarios to validate the effectiveness of the\nproposed method. The results indicate that cross-view geo-localization in this\nrealistic setting is still challenging, fostering new research in this\ndirection. Our dataset and code will be released at\n\\url{https://github.com/Jeff-Zilence/VIGOR}\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:50:54 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 04:01:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2011.12193", "submitter": "Susie Xi Rao", "authors": "Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Zhiyao\n  Chen, Yinan Shan, Yang Zhao, Ce Zhang", "title": "xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs", "comments": "15 pages, 3 figures, under review in WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  At online retail platforms, it is crucial to actively detect risks of\nfraudulent transactions to improve our customer experience, minimize loss, and\nprevent unauthorized chargebacks. Traditional rule-based methods and simple\nfeature-based models are either inefficient or brittle and uninterpretable. The\ngraph structure that exists among the heterogeneous typed entities of the\ntransaction logs is informative and difficult to fake. To utilize the\nheterogeneous graph relationships and enrich the explainability, we present\nxFraud, an explainable Fraud transaction prediction system. xFraud is composed\nof a predictor which learns expressive representations for malicious\ntransaction detection from the heterogeneous transaction graph via a\nself-attentive heterogeneous graph neural network, and an explainer that\ngenerates meaningful and human understandable explanations from graphs to\nfacilitate further process in business unit. In our experiments with xFraud on\ntwo real transaction networks with up to ten millions transactions, we are able\nto achieve an area under a curve (AUC) score that outperforms baseline models\nand graph embedding methods. In addition, we show how the explainer could\nbenefit the understanding towards model predictions and enhance model\ntrustworthiness for real-world fraud transaction cases.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 16:37:15 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Rao", "Susie Xi", ""], ["Zhang", "Shuai", ""], ["Han", "Zhichao", ""], ["Zhang", "Zitao", ""], ["Min", "Wei", ""], ["Chen", "Zhiyao", ""], ["Shan", "Yinan", ""], ["Zhao", "Yang", ""], ["Zhang", "Ce", ""]]}, {"id": "2011.12203", "submitter": "Aneesh Pappu", "authors": "Aneesh Pappu, Brooks Paige", "title": "Making Graph Neural Networks Worth It for Low-Data Molecular Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks have become very popular for machine learning on\nmolecules due to the expressive power of their learnt representations. However,\nmolecular machine learning is a classically low-data regime and it isn't clear\nthat graph neural networks can avoid overfitting in low-resource settings. In\ncontrast, fingerprint methods are the traditional standard for low-data\nenvironments due to their reduced number of parameters and manually engineered\nfeatures. In this work, we investigate whether graph neural networks are\ncompetitive in small data settings compared to the parametrically 'cheaper'\nalternative of fingerprint methods. When we find that they are not, we explore\npretraining and the meta-learning method MAML (and variants FO-MAML and ANIL)\nfor improving graph neural network performance by transfer learning from\nrelated tasks. We find that MAML and FO-MAML do enable the graph neural network\nto outperform models based on fingerprints, providing a path to using graph\nneural networks even in settings with severely restricted data availability. In\ncontrast to previous work, we find ANIL performs worse that other meta-learning\napproaches in this molecule setting. Our results suggest two reasons: molecular\nmachine learning tasks may require significant task-specific adaptation, and\ndistribution shifts in test tasks relative to train tasks may contribute to\nworse ANIL performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 16:52:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Pappu", "Aneesh", ""], ["Paige", "Brooks", ""]]}, {"id": "2011.12216", "submitter": "Shuang Li", "authors": "Shuang Li, Yilun Du, Gido M. van de Ven, Igor Mordatch", "title": "Energy-Based Models for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate Energy-Based Models (EBMs) as a promising model class for\ncontinual learning problems. Instead of tackling continual learning via the use\nof external memory, growing models, or regularization, EBMs have a natural way\nto support a dynamically-growing number of tasks or classes that causes less\ninterference with previously learned information. Our proposed version of EBMs\nfor continual learning is simple, efficient and outperforms baseline methods by\na large margin on several benchmarks. Moreover, our proposed contrastive\ndivergence based training objective can be applied to other continual learning\nmethods, resulting in substantial boosts in their performance. We also show\nthat EBMs are adaptable to a more general continual learning setting where the\ndata distribution changes without the notion of explicitly delineated tasks.\nThese observations point towards EBMs as a class of models naturally inclined\ntowards the continual learning regime.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 17:08:13 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:00:33 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Shuang", ""], ["Du", "Yilun", ""], ["van de Ven", "Gido M.", ""], ["Mordatch", "Igor", ""]]}, {"id": "2011.12262", "submitter": "Sachin Grover", "authors": "Sachin Grover, David Smith, Subbarao Kambhampati", "title": "Model Elicitation through Direct Questioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The future will be replete with scenarios where humans are robots will be\nworking together in complex environments. Teammates interact, and the robot's\ninteraction has to be about getting useful information about the human's\n(teammate's) model. There are many challenges before a robot can interact, such\nas incorporating the structural differences in the human's model, ensuring\nsimpler responses, etc. In this paper, we investigate how a robot can interact\nto localize the human model from a set of models. We show how to generate\nquestions to refine the robot's understanding of the teammate's model. We\nevaluate the method in various planning domains. The evaluation shows that\nthese questions can be generated offline, and can help refine the model through\nsimple answers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:17:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Grover", "Sachin", ""], ["Smith", "David", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2011.12334", "submitter": "Nan Jiang", "authors": "Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue", "title": "Language Generation via Combinatorial Constraint Satisfaction: A Tree\n  Search Enhanced Monte-Carlo Approach", "comments": "Findings of the Association for Computational Linguistics: EMNLP\n  2020, pages 1286-1298. November 16 - 20, 2020. 2020 Association for\n  Computational Linguistics", "journal-ref": "Findings of the Association for Computational Linguistics: EMNLP\n  2020, pages 1286-1298. November 16 - 20, 2020. 2020 Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating natural language under complex constraints is a principled\nformulation towards controllable text generation. We present a framework to\nallow specification of combinatorial constraints for sentence generation. We\npropose TSMH, an efficient method to generate high likelihood sentences with\nrespect to a pre-trained language model while satisfying the constraints. Our\napproach is highly flexible, requires no task-specific training, and leverages\nefficient constraint satisfaction solving techniques. To better handle the\ncombinatorial constraints, a tree search algorithm is embedded into the\nproposal process of the Markov chain Monte Carlo (MCMC) to explore candidates\nthat satisfy more constraints. Compared to existing MCMC approaches, our\nsampling approach has a better mixing performance. Experiments show that TSMH\nachieves consistent and significant improvement on multiple language generation\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 19:21:00 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 00:15:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhang", "Maosen", ""], ["Jiang", "Nan", ""], ["Li", "Lei", ""], ["Xue", "Yexiang", ""]]}, {"id": "2011.12340", "submitter": "Larry Heck", "authors": "Larry Heck and Simon Heck", "title": "Zero-Shot Visual Slot Filling as Question Answering", "comments": "5 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new approach to visual zero-shot slot filling. The\napproach extends previous approaches by reformulating the slot filling task as\nQuestion Answering. Slot tags are converted to rich natural language questions\nthat capture the semantics of visual information and lexical text on the GUI\nscreen. These questions are paired with the user's utterance and slots are\nextracted from the utterance using a state-of-the-art ALBERT-based Question\nAnswering system trained on the Stanford Question Answering dataset (SQuaD2).\nAn approach to further refine the model with multi-task training is presented.\nThe multi-task approach facilitates the incorporation of a large number of\nsuccessive refinements and transfer learning across similar tasks. A new Visual\nSlot dataset and a visual extension of the popular ATIS dataset is introduced\nto support research and experimentation on visual slot filling. Results show F1\nscores between 0.52 and 0.60 on the Visual Slot and ATIS datasets with no\ntraining data (zero-shot).\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 19:47:53 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Heck", "Larry", ""], ["Heck", "Simon", ""]]}, {"id": "2011.12351", "submitter": "Kenny Young", "authors": "Kenny Young", "title": "Hindsight Network Credit Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hindsight Network Credit Assignment (HNCA), a novel learning\nmethod for stochastic neural networks, which works by assigning credit to each\nneuron's stochastic output based on how it influences the output of its\nimmediate children in the network. We prove that HNCA provides unbiased\ngradient estimates while reducing variance compared to the REINFORCE estimator.\nWe also experimentally demonstrate the advantage of HNCA over REINFORCE in a\ncontextual bandit version of MNIST. The computational complexity of HNCA is\nsimilar to that of backpropagation. We believe that HNCA can help stimulate new\nways of thinking about credit assignment in stochastic compute graphs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 20:16:45 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Young", "Kenny", ""]]}, {"id": "2011.12363", "submitter": "Gabriel Loaiza-Ganem", "authors": "Panteha Naderian, Gabriel Loaiza-Ganem, Harry J. Braviner, Anthony L.\n  Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg", "title": "C-Learning: Horizon-Aware Cumulative Accessibility Estimation", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-goal reaching is an important problem in reinforcement learning needed\nto achieve algorithmic generalization. Despite recent advances in this field,\ncurrent algorithms suffer from three major challenges: high sample complexity,\nlearning only a single way of reaching the goals, and difficulties in solving\ncomplex motion planning tasks. In order to address these limitations, we\nintroduce the concept of cumulative accessibility functions, which measure the\nreachability of a goal from a given state within a specified horizon. We show\nthat these functions obey a recurrence relation, which enables learning from\noffline interactions. We also prove that optimal cumulative accessibility\nfunctions are monotonic in the planning horizon. Additionally, our method can\ntrade off speed and reliability in goal-reaching by suggesting multiple paths\nto a single goal depending on the provided horizon. We evaluate our approach on\na set of multi-goal discrete and continuous control tasks. We show that our\nmethod outperforms state-of-the-art goal-reaching algorithms in success rate,\nsample complexity, and path optimality. Our code is available at\nhttps://github.com/layer6ai-labs/CAE, and additional visualizations can be\nfound at https://sites.google.com/view/learning-cae/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 20:34:31 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:20:47 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 03:05:38 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Naderian", "Panteha", ""], ["Loaiza-Ganem", "Gabriel", ""], ["Braviner", "Harry J.", ""], ["Caterini", "Anthony L.", ""], ["Cresswell", "Jesse C.", ""], ["Li", "Tong", ""], ["Garg", "Animesh", ""]]}, {"id": "2011.12384", "submitter": "Chen Chen", "authors": "Sijie Zhu and Taojiannan Yang and Matias Mendieta and Chen Chen", "title": "A3D: Adaptive 3D Networks for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents A3D, an adaptive 3D network that can infer at a wide\nrange of computational constraints with one-time training. Instead of training\nmultiple models in a grid-search manner, it generates good configurations by\ntrading off between network width and spatio-temporal resolution. Furthermore,\nthe computation cost can be adapted after the model is deployed to meet\nvariable constraints, for example, on edge devices. Even under the same\ncomputational constraints, the performance of our adaptive networks can be\nsignificantly boosted over the baseline counterparts by the mutual training\nalong three dimensions. When a multiple pathway framework, e.g. SlowFast, is\nadopted, our adaptive method encourages a better trade-off between pathways\nthan manual designs. Extensive experiments on the Kinetics dataset show the\neffectiveness of the proposed framework. The performance gain is also verified\nto transfer well between datasets and tasks. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:01:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Mendieta", "Matias", ""], ["Chen", "Chen", ""]]}, {"id": "2011.12424", "submitter": "Yang Li", "authors": "Yang Li", "title": "Interpretable Models in ANNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial neural networks are often very complex and too deep for a human to\nunderstand. As a result, they are usually referred to as black boxes. For a lot\nof real-world problems, the underlying pattern itself is very complicated, such\nthat an analytic solution does not exist. However, in some cases, laws of\nphysics, for example, the pattern can be described by relatively simple\nmathematical expressions. In that case, we want to get a readable equation\nrather than a black box. In this paper, we try to find a way to explain a\nnetwork and extract a human-readable equation that describes the model.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:09:10 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Li", "Yang", ""]]}, {"id": "2011.12439", "submitter": "Spyros Angelopoulos", "authors": "Spyros Angelopoulos and Shahin Kamali", "title": "Contract Scheduling With Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contract scheduling is a general technique that allows to design a system\nwith interruptible capabilities, given an algorithm that is not necessarily\ninterruptible. Previous work on this topic has largely assumed that the\ninterruption is a worst-case deadline that is unknown to the scheduler. In this\nwork, we study the setting in which there is a potentially erroneous prediction\nconcerning the interruption. Specifically, we consider the setting in which the\nprediction describes the time that the interruption occurs, as well as the\nsetting in which the prediction is obtained as a response to a single or\nmultiple binary queries. For both settings, we investigate tradeoffs between\nthe robustness (i.e., the worst-case performance assuming adversarial\nprediction) and the consistency (i.e, the performance assuming that the\nprediction is error-free), both from the side of positive and negative results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:00:04 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Angelopoulos", "Spyros", ""], ["Kamali", "Shahin", ""]]}, {"id": "2011.12443", "submitter": "Kyle Tilbury", "authors": "Kyle Tilbury, Jesse Hoey", "title": "The Human Effect Requires Affect: Addressing Social-Psychological\n  Factors of Climate Change with Machine Learning", "comments": "Accepted paper at the Tackling Climate Change with Machine Learning\n  workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning has the potential to aid in mitigating the human effects of\nclimate change. Previous applications of machine learning to tackle the human\neffects in climate change include approaches like informing individuals of\ntheir carbon footprint and strategies to reduce it. For these methods to be the\nmost effective they must consider relevant social-psychological factors for\neach individual. Of social-psychological factors at play in climate change,\naffect has been previously identified as a key element in perceptions and\nwillingness to engage in mitigative behaviours. In this work, we propose an\ninvestigation into how affect could be incorporated to enhance machine learning\nbased interventions for climate change. We propose using affective agent-based\nmodelling for climate change as well as the use of a simulated climate change\nsocial dilemma to explore the potential benefits of affective machine learning\ninterventions. Behavioural and informational interventions can be a powerful\ntool in helping humans adopt mitigative behaviours. We expect that utilizing\naffective ML can make interventions an even more powerful tool and help\nmitigative behaviours become widely adopted.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:34:54 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Tilbury", "Kyle", ""], ["Hoey", "Jesse", ""]]}, {"id": "2011.12445", "submitter": "Jing Yang", "authors": "Jing Yang, Chun Ouyang, Wil M.P. van der Aalst, Arthur H.M. ter\n  Hofstede, Yang Yu", "title": "OrgMining 2.0: A Novel Framework for Organizational Model Mining from\n  Event Logs", "comments": "Manuscript initially submitted for review on 13/5/2020 with 38 pages,\n  10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing appropriate structures around human resources can streamline\noperations and thus facilitate the competitiveness of an organization. To\nachieve this goal, modern organizations need to acquire an accurate and timely\nunderstanding of human resource grouping while faced with an ever-changing\nenvironment. The use of process mining offers a promising way to help address\nthe need through utilizing event log data stored in information systems. By\nextracting knowledge about the actual behavior of resources participating in\nbusiness processes from event logs, organizational models can be constructed,\nwhich facilitate the analysis of the de facto grouping of human resources\nrelevant to process execution. Nevertheless, open research gaps remain to be\naddressed when applying the state-of-the-art process mining to analyze resource\ngrouping. For one, the discovery of organizational models has only limited\nconnections with the context of process execution. For another, a rigorous\nsolution that evaluates organizational models against event log data is yet to\nbe proposed. In this paper, we aim to tackle these research challenges by\ndeveloping a novel framework built upon a richer definition of organizational\nmodels coupling resource grouping with process execution knowledge. By\nintroducing notions of conformance checking for organizational models, the\nframework allows effective evaluation of organizational models, and therefore\nprovides a foundation for analyzing and improving resource grouping based on\nevent logs. We demonstrate the feasibility of this framework by proposing an\napproach underpinned by the framework for organizational model discovery, and\nalso conduct experiments on real-life event logs to discover and evaluate\norganizational models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:46:47 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 02:15:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yang", "Jing", ""], ["Ouyang", "Chun", ""], ["van der Aalst", "Wil M. P.", ""], ["ter Hofstede", "Arthur H. M.", ""], ["Yu", "Yang", ""]]}, {"id": "2011.12461", "submitter": "Wei Wang", "authors": "Wei Wang, Chao Zhang, Xiaopei Wu", "title": "SAR-Net: A End-to-End Deep Speech Accent Recognition Network", "comments": "10 pages, 7 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a end-to-end deep network to recognize kinds of accents\nunder the same language, where we develop and transfer the deep architecture in\nspeaker-recognition area to accent classification task for learning\nutterance-level accent representation. Compared with the individual-level\nfeature in speaker-recognition, accent recognition throws a more challenging\nissue in acquiring compact group-level features for the speakers with the same\naccent, hence a good discriminative accent feature space is desired. Our deep\nframework adopts multitask-learning mechanism and mainly consists of three\nmodules: a shared CNNs and RNNs based front-end encoder, a core accent\nrecognition branch, and an auxiliary speech recognition branch, where we take\nspeech spectrogram as input. More specifically, with the sequential descriptors\nlearned from a shared encoder, the accent recognition branch first condenses\nall descriptors into an embedding vector, and then explores different\ndiscriminative loss functions which are popular in face recognition domain to\nenhance embedding discrimination. Additionally, due to the accent is a\nspeaking-related timbre, adding speech recognition branch effectively curbs the\nover-fitting phenomenon in accent recognition during training. We show that our\nnetwork without any data-augment preproccessings is significantly ahead of the\nbaseline system on the accent classification track in the Accented English\nSpeech Recognition Challenge 2020 (AESRC2020), where the state-of-the-art loss\nfunction Circle-Loss achieves the best discriminative optimization for accent\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 00:46:47 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 03:44:41 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 13:32:32 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wang", "Wei", ""], ["Zhang", "Chao", ""], ["Wu", "Xiaopei", ""]]}, {"id": "2011.12465", "submitter": "Sunipa Dev", "authors": "Sunipa Dev", "title": "The Geometry of Distributed Representations for Better Alignment,\n  Attenuated Bias, and Improved Interpretability", "comments": "PhD thesis, University of Utah (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional representations for words, text, images, knowledge graphs\nand other structured data are commonly used in different paradigms of machine\nlearning and data mining. These representations have different degrees of\ninterpretability, with efficient distributed representations coming at the cost\nof the loss of feature to dimension mapping. This implies that there is\nobfuscation in the way concepts are captured in these embedding spaces. Its\neffects are seen in many representations and tasks, one particularly\nproblematic one being in language representations where the societal biases,\nlearned from underlying data, are captured and occluded in unknown dimensions\nand subspaces. As a result, invalid associations (such as different races and\ntheir association with a polar notion of good versus bad) are made and\npropagated by the representations, leading to unfair outcomes in different\ntasks where they are used. This work addresses some of these problems\npertaining to the transparency and interpretability of such representations. A\nprimary focus is the detection, quantification, and mitigation of socially\nbiased associations in language representation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:04:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Dev", "Sunipa", ""]]}, {"id": "2011.12468", "submitter": "Chandra Maddila", "authors": "Chandra Maddila, Sai Surya Upadrasta, Chetan Bansal, Nachiappan\n  Nagappan, Georgios Gousios, Arie van Deursen", "title": "Nudge: Accelerating Overdue Pull Requests Towards Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pull requests are a key part of the collaborative software development and\ncode review process today. However, pull requests can also slow down the\nsoftware development process when the reviewer(s) or the author do not actively\nengage with the pull request. In this work, we design an end-to-end service,\nNudge, for accelerating overdue pull requests towards completion by reminding\nthe author or the reviewer(s) to engage with their overdue pull requests.\nFirst, we use models based on effort estimation and machine learning to predict\nthe completion time for a given pull request. Second, we use activity detection\nto reduce false positives. Lastly, we use dependency determination to\nunderstand the blocker of the pull request and nudge the appropriate\nactor(author or reviewer(s)). We also do a correlation analysis to understand\nthe statistical relationship between the pull request completion times and\nvarious pull request and developer related attributes. Nudge has been deployed\non 147 repositories at Microsoft since 2019. We do a large scale evaluation\nbased on the implicit and explicit feedback we received from sending the Nudge\nnotifications on 8,500 pull requests. We observe significant reduction in\ncompletion time, by over 60%, for pull requests which were nudged thus\nincreasing the efficiency of the code review process and accelerating the pull\nrequest progression.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:22:29 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:06:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Maddila", "Chandra", ""], ["Upadrasta", "Sai Surya", ""], ["Bansal", "Chetan", ""], ["Nagappan", "Nachiappan", ""], ["Gousios", "Georgios", ""], ["van Deursen", "Arie", ""]]}, {"id": "2011.12470", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xuanbai Chen, Xiangyu Yue, Chuang Lin, Pengfei Xu, Ravi\n  Krishna, Jufeng Yang, Guiguang Ding, Alberto L. Sangiovanni-Vincentelli, Kurt\n  Keutzer", "title": "Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual\n  Emotion Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to large-scale labeled training data, deep neural networks (DNNs) have\nobtained remarkable success in many vision and multimedia tasks. However,\nbecause of the presence of domain shift, the learned knowledge of the\nwell-trained DNNs cannot be well generalized to new domains or datasets that\nhave few labels. Unsupervised domain adaptation (UDA) studies the problem of\ntransferring models trained on one labeled source domain to another unlabeled\ntarget domain. In this paper, we focus on UDA in visual emotion analysis for\nboth emotion distribution learning and dominant emotion classification.\nSpecifically, we design a novel end-to-end cycle-consistent adversarial model,\ntermed CycleEmotionGAN++. First, we generate an adapted domain to align the\nsource and target domains on the pixel-level by improving CycleGAN with a\nmulti-scale structured cycle-consistency loss. During the image translation, we\npropose a dynamic emotional semantic consistency loss to preserve the emotion\nlabels of the source images. Second, we train a transferable task classifier on\nthe adapted domain with feature-level alignment between the adapted and target\ndomains. We conduct extensive UDA experiments on the Flickr-LDL & Twitter-LDL\ndatasets for distribution learning and ArtPhoto & FI datasets for emotion\nclassification. The results demonstrate the significant improvements yielded by\nthe proposed CycleEmotionGAN++ as compared to state-of-the-art UDA approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:31:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhao", "Sicheng", ""], ["Chen", "Xuanbai", ""], ["Yue", "Xiangyu", ""], ["Lin", "Chuang", ""], ["Xu", "Pengfei", ""], ["Krishna", "Ravi", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2011.12491", "submitter": "Bradly Stadie", "authors": "Lunjun Zhang, Ge Yang, Bradly C. Stadie", "title": "World Model as a Graph: Learning Latent Landmarks for Planning", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML). 2021", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning - the ability to analyze the structure of a problem in the large and\ndecompose it into interrelated subproblems - is a hallmark of human\nintelligence. While deep reinforcement learning (RL) has shown great promise\nfor solving relatively straightforward control tasks, it remains an open\nproblem how to best incorporate planning into existing deep RL paradigms to\nhandle increasingly complex environments. One prominent framework, Model-Based\nRL, learns a world model and plans using step-by-step virtual rollouts. This\ntype of world model quickly diverges from reality when the planning horizon\nincreases, thus struggling at long-horizon planning. How can we learn world\nmodels that endow agents with the ability to do temporally extended reasoning?\nIn this work, we propose to learn graph-structured world models composed of\nsparse, multi-step transitions. We devise a novel algorithm to learn latent\nlandmarks that are scattered (in terms of reachability) across the goal space\nas the nodes on the graph. In this same graph, the edges are the reachability\nestimates distilled from Q-functions. On a variety of high-dimensional\ncontinuous control tasks ranging from robotic manipulation to navigation, we\ndemonstrate that our method, named L3P, significantly outperforms prior work,\nand is oftentimes the only method capable of leveraging both the robustness of\nmodel-free RL and generalization of graph-search algorithms. We believe our\nwork is an important step towards scalable planning in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:49:21 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:40:47 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 21:00:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhang", "Lunjun", ""], ["Yang", "Ge", ""], ["Stadie", "Bradly C.", ""]]}, {"id": "2011.12511", "submitter": "Sen Lin", "authors": "Sen Lin, Li Yang, Zhezhi He, Deliang Fan, Junshan Zhang", "title": "MetaGater: Fast Learning of Conditional Channel Gated Networks via\n  Federated Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has achieved phenomenal successes in many AI\napplications, its enormous model size and intensive computation requirements\npose a formidable challenge to the deployment in resource-limited nodes. There\nhas recently been an increasing interest in computationally-efficient learning\nmethods, e.g., quantization, pruning and channel gating. However, most existing\ntechniques cannot adapt to different tasks quickly. In this work, we advocate a\nholistic approach to jointly train the backbone network and the channel gating\nwhich enables dynamical selection of a subset of filters for more efficient\nlocal computation given the data input. Particularly, we develop a federated\nmeta-learning approach to jointly learn good meta-initializations for both\nbackbone networks and gating modules, by making use of the model similarity\nacross learning tasks on different nodes. In this way, the learnt meta-gating\nmodule effectively captures the important filters of a good meta-backbone\nnetwork, based on which a task-specific conditional channel gated network can\nbe quickly adapted, i.e., through one-step gradient descent, from the\nmeta-initializations in a two-stage procedure using new samples of that task.\nThe convergence of the proposed federated meta-learning algorithm is\nestablished under mild conditions. Experimental results corroborate the\neffectiveness of our method in comparison to related work.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 04:26:23 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 16:29:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lin", "Sen", ""], ["Yang", "Li", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""], ["Zhang", "Junshan", ""]]}, {"id": "2011.12517", "submitter": "Yadan Luo", "authors": "Yadan Luo, Zi Huang, Hongxu Chen, Yang Yang, Mahsa Baktashmotlagh", "title": "Interpretable Signed Link Prediction with Signed Infomax Hyperbolic\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Signed link prediction in social networks aims to reveal the underlying\nrelationships (i.e. links) among users (i.e. nodes) given their existing\npositive and negative interactions observed. Most of the prior efforts are\ndevoted to learning node embeddings with graph neural networks (GNNs), which\npreserve the signed network topology by message-passing along edges to\nfacilitate the downstream link prediction task. Nevertheless, the existing\ngraph-based approaches could hardly provide human-intelligible explanations for\nthe following three questions: (1) which neighbors to aggregate, (2) which path\nto propagate along, and (3) which social theory to follow in the learning\nprocess. To answer the aforementioned questions, in this paper, we investigate\nhow to reconcile the \\textit{balance} and \\textit{status} social rules with\ninformation theory and develop a unified framework, termed as Signed Infomax\nHyperbolic Graph (\\textbf{SIHG}). By maximizing the mutual information between\nedge polarities and node embeddings, one can identify the most representative\nneighboring nodes that support the inference of edge sign. Different from\nexisting GNNs that could only group features of friends in the subspace, the\nproposed SIHG incorporates the signed attention module, which is also capable\nof pushing hostile users far away from each other to preserve the geometry of\nantagonism. The polarity of the learned edge attention maps, in turn, provide\ninterpretations of the social theories used in each aggregation. In order to\nmodel high-order user relations and complex hierarchies, the node embeddings\nare projected and measured in a hyperbolic space with a lower distortion.\nExtensive experiments on four signed network benchmarks demonstrate that the\nproposed SIHG framework significantly outperforms the state-of-the-arts in\nsigned link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 05:09:03 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 07:42:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Chen", "Hongxu", ""], ["Yang", "Yang", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2011.12548", "submitter": "Y. Sinan Hanay", "authors": "Rustem Ozakar, Rafet Efe Gazanfer and Y. Sinan Hanay", "title": "Measuring Happiness Around the World Through Artificial Intelligence", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we analyze the happiness levels of countries using an unbiased\nemotion detector, artificial intelligence (AI). To date, researchers proposed\nmany factors that may affect happiness such as wealth, health and safety. Even\nthough these factors all seem relevant, there is no clear consensus between\nsociologists on how to interpret these, and the models to estimate the cost of\nthese utilities include some assumptions. Researchers in social sciences have\nbeen working on determination of the happiness levels in society and\nexploration of the factors correlated with it through polls and different\nstatistical methods. In our work, by using artificial intelligence, we\nintroduce a different and relatively unbiased approach to this problem. By\nusing AI, we make no assumption about what makes a person happy, and leave the\ndecision to AI to detect the emotions from the faces of people collected from\npublicly available street footages. We analyzed the happiness levels in eight\ndifferent cities around the world through available footage on the Internet and\nfound out that there is no statistically significant difference between\ncountries in terms of happiness.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 07:12:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ozakar", "Rustem", ""], ["Gazanfer", "Rafet Efe", ""], ["Hanay", "Y. Sinan", ""]]}, {"id": "2011.12566", "submitter": "Po-Lin Lai", "authors": "Po-Lin Lai, Chih-Yun Chen, Liang-Wei Lo, Chien-Chin Chen", "title": "ColdGAN: Resolving Cold Start User Recommendation by using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mitigating the new user cold-start problem has been critical in the\nrecommendation system for online service providers to influence user experience\nin decision making which can ultimately affect the intention of users to use a\nparticular service. Previous studies leveraged various side information from\nusers and items; however, it may be impractical due to privacy concerns. In\nthis paper, we present ColdGAN, an end-to-end GAN based model with no use of\nside information to resolve this problem. The main idea of the proposed model\nis to train a network that learns the rating distributions of experienced users\ngiven their cold-start distributions. We further design a time-based function\nto restore the preferences of users to cold-start states. With extensive\nexperiments on two real-world datasets, the results show that our proposed\nmethod achieves significantly improved performance compared with the\nstate-of-the-art recommenders.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:10:35 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lai", "Po-Lin", ""], ["Chen", "Chih-Yun", ""], ["Lo", "Liang-Wei", ""], ["Chen", "Chien-Chin", ""]]}, {"id": "2011.12582", "submitter": "Deheng Ye", "authors": "Deheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang,\n  Sheng Chen, Mingfei Sun, Xiaoqian Li, Siqin Li, Jing Liang, Zhenjie Lian, Bei\n  Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang", "title": "Supervised Learning Achieves Human-Level Performance in MOBA Games: A\n  Case Study of Honor of Kings", "comments": "IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3029475", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present JueWu-SL, the first supervised-learning-based artificial\nintelligence (AI) program that achieves human-level performance in playing\nmultiplayer online battle arena (MOBA) games. Unlike prior attempts, we\nintegrate the macro-strategy and the micromanagement of MOBA-game-playing into\nneural networks in a supervised and end-to-end manner. Tested on Honor of\nKings, the most popular MOBA at present, our AI performs competitively at the\nlevel of High King players in standard 5v5 games.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:45:55 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ye", "Deheng", ""], ["Chen", "Guibin", ""], ["Zhao", "Peilin", ""], ["Qiu", "Fuhao", ""], ["Yuan", "Bo", ""], ["Zhang", "Wen", ""], ["Chen", "Sheng", ""], ["Sun", "Mingfei", ""], ["Li", "Xiaoqian", ""], ["Li", "Siqin", ""], ["Liang", "Jing", ""], ["Lian", "Zhenjie", ""], ["Shi", "Bei", ""], ["Wang", "Liang", ""], ["Shi", "Tengfei", ""], ["Fu", "Qiang", ""], ["Yang", "Wei", ""], ["Huang", "Lanxiao", ""]]}, {"id": "2011.12586", "submitter": "Linhao Luo", "authors": "Linhao Luo, Liqi Yang, Ju Xin, Yixiang Fang, Xiaofeng Zhang, Xiaofei\n  Yang, Kai Chen, Zhiyuan Zhang, Kai Liu", "title": "RRCN: A Reinforced Random Convolutional Network based Reciprocal\n  Recommendation Approach for Online Dating", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the reciprocal recommendation, especially for online dating\napplications, has attracted more and more research attention. Different from\nconventional recommendation problems, the reciprocal recommendation aims to\nsimultaneously best match users' mutual preferences. Intuitively, the mutual\npreferences might be affected by a few key attributes that users like or\ndislike. Meanwhile, the interactions between users' attributes and their key\nattributes are also important for key attributes selection. Motivated by these\nobservations, in this paper we propose a novel reinforced random convolutional\nnetwork (RRCN) approach for the reciprocal recommendation task. In particular,\nwe technically propose a novel random CNN component that can randomly convolute\nnon-adjacent features to capture their interaction information and learn\nfeature embeddings of key attributes to make the final recommendation.\nMoreover, we design a reinforcement learning based strategy to integrate with\nthe random CNN component to select salient attributes to form the candidate set\nof key attributes. We evaluate the proposed RRCN against a number of both\nbaselines and the state-of-the-art approaches on two real-world datasets, and\nthe promising results have demonstrated the superiority of RRCN against the\ncompared approaches in terms of a number of evaluation criteria.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:55:17 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Luo", "Linhao", ""], ["Yang", "Liqi", ""], ["Xin", "Ju", ""], ["Fang", "Yixiang", ""], ["Zhang", "Xiaofeng", ""], ["Yang", "Xiaofei", ""], ["Chen", "Kai", ""], ["Zhang", "Zhiyuan", ""], ["Liu", "Kai", ""]]}, {"id": "2011.12599", "submitter": "Valentina Anita Carriero", "authors": "Valentina Anita Carriero, Marilena Daquino, Aldo Gangemi, Andrea\n  Giovanni Nuzzolese, Silvio Peroni, Valentina Presutti, Francesca Tomasi", "title": "The Landscape of Ontology Reuse Approaches", "comments": null, "journal-ref": null, "doi": "10.3233/SSW200033", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology reuse aims to foster interoperability and facilitate knowledge\nreuse. Several approaches are typically evaluated by ontology engineers when\nbootstrapping a new project. However, current practices are often motivated by\nsubjective, case-by-case decisions, which hamper the definition of a\nrecommended behaviour. In this chapter we argue that to date there are no\neffective solutions for supporting developers' decision-making process when\ndeciding on an ontology reuse strategy. The objective is twofold: (i) to survey\ncurrent approaches to ontology reuse, presenting motivations, strategies,\nbenefits and limits, and (ii) to analyse two representative approaches and\ndiscuss their merits.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:21:07 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Carriero", "Valentina Anita", ""], ["Daquino", "Marilena", ""], ["Gangemi", "Aldo", ""], ["Nuzzolese", "Andrea Giovanni", ""], ["Peroni", "Silvio", ""], ["Presutti", "Valentina", ""], ["Tomasi", "Francesca", ""]]}, {"id": "2011.12662", "submitter": "Jie Ma", "authors": "Jie Ma, Jun Liu, Junjun Li, Qinghua Zheng, Qingyu Yin, Jianlong Zhou,\n  Yi Huang", "title": "XTQA: Span-Level Explanations of the Textbook Question Answering", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textbook Question Answering (TQA) is a task that one should answer a\ndiagram/non-diagram question given a large multi-modal context consisting of\nabundant essays and diagrams. We argue that the explainability of this task\nshould place students as a key aspect to be considered. To address this issue,\nwe devise a novel architecture towards span-level eXplanations of the TQA\n(XTQA) based on our proposed coarse-to-fine grained algorithm, which can\nprovide not only the answers but also the span-level evidences to choose them\nfor students. This algorithm first coarsely chooses top $M$ paragraphs relevant\nto questions using the TF-IDF method, and then chooses top $K$ evidence spans\nfinely from all candidate spans within these paragraphs by computing the\ninformation gain of each span to questions. Experimental results shows that\nXTQA significantly improves the state-of-the-art performance compared with\nbaselines. The source code is available at\nhttps://github.com/keep-smile-001/opentqa\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:44:12 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 06:23:45 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 01:13:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ma", "Jie", ""], ["Liu", "Jun", ""], ["Li", "Junjun", ""], ["Zheng", "Qinghua", ""], ["Yin", "Qingyu", ""], ["Zhou", "Jianlong", ""], ["Huang", "Yi", ""]]}, {"id": "2011.12691", "submitter": "Yong Xiao", "authors": "Yong Xiao and Yingyu Li and Guangming Shi and H. Vincent Poor", "title": "Optimizing Resource-Efficiency for Federated Edge Intelligence in IoT\n  Networks", "comments": "Accepted at International Conference on Wireless Communications and\n  Signal Processing (WCSP), Nanjing, China, October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an edge intelligence-based IoT network in which a set of\nedge servers learn a shared model using federated learning (FL) based on the\ndatasets uploaded from a multi-technology-supported IoT network. The data\nuploading performance of IoT network and the computational capacity of edge\nservers are entangled with each other in influencing the FL model training\nprocess. We propose a novel framework, called federated edge intelligence\n(FEI), that allows edge servers to evaluate the required number of data samples\naccording to the energy cost of the IoT network as well as their local data\nprocessing capacity and only request the amount of data that is sufficient for\ntraining a satisfactory model. We evaluate the energy cost for data uploading\nwhen two widely-used IoT solutions: licensed band IoT (e.g., 5G NB-IoT) and\nunlicensed band IoT (e.g., Wi-Fi, ZigBee, and 5G NR-U) are available to each\nIoT device. We prove that the cost minimization problem of the entire IoT\nnetwork is separable and can be divided into a set of subproblems, each of\nwhich can be solved by an individual edge server. We also introduce a mapping\nfunction to quantify the computational load of edge servers under different\ncombinations of three key parameters: size of the dataset, local batch size,\nand number of local training passes. Finally, we adopt an Alternative Direction\nMethod of Multipliers (ADMM)-based approach to jointly optimize energy cost of\nthe IoT network and average computing resource utilization of edge servers. We\nprove that our proposed algorithm does not cause any data leakage nor disclose\nany topological information of the IoT network. Simulation results show that\nour proposed framework significantly improves the resource efficiency of the\nIoT network and edge servers with only a limited sacrifice on the model\nconvergence performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:51:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xiao", "Yong", ""], ["Li", "Yingyu", ""], ["Shi", "Guangming", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2011.12692", "submitter": "Deheng Ye", "authors": "Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia\n  Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang,\n  Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu", "title": "Towards Playing Full MOBA Games with Deep Reinforcement Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand\nchallenges to AI systems such as multi-agent, enormous state-action space,\ncomplex action control, etc. Developing AI for playing MOBA games has raised\nmuch attention accordingly. However, existing work falls short in handling the\nraw game complexity caused by the explosion of agent combinations, i.e.,\nlineups, when expanding the hero pool in case that OpenAI's Dota AI limits the\nplay to a pool of only 17 heroes. As a result, full MOBA games without\nrestrictions are far from being mastered by any existing AI system. In this\npaper, we propose a MOBA AI learning paradigm that methodologically enables\nplaying full MOBA games with deep reinforcement learning. Specifically, we\ndevelop a combination of novel and existing learning techniques, including\ncurriculum self-play learning, policy distillation, off-policy adaption,\nmulti-head value estimation, and Monte-Carlo tree-search, in training and\nplaying a large pool of heroes, meanwhile addressing the scalability issue\nskillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build\nsuperhuman AI agents that can defeat top esports players. The superiority of\nour AI is demonstrated by the first large-scale performance test of MOBA AI\nagent in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:52:33 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 03:30:08 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 11:58:54 GMT"}, {"version": "v4", "created": "Thu, 31 Dec 2020 13:25:17 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ye", "Deheng", ""], ["Chen", "Guibin", ""], ["Zhang", "Wen", ""], ["Chen", "Sheng", ""], ["Yuan", "Bo", ""], ["Liu", "Bo", ""], ["Chen", "Jia", ""], ["Liu", "Zhao", ""], ["Qiu", "Fuhao", ""], ["Yu", "Hongsheng", ""], ["Yin", "Yinyuting", ""], ["Shi", "Bei", ""], ["Wang", "Liang", ""], ["Shi", "Tengfei", ""], ["Fu", "Qiang", ""], ["Yang", "Wei", ""], ["Huang", "Lanxiao", ""], ["Liu", "Wei", ""]]}, {"id": "2011.12715", "submitter": "Jayant A. Gupchup", "authors": "Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley,\n  Scott Inglis, Marcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz,\n  Vishak Gopal, Vinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau,\n  Huazhou Liu, Marco Rossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper,\n  Nilesh Javar, Dmitriy Perednya, Sriram Srinivasan, John Langford, Ross\n  Cutler, Johannes Gehrke", "title": "Resonance: Replacing Software Constants with Context-Aware Models in\n  Real-time Communication", "comments": "Workshop on ML for Systems at NeurIPS 2020, Accepted", "journal-ref": "ML for Systems, NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large software systems tune hundreds of 'constants' to optimize their runtime\nperformance. These values are commonly derived through intuition, lab tests, or\nA/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best\nvalue depends on runtime context. In this paper, we provide an experimental\napproach to replace constants with learned contextual functions for Skype - a\nwidely used real-time communication (RTC) application. We present Resonance, a\nsystem based on contextual bandits (CB). We describe experiences from three\nreal-world experiments: applying it to the audio, video, and transport\ncomponents in Skype. We surface a unique and practical challenge of performing\nmachine learning (ML) inference in large software systems written using\nencapsulation principles. Finally, we open-source FeatureBroker, a library to\nreduce the friction in adopting ML models in such development environments\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:34:56 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Gupchup", "Jayant", ""], ["Aazami", "Ashkan", ""], ["Fan", "Yaran", ""], ["Filipi", "Senja", ""], ["Finley", "Tom", ""], ["Inglis", "Scott", ""], ["Asteborg", "Marcus", ""], ["Caroll", "Luke", ""], ["Chari", "Rajan", ""], ["Cozowicz", "Markus", ""], ["Gopal", "Vishak", ""], ["Prakash", "Vinod", ""], ["Bendapudi", "Sasikanth", ""], ["Gerrits", "Jack", ""], ["Lau", "Eric", ""], ["Liu", "Huazhou", ""], ["Rossi", "Marco", ""], ["Slobodianyk", "Dima", ""], ["Birjukov", "Dmitri", ""], ["Cooper", "Matty", ""], ["Javar", "Nilesh", ""], ["Perednya", "Dmitriy", ""], ["Srinivasan", "Sriram", ""], ["Langford", "John", ""], ["Cutler", "Ross", ""], ["Gehrke", "Johannes", ""]]}, {"id": "2011.12728", "submitter": "Alexander Klimenko Y", "authors": "Alexander Y Klimenko and Dimitri A Klimenko", "title": "On limitations of learning algorithms in competitive environments", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss conceptual limitations of generic learning algorithms pursuing\nadversarial goals in competitive environments, and prove that they are subject\nto limitations that are analogous to the constraints on knowledge imposed by\nthe famous theorems of G\\\"odel and Turing. These limitations are shown to be\nrelated to intransitivity, which is commonly present in competitive\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:40:08 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:07:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Klimenko", "Alexander Y", ""], ["Klimenko", "Dimitri A", ""]]}, {"id": "2011.12750", "submitter": "Thilo Hagendorff", "authors": "Thilo Hagendorff", "title": "AI virtues -- The missing link in putting AI ethics into practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several seminal ethics initiatives have stipulated sets of principles and\nstandards for good technology development in the AI sector. However, widespread\ncriticism has pointed out a lack of practical realization of these principles.\nFollowing that, AI ethics underwent a practical turn, but without deviating\nfrom the principled approach and the many shortcomings associated with it. This\npaper proposes a different approach. It defines four basic AI virtues, namely\njustice, honesty, responsibility and care, all of which represent specific\nmotivational settings that constitute the very precondition for ethical\ndecision making in the AI field. Moreover, it defines two second-order AI\nvirtues, prudence and fortitude, that bolster achieving the basic virtues by\nhelping with overcoming bounded ethicality or the many hidden psychological\nforces that impair ethical decision making and that are hitherto disregarded in\nAI ethics. Lastly, the paper describes measures for successfully cultivating\nthe mentioned virtues in organizations dealing with AI research and\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:14:47 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 10:23:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Hagendorff", "Thilo", ""]]}, {"id": "2011.12757", "submitter": "Woongsup Lee", "authors": "Woongsup Lee and Robert Schober", "title": "Deep Learning-based Resource Allocation For Device-to-Device\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a deep learning (DL) framework for the optimization of the\nresource allocation in multi-channel cellular systems with device-to-device\n(D2D) communication is proposed. Thereby, the channel assignment and discrete\ntransmit power levels of the D2D users, which are both integer variables, are\noptimized to maximize the overall spectral efficiency whilst maintaining the\nquality-of-service (QoS) of the cellular users. Depending on the availability\nof channel state information (CSI), two different configurations are\nconsidered, namely 1) centralized operation with full CSI and 2) distributed\noperation with partial CSI, where in the latter case, the CSI is encoded\naccording to the capacity of the feedback channel. Instead of solving the\nresulting resource allocation problem for each channel realization, a DL\nframework is proposed, where the optimal resource allocation strategy for\narbitrary channel conditions is approximated by deep neural network (DNN)\nmodels. Furthermore, we propose a new training strategy that combines\nsupervised and unsupervised learning methods and a local CSI sharing strategy\nto achieve near-optimal performance while enforcing the QoS constraints of the\ncellular users and efficiently handling the integer optimization variables\nbased on a few ground-truth labels. Our simulation results confirm that\nnear-optimal performance can be attained with low computation time, which\nunderlines the real-time capability of the proposed scheme. Moreover, our\nresults show that not only the resource allocation strategy but also the CSI\nencoding strategy can be efficiently determined using a DNN. Furthermore, we\nshow that the proposed DL framework can be easily extended to communications\nsystems with different design objectives.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:19:23 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lee", "Woongsup", ""], ["Schober", "Robert", ""]]}, {"id": "2011.12786", "submitter": "Vinayak Elangovan", "authors": "Kavan Adeshara and Vinayak Elangovan", "title": "Face recognition using PCA integrated with Delaunay triangulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face Recognition is most used for biometric user authentication that\nidentifies a user based on his or her facial features. The system is in high\ndemand, as it is used by many businesses and employed in many devices such as\nsmartphones and surveillance cameras. However, one frequent problem that is\nstill observed in this user-verification method is its accuracy rate. Numerous\napproaches and algorithms have been experimented to improve the stated flaw of\nthe system. This research develops one such algorithm that utilizes a\ncombination of two different approaches. Using the concepts from Linear Algebra\nand computational geometry, the research examines the integration of Principal\nComponent Analysis with Delaunay Triangulation; the method triangulates a set\nof face landmark points and obtains eigenfaces of the provided images. It\ncompares the algorithm with traditional PCA and discusses the inclusion of\ndifferent face landmark points to deliver an effective recognition rate.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:46:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Adeshara", "Kavan", ""], ["Elangovan", "Vinayak", ""]]}, {"id": "2011.12807", "submitter": "Thibault Maho", "authors": "Thibault Maho, Teddy Furon, Erwan Le Merrer", "title": "SurFree: a fast surrogate-free black-box attack", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers are critically prone to evasion attacks.\nAdversarial examples are slightly modified inputs that are then misclassified,\nwhile remaining perceptively close to their originals. Last couple of years\nhave witnessed a striking decrease in the amount of queries a black box attack\nsubmits to the target classifier, in order to forge adversarials. This\nparticularly concerns the black-box score-based setup, where the attacker has\naccess to top predicted probabilites: the amount of queries went from to\nmillions of to less than a thousand. This paper presents SurFree, a geometrical\napproach that achieves a similar drastic reduction in the amount of queries in\nthe hardest setup: black box decision-based attacks (only the top-1 label is\navailable). We first highlight that the most recent attacks in that setup,\nHSJA, QEBA and GeoDA all perform costly gradient surrogate estimations. SurFree\nproposes to bypass these, by instead focusing on careful trials along diverse\ndirections, guided by precise indications of geometrical properties of the\nclassifier decision boundaries. We motivate this geometric approach before\nperforming a head-to-head comparison with previous attacks with the amount of\nqueries as a first class citizen. We exhibit a faster distortion decay under\nlow query amounts (few hundreds to a thousand), while remaining competitive at\nhigher query budgets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:08:19 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Maho", "Thibault", ""], ["Furon", "Teddy", ""], ["Merrer", "Erwan Le", ""]]}, {"id": "2011.12854", "submitter": "Wolfgang Stammer", "authors": "Wolfgang Stammer, Patrick Schramowski and Kristian Kersting", "title": "Right for the Right Concept: Revising Neuro-Symbolic Concepts by\n  Interacting with their Explanations", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021, p. 3619-3629", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most explanation methods in deep learning map importance estimates for a\nmodel's prediction back to the original input space. These \"visual\"\nexplanations are often insufficient, as the model's actual concept remains\nelusive. Moreover, without insights into the model's semantic concept, it is\ndifficult -- if not impossible -- to intervene on the model's behavior via its\nexplanations, called Explanatory Interactive Learning. Consequently, we propose\nto intervene on a Neuro-Symbolic scene representation, which allows one to\nrevise the model on the semantic level, e.g. \"never focus on the color to make\nyour decision\". We compiled a novel confounded visual scene data set, the\nCLEVR-Hans data set, capturing complex compositions of different objects. The\nresults of our experiments on CLEVR-Hans demonstrate that our semantic\nexplanations, i.e. compositional explanations at a per-object level, can\nidentify confounders that are not identifiable using \"visual\" explanations\nonly. More importantly, feedback on this semantic level makes it possible to\nrevise the model from focusing on these factors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:23:26 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 10:46:44 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 16:09:33 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 19:15:00 GMT"}, {"version": "v5", "created": "Tue, 16 Mar 2021 13:40:04 GMT"}, {"version": "v6", "created": "Mon, 21 Jun 2021 08:25:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Stammer", "Wolfgang", ""], ["Schramowski", "Patrick", ""], ["Kersting", "Kristian", ""]]}, {"id": "2011.12859", "submitter": "Elena Sizikova", "authors": "Omkar Kumbhar, Elena Sizikova, Najib Majaj, Denis G. Pelli", "title": "Anytime Prediction as a Model of Human Reaction Time", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks today often recognize objects as well as people do, and thus\nmight serve as models of the human recognition process. However, most such\nnetworks provide their answer after a fixed computational effort, whereas human\nreaction time varies, e.g. from 0.2 to 10 s, depending on the properties of\nstimulus and task. To model the effect of difficulty on human reaction time, we\nconsidered a classification network that uses early-exit classifiers to make\nanytime predictions. Comparing human and MSDNet accuracy in classifying\nCIFAR-10 images in added Gaussian noise, we find that the network equivalent\ninput noise SD is 15 times higher than human, and that human efficiency is only\n0.6\\% that of the network. When appropriate amounts of noise are present to\nbring the two observers (human and network) into the same accuracy range, they\nshow very similar dependence on duration or FLOPS, i.e. very similar\nspeed-accuracy tradeoff. We conclude that Anytime classification (i.e. early\nexits) is a promising model for human reaction time in recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:30:52 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kumbhar", "Omkar", ""], ["Sizikova", "Elena", ""], ["Majaj", "Najib", ""], ["Pelli", "Denis G.", ""]]}, {"id": "2011.12860", "submitter": "Manfred Eppe", "authors": "Phuong D.H. Nguyen, Yasmin Kim Georgie, Ezgi Kayhan, Manfred Eppe,\n  Verena Vanessa Hafner, and Stefan Wermter", "title": "Sensorimotor representation learning for an \"active self\" in robots: A\n  model survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe human-robot interactions require robots to be able to learn how to\nbehave appropriately in \\sout{humans' world} \\rev{spaces populated by people}\nand thus to cope with the challenges posed by our dynamic and unstructured\nenvironment, rather than being provided a rigid set of rules for operations. In\nhumans, these capabilities are thought to be related to our ability to perceive\nour body in space, sensing the location of our limbs during movement, being\naware of other objects and agents, and controlling our body parts to interact\nwith them intentionally. Toward the next generation of robots with bio-inspired\ncapacities, in this paper, we first review the developmental processes of\nunderlying mechanisms of these abilities: The sensory representations of body\nschema, peripersonal space, and the active self in humans. Second, we provide a\nsurvey of robotics models of these sensory representations and robotics models\nof the self; and we compare these models with the human counterparts. Finally,\nwe analyse what is missing from these robotics models and propose a theoretical\ncomputational framework, which aims to allow the emergence of the sense of self\nin artificial agents by developing sensory representations through\nself-exploration.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:31:01 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 14:03:40 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Nguyen", "Phuong D. H.", ""], ["Georgie", "Yasmin Kim", ""], ["Kayhan", "Ezgi", ""], ["Eppe", "Manfred", ""], ["Hafner", "Verena Vanessa", ""], ["Wermter", "Stefan", ""]]}, {"id": "2011.12862", "submitter": "Sophia Saller", "authors": "Jana Koehler, Joseph B\\\"urgler, Urs Fontana, Etienne Fux, Florian\n  Herzog, Marc Pouly, Sophia Saller, Anastasia Salyaeva, Peter Scheiblechner,\n  Kai Waelti", "title": "Cable Tree Wiring -- Benchmarking Solvers on a Real-World Scheduling\n  Problem with a Variety of Precedence Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cable trees are used in industrial products to transmit energy and\ninformation between different product parts. To this date, they are mostly\nassembled by humans and only few automated manufacturing solutions exist using\ncomplex robotic machines. For these machines, the wiring plan has to be\ntranslated into a wiring sequence of cable plugging operations to be followed\nby the machine. In this paper, we study and formalize the problem of deriving\nthe optimal wiring sequence for a given layout of a cable tree. We summarize\nour investigations to model this cable tree wiring Problem (CTW) as a traveling\nsalesman problem with atomic, soft atomic, and disjunctive precedence\nconstraints as well as tour-dependent edge costs such that it can be solved by\nstate-of-the-art constraint programming (CP), Optimization Modulo Theories\n(OMT), and mixed-integer programming (MIP) solvers. It is further shown, how\nthe CTW problem can be viewed as a soft version of the coupled tasks scheduling\nproblem. We discuss various modeling variants for the problem, prove its\nNP-hardness, and empirically compare CP, OMT, and MIP solvers on a benchmark\nset of 278 instances. The complete benchmark set with all models and instance\ndata is available on github and is accepted for inclusion in the MiniZinc\nchallenge 2020.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:34:04 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Koehler", "Jana", ""], ["B\u00fcrgler", "Joseph", ""], ["Fontana", "Urs", ""], ["Fux", "Etienne", ""], ["Herzog", "Florian", ""], ["Pouly", "Marc", ""], ["Saller", "Sophia", ""], ["Salyaeva", "Anastasia", ""], ["Scheiblechner", "Peter", ""], ["Waelti", "Kai", ""]]}, {"id": "2011.12863", "submitter": "Francesca Foffano", "authors": "Francesca Foffano, Teresa Scantamburlo, Atia Cort\\'es, and Chiara\n  Bissolo", "title": "European Strategy on AI: Are we truly fostering social good?", "comments": "6 pages, 1 figures, submitted at IJCAI 2020 Workshop on AI for Social\n  Good", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) is already part of our daily lives and is\nplaying a key role in defining the economic and social shape of the future. In\n2018, the European Commission introduced its AI strategy able to compete in the\nnext years with world powers such as China and US, but relying on the respect\nof European values and fundamental rights. As a result, most of the Member\nStates have published their own National Strategy with the aim to work on a\ncoordinated plan for Europe. In this paper, we present an ongoing study on how\nEuropean countries are approaching the field of Artificial Intelligence, with\nits promises and risks, through the lens of their national AI strategies. In\nparticular, we aim to investigate how European countries are investing in AI\nand to what extent the stated plans can contribute to the benefit of the whole\nsociety. This paper reports the main findings of a qualitative analysis of the\ninvestment plans reported in 15 European National Strategies\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:39:12 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Foffano", "Francesca", ""], ["Scantamburlo", "Teresa", ""], ["Cort\u00e9s", "Atia", ""], ["Bissolo", "Chiara", ""]]}, {"id": "2011.12870", "submitter": "Yi Zhou", "authors": "Yi Zhou, Zhenhao Chen", "title": "Multimodal Learning for Hateful Memes Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memes are used for spreading ideas through social networks. Although most\nmemes are created for humor, some memes become hateful under the combination of\npictures and text. Automatically detecting the hateful memes can help reduce\ntheir harmful social impact. Unlike the conventional multimodal tasks, where\nthe visual and textual information is semantically aligned, the challenge of\nhateful memes detection lies in its unique multimodal information. The image\nand text in memes are weakly aligned or even irrelevant, which requires the\nmodel to understand the content and perform reasoning over multiple modalities.\nIn this paper, we focus on multimodal hateful memes detection and propose a\nnovel method that incorporates the image captioning process into the memes\ndetection process. We conduct extensive experiments on multimodal meme datasets\nand illustrated the effectiveness of our approach. Our model achieves promising\nresults on the Hateful Memes Detection Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:49:15 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 03:57:32 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 22:16:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhou", "Yi", ""], ["Chen", "Zhenhao", ""]]}, {"id": "2011.12888", "submitter": "Ignacio Sarasua", "authors": "Ignacio Sarasua, Sebastian Poelsterl, Christian Wachinger", "title": "Recalibration of Neural Networks for Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and channel re-calibration have become powerful concepts in computer\nvision. Their ability to capture long-range dependencies is especially useful\nfor those networks that extract local features, such as CNNs. While\nre-calibration has been widely studied for image analysis, it has not yet been\nused on shape representations. In this work, we introduce re-calibration\nmodules on deep neural networks for 3D point clouds. We propose a set of\nre-calibration blocks that extend Squeeze and Excitation blocks and that can be\nadded to any network for 3D point cloud analysis that builds a global\ndescriptor by hierarchically combining features from multiple local\nneighborhoods. We run two sets of experiments to validate our approach. First,\nwe demonstrate the benefit and versatility of our proposed modules by\nincorporating them into three state-of-the-art networks for 3D point cloud\nanalysis: PointNet++, DGCNN, and RSCNN. We evaluate each network on two tasks:\nobject classification on ModelNet40, and object part segmentation on ShapeNet.\nOur results show an improvement of up to 1% in accuracy for ModelNet40 compared\nto the baseline method. In the second set of experiments, we investigate the\nbenefits of re-calibration blocks on Alzheimer's Disease (AD) diagnosis. Our\nresults demonstrate that our proposed methods yield a 2% increase in accuracy\nfor diagnosing AD and a 2.3% increase in concordance index for predicting AD\nonset with time-to-event analysis. Concluding, re-calibration improves the\naccuracy of point cloud architectures, while only minimally increasing the\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:14:34 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sarasua", "Ignacio", ""], ["Poelsterl", "Sebastian", ""], ["Wachinger", "Christian", ""]]}, {"id": "2011.12895", "submitter": "Peng Sun", "authors": "Peng Sun, Jiechao Xiong, Lei Han, Xinghai Sun, Shuxing Li, Jiawei Xu,\n  Meng Fang, Zhengyou Zhang", "title": "TLeague: A Framework for Competitive Self-Play based Distributed\n  Multi-Agent Reinforcement Learning", "comments": "21 pages, 3 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL)\nhas shown phenomenal breakthroughs recently. Strong AIs are achieved for\nseveral benchmarks, including Dota 2, Glory of Kings, Quake III, StarCraft II,\nto name a few. Despite the success, the MARL training is extremely data\nthirsty, requiring typically billions of (if not trillions of) frames be seen\nfrom the environment during training in order for learning a high performance\nagent. This poses non-trivial difficulties for researchers or engineers and\nprevents the application of MARL to a broader range of real-world problems. To\naddress this issue, in this manuscript we describe a framework, referred to as\nTLeague, that aims at large-scale training and implements several main-stream\nCSP-MARL algorithms. The training can be deployed in either a single machine or\na cluster of hybrid machines (CPUs and GPUs), where the standard Kubernetes is\nsupported in a cloud native manner. TLeague achieves a high throughput and a\nreasonable scale-up when performing distributed training. Thanks to the modular\ndesign, it is also easy to extend for solving other multi-agent problems or\nimplementing and verifying MARL algorithms. We present experiments over\nStarCraft II, ViZDoom and Pommerman to show the efficiency and effectiveness of\nTLeague. The code is open-sourced and available at\nhttps://github.com/tencent-ailab/tleague_projpage\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:24:20 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 03:23:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sun", "Peng", ""], ["Xiong", "Jiechao", ""], ["Han", "Lei", ""], ["Sun", "Xinghai", ""], ["Li", "Shuxing", ""], ["Xu", "Jiawei", ""], ["Fang", "Meng", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "2011.12902", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian\n  Canton Ferrer", "title": "Adversarial Evaluation of Multimodal Models under Realistic Gray Box\n  Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the vulnerability of multimodal (image + text) models to\nadversarial threats similar to those discussed in previous literature on\nunimodal (image- or text-only) models. We introduce realistic assumptions of\npartial model knowledge and access, and discuss how these assumptions differ\nfrom the standard \"black-box\"/\"white-box\" dichotomy common in current\nliterature on adversarial attacks. Working under various levels of these\n\"gray-box\" assumptions, we develop new attack methodologies unique to\nmultimodal classification and evaluate them on the Hateful Memes Challenge\nclassification task. We find that attacking multiple modalities yields stronger\nattacks than unimodal attacks alone (inducing errors in up to 73% of cases),\nand that the unimodal image attacks on multimodal classifiers we explored were\nstronger than character-based text augmentation attacks (inducing errors on\naverage in 45% and 30% of cases, respectively).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:37:40 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 09:03:45 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 16:23:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Evtimov", "Ivan", ""], ["Howes", "Russel", ""], ["Dolhansky", "Brian", ""], ["Firooz", "Hamed", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2011.12906", "submitter": "Mohsen Jafarzadeh", "authors": "Mohsen Jafarzadeh, Akshay Raj Dhamija, Steve Cruz, Chunchun Li,\n  Touqeer Ahmad, Terrance E. Boult", "title": "Open-World Learning Without Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-world learning is a problem where an autonomous agent detects things\nthat it does not know and learns them over time from a non-stationary and\nnever-ending stream of data; in an open-world environment, the training data\nand objective criteria are never available at once. The agent should grasp new\nknowledge from learning without forgetting acquired prior knowledge.\nResearchers proposed a few open-world learning agents for image classification\ntasks that operate in complex scenarios. However, all prior work on open-world\nlearning has all labeled data to learn the new classes from the stream of\nimages. In scenarios where autonomous agents should respond in near real-time\nor work in areas with limited communication infrastructure, human labeling of\ndata is not possible. Therefore, supervised open-world learning agents are not\nscalable solutions for such applications. Herein, we propose a new framework\nthat enables agents to learn new classes from a stream of unlabeled data in an\nunsupervised manner. Also, we study the robustness and learning speed of such\nagents with supervised and unsupervised feature representation. We also\nintroduce a new metric for open-world learning without labels. We anticipate\nour theories and method to be a starting point for developing autonomous true\nopen-world never-ending learning agents.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:41:03 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 01:39:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jafarzadeh", "Mohsen", ""], ["Dhamija", "Akshay Raj", ""], ["Cruz", "Steve", ""], ["Li", "Chunchun", ""], ["Ahmad", "Touqeer", ""], ["Boult", "Terrance E.", ""]]}, {"id": "2011.12912", "submitter": "Rahul Sajnani", "authors": "Rahul Sajnani, AadilMehdi Sanchawala, Krishna Murthy Jatavallabhula,\n  Srinath Sridhar, K. Madhava Krishna", "title": "DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of\n  Objects", "comments": "Preprint. For project page and code, see\n  https://aadilmehdis.github.io/DRACO-Project-Page/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DRACO, a method for Dense Reconstruction And Canonicalization of\nObject shape from one or more RGB images. Canonical shape reconstruction,\nestimating 3D object shape in a coordinate space canonicalized for scale,\nrotation, and translation parameters, is an emerging paradigm that holds\npromise for a multitude of robotic applications. Prior approaches either rely\non painstakingly gathered dense 3D supervision, or produce only sparse\ncanonical representations, limiting real-world applicability. DRACO performs\ndense canonicalization using only weak supervision in the form of camera poses\nand semantic keypoints at train time. During inference, DRACO predicts dense\nobject-centric depth maps in a canonical coordinate-space, solely using one or\nmore RGB images of an object. Extensive experiments on canonical shape\nreconstruction and pose estimation show that DRACO is competitive or superior\nto fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:50:56 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sajnani", "Rahul", ""], ["Sanchawala", "AadilMehdi", ""], ["Jatavallabhula", "Krishna Murthy", ""], ["Sridhar", "Srinath", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2011.12919", "submitter": "Keshav Ganapathy", "authors": "David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric\n  Slud, Micah Goldblum, Tom Goldstein", "title": "Analyzing the Machine Learning Conference Review Process", "comments": "NeurIPS Workshop on Navigating the Broader Impacts of AI Research.\n  Full version at arXiv:2010.05137", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mainstream machine learning conferences have seen a dramatic increase in the\nnumber of participants, along with a growing range of perspectives, in recent\nyears. Members of the machine learning community are likely to overhear\nallegations ranging from randomness of acceptance decisions to institutional\nbias. In this work, we critically analyze the review process through a\ncomprehensive study of papers submitted to ICLR between 2017 and 2020. We\nquantify reproducibility/randomness in review scores and acceptance decisions,\nand examine whether scores correlate with paper impact. Our findings suggest\nstrong institutional bias in accept/reject decisions, even after controlling\nfor paper quality. Furthermore, we find evidence for a gender gap, with female\nauthors receiving lower scores, lower acceptance rates, and fewer citations per\npaper than their male counterparts. We conclude our work with recommendations\nfor future conference organizers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:40:27 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 01:34:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Tran", "David", ""], ["Valtchanov", "Alex", ""], ["Ganapathy", "Keshav", ""], ["Feng", "Raymond", ""], ["Slud", "Eric", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""]]}, {"id": "2011.12930", "submitter": "Anand Gopalakrishnan", "authors": "Anand Gopalakrishnan, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Unsupervised Object Keypoint Learning using Local Spatial Predictability", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose PermaKey, a novel approach to representation learning based on\nobject keypoints. It leverages the predictability of local image regions from\nspatial neighborhoods to identify salient regions that correspond to object\nparts, which are then converted to keypoints. Unlike prior approaches, it\nutilizes predictability to discover object keypoints, an intrinsic property of\nobjects. This ensures that it does not overly bias keypoints to focus on\ncharacteristics that are not unique to objects, such as movement, shape, colour\netc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints\ncorresponding to the most salient object parts and is robust to certain visual\ndistractors. Further, on downstream RL tasks in the Atari domain we demonstrate\nhow agents equipped with our keypoints outperform those using competing\nalternatives, even on challenging environments with moving backgrounds or\ndistractor objects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:27:05 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:10:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gopalakrishnan", "Anand", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2011.12956", "submitter": "Bernardo Cortez", "authors": "Bernardo Cortez", "title": "Reinforcement Learning for Robust Missile Autopilot Design", "comments": "Masters Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing missiles' autopilot controllers has been a complex task, given the\nextensive flight envelope and the nonlinear flight dynamics. A solution that\ncan excel both in nominal performance and in robustness to uncertainties is\nstill to be found. While Control Theory often debouches into parameters'\nscheduling procedures, Reinforcement Learning has presented interesting results\nin ever more complex tasks, going from videogames to robotic tasks with\ncontinuous action domains. However, it still lacks clearer insights on how to\nfind adequate reward functions and exploration strategies. To the best of our\nknowledge, this work is pioneer in proposing Reinforcement Learning as a\nframework for flight control. In fact, it aims at training a model-free agent\nthat can control the longitudinal flight of a missile, achieving optimal\nperformance and robustness to uncertainties. To that end, under TRPO's\nmethodology, the collected experience is augmented according to HER, stored in\na replay buffer and sampled according to its significance. Not only does this\nwork enhance the concept of prioritized experience replay into BPER, but it\nalso reformulates HER, activating them both only when the training progress\nconverges to suboptimal policies, in what is proposed as the SER methodology.\nBesides, the Reward Engineering process is carefully detailed. The results show\nthat it is possible both to achieve the optimal performance and to improve the\nagent's robustness to uncertainties (with low damage on nominal performance) by\nfurther training it in non-nominal environments, therefore validating the\nproposed approach and encouraging future research in this field.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:30:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Cortez", "Bernardo", ""]]}, {"id": "2011.12979", "submitter": "Hemant Yadav", "authors": "Hemant Yadav, Janvijay Singh, Atul Anshuman Singh, Rachit Mittal,\n  Rajiv Ratn Shah", "title": "De-STT: De-entaglement of unwanted Nuisances and Biases in Speech to\n  Text System using Adversarial Forgetting", "comments": "Need to add substantial findings during the new experiments, which\n  would mean to update the claim made in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Training a robust Speech to Text (STT) system requires tens of thousands of\nhours of data. Variabilities present in the dataset such as unwanted nuisances\n(environmental noise, etc) and biases (accent, gender, age, etc) are reasons\nfor the need of large datasets to learn general representations, which is often\nnot feasible for low resource languages. In many computer vision tasks, a\nrecently proposed adversarial forgetting approach to remove unwanted features\nhas produced good results. This motivates us to study the effect of\nde-entangling the accent information from the input speech signal while\ntraining STT systems. To this end, we use an information bottleneck\narchitecture based on adversarial forgetting. This training scheme aims to\nenforce the model to learn general accent invariant speech representations. Two\nSTT models trained on just 20 hrs of audio, with and without adversarial\nforgetting, are tested on two unseen accents not present in the training set.\nThe results favour the adversarial forgetting scheme with an absolute average\nimprovement of 6\\% over the standard training scheme. Furthermore, we also\nobserve an absolute improvement of 5.5\\% when tested on the seen accents\npresent in the training set.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:02:13 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 22:20:21 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 17:27:17 GMT"}, {"version": "v4", "created": "Sat, 5 Jun 2021 03:49:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yadav", "Hemant", ""], ["Singh", "Janvijay", ""], ["Singh", "Atul Anshuman", ""], ["Mittal", "Rachit", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2011.13006", "submitter": "Mervyn OLuing Mr", "authors": "Mervyn O'Luing, Steven Prestwich, S. Armagan Tarim", "title": "A Simulated Annealing Algorithm for Joint Stratification and Sample\n  Allocation Designs", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study combined simulated annealing with delta evaluation to solve the\njoint stratification and sample allocation problem. In this problem, atomic\nstrata are partitioned into mutually exclusive and collectively exhaustive\nstrata. Each stratification is a solution, the quality of which is measured by\nits cost. The Bell number of possible solutions is enormous for even a moderate\nnumber of atomic strata and an additional layer of complexity is added with the\nevaluation time of each solution. Many larger scale combinatorial optimisation\nproblems cannot be solved to optimality because the search for an optimum\nsolution requires a prohibitive amount of computation time; a number of local\nsearch heuristic algorithms have been designed for this problem but these can\nbecome trapped in local minima preventing any further improvements. We add to\nthe existing suite of local search algorithms with a simulated annealing\nalgorithm that allows for an escape from local minima and uses delta evaluation\nto exploit the similarity between consecutive solutions and thereby reduce the\nevaluation time.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:27:49 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["O'Luing", "Mervyn", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "2011.13062", "submitter": "Nao Tokui PhD", "authors": "Nao Tokui", "title": "Can GAN originate new electronic dance music genres? -- Generating novel\n  rhythm patterns using GAN with Genre Ambiguity Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the introduction of deep learning, researchers have proposed content\ngeneration systems using deep learning and proved that they are competent to\ngenerate convincing content and artistic output, including music. However, one\ncan argue that these deep learning-based systems imitate and reproduce the\npatterns inherent within what humans have created, instead of generating\nsomething new and creative. This paper focuses on music generation, especially\nrhythm patterns of electronic dance music, and discusses if we can use deep\nlearning to generate novel rhythms, interesting patterns not found in the\ntraining dataset. We extend the framework of Generative Adversarial\nNetworks(GAN) and encourage it to diverge from the dataset's inherent\ndistributions by adding additional classifiers to the framework. The paper\nshows that our proposed GAN can generate rhythm patterns that sound like music\nrhythms but do not belong to any genres in the training dataset. The source\ncode, generated rhythm patterns, and a supplementary plugin software for a\npopular Digital Audio Workstation software are available on our website.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:22:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Tokui", "Nao", ""]]}, {"id": "2011.13066", "submitter": "Li Liu", "authors": "Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong,\n  Yongfang Luo, Xiang Wan", "title": "Effective Sample Pair Generation for Ultrasound Video Contrastive\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most deep neural networks (DNNs) based ultrasound (US) medical image analysis\nmodels use pretrained backbones (e.g., ImageNet) for better model\ngeneralization. However, the domain gap between natural and medical images\ncauses an inevitable performance bottleneck when applying to US image analysis.\nOur idea is to pretrain DNNs on US images directly to avoid this bottleneck.\nDue to the lack of annotated large-scale datasets of US images, we first\nconstruct a new large-scale US video-based image dataset named US-4, containing\nover 23,000 high-resolution images from four US video sub-datasets, where two\nsub-datasets are newly collected by our local experienced doctors. To make full\nuse of this dataset, we then innovatively propose an US semi-supervised\ncontrastive learning (USCL) method to effectively learn feature representations\nof US images, with a new sample pair generation (SPG) scheme to tackle the\nproblem that US images extracted from videos have high similarities. Moreover,\nthe USCL treats contrastive loss as a consistent regularization, which boosts\nthe performance of pretrained backbones by combining the supervised loss in a\nmutually reinforcing way. Extensive experiments on down-stream tasks'\nfine-tuning show the superiority of our approach against ImageNet pretraining\nand pretraining using previous state-of-the-art semi-supervised learning\napproaches. In particular, our pretrained backbone gets fine-tuning accuracy of\nover 94%, which is 9% higher than 85% of the ImageNet pretrained model on the\nwidely used POCUS dataset. The constructed US-4 dataset and source codes of\nthis work will be made public.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:44:38 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Yixiong", ""], ["Zhang", "Chunhui", ""], ["Liu", "Li", ""], ["Feng", "Cheng", ""], ["Dong", "Changfeng", ""], ["Luo", "Yongfang", ""], ["Wan", "Xiang", ""]]}, {"id": "2011.13074", "submitter": "Peng Zhou", "authors": "Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, Qi Tian", "title": "Omni-GAN: On the Secrets of cGANs and Beyond", "comments": "Introducing Omni-INR-GAN, which can extrapolate low-resolution images\n  to arbitrary resolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional generative adversarial network (cGAN) is a powerful tool of\ngenerating high-quality images, but existing approaches mostly suffer\nunsatisfying performance or the risk of mode collapse. This paper presents\nOmni-GAN, a variant of cGAN that reveals the devil in designing a proper\ndiscriminator for training the model. The key is to ensure that the\ndiscriminator receives strong supervision to perceive the concepts and moderate\nregularization to avoid collapse. Omni-GAN is easily implemented and freely\nintegrated with off-the-shelf encoding methods (e.g., implicit neural\nrepresentation, INR). Experiments validate the superior performance of Omni-GAN\nand Omni-INR-GAN in a wide range of image generation and restoration tasks. In\nparticular, Omni-INR-GAN sets new records on the ImageNet dataset with\nimpressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and\n256, respectively, surpassing the previous records by 100+ points. Moreover,\nleveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution\nimages to arbitrary resolution, even up to x60+ higher resolution. Code is\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:30:20 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 05:33:05 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 03:00:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Peng", ""], ["Xie", "Lingxi", ""], ["Ni", "Bingbing", ""], ["Geng", "Cong", ""], ["Tian", "Qi", ""]]}, {"id": "2011.13085", "submitter": "Minji Yoon", "authors": "Minji Yoon, Bryan Hooi, Kijung Shin, Christos Faloutsos", "title": "Fast and Accurate Anomaly Detection in Dynamic Graphs with a Two-Pronged\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a dynamic graph stream, how can we detect the sudden appearance of\nanomalous patterns, such as link spam, follower boosting, or denial of service\nattacks? Additionally, can we categorize the types of anomalies that occur in\npractice, and theoretically analyze the anomalous signs arising from each type?\nIn this work, we propose AnomRank, an online algorithm for anomaly detection in\ndynamic graphs. AnomRank uses a two-pronged approach defining two novel metrics\nfor anomalousness. Each metric tracks the derivatives of its own version of a\n'node score' (or node importance) function. This allows us to detect sudden\nchanges in the importance of any node. We show theoretically and experimentally\nthat the two-pronged approach successfully detects two common types of\nanomalies: sudden weight changes along an edge, and sudden structural changes\nto the graph. AnomRank is (a) Fast and Accurate: up to 49.5x faster or 35% more\naccurate than state-of-the-art methods, (b) Scalable: linear in the number of\nedges in the input graph, processing millions of edges within 2 seconds on a\nstock laptop/desktop, and (c) Theoretically Sound: providing theoretical\nguarantees of the two-pronged approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 01:38:27 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yoon", "Minji", ""], ["Hooi", "Bryan", ""], ["Shin", "Kijung", ""], ["Faloutsos", "Christos", ""]]}, {"id": "2011.13089", "submitter": "Hui Wei Dr.", "authors": "Hui Wei", "title": "The Evolution of Concept-Acquisition based on Developmental Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A conceptual system with rich connotation is key to improving the performance\nof knowledge-based artificial intelligence systems. While a conceptual system,\nwhich has abundant concepts and rich semantic relationships, and is\ndevelopable, evolvable, and adaptable to multi-task environments, its actual\nconstruction is not only one of the major challenges of knowledge engineering,\nbut also the fundamental goal of research on knowledge and conceptualization.\nFinding a new method to represent concepts and construct a conceptual system\nwill therefore greatly improve the performance of many intelligent systems.\nFortunately the core of human cognition is a system with relatively complete\nconcepts and a mechanism that ensures the establishment and development of the\nsystem. The human conceptual system can not be achieved immediately, but rather\nmust develop gradually. Developmental psychology carefully observes the process\nof concept acquisition in humans at the behavioral level, and along with\ncognitive psychology has proposed some rough explanations of those\nobservations. However, due to the lack of research in aspects such as\nrepresentation, systematic models, algorithm details and realization, many of\nthe results of developmental psychology have not been applied directly to the\nbuilding of artificial conceptual systems. For example, Karmiloff-Smith's\nRepresentation Redescription (RR) supposition reflects a concept-acquisition\nprocess that re-describes a lower level representation of a concept to a higher\none. This paper is inspired by this developmental psychology viewpoint. We use\nan object-oriented approach to re-explain and materialize RR supposition from\nthe formal semantic perspective, because the OO paradigm is a natural way to\ndescribe the outside world, and it also has strict grammar regulations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 01:57:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wei", "Hui", ""]]}, {"id": "2011.13093", "submitter": "Sanghwa Lee", "authors": "Sanghwa Lee, Jaeyoung Lee, Ichiro Hasuo", "title": "Predictive PER: Balancing Priority and Diversity towards Stable Deep\n  Reinforcement Learning", "comments": "Presented at Deep Reinforcement Learning Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prioritized experience replay (PER) samples important transitions, rather\nthan uniformly, to improve the performance of a deep reinforcement learning\nagent. We claim that such prioritization has to be balanced with sample\ndiversity for making the DQN stabilized and preventing forgetting. Our proposed\nimprovement over PER, called Predictive PER (PPER), takes three countermeasures\n(TDInit, TDClip, TDPred) to (i) eliminate priority outliers and explosions and\n(ii) improve the sample diversity and distributions, weighted by priorities,\nboth leading to stabilizing the DQN. The most notable among the three is the\nintroduction of the second DNN called TDPred to generalize the in-distribution\npriorities. Ablation study and full experiments with Atari games show that each\ncountermeasure by its own way and PPER contribute to successfully enhancing\nstability and thus performance over PER.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 02:12:31 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lee", "Sanghwa", ""], ["Lee", "Jaeyoung", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "2011.13098", "submitter": "Majid Moghadam", "authors": "Majid Moghadam, Ali Alizadeh, Engin Tekin and Gabriel Hugh Elkaim", "title": "An End-to-end Deep Reinforcement Learning Approach for the Long-term\n  Short-term Planning on the Frenet Space", "comments": "submitted to International Conference on Robotics and Automation\n  (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactical decision making and strategic motion planning for autonomous highway\ndriving are challenging due to the complication of predicting other road users'\nbehaviors, diversity of environments, and complexity of the traffic\ninteractions. This paper presents a novel end-to-end continuous deep\nreinforcement learning approach towards autonomous cars' decision-making and\nmotion planning. For the first time, we define both states and action spaces on\nthe Frenet space to make the driving behavior less variant to the road\ncurvatures than the surrounding actors' dynamics and traffic interactions. The\nagent receives time-series data of past trajectories of the surrounding\nvehicles and applies convolutional neural networks along the time channels to\nextract features in the backbone. The algorithm generates continuous\nspatiotemporal trajectories on the Frenet frame for the feedback controller to\ntrack. Extensive high-fidelity highway simulations on CARLA show the\nsuperiority of the presented approach compared with commonly used baselines and\ndiscrete reinforcement learning on various traffic scenarios. Furthermore, the\nproposed method's advantage is confirmed with a more comprehensive performance\nevaluation against 1000 randomly generated test scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 02:40:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Moghadam", "Majid", ""], ["Alizadeh", "Ali", ""], ["Tekin", "Engin", ""], ["Elkaim", "Gabriel Hugh", ""]]}, {"id": "2011.13113", "submitter": "David Romain Djoumbissie", "authors": "Djoumbissie David Romain", "title": "Predicting S&P500 Index direction with Transfer Learning and a Causal\n  Graph as main Input", "comments": "Revised description in section II", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified multi-tasking framework to represent the complex and\nuncertain causal process of financial market dynamics, and then to predict the\nmovement of any type of index with an application on the monthly direction of\nthe S&P500 index. our solution is based on three main pillars: (i) the use of\ntransfer learning to share knowledge and feature (representation, learning)\nbetween all financial markets, increase the size of the training sample and\npreserve the stability between training, validation and test sample. (ii) The\ncombination of multidisciplinary knowledge (Financial economics, behavioral\nfinance, market microstructure and portfolio construction theories) to\nrepresent a global top-down dynamics of any financial market, through a graph.\n(iii) The integration of forward looking unstructured data, different types of\ncontexts (long, medium and short term) through latent variables/nodes and then,\nuse a unique VAE network (parameter sharing) to learn simultaneously their\ndistributional representation. We obtain Accuracy, F1-score, and Matthew\nCorrelation of 74.3 %, 67 % and 0.42 above the industry and other benchmark on\n12 years test period which include three unstable and difficult sub-period to\npredict.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 03:45:51 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 23:44:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Romain", "Djoumbissie David", ""]]}, {"id": "2011.13122", "submitter": "Georgi Marinov", "authors": "Georgi Marinov", "title": "Real-time error correction and performance aid for MIDI instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making a slight mistake during live music performance can easily be spotted\nby an astute listener, even if the performance is an improvisation or an\nunfamiliar piece. An example might be a highly dissonant chord played by\nmistake in a classical-era sonata, or a sudden off-key note in a recurring\nmotif. The problem of identifying and correcting such errors can be approached\nwith artificial intelligence -- if a trained human can easily do it, maybe a\ncomputer can be trained to spot the errors quickly and just as accurately. The\nability to identify and auto-correct errors in real-time would be not only\nextremely useful to performing musicians, but also a valuable asset for\nproducers, allowing much fewer overdubs and re-recording of takes due to small\nimperfections. This paper examines state-of-the-art solutions to related\nproblems and explores novel solutions for music error detection and correction,\nfocusing on their real-time applicability. The explored approaches consider\nerror detection through music context and theory, as well as supervised\nlearning models with no predefined musical information or rules, trained on\nappropriate datasets. Focusing purely on correcting musical errors, the\npresented solutions operate on a high-level representation of the audio (MIDI)\ninstead of the raw audio domain, taking input from an electronic instrument\n(MIDI keyboard/piano) and altering it when needed before it is sent to the\nsampler. This work proposes multiple general recurrent neural network designs\nfor real-time error correction and performance aid for MIDI instruments,\ndiscusses the results, limitations, and possible future improvements. It also\nemphasizes on making the research results easily accessible to the end user -\nmusic enthusiasts, producers and performers -- by using the latest artificial\nintelligence platforms and tools.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 04:28:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Marinov", "Georgi", ""]]}, {"id": "2011.13137", "submitter": "Yifan Gao", "authors": "Yifan Gao, Henghui Zhu, Patrick Ng, Cicero Nogueira dos Santos, Zhiguo\n  Wang, Feng Nan, Dejiao Zhang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang", "title": "Answering Ambiguous Questions through Generative Evidence Fusion and\n  Round-Trip Prediction", "comments": "ACL 2021 main conference, 14 pages, 7 figures. Code will be released\n  at https://github.com/amzn/refuel-open-domain-qa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In open-domain question answering, questions are highly likely to be\nambiguous because users may not know the scope of relevant topics when\nformulating them. Therefore, a system needs to find possible interpretations of\nthe question, and predict one or multiple plausible answers. When multiple\nplausible answers are found, the system should rewrite the question for each\nanswer to resolve the ambiguity. In this paper, we present a model that\naggregates and combines evidence from multiple passages to adaptively predict a\nsingle answer or a set of question-answer pairs for ambiguous questions. In\naddition, we propose a novel round-trip prediction approach to iteratively\ngenerate additional interpretations that our model fails to find in the first\npass, and then verify and filter out the incorrect question-answer pairs to\narrive at the final disambiguated output. Our model, named Refuel, achieves a\nnew state-of-the-art performance on the AmbigQA dataset, and shows competitive\nperformance on NQ-Open and TriviaQA. The proposed round-trip prediction is a\nmodel-agnostic general approach for answering ambiguous open-domain questions,\nwhich improves our Refuel as well as several baseline models. We release source\ncode for our models and experiments at\nhttps://github.com/amzn/refuel-open-domain-qa.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 05:48:55 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:07:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gao", "Yifan", ""], ["Zhu", "Henghui", ""], ["Ng", "Patrick", ""], ["Santos", "Cicero Nogueira dos", ""], ["Wang", "Zhiguo", ""], ["Nan", "Feng", ""], ["Zhang", "Dejiao", ""], ["Nallapati", "Ramesh", ""], ["Arnold", "Andrew O.", ""], ["Xiang", "Bing", ""]]}, {"id": "2011.13152", "submitter": "Shengheng Liu", "authors": "Yongming Huang, Shengheng Liu, Cheng Zhang, Xiaohu You, Hequan Wu", "title": "True-data Testbed for 5G/B5G Intelligent Network", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile\ncommunications will shift from facilitating interpersonal communications to\nsupporting Internet of Everything (IoE), where intelligent communications with\nfull integration of big data and artificial intelligence (AI) will play an\nimportant role in improving network efficiency and providing high-quality\nservice. As a rapid evolving paradigm, the AI-empowered mobile communications\ndemand large amounts of data acquired from real network environment for\nsystematic test and verification. Hence, we build the world's first true-data\ntestbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site\nexperimental networks, data acquisition & data warehouse, and AI engine &\nnetwork optimization. In the TTIN, true network data acquisition, storage,\nstandardization, and analysis are available, which enable system-level online\nverification of B5G/6G-orientated key technologies and support data-driven\nnetwork optimization through the closed-loop control mechanism. This paper\nelaborates on the system architecture and module design of TTIN. Detailed\ntechnical specifications and some of the established use cases are also\nshowcased.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:42:36 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 08:51:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Huang", "Yongming", ""], ["Liu", "Shengheng", ""], ["Zhang", "Cheng", ""], ["You", "Xiaohu", ""], ["Wu", "Hequan", ""]]}, {"id": "2011.13160", "submitter": "Xin Hong", "authors": "Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo and Xueqi Cheng", "title": "Transformation Driven Visual Reasoning", "comments": "Accepted to CVPR 2021. Resources including the TRANCE dataset and the\n  code can be found at our homepage https://hongxin2019.github.io/TVR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a new visual reasoning paradigm by introducing an\nimportant factor, i.e.~transformation. The motivation comes from the fact that\nmost existing visual reasoning tasks, such as CLEVR in VQA, are solely defined\nto test how well the machine understands the concepts and relations within\nstatic settings, like one image. We argue that this kind of \\textbf{state\ndriven visual reasoning} approach has limitations in reflecting whether the\nmachine has the ability to infer the dynamics between different states, which\nhas been shown as important as state-level reasoning for human cognition in\nPiaget's theory. To tackle this problem, we propose a novel\n\\textbf{transformation driven visual reasoning} task. Given both the initial\nand final states, the target is to infer the corresponding single-step or\nmulti-step transformation, represented as a triplet (object, attribute, value)\nor a sequence of triplets, respectively. Following this definition, a new\ndataset namely TRANCE is constructed on the basis of CLEVR, including three\nlevels of settings, i.e.~Basic (single-step transformation), Event (multi-step\ntransformation), and View (multi-step transformation with variant views).\nExperimental results show that the state-of-the-art visual reasoning models\nperform well on Basic, but are still far from human-level intelligence on Event\nand View. We believe the proposed new paradigm will boost the development of\nmachine visual reasoning. More advanced methods and real data need to be\ninvestigated in this direction. The resource of TVR is available at\nhttps://hongxin2019.github.io/TVR.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 07:11:31 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 06:25:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hong", "Xin", ""], ["Lan", "Yanyan", ""], ["Pang", "Liang", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2011.13163", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate and Cosmin Bonchi\\c{s}", "title": "Being Central on the Cheap: Stability in Heterogeneous Multiagent\n  Centrality Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cond-mat.dis-nn cs.AI cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study strategic network formation games in which agents attempt to form\n(costly) links in order to maximize their network centrality. Our model derives\nfrom Jackson and Wolinsky's symmetric connection model, but allows for\nheterogeneity in agent utilities by replacing decay centrality (implicit in the\nJackson-Wolinsky model) by a variety of classical centrality and game-theoretic\nmeasures of centrality. We are primarily interested in characterizing the\nasymptotically pairwise stable networks, i.e. those networks that are pairwise\nstable for all sufficiently small, positive edge costs. We uncover a rich\ntypology of stability:\n  - we give an axiomatic approach to network centrality that allows us to\npredict the stable network for a rich set of combination of centrality utility\nfunctions, yielding stable networks with features reminiscent of structural\nproperties such as \"core periphery\" and \"rich club\" networks.\n  - We show that a simple variation on the model renders it universal, i.e.\nevery network may be a stable network.\n  - We also show that often we can infer a significant amount about agent\nutilities from the structure of stable networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 07:39:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Istrate", "Gabriel", ""], ["Bonchi\u015f", "Cosmin", ""]]}, {"id": "2011.13169", "submitter": "Muhammad Naseer Bajwa", "authors": "Adriano Lucieri, Muhammad Naseer Bajwa, Andreas Dengel, Sheraz Ahmed", "title": "Achievements and Challenges in Explaining Deep Learning based\n  Computer-Aided Diagnosis Systems", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Remarkable success of modern image-based AI methods and the resulting\ninterest in their applications in critical decision-making processes has led to\na surge in efforts to make such intelligent systems transparent and\nexplainable. The need for explainable AI does not stem only from ethical and\nmoral grounds but also from stricter legislation around the world mandating\nclear and justifiable explanations of any decision taken or assisted by AI.\nEspecially in the medical context where Computer-Aided Diagnosis can have a\ndirect influence on the treatment and well-being of patients, transparency is\nof utmost importance for safe transition from lab research to real world\nclinical practice. This paper provides a comprehensive overview of current\nstate-of-the-art in explaining and interpreting Deep Learning based algorithms\nin applications of medical research and diagnosis of diseases. We discuss early\nachievements in development of explainable AI for validation of known disease\ncriteria, exploration of new potential biomarkers, as well as methods for the\nsubsequent correction of AI models. Various explanation methods like visual,\ntextual, post-hoc, ante-hoc, local and global have been thoroughly and\ncritically analyzed. Subsequently, we also highlight some of the remaining\nchallenges that stand in the way of practical applications of AI as a clinical\ndecision support tool and provide recommendations for the direction of future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 08:08:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lucieri", "Adriano", ""], ["Bajwa", "Muhammad Naseer", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2011.13200", "submitter": "Sourav Dutta", "authors": "Silviu Oprea and Sourav Dutta and Haytham Assem", "title": "Unsupervised Word Translation Pairing using Refinement based Point Set\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual alignment of word embeddings play an important role in\nknowledge transfer across languages, for improving machine translation and\nother multi-lingual applications. Current unsupervised approaches rely on\nsimilarities in geometric structure of word embedding spaces across languages,\nto learn structure-preserving linear transformations using adversarial networks\nand refinement strategies. However, such techniques, in practice, tend to\nsuffer from instability and convergence issues, requiring tedious fine-tuning\nfor precise parameter setting. This paper proposes BioSpere, a novel framework\nfor unsupervised mapping of bi-lingual word embeddings onto a shared vector\nspace, by combining adversarial initialization and refinement procedure with\npoint set registration algorithm used in image processing. We show that our\nframework alleviates the shortcomings of existing methodologies, and is\nrelatively invariant to variable adversarial learning performance, depicting\nrobustness in terms of parameter choices and training losses. Experimental\nevaluation on parallel dictionary induction task demonstrates state-of-the-art\nresults for our framework on diverse language pairs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:51:29 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Oprea", "Silviu", ""], ["Dutta", "Sourav", ""], ["Assem", "Haytham", ""]]}, {"id": "2011.13203", "submitter": "Hans van Ditmarsch", "authors": "Hans van Ditmarsch, Malvin Gattinger, Rahim Ramezanian", "title": "Everyone Knows that Everyone Knows: Gossip Protocols for Super Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A gossip protocol is a procedure for sharing secrets in a network. The basic\naction in a gossip protocol is a telephone call wherein the calling agents\nexchange all the secrets they know. An agent who knows all secrets is an\nexpert. The usual termination condition is that all agents are experts.\n  Instead, we explore protocols wherein the termination condition is that all\nagents know that all agents are experts. We call such agents super experts.\nAdditionally, we model that agents who are super experts do not make and do not\nanswer calls. Such agents are called engaged agents. We also model that such\ngossip protocols are common knowledge among the agents. We investigate\nconditions under which protocols terminate, both in the synchronous case, where\nthere is a global clock, and in the asynchronous case, where there is not. We\nshow that a commonly known protocol with engaged agents may terminate faster\nthan the same protocol without engaged agents.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:57:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["van Ditmarsch", "Hans", ""], ["Gattinger", "Malvin", ""], ["Ramezanian", "Rahim", ""]]}, {"id": "2011.13230", "submitter": "Mohamed Ahmed", "authors": "Benedek Fabian, Thomas Edlich, H\\'el\\'ena Gaspar, Marwin Segler,\n  Joshua Meyers, Marco Fiscato, Mohamed Ahmed", "title": "Molecular representation learning with language models and\n  domain-relevant auxiliary tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a Transformer architecture, specifically BERT, to learn flexible and\nhigh quality molecular representations for drug discovery problems. We study\nthe impact of using different combinations of self-supervised tasks for\npre-training, and present our results for the established Virtual Screening and\nQSAR benchmarks. We show that: i) The selection of appropriate self-supervised\ntask(s) for pre-training has a significant impact on performance in subsequent\ndownstream tasks such as Virtual Screening. ii) Using auxiliary tasks with more\ndomain relevance for Chemistry, such as learning to predict calculated\nmolecular properties, increases the fidelity of our learnt representations.\niii) Finally, we show that molecular representations learnt by our model\n`MolBert' improve upon the current state of the art on the benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:55:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fabian", "Benedek", ""], ["Edlich", "Thomas", ""], ["Gaspar", "H\u00e9l\u00e9na", ""], ["Segler", "Marwin", ""], ["Meyers", "Joshua", ""], ["Fiscato", "Marco", ""], ["Ahmed", "Mohamed", ""]]}, {"id": "2011.13246", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti, Vanessa Gonzalez Duque, Marion Crouzier, Antoine\n  Nordez, Lilian Lacourpaille, and Diana Mateus", "title": "IFSS-Net: Interactive Few-Shot Siamese Network for Faster Muscle\n  Segmentation and Propagation in Volumetric Ultrasound", "comments": "14 pages, 18 figures, 10 Tables", "journal-ref": null, "doi": "10.1109/TMI.2021.3058303", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an accurate, fast and efficient method for segmentation and muscle\nmask propagation in 3D freehand ultrasound data, towards accurate volume\nquantification. A deep Siamese 3D Encoder-Decoder network that captures the\nevolution of the muscle appearance and shape for contiguous slices is deployed.\nWe uses it to propagate a reference mask annotated by a clinical expert. To\nhandle longer changes of the muscle shape over the entire volume and to provide\nan accurate propagation, we devise a Bidirectional Long Short Term Memory\nmodule. Also, to train our model with a minimal amount of training samples, we\npropose a strategy combining learning from few annotated 2D ultrasound slices\nwith sequential pseudo-labeling of the unannotated slices. We introduce a\ndecremental update of the objective function to guide the model convergence in\nthe absence of large amounts of annotated data. After training with a small\nnumber of volumes, the decremental update transitions from a weakly-supervised\ntraining to a few-shot setting. Finally, to handle the class-imbalance between\nforeground and background muscle pixels, we propose a parametric Tversky loss\nfunction that learns to adaptively penalize false positives and false\nnegatives. We validate our approach for the segmentation, label propagation,\nand volume computation of the three low-limb muscles on a dataset of 61600\nimages from 44 subjects. We achieve a Dice score coefficient of over $95~\\%$\nand a volumetric error \\textcolor{black}{of} $1.6035 \\pm 0.587~\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:37:25 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 11:40:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Duque", "Vanessa Gonzalez", ""], ["Crouzier", "Marion", ""], ["Nordez", "Antoine", ""], ["Lacourpaille", "Lilian", ""], ["Mateus", "Diana", ""]]}, {"id": "2011.13265", "submitter": "Sandesh Ramesh", "authors": "Sandesh Ramesh, Anirudh Hebbar, Varun Yadav, Thulasiram Gunta, and A\n  Balachandra", "title": "CYPUR-NN: Crop Yield Prediction Using Regression and Neural Networks", "comments": "Advances in Intelligent Systems and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Our recent study using historic data of paddy yield and associated conditions\ninclude humidity, luminescence, and temperature. By incorporating regression\nmodels and neural networks (NN), one can produce highly satisfactory\nforecasting of paddy yield. Simulations indicate that our model can predict\npaddy yield with high accuracy while concurrently detecting diseases that may\nexist and are oblivious to the human eye. Crop Yield Prediction Using\nRegression and Neural Networks (CYPUR-NN) is developed here as a system that\nwill facilitate agriculturists and farmers to predict yield from a picture or\nby entering values via a web interface. CYPUR-NN has been tested on stock\nimages and the experimental results are promising.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 12:50:58 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ramesh", "Sandesh", ""], ["Hebbar", "Anirudh", ""], ["Yadav", "Varun", ""], ["Gunta", "Thulasiram", ""], ["Balachandra", "A", ""]]}, {"id": "2011.13277", "submitter": "Damien Pellier", "authors": "Maxence Grand, Humbert Fiorino, Damien Pellier", "title": "AMLSI: A Novel Accurate Action Model Learning Algorithm", "comments": "8 pages", "journal-ref": "Proceedings of the International Workshop on Knowledge Engineering\n  for Planning and Scheduling (KEPS), ICAPS, Oct 2020, Nancy, France", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents new approach based on grammar induction called AMLSI\nAction Model Learning with State machine Interactions. The AMLSI approach does\nnot require a training dataset of plan traces to work. AMLSI proceeds by trial\nand error: it queries the system to learn with randomly generated action\nsequences, and it observes the state transitions of the system, then AMLSI\nreturns a PDDL domain corresponding to the system. A key issue for domain\nlearning is the ability to plan with the learned domains. It often happens that\na small learning error leads to a domain that is unusable for planning. Unlike\nother algorithms, we show that AMLSI is able to lift this lock by learning\ndomains from partial and noisy observations with sufficient accuracy to allow\nplanners to solve new problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 13:25:08 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Grand", "Maxence", ""], ["Fiorino", "Humbert", ""], ["Pellier", "Damien", ""]]}, {"id": "2011.13297", "submitter": "Damien Pellier", "authors": "Damien Pellier, Humbert Fiorino", "title": "Totally and Partially Ordered Hierarchical Planners in PDDL4J Library", "comments": "2 pages", "journal-ref": "Proceedings of the International Planning Competition, ICAPS,\n  Nancy, France, 2020", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we outline the implementation of the TFD (Totally Ordered Fast\nDownward) and the PFD (Partially ordered Fast Downward) hierarchical planners\nthat participated in the first HTN IPC competition in 2020. These two planners\nare based on forward-chaining task decomposition coupled with a compact\ngrounding of actions, methods, tasks and HTN problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:00:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Pellier", "Damien", ""], ["Fiorino", "Humbert", ""]]}, {"id": "2011.13305", "submitter": "Carsten Hahn", "authors": "Carsten Hahn, Sebastian Feld, Hannes Schroter", "title": "Predictive Collision Management for Time and Risk Dependent Path\n  Planning", "comments": "Extended version of the SIGSPATIAL '20 paper", "journal-ref": "Proceedings of the 28th ACM SIGSPATIAL International Conference on\n  Advances in Geographic Information Systems (SIGSPATIAL '20), 2020, Pages\n  405-408", "doi": "10.1145/3397536.3422252", "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents such as self-driving cars or parcel robots need to\nrecognize and avoid possible collisions with obstacles in order to move\nsuccessfully in their environment. Humans, however, have learned to predict\nmovements intuitively and to avoid obstacles in a forward-looking way. The task\nof collision avoidance can be divided into a global and a local level.\nRegarding the global level, we propose an approach called \"Predictive Collision\nManagement Path Planning\" (PCMP). At the local level, solutions for collision\navoidance are used that prevent an inevitable collision. Therefore, the aim of\nPCMP is to avoid unnecessary local collision scenarios using predictive\ncollision management. PCMP is a graph-based algorithm with a focus on the time\ndimension consisting of three parts: (1) movement prediction, (2) integration\nof movement prediction into a time-dependent graph, and (3) time and\nrisk-dependent path planning. The algorithm combines the search for a shortest\npath with the question: is the detour worth avoiding a possible collision\nscenario? We evaluate the evasion behavior in different simulation scenarios\nand the results show that a risk-sensitive agent can avoid 47.3% of the\ncollision scenarios while making a detour of 1.3%. A risk-averse agent avoids\nup to 97.3% of the collision scenarios with a detour of 39.1%. Thus, an agent's\nevasive behavior can be controlled actively and risk-dependent using PCMP.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:15:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hahn", "Carsten", ""], ["Feld", "Sebastian", ""], ["Schroter", "Hannes", ""]]}, {"id": "2011.13322", "submitter": "Zhen Huang", "authors": "Zhen Huang, Xu Shen, Xinmei Tian, Houqiang Li, Jianqiang Huang and\n  Xian-Sheng Hua", "title": "Spatio-Temporal Inception Graph Convolutional Networks for\n  Skeleton-Based Action Recognition", "comments": "ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:43:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Huang", "Zhen", ""], ["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Li", "Houqiang", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2011.13332", "submitter": "Eugenio Chisari", "authors": "Eugenio Chisari, Alexander Liniger, Alisa Rupenyan, Luc Van Gool, John\n  Lygeros", "title": "Learning from Simulation, Racing in Reality", "comments": "Presented at ICRA 2021. For associated video, see\n  https://youtu.be/Z2A82AkT7GI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reinforcement learning-based solution to autonomously race on a\nminiature race car platform. We show that a policy that is trained purely in\nsimulation using a relatively simple vehicle model, including model\nrandomization, can be successfully transferred to the real robotic setup. We\nachieve this by using novel policy output regularization approach and a lifted\naction space which enables smooth actions but still aggressive race car\ndriving. We show that this regularized policy does outperform the Soft Actor\nCritic (SAC) baseline method, both in simulation and on the real car, but it is\nstill outperformed by a Model Predictive Controller (MPC) state of the art\nmethod. The refinement of the policy with three hours of real-world interaction\ndata allows the reinforcement learning policy to achieve lap times similar to\nthe MPC controller while reducing track constraint violations by 50%.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:58:49 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 08:21:06 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chisari", "Eugenio", ""], ["Liniger", "Alexander", ""], ["Rupenyan", "Alisa", ""], ["Van Gool", "Luc", ""], ["Lygeros", "John", ""]]}, {"id": "2011.13374", "submitter": "Huy Kang Kim", "authors": "Eunji Park, Kyung Ho Park, Huy Kang Kim", "title": "Understand Watchdogs: Discover How Game Bot Get Discovered", "comments": "9 pages, 3 figures, 3 tables, this paper is accepted in ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game industry has long been troubled by malicious activities utilizing\ngame bots. The game bots disturb other game players and destroy the\nenvironmental system of the games. For these reasons, the game industry put\ntheir best efforts to detect the game bots among players' characters using the\nlearning-based detections. However, one problem with the detection\nmethodologies is that they do not provide rational explanations about their\ndecisions. To resolve this problem, in this work, we investigate the\nexplainabilities of the game bot detection. We develop the XAI model using a\ndataset from the Korean MMORPG, AION, which includes game logs of human players\nand game bots. More than one classification model has been applied to the\ndataset to be analyzed by applying interpretable models. This provides us\nexplanations about the game bots' behavior, and the truthfulness of the\nexplanations has been evaluated. Besides, interpretability contributes to\nminimizing false detection, which imposes unfair restrictions on human players.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:34:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 12:29:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Park", "Eunji", ""], ["Park", "Kyung Ho", ""], ["Kim", "Huy Kang", ""]]}, {"id": "2011.13380", "submitter": "Chaopeng Shen", "authors": "Dapeng Feng, Kathryn Lawson and Chaopeng Shen", "title": "Prediction in ungauged regions with sparse flow duration curves and\n  input-selection ensemble modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While long short-term memory (LSTM) models have demonstrated stellar\nperformance with streamflow predictions, there are major risks in applying\nthese models in contiguous regions with no gauges, or predictions in ungauged\nregions (PUR) problems. However, softer data such as the flow duration curve\n(FDC) may be already available from nearby stations, or may become available.\nHere we demonstrate that sparse FDC data can be migrated and assimilated by an\nLSTM-based network, via an encoder. A stringent region-based holdout test\nshowed a median Kling-Gupta efficiency (KGE) of 0.62 for a US dataset,\nsubstantially higher than previous state-of-the-art global-scale ungauged basin\ntests. The baseline model without FDC was already competitive (median KGE\n0.56), but integrating FDCs had substantial value. Because of the inaccurate\nrepresentation of inputs, the baseline models might sometimes produce\ncatastrophic results. However, model generalizability was further meaningfully\nimproved by compiling an ensemble based on models with different input\nselections.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:40:22 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Feng", "Dapeng", ""], ["Lawson", "Kathryn", ""], ["Shen", "Chaopeng", ""]]}, {"id": "2011.13464", "submitter": "Jane Wang", "authors": "Jane X. Wang", "title": "Meta-learning in natural and artificial intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Meta-learning, or learning to learn, has gained renewed interest in recent\nyears within the artificial intelligence community. However, meta-learning is\nincredibly prevalent within nature, has deep roots in cognitive science and\npsychology, and is currently studied in various forms within neuroscience. The\naim of this review is to recast previous lines of research in the study of\nbiological intelligence within the lens of meta-learning, placing these works\ninto a common framework. More recent points of interaction between AI and\nneuroscience will be discussed, as well as interesting new directions that\narise under this perspective.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 20:21:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Jane X.", ""]]}, {"id": "2011.13467", "submitter": "Tianhong Dai", "authors": "Tianhong Dai, Hengyan Liu, Anil Anthony Bharath", "title": "Episodic Self-Imitation Learning with Hindsight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic self-imitation learning, a novel self-imitation algorithm with a\ntrajectory selection module and an adaptive loss function, is proposed to speed\nup reinforcement learning. Compared to the original self-imitation learning\nalgorithm, which samples good state-action pairs from the experience replay\nbuffer, our agent leverages entire episodes with hindsight to aid\nself-imitation learning. A selection module is introduced to filter\nuninformative samples from each episode of the update. The proposed method\novercomes the limitations of the standard self-imitation learning algorithm, a\ntransitions-based method which performs poorly in handling continuous control\nenvironments with sparse rewards. From the experiments, episodic self-imitation\nlearning is shown to perform better than baseline on-policy algorithms,\nachieving comparable performance to state-of-the-art off-policy algorithms in\nseveral simulated robot control tasks. The trajectory selection module is shown\nto prevent the agent learning undesirable hindsight experiences. With the\ncapability of solving sparse reward problems in continuous control settings,\nepisodic self-imitation learning has the potential to be applied to real-world\nproblems that have continuous action spaces, such as robot guidance and\nmanipulation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 20:36:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Dai", "Tianhong", ""], ["Liu", "Hengyan", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "2011.13565", "submitter": "Yuanhao Shen", "authors": "Yuanhao Shen and Jungang Han", "title": "Joint Extraction of Entity and Relation with Information Redundancy\n  Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To solve the problem of redundant information and overlapping relations of\nthe entity and relation extraction model, we propose a joint extraction model.\nThis model can directly extract multiple pairs of related entities without\ngenerating unrelated redundant information. We also propose a recurrent neural\nnetwork named Encoder-LSTM that enhances the ability of recurrent units to\nmodel sentences. Specifically, the joint model includes three sub-modules: the\nNamed Entity Recognition sub-module consisted of a pre-trained language model\nand an LSTM decoder layer, the Entity Pair Extraction sub-module which uses\nEncoder-LSTM network to model the order relationship between related entity\npairs, and the Relation Classification sub-module including Attention\nmechanism. We conducted experiments on the public datasets ADE and CoNLL04 to\nevaluate the effectiveness of our model. The results show that the proposed\nmodel achieves good performance in the task of entity and relation extraction\nand can greatly reduce the amount of redundant information.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 05:47:26 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shen", "Yuanhao", ""], ["Han", "Jungang", ""]]}, {"id": "2011.13570", "submitter": "Yekyung Kim", "authors": "Yekyung Kim", "title": "Deep Active Learning for Sequence Labeling Based on Diversity and\n  Uncertainty in Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several studies have investigated active learning (AL) for natural\nlanguage processing tasks to alleviate data dependency. However, for query\nselection, most of these studies mainly rely on uncertainty-based sampling,\nwhich generally does not exploit the structural information of the unlabeled\ndata. This leads to a sampling bias in the batch active learning setting, which\nselects several samples at once. In this work, we demonstrate that the amount\nof labeled training data can be reduced using active learning when it\nincorporates both uncertainty and diversity in the sequence labeling task. We\nexamined the effects of our sequence-based approach by selecting weighted\ndiverse in the gradient embedding approach across multiple tasks, datasets,\nmodels, and consistently outperform classic uncertainty-based sampling and\ndiversity-based sampling.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 06:03:27 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kim", "Yekyung", ""]]}, {"id": "2011.13572", "submitter": "Sijie Mai", "authors": "Sijie Mai, Songlong Xing, Jiaxuan He, Ying Zeng, Haifeng Hu", "title": "Analyzing Unaligned Multimodal Sequence via Graph Convolution and Graph\n  Pooling Fusion", "comments": "preprint, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of multimodal sequence analysis which aims\nto draw inferences from visual, language and acoustic sequences. A majority of\nexisting works generally focus on aligned fusion, mostly at word level, of the\nthree modalities to accomplish this task, which is impractical in real-world\nscenarios. To overcome this issue, we seek to address the task of multimodal\nsequence analysis on unaligned modality sequences which is still relatively\nunderexplored and also more challenging. Recurrent neural network (RNN) and its\nvariants are widely used in multimodal sequence analysis, but they are\nsusceptible to the issues of gradient vanishing/explosion and high time\ncomplexity due to its recurrent nature. Therefore, we propose a novel model,\ntermed Multimodal Graph, to investigate the effectiveness of graph neural\nnetworks (GNN) on modeling multimodal sequential data. The graph-based\nstructure enables parallel computation in time dimension and can learn longer\ntemporal dependency in long unaligned sequences. Specifically, our Multimodal\nGraph is hierarchically structured to cater to two stages, i.e., intra- and\ninter-modal dynamics learning. For the first stage, a graph convolutional\nnetwork is employed for each modality to learn intra-modal dynamics. In the\nsecond stage, given that the multimodal sequences are unaligned, the commonly\nconsidered word-level fusion does not pertain. To this end, we devise a graph\npooling fusion network to automatically learn the associations between various\nnodes from different modalities. Additionally, we define multiple ways to\nconstruct the adjacency matrix for sequential data. Experimental results\nsuggest that our graph-based model reaches state-of-the-art performance on two\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 06:12:14 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 04:36:37 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 17:09:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mai", "Sijie", ""], ["Xing", "Songlong", ""], ["He", "Jiaxuan", ""], ["Zeng", "Ying", ""], ["Hu", "Haifeng", ""]]}, {"id": "2011.13600", "submitter": "Junhao Hua", "authors": "Junhao Hua, Chunguang Li", "title": "Distributed Variational Bayesian Algorithms Over Sensor Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2493979", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed inference/estimation in Bayesian framework in the context of\nsensor networks has recently received much attention due to its broad\napplicability. The variational Bayesian (VB) algorithm is a technique for\napproximating intractable integrals arising in Bayesian inference. In this\npaper, we propose two novel distributed VB algorithms for general Bayesian\ninference problem, which can be applied to a very general class of\nconjugate-exponential models. In the first approach, the global natural\nparameters at each node are optimized using a stochastic natural gradient that\nutilizes the Riemannian geometry of the approximation space, followed by an\ninformation diffusion step for cooperation with the neighbors. In the second\nmethod, a constrained optimization formulation for distributed estimation is\nestablished in natural parameter space and solved by alternating direction\nmethod of multipliers (ADMM). An application of the distributed\ninference/estimation of a Bayesian Gaussian mixture model is then presented, to\nevaluate the effectiveness of the proposed algorithms. Simulations on both\nsynthetic and real datasets demonstrate that the proposed algorithms have\nexcellent performance, which are almost as good as the corresponding\ncentralized VB algorithm relying on all data available in a fusion center.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:12:18 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hua", "Junhao", ""], ["Li", "Chunguang", ""]]}, {"id": "2011.13604", "submitter": "Carole Adam", "authors": "Elise Beck, Julie Dugdale, Carole Adam, Christelle Ga\\\"idatzis, Julius\n  Ba\\~ngate", "title": "A methodology for co-constructing an interdisciplinary model: from model\n  to survey, from survey to model", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How should computer science and social science collaborate to build a common\nmodel? How should they proceed to gather data that is really useful to the\nmodelling? How can they design a survey that is tailored to the target model?\nThis paper aims to answer those crucial questions in the framework of a\nmultidisciplinary research project. This research addresses the issue of\nco-constructing a model when several disciplines are involved, and is applied\nto modelling human behaviour immediately after an earthquake. The main\ncontribution of the work is to propose a tool dedicated to multidisciplinary\ndialogue. It also proposes a reflexive analysis of the enriching intellectual\nprocess carried out by the different disciplines involved. Finally, from\nworking with an anthropologist, a complementary view of the multidisciplinary\nprocess is given.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:41:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Beck", "Elise", ""], ["Dugdale", "Julie", ""], ["Adam", "Carole", ""], ["Ga\u00efdatzis", "Christelle", ""], ["Ba\u00f1gate", "Julius", ""]]}, {"id": "2011.13614", "submitter": "Shanshan Wang", "authors": "Kehan Qi, Yu Gong, Xinfeng Liu, Xin Liu, Hairong Zheng, Shanshan Wang", "title": "Multi-task MR Imaging with Iterative Teacher Forcing and Re-weighted\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Noises, artifacts, and loss of information caused by the magnetic resonance\n(MR) reconstruction may compromise the final performance of the downstream\napplications. In this paper, we develop a re-weighted multi-task deep learning\nmethod to learn prior knowledge from the existing big dataset and then utilize\nthem to assist simultaneous MR reconstruction and segmentation from the\nunder-sampled k-space data. The multi-task deep learning framework is equipped\nwith two network sub-modules, which are integrated and trained by our designed\niterative teacher forcing scheme (ITFS) under the dynamic re-weighted loss\nconstraint (DRLC). The ITFS is designed to avoid error accumulation by\ninjecting the fully-sampled data into the training process. The DRLC is\nproposed to dynamically balance the contributions from the reconstruction and\nsegmentation sub-modules so as to co-prompt the multi-task accuracy. The\nproposed method has been evaluated on two open datasets and one in vivo\nin-house dataset and compared to six state-of-the-art methods. Results show\nthat the proposed method possesses encouraging capabilities for simultaneous\nand accurate MR reconstruction and segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:08:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Qi", "Kehan", ""], ["Gong", "Yu", ""], ["Liu", "Xinfeng", ""], ["Liu", "Xin", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "2011.13615", "submitter": "Siyu Liu", "authors": "Siyu Liu, Jason A. Dowling, Craig Engstrom, Peter B. Greer, Stuart\n  Crozier, Shekhar S. Chandra", "title": "Manipulating Medical Image Translation with Manifold Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image translation (e.g. CT to MR) is a challenging task as it\nrequires I) faithful translation of domain-invariant features (e.g. shape\ninformation of anatomical structures) and II) realistic synthesis of\ntarget-domain features (e.g. tissue appearance in MR). In this work, we propose\nManifold Disentanglement Generative Adversarial Network (MDGAN), a novel image\ntranslation framework that explicitly models these two types of features. It\nemploys a fully convolutional generator to model domain-invariant features, and\nit uses style codes to separately model target-domain features as a manifold.\nThis design aims to explicitly disentangle domain-invariant features and\ndomain-specific features while gaining individual control of both. The image\ntranslation process is formulated as a stylisation task, where the input is\n\"stylised\" (translated) into diverse target-domain images based on style codes\nsampled from the learnt manifold. We test MDGAN for multi-modal medical image\ntranslation, where we create two domain-specific manifold clusters on the\nmanifold to translate segmentation maps into pseudo-CT and pseudo-MR images,\nrespectively. We show that by traversing a path across the MR manifold cluster,\nthe target output can be manipulated while still retaining the shape\ninformation from the input.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:11:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Liu", "Siyu", ""], ["Dowling", "Jason A.", ""], ["Engstrom", "Craig", ""], ["Greer", "Peter B.", ""], ["Crozier", "Stuart", ""], ["Chandra", "Shekhar S.", ""]]}, {"id": "2011.13629", "submitter": "Pengfei Wei", "authors": "Pengfei Wei and Tze Yun Leong", "title": "Randomized Transferable Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Feature-based transfer is one of the most effective methodologies for\ntransfer learning. Existing studies usually assume that the learned new feature\nrepresentation is truly \\emph{domain-invariant}, and thus directly train a\ntransfer model $\\mathcal{M}$ on source domain. In this paper, we consider a\nmore realistic scenario where the new feature representation is suboptimal and\nsmall divergence still exists across domains. We propose a new learning\nstrategy with a transfer model called Randomized Transferable Machine (RTM).\nMore specifically, we work on source data with the new feature representation\nlearned from existing feature-based transfer methods. The key idea is to\nenlarge source training data populations by randomly corrupting source data\nusing some noises, and then train a transfer model $\\widetilde{\\mathcal{M}}$\nthat performs well on all the corrupted source data populations. In principle,\nthe more corruptions are made, the higher the probability of the target data\ncan be covered by the constructed source populations, and thus better transfer\nperformance can be achieved by $\\widetilde{\\mathcal{M}}$. An ideal case is with\ninfinite corruptions, which however is infeasible in reality. We develop a\nmarginalized solution with linear regression model and dropout noise. With a\nmarginalization trick, we can train an RTM that is equivalently to training\nusing infinite source noisy populations without truly conducting any\ncorruption. More importantly, such an RTM has a closed-form solution, which\nenables very fast and efficient training. Extensive experiments on various\nreal-world transfer tasks show that RTM is a promising transfer model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:37:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wei", "Pengfei", ""], ["Leong", "Tze Yun", ""]]}, {"id": "2011.13636", "submitter": "Miao Qin", "authors": "Miao Qin, Yongchuan Tang", "title": "Combination of interval-valued belief structures based on belief entropy", "comments": "Simply using MDPI as a template. It does not indicate that this\n  article will be submitted to Entropy in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the issues of combination and normalization of\ninterval-valued belief structures within the framework of Dempster-Shafer\ntheory of evidence. Existing approaches are reviewed and thoroughly analyzed.\nThe advantages and drawbacks of previous approach are presented. A new\noptimality approach based on uncertainty measure is developed, where the\nproblem of combining interval-valued belief structures degenerates into\ncombining basic probability assignments. Numerical examples are provided to\nillustrate the rationality of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:09:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Qin", "Miao", ""], ["Tang", "Yongchuan", ""]]}, {"id": "2011.13647", "submitter": "Kanjirangat Vani", "authors": "Simone Mellace, K Vani, Alessandro Antonucci", "title": "Relation Clustering in Narrative Knowledge Graphs", "comments": "Accepted for AI4Narratives Workshop at 29th International Joint\n  Conference on Artificial Intelligence and the 17th Pacific Rim International\n  Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When coping with literary texts such as novels or short stories, the\nextraction of structured information in the form of a knowledge graph might be\nhindered by the huge number of possible relations between the entities\ncorresponding to the characters in the novel and the consequent hurdles in\ngathering supervised information about them. Such issue is addressed here as an\nunsupervised task empowered by transformers: relational sentences in the\noriginal text are embedded (with SBERT) and clustered in order to merge\ntogether semantically similar relations. All the sentences in the same cluster\nare finally summarized (with BART) and a descriptive label extracted from the\nsummary. Preliminary tests show that such clustering might successfully detect\nsimilar relations, and provide a valuable preprocessing for semi-supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:43:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Mellace", "Simone", ""], ["Vani", "K", ""], ["Antonucci", "Alessandro", ""]]}, {"id": "2011.13689", "submitter": "Andrei Haidu", "authors": "Andrei Haidu and Michael Beetz", "title": "Automated acquisition of structured, semantic models of manipulation\n  activities from human VR demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a system capable of collecting and annotating, human\nperformed, robot understandable, everyday activities from virtual environments.\nThe human movements are mapped in the simulated world using off-the-shelf\nvirtual reality devices with full body, and eye tracking capabilities. All the\ninteractions in the virtual world are physically simulated, thus movements and\ntheir effects are closely relatable to the real world. During the activity\nexecution, a subsymbolic data logger is recording the environment and the human\ngaze on a per-frame basis, enabling offline scene reproduction and replays.\nCoupled with the physics engine, online monitors (symbolic data loggers) are\nparsing (using various grammars) and recording events, actions, and their\neffects in the simulated world.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:58:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Haidu", "Andrei", ""], ["Beetz", "Michael", ""]]}, {"id": "2011.13721", "submitter": "Alexis De Colnet", "authors": "Alexis de Colnet and Stefan Mengel", "title": "Lower Bounds for Approximate Knowledge Compilation", "comments": "11 pages, including appendices", "journal-ref": null, "doi": "10.24963/ijcai.2020/254", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge compilation studies the trade-off between succinctness and\nefficiency of different representation languages. For many languages, there are\nknown strong lower bounds on the representation size, but recent work shows\nthat, for some languages, one can bypass these bounds using approximate\ncompilation. The idea is to compile an approximation of the knowledge for which\nthe number of errors can be controlled. We focus on circuits in deterministic\ndecomposable negation normal form (d-DNNF), a compilation language suitable in\ncontexts such as probabilistic reasoning, as it supports efficient model\ncounting and probabilistic inference. Moreover, there are known size lower\nbounds for d-DNNF which by relaxing to approximation one might be able to\navoid. In this paper we formalize two notions of approximation: weak\napproximation which has been studied before in the decision diagram literature\nand strong approximation which has been used in recent algorithmic results. We\nthen show lower bounds for approximation by d-DNNF, complementing the positive\nresults from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 13:11:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["de Colnet", "Alexis", ""], ["Mengel", "Stefan", ""]]}, {"id": "2011.13729", "submitter": "Lei Han", "authors": "Lei Han, Jiechao Xiong, Peng Sun, Xinghai Sun, Meng Fang, Qingwei Guo,\n  Qiaobo Chen, Tengfei Shi, Hongsheng Yu, Xipeng Wu, Zhengyou Zhang", "title": "TStarBot-X: An Open-Sourced and Comprehensive Study for Efficient League\n  Training in StarCraft II Full Game", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  StarCraft, one of the most difficult esport games with long-standing history\nof professional tournaments, has attracted generations of players and fans, and\nalso, intense attentions in artificial intelligence research. Recently,\nGoogle's DeepMind announced AlphaStar, a grandmaster level AI in StarCraft II\nthat can play with humans using comparable action space and operations. In this\npaper, we introduce a new AI agent, named TStarBot-X, that is trained under\norders of less computations and can play competitively with expert human\nplayers. TStarBot-X takes advantage of important techniques introduced in\nAlphaStar, and also benefits from substantial innovations including new league\ntraining methods, novel multi-agent roles, rule-guided policy search,\nstabilized policy improvement, lightweight neural network architecture, and\nimportance sampling in imitation learning, etc. We show that with orders of\nless computation scale, a faithful reimplementation of AlphaStar's methods can\nnot succeed and the proposed techniques are necessary to ensure TStarBot-X's\ncompetitive performance. We reveal all technical details that are complementary\nto those mentioned in AlphaStar, showing the most sensitive parts in league\ntraining, reinforcement learning and imitation learning that affect the\nperformance of the agents. Most importantly, this is an open-sourced study that\nall codes and resources (including the trained model parameters) are publicly\naccessible via \\url{https://github.com/tencent-ailab/tleague_projpage}. We\nexpect this study could be beneficial for both academic and industrial future\nresearch in solving complex problems like StarCraft, and also, might provide a\nsparring partner for all StarCraft II players and other AI agents.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 13:31:49 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 08:31:32 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Han", "Lei", ""], ["Xiong", "Jiechao", ""], ["Sun", "Peng", ""], ["Sun", "Xinghai", ""], ["Fang", "Meng", ""], ["Guo", "Qingwei", ""], ["Chen", "Qiaobo", ""], ["Shi", "Tengfei", ""], ["Yu", "Hongsheng", ""], ["Wu", "Xipeng", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "2011.13741", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul, Puranjay Mohan, Stuti Sehgal", "title": "Rethinking Generalization in American Sign Language Prediction for Edge\n  Devices with Extremely Low Memory Footprint", "comments": "6 pages, Published in IEEE RAICS 2020, see https://raics.in", "journal-ref": "2020 IEEE Recent Advances in Intelligent Computational Systems\n  (RAICS), 2020, pp. 147-152", "doi": "10.1109/RAICS51191.2020.9332480", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the boom in technical compute in the last few years, the world has\nseen massive advances in artificially intelligent systems solving diverse\nreal-world problems. But a major roadblock in the ubiquitous acceptance of\nthese models is their enormous computational complexity and memory footprint.\nHence efficient architectures and training techniques are required for\ndeployment on extremely low resource inference endpoints. This paper proposes\nan architecture for detection of alphabets in American Sign Language on an ARM\nCortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging\nparameter quantization is a common technique that might cause varying drops in\ntest accuracy. This paper proposes using interpolation as augmentation amongst\nother techniques as an efficient method of reducing this drop, which also helps\nthe model generalize well to previously unseen noisy data. The proposed model\nis about 185 KB post-quantization and inference speed is 20 frames per second.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:05:42 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 10:24:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""], ["Mohan", "Puranjay", ""], ["Sehgal", "Stuti", ""]]}, {"id": "2011.13775", "submitter": "Denis Korzhenkov", "authors": "Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor\n  Lempitsky, Denis Korzhenkov", "title": "Image Generators with Conditionally-Independent Pixel Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing image generator networks rely heavily on spatial convolutions and,\noptionally, self-attention blocks in order to gradually synthesize images in a\ncoarse-to-fine manner. Here, we present a new architecture for image\ngenerators, where the color value at each pixel is computed independently given\nthe value of a random latent vector and the coordinate of that pixel. No\nspatial convolutions or similar operations that propagate information across\npixels are involved during the synthesis. We analyze the modeling capabilities\nof such generators when trained in an adversarial fashion, and observe the new\ngenerators to achieve similar generation quality to state-of-the-art\nconvolutional generators. We also investigate several interesting properties\nunique to the new architecture.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:16:11 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anokhin", "Ivan", ""], ["Demochkin", "Kirill", ""], ["Khakhulin", "Taras", ""], ["Sterkin", "Gleb", ""], ["Lempitsky", "Victor", ""], ["Korzhenkov", "Denis", ""]]}, {"id": "2011.13782", "submitter": "Michael Luo Zhiyu", "authors": "Rachit Dubey, Erin Grant, Michael Luo, Karthik Narasimhan, Thomas\n  Griffiths", "title": "Connecting Context-specific Adaptation in Humans to Meta-learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cognitive control, the ability of a system to adapt to the demands of a task,\nis an integral part of cognition. A widely accepted fact about cognitive\ncontrol is that it is context-sensitive: Adults and children alike infer\ninformation about a task's demands from contextual cues and use these\ninferences to learn from ambiguous cues. However, the precise way in which\npeople use contextual cues to guide adaptation to a new task remains poorly\nunderstood. This work connects the context-sensitive nature of cognitive\ncontrol to a method for meta-learning with context-conditioned adaptation. We\nbegin by identifying an essential difference between human learning and current\napproaches to meta-learning: In contrast to humans, existing meta-learning\nalgorithms do not make use of task-specific contextual cues but instead rely\nexclusively on online feedback in the form of task-specific labels or rewards.\nTo remedy this, we introduce a framework for using contextual information about\na task to guide the initialization of task-specific models before adaptation to\nonline feedback. We show how context-conditioned meta-learning can capture\nhuman behavior in a cognitive task and how it can be scaled to improve the\nspeed of learning in various settings, including few-shot classification and\nlow-sample reinforcement learning. Our work demonstrates that guiding\nmeta-learning with task information can capture complex, human-like behavior,\nthereby deepening our understanding of cognitive control.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:31:39 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 01:33:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dubey", "Rachit", ""], ["Grant", "Erin", ""], ["Luo", "Michael", ""], ["Narasimhan", "Karthik", ""], ["Griffiths", "Thomas", ""]]}, {"id": "2011.13824", "submitter": "Kaidi Xu", "authors": "Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin,\n  Cho-Jui Hsieh", "title": "Fast and Complete: Enabling Complete Neural Network Verification with\n  Rapid and Massively Parallel Incomplete Verifiers", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal verification of neural networks (NNs) is a challenging and important\nproblem. Existing efficient complete solvers typically require the\nbranch-and-bound (BaB) process, which splits the problem domain into\nsub-domains and solves each sub-domain using faster but weaker incomplete\nverifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In\nthis paper, we propose to use the backward mode linear relaxation based\nperturbation analysis (LiRPA) to replace LP during the BaB process, which can\nbe efficiently implemented on the typical machine learning accelerators such as\nGPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much\nweaker bounds and even cannot check certain conflicts of sub-domains during\nsplitting, making the entire procedure incomplete after BaB. To address these\nchallenges, we apply a fast gradient based bound tightening procedure combined\nwith batch splits and the design of minimal usage of LP bound procedure,\nenabling us to effectively use LiRPA on the accelerator hardware for the\nchallenging complete NN verification problem and significantly outperform\nLP-based approaches. On a single GPU, we demonstrate an order of magnitude\nspeedup compared to existing LP-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 16:42:12 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 16:35:00 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Xu", "Kaidi", ""], ["Zhang", "Huan", ""], ["Wang", "Shiqi", ""], ["Wang", "Yihan", ""], ["Jana", "Suman", ""], ["Lin", "Xue", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2011.13844", "submitter": "James Smith", "authors": "James E. Smith", "title": "A Temporal Neural Network Architecture for Online Learning", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing proposition is that by emulating the operation of the brain's\nneocortex, a spiking neural network (SNN) can achieve similar desirable\nfeatures: flexible learning, speed, and efficiency. Temporal neural networks\n(TNNs) are SNNs that communicate and process information encoded as relative\nspike times (in contrast to spike rates). A TNN architecture is proposed, and,\nas a proof-of-concept, TNN operation is demonstrated within the larger context\nof online supervised classification. First, through unsupervised learning, a\nTNN partitions input patterns into clusters based on similarity. The TNN then\npasses a cluster identifier to a simple online supervised decoder which\nfinishes the classification task. The TNN learning process adjusts synaptic\nweights by using only signals local to each synapse, and clustering behavior\nemerges globally. The system architecture is described at an abstraction level\nanalogous to the gate and register transfer levels in conventional digital\ndesign. Besides features of the overall architecture, several TNN components\nare new to this work. Although not addressed directly, the overall research\nobjective is a direct hardware implementation of TNNs. Consequently, all the\narchitecture elements are simple, and processing is done at very low precision.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:15:29 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 22:29:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Smith", "James E.", ""]]}, {"id": "2011.13851", "submitter": "Mahdi Rezaei", "authors": "Soheil Khatibi, Meisam Teimouri, Mahdi Rezaei", "title": "Real-time Active Vision for a Humanoid Soccer Robot Using Deep\n  Reinforcement Learning", "comments": "The paper has been accepted in ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present an active vision method using a deep reinforcement\nlearning approach for a humanoid soccer-playing robot. The proposed method\nadaptively optimises the viewpoint of the robot to acquire the most useful\nlandmarks for self-localisation while keeping the ball into its viewpoint.\nActive vision is critical for humanoid decision-maker robots with a limited\nfield of view. To deal with an active vision problem, several probabilistic\nentropy-based approaches have previously been proposed which are highly\ndependent on the accuracy of the self-localisation model. However, in this\nresearch, we formulate the problem as an episodic reinforcement learning\nproblem and employ a Deep Q-learning method to solve it. The proposed network\nonly requires the raw images of the camera to move the robot's head toward the\nbest viewpoint. The model shows a very competitive rate of 80% success rate in\nachieving the best viewpoint. We implemented the proposed method on a humanoid\nrobot simulated in Webots simulator. Our evaluations and experimental results\nshow that the proposed method outperforms the entropy-based methods in the\nRoboCup context, in cases with high self-localisation errors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:29:48 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Khatibi", "Soheil", ""], ["Teimouri", "Meisam", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2011.13880", "submitter": "Emilio Cartoni", "authors": "Emilio Cartoni (1), Davide Montella (1), Jochen Triesch (1), Gianluca\n  Baldassarre (1) ((1) Institute of Cognitive Sciences and Technologies, (2)\n  Frankfurt Institute for Advanced Studies)", "title": "An open-ended learning architecture to face the REAL 2020 simulated\n  robot competition", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended learning is a core research field of machine learning and robotics\naiming to build learning machines and robots able to autonomously acquire\nknowledge and skills and to reuse them to solve novel tasks. The multiple\nchallenges posed by open-ended learning have been operationalized in the\nrobotic competition REAL 2020. This requires a simulated camera-arm-gripper\nrobot to (a) autonomously learn to interact with objects during an intrinsic\nphase where it can learn how to move objects and then (b) during an extrinsic\nphase, to re-use the acquired knowledge to accomplish externally given goals\nrequiring the robot to move objects to specific locations unknown during the\nintrinsic phase. Here we present a 'baseline architecture' for solving the\nchallenge, provided as baseline model for REAL 2020. Few models have all the\nfunctionalities needed to solve the REAL 2020 benchmark and none has been\ntested with it yet. The architecture we propose is formed by three components:\n(1) Abstractor: abstracting sensory input to learn relevant control variables\nfrom images; (2) Explorer: generating experience to learn goals and actions;\n(3) Planner: formulating and executing action plans to accomplish the\nexternally provided goals. The architecture represents the first model to solve\nthe simpler REAL 2020 'Round 1' allowing the use of a simple parameterised push\naction. On Round 2, the architecture was used with a more general action\n(sequence of joints positions) achieving again higher than chance level\nperformance. The baseline software is well documented and available for\ndownload and use at https://github.com/AIcrowd/REAL2020_starter_kit.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:12:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Cartoni", "Emilio", ""], ["Montella", "Davide", ""], ["Triesch", "Jochen", ""], ["Baldassarre", "Gianluca", ""]]}, {"id": "2011.13885", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre,\n  Ziyu Wang, Yusuf Aytar, Misha Denil, Nando de Freitas, Scott Reed", "title": "Offline Learning from Demonstrations and Unlabeled Experience", "comments": "Accepted to Offline Reinforcement Learning Workshop at Neural\n  Information Processing Systems (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior cloning (BC) is often practical for robot learning because it allows\na policy to be trained offline without rewards, by supervised learning on\nexpert demonstrations. However, BC does not effectively leverage what we will\nrefer to as unlabeled experience: data of mixed and unknown quality without\nreward annotations. This unlabeled data can be generated by a variety of\nsources such as human teleoperation, scripted policies and other agents on the\nsame robot. Towards data-driven offline robot learning that can use this\nunlabeled experience, we introduce Offline Reinforced Imitation Learning\n(ORIL). ORIL first learns a reward function by contrasting observations from\ndemonstrator and unlabeled trajectories, then annotates all data with the\nlearned reward, and finally trains an agent via offline reinforcement learning.\nAcross a diverse set of continuous control and simulated robotic manipulation\ntasks, we show that ORIL consistently outperforms comparable BC agents by\neffectively leveraging unlabeled experience.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:20:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zolna", "Konrad", ""], ["Novikov", "Alexander", ""], ["Konyushkova", "Ksenia", ""], ["Gulcehre", "Caglar", ""], ["Wang", "Ziyu", ""], ["Aytar", "Yusuf", ""], ["Denil", "Misha", ""], ["de Freitas", "Nando", ""], ["Reed", "Scott", ""]]}, {"id": "2011.13897", "submitter": "Homanga Bharadhwaj", "authors": "Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, Florian\n  Shkurti", "title": "Latent Skill Planning for Exploration and Transfer", "comments": "First two authors contributed equally. Published as a conference\n  paper in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To quickly solve new tasks in complex environments, intelligent agents need\nto build up reusable knowledge. For example, a learned world model captures\nknowledge about the environment that applies to new tasks. Similarly, skills\ncapture general behaviors that can apply to new tasks. In this paper, we\ninvestigate how these two approaches can be integrated into a single\nreinforcement learning agent. Specifically, we leverage the idea of partial\namortization for fast adaptation at test time. For this, actions are produced\nby a policy that is learned over time while the skills it conditions on are\nchosen using online planning. We demonstrate the benefits of our design\ndecisions across a suite of challenging locomotion tasks and demonstrate\nimproved sample efficiency in single tasks as well as in transfer from one task\nto another, as compared to competitive baselines. Videos are available at:\nhttps://sites.google.com/view/latent-skill-planning/\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:40:03 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 15:53:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xie", "Kevin", ""], ["Bharadhwaj", "Homanga", ""], ["Hafner", "Danijar", ""], ["Garg", "Animesh", ""], ["Shkurti", "Florian", ""]]}, {"id": "2011.13977", "submitter": "Vijay  Menon", "authors": "Thomas Ma, Vijay Menon, Kate Larson", "title": "Improving Welfare in One-sided Matching using Simple Threshold Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study one-sided matching problems where $n$ agents have preferences over\n$m$ objects and each of them need to be assigned to at most one object. Most\nwork on such problems assume that the agents only have ordinal preferences and\nusually the goal in them is to compute a matching that satisfies some notion of\neconomic efficiency. However, in reality, agents may have some preference\nintensities or cardinal utilities that, e.g., indicate that they like an object\nmuch more than another object, and not taking these into account can result in\na loss in welfare. While one way to potentially account for these is to\ndirectly ask the agents for this information, such an elicitation process is\ncognitively demanding. Therefore, we focus on learning more about their\ncardinal preferences using simple threshold queries which ask an agent if they\nvalue an object greater than a certain value, and use this in turn to come up\nwith algorithms that produce a matching that, for a particular economic notion\n$X$, satisfies $X$ and also achieves a good approximation to the optimal\nwelfare among all matchings that satisfy $X$. We focus on several notions of\neconomic efficiency, and look at both adaptive and non-adaptive algorithms.\nOverall, our results show how one can improve welfare by even non-adaptively\nasking the agents for just one bit of extra information per object.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 20:02:57 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 14:36:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Thomas", ""], ["Menon", "Vijay", ""], ["Larson", "Kate", ""]]}, {"id": "2011.13978", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis and Eric Fosler-Lussier", "title": "Automated Coding of Under-Studied Medical Concept Domains: Linking\n  Physical Activity Reports to the International Classification of Functioning,\n  Disability, and Health", "comments": "Updated final version, published in Frontiers in Digital Health,\n  https://doi.org/10.3389/fdgth.2021.620828. 34 pages (23 text + 11\n  references); 9 figures, 2 tables", "journal-ref": "Frontiers in Digital Health, 3:620828 (2021)", "doi": "10.3389/fdgth.2021.620828", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking clinical narratives to standardized vocabularies and coding systems\nis a key component of unlocking the information in medical text for analysis.\nHowever, many domains of medical concepts lack well-developed terminologies\nthat can support effective coding of medical text. We present a framework for\ndeveloping natural language processing (NLP) technologies for automated coding\nof under-studied types of medical information, and demonstrate its\napplicability via a case study on physical mobility function. Mobility is a\ncomponent of many health measures, from post-acute care and surgical outcomes\nto chronic frailty and disability, and is coded in the International\nClassification of Functioning, Disability, and Health (ICF). However, mobility\nand other types of functional activity remain under-studied in medical\ninformatics, and neither the ICF nor commonly-used medical terminologies\ncapture functional status terminology in practice. We investigated two\ndata-driven paradigms, classification and candidate selection, to link\nnarrative observations of mobility to standardized ICF codes, using a dataset\nof clinical narratives from physical therapy encounters. Recent advances in\nlanguage modeling and word embedding were used as features for established\nmachine learning models and a novel deep learning approach, achieving a macro\nF-1 score of 84% on linking mobility activity reports to ICF codes. Both\nclassification and candidate selection approaches present distinct strengths\nfor automated coding in under-studied domains, and we highlight that the\ncombination of (i) a small annotated data set; (ii) expert definitions of codes\nof interest; and (iii) a representative text corpus is sufficient to produce\nhigh-performing automated coding systems. This study has implications for the\nongoing growth of NLP tools for a variety of specialized applications in\nclinical care and research.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 20:02:59 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 18:05:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "2011.13986", "submitter": "Johannes Schneider", "authors": "Johannes Schneider and Michalis Vlachos", "title": "Reflective-Net: Learning from Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans possess a remarkable capability to make fast, intuitive decisions, but\nalso to self-reflect, i.e., to explain to oneself, and to efficiently learn\nfrom explanations by others. This work provides the first steps toward\nmimicking this process by capitalizing on the explanations generated based on\nexisting explanation methods, i.e. Grad-CAM. Learning from explanations\ncombined with conventional labeled data yields significant improvements for\nclassification in terms of accuracy and training time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 20:40:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Schneider", "Johannes", ""], ["Vlachos", "Michalis", ""]]}, {"id": "2011.14015", "submitter": "Udai Nagpal", "authors": "Udai G. Nagpal, David A Knowles", "title": "Active Learning in CNNs via Expected Improvement Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models such as Convolutional Neural Networks (CNNs) have\ndemonstrated high levels of effectiveness in a variety of domains, including\ncomputer vision and more recently, computational biology. However, training\neffective models often requires assembling and/or labeling large datasets,\nwhich may be prohibitively time-consuming or costly. Pool-based active learning\ntechniques have the potential to mitigate these issues, leveraging models\ntrained on limited data to selectively query unlabeled data points from a pool\nin an attempt to expedite the learning process. Here we present \"Dropout-based\nExpected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient\napproach to active learning that queries points that are expected to maximize\nthe model's improvement across a representative sample of points. The proposed\nframework enables us to maintain a prediction covariance matrix capturing model\nuncertainty, and to dynamically update this matrix in order to generate diverse\nbatches of points in the batch-mode setting. Our active learning results\ndemonstrate that DEIMOS outperforms several existing baselines across multiple\nregression and classification tasks taken from computer vision and genomics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 22:06:52 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nagpal", "Udai G.", ""], ["Knowles", "David A", ""]]}, {"id": "2011.14016", "submitter": "Alan Lindsay", "authors": "Alan Lindsay, Bart Craenen, Sara Dalzel-Job, Robin L. Hill, Ronald P.\n  A. Petrick", "title": "Investigating Human Response, Behaviour, and Preference in Joint-Task\n  Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human interaction relies on a wide range of signals, including non-verbal\ncues. In order to develop effective Explainable Planning (XAIP) agents it is\nimportant that we understand the range and utility of these communication\nchannels. Our starting point is existing results from joint task interaction\nand their study in cognitive science. Our intention is that these lessons can\ninform the design of interaction agents -- including those using planning\ntechniques -- whose behaviour is conditioned on the user's response, including\naffective measures of the user (i.e., explicitly incorporating the user's\naffective state within the planning model). We have identified several concepts\nat the intersection of plan-based agent behaviour and joint task interaction\nand have used these to design two agents: one reactive and the other partially\npredictive. We have designed an experiment in order to examine human behaviour\nand response as they interact with these agents. In this paper we present the\ndesigned study and the key questions that are being investigated. We also\npresent the results from an empirical analysis where we examined the behaviour\nof the two agents for simulated users.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 22:16:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lindsay", "Alan", ""], ["Craenen", "Bart", ""], ["Dalzel-Job", "Sara", ""], ["Hill", "Robin L.", ""], ["Petrick", "Ronald P. A.", ""]]}, {"id": "2011.14027", "submitter": "Jack Lanchantin", "authors": "Jack Lanchantin, Tianlu Wang, Vicente Ordonez, Yanjun Qi", "title": "General Multi-label Image Classification with Transformers", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-label image classification is the task of predicting a set of labels\ncorresponding to objects, attributes or other entities present in an image. In\nthis work we propose the Classification Transformer (C-Tran), a general\nframework for multi-label image classification that leverages Transformers to\nexploit the complex dependencies among visual features and labels. Our approach\nconsists of a Transformer encoder trained to predict a set of target labels\ngiven an input set of masked labels, and visual features from a convolutional\nneural network. A key ingredient of our method is a label mask training\nobjective that uses a ternary encoding scheme to represent the state of the\nlabels as positive, negative, or unknown during training. Our model shows\nstate-of-the-art performance on challenging datasets such as COCO and Visual\nGenome. Moreover, because our model explicitly represents the uncertainty of\nlabels during training, it is more general by allowing us to produce improved\nresults for images with partial or extra label annotations during inference. We\ndemonstrate this additional capability in the COCO, Visual Genome, News500, and\nCUB image datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 23:20:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lanchantin", "Jack", ""], ["Wang", "Tianlu", ""], ["Ordonez", "Vicente", ""], ["Qi", "Yanjun", ""]]}, {"id": "2011.14058", "submitter": "Wei He", "authors": "Zhongzhan Huang, Senwei Liang, Mingfu Liang, Wei He, Haizhao Yang", "title": "Efficient Attention Network: Accelerate Attention by Searching Where to\n  Plug", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 03:31:08 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 12:44:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Huang", "Zhongzhan", ""], ["Liang", "Senwei", ""], ["Liang", "Mingfu", ""], ["He", "Wei", ""], ["Yang", "Haizhao", ""]]}, {"id": "2011.14084", "submitter": "Ke Shen", "authors": "Ke Shen and Mayank Kejriwal", "title": "A Data-Driven Study of Commonsense Knowledge using the ConceptNet\n  Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acquiring commonsense knowledge and reasoning is recognized as an important\nfrontier in achieving general Artificial Intelligence (AI). Recent research in\nthe Natural Language Processing (NLP) community has demonstrated significant\nprogress in this problem setting. Despite this progress, which is mainly on\nmultiple-choice question answering tasks in limited settings, there is still a\nlack of understanding (especially at scale) of the nature of commonsense\nknowledge itself. In this paper, we propose and conduct a systematic study to\nenable a deeper understanding of commonsense knowledge by doing an empirical\nand structural analysis of the ConceptNet knowledge base. ConceptNet is a\nfreely available knowledge base containing millions of commonsense assertions\npresented in natural language. Detailed experimental results on three carefully\ndesigned research questions, using state-of-the-art unsupervised graph\nrepresentation learning ('embedding') and clustering techniques, reveal deep\nsubstructures in ConceptNet relations, allowing us to make data-driven and\ncomputational claims about the meaning of phenomena such as 'context' that are\ntraditionally discussed only in qualitative terms. Furthermore, our methodology\nprovides a case study in how to use data-science and computational\nmethodologies for understanding the nature of an everyday (yet complex)\npsychological phenomenon that is an essential feature of human intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 08:08:25 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 07:21:20 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Shen", "Ke", ""], ["Kejriwal", "Mayank", ""]]}, {"id": "2011.14097", "submitter": "Shohreh Deldari", "authors": "Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim", "title": "Time Series Change Point Detection with Self-Supervised Contrastive\n  Predictive Coding", "comments": "Accepted at The WEB Conference 2021 (WWW'21)", "journal-ref": null, "doi": "10.1145/3442381.3449903", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change Point Detection (CPD) methods identify the times associated with\nchanges in the trends and properties of time series data in order to describe\nthe underlying behaviour of the system. For instance, detecting the changes and\nanomalies associated with web service usage, application usage or human\nbehaviour can provide valuable insights for downstream modelling tasks. We\npropose a novel approach for self-supervised Time Series Change Point detection\nmethod based onContrastivePredictive coding (TS-CP^2). TS-CP^2 is the first\napproach to employ a contrastive learning strategy for CPD by learning an\nembedded representation that separates pairs of embeddings of time adjacent\nintervals from pairs of interval embeddings separated across time. Through\nextensive experiments on three diverse, widely used time series datasets, we\ndemonstrate that our method outperforms five state-of-the-art CPD methods,\nwhich include unsupervised and semi-supervisedapproaches. TS-CP^2 is shown to\nimprove the performance of methods that use either handcrafted statistical or\ntemporal features by 79.4% and deep learning-based methods by 17.0% with\nrespect to the F1-score averaged across the three datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 09:36:18 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 23:21:33 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 22:26:19 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 09:20:41 GMT"}, {"version": "v5", "created": "Fri, 5 Mar 2021 00:24:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Deldari", "Shohreh", ""], ["Smith", "Daniel V.", ""], ["Xue", "Hao", ""], ["Salim", "Flora D.", ""]]}, {"id": "2011.14124", "submitter": "Edward Lockhart", "authors": "Edward Lockhart, Neil Burch, Nolan Bard, Sebastian Borgeaud, Tom\n  Eccles, Lucas Smaira, Ray Smith", "title": "Human-Agent Cooperation in Bridge Bidding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a human-compatible reinforcement-learning approach to a\ncooperative game, making use of a third-party hand-coded human-compatible bot\nto generate initial training data and to perform initial evaluation. Our\nlearning approach consists of imitation learning, search, and policy iteration.\nOur trained agents achieve a new state-of-the-art for bridge bidding in three\nsettings: an agent playing in partnership with a copy of itself; an agent\npartnering a pre-existing bot; and an agent partnering a human player.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 12:37:02 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lockhart", "Edward", ""], ["Burch", "Neil", ""], ["Bard", "Nolan", ""], ["Borgeaud", "Sebastian", ""], ["Eccles", "Tom", ""], ["Smaira", "Lucas", ""], ["Smith", "Ray", ""]]}, {"id": "2011.14193", "submitter": "Wen-Hua Chen Prof", "authors": "Wen-Hua Chen", "title": "Model Predictive Control with and without Terminal Weight: Stability and\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents stability analysis tools for model predictive control\n(MPC) with and without terminal weight. Stability analysis of MPC with a\nlimited horizon but without terminal weight is a long-standing open problem. By\nusing a modified value function as an Lyapunov function candidate and the\nprinciple of optimality, this paper establishes stability conditions for this\ntype of widely spread MPC algorithms. A new stability guaranteed MPC algorithm\nwithout terminal weight (MPCS) is presented. With the help of designing a new\nsublevel set defined by the value function of one-step ahead stage cost,\nconditions for checking its recursive feasibility and stability of the proposed\nMPC algorithm are presented. The new stability condition and the derived MPCS\novercome the difficulties arising in the existing terminal weight based MPC\nframework, including the need of searching a suitable terminal weight and\npossible poor performance caused by an inappropriate terminal weight. This work\nis further extended to MPC with a terminal weight for the completeness.\nNumerical examples are presented to demonstrate the effectiveness of the\nproposed tool, whereas the existing stability analysis tools are either not\napplicable or lead to quite conservative results. It shows that the proposed\ntools offer a number of mechanisms to achieve stability: adjusting state and/or\ncontrol weights, extending the length of horizon, and adding a simple extra\nconstraint on the first or second state in the optimisation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:48:31 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 15:12:05 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Wen-Hua", ""]]}, {"id": "2011.14197", "submitter": "Jun Zhao", "authors": "Helin Yang, Jun Zhao, Zehui Xiong, Kwok-Yan Lam, Sumei Sun, Liang Xiao", "title": "Privacy-Preserving Federated Learning for UAV-Enabled Networks:\n  Learning-Based Joint Scheduling and Resource Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned aerial vehicles (UAVs) are capable of serving as flying base\nstations (BSs) for supporting data collection, artificial intelligence (AI)\nmodel training, and wireless communications. However, due to the privacy\nconcerns of devices and limited computation or communication resource of UAVs,\nit is impractical to send raw data of devices to UAV servers for model\ntraining. Moreover, due to the dynamic channel condition and heterogeneous\ncomputing capacity of devices in UAV-enabled networks, the reliability and\nefficiency of data sharing require to be further improved. In this paper, we\ndevelop an asynchronous federated learning (AFL) framework for\nmulti-UAV-enabled networks, which can provide asynchronous distributed\ncomputing by enabling model training locally without transmitting raw sensitive\ndata to UAV servers. The device selection strategy is also introduced into the\nAFL framework to keep the low-quality devices from affecting the learning\nefficiency and accuracy. Moreover, we propose an asynchronous advantage\nactor-critic (A3C) based joint device selection, UAVs placement, and resource\nmanagement algorithm to enhance the federated convergence speed and accuracy.\nSimulation results demonstrate that our proposed framework and algorithm\nachieve higher learning accuracy and faster federated execution time compared\nto other existing solutions.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:58:34 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yang", "Helin", ""], ["Zhao", "Jun", ""], ["Xiong", "Zehui", ""], ["Lam", "Kwok-Yan", ""], ["Sun", "Sumei", ""], ["Xiao", "Liang", ""]]}, {"id": "2011.14200", "submitter": "Sandesh Ramesh", "authors": "Sandesh Ramesh, Manoj Kumar M V, and Sanjay H A", "title": "E-Pro: Euler Angle and Probabilistic Model for Face Detection and\n  Recognition", "comments": "4th International Conference on Inventive Systems and Control\n  (ICISC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is human nature to give prime importance to facial appearances. Often, to\nlook good is to feel good. Also, facial features are unique to every individual\non this planet, which means it is a source of vital information. This work\nproposes a framework named E-Pro for the detection and recognition of faces by\ntaking facial images as inputs. E-Pro has its potential application in various\ndomains, namely attendance, surveillance, crowd monitoring, biometric-based\nauthentication etc. E-Pro is developed here as a mobile application that aims\nto aid lecturers to mark attendance in a classroom by detecting and recognizing\nthe faces of students from a picture clicked through the app. E-Pro has been\ndeveloped using Google Firebase Face Recognition APIs, which uses Euler Angles,\nand Probabilistic Model. E-Pro has been tested on stock images and the\nexperimental results are promising.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 19:12:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ramesh", "Sandesh", ""], ["M", "Manoj Kumar", "V"], ["A", "Sanjay H", ""]]}, {"id": "2011.14214", "submitter": "Anadi Chaman", "authors": "Anadi Chaman (1), Ivan Dokmani\\'c (2) ((1) University of Illinois at\n  Urbana-Champaign, (2) University of Basel)", "title": "Truly shift-invariant convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the use of convolution and pooling layers, convolutional neural\nnetworks were for a long time thought to be shift-invariant. However, recent\nworks have shown that the output of a CNN can change significantly with small\nshifts in input: a problem caused by the presence of downsampling (stride)\nlayers. The existing solutions rely either on data augmentation or on\nanti-aliasing, both of which have limitations and neither of which enables\nperfect shift invariance. Additionally, the gains obtained from these methods\ndo not extend to image patterns not seen during training. To address these\nchallenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling\nscheme that allows convolutional neural networks to achieve 100% consistency in\nclassification performance under shifts, without any loss in accuracy. With\nAPS, the networks exhibit perfect consistency to shifts even before training,\nmaking it the first approach that makes convolutional neural networks truly\nshift-invariant.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 20:57:35 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:46:12 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 12:18:15 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 19:47:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chaman", "Anadi", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "2011.14244", "submitter": "Yao Fu", "authors": "Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, Alexander M.\n  Rush", "title": "Latent Template Induction with Gumbel-CRFs", "comments": "NeurIPS 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to control the structure of sentences is a challenging problem in\ntext generation. Existing work either relies on simple deterministic approaches\nor RL-based hard structures. We explore the use of structured variational\nautoencoders to infer latent templates for sentence generation using a soft,\ncontinuous relaxation in order to utilize reparameterization for training.\nSpecifically, we propose a Gumbel-CRF, a continuous relaxation of the CRF\nsampling algorithm using a relaxed Forward-Filtering Backward-Sampling (FFBS)\napproach. As a reparameterized gradient estimator, the Gumbel-CRF gives more\nstable gradients than score-function based estimators. As a structured\ninference network, we show that it learns interpretable templates during\ntraining, which allows us to control the decoder during testing. We demonstrate\nthe effectiveness of our methods with experiments on data-to-text generation\nand unsupervised paraphrase generation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 01:00:57 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Fu", "Yao", ""], ["Tan", "Chuanqi", ""], ["Bi", "Bin", ""], ["Chen", "Mosha", ""], ["Feng", "Yansong", ""], ["Rush", "Alexander M.", ""]]}, {"id": "2011.14266", "submitter": "Samuel Daulton", "authors": "Hongseok Namkoong, Samuel Daulton, Eytan Bakshy", "title": "Distilled Thompson Sampling: Practical and Efficient Thompson Sampling\n  via Imitation Learning", "comments": null, "journal-ref": "Offline Reinforcement Learning Workshop at Neural Information\n  Processing Systems, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling (TS) has emerged as a robust technique for contextual\nbandit problems. However, TS requires posterior inference and optimization for\naction generation, prohibiting its use in many internet applications where\nlatency and ease of deployment are of concern. We propose a novel\nimitation-learning-based algorithm that distills a TS policy into an explicit\npolicy representation by performing posterior inference and optimization\noffline. The explicit policy representation enables fast online decision-making\nand easy deployment in mobile and server-based environments. Our algorithm\niteratively performs offline batch updates to the TS policy and learns a new\nimitation policy. Since we update the TS policy with observations collected\nunder the imitation policy, our algorithm emulates an off-policy version of TS.\nOur imitation algorithm guarantees Bayes regret comparable to TS, up to the sum\nof single-step imitation errors. We show these imitation errors can be made\narbitrarily small when unlabeled contexts are cheaply available, which is the\ncase for most large-scale internet applications. Empirically, we show that our\nimitation policy achieves comparable regret to TS, while reducing decision-time\nlatency by over an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 03:57:42 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:08:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Namkoong", "Hongseok", ""], ["Daulton", "Samuel", ""], ["Bakshy", "Eytan", ""]]}, {"id": "2011.14276", "submitter": "Hector Javier Hortua", "authors": "Hector J. Hortua, Riccardo Volpi, Dimitri Marinelli, Luigi Malago", "title": "Accelerating MCMC algorithms through Bayesian Deep Networks", "comments": "Accepted in the Third Workshop on Machine Learning and the Physical\n  Sciences, NeurIPS 2020, Vancouver, Canada. Text overlap with\n  arXiv:1911.08508v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms are commonly used for their\nversatility in sampling from complicated probability distributions. However, as\nthe dimension of the distribution gets larger, the computational costs for a\nsatisfactory exploration of the sampling space become challenging. Adaptive\nMCMC methods employing a choice of proposal distribution can address this issue\nspeeding up the convergence. In this paper we show an alternative way of\nperforming adaptive MCMC, by using the outcome of Bayesian Neural Networks as\nthe initial proposal for the Markov Chain. This combined approach increases the\nacceptance rate in the Metropolis-Hasting algorithm and accelerate the\nconvergence of the MCMC while reaching the same final accuracy. Finally, we\ndemonstrate the main advantages of this approach by constraining the\ncosmological parameters directly from Cosmic Microwave Background maps.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 04:29:00 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hortua", "Hector J.", ""], ["Volpi", "Riccardo", ""], ["Marinelli", "Dimitri", ""], ["Malago", "Luigi", ""]]}, {"id": "2011.14330", "submitter": "Yanping Chen", "authors": "Yanping Chen, Lefei Wu, Liyuan Deng, Yongbin Qing, Ruizhang Huang,\n  Qinghua Zheng, Ping Chen", "title": "A Boundary Regression Model for Nested Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing named entities (NEs) is commonly conducted as a classification\nproblem that predicts a class tag for an NE candidate in a sentence. In shallow\nstructures, categorized features are weighted to support the prediction. Recent\ndevelopments in neural networks have adopted deep structures that map\ncategorized features into continuous representations. This approach unfolds a\ndense space saturated with high-order abstract semantic information, where the\nprediction is based on distributed feature representations. In this paper, the\nregression operation is introduced to locate NEs in a sentence. In this\napproach, a deep network is first designed to transform an input sentence into\nrecurrent feature maps. Bounding boxes are generated from the feature maps,\nwhere a box is an abstract representation of an NE candidate. In addition to\nthe class tag, each bounding box has two parameters denoting the start position\nand the length of an NE candidate. In the training process, the location offset\nbetween a bounding box and a true NE are learned to minimize the location loss.\nBased on this motivation, a multiobjective learning framework is designed to\nsimultaneously locate entities and predict the class probability. By sharing\nparameters for locating and predicting, the framework can take full advantage\nof annotated data and enable more potent nonlinear function approximators to\nenhance model discriminability. Experiments demonstrate state-of-the-art\nperformance for nested named entities\\footnote{Our codes will be available at:\n\\url{https://github.com/wuyuefei3/BR}}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 10:04:38 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 22:09:22 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chen", "Yanping", ""], ["Wu", "Lefei", ""], ["Deng", "Liyuan", ""], ["Qing", "Yongbin", ""], ["Huang", "Ruizhang", ""], ["Zheng", "Qinghua", ""], ["Chen", "Ping", ""]]}, {"id": "2011.14340", "submitter": "Dawid Rymarczyk", "authors": "Dawid Rymarczyk, {\\L}ukasz Struski, Jacek Tabor, Bartosz Zieli\\'nski", "title": "ProtoPShare: Prototype Sharing for Interpretable Image Classification\n  and Similarity Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce ProtoPShare, a self-explained method that\nincorporates the paradigm of prototypical parts to explain its predictions. The\nmain novelty of the ProtoPShare is its ability to efficiently share\nprototypical parts between the classes thanks to our data-dependent\nmerge-pruning. Moreover, the prototypes are more consistent and the model is\nmore robust to image perturbations than the state of the art method ProtoPNet.\nWe verify our findings on two datasets, the CUB-200-2011 and the Stanford Cars.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 11:23:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rymarczyk", "Dawid", ""], ["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""], ["Zieli\u0144ski", "Bartosz", ""]]}, {"id": "2011.14430", "submitter": "Tanvir Ahamed", "authors": "Tanvir Ahamed, Bo Zou, Nahid Parvez Farazi and Theja Tulabandhula", "title": "Deep Reinforcement Learning for Crowdsourced Urban Delivery: System\n  States Characterization, Heuristics-guided Action Choice, and\n  Rule-Interposing Integration", "comments": "50 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of assigning shipping requests to ad hoc\ncouriers in the context of crowdsourced urban delivery. The shipping requests\nare spatially distributed each with a limited time window between the earliest\ntime for pickup and latest time for delivery. The ad hoc couriers, termed\ncrowdsourcees, also have limited time availability and carrying capacity. We\npropose a new deep reinforcement learning (DRL)-based approach to tackling this\nassignment problem. A deep Q network (DQN) algorithm is trained which entails\ntwo salient features of experience replay and target network that enhance the\nefficiency, convergence, and stability of DRL training. More importantly, this\npaper makes three methodological contributions: 1) presenting a comprehensive\nand novel characterization of crowdshipping system states that encompasses\nspatial-temporal and capacity information of crowdsourcees and requests; 2)\nembedding heuristics that leverage the information offered by the state\nrepresentation and are based on intuitive reasoning to guide specific actions\nto take, to preserve tractability and enhance efficiency of training; and 3)\nintegrating rule-interposing to prevent repeated visiting of the same routes\nand node sequences during routing improvement, thereby further enhancing the\ntraining efficiency by accelerating learning. The effectiveness of the proposed\napproach is demonstrated through extensive numerical analysis. The results show\nthe benefits brought by the heuristics-guided action choice and\nrule-interposing in DRL training, and the superiority of the proposed approach\nover existing heuristics in both solution quality, time, and scalability.\nBesides the potential to improve the efficiency of crowdshipping operation\nplanning, the proposed approach also provides a new avenue and generic\nframework for other problems in the vehicle routing context.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 19:50:34 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ahamed", "Tanvir", ""], ["Zou", "Bo", ""], ["Farazi", "Nahid Parvez", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "2011.14473", "submitter": "Gabriel Gusm\\~ao S.", "authors": "Gabriel S. Gusm\\~ao, Adhika P. Retnanto, Shashwati C. da Cunha, Andrew\n  J. Medford", "title": "Kinetics-Informed Neural Networks", "comments": "Pre-print for first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical kinetics consists of the phenomenological framework for the\ndisentanglement of reaction mechanisms, optimization of reaction performance\nand the rational design of chemical processes. Here, we utilize feed-forward\nartificial neural networks as basis functions for the construction of surrogate\nmodels to solve ordinary differential equations (ODEs) that describe\nmicrokinetic models (MKMs). We present an algebraic framework for the\nmathematical description and classification of reaction networks, types of\nelementary reaction, and chemical species. Under this framework, we demonstrate\nthat the simultaneous training of neural nets and kinetic model parameters in a\nregularized multiobjective optimization setting leads to the solution of the\ninverse problem through the estimation of kinetic parameters from synthetic\nexperimental data. We probe the limits at which kinetic parameters can be\nretrieved as a function of knowledge about the chemical system states over\ntime, and assess the robustness of the methodology with respect to statistical\nnoise. This surrogate approach to inverse kinetic ODEs can assist in the\nelucidation of reaction mechanisms based on transient data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:07:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gusm\u00e3o", "Gabriel S.", ""], ["Retnanto", "Adhika P.", ""], ["da Cunha", "Shashwati C.", ""], ["Medford", "Andrew J.", ""]]}, {"id": "2011.14475", "submitter": "Eduardo C\\'esar Garrido-Merch\\'an", "authors": "Eduardo C. Garrido-Merch\\'an and Martin Molina and Francisco M.\n  Mendoza", "title": "An Artificial Consciousness Model and its relations with Philosophy of\n  Mind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work seeks to study the beneficial properties that an autonomous agent\ncan obtain by implementing a cognitive architecture similar to the one of\nconscious beings. Along this document, a conscious model of autonomous agent\nbased in a global workspace architecture is presented. We describe how this\nagent is viewed from different perspectives of philosophy of mind, being\ninspired by their ideas. The goal of this model is to create autonomous agents\nable to navigate within an environment composed of multiple independent\nmagnitudes, adapting to its surroundings in order to find the best possible\nposition in base of its inner preferences. The purpose of the model is to test\nthe effectiveness of many cognitive mechanisms that are incorporated, such as\nan attention mechanism for magnitude selection, pos-session of inner feelings\nand preferences, usage of a memory system to storage beliefs and past\nexperiences, and incorporating a global workspace which controls and integrates\ninformation processed by all the subsystem of the model. We show in a large\nexperiment set how an autonomous agent can benefit from having a cognitive\narchitecture such as the one described.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:24:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:27:10 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Garrido-Merch\u00e1n", "Eduardo C.", ""], ["Molina", "Martin", ""], ["Mendoza", "Francisco M.", ""]]}, {"id": "2011.14479", "submitter": "Huaxiong Li", "authors": "Haoxing Chen and Huaxiong Li and Yaohui Li and Chunlin Chen", "title": "Multi-scale Adaptive Task Attention Network for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of few-shot learning is to classify unseen categories with few\nlabeled samples. Recently, the low-level information metric-learning based\nmethods have achieved satisfying performance, since local representations (LRs)\nare more consistent between seen and unseen classes. However, most of these\nmethods deal with each category in the support set independently, which is not\nsufficient to measure the relation between features, especially in a certain\ntask. Moreover, the low-level information-based metric learning method suffers\nwhen dominant objects of different scales exist in a complex background. To\naddress these issues, this paper proposes a novel Multi-scale Adaptive Task\nAttention Network (MATANet) for few-shot learning. Specifically, we first use a\nmulti-scale feature generator to generate multiple features at different\nscales. Then, an adaptive task attention module is proposed to select the most\nimportant LRs among the entire task. Afterwards, a similarity-to-class module\nand a fusion layer are utilized to calculate a joint multi-scale similarity\nbetween the query image and the support set. Extensive experiments on popular\nbenchmarks clearly show the effectiveness of the proposed MATANet compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:36:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chen", "Haoxing", ""], ["Li", "Huaxiong", ""], ["Li", "Yaohui", ""], ["Chen", "Chunlin", ""]]}, {"id": "2011.14486", "submitter": "Benoit Steiner", "authors": "Benoit Steiner and Chris Cummins and Horace He and Hugh Leather", "title": "Value Function Based Performance Optimization of Deep Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning techniques become ubiquitous, the efficiency of neural\nnetwork implementations is becoming correspondingly paramount. Frameworks, such\nas Halide and TVM, separate out the algorithmic representation of the network\nfrom the schedule that determines its implementation. Finding good schedules,\nhowever, remains extremely challenging. We model this scheduling problem as a\nsequence of optimization choices, and present a new technique to accurately\npredict the expected performance of a partial schedule. By leveraging these\npredictions we can make these optimization decisions greedily and rapidly\nidentify an efficient schedule. This enables us to find schedules that improve\nthe throughput of deep neural networks by 2.6x over Halide and 1.5x over TVM.\nMoreover, our technique is two to three orders of magnitude faster than that of\nthese tools, and completes in seconds instead of hours.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:20:14 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Steiner", "Benoit", ""], ["Cummins", "Chris", ""], ["He", "Horace", ""], ["Leather", "Hugh", ""]]}, {"id": "2011.14487", "submitter": "Junfan Lin", "authors": "Junfan Lin, Zhongzhan Huang, Keze Wang, Xiaodan Liang, Weiwei Chen,\n  and Liang Lin", "title": "Continuous Transition: Improving Sample Efficiency for Continuous\n  Control Problems via MixUp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep reinforcement learning (RL) has been successfully applied to a\nvariety of robotic control tasks, it's still challenging to apply it to\nreal-world tasks, due to the poor sample efficiency. Attempting to overcome\nthis shortcoming, several works focus on reusing the collected trajectory data\nduring the training by decomposing them into a set of policy-irrelevant\ndiscrete transitions. However, their improvements are somewhat marginal since\ni) the amount of the transitions is usually small, and ii) the value assignment\nonly happens in the joint states. To address these issues, this paper\nintroduces a concise yet powerful method to construct Continuous Transition,\nwhich exploits the trajectory information by exploiting the potential\ntransitions along the trajectory. Specifically, we propose to synthesize new\ntransitions for training by linearly interpolating the consecutive transitions.\nTo keep the constructed transitions authentic, we also develop a discriminator\nto guide the construction process automatically. Extensive experiments\ndemonstrate that our proposed method achieves a significant improvement in\nsample efficiency on various complex continuous robotic control problems in\nMuJoCo and outperforms the advanced model-based / model-free RL methods. The\nsource code is available.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:20:23 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 04:59:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lin", "Junfan", ""], ["Huang", "Zhongzhan", ""], ["Wang", "Keze", ""], ["Liang", "Xiaodan", ""], ["Chen", "Weiwei", ""], ["Lin", "Liang", ""]]}, {"id": "2011.14495", "submitter": "Marek Petrik", "authors": "Elita A. Lobo, Mohammad Ghavamzadeh, Marek Petrik", "title": "Soft-Robust Algorithms for Batch Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, robust policies for high-stakes decision-making\nproblems with limited data are usually computed by optimizing the percentile\ncriterion, which minimizes the probability of a catastrophic failure.\nUnfortunately, such policies are typically overly conservative as the\npercentile criterion is non-convex, difficult to optimize, and ignores the mean\nperformance. To overcome these shortcomings, we study the soft-robust\ncriterion, which uses risk measures to balance the mean and percentile\ncriterion better. In this paper, we establish the soft-robust criterion's\nfundamental properties, show that it is NP-hard to optimize, and propose and\nanalyze two algorithms to approximately optimize it. Our theoretical analyses\nand empirical evaluations demonstrate that our algorithms compute much less\nconservative solutions than the existing approximate methods for optimizing the\npercentile-criterion.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:36:16 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 17:46:32 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Lobo", "Elita A.", ""], ["Ghavamzadeh", "Mohammad", ""], ["Petrik", "Marek", ""]]}, {"id": "2011.14496", "submitter": "Yikai Wang", "authors": "Yikai Wang and Weijian Li", "title": "Blind signal decomposition of various word embeddings based on join and\n  individual variance explained", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, natural language processing (NLP) has become one of the most\nimportant areas with various applications in human's life. As the most\nfundamental task, the field of word embedding still requires more attention and\nresearch. Currently, existing works about word embedding are focusing on\nproposing novel embedding algorithms and dimension reduction techniques on\nwell-trained word embeddings. In this paper, we propose to use a novel joint\nsignal separation method - JIVE to jointly decompose various trained word\nembeddings into joint and individual components. Through this decomposition\nframework, we can easily investigate the similarity and difference among\ndifferent word embeddings. We conducted extensive empirical study on word2vec,\nFastText and GLoVE trained on different corpus and with different dimensions.\nWe compared the performance of different decomposed components based on\nsentiment analysis on Twitter and Stanford sentiment treebank. We found that by\nmapping different word embeddings into the joint component, sentiment\nperformance can be greatly improved for the original word embeddings with lower\nperformance. Moreover, we found that by concatenating different components\ntogether, the same model can achieve better performance. These findings provide\ngreat insights into the word embeddings and our work offer a new of generating\nword embeddings by fusing.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:36:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Yikai", ""], ["Li", "Weijian", ""]]}, {"id": "2011.14504", "submitter": "Jiayi Yang", "authors": "Jiayi Yang, Lei Deng, Yukuan Yang, Yuan Xie, Guoqi Li", "title": "Training and Inference for Integer-Based Semantic Segmentation Network", "comments": "17 page, 12 figures, submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation has been a major topic in research and industry in\nrecent years. However, due to the computation complexity of pixel-wise\nprediction and backpropagation algorithm, semantic segmentation has been\ndemanding in computation resources, resulting in slow training and inference\nspeed and large storage space to store models. Existing schemes that speed up\nsegmentation network change the network structure and come with noticeable\naccuracy degradation. However, neural network quantization can be used to\nreduce computation load while maintaining comparable accuracy and original\nnetwork structure. Semantic segmentation networks are different from\ntraditional deep convolutional neural networks (DCNNs) in many ways, and this\ntopic has not been thoroughly explored in existing works. In this paper, we\npropose a new quantization framework for training and inference of segmentation\nnetworks, where parameters and operations are constrained to 8-bit\ninteger-based values for the first time. Full quantization of the data flow and\nthe removal of square and root operations in batch normalization give our\nframework the ability to perform inference on fixed-point devices. Our proposed\nframework is evaluated on mainstream semantic segmentation networks like\nFCN-VGG16 and DeepLabv3-ResNet50, achieving comparable accuracy against\nfloating-point framework on ADE20K dataset and PASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 02:07:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yang", "Jiayi", ""], ["Deng", "Lei", ""], ["Yang", "Yukuan", ""], ["Xie", "Yuan", ""], ["Li", "Guoqi", ""]]}, {"id": "2011.14551", "submitter": "Jay Shenoy", "authors": "Jay Shenoy, Edward Kim, Xiangyu Yue, Taesung Park, Daniel Fremont,\n  Alberto Sangiovanni-Vincentelli, Sanjit Seshia", "title": "A Customizable Dynamic Scenario Modeling and Data Generation Platform\n  for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Safely interacting with humans is a significant challenge for autonomous\ndriving. The performance of this interaction depends on machine learning-based\nmodules of an autopilot, such as perception, behavior prediction, and planning.\nThese modules require training datasets with high-quality labels and a diverse\nrange of realistic dynamic behaviors. Consequently, training such modules to\nhandle rare scenarios is difficult because they are, by definition, rarely\nrepresented in real-world datasets. Hence, there is a practical need to augment\ndatasets with synthetic data covering these rare scenarios. In this paper, we\npresent a platform to model dynamic and interactive scenarios, generate the\nscenarios in simulation with different modalities of labeled sensor data, and\ncollect this information for data augmentation. To our knowledge, this is the\nfirst integrated platform for these tasks specialized to the autonomous driving\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 05:11:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Shenoy", "Jay", ""], ["Kim", "Edward", ""], ["Yue", "Xiangyu", ""], ["Park", "Taesung", ""], ["Fremont", "Daniel", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Seshia", "Sanjit", ""]]}, {"id": "2011.14554", "submitter": "Jeong-Hoe Ku", "authors": "Jeong-Hoe Ku, JiHun Oh, YoungYoon Lee, Gaurav Pooniwala, SangJeong Lee", "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for\n  Neural Network Models", "comments": "15 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to provide a selective survey about knowledge\ndistillation(KD) framework for researchers and practitioners to take advantage\nof it for developing new optimized models in the deep neural network field. To\nthis end, we give a brief overview of knowledge distillation and some related\nworks including learning using privileged information(LUPI) and generalized\ndistillation(GD). Even though knowledge distillation based on the\nteacher-student architecture was initially devised as a model compression\ntechnique, it has found versatile applications over various frameworks.\n  In this paper, we review the characteristics of knowledge distillation from\nthe hypothesis that the three important ingredients of knowledge distillation\nare distilled knowledge and loss,teacher-student paradigm, and the distillation\nprocess. In addition, we survey the versatility of the knowledge distillation\nby studying its direct applications and its usage in combination with other\ndeep learning paradigms. Finally we present some future works in knowledge\ndistillation including explainable knowledge distillation where the analytical\nanalysis of the performance gain is studied and the self-supervised learning\nwhich is a hot research topic in deep learning community.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 05:22:02 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ku", "Jeong-Hoe", ""], ["Oh", "JiHun", ""], ["Lee", "YoungYoon", ""], ["Pooniwala", "Gaurav", ""], ["Lee", "SangJeong", ""]]}, {"id": "2011.14584", "submitter": "Hsin-Pai Cheng", "authors": "Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng, Feng Yan, Hai Li,\n  Vikas Chandra, Yiran Chen", "title": "ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale variance among different sizes of body parts and objects is a\nchallenging problem for visual recognition tasks. Existing works usually design\ndedicated backbone or apply Neural architecture Search(NAS) for each task to\ntackle this challenge. However, existing works impose significant limitations\non the design or search space. To solve these problems, we present ScaleNAS, a\none-shot learning method for exploring scale-aware representations. ScaleNAS\nsolves multiple tasks at a time by searching multi-scale feature aggregation.\nScaleNAS adopts a flexible search space that allows an arbitrary number of\nblocks and cross-scale feature fusions. To cope with the high search cost\nincurred by the flexible space, ScaleNAS employs one-shot learning for\nmulti-scale supernet driven by grouped sampling and evolutionary search.\nWithout further retraining, ScaleNet can be directly deployed for different\nvisual recognition tasks with superior performance. We use ScaleNAS to create\nhigh-resolution models for two different tasks, ScaleNet-P for human pose\nestimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S\noutperform existing manually crafted and NAS-based methods in both tasks. When\napplying ScaleNet-P to bottom-up human pose estimation, it surpasses the\nstate-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on\nCOCO test-dev, achieving new state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:11:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cheng", "Hsin-Pai", ""], ["Liang", "Feng", ""], ["Li", "Meng", ""], ["Cheng", "Bowen", ""], ["Yan", "Feng", ""], ["Li", "Hai", ""], ["Chandra", "Vikas", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.14593", "submitter": "Christina Baek", "authors": "Ziyang Wu, Christina Baek, Chong You, Yi Ma", "title": "Incremental Learning via Rate Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning architectures suffer from catastrophic forgetting, a\nfailure to retain knowledge of previously learned classes when incrementally\ntrained on new classes. The fundamental roadblock faced by deep learning\nmethods is that deep learning models are optimized as \"black boxes,\" making it\ndifficult to properly adjust the model parameters to preserve knowledge about\npreviously seen data. To overcome the problem of catastrophic forgetting, we\npropose utilizing an alternative \"white box\" architecture derived from the\nprinciple of rate reduction, where each layer of the network is explicitly\ncomputed without back propagation. Under this paradigm, we demonstrate that,\ngiven a pre-trained network and new data classes, our approach can provably\nconstruct a new network that emulates joint training with all past and new\nclasses. Finally, our experiments show that our proposed learning algorithm\nobserves significantly less decay in classification performance, outperforming\nstate of the art methods on MNIST and CIFAR-10 by a large margin and justifying\nthe use of \"white box\" algorithms for incremental learning even for\nsufficiently complex image data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:23:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wu", "Ziyang", ""], ["Baek", "Christina", ""], ["You", "Chong", ""], ["Ma", "Yi", ""]]}, {"id": "2011.14620", "submitter": "Przemys{\\l}aw Spurek", "authors": "Maciej Zi\\k{e}ba, Marcin Przewi\\k{e}\\'zlikowski, Marek \\'Smieja, Jacek\n  Tabor, Tomasz Trzcinski, Przemys{\\l}aw Spurek", "title": "RegFlow: Probabilistic Flow-based Regression for Future Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future states or actions of a given system remains a fundamental,\nyet unsolved challenge of intelligence, especially in the scope of complex and\nnon-deterministic scenarios, such as modeling behavior of humans. Existing\napproaches provide results under strong assumptions concerning unimodality of\nfuture states, or, at best, assuming specific probability distributions that\noften poorly fit to real-life conditions. In this work we introduce a robust\nand flexible probabilistic framework that allows to model future predictions\nwith virtually no constrains regarding the modality or underlying probability\ndistribution. To achieve this goal, we leverage a hypernetwork architecture and\ntrain a continuous normalizing flow model. The resulting method dubbed RegFlow\nachieves state-of-the-art results on several benchmark datasets, outperforming\ncompeting approaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:45:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zi\u0119ba", "Maciej", ""], ["Przewi\u0119\u017alikowski", "Marcin", ""], ["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""], ["Trzcinski", "Tomasz", ""], ["Spurek", "Przemys\u0142aw", ""]]}, {"id": "2011.14632", "submitter": "Ilya Trofimov", "authors": "N. Mazyavkina, S. Moustafa, I. Trofimov, E. Burnaev", "title": "Optimizing the Neural Architecture of Reinforcement Learning Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) enjoyed significant progress over the last years.\nOne of the most important steps forward was the wide application of neural\nnetworks. However, architectures of these neural networks are typically\nconstructed manually. In this work, we study recently proposed neural\narchitecture search (NAS) methods for optimizing the architecture of RL agents.\nWe carry out experiments on the Atari benchmark and conclude that modern NAS\nmethods find architectures of RL agents outperforming a manually selected one.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:18:05 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 13:25:20 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 10:21:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Mazyavkina", "N.", ""], ["Moustafa", "S.", ""], ["Trofimov", "I.", ""], ["Burnaev", "E.", ""]]}, {"id": "2011.14684", "submitter": "Francesco Salvetti", "authors": "Simone Angarano, Vittorio Mazzia, Francesco Salvetti, Giovanni Fantin\n  and Marcello Chiaberge", "title": "Robust Ultra-wideband Range Error Mitigation with Deep Learning at the\n  Edge", "comments": "Submitted to Engineering Applications of Artificial Intelligence", "journal-ref": "Engineering Applications of Artificial Intelligence, Volume 102,\n  June 2021, 104278", "doi": "10.1016/j.engappai.2021.104278", "report-no": null, "categories": "cs.LG cs.AI cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ultra-wideband (UWB) is the state-of-the-art and most popular technology for\nwireless localization. Nevertheless, precise ranging and localization in\nnon-line-of-sight (NLoS) conditions is still an open research topic. Indeed,\nmultipath effects, reflections, refractions, and complexity of the indoor radio\nenvironment can easily introduce a positive bias in the ranging measurement,\nresulting in highly inaccurate and unsatisfactory position estimation. This\narticle proposes an efficient representation learning methodology that exploits\nthe latest advancement in deep learning and graph optimization techniques to\nachieve effective ranging error mitigation at the edge. Channel Impulse\nResponse (CIR) signals are directly exploited to extract high semantic features\nto estimate corrections in either NLoS or LoS conditions. Extensive\nexperimentation with different settings and configurations has proved the\neffectiveness of our methodology and demonstrated the feasibility of a robust\nand low computational power UWB range error mitigation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:52:21 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:16:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Angarano", "Simone", ""], ["Mazzia", "Vittorio", ""], ["Salvetti", "Francesco", ""], ["Fantin", "Giovanni", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2011.14808", "submitter": "Xiangzhong Luo Mr.", "authors": "Di Liu, Hao Kong, Xiangzhong Luo, Weichen Liu, Ravi Subramaniam", "title": "Bringing AI To Edge: From Deep Learning's Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Edge computing and artificial intelligence (AI), especially deep learning for\nnowadays, are gradually intersecting to build a novel system, called edge\nintelligence. However, the development of edge intelligence systems encounters\nsome challenges, and one of these challenges is the \\textit{computational gap}\nbetween computation-intensive deep learning algorithms and less-capable edge\nsystems. Due to the computational gap, many edge intelligence systems cannot\nmeet the expected performance requirements. To bridge the gap, a plethora of\ndeep learning techniques and optimization methods are proposed in the past\nyears: light-weight deep learning models, network compression, and efficient\nneural architecture search. Although some reviews or surveys have partially\ncovered this large body of literature, we lack a systematic and comprehensive\nreview to discuss all aspects of these deep learning techniques which are\ncritical for edge intelligence implementation. As various and diverse methods\nwhich are applicable to edge systems are proposed intensively, a holistic\nreview would enable edge computing engineers and community to know the\nstate-of-the-art deep learning techniques which are instrumental for edge\nintelligence and to facilitate the development of edge intelligence systems.\nThis paper surveys the representative and latest deep learning techniques that\nare useful for edge intelligence systems, including hand-crafted models, model\ncompression, hardware-aware neural architecture search and adaptive deep\nlearning models. Finally, based on observations and simple experiments we\nconducted, we discuss some future directions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:07:21 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Di", ""], ["Kong", "Hao", ""], ["Luo", "Xiangzhong", ""], ["Liu", "Weichen", ""], ["Subramaniam", "Ravi", ""]]}, {"id": "2011.14818", "submitter": "Chandra Thapa", "authors": "Chandra Thapa and M.A.P. Chamikara and Seyit A. Camtepe", "title": "Advancements of federated learning towards privacy preservation: from\n  federated learning to split learning", "comments": "Authors' preprint version (before any peer-review) of a book chapter\n  to appear in the Book series \"Studies in Computational Intelligence\", Book\n  title \"Federated Learning Systems: Towards Next-generation AI\", Book eds.\n  Muhammad Habib ur Rehman and Mohamed Medhat Gaber, Publisher \"Springer Nature\n  Switzerland AG Gewerbestrasse 11, 6330 Cham, Switzerland.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the distributed collaborative machine learning (DCML) paradigm, federated\nlearning (FL) recently attracted much attention due to its applications in\nhealth, finance, and the latest innovations such as industry 4.0 and smart\nvehicles. FL provides privacy-by-design. It trains a machine learning model\ncollaboratively over several distributed clients (ranging from two to millions)\nsuch as mobile phones, without sharing their raw data with any other\nparticipant. In practical scenarios, all clients do not have sufficient\ncomputing resources (e.g., Internet of Things), the machine learning model has\nmillions of parameters, and its privacy between the server and the clients\nwhile training/testing is a prime concern (e.g., rival parties). In this\nregard, FL is not sufficient, so split learning (SL) is introduced. SL is\nreliable in these scenarios as it splits a model into multiple portions,\ndistributes them among clients and server, and trains/tests their respective\nmodel portions to accomplish the full model training/testing. In SL, the\nparticipants do not share both data and their model portions to any other\nparties, and usually, a smaller network portion is assigned to the clients\nwhere data resides. Recently, a hybrid of FL and SL, called splitfed learning,\nis introduced to elevate the benefits of both FL (faster training/testing time)\nand SL (model split and training). Following the developments from FL to SL,\nand considering the importance of SL, this chapter is designed to provide\nextensive coverage in SL and its variants. The coverage includes fundamentals,\nexisting findings, integration with privacy measures such as differential\nprivacy, open problems, and code implementation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 05:01:33 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Thapa", "Chandra", ""], ["Chamikara", "M. A. P.", ""], ["Camtepe", "Seyit A.", ""]]}, {"id": "2011.14824", "submitter": "Wenyu Zhao", "authors": "Wenyu Zhao, Teli Ma, Xuan Gong, Baochang Zhang, and David Doermann", "title": "A Review of Recent Advances of Binary Neural Networks for Edge Computing", "comments": null, "journal-ref": null, "doi": "10.1109/JMASS.2020.3034205", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is promising to become one of the next hottest topics in\nartificial intelligence because it benefits various evolving domains such as\nreal-time unmanned aerial systems, industrial applications, and the demand for\nprivacy protection. This paper reviews recent advances on binary neural network\n(BNN) and 1-bit CNN technologies that are well suitable for front-end,\nedge-based computing. We introduce and summarize existing work and classify\nthem based on gradient approximation, quantization, architecture, loss\nfunctions, optimization method, and binary neural architecture search. We also\nintroduce applications in the areas of computer vision and speech recognition\nand discuss future applications for edge computing.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 01:10:21 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhao", "Wenyu", ""], ["Ma", "Teli", ""], ["Gong", "Xuan", ""], ["Zhang", "Baochang", ""], ["Doermann", "David", ""]]}, {"id": "2011.14826", "submitter": "Pablo Samuel Castro", "authors": "Johan S. Obando-Ceron and Pablo Samuel Castro", "title": "Revisiting Rainbow: Promoting more Insightful and Inclusive Deep\n  Reinforcement Learning Research", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of DQN, a vast majority of reinforcement learning\nresearch has focused on reinforcement learning with deep neural networks as\nfunction approximators. New methods are typically evaluated on a set of\nenvironments that have now become standard, such as Atari 2600 games. While\nthese benchmarks help standardize evaluation, their computational cost has the\nunfortunate side effect of widening the gap between those with ample access to\ncomputational resources, and those without. In this work we argue that, despite\nthe community's emphasis on large-scale environments, the traditional\nsmall-scale environments can still yield valuable scientific insights and can\nhelp reduce the barriers to entry for underprivileged communities. To\nsubstantiate our claims, we empirically revisit the paper which introduced the\nRainbow algorithm [Hessel et al., 2018] and present some new insights into the\nalgorithms used by Rainbow.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:23:40 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 19:53:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Obando-Ceron", "Johan S.", ""], ["Castro", "Pablo Samuel", ""]]}, {"id": "2011.14843", "submitter": "Tatiana Makhalova", "authors": "Tatiana Makhalova, Sergei O. Kuznetsov, Amedeo Napoli", "title": "Mint: MDL-based approach for Mining INTeresting Numerical Pattern Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern mining is well established in data mining research, especially for\nmining binary datasets. Surprisingly, there is much less work about numerical\npattern mining and this research area remains under-explored. In this paper, we\npropose Mint, an efficient MDL-based algorithm for mining numerical datasets.\nThe MDL principle is a robust and reliable framework widely used in pattern\nmining, and as well in subgroup discovery. In Mint we reuse MDL for discovering\nuseful patterns and returning a set of non-redundant overlapping patterns with\nwell-defined boundaries and covering meaningful groups of objects. Mint is not\nalone in the category of numerical pattern miners based on MDL. In the\nexperiments presented in the paper we show that Mint outperforms competitors\namong which Slim and RealKrimp.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:36:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Makhalova", "Tatiana", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""]]}, {"id": "2011.14859", "submitter": "Derek Lim", "authors": "Derek Lim, Ren\\'e Vidal, Benjamin D. Haeffele", "title": "Doubly Stochastic Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art subspace clustering methods follow a two-step process\nby first constructing an affinity matrix between data points and then applying\nspectral clustering to this affinity. Most of the research into these methods\nfocuses on the first step of generating the affinity, which often exploits the\nself-expressive property of linear subspaces, with little consideration\ntypically given to the spectral clustering step that produces the final\nclustering. Moreover, existing methods often obtain the final affinity that is\nused in the spectral clustering step by applying ad-hoc or arbitrarily chosen\npostprocessing steps to the affinity generated by a self-expressive clustering\nformulation, which can have a significant impact on the overall clustering\nperformance. In this work, we unify these two steps by learning both a\nself-expressive representation of the data and an affinity matrix that is\nwell-normalized for spectral clustering. In our proposed models, we constrain\nthe affinity matrix to be doubly stochastic, which results in a principled\nmethod for affinity matrix normalization while also exploiting known benefits\nof doubly stochastic normalization in spectral clustering. We develop a general\nframework and derive two models: one that jointly learns the self-expressive\nrepresentation along with the doubly stochastic affinity, and one that\nsequentially solves for one then the other. Furthermore, we leverage sparsity\nin the problem to develop a fast active-set method for the sequential solver\nthat enables efficient computation on large datasets. Experiments show that our\nmethod achieves state-of-the-art subspace clustering performance on many common\ndatasets in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:56:54 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 23:50:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lim", "Derek", ""], ["Vidal", "Ren\u00e9", ""], ["Haeffele", "Benjamin D.", ""]]}, {"id": "2011.14870", "submitter": "Luis Felipe M\\\"uller de Oliveira Henriques", "authors": "Luis Felipe M.O. Henriques, Eduardo Morgan, Sergio Colcher, Ruy Luiz\n  Milidi\\'u", "title": "Prior Flow Variational Autoencoder: A density estimation model for\n  Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:05:59 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 22:51:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Henriques", "Luis Felipe M. O.", ""], ["Morgan", "Eduardo", ""], ["Colcher", "Sergio", ""], ["Milidi\u00fa", "Ruy Luiz", ""]]}, {"id": "2011.14890", "submitter": "Niko Grupen", "authors": "Niko A. Grupen, Daniel D. Lee, Bart Selman", "title": "Low-Bandwidth Communication Emerges Naturally in Multi-Agent Learning\n  Systems", "comments": "10 pages, 6 figures, Appearing in Talking to Strangers: Zero-Shot\n  Emergent Communication Workshop NeurIPS 2020. Fixed part (a) of Figure 2 to\n  include correct baseline reported in quantitative results section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study emergent communication through the lens of cooperative\nmulti-agent behavior in nature. Using insights from animal communication, we\npropose a spectrum from low-bandwidth (e.g. pheromone trails) to high-bandwidth\n(e.g. compositional language) communication that is based on the cognitive,\nperceptual, and behavioral capabilities of social agents. Through a series of\nexperiments with pursuit-evasion games, we identify multi-agent reinforcement\nlearning algorithms as a computational model for the low-bandwidth end of the\ncommunication spectrum.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:29:57 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 20:21:14 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Grupen", "Niko A.", ""], ["Lee", "Daniel D.", ""], ["Selman", "Bart", ""]]}, {"id": "2011.14917", "submitter": "Muhammad Umer", "authors": "Muhammad Umer, Robi Polikar", "title": "Comparative Analysis of Extreme Verification Latency Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the more challenging real-world problems in computational intelligence\nis to learn from non-stationary streaming data, also known as concept drift.\nPerhaps even a more challenging version of this scenario is when -- following a\nsmall set of initial labeled data -- the data stream consists of unlabeled data\nonly. Such a scenario is typically referred to as learning in initially labeled\nnonstationary environment, or simply as extreme verification latency (EVL).\nBecause of the very challenging nature of the problem, very few algorithms have\nbeen proposed in the literature up to date. This work is a very first effort to\nprovide a review of some of the existing algorithms (important/prominent) in\nthis field to the research community. More specifically, this paper is a\ncomprehensive survey and comparative analysis of some of the EVL algorithms to\npoint out the weaknesses and strengths of different approaches from three\ndifferent perspectives: classification accuracy, computational complexity and\nparameter sensitivity using several synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:34:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Umer", "Muhammad", ""], ["Polikar", "Robi", ""]]}, {"id": "2011.14922", "submitter": "Hanwen Miao", "authors": "Hanwen Miao, Shengan Zhang, Carol Flannagan", "title": "Driver Behavior Extraction from Videos in Naturalistic Driving Datasets\n  with 3D ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Naturalistic driving data (NDD) is an important source of information to\nunderstand crash causation and human factors and to further develop crash\navoidance countermeasures. Videos recorded while driving are often included in\nsuch datasets. While there is often a large amount of video data in NDD, only a\nsmall portion of them can be annotated by human coders and used for research,\nwhich underuses all video data. In this paper, we explored a computer vision\nmethod to automatically extract the information we need from videos. More\nspecifically, we developed a 3D ConvNet algorithm to automatically extract\ncell-phone-related behaviors from videos. The experiments show that our method\ncan extract chunks from videos, most of which (~79%) contain the automatically\nlabeled cell phone behaviors. In conjunction with human review of the extracted\nchunks, this approach can find cell-phone-related driver behaviors much more\nefficiently than simply viewing video.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:53:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Miao", "Hanwen", ""], ["Zhang", "Shengan", ""], ["Flannagan", "Carol", ""]]}, {"id": "2011.14925", "submitter": "Minji Yoon", "authors": "Minji Yoon, Th\\'eophile Gervet, Bryan Hooi, and Christos Faloutsos", "title": "Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy\n  Trade-off", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph data is ubiquitous in academia and industry, from social networks to\nbioinformatics. The pervasiveness of graphs today has raised the demand for\nalgorithms that can answer various questions: Which products would a user like\nto purchase given her order list? Which users are buying fake followers to\nincrease their public reputation? Myriads of new graph mining algorithms are\nproposed every year to answer such questions - each with a distinct problem\nformulation, computational time, and memory footprint. This lack of unity makes\nit difficult for a practitioner to compare different algorithms and pick the\nmost suitable one for a specific application. These challenges - even more\nsevere for non-experts - create a gap in which state-of-the-art techniques\ndeveloped in academic settings fail to be optimally deployed in real-world\napplications. To bridge this gap, we propose AUTOGM, an automated system for\ngraph mining algorithm development. We first define a unified framework\nUNIFIEDGM that integrates various message-passing based graph algorithms,\nranging from conventional algorithms like PageRank to graph neural networks.\nThen UNIFIEDGM defines a search space in which five parameters are required to\ndetermine a graph algorithm. Under this search space, AUTOGM explicitly\noptimizes for the optimal parameter set of UNIFIEDGM using Bayesian\nOptimization. AUTOGM defines a novel budget-aware objective function for the\noptimization to incorporate a practical issue - finding the best speed-accuracy\ntrade-off under a computation budget - into the graph algorithm generation\nproblem. Experiments on real-world benchmark datasets demonstrate that AUTOGM\ngenerates novel graph mining algorithms with the best speed/accuracy trade-off\ncompared to existing models with heuristic parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 02:37:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yoon", "Minji", ""], ["Gervet", "Th\u00e9ophile", ""], ["Hooi", "Bryan", ""], ["Faloutsos", "Christos", ""]]}, {"id": "2011.14934", "submitter": "Sahil Suneja", "authors": "Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro\n  Morari", "title": "Probing Model Signal-Awareness via Prediction-Preserving Input\n  Minimization", "comments": "Authors Sahil Suneja, Yunhui Zheng, and Yufan Zhuang contributed\n  equally to this research. FSE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the signal awareness of AI models for source code\nunderstanding. Using a software vulnerability detection use case, we evaluate\nthe models' ability to capture the correct vulnerability signals to produce\ntheir predictions. Our prediction-preserving input minimization (P2IM) approach\nsystematically reduces the original source code to a minimal snippet which a\nmodel needs to maintain its prediction. The model's reliance on incorrect\nsignals is then uncovered when the vulnerability in the original code is\nmissing in the minimal snippet, both of which the model however predicts as\nbeing vulnerable. We measure the signal awareness of models using a new metric\nwe propose- Signal-aware Recall (SAR). We apply P2IM on three different neural\nnetwork architectures across multiple datasets. The results show a sharp drop\nin the model's Recall from the high 90s to sub-60s with the new metric,\nhighlighting that the models are presumably picking up a lot of noise or\ndataset nuances while learning their vulnerability detection logic. Although\nthe drop in model performance may be perceived as an adversarial attack, but\nthis isn't P2IM's objective. The idea is rather to uncover the signal-awareness\nof a black-box model in a data-driven manner via controlled queries. SAR's\npurpose is to measure the impact of task-agnostic model training, and not to\nsuggest a shortcoming in the Recall metric. The expectation, in fact, is for\nSAR to match Recall in the ideal scenario where the model truly captures\ntask-specific signals.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:05:23 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 21:44:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Suneja", "Sahil", ""], ["Zheng", "Yunhui", ""], ["Zhuang", "Yufan", ""], ["Laredo", "Jim", ""], ["Morari", "Alessandro", ""]]}, {"id": "2011.14956", "submitter": "Yongquan Yang", "authors": "Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng", "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make corrections corresponding to\npotentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with heavy noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the heavy noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present a\none-step abductive multi-target learning framework (OSAMTLF) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to abduct the predictions of the learning model to be subject to our\nprior knowledge. For the problem 2), we propose a logical assessment formula\n(LAF) that evaluates the logical rationality of the outputs of an approach by\nestimating the consistencies between the predictions of the learning model and\nthe logical facts narrated from the results of the one-step logical reasoning\nof OSAMTLF. Applying OSAMTLF and LAF to the Helicobacter pylori (H. pylori)\nsegmentation task in MHWSIA, we show that OSAMTLF is able to abduct the machine\nlearning model achieving logically more rational predictions, which is beyond\nthe capability of various state-of-the-art approaches for learning from noisy\nlabels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:40:34 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yang", "Yongquan", ""], ["Yang", "Yiming", ""], ["Chen", "Jie", ""], ["Zheng", "Jiayi", ""], ["Zheng", "Zhongxi", ""]]}, {"id": "2011.15007", "submitter": "Brady Neal", "authors": "Brady Neal, Chin-Wei Huang, Sunand Raghupathi", "title": "RealCause: Realistic Causal Inference Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many different causal effect estimators in causal inference.\nHowever, it is unclear how to choose between these estimators because there is\nno ground-truth for causal effects. A commonly used option is to simulate\nsynthetic data, where the ground-truth is known. However, the best causal\nestimators on synthetic data are unlikely to be the best causal estimators on\nreal data. An ideal benchmark for causal estimators would both (a) yield\nground-truth values of the causal effects and (b) be representative of real\ndata. Using flexible generative models, we provide a benchmark that both yields\nground-truth and is realistic. Using this benchmark, we evaluate over 1500\ndifferent causal estimators and provide evidence that it is rational to choose\nhyperparameters for causal estimators using predictive metrics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:12:18 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:14:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Neal", "Brady", ""], ["Huang", "Chin-Wei", ""], ["Raghupathi", "Sunand", ""]]}, {"id": "2011.15038", "submitter": "Rafi Trad", "authors": "Rafi Trad, Myra Spiliopoulou", "title": "A Framework for Authorial Clustering of Shorter Texts in Latent Semantic\n  Spaces", "comments": "8 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Authorial clustering involves the grouping of documents written by the same\nauthor or team of authors without any prior positive examples of an author's\nwriting style or thematic preferences. For authorial clustering on shorter\ntexts (paragraph-length texts that are typically shorter than conventional\ndocuments), the document representation is particularly important: very\nhigh-dimensional feature spaces lead to data sparsity and suffer from serious\nconsequences like the curse of dimensionality, while feature selection may lead\nto information loss. We propose a high-level framework which utilizes a compact\ndata representation in a latent feature space derived with non-parametric topic\nmodeling. Authorial clusters are identified thereafter in two scenarios: (a)\nfully unsupervised and (b) semi-supervised where a small number of shorter\ntexts are known to belong to the same author (must-link constraints) or not\n(cannot-link constraints). We report on experiments with 120 collections in\nthree languages and two genres and show that the topic-based latent feature\nspace provides a promising level of performance while reducing the\ndimensionality by a factor of 1500 compared to state-of-the-arts. We also\ndemonstrate that, while prior knowledge on the precise number of authors (i.e.\nauthorial clusters) does not contribute much to additional quality, little\nknowledge on constraints in authorial clusters memberships leads to clear\nperformance improvements in front of this difficult task. Thorough\nexperimentation with standard metrics indicates that there still remains an\nample room for improvement for authorial clustering, especially with shorter\ntexts\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:39:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Trad", "Rafi", ""], ["Spiliopoulou", "Myra", ""]]}, {"id": "2011.15067", "submitter": "Marlene Berke", "authors": "Marlene Berke, Mario Belledonne, and Julian Jara-Ettinger", "title": "Learning a metacognition for object perception", "comments": "SVRHM workshop at NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beyond representing the external world, humans also represent their own\ncognitive processes. In the context of perception, this metacognition helps us\nidentify unreliable percepts, such as when we recognize that we are seeing an\nillusion. Here we propose MetaGen, a model for the unsupervised learning of\nmetacognition. In MetaGen, metacognition is expressed as a generative model of\nhow a perceptual system produces noisy percepts. Using basic principles of how\nthe world works (such as object permanence, part of infants' core knowledge),\nMetaGen jointly infers the objects in the world causing the percepts and a\nrepresentation of its own perceptual system. MetaGen can then use this\nmetacognition to infer which objects are actually present in the world. On\nsimulated data, we find that MetaGen quickly learns a metacognition and\nimproves overall accuracy, outperforming models that lack a metacognition.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:05:00 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Berke", "Marlene", ""], ["Belledonne", "Mario", ""], ["Jara-Ettinger", "Julian", ""]]}, {"id": "2011.15091", "submitter": "Anirudh Goyal", "authors": "Anirudh Goyal, Yoshua Bengio", "title": "Inductive Biases for Deep Learning of Higher-Level Cognition", "comments": "This document contains a review of authors research as part of the\n  requirement of AG's predoctoral exam, an overview of the main contributions\n  of the authors few recent papers (co-authored with several other co-authors)\n  as well as a vision of proposed future research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fascinating hypothesis is that human and animal intelligence could be\nexplained by a few principles (rather than an encyclopedic list of heuristics).\nIf that hypothesis was correct, we could more easily both understand our own\nintelligence and build intelligent machines. Just like in physics, the\nprinciples themselves would not be sufficient to predict the behavior of\ncomplex systems like brains, and substantial computation might be needed to\nsimulate human-like intelligence. This hypothesis would suggest that studying\nthe kind of inductive biases that humans and animals exploit could help both\nclarify these principles and provide inspiration for AI research and\nneuroscience theories. Deep learning already exploits several key inductive\nbiases, and this work considers a larger list, focusing on those which concern\nmostly higher-level and sequential conscious processing. The objective of\nclarifying these particular principles is that they could potentially help us\nbuild AI systems benefiting from humans' abilities in terms of flexible\nout-of-distribution and systematic generalization, which is currently an area\nwhere a large gap exists between state-of-the-art machine learning and human\nintelligence.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:29:25 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 17:51:00 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 21:54:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Goyal", "Anirudh", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2011.15102", "submitter": "Pengtao Xie", "authors": "Xuefeng Du, Haochen Zhang, Pengtao Xie", "title": "Learning by Passing Tests, with Application to Neural Architecture\n  Search", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.04863,\n  arXiv:2012.12502, arXiv:2012.12899", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning through tests is a broadly used methodology in human learning and\nshows great effectiveness in improving learning outcome: a sequence of tests\nare made with increasing levels of difficulty; the learner takes these tests to\nidentify his/her weak points in learning and continuously addresses these weak\npoints to successfully pass these tests. We are interested in investigating\nwhether this powerful learning technique can be borrowed from humans to improve\nthe learning abilities of machines. We propose a novel learning approach called\nlearning by passing tests (LPT). In our approach, a tester model creates\nincreasingly more-difficult tests to evaluate a learner model. The learner\ntries to continuously improve its learning ability so that it can successfully\npass however difficult tests created by the tester. We propose a multi-level\noptimization framework to formulate LPT, where the tester learns to create\ndifficult and meaningful tests and the learner learns to pass these tests. We\ndevelop an efficient algorithm to solve the LPT problem. Our method is applied\nfor neural architecture search and achieves significant improvement over\nstate-of-the-art baselines on CIFAR-100, CIFAR-10, and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:33:34 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 03:43:01 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Du", "Xuefeng", ""], ["Zhang", "Haochen", ""], ["Xie", "Pengtao", ""]]}]