[{"id": "1412.0315", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck and Mathias Niepert", "title": "Lifted Probabilistic Inference for Asymmetric Graphical Models", "comments": "To appear in Proceedings of AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifted probabilistic inference algorithms have been successfully applied to a\nlarge number of symmetric graphical models. Unfortunately, the majority of\nreal-world graphical models is asymmetric. This is even the case for relational\nrepresentations when evidence is given. Therefore, more recent work in the\ncommunity moved to making the models symmetric and then applying existing\nlifted inference algorithms. However, this approach has two shortcomings.\nFirst, all existing over-symmetric approximations require a relational\nrepresentation such as Markov logic networks. Second, the induced symmetries\noften change the distribution significantly, making the computed probabilities\nhighly biased. We present a framework for probabilistic sampling-based\ninference that only uses the induced approximate symmetries to propose steps in\na Metropolis-Hastings style Markov chain. The framework, therefore, leads to\nimproved probability estimates while remaining unbiased. Experiments\ndemonstrate that the approach outperforms existing MCMC algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 00:40:33 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Niepert", "Mathias", ""]]}, {"id": "1412.0320", "submitter": "Yuping Shen", "authors": "Yuping Shen and Xishun Zhao", "title": "Canonical Logic Programs are Succinctly Incomparable with Propositional\n  Formulas", "comments": "This is an extended version of a conference paper with the same name\n  in KR2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Canonical (logic) programs} (CP) refer to normal logic programs\naugmented with connective $not\\ not$. In this paper we address the question of\nwhether CP are \\emph{succinctly incomparable} with \\emph{propositional\nformulas} (PF). Our main result shows that the PARITY problem, which can be\npolynomially represented in PF but \\emph{only} has exponential representations\nin CP. In other words, PARITY \\emph{separates} PF from CP. Simply speaking,\nthis means that exponential size blowup is generally inevitable when\ntranslating a set of formulas in PF into an equivalent program in CP (without\nintroducing new variables). Furthermore, since it has been shown by Lifschitz\nand Razborov that there is also a problem that separates CP from PF (assuming\n$\\mathsf{P}\\nsubseteq \\mathsf{NC^1/poly}$), it follows that CP and PF are\nindeed succinctly incomparable. From the view of the theory of computation, the\nabove result may also be considered as the separation of two \\emph{models of\ncomputation}, i.e., we identify a language in $\\mathsf{NC^1/poly}$ which is not\nin the set of languages computable by polynomial size CP programs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 01:10:30 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 11:47:04 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Shen", "Yuping", ""], ["Zhao", "Xishun", ""]]}, {"id": "1412.0439", "submitter": "Chee Seng Chan", "authors": "Chern Hong Lim, Ekta Vats and Chee Seng Chan", "title": "Fuzzy human motion analysis: A review", "comments": "Accepted in Pattern Recognition, first survey paper that discusses\n  and reviews fuzzy approaches towards HMA", "journal-ref": "Pattern Recognition 48(5) 2015 1773-1796", "doi": "10.1016/j.patcog.2014.11.016", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Motion Analysis (HMA) is currently one of the most popularly active\nresearch domains as such significant research interests are motivated by a\nnumber of real world applications such as video surveillance, sports analysis,\nhealthcare monitoring and so on. However, most of these real world applications\nface high levels of uncertainties that can affect the operations of such\napplications. Hence, the fuzzy set theory has been applied and showed great\nsuccess in the recent past. In this paper, we aim at reviewing the fuzzy set\noriented approaches for HMA, individuating how the fuzzy set may improve the\nHMA, envisaging and delineating the future perspectives. To the best of our\nknowledge, there is not found a single survey in the current literature that\nhas discussed and reviewed fuzzy approaches towards the HMA. For ease of\nunderstanding, we conceptually classify the human motion into three broad\nlevels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:42:51 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 18:19:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lim", "Chern Hong", ""], ["Vats", "Ekta", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1412.0691", "submitter": "Hema Swetha Koppula", "authors": "Ashutosh Saxena, Ashesh Jain, Ozan Sener, Aditya Jami, Dipendra K.\n  Misra, Hema S. Koppula", "title": "RoboBrain: Large-Scale Knowledge Engine for Robots", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a knowledge engine, which learns and shares\nknowledge representations, for robots to carry out a variety of tasks. Building\nsuch an engine brings with it the challenge of dealing with multiple data\nmodalities including symbols, natural language, haptic senses, robot\ntrajectories, visual features and many others. The \\textit{knowledge} stored in\nthe engine comes from multiple sources including physical interactions that\nrobots have while performing tasks (perception, planning and control),\nknowledge bases from the Internet and learned representations from several\nrobotics research groups.\n  We discuss various technical aspects and associated challenges such as\nmodeling the correctness of knowledge, inferring latent information and\nformulating different robotic tasks as queries to the knowledge engine. We\ndescribe the system architecture and how it supports different mechanisms for\nusers and robots to interact with the engine. Finally, we demonstrate its use\nin three important research areas: grounding natural language, perception, and\nplanning, which are the key building blocks for many robotic tasks. This\nknowledge engine is a collaborative effort and we call it RoboBrain.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:22:46 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 06:16:39 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Saxena", "Ashutosh", ""], ["Jain", "Ashesh", ""], ["Sener", "Ozan", ""], ["Jami", "Aditya", ""], ["Misra", "Dipendra K.", ""], ["Koppula", "Hema S.", ""]]}, {"id": "1412.0773", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang", "title": "Expressiveness of Logic Programs under General Stable Model Semantics", "comments": "Technical report, an extended version of arXiv:1304.0620", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stable model semantics had been recently generalized to non-Herbrand\nstructures by several works, which provides a unified framework and solid\nlogical foundations for answer set programming. This paper focuses on the\nexpressiveness of normal and disjunctive programs under the general stable\nmodel semantics. A translation from disjunctive programs to normal programs is\nproposed for infinite structures. Over finite structures, some disjunctive\nprograms are proved to be intranslatable to normal programs if the arities of\nauxiliary predicates and functions are bounded in a certain way. The\nequivalence of the expressiveness of normal programs and disjunctive programs\nover arbitrary structures is also shown to coincide with that over finite\nstructures, and coincide with whether NP is closed under complement. Moreover,\nto capture the exact expressiveness, some intertranslatability results between\nlogic program classes and fragments of second-order logic are obtained.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 03:29:20 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""]]}, {"id": "1412.0854", "submitter": "Thomas Hassan", "authors": "Thomas Hassan (Le2i), Rafael Peixoto, Christophe Cruz (Le2i), Aurlie\n  Bertaux (Le2i), Nuno Silva", "title": "Semantic HMC for Big Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing Big Data can help corporations to im-prove their efficiency. In\nthis work we present a new vision to derive Value from Big Data using a\nSemantic Hierarchical Multi-label Classification called Semantic HMC based in a\nnon-supervised Ontology learning process. We also proposea Semantic HMC\nprocess, using scalable Machine-Learning techniques and Rule-based reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 10:44:24 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Hassan", "Thomas", "", "Le2i"], ["Peixoto", "Rafael", "", "Le2i"], ["Cruz", "Christophe", "", "Le2i"], ["Bertaux", "Aurlie", "", "Le2i"], ["Silva", "Nuno", ""]]}, {"id": "1412.1042", "submitter": "Marco Peressotti", "authors": "Marino Miculan and Marco Peressotti", "title": "A CSP implementation of the bigraph embedding problem", "comments": "arXiv admin note: text overlap with arXiv:1503.02434", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial problem for many results and tools about bigraphs and bigraphical\nreactive systems is bigraph embedding. An embedding is more informative than a\nbigraph matching, since it keeps track of the correspondence between the\nvarious components of the redex (guest) within the agent (host). In this paper,\nwe present an algorithm for computing embeddings based on a reduction to a\nconstraint satisfaction problem. This algorithm, that we prove to be sound and\ncomplete, has been successfully implemented in LibBig, a library for\nmanipulating bigraphical reactive systems. This library can be used for\nimplementing a wide range of tools, and it can be adapted to various extensions\nof bigraphs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 08:52:16 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:16:25 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Miculan", "Marino", ""], ["Peressotti", "Marco", ""]]}, {"id": "1412.1044", "submitter": "Ram\\'on Casares", "authors": "Ram\\'on Casares", "title": "Problem Theory", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Turing machine, as it was presented by Turing himself, models the\ncalculations done by a person. This means that we can compute whatever any\nTuring machine can compute, and therefore we are Turing complete. The question\naddressed here is why, Why are we Turing complete? Being Turing complete also\nmeans that somehow our brain implements the function that a universal Turing\nmachine implements. The point is that evolution achieved Turing completeness,\nand then the explanation should be evolutionary, but our explanation is\nmathematical. The trick is to introduce a mathematical theory of problems,\nunder the basic assumption that solving more problems provides more survival\nopportunities. So we build a problem theory by fusing set and computing\ntheories. Then we construct a series of resolvers, where each resolver is\ndefined by its computing capacity, that exhibits the following property: all\nproblems solved by a resolver are also solved by the next resolver in the\nseries if certain condition is satisfied. The last of the conditions is to be\nTuring complete. This series defines a resolvers hierarchy that could be seen\nas a framework for the evolution of cognition. Then the answer to our question\nwould be: to solve most problems. By the way, the problem theory defines\nadaptation, perception, and learning, and it shows that there are just three\nways to resolve any problem: routine, trial, and analogy. And, most\nimportantly, this theory demonstrates how problems can be used to found\nmathematics and computing on biology.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 18:13:34 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 10:03:24 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 10:37:07 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 08:55:52 GMT"}, {"version": "v5", "created": "Tue, 4 Aug 2015 08:46:12 GMT"}, {"version": "v6", "created": "Fri, 2 Sep 2016 09:08:05 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Casares", "Ram\u00f3n", ""]]}, {"id": "1412.1069", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Approximate Lifted Inference with Probabilistic Databases", "comments": "12 pages, 5 figures, pre-print for a paper appearing in VLDB 2015.\n  arXiv admin note: text overlap with arXiv:1310.6257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for approximate evaluation of #P-hard\nqueries with probabilistic databases. In our approach, every query is evaluated\nentirely in the database engine by evaluating a fixed number of query plans,\neach providing an upper bound on the true probability, then taking their\nminimum. We provide an algorithm that takes into account important schema\ninformation to enumerate only the minimal necessary plans among all possible\nplans. Importantly, this algorithm is a strict generalization of all known\nresults of PTIME self-join-free conjunctive queries: A query is safe if and\nonly if our algorithm returns one single plan. We also apply three relational\nquery optimization techniques to evaluate all minimal safe plans very fast. We\ngive a detailed experimental evaluation of our approach and, in the process,\nprovide a new way of thinking about the value of probabilistic methods over\nnon-probabilistic methods for ranking query answers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 20:58:20 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.1138", "submitter": "Ben Fulcher", "authors": "B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones", "title": "Highly comparative fetal heart rate analysis", "comments": "7 pages, 4 figures", "journal-ref": "Fulcher, B. D., Georgieva, A., Redman, C. W., & Jones, N. S.\n  (2012). Highly comparative fetal heart rate analysis (pp. 3135-3138).\n  Presented at the 34th Annual International Conference of the IEEE EMBS, San\n  Diego, CA, USA", "doi": "10.1109/EMBC.2012.6346629", "report-no": null, "categories": "cs.LG cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database of fetal heart rate (FHR) time series measured from 7221 patients\nduring labor is analyzed with the aim of learning the types of features of\nthese recordings that are informative of low cord pH. Our 'highly comparative'\nanalysis involves extracting over 9000 time-series analysis features from each\nFHR time series, including measures of autocorrelation, entropy, distribution,\nand various model fits. This diverse collection of features was developed in\nprevious work, and is publicly available. We describe five features that most\naccurately classify a balanced training set of 59 'low pH' and 59 'normal pH'\nFHR recordings. We then describe five of the features with the strongest linear\ncorrelation to cord pH across the full dataset of FHR time series. The features\nidentified in this work may be used as part of a system for guiding\nintervention during labor in future. This work successfully demonstrates the\nutility of comparing across a large, interdisciplinary literature on\ntime-series analysis to automatically contribute new scientific results for\nspecific biomedical signal processing challenges.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 00:00:42 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fulcher", "B. D.", ""], ["Georgieva", "A. E.", ""], ["Redman", "C. W. G.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1412.1505", "submitter": "Guy Van den Broeck", "authors": "Paul Beame, Guy Van den Broeck, Eric Gribkoff, Dan Suciu", "title": "Symmetric Weighted First-Order Model Counting", "comments": "To appear at PODS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FO Model Counting problem (FOMC) is the following: given a sentence\n$\\Phi$ in FO and a number $n$, compute the number of models of $\\Phi$ over a\ndomain of size $n$; the Weighted variant (WFOMC) generalizes the problem by\nassociating a weight to each tuple and defining the weight of a model to be the\nproduct of weights of its tuples. In this paper we study the complexity of the\nsymmetric WFOMC, where all tuples of a given relation have the same weight. Our\nmotivation comes from an important application, inference in Knowledge Bases\nwith soft constraints, like Markov Logic Networks, but the problem is also of\nindependent theoretical interest. We study both the data complexity, and the\ncombined complexity of FOMC and WFOMC. For the data complexity we prove the\nexistence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the\nexistence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also\nprove that all $\\gamma$-acyclic queries have polynomial time data complexity.\nFor the combined complexity, we prove that, for every fragment FO$^{k}$, $k\\geq\n2$, the combined complexity of FOMC (or WFOMC) is #P-complete.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 22:03:52 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 13:29:54 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 14:58:14 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Beame", "Paul", ""], ["Broeck", "Guy Van den", ""], ["Gribkoff", "Eric", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.1862", "submitter": "Bryan Renne", "authors": "Paul Egr\\'e and Paul Marty and Bryan Renne", "title": "Knowledge, Justification, and Reason-Based Belief", "comments": "v3 edits acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is knowledge definable as justified true belief (\"JTB\")? We argue that one\ncan legitimately answer positively or negatively, depending on how the notion\nof justification is understood. To facilitate our argument, we introduce a\nsimple propositional logic of reason-based belief. We show that this logic is\nsufficiently flexible to accommodate various useful features, including\nquantification over reasons. We use our framework to contrast two notions of\nJTB: one internalist, the other externalist. We argue that Gettier cases\nessentially challenge the internalist notion but not the externalist one. In\nparticular, we may equate knowledge and JTB if the latter is grounded in what\nwe call \"adequate\" reasons.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 23:21:12 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 05:05:05 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 13:54:36 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Egr\u00e9", "Paul", ""], ["Marty", "Paul", ""], ["Renne", "Bryan", ""]]}, {"id": "1412.1897", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Jason Yosinski, Jeff Clune", "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for\n  Unrecognizable Images", "comments": "To appear at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 05:29:43 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 22:27:00 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 16:41:04 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 23:12:56 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1412.1913", "submitter": "Santosh Mungle", "authors": "Santosh Mungle", "title": "A Portfolio Approach to Algorithm Selection for Discrete Time-Cost\n  Trade-off Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a known fact that the performance of optimization algorithms for\nNP-Hard problems vary from instance to instance. We observed the same trend\nwhen we comprehensively studied multi-objective evolutionary algorithms (MOEAs)\non a six benchmark instances of discrete time-cost trade-off problem (DTCTP) in\na construction project. In this paper, instead of using a single algorithm to\nsolve DTCTP, we use a portfolio approach that takes multiple algorithms as its\nconstituent. We proposed portfolio comprising of four MOEAs, Non-dominated\nSorting Genetic Algorithm II (NSGA-II), the strength Pareto Evolutionary\nAlgorithm II (SPEA-II), Pareto archive evolutionary strategy (PAES) and Niched\nPareto Genetic Algorithm II (NPGA-II) to solve DTCTP. The result shows that the\nportfolio approach is computationally fast and qualitatively superior to its\nconstituent algorithms for all benchmark instances. Moreover, portfolio\napproach provides an insight in selecting the best algorithm for all benchmark\ninstances of DTCTP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 07:58:30 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 16:48:09 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Mungle", "Santosh", ""]]}, {"id": "1412.2114", "submitter": "Toru Ohira", "authors": "Toru Ohira", "title": "Chases and Escapes, and Optimization Problems", "comments": "3 pages, 4 figures. To appear in the Proceedings of the International\n  Symposium on Artificial Life and Robotics (AROB20th), Beppu, Oita Japan,\n  January 21-23, 2015", "journal-ref": "Artificial Life Robotics (2015) 20: 257", "doi": "10.1007/s10015-015-0220-2", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for solving combinatorial optimization problem by\nutilizing the mechanism of chases and escapes, which has a long history in\nmathematics. In addition to the well-used steepest descent and neighboring\nsearch, we perform a chase and escape game on the \"landscape\" of the cost\nfunction. We have created a concrete algorithm for the Traveling Salesman\nProblem. Our preliminary test indicates a possibility that this new fusion of\nchases and escapes problem into combinatorial optimization search is fruitful.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 06:47:28 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Ohira", "Toru", ""]]}, {"id": "1412.2122", "submitter": "V\\'ictor Ponce-L\\'opez", "authors": "V\\'ictor Ponce-L\\'opez, Sergio Escalera, Marc P\\'erez, Oriol Jan\\'es,\n  Xavier Bar\\'o", "title": "Non-Verbal Communication Analysis in Victim-Offender Mediations", "comments": "Please, find the supplementary video material at:\n  http://sunai.uoc.edu/~vponcel/video/VOMSessionSample.mp4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a non-invasive ambient intelligence framework for\nthe semi-automatic analysis of non-verbal communication applied to the\nrestorative justice field. In particular, we propose the use of computer vision\nand social signal processing technologies in real scenarios of Victim-Offender\nMediations, applying feature extraction techniques to multi-modal\naudio-RGB-depth data. We compute a set of behavioral indicators that define\ncommunicative cues from the fields of psychology and observational methodology.\nWe test our methodology on data captured in real world Victim-Offender\nMediation sessions in Catalonia in collaboration with the regional government.\nWe define the ground truth based on expert opinions when annotating the\nobserved social responses. Using different state-of-the-art binary\nclassification approaches, our system achieves recognition accuracies of 86%\nwhen predicting satisfaction, and 79% when predicting both agreement and\nreceptivity. Applying a regression strategy, we obtain a mean deviation for the\npredictions between 0.5 and 0.7 in the range [1-5] for the computed social\nsignals.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 12:56:43 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 18:12:48 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ponce-L\u00f3pez", "V\u00edctor", ""], ["Escalera", "Sergio", ""], ["P\u00e9rez", "Marc", ""], ["Jan\u00e9s", "Oriol", ""], ["Bar\u00f3", "Xavier", ""]]}, {"id": "1412.2186", "submitter": "Babatunji Omoniwa", "authors": "Hasan M. H. Owda, Babatunji Omoniwa, Ahmad R. Shahid, Sheikh Ziauddin", "title": "Using Artificial Neural Network Techniques for Prediction of Electric\n  Energy Consumption", "comments": "10 pages, 5 figures, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to imprecision and uncertainties in predicting real world problems,\nartificial neural network (ANN) techniques have become increasingly useful for\nmodeling and optimization. This paper presents an artificial neural network\napproach for forecasting electric energy consumption. For effective planning\nand operation of power systems, optimal forecasting tools are needed for energy\noperators to maximize profit and also to provide maximum satisfaction to energy\nconsumers. Monthly data for electric energy consumed in the Gaza strip was\ncollected from year 1994 to 2013. Data was trained and the proposed model was\nvalidated using 2-Fold and K-Fold cross validation techniques. The model has\nbeen tested with actual energy consumption data and yields satisfactory\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 00:31:22 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Owda", "Hasan M. H.", ""], ["Omoniwa", "Babatunji", ""], ["Shahid", "Ahmad R.", ""], ["Ziauddin", "Sheikh", ""]]}, {"id": "1412.2221", "submitter": "Dan Olteanu", "authors": "Vince Barany and Balder ten Cate and Benny Kimelfeld and Dan Olteanu\n  and Zografoula Vagena", "title": "Declarative Statistical Modeling with Datalog", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formalisms for specifying statistical models, such as\nprobabilistic-programming languages, typically consist of two components: a\nspecification of a stochastic process (the prior), and a specification of\nobservations that restrict the probability space to a conditional subspace (the\nposterior). Use cases of such formalisms include the development of algorithms\nin machine learning and artificial intelligence. We propose and investigate a\ndeclarative framework for specifying statistical models on top of a database,\nthrough an appropriate extension of Datalog. By virtue of extending Datalog,\nour framework offers a natural integration with the database, and has a robust\ndeclarative semantics. Our Datalog extension provides convenient mechanisms to\ninclude numerical probability functions; in particular, conclusions of rules\nmay contain values drawn from such functions. The semantics of a program is a\nprobability distribution over the possible outcomes of the input database with\nrespect to the program; these outcomes are minimal solutions with respect to a\nrelated program with existentially quantified variables in conclusions.\nObservations are naturally incorporated by means of integrity constraints over\nthe extensional and intensional relations. We focus on programs that use\ndiscrete numerical distributions, but even then the space of possible outcomes\nmay be uncountable (as a solution can be infinite). We define a probability\nmeasure over possible outcomes by applying the known concept of cylinder sets\nto a probabilistic chase procedure. We show that the resulting semantics is\nrobust under different chases. We also identify conditions guaranteeing that\nall possible outcomes are finite (and then the probability space is discrete).\nWe argue that the framework we propose retains the purely declarative nature of\nDatalog, and allows for natural specifications of statistical models.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 11:04:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 19:49:24 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Barany", "Vince", ""], ["Cate", "Balder ten", ""], ["Kimelfeld", "Benny", ""], ["Olteanu", "Dan", ""], ["Vagena", "Zografoula", ""]]}, {"id": "1412.2226", "submitter": "Haris Aziz", "authors": "Haris Aziz and Toby Walsh and Lirong Xia", "title": "Possible and Necessary Allocations via Sequential Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple mechanism for allocating indivisible resources is sequential\nallocation in which agents take turns to pick items. We focus on possible and\nnecessary allocation problems, checking whether allocations of a given form\noccur in some or all mechanisms for several commonly used classes of sequential\nallocation mechanisms. In particular, we consider whether a given agent\nreceives a given item, a set of items, or a subset of items for five natural\nclasses of sequential allocation mechanisms: balanced, recursively balanced,\nbalanced alternating, strictly alternating and all policies. We identify\ncharacterizations of allocations produced balanced, recursively balanced,\nbalanced alternating policies and strictly alternating policies respectively,\nwhich extend the well-known characterization by Brams and King [2005] for\npolicies without restrictions. In addition, we examine the computational\ncomplexity of possible and necessary allocation problems for these classes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 11:54:33 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Aziz", "Haris", ""], ["Walsh", "Toby", ""], ["Xia", "Lirong", ""]]}, {"id": "1412.2309", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka and Pietro Perona and Frederick Eberhardt", "title": "Visual Causal Feature Learning", "comments": "Accepted at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:13:27 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 22:35:30 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1412.2328", "submitter": "Muhammad Taimoor Khan", "authors": "Muhammad Taimoor Khan, Dimitrios Serpanos, Howard Shrobe", "title": "On the Behavioural Formalization of the Cognitive Middleware AWDRAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our ongoing work and initial results towards the (behavioral)\ncorrectness analysis of the cognitive middleware AWDRAT. Since, the (provable)\nbehavioral correctness of a software system is a fundamental pre-requisite of\nthe system's security. Therefore, the goal of the work is to first formalize\nthe behavioral semantics of the middleware as a pre-requisite for our proof of\nthe behavioral correctness. However, in this paper, we focus only on the core\nand critical component of the middleware, i.e. Execution Monitor which is a\npart of the module \"Architectural Differencer\" of AWDRAT. The role of the\nexecution monitor is to identify inconsistencies between runtime observations\nof the target system and predictions of the specification System Architectural\nModel of the system. As a starting point we have defined the formal\n(denotational) semantics of the observations (runtime events) and predictions\n(executable specifications as of System Architectural Model); then based on the\naforementioned formal semantices, we have formalized the behavior of the\n\"Execution Monitor\" of the middleware.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 07:50:25 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Khan", "Muhammad Taimoor", ""], ["Serpanos", "Dimitrios", ""], ["Shrobe", "Howard", ""]]}, {"id": "1412.2620", "submitter": "Gundram Leifert", "authors": "G. Leifert, T. Strau{\\ss}, T. Gr\\\"uning, R. Labahn (University of\n  Rostock)", "title": "Cells in Multidimensional Recurrent Neural Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-37", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transcription of handwritten text on images is one task in machine\nlearning and one solution to solve it is using multi-dimensional recurrent\nneural networks (MDRNN) with connectionist temporal classification (CTC). The\nRNNs can contain special units, the long short-term memory (LSTM) cells. They\nare able to learn long term dependencies but they get unstable when the\ndimension is chosen greater than one. We defined some useful and necessary\nproperties for the one-dimensional LSTM cell and extend them in the\nmulti-dimensional case. Thereby we introduce several new cells with better\nstability. We present a method to design cells using the theory of linear shift\ninvariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT\nand Rimes database, where we can improve the recognition rate compared to the\nLSTM cell. So each application where the LSTM cells in MDRNNs are used could be\nimproved by substituting them by the new developed cells.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:47:45 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 12:26:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Leifert", "G.", "", "University of\n  Rostock"], ["Strau\u00df", "T.", "", "University of\n  Rostock"], ["Gr\u00fcning", "T.", "", "University of\n  Rostock"], ["Labahn", "R.", "", "University of\n  Rostock"]]}, {"id": "1412.2672", "submitter": "Daniel Harari", "authors": "Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman", "title": "When Computer Vision Gazes at Cognition", "comments": "Tao Gao and Daniel Harari contributed equally to this work", "journal-ref": null, "doi": null, "report-no": "CBMM Memo No. 025, MIT", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint attention is a core, early-developing form of social interaction. It is\nbased on our ability to discriminate the third party objects that other people\nare looking at. While it has been shown that people can accurately determine\nwhether another person is looking directly at them versus away, little is known\nabout human ability to discriminate a third person gaze directed towards\nobjects that are further away, especially in unconstraint cases where the\nlooker can move her head and eyes freely. In this paper we address this\nquestion by jointly exploring human psychophysics and a cognitively motivated\ncomputer vision model, which can detect the 3D direction of gaze from 2D face\nimages. The synthesis of behavioral study and computer vision yields several\ninteresting discoveries. (1) Human accuracy of discriminating targets\n8{\\deg}-10{\\deg} of visual angle apart is around 40% in a free looking gaze\ntask; (2) The ability to interpret gaze of different lookers vary dramatically;\n(3) This variance can be captured by the computational model; (4) Human\noutperforms the current model significantly. These results collectively show\nthat the acuity of human joint attention is indeed highly impressive, given the\ncomputational challenge of the natural looking task. Moreover, the gap between\nhuman and model performance, as well as the variability of gaze interpretation\nacross different lookers, require further understanding of the underlying\nmechanisms utilized by humans for this challenging task.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 17:25:57 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Gao", "Tao", ""], ["Harari", "Daniel", ""], ["Tenenbaum", "Joshua", ""], ["Ullman", "Shimon", ""]]}, {"id": "1412.2689", "submitter": "Afdel Karim", "authors": "Ali Aajli, Karim Afdel", "title": "A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic", "comments": "58-66", "journal-ref": "Journal of Engineering Research and Applications,ISSN : 2248-9622,\n  Vol. 4, Issue 10( Part - 3), October 2014, pp.58-66", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, adaptive learning systems rely increasingly on learning\nhierarchy to customize the educational logic developed in their courses. Most\napproaches do not consider that the relationships of prerequisites between the\nskills are fuzzy relationships. In this article, we describe a new approach of\na practical application of fuzzy logic techniques to the construction of\nlearning hierarchies. For this, we use a learning hierarchy predefined by one\nor more experts of a specific field. However, the relationships of\nprerequisites between the skills in the learning hierarchy are not definitive\nand they are fuzzy relationships. Indeed, we measure relevance degree of all\nrelationships existing in this learning hierarchy and we try to answer to the\nfollowing question: Is the relationships of prerequisites predefined in initial\nlearning hierarchy are correctly established or not?\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:34:07 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Aajli", "Ali", ""], ["Afdel", "Karim", ""]]}, {"id": "1412.2812", "submitter": "Ehsan Khoddam", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:40:41 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.2824", "submitter": "Yu Zhang", "authors": "Vignesh Narayanan, Yu Zhang, Nathaniel Mendoza and Subbarao\n  Kambhampati", "title": "Plan or not: Remote Human-robot Teaming with Incomplete Task Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction can be divided into two categories based on the\nphysical distance between the human and robot: remote and proximal. In proximal\ninteraction, the human and robot often engage in close coordination; in remote\ninteraction, the human and robot are less coupled due to communication\nconstraints. As a result, providing automation for the robot in remote\ninteraction becomes more important. Thus far, human factor studies on\nautomation in remote human-robot interaction have been restricted to various\nforms of supervision, in which the robot is essentially being used as a smart\nmobile manipulation platform with sensing capabilities. In this paper, we\ninvestigate the incorporation of general planning capability into the robot to\nfacilitate peer-to-peer human-robot teaming, in which the human and robot are\nviewed as teammates that are physically separated. The human and robot share\nthe same global goal and collaborate to achieve it. Note that humans may feel\nuncomfortable at such robot autonomy, which can potentially reduce teaming\nperformance. One important difference between peer-to-peer teaming and\nsupervised teaming is that an autonomous robot in peer-to-peer teaming can\nachieve the goal alone when the task information is completely specified.\nHowever, incompleteness often exists, which implies information asymmetry.\nWhile information asymmetry can be desirable sometimes, it may also lead to the\nrobot choosing improper actions that negatively influence the teaming\nperformance. We aim to investigate the various trade-offs, e.g., mental\nworkload and situation awareness, between these two types of remote human-robot\nteaming.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 01:05:59 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Narayanan", "Vignesh", ""], ["Zhang", "Yu", ""], ["Mendoza", "Nathaniel", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1412.2985", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "Cause, Responsibility, and Blame: oA Structural-Model Approach", "comments": "To appear, Law, Probability, and Risk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A definition of causality introduced by Halpern and Pearl, which uses\nstructural equations, is reviewed. A more refined definition is then\nconsidered, which takes into account issues of normality and typicality, which\nare well known to affect causal ascriptions. Causality is typically an\nall-or-nothing notion: either A is a cause of B or it is not. An extension of\nthe definition of causality to capture notions of degree of responsibility and\ndegree of blame, due to Chockler and Halpern, is reviewed. For example, if\nsomeone wins an election 11-0, then each person who votes for him is less\nresponsible for the victory than if he had won 6-5. Degree of blame takes into\naccount an agent's epistemic state. Roughly speaking, the degree of blame of A\nfor B is the expected degree of responsibility of A for B, taken over the\nepistemic state of an agent. Finally, the structural-equations definition of\ncausality is compared to Wright's NESS test.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:58:58 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1412.3056", "submitter": "Mohammad S. Qaseem", "authors": "Mohammad S. Qaseem and A. Govardhan", "title": "Phishing Detection in IMs using Domain Ontology and CBA - An innovative\n  Rule Generation Approach", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.5121/ijist.2014.4601", "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User ignorance towards the use of communication services like Instant\nMessengers, emails, websites, social networks etc. is becoming the biggest\nadvantage for phishers. It is required to create technical awareness in users\nby educating them to create a phishing detection application which would\ngenerate phishing alerts for the user so that phishing messages are not\nignored. The lack of basic security features to detect and prevent phishing has\nhad a profound effect on the IM clients, as they lose their faith in e-banking\nand e-commerce transactions, which will have a disastrous impact on the\ncorporate and banking sectors and businesses which rely heavily on the\ninternet. Very little research contributions were available in for phishing\ndetection in Instant messengers. A context based, dynamic and intelligent\nphishing detection methodology in IMs is proposed, to analyze and detect\nphishing in Instant Messages with relevance to domain ontology (OBIE) and\nutilizes the Classification based on Association (CBA) for generating phishing\nrules and alerting the victims. A PDS Monitoring system algorithm is used to\nidentify the phishing activity during exchange of messages in IMs, with high\nratio of precision and recall. The results have shown improvement by the\nincreased percentage of precision and recall when compared to the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 11:02:24 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Qaseem", "Mohammad S.", ""], ["Govardhan", "A.", ""]]}, {"id": "1412.3076", "submitter": "Joseph Y. Halpern", "authors": "Gadi Aleksandrowicz, Hana Chockler, Joseph Y. Halpern, Alexander Ivrii", "title": "The Computational Complexity of Structure-Based Causality", "comments": "Appears in AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Halpern and Pearl introduced a definition of actual causality; Eiter and\nLukasiewicz showed that computing whether X=x is a cause of Y=y is NP-complete\nin binary models (where all variables can take on only two values) and\\\nSigma_2^P-complete in general models. In the final version of their paper,\nHalpern and Pearl slightly modified the definition of actual cause, in order to\ndeal with problems pointed by Hopkins and Pearl. As we show, this modification\nhas a nontrivial impact on the complexity of computing actual cause. To\ncharacterize the complexity, a new family D_k^P, k= 1, 2, 3, ..., of complexity\nclasses is introduced, which generalizes the class DP introduced by\nPapadimitriou and Yannakakis (DP is just D_1^P). %joe2 %We show that the\ncomplexity of computing causality is $\\D_2$-complete %under the new definition.\nChockler and Halpern \\citeyear{CH04} extended the We show that the complexity\nof computing causality under the updated definition is $D_2^P$-complete.\n  Chockler and Halpern extended the definition of causality by introducing\nnotions of responsibility and blame. The complexity of determining the degree\nof responsibility and blame using the original definition of causality was\ncompletely characterized. Again, we show that changing the definition of\ncausality affects the complexity, and completely characterize it using the\nupdated definition.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 19:58:51 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Aleksandrowicz", "Gadi", ""], ["Chockler", "Hana", ""], ["Halpern", "Joseph Y.", ""], ["Ivrii", "Alexander", ""]]}, {"id": "1412.3078", "submitter": "Jun Wei Ng", "authors": "Jun Wei Ng and Marc Peter Deisenroth", "title": "Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a practical and scalable Gaussian process model for large-scale\nnonlinear probabilistic regression. Our mixture-of-experts model is\nconceptually simple and hierarchically recombines computations for an overall\napproximation of a full Gaussian process. Closed-form and distributed\ncomputations allow for efficient and massive parallelisation while keeping the\nmemory consumption small. Given sufficient computing resources, our model can\nhandle arbitrarily large data sets, without explicit sparse approximations. We\nprovide strong experimental evidence that our model can be applied to large\ndata sets of sizes far beyond millions. Hence, our model has the potential to\nlay the foundation for general large-scale Gaussian process research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:03:06 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Ng", "Jun Wei", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1412.3079", "submitter": "Bozhidar Bozhanov", "authors": "Bozhidar Bozhanov", "title": "Computoser - rule-based, probability-driven algorithmic music\n  composition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents the Computoser hybrid probability/rule based algorithm\nfor music composition (http://computoser.com) and provides a reference\nimplementation. It addresses the issues of unpleasantness and lack of variation\nexhibited by many existing approaches by combining the two methods (basing the\nparameters of the rules on data obtained from preliminary analysis).\n  A sample of 500+ musical pieces was analyzed to derive probabilities for\nmusical characteristics and events (e.g. scale, tempo, intervals). The\nalgorithm was constructed to produce musical pieces using the derived\nprobabilities combined with a large set of composition rules, which were\nobtained and structured after studying established composition practices.\nGenerated pieces were published on the Computoser website where evaluation was\nperformed by listeners. The feedback was positive (58.4% approval), asserting\nthe merits of the undertaken approach.\n  The paper compares this hybrid approach to other approaches to algorithmic\ncomposition and presents a survey of the pleasantness of the resulting music.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:06:10 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Bozhanov", "Bozhidar", ""]]}, {"id": "1412.3131", "submitter": "Afdel Karim", "authors": "Ali Aajli, Karim Afdel", "title": "A tool for implementation of a domain model based on fuzzy relationships", "comments": "61-65", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The domain model is one of the important components used by adaptive learning\nsystems to automatically generate customized courses for the learners. In this\npaper our contribution is to propose a new tool for implementation of a domain\nmodel based on fuzzy relationships among concepts. This tool allows the experts\nand teachers to find the best parameters in order to adapt the learners's\ndifferences.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 18:28:01 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Aajli", "Ali", ""], ["Afdel", "Karim", ""]]}, {"id": "1412.3137", "submitter": "Shashishekar Ramakrishna", "authors": "Naouel Karam, Shashishekar Ramakrishna and Adrian Paschke", "title": "Rule reasoning for legal norm validation of FSTP facts", "comments": "1st International workshop on Artificial Intelligence and IP Law,\n  AIIP- Jurix 2012- Amsterdam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Non-obviousness or inventive step is a general requirement for patentability\nin most patent law systems. An invention should be at an adequate distance\nbeyond its prior art in order to be patented. This short paper provides an\noverview on a methodology proposed for legal norm validation of FSTP facts\nusing rule reasoning approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 21:03:53 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Karam", "Naouel", ""], ["Ramakrishna", "Shashishekar", ""], ["Paschke", "Adrian", ""]]}, {"id": "1412.3138", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Yuexin Wu, and Jianyang Zeng", "title": "Computational Protein Design Using AND/OR Branch-and-Bound Search", "comments": "RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the global minimum energy conformation (GMEC) is an\nimportant and challenging topic in structure-based computational protein\ndesign. In this paper, we propose a new protein design algorithm based on the\nAND/OR branch-and-bound (AOBB) search, which is a variant of the traditional\nbranch-and-bound search algorithm, to solve this combinatorial optimization\nproblem. By integrating with a powerful heuristic function, AOBB is able to\nfully exploit the graph structure of the underlying residue interaction network\nof a backbone template to significantly accelerate the design process. Tests on\nreal protein data show that our new protein design algorithm is able to solve\nmany prob- lems that were previously unsolvable by the traditional exact search\nalgorithms, and for the problems that can be solved with traditional provable\nalgorithms, our new method can provide a large speedup by several orders of\nmagnitude while still guaranteeing to find the global minimum energy\nconformation (GMEC) solution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 12:23:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:06:36 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Zhou", "Yichao", ""], ["Wu", "Yuexin", ""], ["Zeng", "Jianyang", ""]]}, {"id": "1412.3191", "submitter": "I-Ting Liu", "authors": "I-Ting Liu, Bhiksha Ramakrishnan", "title": "Bach in 2014: Music Composition with Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a framework for computer music composition that uses resilient\npropagation (RProp) and long short term memory (LSTM) recurrent neural network.\nIn this paper, we show that LSTM network learns the structure and\ncharacteristics of music pieces properly by demonstrating its ability to\nrecreate music. We also show that predicting existing music using RProp\noutperforms Back propagation through time (BPTT).\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 04:06:38 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 03:18:33 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Liu", "I-Ting", ""], ["Ramakrishnan", "Bhiksha", ""]]}, {"id": "1412.3279", "submitter": "Jerome Euzenat", "authors": "J\\'er\\^ome Euzenat (INRIA Grenoble Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble)", "title": "The category of networks of ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic web has led to the deployment of ontologies on the web connected\nthrough various relations and, in particular, alignments of their vocabularies.\nThere exists several semantics for alignments which make difficult\ninteroperation between different interpretation of networks of ontologies. Here\nwe present an abstraction of these semantics which allows for defining the\nnotions of closure and consistency for networks of ontologies independently\nfrom the precise semantics. We also show that networks of ontologies with\nspecific notions of morphisms define categories of networks of ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 12:34:04 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Euzenat", "J\u00e9r\u00f4me", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"]]}, {"id": "1412.3409", "submitter": "Christopher Clark", "authors": "Christopher Clark and Amos Storkey", "title": "Teaching Deep Convolutional Neural Networks to Play Go", "comments": "9 pages, 8 figures, 5 tables. Corrected typos, minor adjustment to\n  table format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering the game of Go has remained a long standing challenge to the field\nof AI. Modern computer Go systems rely on processing millions of possible\nfuture positions to play well, but intuitively a stronger and more 'humanlike'\nway to play the game would be to rely on pattern recognition abilities rather\nthen brute force computation. Following this sentiment, we train deep\nconvolutional neural networks to play Go by training them to predict the moves\nmade by expert Go players. To solve this problem we introduce a number of novel\ntechniques, including a method of tying weights in the network to 'hard code'\nsymmetries that are expect to exist in the target function, and demonstrate in\nan ablation study they considerably improve performance. Our final networks are\nable to achieve move prediction accuracies of 41.1% and 44.4% on two different\nGo datasets, surpassing previous state of the art on this task by significant\nmargins. Additionally, while previous move prediction programs have not yielded\nstrong Go playing programs, we show that the networks trained in this work\nacquired high levels of skill. Our convolutional neural networks can\nconsistently defeat the well known Go program GNU Go, indicating it is state of\nthe art among programs that do not use Monte Carlo Tree Search. It is also able\nto win some games against state of the art Go playing program Fuego while using\na fraction of the play time. This success at playing Go indicates high level\nprinciples of the game were learned.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 18:59:43 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 10:31:31 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Clark", "Christopher", ""], ["Storkey", "Amos", ""]]}, {"id": "1412.3518", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "Appropriate Causal Models and the Stability of Causation", "comments": "A preliminary version of this paper appears in the Proceedings of the\n  Fourteenth International Conference on Principles of Knowledge Representation\n  and Reasoning (KR 2014)}, 2014. To appear, Review of Symbolic Logic", "journal-ref": "The Review of Symbolic Logic 9 (2016) 76-102", "doi": "10.1017/S1755020315000246", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal models defined in terms of structural equations have proved to be\nquite a powerful way of representing knowledge regarding causality. However, a\nnumber of authors have given examples that seem to show that the Halpern-Pearl\n(HP) definition of causality gives intuitively unreasonable answers. Here it is\nshown that, for each of these examples, we can give two stories consistent with\nthe description in the example, such that intuitions regarding causality are\nquite different for each story. By adding additional variables, we can\ndisambiguate the stories. Moreover, in the resulting causal models, the HP\ndefinition of causality gives the intuitively correct answer. It is also shown\nthat, by adding extra variables, a modification to the original HP definition\nmade to deal with an example of Hopkins and Pearl may not be necessary. Given\nhow much can be done by adding extra variables, there might be a concern that\nthe notion of causality is somewhat unstable. Can adding extra variables in a\n\"conservative\" way (i.e., maintaining all the relations between the variables\nin the original model) cause the answer to the question \"Is X=x a cause of Y=y\"\nto alternate between \"yes\" and \"no\"? It is shown that we can have such\nalternation infinitely often, but if we take normality into consideration, we\ncannot. Indeed, under appropriate normality assumptions. adding an extra\nvariable can change the answer from \"yes\" to \"no\", but after that, it cannot\ncannot change back to \"yes\".\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 02:16:39 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 17:14:55 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1412.3633", "submitter": "Jan Triska", "authors": "Jan Triska and Vilem Vychodil", "title": "Logic of temporal attribute implications", "comments": null, "journal-ref": null, "doi": "10.1007/s10472-016-9526-6", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study logic for reasoning with if-then formulas describing dependencies\nbetween attributes of objects which are observed in consecutive points in time.\nWe introduce semantic entailment of the formulas, show its fixed-point\ncharacterization, investigate closure properties of model classes, present an\naxiomatization and prove its completeness, and investigate alternative\naxiomatizations and normalized proofs. We investigate decidability and\ncomplexity issues of the logic and prove that the entailment problem is NP-hard\nand belongs to EXPSPACE. We show that by restricting to predictive formulas,\nthe entailment problem is decidable in pseudo-linear time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 12:41:51 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 08:01:28 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Triska", "Jan", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1412.3714", "submitter": "Jiwei Li", "authors": "Jiwei Li", "title": "Feature Weight Tuning for Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:35:27 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 00:57:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Li", "Jiwei", ""]]}, {"id": "1412.3773", "submitter": "Joris Mooij", "authors": "Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler,\n  Bernhard Sch\\\"olkopf", "title": "Distinguishing cause from effect using observational data: methods and\n  benchmarks", "comments": "101 pages, second revision submitted to Journal of Machine Learning\n  Research", "journal-ref": "Journal of Machine Learning Research 17(32):1-102, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of causal relationships from purely observational data is a\nfundamental problem in science. The most elementary form of such a causal\ndiscovery problem is to decide whether X causes Y or, alternatively, Y causes\nX, given joint observations of two variables X, Y. An example is to decide\nwhether altitude causes temperature, or vice versa, given only joint\nmeasurements of both variables. Even under the simplifying assumptions of no\nconfounding, no feedback loops, and no selection bias, such bivariate causal\ndiscovery problems are challenging. Nevertheless, several approaches for\naddressing those problems have been proposed in recent years. We review two\nfamilies of such methods: Additive Noise Methods (ANM) and Information\nGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs\nthat consists of data for 100 different cause-effect pairs selected from 37\ndatasets from various domains (e.g., meteorology, biology, medicine,\nengineering, economy, etc.) and motivate our decisions regarding the \"ground\ntruth\" causal directions of all pairs. We evaluate the performance of several\nbivariate causal discovery methods on these real-world benchmark data and in\naddition on artificially simulated data. Our empirical results on real-world\ndata indicate that certain methods are indeed able to distinguish cause from\neffect using only purely observational data, although more benchmark data would\nbe needed to obtain statistically significant conclusions. One of the best\nperforming methods overall is the additive-noise method originally proposed by\nHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of\n0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of\nthis work we prove the consistency of that method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 19:34:39 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 14:51:36 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 11:37:57 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Mooij", "Joris M.", ""], ["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Zscheischler", "Jakob", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1412.3802", "submitter": "Neil Rubens", "authors": "Neil Rubens", "title": "Turing Test for the Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How smart is your kettle? How smart are things in your kitchen, your house,\nyour neighborhood, on the internet? With the advent of Internet of Things, and\nthe move of making devices `smart' by utilizing AI, a natural question arrises,\nhow can we evaluate the progress. The standard way of evaluating AI is through\nthe Turing Test. While Turing Test was designed for AI; the device that it was\ntailored to was a computer. Applying the test to variety of devices that\nconstitute Internet of Things poses a number of challenges which could be\naddressed through a number of adaptations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 09:49:07 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Rubens", "Neil", ""]]}, {"id": "1412.3908", "submitter": "Valmi Dufour-Lussier", "authors": "Valmi Dufour-Lussier (INRIA Nancy - Grand Est / LORIA), Alice Hermann\n  (INRIA Nancy - Grand Est / LORIA), Florence Le Ber (ICube), Jean Lieber\n  (INRIA Nancy - Grand Est / LORIA)", "title": "Belief revision in the propositional closure of a qualitative algebra", "comments": null, "journal-ref": "14th International Conference on Principles of Knowledge\n  Representation and Reasoning, Jul 2014, Vienne, Austria. AAAI Press, pp.4", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief revision is an operation that aims at modifying old be-liefs so that\nthey become consistent with new ones. The issue of belief revision has been\nstudied in various formalisms, in particular, in qualitative algebras (QAs) in\nwhich the result is a disjunction of belief bases that is not necessarily\nrepre-sentable in a QA. This motivates the study of belief revision in\nformalisms extending QAs, namely, their propositional clo-sures: in such a\nclosure, the result of belief revision belongs to the formalism. Moreover, this\nmakes it possible to define a contraction operator thanks to the Harper\nidentity. Belief revision in the propositional closure of QAs is studied, an\nal-gorithm for a family of revision operators is designed, and an open-source\nimplementation is made freely available on the web.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 07:52:28 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Dufour-Lussier", "Valmi", "", "INRIA Nancy - Grand Est / LORIA"], ["Hermann", "Alice", "", "INRIA Nancy - Grand Est / LORIA"], ["Ber", "Florence Le", "", "ICube"], ["Lieber", "Jean", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1412.4271", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos", "title": "Multi-Context Models for Reasoning under Partial Knowledge: Generative\n  Process and Inference Grammar", "comments": "To appear in the Proceedings of the 31st Conference on Uncertainty in\n  Artificial Intelligence (UAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arriving at the complete probabilistic knowledge of a domain, i.e., learning\nhow all variables interact, is indeed a demanding task. In reality, settings\noften arise for which an individual merely possesses partial knowledge of the\ndomain, and yet, is expected to give adequate answers to a variety of posed\nqueries. That is, although precise answers to some queries, in principle,\ncannot be achieved, a range of plausible answers is attainable for each query\ngiven the available partial knowledge. In this paper, we propose the\nMulti-Context Model (MCM), a new graphical model to represent the state of\npartial knowledge as to a domain. MCM is a middle ground between Probabilistic\nLogic, Bayesian Logic, and Probabilistic Graphical Models. For this model we\ndiscuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to\nform partial beliefs regarding a domain in a gradual and probabilistically\nconsistent way, and (ii) how to perform inference, i.e., to evaluate a\nprobability of interest involving some variables of the domain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 19:13:09 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 18:19:08 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1412.4465", "submitter": "Meysam Ghaffari", "authors": "Mostafa Sepahvand, Ghasem Alikhajeh, Meysam Ghaffari, Abdolreza\n  Mirzaei", "title": "Generating Graphical Chain by Mutual Matching of Bayesian Network and\n  Extracted Rules of Bayesian Network Using Genetic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the technology development, the need of analyze and extraction of useful\ninformation is increasing. Bayesian networks contain knowledge from data and\nexperts that could be used for decision making processes But they are not\neasily understandable thus the rule extraction methods have been used but they\nhave high computation costs. To overcome this problem we extract rules from\nBayesian network using genetic algorithm. Then we generate the graphical chain\nby mutually matching the extracted rules and Bayesian network. This graphical\nchain could shows the sequence of events that lead to the target which could\nhelp the decision making process. The experimental results on small networks\nshow that the proposed method has comparable results with brute force method\nwhich has a significantly higher computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 05:33:21 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sepahvand", "Mostafa", ""], ["Alikhajeh", "Ghasem", ""], ["Ghaffari", "Meysam", ""], ["Mirzaei", "Abdolreza", ""]]}, {"id": "1412.4485", "submitter": "Micha\\\"el Thomazo", "authors": "Sebastian Rudolph, Micha\\\"el Thomazo, Jean-Fran\\c{c}ois Baget,\n  Marie-Laure Mugnier", "title": "Worst-case Optimal Query Answering for Greedy Sets of Existential Rules\n  and Their Subclasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for an ontological layer on top of data, associated with advanced\nreasoning mechanisms able to exploit the semantics encoded in ontologies, has\nbeen acknowledged both in the database and knowledge representation\ncommunities. We focus in this paper on the ontological query answering problem,\nwhich consists of querying data while taking ontological knowledge into\naccount. More specifically, we establish complexities of the conjunctive query\nentailment problem for classes of existential rules (also called\ntuple-generating dependencies, Datalog+/- rules, or forall-exists-rules. Our\ncontribution is twofold. First, we introduce the class of greedy\nbounded-treewidth sets (gbts) of rules, which covers guarded rules, and their\nmost well-known generalizations. We provide a generic algorithm for query\nentailment under gbts, which is worst-case optimal for combined complexity with\nor without bounded predicate arity, as well as for data complexity and query\ncomplexity. Secondly, we classify several gbts classes, whose complexity was\nunknown, with respect to combined complexity (with both unbounded and bounded\npredicate arity) and data complexity to obtain a comprehensive picture of the\ncomplexity of existential rule fragments that are based on diverse guardedness\nnotions. Upper bounds are provided by showing that the proposed algorithm is\noptimal for all of them.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 07:46:19 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Rudolph", "Sebastian", ""], ["Thomazo", "Micha\u00ebl", ""], ["Baget", "Jean-Fran\u00e7ois", ""], ["Mugnier", "Marie-Laure", ""]]}, {"id": "1412.4736", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Inductive Bias of Dropout", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16, 3403-3454 (2015). (See\n  http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf.)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:40:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 02:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:22:30 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 18:59:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1412.4802", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Neutrosophic information in the framework of multi-valued representation", "comments": null, "journal-ref": null, "doi": "10.13140/2.1.4717.2169", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents some steps for multi-valued representation of neutrosophic\ninformation. These steps are provided in the framework of multi-valued logics\nusing the following logical value: true, false, neutral, unknown and saturated.\nAlso, this approach provides some calculus formulae for the following\nneutrosophic features: truth, falsity, neutrality, ignorance,\nunder-definedness, over-definedness, saturation and entropy. In addition, it\nwas defined net truth, definedness and neutrosophic score.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 16:07:24 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1412.4972", "submitter": "Sejun Park", "authors": "Sejun Park, Jinwoo Shin", "title": "Max-Product Belief Propagation for Linear Programming: Applications to\n  Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The max-product {belief propagation} (BP) is a popular message-passing\nheuristic for approximating a maximum-a-posteriori (MAP) assignment in a joint\ndistribution represented by a graphical model (GM). In the past years, it has\nbeen shown that BP can solve a few classes of linear programming (LP)\nformulations to combinatorial optimization problems including maximum weight\nmatching, shortest path and network flow, i.e., BP can be used as a\nmessage-passing solver for certain combinatorial optimizations. However, those\nLPs and corresponding BP analysis are very sensitive to underlying problem\nsetups, and it has been not clear what extent these results can be generalized\nto. In this paper, we obtain a generic criteria that BP converges to the\noptimal solution of given LP, and show that it is satisfied in LP formulations\nassociated to many classical combinatorial optimization problems including\nmaximum weight perfect matching, shortest path, traveling salesman, cycle\npacking, vertex/edge cover and network flow.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 12:18:34 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 01:43:00 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2015 06:03:41 GMT"}, {"version": "v4", "created": "Thu, 8 Dec 2016 10:37:48 GMT"}, {"version": "v5", "created": "Wed, 28 Jun 2017 17:15:25 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Park", "Sejun", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1412.5077", "submitter": "Ridvan Sahin", "authors": "R{\\i}dvan \\c{S}ahin and Muhammed Yi\\u{g}ider", "title": "A Multi-criteria neutrosophic group decision making metod based TOPSIS\n  for supplier selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of multiple criteria decision making (MCDM) is of determining the\nbest choice among all of the probable alternatives. The problem of supplier\nselection on which decision maker has usually vague and imprecise knowledge is\na typical example of multi criteria group decision-making problem. The\nconventional crisp techniques has not much effective for solving MCDM problems\nbecause of imprecise or fuzziness nature of the linguistic assessments. To find\nthe exact values for MCDM problems is both difficult and impossible in more\ncases in real world. So, it is more reasonable to consider the values of\nalternatives according to the criteria as single valued neutrosophic sets\n(SVNS). This paper deal with the technique for order preference by similarity\nto ideal solution (TOPSIS) approach and extend the TOPSIS method to MCDM\nproblem with single valued neutrosophic information. The value of each\nalternative and the weight of each criterion are characterized by single valued\nneutrosophic numbers. Here, the importance of criteria and alternatives is\nidentified by aggregating individual opinions of decision makers (DMs) via\nsingle valued neutrosophic weighted averaging (IFWA) operator. The proposed\nmethod is, easy use, precise and practical for solving MCDM problem with single\nvalued neutrosophic data. Finally, to show the applicability of the developed\nmethod, a numerical experiment for supplier choice is given as an application\nof single valued neutrosophic TOPSIS method at end of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 18:39:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["\u015eahin", "R\u0131dvan", ""], ["Yi\u011fider", "Muhammed", ""]]}, {"id": "1412.5090", "submitter": "Bryan Renne", "authors": "Jan van Eijck and Bryan Renne", "title": "Belief as Willingness to Bet", "comments": "Removed date from v1 to avoid confusion on citation/reference,\n  otherwise identical to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate modal logics of high probability having two unary modal\noperators: an operator $K$ expressing probabilistic certainty and an operator\n$B$ expressing probability exceeding a fixed rational threshold $c\\geq\\frac\n12$. Identifying knowledge with the former and belief with the latter, we may\nthink of $c$ as the agent's betting threshold, which leads to the motto \"belief\nis willingness to bet.\" The logic $\\mathsf{KB.5}$ for $c=\\frac 12$ has an\n$\\mathsf{S5}$ $K$ modality along with a sub-normal $B$ modality that extends\nthe minimal modal logic $\\mathsf{EMND45}$ by way of four schemes relating $K$\nand $B$, one of which is a complex scheme arising out of a theorem due to\nScott. Lenzen was the first to use Scott's theorem to show that a version of\nthis logic is sound and complete for the probability interpretation. We\nreformulate Lenzen's results and present them here in a modern and accessible\nform. In addition, we introduce a new epistemic neighborhood semantics that\nwill be more familiar to modern modal logicians. Using Scott's theorem, we\nprovide the Lenzen-derivative properties that must be imposed on finite\nepistemic neighborhood models so as to guarantee the existence of a probability\nmeasure respecting the neighborhood function in the appropriate way for\nthreshold $c=\\frac 12$. This yields a link between probabilistic and modal\nneighborhood semantics that we hope will be of use in future work on modal\nlogics of qualitative probability. We leave open the question of which\nproperties must be imposed on finite epistemic neighborhood models so as to\nguarantee existence of an appropriate probability measure for thresholds\n$c\\neq\\frac 12$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 17:34:25 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 22:49:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["van Eijck", "Jan", ""], ["Renne", "Bryan", ""]]}, {"id": "1412.5202", "submitter": "Ridvan Sahin", "authors": "R{\\i}dvan \\c{S}ahin", "title": "Multi-criteria neutrosophic decision making method based on score and\n  accuracy functions under neutrosophic environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neutrosophic set is a more general platform, which can be used to present\nuncertainty, imprecise, incomplete and inconsistent. In this paper a score\nfunction and an accuracy function for single valued neutrosophic sets is\nfirstly proposed to make the distinction between them. Then the idea is\nextended to interval neutrosophic sets. A multi-criteria decision making method\nbased on the developed score-accuracy functions is established in which\ncriterion values for alternatives are single valued neutrosophic sets and\ninterval neutrosophic sets. In decision making process, the neutrosophic\nweighted aggregation operators (arithmetic and geometric average operators) are\nadopted to aggregate the neutrosophic information related to each alternative.\nThus, we can rank all alternatives and make the selection of the best of one(s)\naccording to the score-accuracy functions. Finally, some illustrative examples\nare presented to verify the developed approach and to demonstrate its\npracticality and effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 12:15:08 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["\u015eahin", "R\u0131dvan", ""]]}, {"id": "1412.5207", "submitter": "Anish Acharya", "authors": "Anish Acharya", "title": "Are We Ready for Driver-less Vehicles? Security vs. Privacy- A Social\n  Perspective", "comments": "17 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At this moment Autonomous cars are probably the biggest and most talked about\ntechnology in the Robotics Research Community. In spite of great technological\nadvances over past few years a full edged autonomous car is still far from\nreality. This article talks about the existing system and discusses the\npossibility of a Computer Vision enabled driving being superior than the LiDar\nbased system. A detailed overview of privacy violations that might arise from\nautonomous driving has been discussed in detail both from a technical as well\nas legal perspective. It has been proved through evidence and arguments that\nefficient and accurate estimation and efficient solution of the constraint\nsatisfaction problem addressed in the case of autonomous cars are negatively\ncorrelated with the preserving the privacy of the user. It is a very difficult\ntrade-off since both are very important aspects and has to be taken into\naccount. The fact that one cannot compromise with the safety issues of the car\nmakes it inevitable to run into serious privacy concerns that might have\nadverse social and political effects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 21:48:53 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Acharya", "Anish", ""]]}, {"id": "1412.5244", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky, Richard Zemel", "title": "Learning unbiased features", "comments": "Published in NIPS 2014 Workshop on Transfer and Multitask Learning,\n  see http://nips.cc/Conferences/2014/Program/event.php?ID=4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element in transfer learning is representation learning; if\nrepresentations can be developed that expose the relevant factors underlying\nthe data, then new tasks and domains can be learned readily based on mappings\nof these salient factors. We propose that an important aim for these\nrepresentations are to be unbiased. Different forms of representation learning\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\nuseful approach to estimating the amount of bias in a representation comes from\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\ndistributions. We are not the first to suggest that MMD can be a useful\ncriterion in developing representations that apply across multiple domains or\ntasks [1]. However, in this paper we describe a number of novel applications of\nthis criterion that we have devised, all based on the idea of developing\nunbiased representations. These formulations include: a standard domain\nadaptation framework; a method of learning invariant representations; an\napproach based on noise-insensitive autoencoders; and a novel form of\ngenerative model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 02:47:22 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1412.5513", "submitter": "Engelbert Mephu Nguifo", "authors": "Cyrine Arouri, Engelbert Mephu Nguifo, Sabeur Aridhi, C\\'ecile\n  Roucelle, Gaelle Bonnet-Loosli, Norbert Tsopz\\'e", "title": "Towards a constructive multilayer perceptron for regression task using\n  non-parametric clustering. A case study of Photo-Z redshift reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of architecture of artificial neuron network (ANN) is still a\nchallenging task that users face every time. It greatly affects the accuracy of\nthe built network. In fact there is no optimal method that is applicable to\nvarious implementations at the same time. In this paper we propose a method to\nconstruct ANN based on clustering, that resolves the problems of random and ad\nhoc approaches for multilayer ANN architecture. Our method can be applied to\nregression problems. Experimental results obtained with different datasets,\nreveals the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 18:36:23 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Arouri", "Cyrine", ""], ["Nguifo", "Engelbert Mephu", ""], ["Aridhi", "Sabeur", ""], ["Roucelle", "C\u00e9cile", ""], ["Bonnet-Loosli", "Gaelle", ""], ["Tsopz\u00e9", "Norbert", ""]]}, {"id": "1412.5980", "submitter": "Swakkhar Shatabda", "authors": "Mohammad Murtaza Mahmud, Swakkhar Shatabda and Mohammad Nurul Huda", "title": "GraATP: A Graph Theoretic Approach for Automated Theorem Proving in\n  Plane Geometry", "comments": "The 8th International Conference on Software, Knowledge, Information\n  Management and Applications (SKIMA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Theorem Proving (ATP) is an established branch of Artificial\nIntelligence. The purpose of ATP is to design a system which can automatically\nfigure out an algorithm either to prove or disprove a mathematical claim, on\nthe basis of a set of given premises, using a set of fundamental postulates and\nfollowing the method of logical inference. In this paper, we propose GraATP, a\ngeneralized framework for automated theorem proving in plane geometry. Our\nproposed method translates the geometric entities into nodes of a graph and the\nrelations between them as edges of that graph. The automated system searches\nfor different ways to reach the conclusion for a claim via graph traversal by\nwhich the validity of the geometric theorem is examined.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 18:10:03 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Mahmud", "Mohammad Murtaza", ""], ["Shatabda", "Swakkhar", ""], ["Huda", "Mohammad Nurul", ""]]}, {"id": "1412.5984", "submitter": "Swakkhar Shatabda", "authors": "Muktadir Hossain, Tajkia Tasnim, Swakkhar Shatabda and Dewan M. Farid", "title": "Stochastic Local Search for Pattern Set Mining", "comments": "The 8th International Conference on Software, Knowledge, Information\n  Management and Applications (SKIMA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search methods can quickly find good quality solutions in cases where\nsystematic search methods might take a large amount of time. Moreover, in the\ncontext of pattern set mining, exhaustive search methods are not applicable due\nto the large search space they have to explore. In this paper, we propose the\napplication of stochastic local search to solve the pattern set mining.\nSpecifically, to the task of concept learning. We applied a number of local\nsearch algorithms on a standard benchmark instances for pattern set mining and\nthe results show the potentials for further exploration.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 18:16:52 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Hossain", "Muktadir", ""], ["Tasnim", "Tajkia", ""], ["Shatabda", "Swakkhar", ""], ["Farid", "Dewan M.", ""]]}, {"id": "1412.6060", "submitter": "Mark Madsen", "authors": "Mark E. Madsen and Carl P. Lipo", "title": "Combinatorial Structure of the Deterministic Seriation Method with\n  Multiple Subset Solutions", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": "10.6084/m9.figshare.1269322", "report-no": null, "categories": "cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seriation methods order a set of descriptions given some criterion (e.g.,\nunimodality or minimum distance between similarity scores). Seriation is thus\ninherently a problem of finding the optimal solution among a set of\npermutations of objects. In this short technical note, we review the\ncombinatorial structure of the classical seriation problem, which seeks a\nsingle solution out of a set of objects. We then extend those results to the\niterative frequency seriation approach introduced by Lipo (1997), which finds\noptimal subsets of objects which each satisfy the unimodality criterion within\neach subset. The number of possible solutions across multiple solution subsets\nis larger than $n!$, which underscores the need to find new algorithms and\nheuristics to assist in the deterministic frequency seriation problem.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:48:48 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Madsen", "Mark E.", ""], ["Lipo", "Carl P.", ""]]}, {"id": "1412.6141", "submitter": "Song-Ju Kim Dr.", "authors": "Song-Ju Kim, Masashi Aono, and Etsushi Nameda", "title": "Efficient Decision-Making by Volume-Conserving Physical Object", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1088/1367-2630/17/8/083023", "report-no": null, "categories": "cs.AI cs.LG nlin.AO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that any physical object, as long as its volume is conserved\nwhen coupled with suitable operations, provides a sophisticated decision-making\ncapability. We consider the problem of finding, as accurately and quickly as\npossible, the most profitable option from a set of options that gives\nstochastic rewards. These decisions are made as dictated by a physical object,\nwhich is moved in a manner similar to the fluctuations of a rigid body in a\ntug-of-war game. Our analytical calculations validate statistical reasons why\nour method exhibits higher efficiency than conventional algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 08:23:13 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Kim", "Song-Ju", ""], ["Aono", "Masashi", ""], ["Nameda", "Etsushi", ""]]}, {"id": "1412.6153", "submitter": "Arjun B Krishnan", "authors": "Arjun B. Krishnan and Jayaram Kollipara", "title": "Intelligent Indoor Mobile Robot Navigation Using Stereo Vision", "comments": "9 pages, SIPIJ August 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the existing robot navigation systems, which facilitate the use\nof laser range finders, sonar sensors or artificial landmarks, has the ability\nto locate itself in an unknown environment and then build a map of the\ncorresponding environment. Stereo vision, while still being a rapidly\ndeveloping technique in the field of autonomous mobile robots, are currently\nless preferable due to its high implementation cost. This paper aims at\ndescribing an experimental approach for the building of a stereo vision system\nthat helps the robots to avoid obstacles and navigate through indoor\nenvironments and at the same time remaining very much cost effective. This\npaper discusses the fusion techniques of stereo vision and ultrasound sensors\nwhich helps in the successful navigation through different types of complex\nenvironments. The data from the sensor enables the robot to create the two\ndimensional topological map of unknown environments and stereo vision systems\nmodels the three dimension model of the same environment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 20:01:45 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Krishnan", "Arjun B.", ""], ["Kollipara", "Jayaram", ""]]}, {"id": "1412.6177", "submitter": "Tomoki Tsuchida", "authors": "Tomoki Tsuchida and Garrison W. Cottrell", "title": "Example Selection For Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised learning, an unbiased uniform sampling strategy is typically\nused, in order that the learned features faithfully encode the statistical\nstructure of the training data. In this work, we explore whether active example\nselection strategies - algorithms that select which examples to use, based on\nthe current estimate of the features - can accelerate learning. Specifically,\nwe investigate effects of heuristic and saliency-inspired selection algorithms\non the dictionary learning task with sparse activations. We show that some\nselection algorithms do improve the speed of learning, and we speculate on why\nthey might work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:25:22 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 23:28:16 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 19:22:18 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Tsuchida", "Tomoki", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1412.6285", "submitter": "Gianluca Bontempi", "authors": "Gianluca Bontempi and Maxime Flauder", "title": "From dependency to causality: a machine learning approach", "comments": "submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between statistical dependency and causality lies at the\nheart of all statistical approaches to causal inference. Recent results in the\nChaLearn cause-effect pair challenge have shown that causal directionality can\nbe inferred with good accuracy also in Markov indistinguishable configurations\nthanks to data driven approaches. This paper proposes a supervised machine\nlearning approach to infer the existence of a directed causal link between two\nvariables in multivariate settings with $n>2$ variables. The approach relies on\nthe asymmetry of some conditional (in)dependence relations between the members\nof the Markov blankets of two variables causally connected. Our results show\nthat supervised learning methods may be successfully used to extract causal\ninformation on the basis of asymmetric statistical descriptors also for $n>2$\nvariate distributions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:50:14 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Bontempi", "Gianluca", ""], ["Flauder", "Maxime", ""]]}, {"id": "1412.6413", "submitter": "Gowri Shankar Ramaswamy", "authors": "Gowri Shankar Ramaswamy, F Sagayaraj Francis", "title": "Towards a Consistent, Sound and Complete Conceptual Knowledge", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V17(2):61-63, Nov 2014", "doi": "10.14445/22312803/IJCTT-V17P112", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge is only good if it is sound, consistent and complete. The same\nholds true for conceptual knowledge, which holds knowledge about concepts and\nits association. Conceptual knowledge no matter what format they are\nrepresented in, must be consistent, sound and complete in order to realise its\npractical use. This paper discusses consistency, soundness and completeness in\nthe ambit of conceptual knowledge and the need to consider these factors as\nfundamental to the development of conceptual knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 14:42:16 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Ramaswamy", "Gowri Shankar", ""], ["Francis", "F Sagayaraj", ""]]}, {"id": "1412.6451", "submitter": "Mark Wernsdorfer", "authors": "Mark Wernsdorfer, Ute Schmid", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge\n  Transfer", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of deep machine learning enable to to reuse low-level representations\nefficiently for generating more abstract high-level representations.\nOriginally, deep learning has been applied passively (e.g., for classification\npurposes). Recently, it has been extended to estimate the value of actions for\nautonomous agents within the framework of reinforcement learning (RL). Explicit\nmodels of the environment can be learned to augment such a value function.\nAlthough \"flat\" connectionist methods have already been used for model-based\nRL, up to now, only model-free variants of RL have been equipped with methods\nfrom deep learning. We propose a variant of deep model-based RL that enables an\nagent to learn arbitrarily abstract hierarchical representations of its\nenvironment. In this paper, we present research on how such hierarchical\nrepresentations can be grounded in sensorimotor interaction between an agent\nand its environment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 17:41:59 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Wernsdorfer", "Mark", ""], ["Schmid", "Ute", ""]]}, {"id": "1412.6464", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew\n  Marsza{\\l}ek, Dawid Po{\\l}ap and Marcin Wo\\'zniak", "title": "Simplified firefly algorithm for 2D image key-points search", "comments": "Published version on: 2014 IEEE Symposium on Computational\n  Intelligence for Human-like Intelligence", "journal-ref": "IEEE Symposium on Computational Intelligence for Human-like\n  Intelligence, pp. 118-125, 2014", "doi": "10.1109/CIHLI.2014.7013395", "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify an object, human eyes firstly search the field of view\nfor points or areas which have particular properties. These properties are used\nto recognise an image or an object. Then this process could be taken as a model\nto develop computer algorithms for images identification. This paper proposes\nthe idea of applying the simplified firefly algorithm to search for key-areas\nin 2D images. For a set of input test images the proposed version of firefly\nalgorithm has been examined. Research results are presented and discussed to\nshow the efficiency of this evolutionary computation method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:00:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""], ["Marsza\u0142ek", "Zbigniew", ""], ["Po\u0142ap", "Dawid", ""], ["Wo\u017aniak", "Marcin", ""]]}, {"id": "1412.6545", "submitter": "Pablo Fillottrani", "authors": "Pablo R. Fillottrani, C. Maria Keet", "title": "KF metamodel formalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The KF metamodel is a comprehensive unifying metamodel covering the static\nstructural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER,\nORM, and ORM2, and intended to boost interoperability of common conceptual data\nmodelling languages. It was originally designed in UML with textual\nconstraints, and in this report we present its formalisations in FOL and OWL,\nwhich accompanies the paper that describes, discusses, and analyses the KF\nmetamodel in detail. These new formalizations contribute to give a precise\nmeaning to the metamodel, to understand its complexity properties and to\nprovide a basis for future implementations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 21:56:59 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Fillottrani", "Pablo R.", ""], ["Keet", "C. Maria", ""]]}, {"id": "1412.6614", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:52:25 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 21:00:09 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 18:51:37 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 18:48:31 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1412.6649", "submitter": "Reinhard Moratz", "authors": "Christopher H. Dorr and Reinhard Moratz", "title": "Qualitative shape representation based on the qualitative relative\n  direction and distance calculus eOPRAm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document serves as a brief technical report, detailing the processes\nused to represent and reconstruct simplified polygons using qualitative spatial\ndescriptions, as defined by the eOPRAm qualitative spatial calculus.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 12:51:24 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Dorr", "Christopher H.", ""], ["Moratz", "Reinhard", ""]]}, {"id": "1412.6703", "submitter": "Hector Zenil", "authors": "Hector Zenil", "title": "Quantifying Natural and Artificial Intelligence in Robots and Natural\n  Systems with an Algorithmic Behavioural Test", "comments": "21 pages, Springer Cosmos Series Book on Metrics of sensory motor\n  integration in robots and animals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important aims of the fields of robotics, artificial\nintelligence and artificial life is the design and construction of systems and\nmachines as versatile and as reliable as living organisms at performing high\nlevel human-like tasks. But how are we to evaluate artificial systems if we are\nnot certain how to measure these capacities in living systems, let alone how to\ndefine life or intelligence? Here I survey a concrete metric towards measuring\nabstract properties of natural and artificial systems, such as the ability to\nreact to the environment and to control one's own behaviour.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 22:35:48 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 16:17:35 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Zenil", "Hector", ""]]}, {"id": "1412.6749", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction", "comments": "Eighteen pages in all, but much of the central ideas are covered in\n  the first five and a half pages; most of the remaining pages are devoted to\n  straightforward mathematical derivations, and the presentation of three\n  algorithms. Manuscript contains no figures at this time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By drawing on ideas from optimisation theory, artificial neural networks\n(ANN), graph embeddings and sparse representations, I develop a novel\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\naddressing the feature extraction problem. The proposed method uses (preferably\ndeep) ANNs for projecting input attribute vectors to an output space wherein\npairwise distances are maximized for vectors belonging to different classes,\nbut minimized for those belonging to the same class, while simultaneously\nenforcing sparsity on the ANN outputs. The vectors that result from the\nprojection can then be used as features in any classifier of choice.\nMathematically, I formulate the proposed method as the minimisation of an\nobjective function which can be interpreted, in the ANN output space, as a\nnegative factor of the sum of the squares of the pair-wise distances between\noutput vectors belonging to different classes, added to a positive factor of\nthe sum of squares of the pair-wise distances between output vectors belonging\nto the same classes, plus sparsity and weight decay terms. To derive an\nalgorithm for minimizing the objective function via gradient descent, I use the\nmulti-variate version of the chain rule to obtain the partial derivatives of\nthe function with respect to ANN weights and biases, and find that each of the\nrequired partial derivatives can be expressed as a sum of six terms. As it\nturns out, four of those six terms can be computed using the standard back\npropagation algorithm; the fifth can be computed via a slight modification of\nthe standard backpropagation algorithm; while the sixth one can be computed via\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\nof digits and letters, the CMU PIE database of faces, the MNIST digits\ndatabase, and other standard machine learning databases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 09:28:05 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1412.6973", "submitter": "Guangming Lang", "authors": "Guangming Lang", "title": "Decision-theoretic rough sets-based three-way approximations of\n  interval-valued fuzzy sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In practical situations, interval-valued fuzzy sets are frequently\nencountered. In this paper, firstly, we present shadowed sets for interpreting\nand understanding interval fuzzy sets. We also provide an analytic solution to\ncomputing the pair of thresholds by searching for a balance of uncertainty in\nthe framework of shadowed sets. Secondly, we construct errors-based three-way\napproximations of interval-valued fuzzy sets. We also provide an alternative\ndecision-theoretic formulation for calculating the pair of thresholds by\ntransforming interval-valued loss functions into single-valued loss functions,\nin which the required thresholds are computed by minimizing decision costs.\nThirdly, we compute errors-based three-way approximations of interval-valued\nfuzzy sets by using interval-valued loss functions. Finally, we employ several\nexamples to illustrate that how to take an action for an object with\ninterval-valued membership grade by using interval-valued loss functions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 13:34:04 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Lang", "Guangming", ""]]}, {"id": "1412.7386", "submitter": "Pietro  Hiram Guzzi", "authors": "Mario Cannataro, Pietro Hiram Guzzi, Marianna Milano, Pierangelo\n  Veltri", "title": "A web-based tool to Analyze Semantic Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational biology, biological entities such as genes or proteins are\nusually annotated with terms extracted from Gene Ontology (GO). The functional\nsimilarity among terms of an ontology is evaluated by using Semantic Similarity\nMeasures (SSM). More recently, the extensive application of SSMs yielded to the\nSemantic Similarity Networks (SSNs). SSNs are edge-weighted graphs where the\nnodes are concepts (e.g. proteins) and each edge has an associated weight that\nrepresents the semantic similarity among related pairs of nodes. The analysis\nof SSNs may reveal biologically meaningful knowledge. For these aims, the need\nfor the introduction of tool able to manage and analyze SSN arises.\nConsequently we developed SSN-Analyzer a web based tool able to build and\npreprocess SSN. As proof of concept we demonstrate that community detection\nalgorithms applied to filtered (thresholded) networks, have better performances\nin terms of biological relevance of the results, with respect to the use of raw\nunfiltered networks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 13:18:39 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Cannataro", "Mario", ""], ["Guzzi", "Pietro Hiram", ""], ["Milano", "Marianna", ""], ["Veltri", "Pierangelo", ""]]}, {"id": "1412.7585", "submitter": "Jia Xu", "authors": "Jia Xu, Patrick Shironoshita, Ubbo Visser, Nigel John, Mansur Kabuka", "title": "Converting Instance Checking to Subsumption: A Rethink for Object\n  Queries over Practical Ontologies", "comments": null, "journal-ref": "International Journal of Intelligence Science, Vol. 5 No. 1,\n  44-62, 2015", "doi": "10.4236/ijis.2015.51005", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently querying Description Logic (DL) ontologies is becoming a vital\ntask in various data-intensive DL applications. Considered as a basic service\nfor answering object queries over DL ontologies, instance checking can be\nrealized by using the most specific concept (MSC) method, which converts\ninstance checking into subsumption problems. This method, however, loses its\nsimplicity and efficiency when applied to large and complex ontologies, as it\ntends to generate very large MSC's that could lead to intractable reasoning. In\nthis paper, we propose a revision to this MSC method for DL SHI, allowing it to\ngenerate much simpler and smaller concepts that are specific-enough to answer a\ngiven query. With independence between computed MSC's, scalability for query\nanswering can also be achieved by distributing and parallelizing the\ncomputations. An empirical evaluation shows the efficacy of our revised MSC\nmethod and the significant efficiency achieved when using it for answering\nobject queries.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 02:18:01 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 20:23:48 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 17:18:41 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Xu", "Jia", ""], ["Shironoshita", "Patrick", ""], ["Visser", "Ubbo", ""], ["John", "Nigel", ""], ["Kabuka", "Mansur", ""]]}, {"id": "1412.7927", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, and J.K. Sahoo", "title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a\n  RNN-DBN", "comments": "8 pages, A4, 1 figure, 1 table, ICANN 2014 oral presentation. arXiv\n  admin note: text overlap with arXiv:1206.6392 by other authors", "journal-ref": "Lecture Notes in Computer Science Volume 8681, 2014, pp 217-224", "doi": "10.1007/978-3-319-11179-7_28", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic technique to model temporal dependencies\nand sequences using a combination of a recurrent neural network and a Deep\nBelief Network. Our technique, RNN-DBN, is an amalgamation of the memory state\nof the RNN that allows it to provide temporal information and a multi-layer DBN\nthat helps in high level representation of the data. This makes RNN-DBNs ideal\nfor sequence generation. Further, the use of a DBN in conjunction with the RNN\nmakes this model capable of significantly more complex data representation than\nan RBM. We apply this technique to the task of polyphonic music generation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 11:08:42 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Sahoo", "J. K.", ""]]}, {"id": "1412.7961", "submitter": "Martin Homola", "authors": "Marjan Alirezaie and Amy Loutfi", "title": "Reasoning for Improved Sensor Data Interpretation in a Smart Home", "comments": "ARCOE-Logic 2014 Workshop Notes, pp. 1-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an ontological representation and reasoning paradigm has been\nproposed for interpretation of time-series signals. The signals come from\nsensors observing a smart environment. The signal chosen for the annotation\nprocess is a set of unintuitive and complex gas sensor data. The ontology of\nthis paradigm is inspired form the SSN ontology (Semantic Sensor Network) and\nused for representation of both the sensor data and the contextual information.\nThe interpretation process is mainly done by an incremental ASP solver which as\ninput receives a logic program that is generated from the contents of the\nontology. The contextual information together with high level domain knowledge\ngiven in the ontology are used to infer explanations (answer sets) for changes\nin the ambient air detected by the gas sensors.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 17:38:19 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Alirezaie", "Marjan", ""], ["Loutfi", "Amy", ""]]}, {"id": "1412.7964", "submitter": "Martin Homola", "authors": "Loris Bozzato and Luciano Serafini", "title": "Knowledge Propagation in Contextualized Knowledge Repositories: an\n  Experimental Evaluation", "comments": "ARCOE-Logic 2014 Workshop Notes, pp. 13-24", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the interest in the representation of context dependent knowledge in the\nSemantic Web has been recognized, a number of logic based solutions have been\nproposed in this regard. In our recent works, in response to this need, we\npresented the description logic-based Contextualized Knowledge Repository (CKR)\nframework. CKR is not only a theoretical framework, but it has been effectively\nimplemented over state-of-the-art tools for the management of Semantic Web\ndata: inference inside and across contexts has been realized in the form of\nforward SPARQL-based rules over different RDF named graphs. In this paper we\npresent the first evaluation results for such CKR implementation. In\nparticular, in first experiment we study its scalability with respect to\ndifferent reasoning regimes. In a second experiment we analyze the effects of\nknowledge propagation on the computation of inferences.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 18:00:45 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Bozzato", "Loris", ""], ["Serafini", "Luciano", ""]]}, {"id": "1412.7965", "submitter": "Martin Homola", "authors": "Diego Calvanese, \\.Ismail \\.Ilkan Ceylan, Marco Montali, and Ario\n  Santoso", "title": "Adding Context to Knowledge and Action Bases", "comments": "ARCOE-Logic 2014 Workshop Notes, pp. 25-36", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge and Action Bases (KABs) have been recently proposed as a formal\nframework to capture the dynamics of systems which manipulate Description Logic\n(DL) Knowledge Bases (KBs) through action execution. In this work, we enrich\nthe KAB setting with contextual information, making use of different context\ndimensions. On the one hand, context is determined by the environment using\ncontext-changing actions that make use of the current state of the KB and the\ncurrent context. On the other hand, it affects the set of TBox assertions that\nare relevant at each time point, and that have to be considered when processing\nqueries posed over the KAB. Here we extend to our enriched setting the results\non verification of rich temporal properties expressed in mu-calculus, which had\nbeen established for standard KABs. Specifically, we show that under a\nrun-boundedness condition, verification stays decidable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 18:14:20 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Calvanese", "Diego", ""], ["Ceylan", "\u0130smail \u0130lkan", ""], ["Montali", "Marco", ""], ["Santoso", "Ario", ""]]}, {"id": "1412.7967", "submitter": "Martin Homola", "authors": "Martin Homola and Theodore Patkos", "title": "Different Types of Conflicting Knowledge in AmI Environments", "comments": "ARCOE-Logic 2014 Workshop Notes, pp. 37-43", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize different types of conflicts that may occur in complex\ndistributed multi-agent scenarios, such as in Ambient Intelligence (AmI)\nenvironments, and we argue that these conflicts should be resolved in a\nsuitable order and with the appropriate strategies for each individual conflict\ntype. We call for further research with the goal of turning conflict resolution\nin AmI environments and similar multi-agent domains into a more coordinated and\nagreed upon process.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 18:21:20 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Homola", "Martin", ""], ["Patkos", "Theodore", ""]]}, {"id": "1412.7968", "submitter": "Martin Homola", "authors": "Martin Ringsquandl, Steffen Lamparter, and Raffaello Lepratti", "title": "Context-Aware Analytics in MOM Applications", "comments": "ARCOE-Logic 2014 Workshop Notes, pp. 44-49", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manufacturing Operations Management (MOM) systems are complex in the sense\nthat they integrate data from heterogeneous systems inside the automation\npyramid. The need for context-aware analytics arises from the dynamics of these\nsystems that influence data generation and hamper comparability of analytics,\nespecially predictive models (e.g. predictive maintenance), where concept drift\naffects application of these models in the future. Recently, an increasing\namount of research has been directed towards data integration using semantic\ncontext models. Manual construction of such context models is an elaborate and\nerror-prone task. Therefore, we pose the challenge to apply combinations of\nknowledge extraction techniques in the domain of analytics in MOM, which\ncomprises the scope of data integration within Product Life-cycle Management\n(PLM), Enterprise Resource Planning (ERP), and Manufacturing Execution Systems\n(MES). We describe motivations, technological challenges and show benefits of\ncontext-aware analytics, which leverage from and regard the interconnectedness\nof semantic context data. Our example scenario shows the need for distribution\nand effective change tracking of context information.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 18:32:58 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Ringsquandl", "Martin", ""], ["Lamparter", "Steffen", ""], ["Lepratti", "Raffaello", ""]]}, {"id": "1412.7978", "submitter": "Daniel Kovach Jr.", "authors": "Daniel Kovach", "title": "The Computational Theory of Intelligence: Information Entropy", "comments": "Published at\n  http://www.scirp.org/journal/PaperInformation.aspx?PaperID=50204", "journal-ref": "International Journal of Modern Nonlinear Theory and Application,\n  3, 182-190 (2014)", "doi": "10.4236/ijmnta.2014.34020", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an information theoretic approach to the concept of\nintelligence in the computational sense. We introduce a probabilistic framework\nfrom which computational intelligence is shown to be an entropy minimizing\nprocess at the local level. Using this new scheme, we develop a simple data\ndriven clustering example and discuss its applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 07:41:45 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kovach", "Daniel", ""]]}, {"id": "1412.8520", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield, Ryan G. James, Sarah Marzen, Dowman P. Varn", "title": "Understanding and Designing Complex Systems: Response to \"A framework\n  for optimal high-level descriptions in science and engineering---preliminary\n  report\"", "comments": "6 pages; http://csc.ucdavis.edu/~cmg/compmech/pubs/ssc_comment.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.AI cs.CE cs.IT math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recount recent history behind building compact models of nonlinear,\ncomplex processes and identifying their relevant macroscopic patterns or\n\"macrostates\". We give a synopsis of computational mechanics, predictive\nrate-distortion theory, and the role of information measures in monitoring\nmodel complexity and predictive performance. Computational mechanics provides a\nmethod to extract the optimal minimal predictive model for a given process.\nRate-distortion theory provides methods for systematically approximating such\nmodels. We end by commenting on future prospects for developing a general\nframework that automatically discovers optimal compact models. As a response to\nthe manuscript cited in the title above, this brief commentary corrects\npotentially misleading claims about its state space compression method and\nplaces it in a broader historical setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:11:40 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Crutchfield", "James P.", ""], ["James", "Ryan G.", ""], ["Marzen", "Sarah", ""], ["Varn", "Dowman P.", ""]]}, {"id": "1412.8529", "submitter": "Jose Hernandez-Orallo", "authors": "Jose Hernandez-Orallo", "title": "A note about the generalisation of the C-tests", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory note we ask the question of what a measure of performance\nfor all tasks is like if we use a weighting of tasks based on a difficulty\nfunction. This difficulty function depends on the complexity of the\n(acceptable) solution for the task (instead of a universal distribution over\ntasks or an adaptive test). The resulting aggregations and decompositions are\n(now retrospectively) seen as the natural (and trivial) interactive\ngeneralisation of the C-tests.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:48:10 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 00:27:30 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Hernandez-Orallo", "Jose", ""]]}, {"id": "1412.8531", "submitter": "Martin Homola", "authors": "Michael Fink, Martin Homola, and Alessandra Mileo", "title": "Workshop Notes of the 6th International Workshop on Acquisition,\n  Representation and Reasoning about Context with Logic (ARCOE-Logic 2014)", "comments": "ARCOE-Logic 2014, 5 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARCOE-Logic 2014, the 6th International Workshop on Acquisition,\nRepresentation and Reasoning about Context with Logic, was held in co-location\nwith the 19th International Conference on Knowledge Engineering and Knowledge\nManagement (EKAW 2014) on November 25, 2014 in Link\\\"oping, Sweden. These notes\ncontain the five papers which were accepted and presented at the workshop.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:55:41 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Fink", "Michael", ""], ["Homola", "Martin", ""], ["Mileo", "Alessandra", ""]]}, {"id": "1412.8704", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Sandro Sozzo and Tomas Veloz", "title": "Quantum Structure in Cognition and the Foundations of Human Reasoning", "comments": "11 pages, no figures", "journal-ref": "International Journal of Theoretical Physics, 54, pp 4557-4569,\n  2015", "doi": "10.1007/s10773-015-2717-9", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cognitive science rests on a foundation of classical logic and\nprobability theory. This foundation has been seriously challenged by several\nfindings in experimental psychology on human decision making. Meanwhile, the\nformalism of quantum theory has provided an efficient resource for modeling\nthese classically problematical situations. In this paper, we start from our\nsuccessful quantum-theoretic approach to the modeling of concept combinations\nto formulate a unifying explanatory hypothesis. In it, human reasoning is the\nsuperposition of two processes -- a conceptual reasoning, whose nature is\nemergence of new conceptuality, and a logical reasoning, founded on an\nalgebraic calculus of the logical type. In most cognitive processes however,\nthe former reasoning prevails over the latter. In this perspective, the\nobserved deviations from classical logical reasoning should not be interpreted\nas biases but, rather, as natural expressions of emergence in its deepest form.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 17:51:16 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}]