[{"id": "1404.0099", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Daniel Selsam and Yura Perov", "title": "Venture: a higher-order probabilistic programming platform with\n  programmable inference", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Venture, an interactive virtual machine for probabilistic\nprogramming that aims to be sufficiently expressive, extensible, and efficient\nfor general-purpose use. Like Church, probabilistic models and inference\nproblems in Venture are specified via a Turing-complete, higher-order\nprobabilistic language descended from Lisp. Unlike Church, Venture also\nprovides a compositional language for custom inference strategies built out of\nscalable exact and approximate techniques. We also describe four key aspects of\nVenture's implementation that build on ideas from probabilistic graphical\nmodels. First, we describe the stochastic procedure interface (SPI) that\nspecifies and encapsulates primitive random variables. The SPI supports custom\ncontrol flow, higher-order probabilistic procedures, partially exchangeable\nsequences and ``likelihood-free'' stochastic simulators. It also supports\nexternal models that do inference over latent variables hidden from Venture.\nSecond, we describe probabilistic execution traces (PETs), which represent\nexecution histories of Venture programs. PETs capture conditional dependencies,\nexistential dependencies and exchangeable coupling. Third, we describe\npartitions of execution histories called scaffolds that factor global inference\nproblems into coherent sub-problems. Finally, we describe a family of\nstochastic regeneration algorithms for efficiently modifying PET fragments\ncontained within scaffolds. Stochastic regeneration linear runtime scaling in\ncases where many previous approaches scaled quadratically. We show how to use\nstochastic regeneration and the SPI to implement general-purpose inference\nstrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals\nbased on particle Markov chain Monte Carlo and mean-field variational inference\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 01:44:05 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Selsam", "Daniel", ""], ["Perov", "Yura", ""]]}, {"id": "1404.0540", "submitter": "Xinyang Deng", "authors": "Li Gou, Yong Deng, Rehan Sadiq, Sankaran Mahadevan", "title": "Modeling contaminant intrusion in water distribution networks based on D\n  numbers", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient modeling on uncertain information plays an important role in\nestimating the risk of contaminant intrusion in water distribution networks.\nDempster-Shafer evidence theory is one of the most commonly used methods.\nHowever, the Dempster-Shafer evidence theory has some hypotheses including the\nexclusive property of the elements in the frame of discernment, which may not\nbe consistent with the real world. In this paper, based on a more effective\nrepresentation of uncertainty, called D numbers, a new method that allows the\nelements in the frame of discernment to be non-exclusive is proposed. To\ndemonstrate the efficiency of the proposed method, we apply it to the water\ndistribution networks to estimate the risk of contaminant intrusion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 13:07:06 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Gou", "Li", ""], ["Deng", "Yong", ""], ["Sadiq", "Rehan", ""], ["Mahadevan", "Sankaran", ""]]}, {"id": "1404.0640", "submitter": "Akin Osman Kazakci", "authors": "Akin Osman Kazakci (CGS)", "title": "Conceptive Artificial Intelligence: Insights from design theory", "comments": null, "journal-ref": "International Design Conference DESIGN2014, Croatia (2014)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper offers a perspective on what we term conceptive\nintelligence - the capacity of an agent to continuously think of new object\ndefinitions (tasks, problems, physical systems, etc.) and to look for methods\nto realize them. The framework, called a Brouwer machine, is inspired by\nprevious research in design theory and modeling, with its roots in the\nconstructivist mathematics of intuitionism. The dual constructivist perspective\nwe describe offers the possibility to create novelty both in terms of the types\nof objects and the methods for constructing objects. More generally, the\ntheoretical work on which Brouwer machines are based is called imaginative\nconstructivism. Based on the framework and the theory, we discuss many\nparadigms and techniques omnipresent in AI research and their merits and\nshortcomings for modeling aspects of design, as described by imaginative\nconstructivism. To demonstrate and explain the type of creative process\nexpressed by the notion of a Brouwer machine, we compare this concept with a\nsystem using genetic algorithms for scientific law discovery.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 18:06:40 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Kazakci", "Akin Osman", "", "CGS"]]}, {"id": "1404.0837", "submitter": "EPTCS", "authors": "Francesco Belardinelli (Universit\\'e d'Evry)", "title": "Reasoning about Knowledge and Strategies: Epistemic Strategy Logic", "comments": "In Proceedings SR 2014, arXiv:1404.0414", "journal-ref": "EPTCS 146, 2014, pp. 27-33", "doi": "10.4204/EPTCS.146.4", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Epistemic Strategy Logic (ESL), an extension of\nStrategy Logic with modal operators for individual knowledge. This enhanced\nframework allows us to represent explicitly and to reason about the knowledge\nagents have of their own and other agents' strategies. We provide a semantics\nto ESL in terms of epistemic concurrent game models, and consider the\ncorresponding model checking problem. We show that the complexity of model\nchecking ESL is not worse than (non-epistemic) Strategy Logic\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 10:37:59 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Belardinelli", "Francesco", "", "Universit\u00e9 d'Evry"]]}, {"id": "1404.0841", "submitter": "EPTCS", "authors": "Cl\\'audia Nalon (Department of Computer Science, University of\n  Bras\\'ilia, Brazil), Lan Zhang (Information School Capital University of\n  Economics and Business, China), Clare Dixon (Department of Computer Science,\n  University of Liverpool, UK), Ullrich Hustadt (Department of Computer\n  Science, University of Liverpool, UK)", "title": "A Resolution Prover for Coalition Logic", "comments": "In Proceedings SR 2014, arXiv:1404.0414", "journal-ref": "EPTCS 146, 2014, pp. 65-73", "doi": "10.4204/EPTCS.146.9", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prototype tool for automated reasoning for Coalition Logic, a\nnon-normal modal logic that can be used for reasoning about cooperative agency.\nThe theorem prover CLProver is based on recent work on a resolution-based\ncalculus for Coalition Logic that operates on coalition problems, a normal form\nfor Coalition Logic. We provide an overview of coalition problems and of the\nresolution-based calculus for Coalition Logic. We then give details of the\nimplementation of CLProver and present the results for a comparison with an\nexisting tableau-based solver.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 10:38:34 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Nalon", "Cl\u00e1udia", "", "Department of Computer Science, University of\n  Bras\u00edlia, Brazil"], ["Zhang", "Lan", "", "Information School Capital University of\n  Economics and Business, China"], ["Dixon", "Clare", "", "Department of Computer Science,\n  University of Liverpool, UK"], ["Hustadt", "Ullrich", "", "Department of Computer\n  Science, University of Liverpool, UK"]]}, {"id": "1404.0854", "submitter": "EPTCS", "authors": "Wei Bai, Emmanuel M. Tadjouddine, Yu Guo", "title": "Enabling Automatic Certification of Online Auctions", "comments": "In Proceedings FESCA 2014, arXiv:1404.0436", "journal-ref": "EPTCS 147, 2014, pp. 123-132", "doi": "10.4204/EPTCS.147.9", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of building up trust in a network of online auctions\nby software agents. This requires agents to have a deeper understanding of\nauction mechanisms and be able to verify desirable properties of a given\nmechanism. We have shown how these mechanisms can be formalised as semantic web\nservices in OWL-S, a good enough expressive machine-readable formalism enabling\nsoftware agents, to discover, invoke, and execute a web service. We have also\nused abstract interpretation to translate the auction's specifications from\nOWL-S, based on description logic, to COQ, based on typed lambda calculus, in\norder to enable automatic verification of desirable properties of the auction\nby the software agents. For this language translation, we have discussed the\nsyntactic transformation as well as the semantics connections between both\nconcrete and abstract domains. This work contributes to the implementation of\nthe vision of agent-mediated e-commerce systems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 10:45:01 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Bai", "Wei", ""], ["Tadjouddine", "Emmanuel M.", ""], ["Guo", "Yu", ""]]}, {"id": "1404.0904", "submitter": "Attila Peth\\H{o}", "authors": "L. Aszal\\'os, L. Hajdu and A. Peth\\H{o}", "title": "On a correlational clustering of integers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation clustering is a concept of machine learning. The ultimate goal of\nsuch a clustering is to find a partition with minimal conflicts. In this paper\nwe investigate a correlation clustering of integers, based upon the greatest\ncommon divisor.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 13:29:25 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Aszal\u00f3s", "L.", ""], ["Hajdu", "L.", ""], ["Peth\u0151", "A.", ""]]}, {"id": "1404.0953", "submitter": "Jochen Burghardt", "authors": "Jochen Burghardt and Birgit Heinz", "title": "Implementing Anti-Unification Modulo Equational Theory", "comments": "113 pages; 57 figures", "journal-ref": "Technical Report \"Arbeitspapiere der GMD\",ISSN 0723-0508,\n  Vol.1006, June 1996", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of E-anti-unification as defined in Heinz\n(1995), where tree-grammar descriptions of equivalence classes of terms are\nused to compute generalizations modulo equational theories. We discuss several\nimprovements, including an efficient implementation of variable-restricted\nE-anti-unification from Heinz (1995), and give some runtime figures about them.\nWe present applications in various areas, including lemma generation in\nequational inductive proofs, intelligence tests, diverging Knuth-Bendix\ncompletion, strengthening of induction hypotheses, and theory formation about\nfinite algebras.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 21:28:53 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 09:16:46 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Burghardt", "Jochen", ""], ["Heinz", "Birgit", ""]]}, {"id": "1404.1140", "submitter": "Christopher Amato", "authors": "Christopher Amato, Frans A. Oliehoek", "title": "Scalable Planning and Learning for Multiagent POMDPs: Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online, sample-based planning algorithms for POMDPs have shown great promise\nin scaling to problems with large state spaces, but they become intractable for\nlarge action and observation spaces. This is particularly problematic in\nmultiagent POMDPs where the action and observation space grows exponentially\nwith the number of agents. To combat this intractability, we propose a novel\nscalable approach based on sample-based planning and factored value functions\nthat exploits structure present in many multiagent settings. This approach\napplies not only in the planning case, but also in the Bayesian reinforcement\nlearning setting. Experimental results show that we are able to provide high\nquality solutions to large multiagent planning and learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 03:02:44 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 03:28:34 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Amato", "Christopher", ""], ["Oliehoek", "Frans A.", ""]]}, {"id": "1404.1511", "submitter": "Aske Plaat", "authors": "Aske Plaat", "title": "MTD(f), A Minimax Algorithm Faster Than NegaScout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  MTD(f) is a new minimax search algorithm, simpler and more efficient than\nprevious algorithms. In tests with a number of tournament game playing programs\nfor chess, checkers and Othello it performed better, on average, than\nNegaScout/PVS (the AlphaBeta variant used in practically all good chess,\ncheckers, and Othello programs). One of the strongest chess programs of the\nmoment, MIT's parallel chess program Cilkchess uses MTD(f) as its search\nalgorithm, replacing NegaScout, which was used in StarSocrates, the previous\nversion of the program.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 19:51:05 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Plaat", "Aske", ""]]}, {"id": "1404.1515", "submitter": "Aske Plaat", "authors": "Aske Plaat, Jonathan Schaeffer, Wim Pijls, Arie de Bruin", "title": "A New Paradigm for Minimax Search", "comments": "Novag Award 1994-1995 Best Computer Chess publication", "journal-ref": null, "doi": null, "report-no": "Univ Alberta TR 94-18", "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper introduces a new paradigm for minimax game-tree search algo-\nrithms. MT is a memory-enhanced version of Pearls Test procedure. By changing\nthe way MT is called, a number of best-first game-tree search algorithms can be\nsimply and elegantly constructed (including SSS*). Most of the assessments of\nminimax search algorithms have been based on simulations. However, these\nsimulations generally do not address two of the key ingredients of high\nperformance game-playing programs: iterative deepening and memory usage. This\npaper presents experimental data from three game-playing programs (checkers,\nOthello and chess), covering the range from low to high branching factor. The\nimproved move ordering due to iterative deepening and memory usage results in\nsignificantly different results from those portrayed in the literature. Whereas\nsome simulations show Alpha-Beta expanding almost 100% more leaf nodes than\nother algorithms [12], our results showed variations of less than 20%. One new\ninstance of our framework (MTD-f) out-performs our best alpha- beta searcher\n(aspiration NegaScout) on leaf nodes, total nodes and execution time. To our\nknowledge, these are the first reported results that compare both depth-first\nand best-first algorithms given the same amount of memory\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 20:05:31 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Plaat", "Aske", ""], ["Schaeffer", "Jonathan", ""], ["Pijls", "Wim", ""], ["de Bruin", "Arie", ""]]}, {"id": "1404.1517", "submitter": "Aske Plaat", "authors": "Aske Plaat, Jonathan Schaeffer, Wim Pijls, Arie de Bruin", "title": "SSS* = Alpha-Beta + TT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In 1979 Stockman introduced the SSS* minimax search algorithm that domi-\nnates Alpha-Beta in the number of leaf nodes expanded. Further investigation of\nthe algorithm showed that it had three serious drawbacks, which prevented its\nuse by practitioners: it is difficult to understand, it has large memory\nrequirements, and it is slow. This paper presents an alternate formulation of\nSSS*, in which it is implemented as a series of Alpha-Beta calls that use a\ntransposition table (AB- SSS*). The reformulation solves all three perceived\ndrawbacks of SSS*, making it a practical algorithm. Further, because the search\nis now based on Alpha-Beta, the extensive research on minimax search\nenhancements can be easily integrated into AB-SSS*. To test AB-SSS* in\npractise, it has been implemented in three state-of-the- art programs: for\ncheckers, Othello and chess. AB-SSS* is comparable in performance to Alpha-Beta\non leaf node count in all three games, making it a viable alternative to\nAlpha-Beta in practise. Whereas SSS* has usually been regarded as being\nentirely different from Alpha-Beta, it turns out to be just an Alpha-Beta\nenhancement, like null-window searching. This runs counter to published\nsimulation results. Our research leads to the surprising result that iterative\ndeepening versions of Alpha-Beta can expand fewer leaf nodes than iterative\ndeepening versions of SSS* due to dynamic move re-ordering.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 20:09:58 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Plaat", "Aske", ""], ["Schaeffer", "Jonathan", ""], ["Pijls", "Wim", ""], ["de Bruin", "Arie", ""]]}, {"id": "1404.1518", "submitter": "Aske Plaat", "authors": "Aske Plaat, Jonathan Schaeffer, Wim Pijls, Arie de Bruin", "title": "Nearly Optimal Minimax Tree Search?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Knuth and Moore presented a theoretical lower bound on the number of leaves\nthat any fixed-depth minimax tree-search algorithm traversing a uniform tree\nmust explore, the so-called minimal tree. Since real-life minimax trees are not\nuniform, the exact size of this tree is not known for most applications.\nFurther, most games have transpositions, implying that there exists a minimal\ngraph which is smaller than the minimal tree. For three games (chess, Othello\nand checkers) we compute the size of the minimal tree and the minimal graph.\nEmpirical evidence shows that in all three games, enhanced Alpha-Beta search is\ncapable of building a tree that is close in size to that of the minimal graph.\nHence, it appears game-playing programs build nearly optimal search trees.\nHowever, the conventional definition of the minimal graph is wrong. There are\nways in which the size of the minimal graph can be reduced: by maximizing the\nnumber of transpositions in the search, and generating cutoffs using branches\nthat lead to smaller search trees. The conventional definition of the minimal\ngraph is just a left-most approximation. Calculating the size of the real\nminimal graph is too computationally intensive. However, upper bound\napproximations show it to be significantly smaller than the left-most minimal\ngraph. Hence, it appears that game-playing programs are not searching as\nefficiently as is widely believed. Understanding the left-most and real minimal\nsearch graphs leads to some new ideas for enhancing Alpha-Beta search. One of\nthem, enhanced transposition cutoffs, is shown to significantly reduce search\ntree size.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 20:13:58 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Plaat", "Aske", ""], ["Schaeffer", "Jonathan", ""], ["Pijls", "Wim", ""], ["de Bruin", "Arie", ""]]}, {"id": "1404.1685", "submitter": "Guido Governatori", "authors": "Guido Governatori", "title": "Thou Shalt is not You Will", "comments": null, "journal-ref": "Fifteenth International Conference on Artificial Intelligence and\n  Law (ICAIL 2015), pp. 63-68", "doi": "10.1145/2746090.2746105", "report-no": "NICTA Technical Report 8026", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss some reasons why temporal logic might not be\nsuitable to model real life norms. To show this, we present a novel deontic\nlogic contrary-to-duty/derived permission paradox based on the interaction of\nobligations, permissions and contrary-to-duty obligations. The paradox is\ninspired by real life norms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 08:11:10 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 07:32:27 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 15:14:33 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Governatori", "Guido", ""]]}, {"id": "1404.1718", "submitter": "Gabriel Leuenberger", "authors": "Gabriel Leuenberger", "title": "Applications of Algorithmic Probability to the Philosophy of Mind", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents formulae that can solve various seemingly hopeless\nphilosophical conundrums. We discuss the simulation argument, teleportation,\nmind-uploading, the rationality of utilitarianism, and the ethics of exploiting\nartificial general intelligence. Our approach arises from combining the\nessential ideas of formalisms such as algorithmic probability, the universal\nintelligence measure, space-time-embedded intelligence, and Hutter's observer\nlocalization. We argue that such universal models can yield the ultimate\nsolutions, but a novel research direction would be required in order to find\ncomputationally efficient approximations thereof.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 10:02:47 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 05:31:18 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 19:20:38 GMT"}, {"version": "v4", "created": "Sun, 27 Jul 2014 01:49:51 GMT"}, {"version": "v5", "created": "Fri, 31 Oct 2014 04:37:52 GMT"}, {"version": "v6", "created": "Sun, 27 Mar 2016 13:07:44 GMT"}, {"version": "v7", "created": "Mon, 18 Apr 2016 16:55:19 GMT"}, {"version": "v8", "created": "Thu, 5 Jan 2017 16:51:55 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Leuenberger", "Gabriel", ""]]}, {"id": "1404.1812", "submitter": "Anugrah Kumar", "authors": "Anugrah Kumar", "title": "Determining the Consistency factor of Autopilot using Rough Set Theory", "comments": "IEEE International Conference on Networking, Sensing and Control 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autopilot is a system designed to guide a vehicle without aid. Due to\nincrease in flight hours and complexity of modern day flight it has become\nimperative to equip the aircrafts with autopilot. Thus reliability and\nconsistency of an Autopilot system becomes a crucial role in a flight. But the\nincreased complexity and demand for better accuracy has made the process of\nevaluating the autopilot for consistency a difficult process .A vast amount of\nimprecise data has been involved. Rough sets can be a potent tool for such kind\nof Applications containing vague data. This paper proposes an approach towards\nConsistency factor determination using Rough Set Theory. The seventeen basic\nfactors, that are crucial in determining the consistency of an Autopilot\nsystem, are grouped into five Payloads based on their functionality.\nConsistency Factor is evaluated through these payloads, using Rough Set Theory.\nConsistency Factor determines the consistency and reliability of an autopilot\nsystem and the conditions under which manual override becomes imperative. Using\nRough set Theory the most and the least influential factors towards Autopilot\nsystem are also determined.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 15:08:09 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kumar", "Anugrah", ""]]}, {"id": "1404.1884", "submitter": "Guoming Tang", "authors": "Guoming Tang, Kui Wu, Jingsheng Lei, and Jiuyang Tang", "title": "Plug and Play! A Simple, Universal Model for Energy Disaggregation", "comments": "12 pages, 5 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation is to discover the energy consumption of individual\nappliances from their aggregated energy values. To solve the problem, most\nexisting approaches rely on either appliances' signatures or their state\ntransition patterns, both hard to obtain in practice. Aiming at developing a\nsimple, universal model that works without depending on sophisticated machine\nlearning techniques or auxiliary equipments, we make use of easily accessible\nknowledge of appliances and the sparsity of the switching events to design a\nSparse Switching Event Recovering (SSER) method. By minimizing the total\nvariation (TV) of the (sparse) event matrix, SSER can effectively recover the\nindividual energy consumption values from the aggregated ones. To speed up the\nprocess, a Parallel Local Optimization Algorithm (PLOA) is proposed to solve\nthe problem in active epochs of appliance activities in parallel. Using\nreal-world trace data, we compare the performance of our method with that of\nthe state-of-the-art solutions, including Least Square Estimation (LSE) and\niterative Hidden Markov Model (HMM). The results show that our approach has an\noverall higher detection accuracy and a smaller overhead.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 19:02:30 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Tang", "Guoming", ""], ["Wu", "Kui", ""], ["Lei", "Jingsheng", ""], ["Tang", "Jiuyang", ""]]}, {"id": "1404.2116", "submitter": "Tshilidzi Marwala", "authors": "Tshilidzi Marwala", "title": "Rational Counterfactuals", "comments": "To appear in Artificial Intelligence for Rational Decision Making\n  (Springer-Verlag)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the concept of rational countefactuals which is an idea\nof identifying a counterfactual from the factual (whether perceived or real)\nthat maximizes the attainment of the desired consequent. In counterfactual\nthinking if we have a factual statement like: Saddam Hussein invaded Kuwait and\nconsequently George Bush declared war on Iraq then its counterfactuals is: If\nSaddam Hussein did not invade Kuwait then George Bush would not have declared\nwar on Iraq. The theory of rational counterfactuals is applied to identify the\nantecedent that gives the desired consequent necessary for rational decision\nmaking. The rational countefactual theory is applied to identify the values of\nvariables Allies, Contingency, Distance, Major Power, Capability, Democracy, as\nwell as Economic Interdependency that gives the desired consequent Peace.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 13:15:06 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Marwala", "Tshilidzi", ""]]}, {"id": "1404.2162", "submitter": "Georg Kaes", "authors": "Georg Kaes, J\\\"urgen Manger, Stefanie Rinderle-Ma, Ralph Vigne", "title": "The NNN Formalization: Review and Development of Guideline Specification\n  in the Care Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to an ageing society, it can be expected that less nursing personnel will\nbe responsible for an increasing number of patients in the future. One way to\naddress this challenge is to provide system-based support for nursing personnel\nin creating, executing, and adapting patient care processes. In care practice,\nthese processes are following the general care process definition and\nindividually specified according to patient-specific data as well as diagnoses\nand guidelines from the NANDA, NIC, and NOC (NNN) standards. In addition,\nadaptations to running patient processes become necessary frequently and are to\nbe conducted by nursing personnel including NNN knowledge. In order to provide\nsemi-automatic support for design and adaption of care processes, a\nformalization of NNN knowledge is indispensable. This technical report presents\nthe NNN formalization that is developed targeting at goals such as\ncompleteness, flexibility, and later exploitation for creating and adapting\npatient care processes. The formalization also takes into consideration an\nextensive evaluation of existing formalization standards for clinical\nguidelines. The NNN formalization as well as its usage are evaluated based on\ncase study FATIGUE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:50:53 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Kaes", "Georg", ""], ["Manger", "J\u00fcrgen", ""], ["Rinderle-Ma", "Stefanie", ""], ["Vigne", "Ralph", ""]]}, {"id": "1404.2267", "submitter": "Peter van der Helm", "authors": "Peter A. van der Helm", "title": "Transparallel mind: Classical computing with quantum power", "comments": "38 pages (incl. Appendix with proofs), 10 figures, Supplementary\n  Material (incl. algorithm) available at\n  http://perswww.kuleuven.be/~u0084530/doc/pisa.html. Minor revision: added 2\n  figures, 7 references, and a few clarifications", "journal-ref": null, "doi": "10.1007/s10462-015-9429-7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the extraordinary computing power promised by quantum computers,\nthe quantum mind hypothesis postulated that quantum mechanical phenomena are\nthe source of neuronal synchronization, which, in turn, might underlie\nconsciousness. Here, I present an alternative inspired by a classical computing\nmethod with quantum power. This method relies on special distributed\nrepresentations called hyperstrings. Hyperstrings are superpositions of up to\nan exponential number of strings, which -- by a single-processor classical\ncomputer -- can be evaluated in a transparallel fashion, that is,\nsimultaneously as if only one string were concerned. Building on a neurally\nplausible model of human visual perceptual organization, in which hyperstrings\nare formal counterparts of transient neural assemblies, I postulate that\nsynchronization in such assemblies is a manifestation of transparallel\ninformation processing. This accounts for the high combinatorial capacity and\nspeed of human visual perceptual organization and strengthens ideas that\nself-organizing cognitive architecture bridges the gap between neurons and\nconsciousness.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 12:36:13 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 15:54:44 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["van der Helm", "Peter A.", ""]]}, {"id": "1404.2313", "submitter": "Eita Nakamura", "authors": "Eita Nakamura, Tomohiko Nakamura, Yasuyuki Saito, Nobutaka Ono,\n  Shigeki Sagayama", "title": "Outer-Product Hidden Markov Model and Polyphonic MIDI Score Following", "comments": "42 pages, 8 figures, version submitted to JNMR. To appear in Journal\n  of New Music Research (2014)", "journal-ref": "Journal of New Music Research, Vol. 43, No. 2 (2014) 183-201", "doi": "10.1080/09298215.2014.884145", "report-no": null, "categories": "cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polyphonic MIDI score-following algorithm capable of following\nperformances with arbitrary repeats and skips, based on a probabilistic model\nof musical performances. It is attractive in practical applications of score\nfollowing to handle repeats and skips which may be made arbitrarily during\nperformances, but the algorithms previously described in the literature cannot\nbe applied to scores of practical length due to problems with large\ncomputational complexity. We propose a new type of hidden Markov model (HMM) as\na performance model which can describe arbitrary repeats and skips including\nperformer tendencies on distributed score positions before and after them, and\nderive an efficient score-following algorithm that reduces computational\ncomplexity without pruning. A theoretical discussion on how much such\ninformation on performer tendencies improves the score-following results is\ngiven. The proposed score-following algorithm also admits performance mistakes\nand is demonstrated to be effective in practical situations by carrying out\nevaluations with human performances. The proposed HMM is potentially valuable\nfor other topics in information processing and we also provide a detailed\ndescription of inference algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 21:48:13 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Nakamura", "Eita", ""], ["Nakamura", "Tomohiko", ""], ["Saito", "Yasuyuki", ""], ["Ono", "Nobutaka", ""], ["Sagayama", "Shigeki", ""]]}, {"id": "1404.2314", "submitter": "Eita Nakamura", "authors": "Eita Nakamura, Nobutaka Ono, Shigeki Sagayama, Kenji Watanabe", "title": "A Stochastic Temporal Model of Polyphonic MIDI Performance with\n  Ornaments", "comments": "35 pages, 6 figures, some explanations and evaluation results added,\n  version accepted to JNMR", "journal-ref": "Journal of New Music Research, Vol. 44, No. 4 (2015) 287-304", "doi": "10.1080/09298215.2015.1078819", "report-no": null, "categories": "cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study indeterminacies in realization of ornaments and how they can be\nincorporated in a stochastic performance model applicable for music information\nprocessing such as score-performance matching. We point out the importance of\ntemporal information, and propose a hidden Markov model which describes it\nexplicitly and represents ornaments with several state types. Following a\nreview of the indeterminacies, they are carefully incorporated into the model\nthrough its topology and parameters, and the state construction for quite\ngeneral polyphonic scores is explained in detail. By analyzing piano\nperformance data, we find significant overlaps in inter-onset-interval\ndistributions of chordal notes, ornaments, and inter-chord events, and the data\nis used to determine details of the model. The model is applied for score\nfollowing and offline score-performance matching, yielding highly accurate\nmatching for performances with many ornaments and relatively frequent errors,\nrepeats, and skips.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 21:48:21 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 00:44:16 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Nakamura", "Eita", ""], ["Ono", "Nobutaka", ""], ["Sagayama", "Shigeki", ""], ["Watanabe", "Kenji", ""]]}, {"id": "1404.2458", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jakub Marecek and Robert Shorten and Jia Yuan Yu", "title": "r-Extreme Signalling for Congestion Control", "comments": null, "journal-ref": "International Journal of Control (2016) 89(10): 1972-1984", "doi": "10.1080/00207179.2016.1146968", "report-no": null, "categories": "math.OC cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many \"smart city\" applications, congestion arises in part due to the\nnature of signals received by individuals from a central authority. In the\nmodel of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each\nagent uses one out of multiple resources at each time instant. The per-use cost\nof a resource depends on the number of concurrent users. A central authority\nhas up-to-date knowledge of the congestion across all resources and uses\nrandomisation to provide a scalar or an interval for each resource at each\ntime. In this paper, the interval to broadcast per resource is obtained by\ntaking the minima and maxima of costs observed within a time window of length\nr, rather than by randomisation. We show that the resulting distribution of\nagents across resources also converges in distribution, under plausible\nassumptions about the evolution of the population over time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 12:20:00 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 11:20:27 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 12:43:24 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Marecek", "Jakub", ""], ["Shorten", "Robert", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1404.2644", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Yingyu Liang, Alireza Bagheri Garakani,\n  Maria-Florina Balcan, Fei Sha", "title": "A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning", "comments": "Extended version of the SIAM Data Mining 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 22:16:39 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 04:08:51 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 15:14:19 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Liang", "Yingyu", ""], ["Garakani", "Alireza Bagheri", ""], ["Balcan", "Maria-Florina", ""], ["Sha", "Fei", ""]]}, {"id": "1404.2768", "submitter": "Einollah Pira", "authors": "Einollah pira, Mohammad Reza Zand Miralvand and Fakhteh Soltani", "title": "Verification of confliction and unreachability in rule-based expert\n  systems with model checking", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to find optimal solutions for structural errors in rule-based\nexpert systems .Solutions to discovering such errors by using model checking\ntechniques have already been proposed, but these solutions have problems such\nas state space explosion. In this paper, to overcome these problems, we model\nthe rule-based systems as finite state transition systems and express\nconfliction and unreachability as Computation Tree Logic (CTL) logic formula\nand then use the technique of model checking to detect confliction and\nunreachability in rule-based systems with the model checker UPPAAL.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 10:55:14 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["pira", "Einollah", ""], ["Miralvand", "Mohammad Reza Zand", ""], ["Soltani", "Fakhteh", ""]]}, {"id": "1404.2984", "submitter": "Kuldeep Meel", "authors": "Supratik Chakraborty, Daniel J. Fremont, Kuldeep S. Meel, Sanjit A.\n  Seshia, Moshe Y. Vardi", "title": "Distribution-Aware Sampling and Weighted Model Counting for SAT", "comments": "This is a full version of AAAI 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a CNF formula and a weight for each assignment of values to variables,\ntwo natural problems are weighted model counting and distribution-aware\nsampling of satisfying assignments. Both problems have a wide variety of\nimportant applications. Due to the inherent complexity of the exact versions of\nthe problems, interest has focused on solving them approximately. Prior work in\nthis area scaled only to small problems in practice, or failed to provide\nstrong theoretical guarantees, or employed a computationally-expensive maximum\na posteriori probability (MAP) oracle that assumes prior knowledge of a\nfactored representation of the weight distribution. We present a novel approach\nthat works with a black-box oracle for weights of assignments and requires only\nan {\\NP}-oracle (in practice, a SAT-solver) to solve both the counting and\nsampling problems. Our approach works under mild assumptions on the\ndistribution of weights of satisfying assignments, provides strong theoretical\nguarantees, and scales to problems involving several thousand variables. We\nalso show that the assumptions can be significantly relaxed while improving\ncomputational efficiency if a factored representation of the weights is known.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 02:24:18 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Fremont", "Daniel J.", ""], ["Meel", "Kuldeep S.", ""], ["Seshia", "Sanjit A.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1404.3141", "submitter": "Mark Kaminski", "authors": "Mark Kaminski and Yavor Nenov and Bernardo Cuenca Grau", "title": "Datalog Rewritability of Disjunctive Datalog Programs and its\n  Applications to Ontology Reasoning", "comments": "14 pages. To appear at AAAI-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of rewriting a disjunctive datalog program into plain\ndatalog. We show that a disjunctive program is rewritable if and only if it is\nequivalent to a linear disjunctive program, thus providing a novel\ncharacterisation of datalog rewritability. Motivated by this result, we propose\nweakly linear disjunctive datalog---a novel rule-based KR language that extends\nboth datalog and linear disjunctive datalog and for which reasoning is\ntractable in data complexity. We then explore applications of weakly linear\nprograms to ontology reasoning and propose a tractable extension of OWL 2 RL\nwith disjunctive axioms. Our empirical results suggest that many non-Horn\nontologies can be reduced to weakly linear programs and that query answering\nover such ontologies using a datalog engine is feasible in practice.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 15:47:40 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Kaminski", "Mark", ""], ["Nenov", "Yavor", ""], ["Grau", "Bernardo Cuenca", ""]]}, {"id": "1404.3285", "submitter": "Mahdi Moeini", "authors": "Mahdi Moeini, Zied Jemai, Evren Sahin", "title": "An Integer Programming Model for the Dynamic Location and Relocation of\n  Emergency Vehicles: A Case Study", "comments": "Proceedings of the 12th International Symposium on Operational\n  Research (SOR'2013), Slovenia, September 2013, pp. 343-350, (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the dynamic Emergency Medical Service (EMS)\nsystems. A dynamic location model is presented that tries to locate and\nrelocate the ambulances. The proposed model controls the movements and\nlocations of ambulances in order to provide a better coverage of the demand\npoints under different fluctuation patterns that may happen during a given\nperiod of time. Some numerical experiments have been carried out by using some\nreal-world data sets that have been collected through the French EMS system.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 12:27:06 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Moeini", "Mahdi", ""], ["Jemai", "Zied", ""], ["Sahin", "Evren", ""]]}, {"id": "1404.3301", "submitter": "William Yang Wang", "authors": "William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom Mitchell, William W.\n  Cohen", "title": "Efficient Inference and Learning in a Large Knowledge Base: Reasoning\n  with Extracted Information using a Locally Groundable First-Order\n  Probabilistic Logic", "comments": "arXiv admin note: substantial text overlap with arXiv:1305.2254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important challenge for probabilistic logics is reasoning with very large\nknowledge bases (KBs) of imperfect information, such as those produced by\nmodern web-scale information extraction systems. One scalability problem shared\nby many probabilistic logics is that answering queries involves \"grounding\" the\nquery---i.e., mapping it to a propositional representation---and the size of a\n\"grounding\" grows with database size. To address this bottleneck, we present a\nfirst-order probabilistic language called ProPPR in which that approximate\n\"local groundings\" can be constructed in time independent of database size.\nTechnically, ProPPR is an extension to stochastic logic programs (SLPs) that is\nbiased towards short derivations; it is also closely related to an earlier\nrelational learning algorithm called the path ranking algorithm (PRA). We show\nthat the problem of constructing proofs for this logic is related to\ncomputation of personalized PageRank (PPR) on a linearized version of the proof\nspace, and using on this connection, we develop a proveably-correct approximate\ngrounding scheme, based on the PageRank-Nibble algorithm. Building on this, we\ndevelop a fast and easily-parallelized weight-learning algorithm for ProPPR. In\nexperiments, we show that learning for ProPPR is orders magnitude faster than\nlearning for Markov logic networks; that allowing mutual recursion (joint\nlearning) in KB inference leads to improvements in performance; and that ProPPR\ncan learn weights for a mutually recursive program with hundreds of clauses,\nwhich define scores of interrelated predicates, over a KB containing one\nmillion entities.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 16:59:30 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Wang", "William Yang", ""], ["Mazaitis", "Kathryn", ""], ["Lao", "Ni", ""], ["Mitchell", "Tom", ""], ["Cohen", "William W.", ""]]}, {"id": "1404.3370", "submitter": "Xinyang Deng", "authors": "Meizhu Li, Qi Zhang, Xinyang Deng, Yong Deng", "title": "Distance function of D numbers", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer theory is widely applied in uncertainty modelling and\nknowledge reasoning due to its ability of expressing uncertain information. A\ndistance between two basic probability assignments(BPAs) presents a measure of\nperformance for identification algorithms based on the evidential theory of\nDempster-Shafer. However, some conditions lead to limitations in practical\napplication for Dempster-Shafer theory, such as exclusiveness hypothesis and\ncompleteness constraint. To overcome these shortcomings, a novel theory called\nD numbers theory is proposed. A distance function of D numbers is proposed to\nmeasure the distance between two D numbers. The distance function of D numbers\nis an generalization of distance between two BPAs, which inherits the advantage\nof Dempster-Shafer theory and strengthens the capability of uncertainty\nmodeling. An illustrative case is provided to demonstrate the effectiveness of\nthe proposed function.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 11:56:08 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Li", "Meizhu", ""], ["Zhang", "Qi", ""], ["Deng", "Xinyang", ""], ["Deng", "Yong", ""]]}, {"id": "1404.3659", "submitter": "Amir Konigsberg", "authors": "Amir Konigsberg", "title": "Avoiding Undesired Choices Using Intelligent Adaptive Systems", "comments": null, "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol. 5, No. 2, March 2014", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a number of heuristics that can be used for identifying when\nintransitive choice behaviour is likely to occur in choice situations. We also\nsuggest two methods for avoiding undesired choice behaviour, namely transparent\ncommunication and adaptive choice-set generation. We believe that these two\nways can contribute to the avoidance of decision biases in choice situations\nthat may often be regretted.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 07:33:04 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Konigsberg", "Amir", ""]]}, {"id": "1404.3675", "submitter": "Clement Carbonnel", "authors": "Clement Carbonnel, Martin C. Cooper and Emmanuel Hebrard", "title": "On Backdoors To Tractable Constraint Languages", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of CSPs, a strong backdoor is a subset of variables such that\nevery complete assignment yields a residual instance guaranteed to have a\nspecified property. If the property allows efficient solving, then a small\nstrong backdoor provides a reasonable decomposition of the original instance\ninto easy instances. An important challenge is the design of algorithms that\ncan find quickly a small strong backdoor if one exists. We present a systematic\nstudy of the parameterized complexity of backdoor detection when the target\nproperty is a restricted type of constraint language defined by means of a\nfamily of polymorphisms. In particular, we show that under the weak assumption\nthat the polymorphisms are idempotent, the problem is unlikely to be FPT when\nthe parameter is either r (the constraint arity) or k (the size of the\nbackdoor) unless P = NP or FPT = W[2]. When the parameter is k+r, however, we\nare able to identify large classes of languages for which the problem of\nfinding a small backdoor is FPT.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 18:13:23 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 14:47:51 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Carbonnel", "Clement", ""], ["Cooper", "Martin C.", ""], ["Hebrard", "Emmanuel", ""]]}, {"id": "1404.3708", "submitter": "Yuxiao Dong", "authors": "Yuxiao Dong, Jie Tang, Nitesh Chawla, Tiancheng Lou, Yang Yang, Bai\n  Wang", "title": "Inferring Social Status and Rich Club Effects in Enterprise\n  Communication Networks", "comments": "13 pages, 4 figures", "journal-ref": "PLoS ONE 10(3): e0119446. 2015", "doi": "10.1371/journal.pone.0119446", "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social status, defined as the relative rank or position that an individual\nholds in a social hierarchy, is known to be among the most important motivating\nforces in social behaviors. In this paper, we consider the notion of status\nfrom the perspective of a position or title held by a person in an enterprise.\nWe study the intersection of social status and social networks in an\nenterprise. We study whether enterprise communication logs can help reveal how\nsocial interactions and individual status manifest themselves in social\nnetworks. To that end, we use two enterprise datasets with three communication\nchannels --- voice call, short message, and email --- to demonstrate the\nsocial-behavioral differences among individuals with different status. We have\nseveral interesting findings and based on these findings we also develop a\nmodel to predict social status. On the individual level, high-status\nindividuals are more likely to be spanned as structural holes by linking to\npeople in parts of the enterprise networks that are otherwise not well\nconnected to one another. On the community level, the principle of homophily,\nsocial balance and clique theory generally indicate a \"rich club\" maintained by\nhigh-status individuals, in the sense that this community is much more\nconnected, balanced and dense. Our model can predict social status of\nindividuals with 93% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 19:32:08 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 01:25:26 GMT"}, {"version": "v3", "created": "Wed, 16 Apr 2014 20:12:21 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2015 19:44:47 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Dong", "Yuxiao", ""], ["Tang", "Jie", ""], ["Chawla", "Nitesh", ""], ["Lou", "Tiancheng", ""], ["Yang", "Yang", ""], ["Wang", "Bai", ""]]}, {"id": "1404.3862", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yonatan Glassner, Shie Mannor", "title": "Optimizing the CVaR via Sampling", "comments": "To appear in AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Value at Risk (CVaR) is a prominent risk measure that is being\nused extensively in various domains. We develop a new formula for the gradient\nof the CVaR in the form of a conditional expectation. Based on this formula, we\npropose a novel sampling-based estimator for the CVaR gradient, in the spirit\nof the likelihood-ratio method. We analyze the bias of the estimator, and prove\nthe convergence of a corresponding stochastic gradient descent algorithm to a\nlocal CVaR optimum. Our method allows to consider CVaR optimization in new\ndomains. As an example, we consider a reinforcement learning application, and\nlearn a risk-sensitive controller for the game of Tetris.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 10:32:05 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 15:35:36 GMT"}, {"version": "v3", "created": "Tue, 16 Sep 2014 15:32:48 GMT"}, {"version": "v4", "created": "Sat, 22 Nov 2014 14:44:54 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Tamar", "Aviv", ""], ["Glassner", "Yonatan", ""], ["Mannor", "Shie", ""]]}, {"id": "1404.4089", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck, Adnan Darwiche", "title": "On the Role of Canonicity in Bottom-up Knowledge Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of bottom-up compilation of knowledge bases, which is\nusually predicated on the existence of a polytime function for combining\ncompilations using Boolean operators (usually called an Apply function). While\nsuch a polytime Apply function is known to exist for certain languages (e.g.,\nOBDDs) and not exist for others (e.g., DNNF), its existence for certain\nlanguages remains unknown. Among the latter is the recently introduced language\nof Sentential Decision Diagrams (SDDs), for which a polytime Apply function\nexists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical\nSDDs). We resolve this open question in this paper and consider some of its\ntheoretical and practical implications. Some of the findings we report question\nthe common wisdom on the relationship between bottom-up compilation, language\ncanonicity and the complexity of the Apply function.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 21:43:41 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1404.4105", "submitter": "Aur\\'elien Bellet", "authors": "Yuan Shi and Aur\\'elien Bellet and Fei Sha", "title": "Sparse Compositional Metric Learning", "comments": "18 pages. To be published in Proceedings of the 27th AAAI Conference\n  on Artificial Intelligence (AAAI 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for metric learning by framing it as learning a\nsparse combination of locally discriminative metrics that are inexpensive to\ngenerate from the training data. This flexible framework allows us to naturally\nderive formulations for global, multi-task and local metric learning. The\nresulting algorithms have several advantages over existing methods in the\nliterature: a much smaller number of parameters to be estimated and a\nprincipled way to generalize learned metrics to new testing data points. To\nanalyze the approach theoretically, we derive a generalization bound that\njustifies the sparse combination. Empirically, we evaluate our algorithms on\nseveral datasets against state-of-the-art metric learning methods. The results\nare consistent with our theoretical findings and demonstrate the superiority of\nour approach in terms of classification performance and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:55:53 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Shi", "Yuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1404.4258", "submitter": "Gavin Taylor", "authors": "Gavin Taylor and Connor Geer and David Piekut", "title": "An Analysis of State-Relevance Weights and Sampling Distributions on\n  L1-Regularized Approximate Linear Programming Approximation Accuracy", "comments": "Identical to the ICML 2014 paper of the same name, but with full\n  proofs. Please cite the ICML paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recent interest in the use of $L_1$ regularization in the use of value\nfunction approximation includes Petrik et al.'s introduction of\n$L_1$-Regularized Approximate Linear Programming (RALP). RALP is unique among\n$L_1$-regularized approaches in that it approximates the optimal value function\nusing off-policy samples. Additionally, it produces policies which outperform\nthose of previous methods, such as LSPI. RALP's value function approximation\nquality is affected heavily by the choice of state-relevance weights in the\nobjective function of the linear program, and by the distribution from which\nsamples are drawn; however, there has been no discussion of these\nconsiderations in the previous literature. In this paper, we discuss and\nexplain the effects of choices in the state-relevance weights and sampling\ndistribution on approximation quality, using both theoretical and experimental\nillustrations. The results provide insight not only onto these effects, but\nalso provide intuition into the types of MDPs which are especially well suited\nfor approximation with RALP.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 14:15:43 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 15:50:46 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Taylor", "Gavin", ""], ["Geer", "Connor", ""], ["Piekut", "David", ""]]}, {"id": "1404.4274", "submitter": "Shqiponja Ahmetaj", "authors": "Shqiponja Ahmetaj, Diego Calvanese, Magdalena Ortiz, Mantas Simkus", "title": "Managing Change in Graph-structured Data Using Description Logics (long\n  version with appendix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the setting of graph-structured data that evolves\nas a result of operations carried out by users or applications. We study\ndifferent reasoning problems, which range from ensuring the satisfaction of a\ngiven set of integrity constraints after a given sequence of updates, to\ndeciding the (non-)existence of a sequence of actions that would take the data\nto an (un)desirable state, starting either from a specific data instance or\nfrom an incomplete description of it. We consider an action language in which\nactions are finite sequences of conditional insertions and deletions of nodes\nand labels, and use Description Logics for describing integrity constraints and\n(partial) states of the data. We then formalize the above data management\nproblems as a static verification problem and several planning problems. We\nprovide algorithms and tight complexity bounds for the formalized problems,\nboth for an expressive DL and for a variant of DL-Lite.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 14:56:22 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 15:58:28 GMT"}, {"version": "v3", "created": "Thu, 29 May 2014 19:15:53 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Ahmetaj", "Shqiponja", ""], ["Calvanese", "Diego", ""], ["Ortiz", "Magdalena", ""], ["Simkus", "Mantas", ""]]}, {"id": "1404.4304", "submitter": "Ronald Hochreiter", "authors": "Christoph Waldhauser and Ronald Hochreiter and Johannes Otepka and\n  Norbert Pfeifer and Sajid Ghuffar and Karolina Korzeniowska and Gerald Wagner", "title": "Automated Classification of Airborne Laser Scanning Point Clouds", "comments": null, "journal-ref": "In: Solving Computationally Expensive Engineering Problems.\n  Springer Proceedings in Mathematics & Statistics Volume 97: 269-292. 2014", "doi": "10.1007/978-3-319-08985-0_12", "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making sense of the physical world has always been at the core of mapping. Up\nuntil recently, this has always dependent on using the human eye. Using\nairborne lasers, it has become possible to quickly \"see\" more of the world in\nmany more dimensions. The resulting enormous point clouds serve as data sources\nfor applications far beyond the original mapping purposes ranging from flooding\nprotection and forestry to threat mitigation. In order to process these large\nquantities of data, novel methods are required. In this contribution, we\ndevelop models to automatically classify ground cover and soil types. Using the\nlogic of machine learning, we critically review the advantages of supervised\nand unsupervised methods. Focusing on decision trees, we improve accuracy by\nincluding beam vector components and using a genetic algorithm. We find that\nour approach delivers consistently high quality classifications, surpassing\nclassical methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 16:27:53 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Waldhauser", "Christoph", ""], ["Hochreiter", "Ronald", ""], ["Otepka", "Johannes", ""], ["Pfeifer", "Norbert", ""], ["Ghuffar", "Sajid", ""], ["Korzeniowska", "Karolina", ""], ["Wagner", "Gerald", ""]]}, {"id": "1404.4388", "submitter": "Yanling Chang", "authors": "Yanling Chang, Alan L. Erera, and Chelsea C. White III", "title": "Partially Observed, Multi-objective Markov Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intent of this research is to generate a set of non-dominated policies\nfrom which one of two agents (the leader) can select a most preferred policy to\ncontrol a dynamic system that is also affected by the control decisions of the\nother agent (the follower). The problem is described by an infinite horizon,\npartially observed Markov game (POMG). At each decision epoch, each agent\nknows: its past and present states, its past actions, and noise corrupted\nobservations of the other agent's past and present states. The actions of each\nagent are determined at each decision epoch based on these data. The leader\nconsiders multiple objectives in selecting its policy. The follower considers a\nsingle objective in selecting its policy with complete knowledge of and in\nresponse to the policy selected by the leader. This leader-follower assumption\nallows the POMG to be transformed into a specially structured, partially\nobserved Markov decision process (POMDP). This POMDP is used to determine the\nfollower's best response policy. A multi-objective genetic algorithm (MOGA) is\nused to create the next generation of leader policies based on the fitness\nmeasures of each leader policy in the current generation. Computing a fitness\nmeasure for a leader policy requires a value determination calculation, given\nthe leader policy and the follower's best response policy. The policies from\nwhich the leader can select a most preferred policy are the non-dominated\npolicies of the final generation of leader policies created by the MOGA. An\nexample is presented that illustrates how these results can be used to support\na manager of a liquid egg production process (the leader) in selecting a\nsequence of actions to best control this process over time, given that there is\nan attacker (the follower) who seeks to contaminate the liquid egg production\nprocess with a chemical or biological toxin.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 21:22:53 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Chang", "Yanling", ""], ["Erera", "Alan L.", ""], ["White", "Chelsea C.", "III"]]}, {"id": "1404.4502", "submitter": "Arnaud Lallouet", "authors": "Thi-Van-Anh Nguyen and Arnaud Lallouet", "title": "A Complete Solver for Constraint Games", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game Theory studies situations in which multiple agents having conflicting\nobjectives have to reach a collective decision. The question of a compact\nrepresentation language for agents utility function is of crucial importance\nsince the classical representation of a $n$-players game is given by a\n$n$-dimensional matrix of exponential size for each player. In this paper we\nuse the framework of Constraint Games in which CSP are used to represent\nutilities. Constraint Programming --including global constraints-- allows to\neasily give a compact and elegant model to many useful games. Constraint Games\ncome in two flavors: Constraint Satisfaction Games and Constraint Optimization\nGames, the first one using satisfaction to define boolean utilities. In\naddition to multimatrix games, it is also possible to model more complex games\nwhere hard constraints forbid certain situations. In this paper we study\ncomplete search techniques and show that our solver using the compact\nrepresentation of Constraint Games is faster than the classical game solver\nGambit by one to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 12:09:51 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 09:14:33 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Nguyen", "Thi-Van-Anh", ""], ["Lallouet", "Arnaud", ""]]}, {"id": "1404.4785", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Ontology as a Source for Rule Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discloses the potential of OWL (Web Ontology Language) ontologies\nfor generation of rules. The main purpose of this paper is to identify new\ntypes of rules, which may be generated from OWL ontologies. Rules, generated\nfrom OWL ontologies, are necessary for the functioning of the Semantic Web\nExpert System. It is expected that the Semantic Web Expert System (SWES) will\nbe able to process ontologies from the Web with the purpose to supplement or\neven to develop its knowledge base.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 13:36:17 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1404.4789", "submitter": "Xinyang Deng", "authors": "Hongming Mo, Yong Deng", "title": "A new combination approach based on improved evidence distance", "comments": "14 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer evidence theory is a powerful tool in information fusion.\nWhen the evidence are highly conflicting, the counter-intuitive results will be\npresented. To adress this open issue, a new method based on evidence distance\nof Jousselme and Hausdorff distance is proposed. Weight of each evidence can be\ncomputed, preprocess the original evidence to generate a new evidence. The\nDempster's combination rule is used to combine the new evidence. Comparing with\nthe existing methods, the new proposed method is efficient.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 13:55:36 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Mo", "Hongming", ""], ["Deng", "Yong", ""]]}, {"id": "1404.4801", "submitter": "Xinyang Deng", "authors": "Yong Deng", "title": "Generalized Evidence Theory", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conflict management is still an open issue in the application of Dempster\nShafer evidence theory. A lot of works have been presented to address this\nissue. In this paper, a new theory, called as generalized evidence theory\n(GET), is proposed. Compared with existing methods, GET assumes that the\ngeneral situation is in open world due to the uncertainty and incomplete\nknowledge. The conflicting evidence is handled under the framework of GET. It\nis shown that the new theory can explain and deal with the conflicting evidence\nin a more reasonable way.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 08:08:56 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Deng", "Yong", ""]]}, {"id": "1404.4884", "submitter": "David Eubanks", "authors": "David A. Eubanks", "title": "Causal Interfaces", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction of two binary variables, assumed to be empirical\nobservations, has three degrees of freedom when expressed as a matrix of\nfrequencies. Usually, the size of causal influence of one variable on the other\nis calculated as a single value, as increase in recovery rate for a medical\ntreatment, for example. We examine what is lost in this simplification, and\npropose using two interface constants to represent positive and negative\nimplications separately. Given certain assumptions about non-causal outcomes,\nthe set of resulting epistemologies is a continuum. We derive a variety of\nparticular measures and contrast them with the one-dimensional index.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 20:51:54 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Eubanks", "David A.", ""]]}, {"id": "1404.4893", "submitter": "Daniele Codecasa", "authors": "Daniele Codecasa and Fabio Stella", "title": "CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Bayesian network classifiers are designed for temporal\nclassification of multivariate streaming data when time duration of events\nmatters and the class does not change over time. This paper introduces the\nCTBNCToolkit: an open source Java toolkit which provides a stand-alone\napplication for temporal classification and a library for continuous time\nBayesian network classifiers. CTBNCToolkit implements the inference algorithm,\nthe parameter learning algorithm, and the structural learning algorithm for\ncontinuous time Bayesian network classifiers. The structural learning algorithm\nis based on scoring functions: the marginal log-likelihood score and the\nconditional log-likelihood score are provided. CTBNCToolkit provides also an\nimplementation of the expectation maximization algorithm for clustering\npurpose. The paper introduces continuous time Bayesian network classifiers. How\nto use the CTBNToolkit from the command line is described in a specific\nsection. Tutorial examples are included to facilitate users to understand how\nthe toolkit must be used. A section dedicate to the Java library is proposed to\nhelp further code extensions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 21:48:34 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Codecasa", "Daniele", ""], ["Stella", "Fabio", ""]]}, {"id": "1404.4983", "submitter": "Nisheeth Joshi", "authors": "Iti Mathur, Nisheeth Joshi, Hemant Darbari and Ajai Kumar", "title": "Shiva++: An Enhanced Graph based Ontology Matcher", "comments": "arXiv admin note: text overlap with arXiv:1403.7465", "journal-ref": "International Journal of Computer Applications 92(16):30-34, April\n  2014", "doi": "10.5120/16095-5393", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the web getting bigger and assimilating knowledge about different\nconcepts and domains, it is becoming very difficult for simple database driven\napplications to capture the data for a domain. Thus developers have come out\nwith ontology based systems which can store large amount of information and can\napply reasoning and produce timely information. Thus facilitating effective\nknowledge management. Though this approach has made our lives easier, but at\nthe same time has given rise to another problem. Two different ontologies\nassimilating same knowledge tend to use different terms for the same concepts.\nThis creates confusion among knowledge engineers and workers, as they do not\nknow which is a better term then the other. Thus we need to merge ontologies\nworking on same domain so that the engineers can develop a better application\nover it. This paper shows the development of one such matcher which merges the\nconcepts available in two ontologies at two levels; 1) at string level and 2)\nat semantic level; thus producing better merged ontologies. We have used a\ngraph matching technique which works at the core of the system. We have also\nevaluated the system and have tested its performance with its predecessor which\nworks only on string matching. Thus current approach produces better results.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 19:12:52 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Mathur", "Iti", ""], ["Joshi", "Nisheeth", ""], ["Darbari", "Hemant", ""], ["Kumar", "Ajai", ""]]}, {"id": "1404.5078", "submitter": "Ethan Petuchowski", "authors": "Ethan Petuchowski, Matthew Lease", "title": "TurKPF: TurKontrol as a Particle Filter", "comments": "8 pages, 6 figures, formula appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TurKontrol, and algorithm presented in (Dai et al. 2010), uses a POMDP to\nmodel and control an iterative workflow for crowdsourced work. Here, TurKontrol\nis re-implemented as \"TurKPF,\" which uses a Particle Filter to reduce\ncomputation time & memory usage. Most importantly, in our experimental\nenvironment with default parameter settings, the action is chosen nearly\ninstantaneously. Through a series of experiments we see that TurKPF and\nTurKontrol perform similarly.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 22:47:32 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Petuchowski", "Ethan", ""], ["Lease", "Matthew", ""]]}, {"id": "1404.5214", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Graph Kernels via Functional Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a representation of graph as a functional object derived from the\npower iteration of the underlying adjacency matrix. The proposed functional\nrepresentation is a graph invariant, i.e., the functional remains unchanged\nunder any reordering of the vertices. This property eliminates the difficulty\nof handling exponentially many isomorphic forms. Bhattacharyya kernel\nconstructed between these functionals significantly outperforms the\nstate-of-the-art graph kernels on 3 out of the 4 standard benchmark graph\nclassification datasets, demonstrating the superiority of our approach. The\nproposed methodology is simple and runs in time linear in the number of edges,\nwhich makes our kernel more efficient and scalable compared to many widely\nadopted graph kernels with running time cubic in the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 14:56:17 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.5454", "submitter": "Adish Singla", "authors": "Adish Singla, Eric Horvitz, Ece Kamar, Ryen White", "title": "Stochastic Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online services such as web search and e-commerce applications typically rely\non the collection of data about users, including details of their activities on\nthe web. Such personal data is used to enhance the quality of service via\npersonalization of content and to maximize revenues via better targeting of\nadvertisements and deeper engagement of users on sites. To date, service\nproviders have largely followed the approach of either requiring or requesting\nconsent for opting-in to share their data. Users may be willing to share\nprivate information in return for better quality of service or for incentives,\nor in return for assurances about the nature and extend of the logging of data.\nWe introduce \\emph{stochastic privacy}, a new approach to privacy centering on\na simple concept: A guarantee is provided to users about the upper-bound on the\nprobability that their personal data will be used. Such a probability, which we\nrefer to as \\emph{privacy risk}, can be assessed by users as a preference or\ncommunicated as a policy by a service provider. Service providers can work to\npersonalize and to optimize revenues in accordance with preferences about\nprivacy risk. We present procedures, proofs, and an overall system for\nmaximizing the quality of services, while respecting bounds on allowable or\ncommunicated privacy risk. We demonstrate the methodology with a case study and\nevaluation of the procedures applied to web search personalization. We show how\nwe can achieve near-optimal utility of accessing information with provable\nguarantees on the probability of sharing data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 10:55:19 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Singla", "Adish", ""], ["Horvitz", "Eric", ""], ["Kamar", "Ece", ""], ["White", "Ryen", ""]]}, {"id": "1404.5528", "submitter": "Mohammad Shojafar", "authors": "Saeed Javanmardi, Mohammad Shojafar, Danilo Amendola, Nicola\n  Cordeschi, Hongbo Liu, Ajith Abraham", "title": "Hybrid Genetic Algorithm for Cloud Computing Applications", "comments": "10 Pages, 5 figures, 1 table, IBICA2014, Accepted to publish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper with the aid of genetic algorithm and fuzzy theory, we present\na hybrid job scheduling approach, which considers the load balancing of the\nsystem and reduces total execution time and execution cost. We try to modify\nthe standard Genetic algorithm and to reduce the iteration of creating\npopulation with the aid of fuzzy theory. The main goal of this research is to\nassign the jobs to the resources with considering the VM MIPS and length of\njobs. The new algorithm assigns the jobs to the resources with considering the\njob length and resources capacities. We evaluate the performance of our\napproach with some famous cloud scheduling models. The results of the\nexperiments show the efficiency of the proposed approach in term of execution\ntime, execution cost and average Degree of Imbalance (DI).\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 15:33:39 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Javanmardi", "Saeed", ""], ["Shojafar", "Mohammad", ""], ["Amendola", "Danilo", ""], ["Cordeschi", "Nicola", ""], ["Liu", "Hongbo", ""], ["Abraham", "Ajith", ""]]}, {"id": "1404.5643", "submitter": "Yu Zhang", "authors": "Yu Zhang and Subbarao Kambhampati", "title": "A Formal Analysis of Required Cooperation in Multi-agent Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on multi-agent planning has been popular in recent years. While\nprevious research has been motivated by the understanding that, through\ncooperation, multi-agent systems can achieve tasks that are unachievable by\nsingle-agent systems, there are no formal characterizations of situations where\ncooperation is required to achieve a goal, thus warranting the application of\nmulti-agent systems. In this paper, we provide such a formal discussion from\nthe planning aspect. We first show that determining whether there is required\ncooperation (RC) is intractable is general. Then, by dividing the problems that\nrequire cooperation (referred to as RC problems) into two classes -- problems\nwith heterogeneous and homogeneous agents, we aim to identify all the\nconditions that can cause RC in these two classes. We establish that when none\nof these identified conditions hold, the problem is single-agent solvable.\nFurthermore, with a few assumptions, we provide an upper bound on the minimum\nnumber of agents required for RC problems with homogeneous agents. This study\nnot only provides new insights into multi-agent planning, but also has many\napplications. For example, in human-robot teaming, when a robot cannot achieve\na task, it may be due to RC. In such cases, the human teammate should be\ninformed and, consequently, coordinate with other available robots for a\nsolution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 20:46:32 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Zhang", "Yu", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1404.5668", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel D. Lee", "title": "An Adversarial Interpretation of Information-Theoretic Bounded\n  Rationality", "comments": "7 pages, 4 figures. Proceedings of AAAI-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in modeling planning with\ninformation constraints. Accordingly, an agent maximizes a regularized expected\nutility known as the free energy, where the regularizer is given by the\ninformation divergence from a prior to a posterior policy. While this approach\ncan be justified in various ways, including from statistical mechanics and\ninformation theory, it is still unclear how it relates to decision-making\nagainst adversarial environments. This connection has previously been suggested\nin work relating the free energy to risk-sensitive control and to extensive\nform games. Here, we show that a single-agent free energy optimization is\nequivalent to a game between the agent and an imaginary adversary. The\nadversary can, by paying an exponential penalty, generate costs that diminish\nthe decision maker's payoffs. It turns out that the optimal strategy of the\nadversary consists in choosing costs so as to render the decision maker\nindifferent among its choices, which is a definining property of a Nash\nequilibrium, thus tightening the connection between free energy optimization\nand game theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 23:21:14 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1404.5711", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter", "title": "Modeling multi-stage decision optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage optimization under uncertainty techniques can be used to solve\nlong-term management problems. Although many optimization modeling language\nextensions as well as computational environments have been proposed, the\nacceptance of this technique is generally low, due to the inherent complexity\nof the modeling and solution process. In this paper a simplification to\nannotate multi-stage decision problems under uncertainty is presented - this\nsimplification contrasts with the common approach to create an extension on top\nof an existing optimization modeling language. This leads to the definition of\nmeta models, which can be instanced in various programming languages. An\nexample using the statistical computing language R is shown.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 05:50:11 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Hochreiter", "Ronald", ""]]}, {"id": "1404.6036", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka (INRIA Saclay - Ile de France)", "title": "Gradual Classical Logic for Attributed Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is knowledge. There is belief. And there is tacit agreement.' 'We may\ntalk about objects. We may talk about attributes of the objects. Or we may talk\nboth about objects and their attributes.' This work inspects tacit agreements\non assumptions about the relation between objects and their attributes, and\nstudies a way of expressing them, presenting as the result what we term gradual\nlogic in which the sense of truth gradually shifts. It extends classical logic\ninstances with a new logical connective capturing the object-attribute\nrelation. A formal semantics is presented. Decidability is proved. Para-\nconsistent/epistemic/conditional/intensional/description/combined logics are\ncompared.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 06:18:38 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Arisaka", "Ryuta", "", "INRIA Saclay - Ile de France"]]}, {"id": "1404.6059", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora and Dr. Anil Kumar Gupta", "title": "A Comparative study Between Fuzzy Clustering Algorithm and Hard\n  Clustering Algorithm", "comments": "Data Clustering,6 pages,6 figures,Published with International\n  Journal of Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V10(2):108-113, Apr 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V10P119", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is an important area of data mining. This is an unsupervised\nstudy where data of similar types are put into one cluster while data of\nanother types are put into different cluster. Fuzzy C means is a very important\nclustering technique based on fuzzy logic. Also we have some hard clustering\ntechniques available like K-means among the popular ones. In this paper a\ncomparative study is done between Fuzzy clustering algorithm and hard\nclustering algorithm\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 09:02:38 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Bora", "Dibya Jyoti", ""], ["Gupta", "Dr. Anil Kumar", ""]]}, {"id": "1404.6071", "submitter": "Chandranath  Adak", "authors": "Chandranath Adak", "title": "Rough Clustering Based Unsupervised Image Change Detection", "comments": "Proc. IEEE Conf. #30853, International Conference on Human Computer\n  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013. (In Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an unsupervised technique to detect the changed region\nof multitemporal images on a same reference plane with the help of rough\nclustering. The proposed technique is a soft-computing approach, based on the\nconcept of rough set with rough clustering and Pawlak's accuracy. It is less\nnoisy and avoids pre-deterministic knowledge about the distribution of the\nchanged and unchanged regions. To show the effectiveness, the proposed\ntechnique is compared with some other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 10:09:41 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Adak", "Chandranath", ""]]}, {"id": "1404.6075", "submitter": "Chandranath  Adak", "authors": "Chandranath Adak", "title": "Unsupervised Text Extraction from G-Maps", "comments": "Proc. IEEE Conf. #30853, International Conference on Human Computer\n  Interactions (ICHCI'13), Chennai, India, 23-24 Aug., 2013", "journal-ref": null, "doi": "10.1109/ICHCI-IEEE.2013.6887782", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper represents an text extraction method from Google maps, GIS\nmaps/images. Due to an unsupervised approach there is no requirement of any\nprior knowledge or training set about the textual and non-textual parts. Fuzzy\nCMeans clustering technique is used for image segmentation and Prewitt method\nis used to detect the edges. Connected component analysis and gridding\ntechnique enhance the correctness of the results. The proposed method reaches\n98.5% accuracy level on the basis of experimental data sets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 10:24:49 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Adak", "Chandranath", ""]]}, {"id": "1404.6445", "submitter": "Stefan R\\\"ummele", "authors": "Nadia Creignou, Odile Papini, Stefan R\\\"ummele, Stefan Woltran", "title": "Belief merging within fragments of propositional logic", "comments": "To appear in the Proceedings of the 15th International Workshop on\n  Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, belief change within the framework of fragments of propositional\nlogic has gained increasing attention. Previous works focused on belief\ncontraction and belief revision on the Horn fragment. However, the problem of\nbelief merging within fragments of propositional logic has been neglected so\nfar. This paper presents a general approach to define new merging operators\nderived from existing ones such that the result of merging remains in the\nfragment under consideration. Our approach is not limited to the case of Horn\nfragment but applicable to any fragment of propositional logic characterized by\na closure property on the sets of models of its formulae. We study the logical\nproperties of the proposed operators in terms of satisfaction of merging\npostulates, considering in particular distance-based merging operators for Horn\nand Krom fragments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 14:57:54 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Creignou", "Nadia", ""], ["Papini", "Odile", ""], ["R\u00fcmmele", "Stefan", ""], ["Woltran", "Stefan", ""]]}, {"id": "1404.6566", "submitter": "Oliver Fernandez Gil", "authors": "Oliver Fern\\'andez Gil", "title": "On the Non-Monotonic Description Logic\n  $\\mathcal{ALC}$+T$_{\\mathsf{min}}$", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last 20 years many proposals have been made to incorporate\nnon-monotonic reasoning into description logics, ranging from approaches based\non default logic and circumscription to those based on preferential semantics.\nIn particular, the non-monotonic description logic\n$\\mathcal{ALC}$+T$_{\\mathsf{min}}$ uses a combination of the preferential\nsemantics with minimization of a certain kind of concepts, which represent\natypical instances of a class of elements. One of its drawbacks is that it\nsuffers from the problem known as the \\emph{property blocking inheritance},\nwhich can be seen as a weakness from an inferential point of view. In this\npaper we propose an extension of $\\mathcal{ALC}$+T$_{\\mathsf{min}}$, namely\n$\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$, with the purpose to solve the mentioned\nproblem. In addition, we show the close connection that exists between\n$\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$ and concept-circumscribed knowledge bases.\nFinally, we study the complexity of deciding the classical reasoning tasks in\n$\\mathcal{ALC}$+T$^+_{\\mathsf{min}}$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 21:45:44 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Gil", "Oliver Fern\u00e1ndez", ""]]}, {"id": "1404.6567", "submitter": "Michel Rueher", "authors": "Mohammed Bekkouche, H\\'el\\`ene Collavizza, Michel Rueher", "title": "Une approche CSP pour l'aide \\`a la localisation d'erreurs", "comments": "10 pages, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new CP-based approach to support errors location\nin a program for which a counter-example is available, i.e. an instantiation of\nthe input variables that violates the post-condition. To provide helpful\ninformation for error location, we generate a constraint system for the paths\nof the CFG (Control Flow Graph) for which at most k conditional statements may\nbe erroneous. Then, we calculate Minimal Correction Sets (MCS) of bounded size\nfor each of these paths. The removal of one of these sets of constraints yields\na maximal satisfiable subset, in other words, a maximal subset of constraints\nsatisfying the post condition. We extend the algorithm proposed by Liffiton and\nSakallah \\cite{LiS08} to handle programs with numerical statements more\nefficiently. We present preliminary experimental results that are quite\nencouraging.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 21:51:17 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Bekkouche", "Mohammed", ""], ["Collavizza", "H\u00e9l\u00e8ne", ""], ["Rueher", "Michel", ""]]}, {"id": "1404.6696", "submitter": "Thibaut Vidal", "authors": "Thibaut Vidal, Maria Battarra, Anand Subramanian, G\\\"une\\c{s}\n  Erdo\\v{g}an", "title": "Hybrid Metaheuristics for the Clustered Vehicle Routing Problem", "comments": "Working Paper, MIT -- 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clustered Vehicle Routing Problem (CluVRP) is a variant of the\nCapacitated Vehicle Routing Problem in which customers are grouped into\nclusters. Each cluster has to be visited once, and a vehicle entering a cluster\ncannot leave it until all customers have been visited. This article presents\ntwo alternative hybrid metaheuristic algorithms for the CluVRP. The first\nalgorithm is based on an Iterated Local Search algorithm, in which only\nfeasible solutions are explored and problem-specific local search moves are\nutilized. The second algorithm is a Hybrid Genetic Search, for which the\nshortest Hamiltonian path between each pair of vertices within each cluster\nshould be precomputed. Using this information, a sequence of clusters can be\nused as a solution representation and large neighborhoods can be efficiently\nexplored by means of bi-directional dynamic programming, sequence\nconcatenations, by using appropriate data structures. Extensive computational\nexperiments are performed on benchmark instances from the literature, as well\nas new large scale ones. Recommendations on promising algorithm choices are\nprovided relatively to average cluster size.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 23:52:47 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Vidal", "Thibaut", ""], ["Battarra", "Maria", ""], ["Subramanian", "Anand", ""], ["Erdo\u01e7an", "G\u00fcne\u015f", ""]]}, {"id": "1404.6699", "submitter": "Paulo Shakarian", "authors": "Paulo Shakarian, Gerardo I. Simari, Geoffrey Moores, Simon Parsons,\n  Marcelo A. Falappa", "title": "An Argumentation-Based Framework to Address the Attribution Problem in\n  Cyber-Warfare", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.1475", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Attributing a cyber-operation through the use of multiple pieces of technical\nevidence (i.e., malware reverse-engineering and source tracking) and\nconventional intelligence sources (i.e., human or signals intelligence) is a\ndifficult problem not only due to the effort required to obtain evidence, but\nthe ease with which an adversary can plant false evidence. In this paper, we\nintroduce a formal reasoning system called the InCA (Intelligent Cyber\nAttribution) framework that is designed to aid an analyst in the attribution of\na cyber-operation even when the available information is conflicting and/or\nuncertain. Our approach combines argumentation-based reasoning, logic\nprogramming, and probabilistic models to not only attribute an operation but\nalso explain to the analyst why the system reaches its conclusions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 00:17:21 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Shakarian", "Paulo", ""], ["Simari", "Gerardo I.", ""], ["Moores", "Geoffrey", ""], ["Parsons", "Simon", ""], ["Falappa", "Marcelo A.", ""]]}, {"id": "1404.6784", "submitter": "Joao Leite", "authors": "Martin Slota, Martin Bal\\'az, Jo\\~ao Leite", "title": "On Strong and Default Negation in Logic Program Updates (Extended\n  Version)", "comments": "14 pages, extended version of the paper to appear in the online\n  supplement of Theory and Practice of Logic Programming (TPLP), and presented\n  at the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014) and\n  at the 30th International Conference on Logic Programming (ICLP 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semantics for answer-set program updates fall into two categories:\neither they consider only strong negation in heads of rules, or they primarily\nrely on default negation in heads of rules and optionally provide support for\nstrong negation by means of a syntactic transformation. In this paper we\npinpoint the limitations of both these approaches and argue that both types of\nnegation should be first-class citizens in the context of updates. We identify\nprinciples that plausibly constrain their interaction but are not\nsimultaneously satisfied by any existing rule update semantics. Then we extend\none of the most advanced semantics with direct support for strong negation and\nshow that it satisfies the outlined principles as well as a variety of other\ndesirable properties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 16:33:42 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 10:46:56 GMT"}, {"version": "v3", "created": "Wed, 11 Jun 2014 23:30:20 GMT"}, {"version": "v4", "created": "Wed, 9 Jul 2014 16:05:40 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Slota", "Martin", ""], ["Bal\u00e1z", "Martin", ""], ["Leite", "Jo\u00e3o", ""]]}, {"id": "1404.6883", "submitter": "Jozef Frtus", "authors": "Jozef Frt\\'us", "title": "Credulous and Skeptical Argument Games for Complete Semantics in\n  Conflict Resolution based Argumentation", "comments": "appears in the Proceedings of the 15th International Workshop on\n  Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation is one of the most popular approaches of defining\na~non-monotonic formalism and several argumentation based semantics were\nproposed for defeasible logic programs. Recently, a new approach based on\nnotions of conflict resolutions was proposed, however with declarative\nsemantics only. This paper gives a more procedural counterpart by developing\nskeptical and credulous argument games for complete semantics and soundness and\ncompleteness theorems for both games are provided. After that, distribution of\ndefeasible logic program into several contexts is investigated and both\nargument games are adapted for multi-context system.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 07:24:57 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Frt\u00fas", "Jozef", ""]]}, {"id": "1404.6974", "submitter": "Claudia Schon", "authors": "Ulrich Furbach and Claudia Schon", "title": "Deontic Logic for Human Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deontic logic is shown to be applicable for modelling human reasoning. For\nthis the Wason selection task and the suppression task are discussed in detail.\nDifferent versions of modelling norms with deontic logic are introduced and in\nthe case of the Wason selection task it is demonstrated how differences in the\nperformance of humans in the abstract and in the social contract case can be\nexplained. Furthermore it is shown that an automated theorem prover can be used\nas a reasoning tool for deontic logic.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 13:34:20 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 08:46:31 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""]]}, {"id": "1404.6999", "submitter": "Carmine Dodaro", "authors": "Mario Alviano, Carmine Dodaro and Francesco Ricca", "title": "Preliminary Report on WASP 2.0", "comments": "The paper appears in the Proceedings of the 15th International\n  Workshop on Non-Monotonic Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a declarative programming paradigm. The\nintrinsic complexity of the evaluation of ASP programs makes the development of\nmore effective and faster systems a challenging research topic. This paper\nreports on the recent improvements of the ASP solver WASP. WASP is undergoing a\nrefactoring process which will end up in the release of a new and more\nperformant version of the software. In particular the paper focus on the\nimprovements to the core evaluation algorithms working on normal programs. A\npreliminary experiment on benchmarks from the 3rd ASP competition belonging to\nthe NP class is reported. The previous version of WASP was often not\ncompetitive with alternative solutions on this class. The new version of WASP\nshows a substantial increase in performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 14:26:12 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Alviano", "Mario", ""], ["Dodaro", "Carmine", ""], ["Ricca", "Francesco", ""]]}, {"id": "1404.7173", "submitter": "Daniel Schwartz", "authors": "Daniel G. Schwartz", "title": "Nonmonotonic Reasoning as a Temporal Activity", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014), Vienna, Austria, 17-19 July 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A {\\it dynamic reasoning system} (DRS) is an adaptation of a conventional\nformal logical system that explicitly portrays reasoning as a temporal\nactivity, with each extralogical input to the system and each inference rule\napplication being viewed as occurring at a distinct time step. Every DRS\nincorporates some well-defined logic together with a controller that serves to\nguide the reasoning process in response to user inputs. Logics are generic,\nwhereas controllers are application-specific. Every controller does,\nnonetheless, provide an algorithm for nonmonotonic belief revision. The general\nnotion of a DRS comprises a framework within which one can formulate the logic\nand algorithms for a given application and prove that the algorithms are\ncorrect, i.e., that they serve to (i) derive all salient information and (ii)\npreserve the consistency of the belief set. This paper illustrates the idea\nwith ordinary first-order predicate calculus, suitably modified for the present\npurpose, and an example. The example revisits some classic nonmonotonic\nreasoning puzzles (Opus the Penguin, Nixon Diamond) and shows how these can be\nresolved in the context of a DRS, using an expanded version of first-order\nlogic that incorporates typed predicate symbols. All concepts are rigorously\ndefined and effectively computable, thereby providing the foundation for a\nfuture software implementation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 21:35:32 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Schwartz", "Daniel G.", ""]]}, {"id": "1404.7205", "submitter": "Jo\\~ao Moura", "authors": "Jo\\~ao Moura and Carlos Dam\\'asio", "title": "Generalizing Modular Logic Programs", "comments": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though modularity has been studied extensively in conventional logic\nprogramming, there are few approaches on how to incorporate modularity into\nAnswer Set Programming, a prominent rule-based declarative programming\nparadigm. A major approach is Oikarinnen and Janhunen's Gaifman-Shapiro-style\narchitecture of program modules, which provides the composition of program\nmodules. Their module theorem properly strengthens Lifschitz and Turner's\nsplitting set theorem for normal logic programs. However, this approach is\nlimited by module conditions that are imposed in order to ensure the\ncompatibility of their module system with the stable model semantics, namely\nforcing output signatures of composing modules to be disjoint and disallowing\npositive cyclic dependencies between different modules. These conditions turn\nout to be too restrictive in practice and in this paper we discuss alternative\nways of lift both restrictions independently, effectively solving the first,\nwidening the applicability of this framework and the scope of the module\ntheorem.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 01:03:19 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Moura", "Jo\u00e3o", ""], ["Dam\u00e1sio", "Carlos", ""]]}, {"id": "1404.7279", "submitter": "Michael  Gr. Voskoglou Prof. Dr.", "authors": "Michael Gr. Voskoglou", "title": "Assessing the players'performance in the game of bridge: A fuzzy logic\n  approach", "comments": "6 pages, 2 figures, 2 tables", "journal-ref": "American Journal of Applied Mathematics and Statistics, vol. 2,\n  no. 3 (2014), 115-120", "doi": "10.12691/ajams-2-3-5", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contract bridge occupies nowadays a position of great prestige being,\ntogether with chess, the only mind games officially recognized by the\nInternational Olympic Committee. In the present paper an innovative method for\nassessing the total performance of bridge- players' belonging to groups of\nspecial interest(e.g. different bridge clubs during a tournament, men and\nwomen, new and old players, etc) is introduced, which is based on principles of\nfuzzy logic. For this, the cohorts under assessment are represented as fuzzy\nsubsets of a set of linguistic labels characterizing their performance and the\ncentroid defuzzification method is used to convert the fuzzy data collected\nfrom the game to a crisp number. This new method of assessment could be used\ninformally as a complement of the official bridge-scoring methods for\nstatistical and other obvious reasons. Two real applications related to\nsimultaneous tournaments with pre-dealt boards, organized by the Hellenic\nBridge Federation, are also presented, illustrating the importance of our\nresults in practice.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 08:56:44 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Voskoglou", "Michael Gr.", ""]]}, {"id": "1404.7428", "submitter": "Anthony Hunter", "authors": "Anthony Hunter", "title": "Analysis of Dialogical Argumentation via Finite State Machines", "comments": "10 pages", "journal-ref": "Proceedings of the International Conference on Scalable\n  Uncertainty Management (SUM'13), LNCS 8078, Pages 1-14, Springer, 2013", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogical argumentation is an important cognitive activity by which agents\nexchange arguments and counterarguments as part of some process such as\ndiscussion, debate, persuasion and negotiation. Whilst numerous formal systems\nhave been proposed, there is a lack of frameworks for implementing and\nevaluating these proposals. First-order executable logic has been proposed as a\ngeneral framework for specifying and analysing dialogical argumentation. In\nthis paper, we investigate how we can implement systems for dialogical\nargumentation using propositional executable logic. Our approach is to present\nand evaluate an algorithm that generates a finite state machine that reflects a\npropositional executable logic specification for a dialogical argumentation\ntogether with an initial state. We also consider how the finite state machines\ncan be analysed, with the minimax strategy being used as an illustration of the\nkinds of empirical analysis that can be undertaken.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 16:49:33 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Hunter", "Anthony", ""]]}, {"id": "1404.7541", "submitter": "Kewen Wang", "authors": "James P. Delgrande and Kewen Wang", "title": "An Approach to Forgetting in Disjunctive Logic Programs that Preserves\n  Strong Equivalence", "comments": "In: Proceedings of 15th International Workshop on Non-Monotonic\n  Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we investigate forgetting in disjunctive logic programs, where\nforgetting an atom from a program amounts to a reduction in the signature of\nthat program. The goal is to provide an approach that is syntax-independent, in\nthat if two programs are strongly equivalent, then the results of forgetting an\natom in each program should also be strongly equivalent. Our central definition\nof forgetting is impractical but satisfies this goal: Forgetting an atom is\ncharacterised by the set of SE consequences of the program that do not mention\nthe atom to be forgotten. We then provide an equivalent, practical definition,\nwherein forgetting an atom $p$ is given by those rules in the program that\ndon't mention $p$, together with rules obtained by a single inference step from\nrules that do mention $p$. Forgetting is shown to have appropriate properties;\nas well, the finite characterisation results in a modest (at worst quadratic)\nblowup. Finally we have also obtained a prototype implementation of this\napproach to forgetting.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 21:37:07 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Delgrande", "James P.", ""], ["Wang", "Kewen", ""]]}, {"id": "1404.7719", "submitter": "Nico Roos", "authors": "Wenzhao Qiao and Nico Roos", "title": "An argumentation system for reasoning with conflict-minimal\n  paraconsistent ALC", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic web is an open and distributed environment in which it is hard\nto guarantee consistency of knowledge and information. Under the standard\ntwo-valued semantics everything is entailed if knowledge and information is\ninconsistent. The semantics of the paraconsistent logic LP offers a solution.\nHowever, if the available knowledge and information is consistent, the set of\nconclusions entailed under the three-valued semantics of the paraconsistent\nlogic LP is smaller than the set of conclusions entailed under the two-valued\nsemantics. Preferring conflict-minimal three-valued interpretations eliminates\nthis difference.\n  Preferring conflict-minimal interpretations introduces non-monotonicity. To\nhandle the non-monotonicity, this paper proposes an assumption-based\nargumentation system. Assumptions needed to close branches of a semantic\ntableaux form the arguments. Stable extensions of the set of derived arguments\ncorrespond to conflict minimal interpretations and conclusions entailed by all\nconflict-minimal interpretations are supported by arguments in all stable\nextensions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 13:32:27 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Qiao", "Wenzhao", ""], ["Roos", "Nico", ""]]}, {"id": "1404.7734", "submitter": "Thomas Linsbichler", "authors": "Ringo Baumann, Wolfgang Dvor\\'ak, Thomas Linsbichler, Hannes Strass\n  and Stefan Woltran", "title": "Compact Argumentation Frameworks", "comments": "Contribution to the 15th International Workshop on Non-Monotonic\n  Reasoning, 2014, Vienna", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract argumentation frameworks (AFs) are one of the most studied\nformalisms in AI. In this work, we introduce a certain subclass of AFs which we\ncall compact. Given an extension-based semantics, the corresponding compact AFs\nare characterized by the feature that each argument of the AF occurs in at\nleast one extension. This not only guarantees a certain notion of fairness;\ncompact AFs are thus also minimal in the sense that no argument can be removed\nwithout changing the outcome. We address the following questions in the paper:\n(1) How are the classes of compact AFs related for different semantics? (2)\nUnder which circumstances can AFs be transformed into equivalent compact ones?\n(3) Finally, we show that compact AFs are indeed a non-trivial subclass, since\nthe verification problem remains coNP-hard for certain semantics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 14:23:40 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Baumann", "Ringo", ""], ["Dvor\u00e1k", "Wolfgang", ""], ["Linsbichler", "Thomas", ""], ["Strass", "Hannes", ""], ["Woltran", "Stefan", ""]]}]