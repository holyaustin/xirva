[{"id": "1012.0065", "submitter": "Pascal Vontobel", "authors": "Pascal O. Vontobel", "title": "Counting in Graph Covers: A Combinatorial Characterization of the Bethe\n  Entropy Function", "comments": "Submitted to IEEE Trans. Inf. Theory, Nov. 20, 2010; rev. Sep. 22,\n  2012; current version, Oct. 9, 2012. Main changes from v1 to v2: new example\n  (Example 34), new lemma (Lemma 35), changed some notation, changed the domain\n  of the Gibbs free energy function and related functions, reordered some\n  sections/appendices, fixed some typos, improved the background discussion,\n  added some new references", "journal-ref": "IEEE Trans. Inf. Theory, vol. 59, pp. 6018-6048, Sept. 2013", "doi": "10.1109/TIT.2013.2264715", "report-no": null, "categories": "cs.IT cond-mat.stat-mech cs.AI math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a combinatorial characterization of the Bethe entropy function of\na factor graph, such a characterization being in contrast to the original,\nanalytical, definition of this function. We achieve this combinatorial\ncharacterization by counting valid configurations in finite graph covers of the\nfactor graph. Analogously, we give a combinatorial characterization of the\nBethe partition function, whose original definition was also of an analytical\nnature. As we point out, our approach has similarities to the replica method,\nbut also stark differences. The above findings are a natural backdrop for\nintroducing a decoder for graph-based codes that we will call symbolwise\ngraph-cover decoding, a decoder that extends our earlier work on blockwise\ngraph-cover decoding. Both graph-cover decoders are theoretical tools that help\ntowards a better understanding of message-passing iterative decoding, namely\nblockwise graph-cover decoding links max-product (min-sum) algorithm decoding\nwith linear programming decoding, and symbolwise graph-cover decoding links\nsum-product algorithm decoding with Bethe free energy function minimization at\ntemperature one. In contrast to the Gibbs entropy function, which is a concave\nfunction, the Bethe entropy function is in general not concave everywhere. In\nparticular, we show that every code picked from an ensemble of regular\nlow-density parity-check codes with minimum Hamming distance growing (with high\nprobability) linearly with the block length has a Bethe entropy function that\nis convex in certain regions of its domain.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 00:00:05 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2012 23:44:16 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Vontobel", "Pascal O.", ""]]}, {"id": "1012.0084", "submitter": "Karthik Shastry R", "authors": "Harshith C, Karthik R. Shastry, Manoj Ravindran, M.V.V.N.S. Srikanth,\n  Naveen Lakshmikhanth", "title": "Survey on Various Gesture Recognition Techniques for Interfacing\n  Machines Based on Ambient Intelligence", "comments": "12 PAGES", "journal-ref": null, "doi": "10.5121/ijcses.2010.1203", "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is mainly apprehensive on analyzing the functionality of\nhuman wits. The main goal of gesture recognition is to create a system which\ncan recognize specific human gestures and use them to convey information or for\ndevice control. Hand gestures provide a separate complementary modality to\nspeech for expressing ones ideas. Information associated with hand gestures in\na conversation is degree,discourse structure, spatial and temporal structure.\nThe approaches present can be mainly divided into Data-Glove Based and Vision\nBased approaches. An important face feature point is the nose tip. Since nose\nis the highest protruding point from the face. Besides that, it is not affected\nby facial expressions.Another important function of the nose is that it is able\nto indicate the head pose. Knowledge of the nose location will enable us to\nalign an unknown 3D face with those in a face database. Eye detection is\ndivided into eye position detection and eye contour detection. Existing works\nin eye detection can be classified into two major categories: traditional\nimage-based passive approaches and the active IR based approaches. The former\nuses intensity and shape of eyes for detection and the latter works on the\nassumption that eyes have a reflection under near IR illumination and produce\nbright/dark pupil effect. The traditional methods can be broadly classified\ninto three categories: template based methods,appearance based methods and\nfeature based methods. The purpose of this paper is to compare various human\nGesture recognition systems for interfacing machines directly to human wits\nwithout any corporeal media in an ambient environment.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 02:54:24 GMT"}], "update_date": "2010-12-02", "authors_parsed": [["C", "Harshith", ""], ["Shastry", "Karthik R.", ""], ["Ravindran", "Manoj", ""], ["Srikanth", "M. V. V. N. S.", ""], ["Lakshmikhanth", "Naveen", ""]]}, {"id": "1012.0322", "submitter": "Vitaly Schetinin", "authors": "Vitaly Schetinin, Jonathan Fieldsend, Derek Partridge, Wojtek\n  Krzanowski, Richard Everson, Trevor Bailey and Adolfo Hernandez", "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in\n  Safety-Critical Systems", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence and Applications. Volume 149,\n  IOS Press Book, 2006. Integrated Intelligent Systems for Engineering Design.\n  Edited by Xuan F. Zha, R.J. Howlett. ISBN 978-1-58603-675-1, pp. 82-96", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty of decisions in safety-critical engineering applications can be\nestimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC)\ntechnique of averaging over decision models. The use of decision tree (DT)\nmodels assists experts to interpret causal relations and find factors of the\nuncertainty. Bayesian averaging also allows experts to estimate the uncertainty\naccurately when a priori information on the favored structure of DTs is\navailable. Then an expert can select a single DT model, typically the Maximum a\nPosteriori model, for interpretation purposes. Unfortunately, a priori\ninformation on favored structure of DTs is not always available. For this\nreason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also\nsuggest a new procedure of selecting a single DT and describe an application\nscenario. In our experiments on the Short-Term Conflict Alert data our\ntechnique outperforms the existing Bayesian techniques in predictive accuracy\nof the selected single DTs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 21:08:04 GMT"}], "update_date": "2010-12-03", "authors_parsed": [["Schetinin", "Vitaly", ""], ["Fieldsend", "Jonathan", ""], ["Partridge", "Derek", ""], ["Krzanowski", "Wojtek", ""], ["Everson", "Richard", ""], ["Bailey", "Trevor", ""], ["Hernandez", "Adolfo", ""]]}, {"id": "1012.0365", "submitter": "Zhouchen Lin", "authors": "Zhouchen Lin and Siming Wei", "title": "A Block Lanczos with Warm Start Technique for Accelerating Nuclear Norm\n  Minimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "Microsoft Technical Report #MSR-TR-2010-162", "categories": "cs.NA cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the popularity of using rank minimization as a\nregularizer for various signal processing and machine learning problems. As\nrank minimization problems are often converted to nuclear norm minimization\n(NNM) problems, they have to be solved iteratively and each iteration requires\ncomputing a singular value decomposition (SVD). Therefore, their solution\nsuffers from the high computation cost of multiple SVDs. To relieve this issue,\nwe propose using the block Lanczos method to compute the partial SVDs, where\nthe principal singular subspaces obtained in the previous iteration are used to\nstart the block Lanczos procedure. To avoid the expensive reorthogonalization\nin the Lanczos procedure, the block Lanczos procedure is performed for only a\nfew steps. Our block Lanczos with warm start (BLWS) technique can be adopted by\ndifferent algorithms that solve NNM problems. We present numerical results on\napplying BLWS to Robust PCA and Matrix Completion problems. Experimental\nresults show that our BLWS technique usually accelerates its host algorithm by\nat least two to three times.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 01:59:21 GMT"}, {"version": "v2", "created": "Sun, 26 Dec 2010 07:39:11 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Lin", "Zhouchen", ""], ["Wei", "Siming", ""]]}, {"id": "1012.0729", "submitter": "Prasad Raghavendra", "authors": "Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, Yi Wu", "title": "Agnostic Learning of Monomials by Halfspaces is Hard", "comments": "37 pages, Preliminary version appeared in FOCS 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the following strong hardness result for learning: Given a\ndistribution of labeled examples from the hypercube such that there exists a\nmonomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a\nhalfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary\nconstants $\\eps > 0$. In learning theory terms, weak agnostic learning of\nmonomials is hard, even if one is allowed to output a hypothesis from the much\nbigger concept class of halfspaces. This hardness result subsumes a long line\nof previous results, including two recent hardness results for the proper\nlearning of monomials and halfspaces. As an immediate corollary of our result\nwe show that weak agnostic learning of decision lists is NP-hard.\n  Our techniques are quite different from previous hardness proofs for\nlearning. We define distributions on positive and negative examples for\nmonomials whose first few moments match. We use the invariance principle to\nargue that regular halfspaces (all of whose coefficients have small absolute\nvalue relative to the total $\\ell_2$ norm) cannot distinguish between\ndistributions whose first few moments match. For highly non-regular subspaces,\nwe use a structural lemma from recent work on fooling halfspaces to argue that\nthey are ``junta-like'' and one can zero out all but the top few coefficients\nwithout affecting the performance of the halfspace. The top few coefficients\nform the natural list decoding of a halfspace in the context of dictatorship\ntests/Label Cover reductions.\n  We note that unlike previous invariance principle based proofs which are only\nknown to give Unique-Games hardness, we are able to reduce from a version of\nLabel Cover problem that is known to be NP-hard. This has inspired follow-up\nwork on bypassing the Unique Games conjecture in some optimal geometric\ninapproximability results.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:11:22 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Feldman", "Vitaly", ""], ["Guruswami", "Venkatesan", ""], ["Raghavendra", "Prasad", ""], ["Wu", "Yi", ""]]}, {"id": "1012.0735", "submitter": "Jos\\'e L Balc\\'azar Navarro", "authors": "Jos\\'e L. Balc\\'azar, Diego Garc\\'ia-Saiz, Domingo G\\'omez-P\\'erez,\n  Cristina T\\^irn\\u{a}uc\\u{a}", "title": "Closed-set-based Discovery of Bases of Association Rules", "comments": "Shorter version in: Ali Khenchaf and Pascal Poncelet (eds.),\n  Extraction et gestion des connaissances (EGC'2011)", "journal-ref": "Revue des Nouvelles Technologies de l'Information RNTI-E-20\n  (2011), pages 635-646", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output of an association rule miner is often huge in practice. This is\nwhy several concise lossless representations have been proposed, such as the\n\"essential\" or \"representative\" rules. We revisit the algorithm given by\nKryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS\n2189, 350-359) for mining representative rules. We show that its output is\nsometimes incomplete, due to an oversight in its mathematical validation. We\npropose alternative complete generators and we extend the approach to an\nexisting closure-aware basis similar to, and often smaller than, the\nrepresentative rules, namely the basis B*.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:29:01 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 16:38:44 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["Garc\u00eda-Saiz", "Diego", ""], ["G\u00f3mez-P\u00e9rez", "Domingo", ""], ["T\u00eern\u0103uc\u0103", "Cristina", ""]]}, {"id": "1012.0742", "submitter": "Jos\\'e L Balc\\'azar Navarro", "authors": "Jos\\'e L. Balc\\'azar, Cristina T\\^irn\\u{a}uc\\u{a}", "title": "Border Algorithms for Computing Hasse Diagrams of Arbitrary Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Border algorithm and the iPred algorithm find the Hasse diagrams of FCA\nlattices. We show that they can be generalized to arbitrary lattices. In the\ncase of iPred, this requires the identification of a join-semilattice\nhomomorphism into a distributive lattice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:57:32 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["T\u00eern\u0103uc\u0103", "Cristina", ""]]}, {"id": "1012.0830", "submitter": "Yves Moinard", "authors": "Yves Moinard (INRIA - IRISA)", "title": "Using ASP with recent extensions for causal explanations", "comments": null, "journal-ref": "ASPOCP10, Answer Set Programming and Other Computing Paradigms\n  Workshop, associated with ICLP, Edinburgh : United Kingdom (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the practicality for a user of using Answer Set Programming (ASP)\nfor representing logical formalisms. We choose as an example a formalism aiming\nat capturing causal explanations from causal information. We provide an\nimplementation, showing the naturalness and relative efficiency of this\ntranslation job. We are interested in the ease for writing an ASP program, in\naccordance with the claimed ``declarative'' aspect of ASP. Limitations of the\nearlier systems (poor data structure and difficulty in reusing pieces of\nprograms) made that in practice, the ``declarative aspect'' was more\ntheoretical than practical. We show how recent improvements in working ASP\nsystems facilitate a lot the translation, even if a few improvements could\nstill be useful.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 20:07:21 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Moinard", "Yves", "", "INRIA - IRISA"]]}, {"id": "1012.0841", "submitter": "Pekka Malo", "authors": "Pekka Malo and Pyry Siitari and Ankur Sinha", "title": "Automated Query Learning with Wikipedia and Genetic Programming", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing information retrieval systems are based on bag of words\nmodel and are not equipped with common world knowledge. Work has been done\ntowards improving the efficiency of such systems by using intelligent\nalgorithms to generate search queries, however, not much research has been done\nin the direction of incorporating human-and-society level knowledge in the\nqueries. This paper is one of the first attempts where such information is\nincorporated into the search queries using Wikipedia semantics. The paper\npresents an essential shift from conventional token based queries to concept\nbased queries, leading to an enhanced efficiency of information retrieval\nsystems. To efficiently handle the automated query learning problem, we propose\nWikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based\nqueries are learnt using a co-evolving evolutionary procedure. Learning concept\nbased queries using an intelligent evolutionary procedure yields significant\nimprovement in performance which is shown through an extensive study using\nReuters newswire documents. Comparison of the proposed framework is performed\nwith other information retrieval systems. Concept based approach has also been\nimplemented on other information retrieval systems to justify the effectiveness\nof a transition from token based queries to concept based queries.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 20:53:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Malo", "Pekka", ""], ["Siitari", "Pyry", ""], ["Sinha", "Ankur", ""]]}, {"id": "1012.0930", "submitter": "Zhi-Hua Zhou", "authors": "Nan Li and Ivor W. Tsang and Zhi-Hua Zhou", "title": "Efficient Optimization of Performance Measures by Classifier Adaptation", "comments": "30 pages, 5 figures, to appear in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 2012", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2013, 35(6): 1370-1382", "doi": "10.1109/TPAMI.2012.172", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications, machine learning algorithms are often needed to\nlearn classifiers that optimize domain specific performance measures.\nPreviously, the research has focused on learning the needed classifier in\nisolation, yet learning nonlinear classifier for nonlinear and nonsmooth\nperformance measures is still hard. In this paper, rather than learning the\nneeded classifier by optimizing specific performance measure directly, we\ncircumvent this problem by proposing a novel two-step approach called as CAPO,\nnamely to first train nonlinear auxiliary classifiers with existing learning\nmethods, and then to adapt auxiliary classifiers for specific performance\nmeasures. In the first step, auxiliary classifiers can be obtained efficiently\nby taking off-the-shelf learning algorithms. For the second step, we show that\nthe classifier adaptation problem can be reduced to a quadratic program\nproblem, which is similar to linear SVMperf and can be efficiently solved. By\nexploiting nonlinear auxiliary classifiers, CAPO can generate nonlinear\nclassifier which optimizes a large variety of performance measures including\nall the performance measure based on the contingency table and AUC, whilst\nkeeping high computational efficiency. Empirical studies show that CAPO is\neffective and of high computational efficiency, and even it is more efficient\nthan linear SVMperf.\n", "versions": [{"version": "v1", "created": "Sat, 4 Dec 2010 16:08:08 GMT"}, {"version": "v2", "created": "Tue, 7 Dec 2010 08:27:25 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2012 11:27:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Li", "Nan", ""], ["Tsang", "Ivor W.", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1012.1255", "submitter": "Predrag Janicic", "authors": "Predrag Janicic (University of Belgrade)", "title": "URSA: A System for Uniform Reduction to SAT", "comments": "39 pages, uses tikz.sty", "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 3 (September\n  30, 2012) lmcs:1171", "doi": "10.2168/LMCS-8(3:30)2012", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a huge number of problems, from various areas, being solved by\nreducing them to SAT. However, for many applications, translation into SAT is\nperformed by specialized, problem-specific tools. In this paper we describe a\nnew system for uniform solving of a wide class of problems by reducing them to\nSAT. The system uses a new specification language URSA that combines imperative\nand declarative programming paradigms. The reduction to SAT is defined\nprecisely by the semantics of the specification language. The domain of the\napproach is wide (e.g., many NP-complete problems can be simply specified and\nthen solved by the system) and there are problems easily solvable by the\nproposed system, while they can be hardly solved by using other programming\nlanguages or constraint programming systems. So, the system can be seen not\nonly as a tool for solving problems by reducing them to SAT, but also as a\ngeneral-purpose constraint solving system (for finite domains). In this paper,\nwe also describe an open-source implementation of the described approach. The\nperformed experiments suggest that the system is competitive to\nstate-of-the-art related modelling systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Dec 2010 17:40:33 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2012 10:30:58 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 21:30:27 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Janicic", "Predrag", "", "University of Belgrade"]]}, {"id": "1012.1552", "submitter": "Emad Saad", "authors": "Emad Saad", "title": "Bridging the Gap between Reinforcement Learning and Knowledge\n  Representation: A Logical Off- and On-Policy Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Representation is important issue in reinforcement learning. In\nthis paper, we bridge the gap between reinforcement learning and knowledge\nrepresentation, by providing a rich knowledge representation framework, based\non normal logic programs with answer set semantics, that is capable of solving\nmodel-free reinforcement learning problems for more complex do-mains and\nexploits the domain-specific knowledge. We prove the correctness of our\napproach. We show that the complexity of finding an offline and online policy\nfor a model-free reinforcement learning problem in our approach is NP-complete.\nMoreover, we show that any model-free reinforcement learning problem in MDP\nenvironment can be encoded as a SAT problem. The importance of that is\nmodel-free reinforcement\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 16:57:54 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Saad", "Emad", ""]]}, {"id": "1012.1615", "submitter": "Adrian Paschke", "authors": "Kenneth McLeod, Gus Ferguson, Albert Burger", "title": "Argudas: arguing with gene expression information", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In situ hybridisation gene expression information helps biologists identify\nwhere a gene is expressed. However, the databases that republish the\nexperimental information are often both incomplete and inconsistent. This paper\nexamines a system, Argudas, designed to help tackle these issues. Argudas is an\nevolution of an existing system, and so that system is reviewed as a means of\nboth explaining and justifying the behaviour of Argudas. Throughout the\ndiscussion of Argudas a number of issues will be raised including the\nappropriateness of argumentation in biology and the challenges faced when\nintegrating apparently similar online biological databases.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 21:34:26 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["McLeod", "Kenneth", ""], ["Ferguson", "Gus", ""], ["Burger", "Albert", ""]]}, {"id": "1012.1619", "submitter": "Adrian Paschke", "authors": "Pablo Lopez-Garcia", "title": "Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in\nthe life sciences, with more than 300,000 concepts and relationships, but is\ndistributed with no associated software tools. In this paper we present MySNOM,\na web-based SNOMED CT browser. MySNOM allows organizations to browse their own\ndistribution of SNOMED CT under a controlled environment, focuses on navigating\nusing the structure of SNOMED CT, and has diagramming capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 21:45:50 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Lopez-Garcia", "Pablo", ""]]}, {"id": "1012.1635", "submitter": "Adrian Paschke", "authors": "He Tan", "title": "A study on the relation between linguistics-oriented and domain-specific\n  semantics", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we dealt with the comparison and linking between lexical\nresources with domain knowledge provided by ontologies. It is one of the issues\nfor the combination of the Semantic Web Ontologies and Text Mining. We\ninvestigated the relations between the linguistics oriented and domain-specific\nsemantics, by associating the GO biological process concepts to the FrameNet\nsemantic frames. The result shows the gaps between the linguistics-oriented and\ndomain-specific semantics on the classification of events and the grouping of\ntarget words. The result provides valuable information for the improvement of\ndomain ontologies supporting for text mining systems. And also, it will result\nin benefits to language understanding technology.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 23:03:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Tan", "He", ""]]}, {"id": "1012.1643", "submitter": "Adrian Paschke", "authors": "Adrian Paschke, Zhili Zhao", "title": "Process Makna - A Semantic Wiki for Scientific Workflows", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual e-Science infrastructures supporting Web-based scientific workflows\nare an example for knowledge-intensive collaborative and weakly-structured\nprocesses where the interaction with the human scientists during process\nexecution plays a central role. In this paper we propose the lightweight\ndynamic user-friendly interaction with humans during execution of scientific\nworkflows via the low-barrier approach of Semantic Wikis as an intuitive\ninterface for non-technical scientists. Our Process Makna Semantic Wiki system\nis a novel combination of an business process management system adapted for\nscientific workflows with a Corporate Semantic Web Wiki user interface\nsupporting knowledge intensive human interaction tasks during scientific\nworkflow execution.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 23:49:29 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Paschke", "Adrian", ""], ["Zhao", "Zhili", ""]]}, {"id": "1012.1646", "submitter": "Adrian Paschke", "authors": "Richard Huber, Kirsten Hantelmann, Alexandru Todor, Sebastian Krebs,\n  Ralf Heese and Adrian Paschke", "title": "Use of semantic technologies for the development of a dynamic\n  trajectories generator in a Semantic Chemistry eLearning platform", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ChemgaPedia is a multimedia, webbased eLearning service platform that\ncurrently contains about 18.000 pages organized in 1.700 chapters covering the\ncomplete bachelor studies in chemistry and related topics of chemistry,\npharmacy, and life sciences. The eLearning encyclopedia contains some 25.000\nmedia objects and the eLearning platform provides services such as virtual and\nremote labs for experiments. With up to 350.000 users per month the platform is\nthe most frequently used scientific educational service in the German spoken\nInternet. In this demo we show the benefit of mapping the static eLearning\ncontents of ChemgaPedia to a Linked Data representation for Semantic Chemistry\nwhich allows for generating dynamic eLearning paths tailored to the semantic\nprofiles of the users.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 23:55:47 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Huber", "Richard", ""], ["Hantelmann", "Kirsten", ""], ["Todor", "Alexandru", ""], ["Krebs", "Sebastian", ""], ["Heese", "Ralf", ""], ["Paschke", "Adrian", ""]]}, {"id": "1012.1648", "submitter": "Adrian Paschke", "authors": "Matt Holford, James McCusker, Kei Cheung and Michael Krauthammer", "title": "Analysis Of Cancer Omics Data In A Semantic Web Framework", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work concerns the elucidation of the cancer (epi)genome, transcriptome\nand proteome to better understand the complex interplay between a cancer cell's\nmolecular state and its response to anti-cancer therapy. To study the problem,\nwe have previously focused on data warehousing technologies and statistical\ndata integration. In this paper, we present recent work on extending our\nanalytical capabilities using Semantic Web technology. A key new component\npresented here is a SPARQL endpoint to our existing data warehouse. This\nendpoint allows the merging of observed quantitative data with existing data\nfrom semantic knowledge sources such as Gene Ontology (GO). We show how such\nvariegated quantitative and functional data can be integrated and accessed in a\nuniversal manner using Semantic Web tools. We also demonstrate how Description\nLogic (DL) reasoning can be used to infer previously unstated conclusions from\nexisting knowledge bases. As proof of concept, we illustrate the ability of our\nsetup to answer complex queries on resistance of cancer cells to Decitabine, a\ndemethylating agent.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:06:28 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Holford", "Matt", ""], ["McCusker", "James", ""], ["Cheung", "Kei", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1012.1654", "submitter": "Adrian Paschke", "authors": "Adrian Groza, Radu Balaj", "title": "Using Semantic Wikis for Structured Argument in Medical Domain", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research applies ideas from argumentation theory in the context of\nsemantic wikis, aiming to provide support for structured-large scale\nargumentation between human agents. The implemented prototype is exemplified by\nmodelling the MMR vaccine controversy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:34:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Groza", "Adrian", ""], ["Balaj", "Radu", ""]]}, {"id": "1012.1658", "submitter": "Adrian Paschke", "authors": "Julia Dmitrieva, Fons J. Verbeek", "title": "Creating a new Ontology: a Modular Approach", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating a new Ontology: a Modular Approach\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:41:20 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Dmitrieva", "Julia", ""], ["Verbeek", "Fons J.", ""]]}, {"id": "1012.1659", "submitter": "Adrian Paschke", "authors": "Ernesto Jimenez-Ruiz, Bernardo Cuenca Grau, Rafael Berlanga and\n  Dietrich Rebholz-Schuhmann", "title": "First steps in the logic-based assessment of post-composed phenotypic\n  descriptions", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a preliminary logic-based evaluation of the\nintegration of post-composed phenotypic descriptions with domain ontologies.\nThe evaluation has been performed using a description logic reasoner together\nwith scalable techniques: ontology modularization and approximations of the\nlogical difference between ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:47:59 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Jimenez-Ruiz", "Ernesto", ""], ["Grau", "Bernardo Cuenca", ""], ["Berlanga", "Rafael", ""], ["Rebholz-Schuhmann", "Dietrich", ""]]}, {"id": "1012.1661", "submitter": "Adrian Paschke", "authors": "Catherine Canevet, Artem Lysenko, Andrea Splendiani, Matthew Pocock,\n  Christopher Rawlings", "title": "Analysis and visualisation of RDF resources in Ondex", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ondex is a data integration and visualization platform developed to support\nSystems Biology Research. At its core is a data model based on two main\nprinciples: first, all information can be represented as a graph and, second,\nall elements of the graph can be annotated with ontologies. This data model is\nconformant to the Semantic Web framework, in particular to RDF, and therefore\nOndex is ideally positioned as a platform that can exploit the semantic web.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:54:27 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Canevet", "Catherine", ""], ["Lysenko", "Artem", ""], ["Splendiani", "Andrea", ""], ["Pocock", "Matthew", ""], ["Rawlings", "Christopher", ""]]}, {"id": "1012.1667", "submitter": "Adrian Paschke", "authors": "Maria Perez, Rafael Berlanga, Ismael Sanz", "title": "A semantic approach for the requirement-driven discovery of web services\n  in the Life Sciences", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the Life Sciences depends on the integration of large,\ndistributed and heterogeneous data sources and web services. The discovery of\nwhich of these resources are the most appropriate to solve a given task is a\ncomplex research question, since there is a large amount of plausible\ncandidates and there is little, mostly unstructured, metadata to be able to\ndecide among them.We contribute a semi-automatic approach,based on semantic\ntechniques, to assist researchers in the discovery of the most appropriate web\nservices to full a set of given requirements.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 01:12:57 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Perez", "Maria", ""], ["Berlanga", "Rafael", ""], ["Sanz", "Ismael", ""]]}, {"id": "1012.1743", "submitter": "Adrian Paschke", "authors": "Eric Leclercq and Marinette Savonnet", "title": "Scientific Collaborations: principles of WikiBridge Design", "comments": "in Adrian Paschke, Albert Burger begin_of_the_skype_highlighting\n  end_of_the_skype_highlighting, Andrea Splendiani, M. Scott Marshall, Paolo\n  Romano: Proceedings of the 3rd International Workshop on Semantic Web\n  Applications and Tools for the Life Sciences, Berlin,Germany, December 8-10,\n  2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic wikis, wikis enhanced with Semantic Web technologies, are\nappropriate systems for community-authored knowledge models. They are\nparticularly suitable for scientific collaboration. This paper details the\ndesign principles ofWikiBridge, a semantic wiki.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 11:43:37 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Leclercq", "Eric", ""], ["Savonnet", "Marinette", ""]]}, {"id": "1012.1745", "submitter": "Adrian Paschke", "authors": "Simon Jupp, Matthew Horridge, Luigi Iannone, Julie Klein, Stuart Owen,\n  Joost Schanstra, Robert Stevens, Katy Wolstencroft", "title": "Populous: A tool for populating ontology templates", "comments": "in Adrian Paschke, Albert Burger begin_of_the_skype_highlighting\n  end_of_the_skype_highlighting, Andrea Splendiani, M. Scott Marshall, Paolo\n  Romano: Proceedings of the 3rd International Workshop on Semantic Web\n  Applications and Tools for the Life Sciences, Berlin,Germany, December 8-10,\n  2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Populous, a tool for gathering content with which to populate an\nontology. Domain experts need to add content, that is often repetitive in its\nform, but without having to tackle the underlying ontological representation.\nPopulous presents users with a table based form in which columns are\nconstrained to take values from particular ontologies; the user can select a\nconcept from an ontology via its meaningful label to give a value for a given\nentity attribute. Populated tables are mapped to patterns that can then be used\nto automatically generate the ontology's content. Populous's contribution is in\nthe knowledge gathering stage of ontology development. It separates knowledge\ngathering from the conceptualisation and also separates the user from the\nstandard ontology authoring environments. As a result, Populous can allow\nknowledge to be gathered in a straight-forward manner that can then be used to\ndo mass production of ontology content.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 11:55:06 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Jupp", "Simon", ""], ["Horridge", "Matthew", ""], ["Iannone", "Luigi", ""], ["Klein", "Julie", ""], ["Owen", "Stuart", ""], ["Schanstra", "Joost", ""], ["Stevens", "Robert", ""], ["Wolstencroft", "Katy", ""]]}, {"id": "1012.1899", "submitter": "Adrian Paschke", "authors": "Halit Erdogan, Umut Oztok, Yelda Erdem, Esra Erdem", "title": "Querying Biomedical Ontologies in Natural Language using Answer Set", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop an intelligent user interface that allows users to\nenter biomedical queries in a natural language, and that presents the answers\n(possibly with explanations if requested) in a natural language. We develop a\nrule layer over biomedical ontologies and databases, and use automated\nreasoners to answer queries considering relevant parts of the rule layer.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 00:12:24 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Erdogan", "Halit", ""], ["Oztok", "Umut", ""], ["Erdem", "Yelda", ""], ["Erdem", "Esra", ""]]}, {"id": "1012.2042", "submitter": "George Giannakopoulos", "authors": "George Giannakopoulos (1) and George Vouros (2) and Vangelis\n  Karkaletsis (1) ((1) NCSR Demokritos, Greece, (2) University of the Aegean,\n  Greece)", "title": "MUDOS-NG: Multi-document Summaries Using N-gram Graphs (Tech Report)", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the MUDOS-NG summarization system, which applies a set\nof language-independent and generic methods for generating extractive\nsummaries. The proposed methods are mostly combinations of simple operators on\na generic character n-gram graph representation of texts. This work defines the\nset of used operators upon n-gram graphs and proposes using these operators\nwithin the multi-document summarization process in such subtasks as document\nanalysis, salient sentence selection, query expansion and redundancy control.\nFurthermore, a novel chunking methodology is used, together with a novel way to\nassign concepts to sentences for query expansion. The experimental results of\nthe summarization system, performed upon widely used corpora from the Document\nUnderstanding and the Text Analysis Conferences, are promising and provide\nevidence for the potential of the generic methods introduced. This work aims to\ndesignate core methods exploiting the n-gram graph representation, providing\nthe basis for more advanced summarization systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 16:15:34 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Giannakopoulos", "George", ""], ["Vouros", "George", ""], ["Karkaletsis", "Vangelis", ""]]}, {"id": "1012.2148", "submitter": "Yongzhi Cao", "authors": "Yongzhi Cao, Guoqing Chen, and Etienne Kerre", "title": "Bisimulations for fuzzy transition systems", "comments": "13 double column pages", "journal-ref": "IEEE Trans. Fuzzy Syst., vol. 19, no. 3, pp. 540-552, 2011", "doi": "10.1109/TFUZZ.2011.2117431", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  There has been a long history of using fuzzy language equivalence to compare\nthe behavior of fuzzy systems, but the comparison at this level is too coarse.\nRecently, a finer behavioral measure, bisimulation, has been introduced to\nfuzzy finite automata. However, the results obtained are applicable only to\nfinite-state systems. In this paper, we consider bisimulation for general fuzzy\nsystems which may be infinite-state or infinite-event, by modeling them as\nfuzzy transition systems. To help understand and check bisimulation, we\ncharacterize it in three ways by enumerating whole transitions, comparing\nindividual transitions, and using a monotonic function. In addition, we address\ncomposition operations, subsystems, quotients, and homomorphisms of fuzzy\ntransition systems and discuss their properties connected with bisimulation.\nThe results presented here are useful for comparing the behavior of general\nfuzzy systems. In particular, this makes it possible to relate an infinite\nfuzzy system to a finite one, which is easier to analyze, with the same\nbehavior.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 00:24:42 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cao", "Yongzhi", ""], ["Chen", "Guoqing", ""], ["Kerre", "Etienne", ""]]}, {"id": "1012.2162", "submitter": "Yongzhi Cao", "authors": "Yongzhi Cao and Yoshinori Ezawa", "title": "Nondeterministic fuzzy automata", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Fuzzy automata have long been accepted as a generalization of\nnondeterministic finite automata. A closer examination, however, shows that the\nfundamental property---nondeterminism---in nondeterministic finite automata has\nnot been well embodied in the generalization. In this paper, we introduce\nnondeterministic fuzzy automata with or without $\\el$-moves and fuzzy languages\nrecognized by them. Furthermore, we prove that (deterministic) fuzzy automata,\nnondeterministic fuzzy automata, and nondeterministic fuzzy automata with\n$\\el$-moves are all equivalent in the sense that they recognize the same class\nof fuzzy languages.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 02:42:30 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cao", "Yongzhi", ""], ["Ezawa", "Yoshinori", ""]]}, {"id": "1012.2496", "submitter": "Salvador Abreu", "authors": "Daniel Diaz, Salvador Abreu and Philippe Codognet", "title": "On the Implementation of GNU Prolog", "comments": "30 pages, 3 figures, To appear in Theory and Practice of Logic\n  Programming (TPLP); Keywords: Prolog, logic programming system, GNU, ISO,\n  WAM, native code compilation, Finite Domain constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GNU Prolog is a general-purpose implementation of the Prolog language, which\ndistinguishes itself from most other systems by being, above all else, a\nnative-code compiler which produces standalone executables which don't rely on\nany byte-code emulator or meta-interpreter. Other aspects which stand out\ninclude the explicit organization of the Prolog system as a multipass compiler,\nwhere intermediate representations are materialized, in Unix compiler\ntradition. GNU Prolog also includes an extensible and high-performance finite\ndomain constraint solver, integrated with the Prolog language but implemented\nusing independent lower-level mechanisms. This article discusses the main\nissues involved in designing and implementing GNU Prolog: requirements, system\norganization, performance and portability issues as well as its position with\nrespect to other Prolog system implementations and the ISO standardization\ninitiative.\n", "versions": [{"version": "v1", "created": "Sat, 11 Dec 2010 23:23:11 GMT"}, {"version": "v2", "created": "Wed, 15 Dec 2010 15:07:58 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Diaz", "Daniel", ""], ["Abreu", "Salvador", ""], ["Codognet", "Philippe", ""]]}, {"id": "1012.2609", "submitter": "Deqing Wang", "authors": "Deqing Wang, Hui Zhang", "title": "Inverse-Category-Frequency based supervised term weighting scheme for\n  text categorization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Term weighting schemes often dominate the performance of many classifiers,\nsuch as kNN, centroid-based classifier and SVMs. The widely used term weighting\nscheme in text categorization, i.e., tf.idf, is originated from information\nretrieval (IR) field. The intuition behind idf for text categorization seems\nless reasonable than IR. In this paper, we introduce inverse category frequency\n(icf) into term weighting scheme and propose two novel approaches, i.e., tf.icf\nand icf-based supervised term weighting schemes. The tf.icf adopts icf to\nsubstitute idf factor and favors terms occurring in fewer categories, rather\nthan fewer documents. And the icf-based approach combines icf and relevance\nfrequency (rf) to weight terms in a supervised way. Our cross-classifier and\ncross-corpus experiments have shown that our proposed approaches are superior\nor comparable to six supervised term weighting schemes and three traditional\nschemes in terms of macro-F1 and micro-F1.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 01:22:36 GMT"}, {"version": "v2", "created": "Tue, 14 Dec 2010 09:26:49 GMT"}, {"version": "v3", "created": "Sat, 24 Dec 2011 02:34:31 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2012 03:29:13 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Wang", "Deqing", ""], ["Zhang", "Hui", ""]]}, {"id": "1012.2713", "submitter": "Minghao Yin", "authors": "Junping Zhou and Minghao Yin", "title": "Phase Transitions of Plan Modification in Conformant Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We explore phase transitions of plan modification, which mainly focus on the\nconformant planning problems. By analyzing features of plan modification in\nconformant planning problems, quantitative results are obtained. If the number\nof operators is less than, almost all conformant planning problems can't be\nsolved with plan modification. If the number of operators is more than, almost\nall conformant planning problems can be solved with plan modification. The\nresults of the experiments also show that there exists an experimental\nthreshold of density (ratio of number of operators to number of propositions),\nwhich separates the region where almost all conformant planning problems can't\nbe solved with plan modification from the region where almost all conformant\nplanning problems can be solved with plan modification.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 12:42:15 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Zhou", "Junping", ""], ["Yin", "Minghao", ""]]}, {"id": "1012.2789", "submitter": "Xiaoyue Wang Dr", "authors": "Xiaoyue Wang and Hui Ding and Goce Trajcevski and Peter Scheuermann\n  and Eamonn Keogh", "title": "Experimental Comparison of Representation Methods and Distance Measures\n  for Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The previous decade has brought a remarkable increase of the interest in\napplications that deal with querying and mining of time series data. Many of\nthe research efforts in this context have focused on introducing new\nrepresentation methods for dimensionality reduction or novel similarity\nmeasures for the underlying data. In the vast majority of cases, each\nindividual work introducing a particular method has made specific claims and,\naside from the occasional theoretical justifications, provided quantitative\nexperimental observations. However, for the most part, the comparative aspects\nof these experiments were too narrowly focused on demonstrating the benefits of\nthe proposed methods over some of the previously introduced ones. In order to\nprovide a comprehensive validation, we conducted an extensive experimental\nstudy re-implementing eight different time series representations and nine\nsimilarity measures and their variants, and testing their effectiveness on\nthirty-eight time series data sets from a wide variety of application domains.\nIn this paper, we give an overview of these different techniques and present\nour comparative experimental findings regarding their effectiveness. In\naddition to providing a unified validation of some of the existing\nachievements, our experiments also indicate that, in some cases, certain claims\nin the literature may be unduly optimistic.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 19:43:53 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wang", "Xiaoyue", ""], ["Ding", "Hui", ""], ["Trajcevski", "Goce", ""], ["Scheuermann", "Peter", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1012.3018", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore and Marco Schaerf", "title": "On the size of data structures used in symbolic model checking", "comments": null, "journal-ref": null, "doi": "10.1109/TC.2015.2512872", "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 13:18:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Liberatore", "Paolo", ""], ["Schaerf", "Marco", ""]]}, {"id": "1012.3148", "submitter": "Kush Agrawal", "authors": "Kush Agrawal", "title": "To study the phenomenon of the Moravec's Paradox", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Encoded in the large, highly evolved sensory and motor portions of the human\nbrain is a billion years of experience about the nature of the world and how to\nsurvive in it. The deliberate process we call reasoning is, I believe, the\nthinnest veneer of human thought, effective only because it is supported by\nthis much older and much powerful, though usually unconscious, sensor motor\nknowledge. We are all prodigious Olympians in perceptual and motor areas, so\ngood that we make the difficult look easy. Abstract thought, though, is a new\ntrick, perhaps less than 100 thousand years old. We have not yet mastered it.\nIt is not all that intrinsically difficult; it just seems so when we do it.\"-\nHans Moravec Moravec's paradox is involved with the fact that it is the\nseemingly easier day to day problems that are harder to implement in a machine,\nthan the seemingly complicated logic based problems of today. The results prove\nthat most artificially intelligent machines are as adept if not more than us at\nunder-taking long calculations or even play chess, but their logic brings them\nnowhere when it comes to carrying out everyday tasks like walking, facial\ngesture recognition or speech recognition.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 19:47:24 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Agrawal", "Kush", ""]]}, {"id": "1012.3280", "submitter": "Cedric Bernier", "authors": "Samuel Nowakowski (LORIA), C\\'edric Bernier (LORIA), Anne Boyer\n  (LORIA)", "title": "A new Recommender system based on target tracking: a Kalman Filter\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach for recommender systems based on\ntarget tracking by Kalman filtering. We assume that users and their seen\nresources are vectors in the multidimensional space of the categories of the\nresources. Knowing this space, we propose an algorithm based on a Kalman filter\nto track users and to predict the best prediction of their future position in\nthe recommendation space.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 11:07:09 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Nowakowski", "Samuel", "", "LORIA"], ["Bernier", "C\u00e9dric", "", "LORIA"], ["Boyer", "Anne", "", "LORIA"]]}, {"id": "1012.3312", "submitter": "Victor Odumuyiwa", "authors": "Bolanle Oladejo (LORIA), Victor Odumuyiwa (LORIA), Amos David (LORIA)", "title": "Dynamic Capitalization and Visualization Strategy in Collaborative\n  Knowledge Management System for EI Process", "comments": null, "journal-ref": "International Conference in Knowledge Management and Knowledge\n  Economy ICKMKE 2010, paris : France (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge is attributed to human whose problem-solving behavior is subjective\nand complex. In today's knowledge economy, the need to manage knowledge\nproduced by a community of actors cannot be overemphasized. This is due to the\nfact that actors possess some level of tacit knowledge which is generally\ndifficult to articulate. Problem-solving requires searching and sharing of\nknowledge among a group of actors in a particular context. Knowledge expressed\nwithin the context of a problem resolution must be capitalized for future\nreuse. In this paper, an approach that permits dynamic capitalization of\nrelevant and reliable actors' knowledge in solving decision problem following\nEconomic Intelligence process is proposed. Knowledge annotation method and\ntemporal attributes are used for handling the complexity in the communication\namong actors and in contextualizing expressed knowledge. A prototype is built\nto demonstrate the functionalities of a collaborative Knowledge Management\nsystem based on this approach. It is tested with sample cases and the result\nshowed that dynamic capitalization leads to knowledge validation hence\nincreasing reliability of captured knowledge for reuse. The system can be\nadapted to various domains\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 12:45:56 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Oladejo", "Bolanle", "", "LORIA"], ["Odumuyiwa", "Victor", "", "LORIA"], ["David", "Amos", "", "LORIA"]]}, {"id": "1012.3320", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Data Conflict Resolution Using Trust Mappings", "comments": "20 pages, 19 figures", "journal-ref": "Full version of SIGMOD 2010 conference paper, pp. 219-230", "doi": "10.1145/1807167.1807193", "report-no": "University of Washington CSE Technical Report 09-11-01", "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massively collaborative projects such as scientific or community\ndatabases, users often need to agree or disagree on the content of individual\ndata items. On the other hand, trust relationships often exist between users,\nallowing them to accept or reject other users' beliefs by default. As those\ntrust relationships become complex, however, it becomes difficult to define and\ncompute a consistent snapshot of the conflicting information. Previous\nsolutions to a related problem, the update reconciliation problem, are\ndependent on the order in which the updates are processed and, therefore, do\nnot guarantee a globally consistent snapshot. This paper proposes the first\nprincipled solution to the automatic conflict resolution problem in a community\ndatabase. Our semantics is based on the certain tuples of all stable models of\na logic program. While evaluating stable models in general is well known to be\nhard, even for very simple logic programs, we show that the conflict resolution\nproblem admits a PTIME solution. To the best of our knowledge, ours is the\nfirst PTIME algorithm that allows conflict resolution in a principled way. We\nfurther discuss extensions to negative beliefs and prove that some of these\nextensions are hard. This work is done in the context of the BeliefDB project\nat the University of Washington, which focuses on the efficient management of\nconflicts in community databases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 13:08:38 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1012.3336", "submitter": "Victor Odumuyiwa", "authors": "Olusoji Okunoye (LORIA), Bolanle Oladejo (LORIA), Victor Odumuyiwa\n  (LORIA)", "title": "Dynamic Knowledge Capitalization through Annotation among Economic\n  Intelligence Actors in a Collaborative Environment", "comments": null, "journal-ref": "Veille strat\\'egique et scientifique VSST 2010, Toulouse : France\n  (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift from industrial economy to knowledge economy in today's world has\nrevolutionalized strategic planning in organizations as well as their problem\nsolving approaches. The point of focus today is knowledge and service\nproduction with more emphasis been laid on knowledge capital. Many\norganizations are investing on tools that facilitate knowledge sharing among\ntheir employees and they are as well promoting and encouraging collaboration\namong their staff in order to build the organization's knowledge capital with\nthe ultimate goal of creating a lasting competitive advantage for their\norganizations. One of the current leading approaches used for solving\norganization's decision problem is the Economic Intelligence (EI) approach\nwhich involves interactions among various actors called EI actors. These actors\ncollaborate to ensure the overall success of the decision problem solving\nprocess. In the course of the collaboration, the actors express knowledge which\ncould be capitalized for future reuse. In this paper, we propose in the first\nplace, an annotation model for knowledge elicitation among EI actors. Because\nof the need to build a knowledge capital, we also propose a dynamic knowledge\ncapitalisation approach for managing knowledge produced by the actors. Finally,\nthe need to manage the interactions and the interdependencies among\ncollaborating EI actors, led to our third proposition which constitute an\nawareness mechanism for group work management.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 13:56:05 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Okunoye", "Olusoji", "", "LORIA"], ["Oladejo", "Bolanle", "", "LORIA"], ["Odumuyiwa", "Victor", "", "LORIA"]]}, {"id": "1012.3410", "submitter": "Joel Ratsaby", "authors": "Laszlo Kovacs and Joel Ratsaby", "title": "Descriptive-complexity based distance for fuzzy sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new distance function dist(A,B) for fuzzy sets A and B is introduced. It is\nbased on the descriptive complexity, i.e., the number of bits (on average) that\nare needed to describe an element in the symmetric difference of the two sets.\nThe distance gives the amount of additional information needed to describe any\none of the two sets given the other. We prove its mathematical properties and\nperform pattern clustering on data based on this distance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 18:02:27 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Kovacs", "Laszlo", ""], ["Ratsaby", "Joel", ""]]}, {"id": "1012.3853", "submitter": "Olivier Bailleux", "authors": "Olivier Bailleux", "title": "On the CNF encoding of cardinality constraints and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we propose a quick survey of the currently known techniques\nfor encoding a Boolean cardinality constraint into a CNF formula, and we\ndiscuss about the relevance of these encodings. We also propose models to\nfacilitate analysis and design of CNF encodings for Boolean constraints.\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 11:20:15 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["Bailleux", "Olivier", ""]]}, {"id": "1012.3947", "submitter": "Agust\\'in Valverde", "authors": "Dov Gabbay and David Pearce and Agust\\'i n Valverde", "title": "Interpolation in Equilibrium Logic and Answer Set Programming: the\n  Propositional Case", "comments": "ASPOCP 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation is an important property of classical and many non classical\nlogics that has been shown to have interesting applications in computer science\nand AI. Here we study the Interpolation Property for the propositional version\nof the non-monotonic system of equilibrium logic, establishing weaker or\nstronger forms of interpolation depending on the precise interpretation of the\ninference relation. These results also yield a form of interpolation for ground\nlogic programs under the answer sets semantics. For disjunctive logic programs\nwe also study the property of uniform interpolation that is closely related to\nthe concept of variable forgetting.\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 18:05:51 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["Gabbay", "Dov", ""], ["Pearce", "David", ""], ["Valverde", "Agust\u00ed n", ""]]}, {"id": "1012.4046", "submitter": "Tshilidzi Marwala", "authors": "Bo Xing, Wen-Jing Gao, Kimberly Battle, Tshildzi Marwala and Fulufhelo\n  V. Nelwamondo", "title": "Artificial Intelligence in Reverse Supply Chain Management: The State of\n  the Art", "comments": "Proceedings of the Twenty-First Annual Symposium of the Pattern\n  Recognition Association of South Africa 22-23 November 2010 Stellenbosch,\n  South Africa, pp. 305-310", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product take-back legislation forces manufacturers to bear the costs of\ncollection and disposal of products that have reached the end of their useful\nlives. In order to reduce these costs, manufacturers can consider reuse,\nremanufacturing and/or recycling of components as an alternative to disposal.\nThe implementation of such alternatives usually requires an appropriate reverse\nsupply chain management. With the concepts of reverse supply chain are gaining\npopularity in practice, the use of artificial intelligence approaches in these\nareas is also becoming popular. As a result, the purpose of this paper is to\ngive an overview of the recent publications concerning the application of\nartificial intelligence techniques to reverse supply chain with emphasis on\ncertain types of product returns.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 01:12:14 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Xing", "Bo", ""], ["Gao", "Wen-Jing", ""], ["Battle", "Kimberly", ""], ["Marwala", "Tshildzi", ""], ["Nelwamondo", "Fulufhelo V.", ""]]}, {"id": "1012.4776", "submitter": "Nicolas Saunier", "authors": "Nicolas Saunier and Sophie Midenet", "title": "Automatic Estimation of the Exposure to Lateral Collision in Signalized\n  Intersections using Video Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersections constitute one of the most dangerous elements in road systems.\nTraffic signals remain the most common way to control traffic at high-volume\nintersections and offer many opportunities to apply intelligent transportation\nsystems to make traffic more efficient and safe. This paper describes an\nautomated method to estimate the temporal exposure of road users crossing the\nconflict zone to lateral collision with road users originating from a different\napproach. This component is part of a larger system relying on video sensors to\nprovide queue lengths and spatial occupancy that are used for real time traffic\ncontrol and monitoring. The method is evaluated on data collected during a real\nworld experiment.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 19:46:21 GMT"}], "update_date": "2010-12-22", "authors_parsed": [["Saunier", "Nicolas", ""], ["Midenet", "Sophie", ""]]}, {"id": "1012.4824", "submitter": "Taufik Abrao", "authors": "Taufik Abr\\~ao, Leonardo D. Oliveira, Bruno A. Angelico and Paul Jean\n  E. Jeszensky", "title": "Input Parameters Optimization in Swarm DS-CDMA Multiuser Detectors", "comments": "21 pages, 15 figures, 4 tables, full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the uplink direct sequence code division multiple access\n(DS-CDMA) multiuser detection problem (MuD) is studied into heuristic\nperspective, named particle swarm optimization (PSO). Regarding different\nsystem improvements for future technologies, such as high-order modulation and\ndiversity exploitation, a complete parameter optimization procedure for the PSO\napplied to MuD problem is provided, which represents the major contribution of\nthis paper. Furthermore, the performance of the PSO-MuD is briefly analyzed via\nMonte-Carlo simulations. Simulation results show that, after convergence, the\nperformance reached by the PSO-MuD is much better than the conventional\ndetector, and somewhat close to the single user bound (SuB). Rayleigh flat\nchannel is initially considered, but the results are further extend to\ndiversity (time and spatial) channels.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 22:25:37 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Abr\u00e3o", "Taufik", ""], ["Oliveira", "Leonardo D.", ""], ["Angelico", "Bruno A.", ""], ["Jeszensky", "Paul Jean E.", ""]]}, {"id": "1012.5506", "submitter": "Adrian Paschke", "authors": "Alejandra Gonzalez-Beltran, Ben Tagger, and Anthony Finkelstein", "title": "Ontology-based Queries over Cancer Data", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing amount of data in biomedical research, and in cancer\nresearch in particular, needs to be managed to support efficient data access,\nexchange and integration. Existing software infrastructures, such caGrid,\nsupport access to distributed information annotated with a domain ontology.\nHowever, caGrid's current querying functionality depends on the structure of\nindividual data resources without exploiting the semantic annotations. In this\npaper, we present the design and development of an ontology-based querying\nfunctionality that consists of: the generation of OWL2 ontologies from the\nunderlying data resources metadata and a query rewriting and translation\nprocess based on reasoning, which converts a query at the domain ontology level\ninto queries at the software infrastructure level. We present a detailed\nanalysis of our approach as well as an extensive performance evaluation. While\nthe implementation and evaluation was performed for the caGrid infrastructure,\nthe approach could be applicable to other model and metadata-driven\nenvironments for data sharing.\n", "versions": [{"version": "v1", "created": "Sun, 26 Dec 2010 10:49:52 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gonzalez-Beltran", "Alejandra", ""], ["Tagger", "Ben", ""], ["Finkelstein", "Anthony", ""]]}, {"id": "1012.5546", "submitter": "Amine Farhat Mr.", "authors": "Mohamed Salah Gouider and Amine Farhat", "title": "Mining Multi-Level Frequent Itemsets under Constraints", "comments": "20 pages", "journal-ref": "Internatinal Journal of Database Theory and Application, Vol. 3,\n  No. 4, PP. 15-35, December, 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules is a task of data mining, which extracts knowledge\nin the form of significant implication relation of useful items (objects) from\na database. Mining multilevel association rules uses concept hierarchies, also\ncalled taxonomies and defined as relations of type 'is-a' between objects, to\nextract rules that items belong to different levels of abstraction. These rules\nare more useful, more refined and more interpretable by the user. Several\nalgorithms have been proposed in the literature to discover the multilevel\nassociation rules. In this article, we are interested in the problem of\ndiscovering multi-level frequent itemsets under constraints, involving the user\nin the research process. We proposed a technique for modeling and\ninterpretation of constraints in a context of use of concept hierarchies. Three\napproaches for discovering multi-level frequent itemsets under constraints were\nproposed and discussed: Basic approach, \"Test and Generate\" approach and\nPruning based Approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Dec 2010 22:23:00 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gouider", "Mohamed Salah", ""], ["Farhat", "Amine", ""]]}, {"id": "1012.5585", "submitter": "Tim Januschowski", "authors": "Tim januschowski and Barbara M. Smith and M. R. C. van Dongen", "title": "Symmetry Breaking with Polynomial Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conservative class of constraint satisfaction problems CSPs is a class for\nwhich membership is preserved under arbitrary domain reductions. Many\nwell-known tractable classes of CSPs are conservative. It is well known that\nlexleader constraints may significantly reduce the number of solutions by\nexcluding symmetric solutions of CSPs. We show that adding certain lexleader\nconstraints to any instance of any conservative class of CSPs still allows us\nto find all solutions with a time which is polynomial between successive\nsolutions. The time is polynomial in the total size of the instance and the\nadditional lexleader constraints. It is well known that for complete symmetry\nbreaking one may need an exponential number of lexleader constraints. However,\nin practice, the number of additional lexleader constraints is typically\npolynomial number in the size of the instance. For polynomially many lexleader\nconstraints, we may in general not have complete symmetry breaking but\npolynomially many lexleader constraints may provide practically useful symmetry\nbreaking -- and they sometimes exclude super-exponentially many solutions. We\nprove that for any instance from a conservative class, the time between finding\nsuccessive solutions of the instance with polynomially many additional\nlexleader constraints is polynomial even in the size of the instance without\nlexleaderconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 27 Dec 2010 09:58:16 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["januschowski", "Tim", ""], ["Smith", "Barbara M.", ""], ["van Dongen", "M. R. C.", ""]]}, {"id": "1012.5594", "submitter": "Kush Agrawal", "authors": "Kush Agrawal", "title": "The Ethics of Robotics", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three laws of Robotics first appeared together in Isaac Asimov's story\n'Runaround' after being mentioned in some form or the other in previous works\nby Asimov. These three laws commonly known as the three laws of robotics are\nthe earliest forms of depiction for the needs of ethics in Robotics. In\nsimplistic language Isaac Asimov is able to explain what rules a robot must\nconfine itself to in order to maintain societal sanctity. However, even though\nthey are outdated they still represent some of our innate fears which are\nbeginning to resurface in present day 21st Century. Our society is on the\nadvent of a new revolution; a revolution led by advances in Computer Science,\nArtificial Intelligence & Nanotechnology. Some of our advances have been so\nphenomenal that we surpassed what was predicted by the Moore's law. With these\nadvancements comes the fear that our future may be at the mercy of these\nandroids. Humans today are scared that we, ourselves, might create something\nwhich we cannot control. We may end up creating something which can not only\nlearn much faster than anyone of us can, but also evolve faster than what the\ntheory of evolution has allowed us to. The greatest fear is not only that we\nmight lose our jobs to these intelligent beings, but that these beings might\nend up replacing us at the top of the cycle. The public hysteria has been\nheightened more so by a number of cultural works which depict annihilation of\nthe human race by robots. Right from Frankenstein to I, Robot mass media has\nalso depicted such issues. This paper is an effort to understand the need for\nethics in Robotics or simply termed as Roboethics. This is achieved by the\nstudy of artificial beings and the thought being put behind them. By the end of\nthe paper, however, it is concluded that there isn't a need for ethical robots\nbut more so ever a need for ethical roboticists.\n", "versions": [{"version": "v1", "created": "Mon, 27 Dec 2010 11:14:56 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Agrawal", "Kush", ""]]}, {"id": "1012.5705", "submitter": "Wan Ahmad Tajuddin Wan Abdullah", "authors": "Wan Ahmad Tajuddin Wan Abdullah", "title": "Looking for plausibility", "comments": "6 pages, invited paper presented at the International Conference on\n  Advanced Computer Science and Information Systems 2010 (ICACSIS2010), Bali,\n  Indonesia, 20-22 November 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the interpretation of experimental data, one is actually looking for\nplausible explanations. We look for a measure of plausibility, with which we\ncan compare different possible explanations, and which can be combined when\nthere are different sets of data. This is contrasted to the conventional\nmeasure for probabilities as well as to the proposed measure of possibilities.\nWe define what characteristics this measure of plausibility should have.\n  In getting to the conception of this measure, we explore the relation of\nplausibility to abductive reasoning, and to Bayesian probabilities. We also\ncompare with the Dempster-Schaefer theory of evidence, which also has its own\ndefinition for plausibility. Abduction can be associated with biconditionality\nin inference rules, and this provides a platform to relate to the\nCollins-Michalski theory of plausibility. Finally, using a formalism for wiring\nlogic onto Hopfield neural networks, we ask if this is relevant in obtaining\nthis measure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 07:14:32 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Abdullah", "Wan Ahmad Tajuddin Wan", ""]]}, {"id": "1012.5754", "submitter": "Stamatia Bibi", "authors": "Efi Papatheocharous, Harris Papadopoulos and Andreas S. Andreou", "title": "Software Effort Estimation with Ridge Regression and Evolutionary\n  Attribute Selection", "comments": "3d Artificial Intelligence Techniques in Software Engineering\n  Workshop, 7 October, 2010, Larnaca, Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software cost estimation is one of the prerequisite managerial activities\ncarried out at the software development initiation stages and also repeated\nthroughout the whole software life-cycle so that amendments to the total cost\nare made. In software cost estimation typically, a selection of project\nattributes is employed to produce effort estimations of the expected human\nresources to deliver a software product. However, choosing the appropriate\nproject cost drivers in each case requires a lot of experience and knowledge on\nbehalf of the project manager which can only be obtained through years of\nsoftware engineering practice. A number of studies indicate that popular\nmethods applied in the literature for software cost estimation, such as linear\nregression, are not robust enough and do not yield accurate predictions.\nRecently the dual variables Ridge Regression (RR) technique has been used for\neffort estimation yielding promising results. In this work we show that results\nmay be further improved if an AI method is used to automatically select\nappropriate project cost drivers (inputs) for the technique. We propose a\nhybrid approach combining RR with a Genetic Algorithm, the latter evolving the\nsubset of attributes for approximating effort more accurately. The proposed\nhybrid cost model has been applied on a widely known high-dimensional dataset\nof software project samples and the results obtained show that accuracy may be\nincreased if redundant attributes are eliminated.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 13:11:51 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Papatheocharous", "Efi", ""], ["Papadopoulos", "Harris", ""], ["Andreou", "Andreas S.", ""]]}, {"id": "1012.5755", "submitter": "Stamatia Bibi", "authors": "Makrina Viola Kosti, Nikolaos Mittas and Lefteris Angelis", "title": "DD-EbA: An algorithm for determining the number of neighbors in cost\n  estimation by analogy using distance distributions", "comments": "3d Artificial Intelligence Tecniques in Software Engineering\n  Workshop,7 October,2010, Larnaca, Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case Based Reasoning and particularly Estimation by Analogy, has been used in\na number of problem-solving areas, such as cost estimation. Conventional\nmethods, despite the lack of a sound criterion for choosing nearest projects,\nwere based on estimation using a fixed and predetermined number of neighbors\nfrom the entire set of historical instances. This approach puts boundaries to\nthe estimation ability of such algorithms, for they do not take into\nconsideration that every project under estimation is unique and requires\ndifferent handling. The notion of distributions of distances together with a\ndistance metric for distributions help us to adapt the proposed method (we call\nit DD-EbA) each time to a specific case that is to be estimated without loosing\nin prediction power or computational cost. The results of this paper show that\nthe proposed technique achieves the above idea in a very efficient way.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 13:12:14 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Kosti", "Makrina Viola", ""], ["Mittas", "Nikolaos", ""], ["Angelis", "Lefteris", ""]]}, {"id": "1012.5813", "submitter": "Tamal Ghosh", "authors": "Sourav Sengupta, Tamal Ghosh, Pranab K Dan, Manojit Chattopadhyay", "title": "Neural Network Influence in Group Technology: A Chronological Survey and\n  Critical Analysis", "comments": "This paper has been withdrawn by the authors. Withdrawn because some\n  critical directions are wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article portrays a chronological review of the influence of Artificial\nNeural Network in group technology applications in the vicinity of Cellular\nManufacturing Systems. The research trend is identified and the evolvement is\ncaptured through a critical analysis of the literature accessible from the very\nbeginning of its practice in the early 90's till the 2010. Analysis of the\ndiverse ANN approaches, spotted research pattern, comparison of the clustering\nefficiencies, the solutions obtained and the tools used make this study\nexclusive in its class.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 18:46:46 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2012 05:43:03 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Sengupta", "Sourav", ""], ["Ghosh", "Tamal", ""], ["Dan", "Pranab K", ""], ["Chattopadhyay", "Manojit", ""]]}, {"id": "1012.5815", "submitter": "Tamal Ghosh Tamal Ghosh", "authors": "Tamal Ghosh, Mousumi Modak and Pranab K Dan", "title": "SAPFOCS: a metaheuristic based approach to part family formation\n  problems in group technology", "comments": "10 pages; 6 figures; 12 tables", "journal-ref": "nternational Journal of Management Science International Journal\n  of Management Science and Engineering Management, 6(3): 231-240, 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with Part family formation problem which is believed to be\nmoderately complicated to be solved in polynomial time in the vicinity of Group\nTechnology (GT). In the past literature researchers investigated that the part\nfamily formation techniques are principally based on production flow analysis\n(PFA) which usually considers operational requirements, sequences and time.\nPart Coding Analysis (PCA) is merely considered in GT which is believed to be\nthe proficient method to identify the part families. PCA classifies parts by\nallotting them to different families based on their resemblances in: (1) design\ncharacteristics such as shape and size, and/or (2) manufacturing\ncharacteristics (machining requirements). A novel approach based on simulated\nannealing namely SAPFOCS is adopted in this study to develop effective part\nfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal design\nmethod is employed to solve the critical issues on the subject of parameters\nselection for the proposed metaheuristic algorithm. The adopted technique is\ntherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9\nand the obtained results are compared with C-Linkage clustering technique. The\nexperimental results reported that the proposed metaheuristic algorithm is\nextremely effective in terms of the quality of the solution obtained and has\noutperformed C-Linkage algorithm in most instances.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 18:57:04 GMT"}, {"version": "v2", "created": "Wed, 11 May 2011 07:18:26 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Ghosh", "Tamal", ""], ["Modak", "Mousumi", ""], ["Dan", "Pranab K", ""]]}, {"id": "1012.5847", "submitter": "Joohyung Lee", "authors": "Martin Gebser, Joohyung Lee and Yuliya Lierler", "title": "On Elementary Loops of Logic Programs", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the notion of an elementary loop, Gebser and Schaub refined the theorem\non loop formulas due to Lin and Zhao by considering loop formulas of elementary\nloops only. In this article, we reformulate their definition of an elementary\nloop, extend it to disjunctive programs, and study several properties of\nelementary loops, including how maximal elementary loops are related to minimal\nunfounded sets. The results provide useful insights into the stable model\nsemantics in terms of elementary loops. For a nondisjunctive program, using a\ngraph-theoretic characterization of an elementary loop, we show that the\nproblem of recognizing an elementary loop is tractable. On the other hand, we\nshow that the corresponding problem is {\\sf coNP}-complete for a disjunctive\nprogram. Based on the notion of an elementary loop, we present the class of\nHead-Elementary-loop-Free (HEF) programs, which strictly generalizes the class\nof Head-Cycle-Free (HCF) programs due to Ben-Eliyahu and Dechter. Like an HCF\nprogram, an HEF program can be turned into an equivalent nondisjunctive program\nin polynomial time by shifting head atoms into the body.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 21:49:11 GMT"}, {"version": "v2", "created": "Sun, 2 Jan 2011 15:34:01 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Gebser", "Martin", ""], ["Lee", "Joohyung", ""], ["Lierler", "Yuliya", ""]]}, {"id": "1012.5960", "submitter": "Reinhard Moratz", "authors": "Reinhard Moratz", "title": "Extending Binary Qualitative Direction Calculi with a Granular Distance\n  Concept: Hidden Feature Attachment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method for extending binary qualitative\ndirection calculi with adjustable granularity like OPRAm or the star calculus\nwith a granular distance concept. This method is similar to the concept of\nextending points with an internal reference direction to get oriented points\nwhich are the basic entities in the OPRAm calculus. Even if the spatial objects\nare from a geometrical point of view infinitesimal small points locally\navailable reference measures are attached. In the case of OPRAm, a reference\ndirection is attached. The same principle works also with local reference\ndistances which are called elevations. The principle of attaching references\nfeatures to a point is called hidden feature attachment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 15:29:33 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Moratz", "Reinhard", ""]]}, {"id": "1012.6018", "submitter": "Fabien Tence", "authors": "Fabien Tenc\\'e (LISYC), C\\'edric Buche (LISYC), Pierre De Loor\n  (LISYC), Olivier Marc (LISYC)", "title": "Learning a Representation of a Believable Virtual Character's\n  Environment with an Imitation Algorithm", "comments": null, "journal-ref": "GAMEON-ARABIA'10, Egypt (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video games, virtual characters' decision systems often use a simplified\nrepresentation of the world. To increase both their autonomy and believability\nwe want those characters to be able to learn this representation from human\nplayers. We propose to use a model called growing neural gas to learn by\nimitation the topology of the environment. The implementation of the model, the\nmodifications and the parameters we used are detailed. Then, the quality of the\nlearned representations and their evolution during the learning are studied\nusing different measures. Improvements for the growing neural gas to give more\ninformation to the character's model are given in the conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 19:58:44 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Tenc\u00e9", "Fabien", "", "LISYC"], ["Buche", "C\u00e9dric", "", "LISYC"], ["De Loor", "Pierre", "", "LISYC"], ["Marc", "Olivier", "", "LISYC"]]}]