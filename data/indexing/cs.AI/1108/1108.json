[{"id": "1108.0039", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Ramon Lopez de Mantaras, Simeon Simoff, Carles\n  Sierra", "title": "CBR with Commonsense Reasoning and Structure Mapping: An Application to\n  Mediation", "comments": "15 pages, 3 figures; updated copyright notice", "journal-ref": "Case-Based Reasoning Research and Development, LNCS (LNAI), Volume\n  6880 (2011), 378-392, Springer Berlin / Heidelberg, ISBN: 978-3-642-23290-9", "doi": "10.1007/978-3-642-23291-6_28", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation is an important method in dispute resolution. We implement a case\nbased reasoning approach to mediation integrating analogical and commonsense\nreasoning components that allow an artificial mediation agent to satisfy\nrequirements expected from a human mediator, in particular: utilizing\nexperience with cases in different domains; and structurally transforming the\nset of issues for a better solution. We utilize a case structure based on\nontologies reflecting the perceptions of the parties in dispute. The analogical\nreasoning component, employing the Structure Mapping Theory from psychology,\nprovides a flexibility to respond innovatively in unusual circumstances, in\ncontrast with conventional approaches confined into specialized problem\ndomains. We aim to build a mediation case base incorporating real world\ninstances ranging from interpersonal or intergroup disputes to international\nconflicts.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2011 05:12:17 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2011 20:11:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["de Mantaras", "Ramon Lopez", ""], ["Simoff", "Simeon", ""], ["Sierra", "Carles", ""]]}, {"id": "1108.0155", "submitter": "Michael Schneider", "authors": "Michael Schneider, Geoff Sutcliffe", "title": "Reasoning in the OWL 2 Full Ontology Language using First-Order\n  Automated Theorem Proving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OWL 2 has been standardized by the World Wide Web Consortium (W3C) as a\nfamily of ontology languages for the Semantic Web. The most expressive of these\nlanguages is OWL 2 Full, but to date no reasoner has been implemented for this\nlanguage. Consistency and entailment checking are known to be undecidable for\nOWL 2 Full. We have translated a large fragment of the OWL 2 Full semantics\ninto first-order logic, and used automated theorem proving systems to do\nreasoning based on this theory. The results are promising, and indicate that\nthis approach can be applied in practice for effective OWL reasoning, beyond\nthe capabilities of current Semantic Web reasoners.\n  This is an extended version of a paper with the same title that has been\npublished at CADE 2011, LNAI 6803, pp. 446-460. The extended version provides\nappendices with additional resources that were used in the reported evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2011 07:51:02 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Schneider", "Michael", ""], ["Sutcliffe", "Geoff", ""]]}, {"id": "1108.0294", "submitter": "Ce Zhang", "authors": "Feng Niu, Ce Zhang, Christopher R\\'e, Jude Shavlik", "title": "Scaling Inference for Markov Logic with a Task-Decomposition Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in large-scale knowledge base construction, we\nstudy the problem of scaling up a sophisticated statistical inference framework\ncalled Markov Logic Networks (MLNs). Our approach, Felix, uses the idea of\nLagrangian relaxation from mathematical programming to decompose a program into\nsmaller tasks while preserving the joint-inference property of the original\nMLN. The advantage is that we can use highly scalable specialized algorithms\nfor common tasks such as classification and coreference. We propose an\narchitecture to support Lagrangian relaxation in an RDBMS which we show enables\nscalable joint inference for MLNs. We empirically validate that Felix is\nsignificantly more scalable and efficient than prior approaches to MLN\ninference by constructing a knowledge base from 1.8M documents as part of the\nTAC challenge. We show that Felix scales and achieves state-of-the-art quality\nnumbers. In contrast, prior approaches do not scale even to a subset of the\ncorpus that is three orders of magnitude smaller.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 12:08:00 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 00:58:13 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2012 16:10:23 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2012 01:36:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Niu", "Feng", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""], ["Shavlik", "Jude", ""]]}, {"id": "1108.0404", "submitter": "Frans A. Oliehoek", "authors": "Frans A. Oliehoek, Shimon Whiteson, Matthijs T.J. Spaan", "title": "Exploiting Agent and Type Independence in Collaborative Graphical\n  Bayesian Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient collaborative decision making is an important challenge for\nmultiagent systems. Finding optimal joint actions is especially challenging\nwhen each agent has only imperfect information about the state of its\nenvironment. Such problems can be modeled as collaborative Bayesian games in\nwhich each agent receives private information in the form of its type. However,\nrepresenting and solving such games requires space and computation time\nexponential in the number of agents. This article introduces collaborative\ngraphical Bayesian games (CGBGs), which facilitate more efficient collaborative\ndecision making by decomposing the global payoff function as the sum of local\npayoff functions that depend on only a few agents. We propose a framework for\nthe efficient solution of CGBGs based on the insight that they posses two\ndifferent types of independence, which we call agent independence and type\nindependence. In particular, we present a factor graph representation that\ncaptures both forms of independence and thus enables efficient solutions. In\naddition, we show how this representation can provide leverage in sequential\ntasks by using it to construct a novel method for decentralized partially\nobservable Markov decision processes. Experimental results in both random and\nbenchmark tasks demonstrate the improved scalability of our methods compared to\nseveral existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 19:51:53 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 15:19:22 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Oliehoek", "Frans A.", ""], ["Whiteson", "Shimon", ""], ["Spaan", "Matthijs T. J.", ""]]}, {"id": "1108.0476", "submitter": "Saverio Perugini", "authors": "Saverio Perugini", "title": "Specifying and Staging Mixed-Initiative Dialogs with Program Generation\n  and Transformation", "comments": "combined/reorganized some figures/tables from version 4, and\n  corrected typos; 22 pages, 5 tables, 5 figures, and 5 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specifying and implementing flexible human-computer dialogs, such as those\nused in kiosks and smart phone apps, is challenging because of the numerous and\nvaried directions in which each user might steer a dialog. The objective of\nthis research is to improve dialog specification and implementation. To do so\nwe enriched a notation based on concepts from programming languages, especially\npartial evaluation, for specifying a variety of unsolicited reporting,\nmixed-initiative dialogs in a concise representation that serves as a design\nfor dialog implementation. We also built a dialog mining system that extracts a\nspecification in this notation from requirements. To demonstrate that such a\nspecification provides a design for dialog implementation, we built a system\nthat automatically generates an implementation of the dialog, called a stager,\nfrom it. These two components constitute a dialog modeling toolkit that\nautomates dialog specification and implementation. These results provide a\nproof of concept and demonstrate the study of dialog specification and\nimplementation from a programming languages perspective. The ubiquity of\ndialogs in domains such as travel, education, and health care combined with the\ndemand for smart phone apps provide a landscape for further investigation of\nthese results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 03:00:09 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 20:58:47 GMT"}, {"version": "v3", "created": "Sat, 30 Aug 2014 20:05:58 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2015 00:29:04 GMT"}, {"version": "v5", "created": "Mon, 21 Dec 2015 15:10:54 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Perugini", "Saverio", ""]]}, {"id": "1108.0502", "submitter": "Ankit Chaudhary", "authors": "Jagdish Lal Raheja, Karen Das, Ankit Chaudhary", "title": "An Efficient Real Time Method of Fingertip Detection", "comments": "This paper was published in the 7th International Conference on\n  Trends in Industrial Measurements and Automation (TIMA 2011), CSIR Complex,\n  Chennai, India, 6-8 Jan, 2011, pp. 447-450", "journal-ref": "7th International Conference on Trends in Industrial Measurements\n  and Automation (TIMA 2011), CSIR Complex, Chennai, India, 6-8 Jan, 2011, pp.\n  447-450", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingertips detection has been used in many applications, and it is very\npopular and commonly used in the area of Human Computer Interaction these days.\nThis paper presents a novel time efficient method that will lead to fingertip\ndetection after cropping the irrelevant parts of input image. Binary silhouette\nof the input image is generated using HSV color space based skin filter and\nhand cropping done based on histogram of the hand image. The cropped image will\nbe used to figure out the fingertips.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 07:46:16 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Raheja", "Jagdish Lal", ""], ["Das", "Karen", ""], ["Chaudhary", "Ankit", ""]]}, {"id": "1108.1045", "submitter": "Asha T", "authors": "Asha.T, S. Natarajan and K.N.B. Murthy", "title": "A Data Mining Approach to the Diagnosis of Tuberculosis by Cascading\n  Clustering and Classification", "comments": "8 pages", "journal-ref": "Journal of computing, volume 3, issue 4,April 2011, ISSN 2151-9617", "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology for the automated detection and classification\nof Tuberculosis(TB) is presented. Tuberculosis is a disease caused by\nmycobacterium which spreads through the air and attacks low immune bodies\neasily. Our methodology is based on clustering and classification that\nclassifies TB into two categories, Pulmonary Tuberculosis(PTB) and retroviral\nPTB(RPTB) that is those with Human Immunodeficiency Virus (HIV) infection.\nInitially K-means clustering is used to group the TB data into two clusters and\nassigns classes to clusters. Subsequently multiple different classification\nalgorithms are trained on the result set to build the final classifier model\nbased on K-fold cross validation method. This methodology is evaluated using\n700 raw TB data obtained from a city hospital. The best obtained accuracy was\n98.7% from support vector machine (SVM) compared to other classifiers. The\nproposed approach helps doctors in their diagnosis decisions and also in their\ntreatment planning procedures for different categories.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 10:52:51 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["T", "Asha.", ""], ["Natarajan", "S.", ""], ["Murthy", "K. N. B.", ""]]}, {"id": "1108.1170", "submitter": "Martin Jaggi", "authors": "Martin Jaggi", "title": "Convex Optimization without Projection Steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the general problem of minimizing a convex function over a compact convex\ndomain, we will investigate a simple iterative approximation algorithm based on\nthe method by Frank & Wolfe 1956, that does not need projection steps in order\nto stay inside the optimization domain. Instead of a projection step, the\nlinearized problem defined by a current subgradient is solved, which gives a\nstep direction that will naturally stay in the domain. Our framework\ngeneralizes the sparse greedy algorithm of Frank & Wolfe and its primal-dual\nanalysis by Clarkson 2010 (and the low-rank SDP approach by Hazan 2008) to\narbitrary convex domains. We give a convergence proof guaranteeing\n{\\epsilon}-small duality gap after O(1/{\\epsilon}) iterations.\n  The method allows us to understand the sparsity of approximate solutions for\nany l1-regularized convex optimization problem (and for optimization over the\nsimplex), expressed as a function of the approximation quality. We obtain\nmatching upper and lower bounds of {\\Theta}(1/{\\epsilon}) for the sparsity for\nl1-problems. The same bounds apply to low-rank semidefinite optimization with\nbounded trace, showing that rank O(1/{\\epsilon}) is best possible here as well.\nAs another application, we obtain sparse matrices of O(1/{\\epsilon}) non-zero\nentries as {\\epsilon}-approximate solutions when optimizing any convex function\nover a class of diagonally dominant symmetric matrices.\n  We show that our proposed first-order method also applies to nuclear norm and\nmax-norm matrix optimization problems. For nuclear norm regularized\noptimization, such as matrix completion and low-rank recovery, we demonstrate\nthe practical efficiency and scalability of our algorithm for large matrix\nproblems, as e.g. the Netflix dataset. For general convex optimization over\nbounded matrix max-norm, our algorithm is the first with a convergence\nguarantee, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 19:15:04 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2011 22:11:51 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2011 22:56:49 GMT"}, {"version": "v4", "created": "Mon, 19 Sep 2011 16:42:01 GMT"}, {"version": "v5", "created": "Wed, 23 Nov 2011 15:38:13 GMT"}, {"version": "v6", "created": "Tue, 27 Dec 2011 17:45:39 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Jaggi", "Martin", ""]]}, {"id": "1108.1488", "submitter": "Paola Di Maio", "authors": "P. Di Maio", "title": "'Just Enough' Ontology Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces 'just enough' principles and 'systems engineering'\napproach to the practice of ontology development to provide a minimal yet\ncomplete, lightweight, agile and integrated development process, supportive of\nstakeholder management and implementation independence.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 15:21:05 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Di Maio", "P.", ""]]}, {"id": "1108.1500", "submitter": "Sahar Yousefi ms", "authors": "Sahar Yousefi, Morteza Zahedi", "title": "Gender Recognition Based on Sift Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a robust approach for face detection and gender\nclassification in color images. Previous researches about gender recognition\nsuppose an expensive computational and time-consuming pre-processing step in\norder to alignment in which face images are aligned so that facial landmarks\nlike eyes, nose, lips, chin are placed in uniform locations in image. In this\npaper, a novel technique based on mathematical analysis is represented in three\nstages that eliminates alignment step. First, a new color based face detection\nmethod is represented with a better result and more robustness in complex\nbackgrounds. Next, the features which are invariant to affine transformations\nare extracted from each face using scale invariant feature transform (SIFT)\nmethod. To evaluate the performance of the proposed algorithm, experiments have\nbeen conducted by employing a SVM classifier on a database of face images which\ncontains 500 images from distinct people with equal ratio of male and female.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 17:52:00 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Yousefi", "Sahar", ""], ["Zahedi", "Morteza", ""]]}, {"id": "1108.1986", "submitter": "Ezhil Arasi M", "authors": "D. P. Acharjya, and L. Ezhilarasi", "title": "A Knowledge Mining Model for Ranking Institutions using Rough Computing\n  with Ordering Rules and Formal Concept analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Emergences of computers and information technological revolution made\ntremendous changes in the real world and provides a different dimension for the\nintelligent data analysis. Well formed fact, the information at right time and\nat right place deploy a better knowledge.However, the challenge arises when\nlarger volume of inconsistent data is given for decision making and knowledge\nextraction. To handle such imprecise data certain mathematical tools of greater\nimportance has developed by researches in recent past namely fuzzy set,\nintuitionistic fuzzy set, rough Set, formal concept analysis and ordering\nrules. It is also observed that many information system contains numerical\nattribute values and therefore they are almost similar instead of exact\nsimilar. To handle such type of information system, in this paper we use two\nprocesses such as pre process and post process. In pre process we use rough set\non intuitionistic fuzzy approximation space with ordering rules for finding the\nknowledge whereas in post process we use formal concept analysis to explore\nbetter knowledge and vital factors affecting decisions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 17:11:02 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Acharjya", "D. P.", ""], ["Ezhilarasi", "L.", ""]]}, {"id": "1108.2054", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Fabio Fassetti", "title": "Uncertain Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the problem of classifying uncertain data. With this aim\nthe Uncertain Nearest Neighbor (UNN) rule is here introduced, which represents\nthe generalization of the deterministic nearest neighbor rule to the case in\nwhich uncertain objects are available. The UNN rule relies on the concept of\nnearest neighbor class, rather than on that of nearest neighbor object. The\nnearest neighbor class of a test object is the class that maximizes the\nprobability of providing its nearest neighbor. It is provided evidence that the\nformer concept is much more powerful than the latter one in the presence of\nuncertainty, in that it correctly models the right semantics of the nearest\nneighbor decision rule when applied to the uncertain scenario. An effective and\nefficient algorithm to perform uncertain nearest neighbor classification of a\ngeneric (un)certain test object is designed, based on properties that greatly\nreduce the temporal cost associated with nearest neighbor class probability\ncomputation. Experimental results are presented, showing that the UNN rule is\neffective and efficient in classifying uncertain data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 21:28:42 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Fassetti", "Fabio", ""]]}, {"id": "1108.2096", "submitter": "Yu Zhang", "authors": "Yu Zhang, Mihaela van der Schaar", "title": "Reputation-based Incentive Protocols in Crowdsourcing Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing websites (e.g. Yahoo! Answers, Amazon Mechanical Turk, and\netc.) emerged in recent years that allow requesters from all around the world\nto post tasks and seek help from an equally global pool of workers. However,\nintrinsic incentive problems reside in crowdsourcing applications as workers\nand requester are selfish and aim to strategically maximize their own benefit.\nIn this paper, we propose to provide incentives for workers to exert effort\nusing a novel game-theoretic model based on repeated games. As there is always\na gap in the social welfare between the non-cooperative equilibria emerging\nwhen workers pursue their self-interests and the desirable Pareto efficient\noutcome, we propose a novel class of incentive protocols based on social norms\nwhich integrates reputation mechanisms into the existing pricing schemes\ncurrently implemented on crowdsourcing websites, in order to improve the\nperformance of the non-cooperative equilibria emerging in such applications. We\nfirst formulate the exchanges on a crowdsourcing website as a two-sided market\nwhere requesters and workers are matched and play gift-giving games repeatedly.\nSubsequently, we study the protocol designer's problem of finding an optimal\nand sustainable (equilibrium) protocol which achieves the highest social\nwelfare for that website. We prove that the proposed incentives protocol can\nmake the website operate close to Pareto efficiency. Moreover, we also examine\nan alternative scenario, where the protocol designer aims at maximizing the\nrevenue of the website and evaluate the performance of the optimal protocol.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 04:47:55 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Zhang", "Yu", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1108.2115", "submitter": "Hans van Ditmarsch", "authors": "Hans van Ditmarsch", "title": "The Ditmarsch Tale of Wonders - The Dynamics of Lying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic logic of lying, wherein a 'lie that phi' (where phi is a\nformula in the logic) is an action in the sense of dynamic modal logic, that is\ninterpreted as a state transformer relative to the formula phi. The states that\nare being transformed are pointed Kripke models encoding the uncertainty of\nagents about their beliefs. Lies can be about factual propositions but also\nabout modal formulas, such as the beliefs of other agents or the belief\nconsequences of the lies of other agents. We distinguish (i) an outside\nobserver who is lying to an agent that is modelled in the system, from (ii) one\nagent who is lying to another agent, and where both are modelled in the system.\nFor either case, we further distinguish (iii) the agent who believes everything\nthat it is told (even at the price of inconsistency), from (iv) the agent who\nonly believes what it is told if that is consistent with its current beliefs,\nand from (v) the agent who believes everything that it is told by consistently\nrevising its current beliefs. The logics have complete axiomatizations, which\ncan most elegantly be shown by way of their embedding in what is known as\naction model logic or the extension of that logic to belief revision.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 07:55:15 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2012 16:08:53 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["van Ditmarsch", "Hans", ""]]}, {"id": "1108.2283", "submitter": "Federico Schl\\\"uter", "authors": "Federico Schl\\\"uter", "title": "A survey on independence-based Markov networks learning", "comments": "35 pages, 1 figure", "journal-ref": "Schl\\\"uter, F. (2011). A survey on independence-based Markov\n  networks learning. Artificial Intelligence Review, 1-25", "doi": "10.1007/s10462-012-9346-y", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work reports the most relevant technical aspects in the problem of\nlearning the \\emph{Markov network structure} from data. Such problem has become\nincreasingly important in machine learning, and many other application fields\nof machine learning. Markov networks, together with Bayesian networks, are\nprobabilistic graphical models, a widely used formalism for handling\nprobability distributions in intelligent systems. Learning graphical models\nfrom data have been extensively applied for the case of Bayesian networks, but\nfor Markov networks learning it is not tractable in practice. However, this\nsituation is changing with time, given the exponential growth of computers\ncapacity, the plethora of available digital data, and the researching on new\nlearning technologies. This work stresses on a technology called\nindependence-based learning, which allows the learning of the independence\nstructure of those networks from data in an efficient and sound manner,\nwhenever the dataset is sufficiently large, and data is a representative\nsampling of the target distribution. In the analysis of such technology, this\nwork surveys the current state-of-the-art algorithms for learning Markov\nnetworks structure, discussing its current limitations, and proposing a series\nof open problems where future works may produce some advances in the area in\nterms of quality and efficiency. The paper concludes by opening a discussion\nabout how to develop a general formalism for improving the quality of the\nstructures learned, when data is scarce.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 20:25:08 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 19:15:05 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Schl\u00fcter", "Federico", ""]]}, {"id": "1108.2865", "submitter": "Norbert B\\'atfai", "authors": "Norbert B\\'atfai", "title": "Conscious Machines and Consciousness Oriented Programming", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the following question: how could you write\nsuch computer programs that can work like conscious beings? The motivation\nbehind this question is that we want to create such applications that can see\nthe future. The aim of this paper is to provide an overall conceptual framework\nfor this new approach to machine consciousness. So we introduce a new\nprogramming paradigm called Consciousness Oriented Programming (COP).\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 12:27:39 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "1108.2989", "submitter": "Indraneel Mukherjee", "authors": "Indraneel Mukherjee and Robert E. Schapire", "title": "A theory of multiclass boosting", "comments": "A preliminary version appeared in NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting combines weak classifiers to form highly accurate predictors.\nAlthough the case of binary classification is well understood, in the\nmulticlass setting, the \"correct\" requirements on the weak classifier, or the\nnotion of the most efficient boosting algorithms are missing. In this paper, we\ncreate a broad and general framework, within which we make precise and identify\nthe optimal requirements on the weak-classifier, as well as design the most\neffective, in a certain sense, boosting algorithms that assume such\nrequirements.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 13:26:26 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Mukherjee", "Indraneel", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1108.3019", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Uwe Aickelin", "title": "A First Approach on Modelling Staff Proactiveness in Retail Simulation\n  Models", "comments": "25 pages, 3 figures, 10 tables", "journal-ref": "Journal of Artificial Societies and Social Simulation, 14 (2),\n  pages 1-25, 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a noticeable shift in the relative composition of the industry\nin the developed countries in recent years; manufacturing is decreasing while\nthe service sector is becoming more important. However, currently most\nsimulation models for investigating service systems are still built in the same\nway as manufacturing simulation models, using a process-oriented world view,\ni.e. they model the flow of passive entities through a system. These kinds of\nmodels allow studying aspects of operational management but are not well suited\nfor studying the dynamics that appear in service systems due to human\nbehaviour. For these kinds of studies we require tools that allow modelling the\nsystem and entities using an object-oriented world view, where intelligent\nobjects serve as abstract \"actors\" that are goal directed and can behave\nproactively. In our work we combine process-oriented discrete event simulation\nmodelling and object-oriented agent based simulation modelling to investigate\nthe impact of people management practices on retail productivity. In this\npaper, we reveal in a series of experiments what impact considering proactivity\ncan have on the output accuracy of simulation models of human centric systems.\nThe model and data we use for this investigation are based on a case study in a\nUK department store. We show that considering proactivity positively influences\nthe validity of these kinds of models and therefore allows analysts to make\nbetter recommendations regarding strategies to apply people management\npractises.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 15:25:15 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1108.3074", "submitter": "Ehtibar Dzhafarov", "authors": "Ehtibar N. Dzhafarov and Janne V. Kujala", "title": "Selectivity in Probabilistic Causality: Drawing Arrows from Inputs to\n  Stochastic Outputs", "comments": "25 pages; minor corrections with respect to the first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of several inputs into a system (e.g., independent variables\ncharacterizing stimuli) and a set of several stochastically non-independent\noutputs (e.g., random variables describing different aspects of responses), how\ncan one determine, for each of the outputs, which of the inputs it is\ninfluenced by? The problem has applications ranging from modeling pairwise\ncomparisons to reconstructing mental processing architectures to conjoint\ntesting. A necessary and sufficient condition for a given pattern of selective\ninfluences is provided by the Joint Distribution Criterion, according to which\nthe problem of \"what influences what\" is equivalent to that of the existence of\na joint distribution for a certain set of random variables. For inputs and\noutputs with finite sets of values this criterion translates into a test of\nconsistency of a certain system of linear equations and inequalities (Linear\nFeasibility Test) which can be performed by means of linear programming. The\nJoint Distribution Criterion also leads to a metatheoretical principle for\ngenerating a broad class of necessary conditions (tests) for diagrams of\nselective influences. Among them is the class of distance-type tests based on\nthe observation that certain functionals on jointly distributed random\nvariables satisfy triangle inequality.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 14:03:55 GMT"}, {"version": "v2", "created": "Sat, 27 Aug 2011 23:28:43 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Dzhafarov", "Ehtibar N.", ""], ["Kujala", "Janne V.", ""]]}, {"id": "1108.3235", "submitter": "Uwe Aickelin", "authors": "Grazziela P. Figueredo, Uwe Aickelin", "title": "Comparing System Dynamics and Agent-Based Simulation for Tumour Growth\n  and its Interactions with Effector Cells", "comments": "8 pages, 8 figures, 2 tables, International Summer Computer\n  Simulation Conference 2011", "journal-ref": "Proceedings of the International Summer Computer Simulation\n  Conference 2011, p15-22, 2011", "doi": null, "report-no": null, "categories": "cs.CE cs.AI q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is little research concerning comparisons and combination of System\nDynamics Simulation (SDS) and Agent Based Simulation (ABS). ABS is a paradigm\nused in many levels of abstraction, including those levels covered by SDS. We\nbelieve that the establishment of frameworks for the choice between these two\nsimulation approaches would contribute to the simulation research. Hence, our\nwork aims for the establishment of directions for the choice between SDS and\nABS approaches for immune system-related problems. Previously, we compared the\nuse of ABS and SDS for modelling agents' behaviour in an environment with\nnomovement or interactions between these agents. We concluded that for these\ntypes of agents it is preferable to use SDS, as it takes up less computational\nresources and produces the same results as those obtained by the ABS model. In\norder to move this research forward, our next research question is: if we\nintroduce interactions between these agents will SDS still be the most\nappropriate paradigm to be used? To answer this question for immune system\nsimulation problems, we will use, as case studies, models involving\ninteractions between tumour cells and immune effector cells. Experiments show\nthat there are cases where SDS and ABS can not be used interchangeably, and\ntherefore, their comparison is not straightforward.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 13:08:23 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Figueredo", "Grazziela P.", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1108.3259", "submitter": "Souhaib Ben Taieb", "authors": "Souhaib Ben Taieb and Gianluca Bontempi and Amir Atiya and Antti\n  Sorjamaa", "title": "A review and comparison of strategies for multi-step ahead time series\n  forecasting based on the NN5 forecasting competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step ahead forecasting is still an open challenge in time series\nforecasting. Several approaches that deal with this complex problem have been\nproposed in the literature but an extensive comparison on a large number of\ntasks is still missing. This paper aims to fill this gap by reviewing existing\nstrategies for multi-step ahead forecasting and comparing them in theoretical\nand practical terms. To attain such an objective, we performed a large scale\ncomparison of these different strategies using a large experimental benchmark\n(namely the 111 series from the NN5 forecasting competition). In addition, we\nconsidered the effects of deseasonalization, input variable selection, and\nforecast combination on these strategies and on multi-step ahead forecasting at\nlarge. The following three findings appear to be consistently supported by the\nexperimental results: Multiple-Output strategies are the best performing\napproaches, deseasonalization leads to uniformly improved forecast accuracy,\nand input selection is more effective when performed in conjunction with\ndeseasonalization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 14:55:20 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Taieb", "Souhaib Ben", ""], ["Bontempi", "Gianluca", ""], ["Atiya", "Amir", ""], ["Sorjamaa", "Antti", ""]]}, {"id": "1108.3260", "submitter": "Esra Erdem", "authors": "Thomas Eiter and Esra Erdem and Halit Erdogan and Michael Fink", "title": "Finding Similar/Diverse Solutions in Answer Set Programming", "comments": "57 pages, 17 figures, 4 tables. To appear in Theory and Practice of\n  Logic Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming, 13(3), 303-359, 2013", "doi": "10.1017/S1471068411000548", "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some computational problems (e.g., product configuration, planning,\ndiagnosis, query answering, phylogeny reconstruction) computing a set of\nsimilar/diverse solutions may be desirable for better decision-making. With\nthis motivation, we studied several decision/optimization versions of this\nproblem in the context of Answer Set Programming (ASP), analyzed their\ncomputational complexity, and introduced offline/online methods to compute\nsimilar/diverse solutions of such computational problems with respect to a\ngiven distance function. All these methods rely on the idea of computing\nsolutions to a problem by means of finding the answer sets for an ASP program\nthat describes the problem. The offline methods compute all solutions in\nadvance using the ASP formulation of the problem with an ASP solver, like\nClasp, and then identify similar/diverse solutions using clustering methods.\nThe online methods compute similar/diverse solutions following one of the three\napproaches: by reformulating the ASP representation of the problem to compute\nsimilar/diverse solutions at once using an ASP solver; by computing\nsimilar/diverse solutions iteratively (one after other) using an ASP solver; by\nmodifying the search algorithm of an ASP solver to compute similar/diverse\nsolutions incrementally. We modified Clasp to implement the last online method\nand called it Clasp-NK. In the first two online methods, the given distance\nfunction is represented in ASP; in the last one it is implemented in C++. We\nshowed the applicability and the effectiveness of these methods on\nreconstruction of similar/diverse phylogenies for Indo-European languages, and\non several planning problems in Blocks World. We observed that in terms of\ncomputational efficiency the last online method outperforms the others; also it\nallows us to compute similar/diverse solutions when the distance function\ncannot be represented in ASP.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 14:59:44 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Eiter", "Thomas", ""], ["Erdem", "Esra", ""], ["Erdogan", "Halit", ""], ["Fink", "Michael", ""]]}, {"id": "1108.3278", "submitter": "Miroslaw Truszczynski", "authors": "Marc Denecker, Victor W. Marek and Miroslaw Truszczynski", "title": "Reiter's Default Logic Is a Logic of Autoepistemic Reasoning And a Good\n  One, Too", "comments": "In G. Brewka, V.M. Marek, and M. Truszczynski, eds. Nonmonotonic\n  Reasoning -- Essays Celebrating its 30th Anniversary, College Publications,\n  2011 (a volume of papers presented at NonMOn at 30 meeting, Lexington, KY,\n  USA, October 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fact apparently not observed earlier in the literature of nonmonotonic\nreasoning is that Reiter, in his default logic paper, did not directly\nformalize informal defaults. Instead, he translated a default into a certain\nnatural language proposition and provided a formalization of the latter. A few\nyears later, Moore noted that propositions like the one used by Reiter are\nfundamentally different than defaults and exhibit a certain autoepistemic\nnature. Thus, Reiter had developed his default logic as a formalization of\nautoepistemic propositions rather than of defaults.\n  The first goal of this paper is to show that some problems of Reiter's\ndefault logic as a formal way to reason about informal defaults are directly\nattributable to the autoepistemic nature of default logic and to the mismatch\nbetween informal defaults and the Reiter's formal defaults, the latter being a\nformal expression of the autoepistemic propositions Reiter used as a\nrepresentation of informal defaults.\n  The second goal of our paper is to compare the work of Reiter and Moore.\nWhile each of them attempted to formalize autoepistemic propositions, the modes\nof reasoning in their respective logics were different. We revisit Moore's and\nReiter's intuitions and present them from the perspective of autotheoremhood,\nwhere theories can include propositions referring to the theory's own theorems.\nWe then discuss the formalization of this perspective in the logics of Moore\nand Reiter, respectively, using the unifying semantic framework for default and\nautoepistemic logics that we developed earlier. We argue that Reiter's default\nlogic is a better formalization of Moore's intuitions about autoepistemic\npropositions than Moore's own autoepistemic logic.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 16:48:31 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Denecker", "Marc", ""], ["Marek", "Victor W.", ""], ["Truszczynski", "Miroslaw", ""]]}, {"id": "1108.3279", "submitter": "Miroslaw Truszczynski", "authors": "Miroslaw Truszczynski", "title": "Revisiting Epistemic Specifications", "comments": "In Marcello Balduccini and Tran Cao Son, Editors, Essays Dedicated to\n  Michael Gelfond on the Occasion of His 65th Birthday, Lexington, KY, USA,\n  October 2010, LNAI 6565, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1991, Michael Gelfond introduced the language of epistemic specifications.\nThe goal was to develop tools for modeling problems that require some form of\nmeta-reasoning, that is, reasoning over multiple possible worlds. Despite their\nrelevance to knowledge representation, epistemic specifications have received\nrelatively little attention so far. In this paper, we revisit the formalism of\nepistemic specification. We offer a new definition of the formalism, propose\nseveral semantics (one of which, under syntactic restrictions we assume, turns\nout to be equivalent to the original semantics by Gelfond), derive some\ncomplexity results and, finally, show the effectiveness of the formalism for\nmodeling problems requiring meta-reasoning considered recently by Faber and\nWoltran. All these results show that epistemic specifications deserve much more\nattention that has been afforded to them so far.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 16:49:26 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Truszczynski", "Miroslaw", ""]]}, {"id": "1108.3281", "submitter": "Miroslaw Truszczynski", "authors": "Victor W. Marek, Ilkka Niemela and Miroslaw Truszczynski", "title": "Origins of Answer-Set Programming - Some Background And Two Personal\n  Accounts", "comments": "In G. Brewka, V.M. Marek, and M. Truszczynski, eds. Nonmonotonic\n  Reasoning -- Essays Celebrating its 30th Anniversary, College Publications,\n  2011 (a volume of papers presented at NonMon at 30 meeting, Lexington, KY,\n  USA, October 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the evolution of aspects of nonmonotonic reasoning towards the\ncomputational paradigm of answer-set programming (ASP). We give a general\noverview of the roots of ASP and follow up with the personal perspective on\nresearch developments that helped verbalize the main principles of ASP and\ndifferentiated it from the classical logic programming.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 16:53:41 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Marek", "Victor W.", ""], ["Niemela", "Ilkka", ""], ["Truszczynski", "Miroslaw", ""]]}, {"id": "1108.3298", "submitter": "Nando de Freitas", "authors": "Byron Knoll, Nando de Freitas", "title": "A Machine Learning Perspective on Predictive Coding with PAQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 18:06:29 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Knoll", "Byron", ""], ["de Freitas", "Nando", ""]]}, {"id": "1108.3372", "submitter": "Miguel L\\'azaro Gredilla", "authors": "Miguel L\\'azaro-Gredilla, Steven Van Vaerenbergh, and Neil Lawrence", "title": "Overlapping Mixtures of Gaussian Processes for the Data Association\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a mixture of GPs to address the data association\nproblem, i.e. to label a group of observations according to the sources that\ngenerated them. Unlike several previously proposed GP mixtures, the novel\nmixture has the distinct characteristic of using no gating function to\ndetermine the association of samples and mixture components. Instead, all the\nGPs in the mixture are global and samples are clustered following\n\"trajectories\" across input space. We use a non-standard variational Bayesian\nalgorithm to efficiently recover sample labels and learn the hyperparameters.\nWe show how multi-object tracking problems can be disambiguated and also\nexplore the characteristics of the model in traditional regression settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 23:46:59 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["L\u00e1zaro-Gredilla", "Miguel", ""], ["Van Vaerenbergh", "Steven", ""], ["Lawrence", "Neil", ""]]}, {"id": "1108.3446", "submitter": "Josef Urban", "authors": "Jesse Alama, Tom Heskes, Daniel K\\\"uhlwein, Evgeni Tsivtsivadze, and\n  Josef Urban", "title": "Premise Selection for Mathematics by Corpus Analysis and Kernel Methods", "comments": "26 pages", "journal-ref": null, "doi": "10.1007/s10817-013-9286-5", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart premise selection is essential when using automated reasoning as a tool\nfor large-theory formal proof development. A good method for premise selection\nin complex mathematical libraries is the application of machine learning to\nlarge corpora of proofs. This work develops learning-based premise selection in\ntwo ways. First, a newly available minimal dependency analysis of existing\nhigh-level formal mathematical proofs is used to build a large knowledge base\nof proof dependencies, providing precise data for ATP-based re-verification and\nfor training premise selection algorithms. Second, a new machine learning\nalgorithm for premise selection based on kernel methods is proposed and\nimplemented. To evaluate the impact of both techniques, a benchmark consisting\nof 2078 large-theory mathematical problems is constructed,extending the older\nMPTP Challenge benchmark. The combined effect of the techniques results in a\n50% improvement on the benchmark over the Vampire/SInE state-of-the-art system\nfor automated reasoning in large theories.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 11:18:55 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 18:52:58 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Alama", "Jesse", ""], ["Heskes", "Tom", ""], ["K\u00fchlwein", "Daniel", ""], ["Tsivtsivadze", "Evgeni", ""], ["Urban", "Josef", ""]]}, {"id": "1108.3614", "submitter": "Phuong Nguyen", "authors": "Phuong Nguyen, Peter Sunehag, and Marcus Hutter", "title": "Feature Reinforcement Learning In Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Following a recent surge in using history-based methods for resolving\nperceptual aliasing in reinforcement learning, we introduce an algorithm based\non the feature reinforcement learning framework called PhiMDP. To create a\npractical algorithm we devise a stochastic search procedure for a class of\ncontext trees based on parallel tempering and a specialized proposal\ndistribution. We provide the first empirical evaluation for PhiMDP. Our\nproposed algorithm achieves superior performance to the classical U-tree\nalgorithm and the recent active-LZ algorithm, and is competitive with\nMC-AIXI-CTW that maintains a bayesian mixture over all context trees up to a\nchosen depth.We are encouraged by our ability to compete with this\nsophisticated method using an algorithm that simply picks one single model, and\nuses Q-learning on the corresponding MDP. Our PhiMDP algorithm is much simpler,\nyet consumes less time and memory. These results show promise for our future\nwork on attacking more complex and larger problems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 03:50:35 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Nguyen", "Phuong", ""], ["Sunehag", "Peter", ""], ["Hutter", "Marcus", ""]]}, {"id": "1108.3711", "submitter": "David Tolpin", "authors": "David Tolpin, Solomon Eyal Shimony", "title": "Doing Better Than UCT: Rational Monte Carlo Sampling in Trees", "comments": "Withdrawn: \"MCTS Based on Simple Regret\" (arXiv:1207.5589) is the\n  final corrected version published in AAAI 2012 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UCT, a state-of-the art algorithm for Monte Carlo tree sampling (MCTS), is\nbased on UCB, a sampling policy for the Multi-armed Bandit Problem (MAB) that\nminimizes the accumulated regret. However, MCTS differs from MAB in that only\nthe final choice, rather than all arm pulls, brings a reward, that is, the\nsimple regret, as opposite to the cumulative regret, must be minimized. This\nongoing work aims at applying meta-reasoning techniques to MCTS, which is\nnon-trivial. We begin by introducing policies for multi-armed bandits with\nlower simple regret than UCB, and an algorithm for MCTS which combines\ncumulative and simple regret minimization and outperforms UCT. We also develop\na sampling scheme loosely based on a myopic version of perfect value of\ninformation. Finite-time and asymptotic analysis of the policies is provided,\nand the algorithms are compared empirically.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 10:47:16 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2012 03:40:29 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1108.3757", "submitter": "Patryk Filipiak", "authors": "Patryk Filipiak", "title": "Self-Organizing Mixture Networks for Representation of Grayscale Digital\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-Organizing Maps are commonly used for unsupervised learning purposes.\nThis paper is dedicated to the certain modification of SOM called SOMN\n(Self-Organizing Mixture Networks) used as a mechanism for representing\ngrayscale digital images. Any grayscale digital image regarded as a\ndistribution function can be approximated by the corresponding Gaussian\nmixture. In this paper, the use of SOMN is proposed in order to obtain such\napproximations for input grayscale images in unsupervised manner.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 14:11:37 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Filipiak", "Patryk", ""]]}, {"id": "1108.3850", "submitter": "Juraj Dzifcak", "authors": "Chitta Baral and Juraj Dzifcak", "title": "Solving puzzles described in English by automated translation to answer\n  set programming and learning how to do that translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system capable of automatically solving combinatorial logic\npuzzles given in (simplified) English. It involves translating the English\ndescriptions of the puzzles into answer set programming(ASP) and using ASP\nsolvers to provide solutions of the puzzles. To translate the descriptions, we\nuse a lambda-calculus based approach using Probabilistic Combinatorial\nCategorial Grammars (PCCG) where the meanings of words are associated with\nparameters to be able to distinguish between multiple meanings of the same\nword. Meaning of many words and the parameters are learned. The puzzles are\nrepresented in ASP using an ontology which is applicable to a large set of\nlogic puzzles.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 20:17:58 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Baral", "Chitta", ""], ["Dzifcak", "Juraj", ""]]}, {"id": "1108.4199", "submitter": "Jean-Louis Dessalles", "authors": "Jean-Louis Dessalles (INFRES, LTCI)", "title": "Biomimetic use of genetic algorithms", "comments": "jld-92062501; Proceedings of the Conference on Parallel Problem\n  Solving from Nature, Amsterdam : Belgium (1992)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms are considered as an original way to solve problems,\nprobably because of their generality and of their \"blind\" nature. But GAs are\nalso unusual since the features of many implementations (among all that could\nbe thought of) are principally led by the biological metaphor, while efficiency\nmeasurements intervene only afterwards. We propose here to examine the\nrelevance of these biomimetic aspects, by pointing out some fundamental\nsimilarities and divergences between GAs and the genome of living beings shaped\nby natural selection. One of the main differences comes from the fact that GAs\nrely principally on the so-called implicit parallelism, while giving to the\nmutation/selection mechanism the second role. Such differences could suggest\nnew ways of employing GAs on complex problems, using complex codings and\nstarting from nearly homogeneous populations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2011 18:57:31 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Dessalles", "Jean-Louis", "", "INFRES, LTCI"]]}, {"id": "1108.4220", "submitter": "Thomas Wolf", "authors": "Thomas Wolf", "title": "A Dynamical Systems Approach for Static Evaluation in Go", "comments": "IEEE Transactions on Computational Intelligence and AI in Games, vol\n  3 (2011), no 2", "journal-ref": null, "doi": "10.1109/TCIAIG.2011.2141669", "report-no": null, "categories": "cs.AI math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper arguments are given why the concept of static evaluation has the\npotential to be a useful extension to Monte Carlo tree search. A new concept of\nmodeling static evaluation through a dynamical system is introduced and\nstrengths and weaknesses are discussed. The general suitability of this\napproach is demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2011 23:20:30 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Wolf", "Thomas", ""]]}, {"id": "1108.4279", "submitter": "Jean-Louis Dessalles", "authors": "Eric Bonabeau, Jean-Louis Dessalles (INFRES, LTCI)", "title": "Detection and emergence", "comments": "jld-98072401", "journal-ref": "Intellectica 25, 2 (1997) 85-94", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two different conceptions of emergence are reconciled as two instances of the\nphenomenon of detection. In the process of comparing these two conceptions, we\nfind that the notions of complexity and detection allow us to form a unified\ndefinition of emergence that clearly delineates the role of the observer.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2011 11:30:49 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Bonabeau", "Eric", "", "INFRES, LTCI"], ["Dessalles", "Jean-Louis", "", "INFRES, LTCI"]]}, {"id": "1108.4440", "submitter": "Juan Pablo Carbajal", "authors": "Juan Pablo Carbajal, Dorit Assaf, Emanuel Benker", "title": "Promoting scientific thinking with robots", "comments": "Conference paper, 2 figures", "journal-ref": "Journal: Proceedings of the 2nd International Conference on\n  Robotics in Education (RIE 2011), Year: 2011, Pages: 59 - 61, ISBN:\n  978-3-200-02273-7", "doi": null, "report-no": null, "categories": "physics.ed-ph cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This article describes an exemplary robot exercise which was conducted in a\nclass for mechatronics students. The goal of this exercise was to engage\nstudents in scientific thinking and reasoning, activities which do not always\nplay an important role in their curriculum. The robotic platform presented here\nis simple in its construction and is customizable to the needs of the teacher.\nTherefore, it can be used for exercises in many different fields of science,\nnot necessarily related to robotics. Here we present a situation where the\nrobot is used like an alien creature from which we want to understand its\nbehavior, resembling an ethological research activity. This robot exercise is\nsuited for a wide range of courses, from general introduction to science, to\nhardware oriented lectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2011 21:03:32 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Carbajal", "Juan Pablo", ""], ["Assaf", "Dorit", ""], ["Benker", "Emanuel", ""]]}, {"id": "1108.4804", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Wolfgang Dvo\\v{r}\\'ak, Michael Morak, Clemens Nopp, Stefan Woltran", "title": "dynPARTIX - A Dynamic Programming Reasoner for Abstract Argumentation", "comments": "The paper appears in the Proceedings of the 19th International\n  Conference on Applications of Declarative Programming and Knowledge\n  Management (INAP 2011)", "journal-ref": null, "doi": "10.1007/978-3-642-41524-1_14", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to announce the release of a novel system for\nabstract argumentation which is based on decomposition and dynamic programming.\nWe provide first experimental evaluations to show the feasibility of this\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 10:53:02 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Morak", "Michael", ""], ["Nopp", "Clemens", ""], ["Woltran", "Stefan", ""]]}, {"id": "1108.4891", "submitter": "Christoph Wernhard", "authors": "Christoph Wernhard", "title": "Computing with Logic as Operator Elimination: The ToyElim System", "comments": "Appears in the Proceedings of the 25th Workshop on Logic Programming\n  (WLP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototype system is described whose core functionality is, based on\npropositional logic, the elimination of second-order operators, such as Boolean\nquantifiers and operators for projection, forgetting and circumscription. This\napproach allows to express many representational and computational tasks in\nknowledge representation - for example computation of abductive explanations\nand models with respect to logic programming semantics - in a uniform\noperational system, backed by a uniform classical semantic framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 17:21:58 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Wernhard", "Christoph", ""]]}, {"id": "1108.4942", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Wolfgang Dvo\\v{r}\\'ak, Sarah Alice Gaggl, Johannes Wallner, Stefan\n  Woltran", "title": "Making Use of Advances in Answer-Set Programming for Abstract\n  Argumentation Systems", "comments": "Paper appears in the Proceedings of the 19th International Conference\n  on Applications of Declarative Programming and Knowledge Management (INAP\n  2011)", "journal-ref": null, "doi": "10.1007/978-3-642-41524-1_7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dung's famous abstract argumentation frameworks represent the core formalism\nfor many problems and applications in the field of argumentation which\nsignificantly evolved within the last decade. Recent work in the field has thus\nfocused on implementations for these frameworks, whereby one of the main\napproaches is to use Answer-Set Programming (ASP). While some of the\nargumentation semantics can be nicely expressed within the ASP language, others\nrequired rather cumbersome encoding techniques. Recent advances in ASP systems,\nin particular, the metasp optimization frontend for the ASP-package\ngringo/claspD provides direct commands to filter answer sets satisfying certain\nsubset-minimality (or -maximality) constraints. This allows for much simpler\nencodings compared to the ones in standard ASP language. In this paper, we\nexperimentally compare the original encodings (for the argumentation semantics\nbased on preferred, semi-stable, and respectively, stage extensions) with new\nmetasp encodings. Moreover, we provide novel encodings for the recently\nintroduced resolution-based grounded semantics. Our experimental results\nindicate that the metasp approach works well in those cases where the\ncomplexity of the encoded problem is adequately mirrored within the metasp\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 20:19:09 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Gaggl", "Sarah Alice", ""], ["Wallner", "Johannes", ""], ["Woltran", "Stefan", ""]]}, {"id": "1108.4973", "submitter": "Alexandre Levada", "authors": "Alexandre L. M. Levada", "title": "Learning from Complex Systems: On the Roles of Entropy and Fisher\n  Information in Pairwise Isotropic Gaussian Markov Random Fields", "comments": "46 pages, 16 Figures", "journal-ref": "Entropy, v. 16, n. 2, Special Issue on Information Geometry, 2014", "doi": "10.3390/e16021002", "report-no": null, "categories": "cs.IT cs.AI cs.CV math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Field models are powerful tools for the study of complex\nsystems. However, little is known about how the interactions between the\nelements of such systems are encoded, especially from an information-theoretic\nperspective. In this paper, our goal is to enlight the connection between\nFisher information, Shannon entropy, information geometry and the behavior of\ncomplex systems modeled by isotropic pairwise Gaussian Markov random fields. We\npropose analytical expressions to compute local and global versions of these\nmeasures using Besag's pseudo-likelihood function, characterizing the system's\nbehavior through its \\emph{Fisher curve}, a parametric trajectory accross the\ninformation space that provides a geometric representation for the study of\ncomplex systems. Computational experiments show how the proposed tools can be\nuseful in extrating relevant information from complex patterns. The obtained\nresults quantify and support our main conclusion, which is: in terms of\ninformation, moving towards higher entropy states (A --> B) is different from\nmoving towards lower entropy states (B --> A), since the \\emph{Fisher curves}\nare not the same given a natural orientation (the direction of time).\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 00:50:42 GMT"}, {"version": "v10", "created": "Mon, 27 May 2013 19:24:52 GMT"}, {"version": "v11", "created": "Tue, 11 Jun 2013 17:15:42 GMT"}, {"version": "v12", "created": "Wed, 16 Oct 2013 20:35:39 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2011 14:36:19 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2011 17:04:07 GMT"}, {"version": "v4", "created": "Fri, 25 Nov 2011 01:50:30 GMT"}, {"version": "v5", "created": "Thu, 24 May 2012 18:59:01 GMT"}, {"version": "v6", "created": "Mon, 26 Nov 2012 23:37:41 GMT"}, {"version": "v7", "created": "Sun, 2 Dec 2012 01:26:00 GMT"}, {"version": "v8", "created": "Sat, 8 Dec 2012 12:40:00 GMT"}, {"version": "v9", "created": "Thu, 23 May 2013 23:45:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Levada", "Alexandre L. M.", ""]]}, {"id": "1108.5002", "submitter": "Yoshitaka Kameya", "authors": "Yoshitaka Kameya, Satoru Nakamura, Tatsuya Iwasaki and Taisuke Sato", "title": "Verbal Characterization of Probabilistic Clusters using Minimal\n  Discriminative Propositions", "comments": "13 pages including 3 figures. This is the full version of a paper at\n  ICTAI-2011 (http://www.cse.fau.edu/ictai2011/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a knowledge discovery process, interpretation and evaluation of the mined\nresults are indispensable in practice. In the case of data clustering, however,\nit is often difficult to see in what aspect each cluster has been formed. This\npaper proposes a method for automatic and objective characterization or\n\"verbalization\" of the clusters obtained by mixture models, in which we collect\nconjunctions of propositions (attribute-value pairs) that help us interpret or\nevaluate the clusters. The proposed method provides us with a new, in-depth and\nconsistent tool for cluster interpretation/evaluation, and works for various\ntypes of datasets including continuous attributes and missing values.\nExperimental results with a couple of standard datasets exhibit the utility of\nthe proposed method, and the importance of the feedbacks from the\ninterpretation/evaluation step.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 03:41:26 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2011 02:48:36 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Kameya", "Yoshitaka", ""], ["Nakamura", "Satoru", ""], ["Iwasaki", "Tatsuya", ""], ["Sato", "Taisuke", ""]]}, {"id": "1108.5016", "submitter": "Maxime Amblard", "authors": "Maxime Amblard (LORIA), Musiol Michel (LABPSYLOR), Rebuschi Manuel\n  (LHSP)", "title": "Une analyse bas\\'ee sur la S-DRT pour la mod\\'elisation de dialogues\n  pathologiques", "comments": "Traitement Automatique des Langues, Montpellier : France (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a corpus of dialogues between a schizophrenic\nspeaker and an interlocutor who drives the dialogue. We had identified specific\ndiscontinuities for paranoid schizophrenics. We propose a modeling of these\ndiscontinuities with S-DRT (its pragmatic part)\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 06:07:06 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Amblard", "Maxime", "", "LORIA"], ["Michel", "Musiol", "", "LABPSYLOR"], ["Manuel", "Rebuschi", "", "LHSP"]]}, {"id": "1108.5017", "submitter": "Maxime Amblard", "authors": "Sai Qian (LORIA), Maxime Amblard (LORIA)", "title": "Event in Compositional Dynamic Semantics", "comments": "16 pages; Logical Aspect of Computational Linguistic, Montpellier :\n  France (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework which constructs an event-style dis- course semantics.\nThe discourse dynamics are encoded in continuation semantics and various\nrhetorical relations are embedded in the resulting interpretation of the\nframework. We assume discourse and sentence are distinct semantic objects, that\nplay different roles in meaning evalua- tion. Moreover, two sets of composition\nfunctions, for handling different discourse relations, are introduced. The\npaper first gives the necessary background and motivation for event and dynamic\nsemantics, then the framework with detailed examples will be introduced.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 06:08:16 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Qian", "Sai", "", "LORIA"], ["Amblard", "Maxime", "", "LORIA"]]}, {"id": "1108.5027", "submitter": "Maxime Amblard", "authors": "Maxime Amblard (LORIA)", "title": "Encoding Phases using Commutativity and Non-commutativity in a Logical\n  Framework", "comments": "Logical Aspect of Computational Linguistic, Montpellier : France\n  (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an extension of Minimalist Categorial Gram- mars (MCG)\nto encode Chomsky's phases. These grammars are based on Par- tially Commutative\nLogic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The\nfirst implementation of MCG were using both non- commutative properties (to\nrespect the linear word order in an utterance) and commutative ones (to model\nfeatures of different constituents). Here, we pro- pose to adding Chomsky's\nphases with the non-commutative tensor product of the logic. Then we could give\naccount of the PIC just by using logical prop- erties of the framework.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 07:22:09 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Amblard", "Maxime", "", "LORIA"]]}, {"id": "1108.5250", "submitter": "Tshilidzi Marwala", "authors": "A.K. Mohamed, T. Marwala, and L.R. John", "title": "Single-trial EEG Discrimination between Wrist and Finger Movement\n  Imagery and Execution in a Sensorimotor BCI", "comments": "33rd Annual International IEEE EMBS Conference 2011", "journal-ref": null, "doi": "10.1109/IEMBS.2011.6091552", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) may be used to control a prosthetic or\northotic hand using neural activity from the brain. The core of this\nsensorimotor BCI lies in the interpretation of the neural information extracted\nfrom electroencephalogram (EEG). It is desired to improve on the interpretation\nof EEG to allow people with neuromuscular disorders to perform daily\nactivities. This paper investigates the possibility of discriminating between\nthe EEG associated with wrist and finger movements. The EEG was recorded from\ntest subjects as they executed and imagined five essential hand movements using\nboth hands. Independent component analysis (ICA) and time-frequency techniques\nwere used to extract spectral features based on event-related\n(de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used\nfor feature reduction. Mahalanobis distance (MD) clustering and artificial\nneural networks (ANN) were used as classifiers and obtained average accuracies\nof 65 % and 71 % respectively. This shows that EEG discrimination between wrist\nand finger movements is possible. The research introduces a new combination of\nmotor tasks to BCI research.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 07:10:04 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mohamed", "A. K.", ""], ["Marwala", "T.", ""], ["John", "L. R.", ""]]}, {"id": "1108.5567", "submitter": "Peter Sch\\\"uller", "authors": "Yuliya Lierler (University of Kentucky) and Peter Sch\\\"uller\n  (Technische Universit\\\"at Wien)", "title": "Parsing Combinatory Categorial Grammar with Answer Set Programming:\n  Preliminary Report", "comments": "12 pages, 2 figures, Proceedings of the 25th Workshop on Logic\n  Programming (WLP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatory categorial grammar (CCG) is a grammar formalism used for natural\nlanguage parsing. CCG assigns structured lexical categories to words and uses a\nsmall set of combinatory rules to combine these categories to parse a sentence.\nIn this work we propose and implement a new approach to CCG parsing that relies\non a prominent knowledge representation formalism, answer set programming (ASP)\n- a declarative programming paradigm. We formulate the task of CCG parsing as a\nplanning problem and use an ASP computational tool to compute solutions that\ncorrespond to valid parses. Compared to other approaches, there is no need to\nimplement a specific parsing algorithm using such a declarative method. Our\napproach aims at producing all semantically distinct parse trees for a given\nsentence. From this goal, normalization and efficiency issues arise, and we\ndeal with them by combining and extending existing strategies. We have\nimplemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main\ncomputational means. The C&C supertagger can be used as a preprocessor within\nAspCcgTk, which allows us to achieve wide-coverage natural language parsing.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 14:27:04 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Lierler", "Yuliya", "", "University of Kentucky"], ["Sch\u00fcller", "Peter", "", "Technische Universit\u00e4t Wien"]]}, {"id": "1108.5586", "submitter": "Petra Hofstedt", "authors": "Denny Schneeweiss and Petra Hofstedt", "title": "FdConfig: A Constraint-Based Interactive Product Configurator", "comments": "19th International Conference on Applications of Declarative\n  Programming and Knowledge Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constraint-based approach to interactive product configuration.\nOur configurator tool FdConfig is based on feature models for the\nrepresentation of the product domain. Such models can be directly mapped into\nconstraint satisfaction problems and dealt with by appropriate constraint\nsolvers. During the interactive configuration process the user generates new\nconstraints as a result of his configuration decisions and even may retract\nconstraints posted earlier. We discuss the configuration process, explain the\nunderlying techniques and show optimizations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 14:55:47 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Schneeweiss", "Denny", ""], ["Hofstedt", "Petra", ""]]}, {"id": "1108.5626", "submitter": "Thomas Krennwallner", "authors": "Thomas Eiter, Thomas Krennwallner, Christoph Redl", "title": "Nested HEX-Programs", "comments": "Proceedings of the 19th International Conference on Applications of\n  Declarative Programming and Knowledge Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer-Set Programming (ASP) is an established declarative programming\nparadigm. However, classical ASP lacks subprogram calls as in procedural\nprogramming, and access to external computations (like remote procedure calls)\nin general. The feature is desired for increasing modularity and---assuming\nproper access in place---(meta-)reasoning over subprogram results. While\nHEX-programs extend classical ASP with external source access, they do not\nsupport calls of (sub-)programs upfront. We present nested HEX-programs, which\nextend HEX-programs to serve the desired feature, in a user-friendly manner.\nNotably, the answer sets of called sub-programs can be individually accessed.\nThis is particularly useful for applications that need to reason over answer\nsets like belief set merging, user-defined aggregate functions, or preferences\nof answer sets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 16:16:14 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Eiter", "Thomas", ""], ["Krennwallner", "Thomas", ""], ["Redl", "Christoph", ""]]}, {"id": "1108.5667", "submitter": "Stef De Pooter", "authors": "Stef De Pooter, Johan Wittocx and Marc Denecker", "title": "A prototype of a knowledge-based programming environment", "comments": "6 pages, appears in the Proceedings of the 19th International\n  Conference on Applications of Declarative Programming and Knowledge\n  Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a proposal for a knowledge-based programming\nenvironment. In such an environment, declarative background knowledge,\nprocedures, and concrete data are represented in suitable languages and\ncombined in a flexible manner. This leads to a highly declarative programming\nstyle. We illustrate our approach on an example and report about our prototype\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 17:38:33 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["De Pooter", "Stef", ""], ["Wittocx", "Johan", ""], ["Denecker", "Marc", ""]]}, {"id": "1108.5668", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Ludovic Denoyer, Philippe Preux and Patrick\n  Gallinari", "title": "Datum-Wise Classification: A Sequential Approach to Sparsity", "comments": "ECML2011", "journal-ref": "Lecture Notes in Computer Science, 2011, Volume 6911/2011, 375-390", "doi": "10.1007/978-3-642-23780-5_34", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel classification technique whose aim is to select an\nappropriate representation for each datapoint, in contrast to the usual\napproach of selecting a representation encompassing the whole dataset. This\ndatum-wise representation is found by using a sparsity inducing empirical risk,\nwhich is a relaxation of the standard L 0 regularized risk. The classification\nproblem is modeled as a sequential decision process that sequentially chooses,\nfor each datapoint, which features to use before classifying. Datum-Wise\nClassification extends naturally to multi-class tasks, and we describe a\nspecific case where our inference has equivalent complexity to a traditional\nlinear classifier, while still using a variable number of features. We compare\nour classifier to classical L 1 regularized linear models (L 1-SVM and LARS) on\na set of common binary and multi-class datasets and show that for an equal\naverage number of features used we can get improved performance using our\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 17:46:08 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Preux", "Philippe", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1108.5710", "submitter": "Mark Schmidt", "authors": "Mark Schmidt (INRIA Paris - Rocquencourt), Karteek Alahari (INRIA\n  Paris - Rocquencourt)", "title": "Generalized Fast Approximate Energy Minimization via Graph Cuts:\n  Alpha-Expansion Beta-Shrink Moves", "comments": "Conference on Uncertainty in Artificial Intelligence (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present alpha-expansion beta-shrink moves, a simple generalization of the\nwidely-used alpha-beta swap and alpha-expansion algorithms for approximate\nenergy minimization. We show that in a certain sense, these moves dominate both\nalpha-beta-swap and alpha-expansion moves, but unlike previous generalizations\nthe new moves require no additional assumptions and are still solvable in\npolynomial-time. We show promising experimental results with the new moves,\nwhich we believe could be used in any context where alpha-expansions are\ncurrently employed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 19:06:30 GMT"}], "update_date": "2012-02-19", "authors_parsed": [["Schmidt", "Mark", "", "INRIA Paris - Rocquencourt"], ["Alahari", "Karteek", "", "INRIA\n  Paris - Rocquencourt"]]}, {"id": "1108.5717", "submitter": "Lilyana Mihalkova", "authors": "Lilyana Mihalkova and Walaa Eldin Moustafa", "title": "Structure Selection from Streaming Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical relational learning techniques have been successfully applied in\na wide range of relational domains. In most of these applications, the human\ndesigners capitalized on their background knowledge by following a\ntrial-and-error trajectory, where relational features are manually defined by a\nhuman engineer, parameters are learned for those features on the training data,\nthe resulting model is validated, and the cycle repeats as the engineer adjusts\nthe set of features. This paper seeks to streamline application development in\nlarge relational domains by introducing a light-weight approach that\nefficiently evaluates relational features on pieces of the relational graph\nthat are streamed to it one at a time. We evaluate our approach on two social\nmedia tasks and demonstrate that it leads to more accurate models that are\nlearned faster.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 19:19:17 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Mihalkova", "Lilyana", ""], ["Moustafa", "Walaa Eldin", ""]]}, {"id": "1108.5794", "submitter": "Christoph Beierle", "authors": "Christoph Beierle, Gabriele Kern-Isberner, Karl S\\\"odler", "title": "A Constraint Logic Programming Approach for Computing Ordinal\n  Conditional Functions", "comments": "To appear in the Proceedings of the 25th Workshop on Logic\n  Programming (WLP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to give appropriate semantics to qualitative conditionals of the\nform \"if A then normally B\", ordinal conditional functions (OCFs) ranking the\npossible worlds according to their degree of plausibility can be used. An OCF\naccepting all conditionals of a knowledge base R can be characterized as the\nsolution of a constraint satisfaction problem. We present a high-level,\ndeclarative approach using constraint logic programming techniques for solving\nthis constraint satisfaction problem. In particular, the approach developed\nhere supports the generation of all minimal solutions; these minimal solutions\nare of special interest as they provide a basis for model-based inference from\nR.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 01:41:34 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Beierle", "Christoph", ""], ["Kern-Isberner", "Gabriele", ""], ["S\u00f6dler", "Karl", ""]]}, {"id": "1108.5825", "submitter": "Lena Wiese", "authors": "Katsumi Inoue and Chiaki Sakama and Lena Wiese", "title": "Confidentiality-Preserving Data Publishing for Credulous Users by\n  Extended Abduction", "comments": "Paper appears in the Proceedings of the 19th International Conference\n  on Applications of Declarative Programming and Knowledge Management (INAP\n  2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publishing private data on external servers incurs the problem of how to\navoid unwanted disclosure of confidential data. We study a problem of\nconfidentiality in extended disjunctive logic programs and show how it can be\nsolved by extended abduction. In particular, we analyze how credulous\nnon-monotonic reasoning affects confidentiality.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 04:19:40 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Inoue", "Katsumi", ""], ["Sakama", "Chiaki", ""], ["Wiese", "Lena", ""]]}, {"id": "1108.5837", "submitter": "Tomi Janhunen", "authors": "Mai Nguyen, Tomi Janhunen, Ilkka Niemel\\\"a", "title": "Translating Answer-Set Programs into Bit-Vector Logic", "comments": "12 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer set programming (ASP) is a paradigm for declarative problem solving\nwhere problems are first formalized as rule sets, i.e., answer-set programs, in\na uniform way and then solved by computing answer sets for programs. The\nsatisfiability modulo theories (SMT) framework follows a similar modelling\nphilosophy but the syntax is based on extensions of propositional logic rather\nthan rules. Quite recently, a translation from answer-set programs into\ndifference logic was provided---enabling the use of particular SMT solvers for\nthe computation of answer sets. In this paper, the translation is revised for\nanother SMT fragment, namely that based on fixed-width bit-vector theories.\nThus, even further SMT solvers can be harnessed for the task of computing\nanswer sets. The results of a preliminary experimental comparison are also\nreported. They suggest a level of performance which is similar to that achieved\nvia difference logic.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 05:52:38 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Nguyen", "Mai", ""], ["Janhunen", "Tomi", ""], ["Niemel\u00e4", "Ilkka", ""]]}, {"id": "1108.5943", "submitter": "Xishun Zhao", "authors": "Xishun Zhao, Yuping Shen", "title": "Proof System for Plan Verification under 0-Approximation Semantics", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a proof system is developed for plan verification problems\n$\\{X\\}c\\{Y\\}$ and $\\{X\\}c\\{KW p\\}$ under 0-approximation semantics for\n${\\mathcal A}_K$. Here, for a plan $c$, two sets $X,Y$ of fluent literals, and\na literal $p$, $\\{X\\}c\\{Y\\}$ (resp. $\\{X\\}c\\{KW p\\}$) means that all literals\nof $Y$ become true (resp. $p$ becomes known) after executing $c$ in any initial\nstate in which all literals in $X$ are true.Then, soundness and completeness\nare proved. The proof system allows verifying plans and generating plans as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 12:48:29 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2011 09:37:04 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Zhao", "Xishun", ""], ["Shen", "Yuping", ""]]}, {"id": "1108.6007", "submitter": "Markus Triska", "authors": "Markus Triska", "title": "Domain-specific Languages in a Finite Domain Constraint Programming\n  System", "comments": "Proceedings of the 19th International Conference on Applications of\n  Declarative Programming and Knowledge Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present domain-specific languages (DSLs) that we devised\nfor their use in the implementation of a finite domain constraint programming\nsystem, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs\nare used in propagator selection and constraint reification. In these areas,\nthey lead to concise specifications that are easy to read and reason about. At\ncompilation time, these specifications are translated to Prolog code, reducing\ninterpretative run-time overheads. The devised languages can be used in the\nimplementation of other finite domain constraint solvers as well and may\ncontribute to their correctness, conciseness and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 16:43:17 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Triska", "Markus", ""]]}, {"id": "1108.6208", "submitter": "Norbert Manthey", "authors": "Norbert Manthey", "title": "Coprocessor - a Standalone SAT Preprocessor", "comments": "system description, short paper, WLP 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a stand-alone preprocessor for SAT is presented that is able to\nperform most of the known preprocessing techniques. Preprocessing a formula in\nSAT is important for performance since redundancy can be removed. The\npreprocessor is part of the SAT solver riss and is called Coprocessor. Not only\nriss, but also MiniSat 2.2 benefit from it, because the SatELite preprocessor\nof MiniSat does not implement recent techniques. By using more advanced\ntechniques, Coprocessor is able to reduce the redundancy in a formula further\nand improves the overall solving performance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 12:38:21 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Manthey", "Norbert", ""]]}, {"id": "1108.6211", "submitter": "Alessandro Lazaric", "authors": "Alessandro Lazaric (INRIA Lille - Nord Europe), Marcello Restelli", "title": "Transfer from Multiple MDPs", "comments": "2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer reinforcement learning (RL) methods leverage on the experience\ncollected on a set of source tasks to speed-up RL algorithms. A simple and\neffective approach is to transfer samples from source tasks and include them\ninto the training set used to solve a given target task. In this paper, we\ninvestigate the theoretical properties of this transfer method and we introduce\nnovel algorithms adapting the transfer process on the basis of the similarity\nbetween source and target tasks. Finally, we report illustrative experimental\nresults in a continuous chain problem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 12:46:11 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2011 09:19:00 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Lazaric", "Alessandro", "", "INRIA Lille - Nord Europe"], ["Restelli", "Marcello", ""]]}, {"id": "1108.6223", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards Configuration of applied Web-based information system", "comments": "13 pages, 9 tables, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DM cs.NI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, combinatorial synthesis of structure for applied Web-based\nsystems is described. The problem is considered as a combination of selected\ndesign alternatives for system parts/components into a resultant composite\ndecision (i.e., system configuration design). The solving framework is based on\nHierarchical Morphological Multicriteria Design (HMMD) approach: (i)\nmulticriteria selection of alternatives for system parts, (ii) composing the\nselected alternatives into a resultant combination (while taking into account\nordinal quality of the alternatives above and their compatibility). A\nlattice-based discrete space is used to evaluate (to integrate) quality of the\nresultant combinations (i.e., composite system decisions or system\nconfigurations). In addition, a simplified solving framework based on\nmulticriteria multiple choice problem is considered. A multistage design\nprocess to obtain a system trajectory is described as well. The basic applied\nexample is targeted to an applied Web-based system for a communication service\nprovider. Two other applications are briefly described (corporate system and\ninformation system for academic application).\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 13:26:14 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1108.6274", "submitter": "Rainer L\\\"udecke", "authors": "Rainer L\\\"udecke", "title": "Every Formula-Based Logic Program Has a Least Infinite-Valued Model", "comments": "This paper appears in the Proceedings of the 19th International\n  Conference on Applications of Declarative Programming and Knowledge\n  Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every definite logic program has as its meaning a least Herbrand model with\nrespect to the program-independent ordering \"set-inclusion\". In the case of\nnormal logic programs there do not exist least models in general. However,\naccording to a recent approach by Rondogiannis and Wadge, who consider\ninfinite-valued models, every normal logic program does have a least model with\nrespect to a program-independent ordering. We show that this approach can be\nextended to formula-based logic programs (i.e., finite sets of rules of the\nform A\\leftarrowF where A is an atom and F an arbitrary first-order formula).\nWe construct for a given program P an interpretation M_P and show that it is\nthe least of all models of P. Keywords: Logic programming, semantics of\nprograms, negation-as-failure, infinite-valued logics, set theory\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 15:47:04 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["L\u00fcdecke", "Rainer", ""]]}]