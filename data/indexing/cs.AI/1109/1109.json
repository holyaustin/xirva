[{"id": "1109.0113", "submitter": "EPTCS", "authors": "Martin Gebser (University of Potsdam), Roland Kaminski (University of\n  Potsdam), Torsten Schaub (University of Potsdam)", "title": "aspcud: A Linux Package Configuration Tool Based on Answer Set\n  Programming", "comments": "In Proceedings LoCoCo 2011, arXiv:1108.6097", "journal-ref": "EPTCS 65, 2011, pp. 12-25", "doi": "10.4204/EPTCS.65.2", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Linux package configuration tool aspcud based on Answer Set\nProgramming. In particular, we detail aspcud's preprocessor turning a CUDF\nspecification into a set of logical facts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 07:34:12 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Gebser", "Martin", "", "University of Potsdam"], ["Kaminski", "Roland", "", "University of\n  Potsdam"], ["Schaub", "Torsten", "", "University of Potsdam"]]}, {"id": "1109.0114", "submitter": "EPTCS", "authors": "Gerhard Friedrich (Alpen-Adria Universit\\\"at), Anna Ryabokon\n  (Alpen-Adria Universit\\\"at), Andreas A. Falkner (Siemens AG \\\"Osterreich),\n  Alois Haselb\\\"ock (Siemens AG \\\"Osterreich), Gottfried Schenner (Siemens AG\n  \\\"Osterreich), Herwig Schreiner (Siemens AG \\\"Osterreich)", "title": "(Re)configuration based on model generation", "comments": "In Proceedings LoCoCo 2011, arXiv:1108.6097", "journal-ref": "EPTCS 65, 2011, pp. 26-35", "doi": "10.4204/EPTCS.65.3", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfiguration is an important activity for companies selling configurable\nproducts or services which have a long life time. However, identification of a\nset of required changes in a legacy configuration is a hard problem, since even\nsmall changes in the requirements might imply significant modifications. In\nthis paper we show a solution based on answer set programming, which is a\nlogic-based knowledge representation formalism well suited for a compact\ndescription of (re)configuration problems. Its applicability is demonstrated on\nsimple abstractions of several real-world scenarios. The evaluation of our\nsolution on a set of benchmark instances derived from commercial\n(re)configuration problems shows its practical applicability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 07:34:15 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Friedrich", "Gerhard", "", "Alpen-Adria Universit\u00e4t"], ["Ryabokon", "Anna", "", "Alpen-Adria Universit\u00e4t"], ["Falkner", "Andreas A.", "", "Siemens AG \u00d6sterreich"], ["Haselb\u00f6ck", "Alois", "", "Siemens AG \u00d6sterreich"], ["Schenner", "Gottfried", "", "Siemens AG\n  \u00d6sterreich"], ["Schreiner", "Herwig", "", "Siemens AG \u00d6sterreich"]]}, {"id": "1109.0333", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "A KIF Formalization for the IFF Category Theory Ontology", "comments": "Paper presented at the Standard Upper Ontology workshop of the 17th\n  International Joint Conference on Artificial Intelligence (IJCAI-01), August,\n  2001, Seattle, Washington", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper begins the discussion of how the Information Flow Framework can be\nused to provide a principled foundation for the metalevel (or structural level)\nof the Standard Upper Ontology (SUO). This SUO structural level can be used as\na logical framework for manipulating collections of ontologies in the object\nlevel of the SUO or other middle level or domain ontologies. From the\nInformation Flow perspective, the SUO structural level resolves into several\nmetalevel ontologies. This paper discusses a KIF formalization for one of those\nmetalevel categories, the Category Theory Ontology. In particular, it discusses\nits category and colimit sub-namespaces.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 00:20:04 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1109.0616", "submitter": "Josef Urban", "authors": "Josef Urban, Piotr Rudnicki and Geoff Sutcliffe", "title": "ATP and Presentation Service for Mizar Formalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Automated Reasoning for Mizar (MizAR) service, which\nintegrates several automated reasoning, artificial intelligence, and\npresentation tools with Mizar and its authoring environment. The service\nprovides ATP assistance to Mizar authors in finding and explaining proofs, and\noffers generation of Mizar problems as challenges to ATP systems. The service\nis based on a sound translation from the Mizar language to that of first-order\nATP systems, and relies on the recent progress in application of ATP systems in\nlarge theories containing tens of thousands of available facts. We present the\nmain features of MizAR services, followed by an account of initial experiments\nin finding proofs with the ATP assistance. Our initial experience indicates\nthat the tool offers substantial help in exploring the Mizar library and in\npreparing new Mizar articles.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2011 11:42:52 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2012 17:45:34 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Urban", "Josef", ""], ["Rudnicki", "Piotr", ""], ["Sutcliffe", "Geoff", ""]]}, {"id": "1109.0621", "submitter": "Krzysztof Kluza", "authors": "Krzysztof Kluza, Grzegorz J. Nalepa and {\\L}ukasz {\\L}ysik", "title": "Visual Inference Specification Methods for Modularized Rulebases.\n  Overview and Integration Proposal", "comments": "from the KESE6 workshop at the 33rd German AI Conference KI-2010 in\n  Karlsruhe (see: http://ai.ia.agh.edu.pl/wiki/kese:kese6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper concerns selected rule modularization techniques. Three visual\nmethods for inference specification for modularized rule- bases are described:\nDrools Flow, BPMN and XTT2. Drools Flow is a popular technology for workflow or\nprocess modeling, BPMN is an OMG standard for modeling business processes, and\nXTT2 is a hierarchical tab- ular system specification method. Because of some\nlimitations of these solutions, several proposals of their integration are\ngiven.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2011 13:46:21 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Kluza", "Krzysztof", ""], ["Nalepa", "Grzegorz J.", ""], ["\u0141ysik", "\u0141ukasz", ""]]}, {"id": "1109.0633", "submitter": "Jesse Alama", "authors": "Jesse Alama", "title": "Eliciting implicit assumptions of proofs in the MIZAR Mathematical\n  Library by property omission", "comments": "11 pages, 3 tables. Preliminary version presented at the 3rd Workshop\n  on Modules and Libraries for Proof Assistants (MLPA-11), affiliated with the\n  2nd Conference on Interactive Theorem Proving (ITP-2011), Nijmegen, the\n  Netherlands", "journal-ref": null, "doi": "10.1007/s10817-012-9264-3", "report-no": null, "categories": "cs.LO cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When formalizing proofs with interactive theorem provers, it often happens\nthat extra background knowledge (declarative or procedural) about mathematical\nconcepts is employed without the formalizer explicitly invoking it, to help the\nformalizer focus on the relevant details of the proof. In the contexts of\nproducing and studying a formalized mathematical argument, such mechanisms are\nclearly valuable. But we may not always wish to suppress background knowledge.\nFor certain purposes, it is important to know, as far as possible, precisely\nwhat background knowledge was implicitly employed in a formal proof. In this\nnote we describe an experiment conducted on the MIZAR Mathematical Library of\nformal mathematical proofs to elicit one such class of implicitly employed\nbackground knowledge: properties of functions and relations (e.g.,\ncommutativity, asymmetry, etc.).\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2011 16:27:08 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Alama", "Jesse", ""]]}, {"id": "1109.0820", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Yonatan Wexler and Amnon Shashua", "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiclass prediction is the problem of classifying an object into a relevant\ntarget class. We consider the problem of learning a multiclass predictor that\nuses only few features, and in particular, the number of used features should\nincrease sub-linearly with the number of possible classes. This implies that\nfeatures should be shared by several classes. We describe and analyze the\nShareBoost algorithm for learning a multiclass predictor that uses few shared\nfeatures. We prove that ShareBoost efficiently finds a predictor that uses few\nshared features (if such a predictor exists) and that it has a small\ngeneralization error. We also describe how to use ShareBoost for learning a\nnon-linear predictor that has a fast evaluation time. In a series of\nexperiments with natural data sets we demonstrate the benefits of ShareBoost\nand evaluate its success relatively to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 07:52:17 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Wexler", "Yonatan", ""], ["Shashua", "Amnon", ""]]}, {"id": "1109.1032", "submitter": "Emanuele Coviello", "authors": "Emanuele Coviello, Antoni B. Chan, Gert R.G. Lanckriet", "title": "Tech Report A Variational HEM Algorithm for Clustering Hidden Markov\n  Models", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hidden Markov model (HMM) is a generative model that treats sequential\ndata under the assumption that each observation is conditioned on the state of\na discrete hidden variable that evolves in time as a Markov chain. In this\npaper, we derive a novel algorithm to cluster HMMs through their probability\ndistributions. We propose a hierarchical EM algorithm that i) clusters a given\ncollection of HMMs into groups of HMMs that are similar, in terms of the\ndistributions they represent, and ii) characterizes each group by a \"cluster\ncenter\", i.e., a novel HMM that is representative for the group. We present\nseveral empirical studies that illustrate the benefits of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 00:12:55 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Coviello", "Emanuele", ""], ["Chan", "Antoni B.", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "1109.1133", "submitter": "Shervan Fekri ershad", "authors": "Shervan Fekri Ershad", "title": "Color Texture Classification Approach Based on Combination of Primitive\n  Pattern Units and Statistical Features", "comments": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.3, No.3, August 2011", "journal-ref": null, "doi": "10.5121/ijma.2011.3301", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Texture classification became one of the problems which has been paid much\nattention on by image processing scientists since late 80s. Consequently, since\nnow many different methods have been proposed to solve this problem. In most of\nthese methods the researchers attempted to describe and discriminate textures\nbased on linear and non-linear patterns. The linear and non-linear patterns on\nany window are based on formation of Grain Components in a particular order.\nGrain component is a primitive unit of morphology that most meaningful\ninformation often appears in the form of occurrence of that. The approach which\nis proposed in this paper could analyze the texture based on its grain\ncomponents and then by making grain components histogram and extracting\nstatistical features from that would classify the textures. Finally, to\nincrease the accuracy of classification, proposed approach is expanded to color\nimages to utilize the ability of approach in analyzing each RGB channels,\nindividually. Although, this approach is a general one and it could be used in\ndifferent applications, the method has been tested on the stone texture and the\nresults can prove the quality of approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 10:24:07 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Ershad", "Shervan Fekri", ""]]}, {"id": "1109.1231", "submitter": "Luis Quesada", "authors": "Hadrien Cambazard, Deepak Mehta, Barry O'Sullivan, Luis Quesada, Marco\n  Ruffini, David Payne, Linda Doyle", "title": "A Combinatorial Optimisation Approach to Designing Dual-Parented\n  Long-Reach Passive Optical Networks", "comments": "University of Ulster, Intelligent System Research Centre, technical\n  report series. ISSN 2041-6407", "journal-ref": "Proceedings of the 22nd Irish Conference on Artificial\n  Intelligence and Cognitive Science (AICS 2011), pp. 26-35, Derry, UK", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application focused on the design of resilient long-reach\npassive optical networks. We specifically consider dual-parented networks\nwhereby each customer must be connected to two metro sites via local exchange\nsites. An important property of such a placement is resilience to single metro\nnode failure. The objective of the application is to determine the optimal\nposition of a set of metro nodes such that the total optical fibre length is\nminimized. We prove that this problem is NP-Complete. We present two\nalternative combinatorial optimisation approaches to finding an optimal metro\nnode placement using: a mixed integer linear programming (MIP) formulation of\nthe problem; and, a hybrid approach that uses clustering as a preprocessing\nstep. We consider a detailed case-study based on a network for Ireland. The\nhybrid approach scales well and finds solutions that are close to optimal, with\na runtime that is two orders-of-magnitude better than the MIP model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 17:06:23 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Cambazard", "Hadrien", ""], ["Mehta", "Deepak", ""], ["O'Sullivan", "Barry", ""], ["Quesada", "Luis", ""], ["Ruffini", "Marco", ""], ["Payne", "David", ""], ["Doyle", "Linda", ""]]}, {"id": "1109.1276", "submitter": "Rohan  Agrawal", "authors": "Rohan Agrawal", "title": "Application of the Modified 2-opt and Jumping Gene Operators in\n  Multi-Objective Genetic Algorithm to solve MOTSP", "comments": "4 pages, 5 figures Selected in ICNCI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Multi-Objective Optimization is becoming a hot research area and\nquite a few papers regarding these algorithms have been published. However the\nrole of local search techniques has not been expanded adequately. This paper\nstudies the role of a local search technique called 2-opt for the\nMulti-Objective Travelling Salesman Problem (MOTSP). A new mutation operator\ncalled Jumping Gene (JG) is also used. Since 2-opt operator was intended for\nthe single objective TSP, its domain has been expanded to MOTSP in this paper.\nThis new technique is applied to the list of KroAB100 cities.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 19:42:28 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Agrawal", "Rohan", ""]]}, {"id": "1109.1314", "submitter": "Tom Schaul", "authors": "Tom Schaul, Julian Togelius, J\\\"urgen Schmidhuber", "title": "Measuring Intelligence through Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial general intelligence (AGI) refers to research aimed at tackling\nthe full problem of artificial intelligence, that is, create truly intelligent\nagents. This sets it apart from most AI research which aims at solving\nrelatively narrow domains, such as character recognition, motion planning, or\nincreasing player satisfaction in games. But how do we know when an agent is\ntruly intelligent? A common point of reference in the AGI community is Legg and\nHutter's formal definition of universal intelligence, which has the appeal of\nsimplicity and generality but is unfortunately incomputable. Games of various\nkinds are commonly used as benchmarks for \"narrow\" AI research, as they are\nconsidered to have many important properties. We argue that many of these\nproperties carry over to the testing of general intelligence as well. We then\nsketch how such testing could practically be carried out. The central part of\nthis sketch is an extension of universal intelligence to deal with finite time,\nand the use of sampling of the space of games expressed in a suitably biased\ngame description language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 22:13:30 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Schaul", "Tom", ""], ["Togelius", "Julian", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1109.1317", "submitter": "Pashootan Vaezipoor", "authors": "Pashootan Vaezipoor and David Mitchell and Maarten Mari\\\"en", "title": "Lifted Unit Propagation for Effective Grounding", "comments": "Appears in the Proceedings of the 19th International Conference on\n  Applications of Declarative Programming and Knowledge Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grounding of a formula $\\phi$ over a given finite domain is a ground\nformula which is equivalent to $\\phi$ on that domain. Very effective\npropositional solvers have made grounding-based methods for problem solving\nincreasingly important, however for realistic problem domains and instances,\nthe size of groundings is often problematic. A key technique in ground (e.g.,\nSAT) solvers is unit propagation, which often significantly reduces ground\nformula size even before search begins. We define a \"lifted\" version of unit\npropagation which may be carried out prior to grounding, and describe\nintegration of the resulting technique into grounding algorithms. We describe\nan implementation of the method in a bottom-up grounder, and an experimental\nstudy of its performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 22:35:48 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Vaezipoor", "Pashootan", ""], ["Mitchell", "David", ""], ["Mari\u00ebn", "Maarten", ""]]}, {"id": "1109.1498", "submitter": "E. Di Sciascio", "authors": "E. Di Sciascio, F. M. Donini, M. Mongiello", "title": "Structured Knowledge Representation for Image Retrieval", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  209-257, 2002", "doi": "10.1613/jair.902", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structured approach to the problem of retrieval of images by\ncontent and present a description logic that has been devised for the semantic\nindexing and retrieval of images containing complex objects. As other\napproaches do, we start from low-level features extracted with image analysis\nto detect and characterize regions in an image. However, in contrast with\nfeature-based approaches, we provide a syntax to describe segmented regions as\nbasic objects and complex objects as compositions of basic ones. Then we\nintroduce a companion extensional semantics for defining reasoning services,\nsuch as retrieval, classification, and subsumption. These services can be used\nfor both exact and approximate matching, using similarity measures. Using our\nlogical approach as a formal specification, we implemented a complete\nclient-server image retrieval system, which allows a user to pose both queries\nby sketch and queries by example. A set of experiments has been carried out on\na testbed of images to assess the retrieval capabilities of the system in\ncomparison with expert users ranking. Results are presented adopting a\nwell-established measure of quality borrowed from textual information\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 17:45:48 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Di Sciascio", "E.", ""], ["Donini", "F. M.", ""], ["Mongiello", "M.", ""]]}, {"id": "1109.1525", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "Conceptual Knowledge Markup Language: The central core", "comments": "Presented at the Twelfth Workshop on Knowledge Acquisition, Modeling\n  and Management (KAW'99), 1999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conceptual knowledge framework OML/CKML needs several components for a\nsuccessful design. One important, but previously overlooked, component is the\ncentral core of OML/CKML. The central core provides a theoretical link between\nthe ontological specification in OML and the conceptual knowledge\nrepresentation in CKML. This paper discusses the formal semantics and syntactic\nstyles of the central core, and also the important role it plays in defining\ninteroperability between OML/CKML, RDF/S and Ontolingua.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 18:09:49 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1109.1724", "submitter": "Jinwoo Shin", "authors": "Jinwoo Shin", "title": "The Complexity of Approximating a Bethe Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper resolves a common complexity issue in the Bethe approximation of\nstatistical physics and the Belief Propagation (BP) algorithm of artificial\nintelligence. The Bethe approximation and the BP algorithm are heuristic\nmethods for estimating the partition function and marginal probabilities in\ngraphical models, respectively. The computational complexity of the Bethe\napproximation is decided by the number of operations required to solve a set of\nnon-linear equations, the so-called Bethe equation. Although the BP algorithm\nwas inspired and developed independently, Yedidia, Freeman and Weiss (2004)\nshowed that the BP algorithm solves the Bethe equation if it converges\n(however, it often does not). This naturally motivates the following question\nto understand limitations and empirical successes of the Bethe and BP methods:\nis the Bethe equation computationally easy to solve? We present a\nmessage-passing algorithm solving the Bethe equation in a polynomial number of\noperations for general binary graphical models of n variables where the maximum\ndegree in the underlying graph is O(log n). Our algorithm can be used as an\nalternative to BP fixing its convergence issue and is the first fully\npolynomial-time approximation scheme for the BP fixed-point computation in such\na large class of graphical models, while the approximate fixed-point\ncomputation is known to be (PPAD-)hard in general. We believe that our\ntechnique is of broader interest to understand the computational complexity of\nthe cavity method in statistical physics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 14:08:01 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 05:33:58 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2011 00:15:17 GMT"}, {"version": "v4", "created": "Thu, 21 Mar 2013 01:29:44 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Shin", "Jinwoo", ""]]}, {"id": "1109.1754", "submitter": "Denis Mau\\'a", "authors": "Denis Deratani Mau\\'a, Cassio Polpo de Campos, Marco Zaffalon", "title": "Solving Limited Memory Influence Diagrams", "comments": "43 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for exactly solving decision making problems\nrepresented as influence diagrams. We do not require the usual assumptions of\nno forgetting and regularity; this allows us to solve problems with\nsimultaneous decisions and limited information. The algorithm is empirically\nshown to outperform a state-of-the-art algorithm on randomly generated problems\nof up to 150 variables and $10^{64}$ solutions. We show that the problem is\nNP-hard even if the underlying graph structure of the problem has small\ntreewidth and the variables take on a bounded number of states, but that a\nfully polynomial time approximation scheme exists for these cases. Moreover, we\nshow that the bound on the number of states is a necessary condition for any\nefficient approximation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 16:17:07 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 10:30:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mau\u00e1", "Denis Deratani", ""], ["de Campos", "Cassio Polpo", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1109.1774", "submitter": "Ozgur Akgun", "authors": "Ozgur Akgun, Alan M. Frisch, Brahim Hnich, Chris Jefferson, Ian Miguel", "title": "Conjure Revisited: Towards Automated Constraint Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the constraint modelling process is one of the key challenges\nfacing the constraints field, and one of the principal obstacles preventing\nwidespread adoption of constraint solving. This paper focuses on the\nrefinement-based approach to automated modelling, where a user specifies a\nproblem in an abstract constraint specification language and it is then\nautomatically refined into a constraint model. In particular, we revisit the\nConjure system that first appeared in prototype form in 2005 and present a new\nimplementation with a much greater coverage of the specification language\nEssence.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 17:09:00 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Akgun", "Ozgur", ""], ["Frisch", "Alan M.", ""], ["Hnich", "Brahim", ""], ["Jefferson", "Chris", ""], ["Miguel", "Ian", ""]]}, {"id": "1109.1841", "submitter": "Robert Kent", "authors": "Robert E. Kent and C. Mic Bowman", "title": "Digital Libraries, Conceptual Knowledge Systems, and the Nebula\n  Interface", "comments": "Technical report, Transarc Corporation, Pittsburgh, Pennsylvania,\n  April 1995", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept Analysis provides a principled approach to effective management of\nwide area information systems, such as the Nebula File System and Interface.\nThis not only offers evidence to support the assertion that a digital library\nis a bounded collection of incommensurate information sources in a logical\nspace, but also sheds light on techniques for collaboration through coordinated\naccess to the shared organization of knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 20:35:02 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Kent", "Robert E.", ""], ["Bowman", "C. Mic", ""]]}, {"id": "1109.1922", "submitter": "Markus Wagner", "authors": "Katya Vladislavleva, Tobias Friedrich, Frank Neumann, Markus Wagner", "title": "Predicting the Energy Output of Wind Farms Based on Weather Data:\n  Important Variables and their Correlation", "comments": "13 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind energy plays an increasing role in the supply of energy world-wide. The\nenergy output of a wind farm is highly dependent on the weather condition\npresent at the wind farm. If the output can be predicted more accurately,\nenergy suppliers can coordinate the collaborative production of different\nenergy sources more efficiently to avoid costly overproductions.\n  With this paper, we take a computer science perspective on energy prediction\nbased on weather data and analyze the important parameters as well as their\ncorrelation on the energy output. To deal with the interaction of the different\nparameters we use symbolic regression based on the genetic programming tool\nDataModeler.\n  Our studies are carried out on publicly available weather and energy data for\na wind farm in Australia. We reveal the correlation of the different variables\nfor the energy output. The model obtained for energy prediction gives a very\nreliable prediction of the energy output for newly given weather data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 07:38:59 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Vladislavleva", "Katya", ""], ["Friedrich", "Tobias", ""], ["Neumann", "Frank", ""], ["Wagner", "Markus", ""]]}, {"id": "1109.1966", "submitter": "Timothy Hunter", "authors": "Timothy Hunter, Pieter Abbeel, and Alexandre Bayen", "title": "The path inference filter: model-based low-latency map matching of probe\n  vehicle data", "comments": "Preprint, 23 pages and 23 figures", "journal-ref": null, "doi": "10.1016/j.trb.2013.03.008", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing vehicle trajectories from sparse\nsequences of GPS points, for which the sampling interval is between 10 seconds\nand 2 minutes. We introduce a new class of algorithms, called altogether path\ninference filter (PIF), that maps GPS data in real time, for a variety of\ntrade-offs and scenarios, and with a high throughput. Numerous prior approaches\nin map-matching can be shown to be special cases of the path inference filter\npresented in this article. We present an efficient procedure for automatically\ntraining the filter on new data, with or without ground truth observations. The\nframework is evaluated on a large San Francisco taxi dataset and is shown to\nimprove upon the current state of the art. This filter also provides insights\nabout driving patterns of drivers. The path inference filter has been deployed\nat an industrial scale inside the Mobile Millennium traffic information system,\nand is used to map fleets of data in San Francisco, Sacramento, Stockholm and\nPorto.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 11:12:35 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2012 17:12:40 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hunter", "Timothy", ""], ["Abbeel", "Pieter", ""], ["Bayen", "Alexandre", ""]]}, {"id": "1109.2048", "submitter": "G. Barish", "authors": "G. Barish, C. A. Knoblock", "title": "An Expressive Language and Efficient Execution System for Software\n  Agents", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  625-666, 2005", "doi": "10.1613/jair.1548", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software agents can be used to automate many of the tedious, time-consuming\ninformation processing tasks that humans currently have to complete manually.\nHowever, to do so, agent plans must be capable of representing the myriad of\nactions and control flows required to perform those tasks. In addition, since\nthese tasks can require integrating multiple sources of remote information ?\ntypically, a slow, I/O-bound process ? it is desirable to make execution as\nefficient as possible. To address both of these needs, we present a flexible\nsoftware agent plan language and a highly parallel execution system that enable\nthe efficient execution of expressive agent plans. The plan language allows\ncomplex tasks to be more easily expressed by providing a variety of operators\nfor flexibly processing the data as well as supporting subplans (for\nmodularity) and recursion (for indeterminate looping). The executor is based on\na streaming dataflow model of execution to maximize the amount of operator and\ndata parallelism possible at runtime. We have implemented both the language and\nexecutor in a system called THESEUS. Our results from testing THESEUS show that\nstreaming dataflow execution can yield significant speedups over both\ntraditional serial (von Neumann) as well as non-streaming dataflow-style\nexecution that existing software and robot agent execution systems currently\nsupport. In addition, we show how plans written in the language we present can\nrepresent certain types of subtasks that cannot be accomplished using the\nlanguages supported by network query engines. Finally, we demonstrate that the\nincreased expressivity of our plan language does not hamper performance;\nspecifically, we show how data can be integrated from multiple remote sources\njust as efficiently using our architecture as is possible with a\nstate-of-the-art streaming-dataflow network query engine.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 15:57:43 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Barish", "G.", ""], ["Knoblock", "C. A.", ""]]}, {"id": "1109.2049", "submitter": "Matti J\\\"arvisalo", "authors": "Anton Belov and Matti J\\\"arvisalo", "title": "Structure-Based Local Search Heuristics for Circuit-Level Boolean\n  Satisfiability", "comments": "15 pages", "journal-ref": "Presented at 8th International Workshop on Local Search Techniques\n  in Constraint Satisfaction (LSCS 2011)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on improving state-of-the-art in stochastic local search\n(SLS) for solving Boolean satisfiability (SAT) instances arising from\nreal-world industrial SAT application domains. The recently introduced SLS\nmethod CRSat has been shown to noticeably improve on previously suggested SLS\ntechniques in solving such real-world instances by combining\njustification-based local search with limited Boolean constraint propagation on\nthe non-clausal formula representation form of Boolean circuits. In this work,\nwe study possibilities of further improving the performance of CRSat by\nexploiting circuit-level structural knowledge for developing new search\nheuristics for CRSat. To this end, we introduce and experimentally evaluate a\nvariety of search heuristics, many of which are motivated by circuit-level\nheuristics originally developed in completely different contexts, e.g., for\nelectronic design automation applications. To the best of our knowledge, most\nof the heuristics are novel in the context of SLS for SAT and, more generally,\nSLS for constraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 15:58:36 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Belov", "Anton", ""], ["J\u00e4rvisalo", "Matti", ""]]}, {"id": "1109.2127", "submitter": "V. Bayer-Zubek", "authors": "V. Bayer-Zubek, T. G. Dietterich", "title": "Integrating Learning from Examples into the Search for Diagnostic\n  Policies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  263-303, 2005", "doi": "10.1613/jair.1512", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning diagnostic policies from training\nexamples. A diagnostic policy is a complete description of the decision-making\nactions of a diagnostician (i.e., tests followed by a diagnostic decision) for\nall possible combinations of test results. An optimal diagnostic policy is one\nthat minimizes the expected total cost, which is the sum of measurement costs\nand misdiagnosis costs. In most diagnostic settings, there is a tradeoff\nbetween these two kinds of costs. This paper formalizes diagnostic decision\nmaking as a Markov Decision Process (MDP). The paper introduces a new family of\nsystematic search algorithms based on the AO* algorithm to solve this MDP. To\nmake AO* efficient, the paper describes an admissible heuristic that enables\nAO* to prune large parts of the search space. The paper also introduces several\ngreedy algorithms including some improvements over previously-published\nmethods. The paper then addresses the question of learning diagnostic policies\nfrom examples. When the probabilities of diseases and test results are computed\nfrom training data, there is a great danger of overfitting. To reduce\noverfitting, regularizers are integrated into the search algorithms. Finally,\nthe paper compares the proposed methods on five benchmark diagnostic data sets.\nThe studies show that in most cases the systematic search methods produce\nbetter diagnostic policies than the greedy methods. In addition, the studies\nshow that for training sets of realistic size, the systematic search algorithms\nare practical on todays desktop computers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:20:12 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Bayer-Zubek", "V.", ""], ["Dietterich", "T. G.", ""]]}, {"id": "1109.2131", "submitter": "J. Larrosa", "authors": "J. Larrosa, E. Morancho, D. Niso", "title": "On the Practical use of Variable Elimination in Constraint Optimization\n  Problems: 'Still-life' as a Case Study", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  421-440, 2005", "doi": "10.1613/jair.1541", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable elimination is a general technique for constraint processing. It is\noften discarded because of its high space complexity. However, it can be\nextremely useful when combined with other techniques. In this paper we study\nthe applicability of variable elimination to the challenging problem of finding\nstill-lifes. We illustrate several alternatives: variable elimination as a\nstand-alone algorithm, interleaved with search, and as a source of good quality\nlower bounds. We show that these techniques are the best known option both\ntheoretically and empirically. In our experiments we have been able to solve\nthe n=20 instance, which is far beyond reach with alternative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:23:06 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Larrosa", "J.", ""], ["Morancho", "E.", ""], ["Niso", "D.", ""]]}, {"id": "1109.2134", "submitter": "H. E. Dixon", "authors": "H. E. Dixon, M. L. Ginsberg, E. M. Luks, A. J. Parkes", "title": "Generalizing Boolean Satisfiability II: Theory", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  481-534, 2004", "doi": "10.1613/jair.1555", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second of three planned papers describing ZAP, a satisfiability\nengine that substantially generalizes existing tools while retaining the\nperformance characteristics of modern high performance solvers. The fundamental\nidea underlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal is to define a representation in which this structure is apparent and can\neasily be exploited to improve computational performance. This paper presents\nthe theoretical basis for the ideas underlying ZAP, arguing that existing ideas\nin this area exploit a single, recurring structure in that multiple database\naxioms can be obtained by operating on a single axiom using a subgroup of the\ngroup of permutations on the literals in the problem. We argue that the group\nstructure precisely captures the general structure at which earlier approaches\nhinted, and give numerous examples of its use. We go on to extend the\nDavis-Putnam-Logemann-Loveland inference procedure to this broader setting, and\nshow that earlier computational improvements are either subsumed or left intact\nby the new method. The third paper in this series discusses ZAPs implementation\nand presents experimental performance results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:23:53 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Dixon", "H. E.", ""], ["Ginsberg", "M. L.", ""], ["Luks", "E. M.", ""], ["Parkes", "A. J.", ""]]}, {"id": "1109.2135", "submitter": "P. Doshi", "authors": "P. Doshi, P. J. Gmytrasiewicz", "title": "A Framework for Sequential Planning in Multi-Agent Settings", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  49-79, 2005", "doi": "10.1613/jair.1579", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the framework of partially observable Markov decision\nprocesses (POMDPs) to multi-agent settings by incorporating the notion of agent\nmodels into the state space. Agents maintain beliefs over physical states of\nthe environment and over models of other agents, and they use Bayesian updates\nto maintain their beliefs over time. The solutions map belief states to\nactions. Models of other agents may include their belief states and are related\nto agent types considered in games of incomplete information. We express the\nagents autonomy by postulating that their models are not directly manipulable\nor observable by other agents. We show that important properties of POMDPs,\nsuch as convergence of value iteration, the rate of convergence, and piece-wise\nlinearity and convexity of the value functions carry over to our framework. Our\napproach complements a more traditional approach to interactive settings which\nuses Nash equilibria as a solution paradigm. We seek to avoid some of the\ndrawbacks of equilibria which may be non-unique and do not capture\noff-equilibrium behaviors. We do so at the cost of having to represent, process\nand continuously revise models of other agents. Since the agents beliefs may be\narbitrarily nested, the optimal solutions to decision making problems are only\nasymptotically computable. However, approximate belief updates and\napproximately optimal plans are computable. We illustrate our framework using a\nsimple application domain, and we show examples of belief updates and value\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:24:38 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Doshi", "P.", ""], ["Gmytrasiewicz", "P. J.", ""]]}, {"id": "1109.2137", "submitter": "P. Domingos", "authors": "P. Domingos, S. Sanghai, D. Weld", "title": "Relational Dynamic Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  759-797, 2005", "doi": "10.1613/jair.1625", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic processes that involve the creation of objects and relations over\ntime are widespread, but relatively poorly studied. For example, accurate fault\ndiagnosis in factory assembly processes requires inferring the probabilities of\nerroneous assembly operations, but doing this efficiently and accurately is\ndifficult. Modeled as dynamic Bayesian networks, these processes have discrete\nvariables with very large domains and extremely high dimensionality. In this\npaper, we introduce relational dynamic Bayesian networks (RDBNs), which are an\nextension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a\ngeneralization of dynamic probabilistic relational models (DPRMs), which we had\nproposed in our previous work to model dynamic uncertain domains. We first\nextend the Rao-Blackwellised particle filtering described in our earlier work\nto RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in\nRDBNs and propose two new forms of particle filtering. The first one uses\nabstraction hierarchies over the predicates to smooth the particle filters\nestimates. The second employs kernel density estimation with a kernel function\nspecifically designed for relational domains. Experiments show these two\nmethods greatly outperform standard particle filtering on the task of assembly\nplan execution monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:29:06 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Domingos", "P.", ""], ["Sanghai", "S.", ""], ["Weld", "D.", ""]]}, {"id": "1109.2138", "submitter": "N. Y. Foo", "authors": "N. Y. Foo, Q. B. Vo", "title": "Reasoning about Action: An Argumentation - Theoretic Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  465-518, 2005", "doi": "10.1613/jair.1602", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a uniform non-monotonic solution to the problems of reasoning\nabout action on the basis of an argumentation-theoretic approach. Our theory is\nprovably correct relative to a sensible minimisation policy introduced on top\nof a temporal propositional logic. Sophisticated problem domains can be\nformalised in our framework. As much attention of researchers in the field has\nbeen paid to the traditional and basic problems in reasoning about actions such\nas the frame, the qualification and the ramification problems, approaches to\nthese problems within our formalisation lie at heart of the expositions\npresented in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:29:24 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Foo", "N. Y.", ""], ["Vo", "Q. B.", ""]]}, {"id": "1109.2139", "submitter": "P. J. Hawkins", "authors": "P. J. Hawkins, V. Lagoon, P. J. Stuckey", "title": "Solving Set Constraint Satisfaction Problems using ROBDDs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  109-156, 2005", "doi": "10.1613/jair.1638", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to modeling finite set domain\nconstraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We\nshow that it is possible to construct an efficient set domain propagator which\ncompactly represents many set domains and set constraints using ROBDDs. We\ndemonstrate that the ROBDD-based approach provides unprecedented flexibility in\nmodeling constraint satisfaction problems, leading to performance improvements.\nWe also show that the ROBDD-based modeling approach can be extended to the\nmodeling of integer and multiset constraint problems in a straightforward\nmanner. Since domain propagation is not always practical, we also show how to\nincorporate less strict consistency notions into the ROBDD framework, such as\nset bounds, cardinality bounds and lexicographic bounds consistency. Finally,\nwe present experimental results that demonstrate the ROBDD-based solver\nperforms better than various more conventional constraint solvers on several\nstandard set constraint problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:30:13 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Hawkins", "P. J.", ""], ["Lagoon", "V.", ""], ["Stuckey", "P. J.", ""]]}, {"id": "1109.2140", "submitter": "P. Cimiano", "authors": "P. Cimiano, A. Hotho, S. Staab", "title": "Learning Concept Hierarchies from Text Corpora using Formal Concept\n  Analysis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  305-339, 2005", "doi": "10.1613/jair.1648", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the automatic acquisition of taxonomies or\nconcept hierarchies from a text corpus. The approach is based on Formal Concept\nAnalysis (FCA), a method mainly used for the analysis of data, i.e. for\ninvestigating and processing explicitly given information. We follow Harris\ndistributional hypothesis and model the context of a certain term as a vector\nrepresenting syntactic dependencies which are automatically acquired from the\ntext corpus with a linguistic parser. On the basis of this context information,\nFCA produces a lattice that we convert into a special kind of partial order\nconstituting a concept hierarchy. The approach is evaluated by comparing the\nresulting concept hierarchies with hand-crafted taxonomies for two domains:\ntourism and finance. We also directly compare our approach with hierarchical\nagglomerative clustering as well as with Bi-Section-KMeans as an instance of a\ndivisive clustering algorithm. Furthermore, we investigate the impact of using\ndifferent measures weighting the contribution of each attribute as well as of\napplying a particular smoothing technique to cope with data sparseness.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:30:44 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Cimiano", "P.", ""], ["Hotho", "A.", ""], ["Staab", "S.", ""]]}, {"id": "1109.2142", "submitter": "H. E. Dixon", "authors": "H. E. Dixon, M. L. Ginsberg, D. Hofer, E. M. Luks, A. J. Parkes", "title": "Generalizing Boolean Satisfiability III: Implementation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  441-531, 2005", "doi": "10.1613/jair.1656", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the third of three papers describing ZAP, a satisfiability engine\nthat substantially generalizes existing tools while retaining the performance\ncharacteristics of modern high-performance solvers. The fundamental idea\nunderlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal has been to define a representation in which this structure is apparent\nand can be exploited to improve computational performance. The first paper\nsurveyed existing work that (knowingly or not) exploited problem structure to\nimprove the performance of satisfiability engines, and the second paper showed\nthat this structure could be understood in terms of groups of permutations\nacting on individual clauses in any particular Boolean theory. We conclude the\nseries by discussing the techniques needed to implement our ideas, and by\nreporting on their performance on a variety of problem instances.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:31:25 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Dixon", "H. E.", ""], ["Ginsberg", "M. L.", ""], ["Hofer", "D.", ""], ["Luks", "E. M.", ""], ["Parkes", "A. J.", ""]]}, {"id": "1109.2143", "submitter": "M. Jaeger", "authors": "M. Jaeger", "title": "Ignorability in Statistical and Probabilistic Inference", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  889-917, 2005", "doi": "10.1613/jair.1657", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with incomplete data in statistical learning, or incomplete\nobservations in probabilistic inference, one needs to distinguish the fact that\na certain event is observed from the fact that the observed event has happened.\nSince the modeling and computational complexities entailed by maintaining this\nproper distinction are often prohibitive, one asks for conditions under which\nit can be safely ignored. Such conditions are given by the missing at random\n(mar) and coarsened at random (car) assumptions. In this paper we provide an\nin-depth analysis of several questions relating to mar/car assumptions. Main\npurpose of our study is to provide criteria by which one may evaluate whether a\ncar assumption is reasonable for a particular data collecting or observational\nprocess. This question is complicated by the fact that several distinct\nversions of mar/car assumptions exist. We therefore first provide an overview\nover these different versions, in which we highlight the distinction between\ndistributional and coarsening variable induced versions. We show that\ndistributional versions are less restrictive and sufficient for most\napplications. We then address from two different perspectives the question of\nwhen the mar/car assumption is warranted. First we provide a static analysis\nthat characterizes the admissibility of the car assumption in terms of the\nsupport structure of the joint probability distribution of complete data and\nincomplete observations. Here we obtain an equivalence characterization that\nimproves and extends a recent result by Grunwald and Halpern. We then turn to a\nprocedural analysis that characterizes the admissibility of the car assumption\nin terms of procedural models for the actual data (or observation) generating\nprocess. The main result of this analysis is that the stronger coarsened\ncompletely at random (ccar) condition is arguably the most reasonable\nassumption, as it alone corresponds to data coarsening procedures that satisfy\na natural robustness property.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:31:47 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Jaeger", "M.", ""]]}, {"id": "1109.2145", "submitter": "M. T.J. Spaan", "authors": "M. T.J. Spaan, N. Vlassis", "title": "Perseus: Randomized Point-based Value Iteration for POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  195-220, 2005", "doi": "10.1613/jair.1659", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) form an attractive\nand principled framework for agent planning under uncertainty. Point-based\napproximate techniques for POMDPs compute a policy based on a finite set of\npoints collected in advance from the agents belief space. We present a\nrandomized point-based value iteration algorithm called Perseus. The algorithm\nperforms approximate value backup stages, ensuring that in each backup stage\nthe value of each point in the belief set is improved; the key observation is\nthat a single backup may improve the value of many belief points. Contrary to\nother point-based methods, Perseus backs up only a (randomly selected) subset\nof points in the belief set, sufficient for improving the value of each belief\npoint in the set. We show how the same idea can be extended to dealing with\ncontinuous action spaces. Experimental results show the potential of Perseus in\nlarge scale POMDP problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:32:03 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Spaan", "M. T. J.", ""], ["Vlassis", "N.", ""]]}, {"id": "1109.2148", "submitter": "L. De Raedt", "authors": "L. De Raedt, K. Kersting, T. Raiko", "title": "Logical Hidden Markov Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  425-456, 2006", "doi": "10.1613/jair.1675", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov\nmodels to deal with sequences of structured symbols in the form of logical\natoms, rather than flat characters.\n  This note formally introduces LOHMMs and presents solutions to the three\ncentral inference problems for LOHMMs: evaluation, most likely hidden state\nsequence and parameter estimation. The resulting representation and algorithms\nare experimentally evaluated on problems from the domain of bioinformatics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:33:14 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["De Raedt", "L.", ""], ["Kersting", "K.", ""], ["Raiko", "T.", ""]]}, {"id": "1109.2153", "submitter": "B. Bonet", "authors": "B. Bonet, H. Geffner", "title": "mGPT: A Probabilistic Planner Based on Heuristic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  933-944, 2005", "doi": "10.1613/jair.1688", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the version of the GPT planner used in the probabilistic track of\nthe 4th International Planning Competition (IPC-4). This version, called mGPT,\nsolves Markov Decision Processes specified in the PPDDL language by extracting\nand using different classes of lower bounds along with various heuristic-search\nalgorithms. The lower bounds are extracted from deterministic relaxations where\nthe alternative probabilistic effects of an action are mapped into different,\nindependent, deterministic actions. The heuristic-search algorithms use these\nlower bounds for focusing the updates and delivering a consistent value\nfunction over all states reachable from the initial state and the greedy\npolicy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:42:50 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Bonet", "B.", ""], ["Geffner", "H.", ""]]}, {"id": "1109.2154", "submitter": "A. Botea", "authors": "A. Botea, M. Enzenberger, M. Mueller, J. Schaeffer", "title": "Macro-FF: Improving AI Planning with Automatically Learned\n  Macro-Operators", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  581-621, 2005", "doi": "10.1613/jair.1696", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in AI planning, many benchmarks remain challenging\nfor current planners. In many domains, the performance of a planner can greatly\nbe improved by discovering and exploiting information about the domain\nstructure that is not explicitly encoded in the initial PDDL formulation. In\nthis paper we present and compare two automated methods that learn relevant\ninformation from previous experience in a domain and use it to solve new\nproblem instances. Our methods share a common four-step strategy. First, a\ndomain is analyzed and structural information is extracted, then\nmacro-operators are generated based on the previously discovered structure. A\nfiltering and ranking procedure selects the most useful macro-operators.\nFinally, the selected macros are used to speed up future searches. We have\nsuccessfully used such an approach in the fourth international planning\ncompetition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art\nplanner FF 2.3 with support for two kinds of macro-operators, and with\nengineering enhancements. We demonstrate the effectiveness of our ideas on\nbenchmarks from international planning competitions. Our results indicate a\nlarge reduction in search effort in those complex domains where structural\ninformation can be inferred.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:43:12 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Botea", "A.", ""], ["Enzenberger", "M.", ""], ["Mueller", "M.", ""], ["Schaeffer", "J.", ""]]}, {"id": "1109.2155", "submitter": "S. Kambhampati", "authors": "S. Kambhampati, M.H.L. van den Briel", "title": "Optiplan: Unifying IP-based and Graph-based Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  919-931, 2005", "doi": "10.1613/jair.1698", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Optiplan planning system is the first integer programming-based planner\nthat successfully participated in the international planning competition. This\nengineering note describes the architecture of Optiplan and provides the\ninteger programming formulation that enabled it to perform reasonably well in\nthe competition. We also touch upon some recent developments that make integer\nprogramming encodings significantly more competitive.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:43:37 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Kambhampati", "S.", ""], ["Briel", "M. H. L. van den", ""]]}, {"id": "1109.2156", "submitter": "A. Fern", "authors": "A. Fern, R. Givan, S. Yoon", "title": "Approximate Policy Iteration with a Policy Language Bias: Solving\n  Relational Markov Decision Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  75-118, 2006", "doi": "10.1613/jair.1700", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an approach to policy selection for large relational Markov Decision\nProcesses (MDPs). We consider a variant of approximate policy iteration (API)\nthat replaces the usual value-function learning step with a learning step in\npolicy space. This is advantageous in domains where good policies are easier to\nrepresent and learn than the corresponding value functions, which is often the\ncase for the relational MDPs we are interested in. In order to apply API to\nsuch problems, we introduce a relational policy language and corresponding\nlearner. In addition, we introduce a new bootstrapping routine for goal-based\nplanning domains, based on random walks. Such bootstrapping is necessary for\nmany large relational MDPs, where reward is extremely sparse, as API is\nineffective in such domains when initialized with an uninformed policy. Our\nexperiments show that the resulting system is able to find good policies for a\nnumber of classical planning domains and their stochastic variants by solving\nthem as extremely large relational MDPs. The experiments also point to some\nlimitations of our approach, suggesting future work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:43:53 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Fern", "A.", ""], ["Givan", "R.", ""], ["Yoon", "S.", ""]]}, {"id": "1109.2271", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Zhao Zheng, Qiuxia Lu, Weinan Zhang, Yong Yu", "title": "Feature-Based Matrix Factorization", "comments": "Minor update, add some related works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system has been more and more popular and widely used in many\napplications recently. The increasing information available, not only in\nquantities but also in types, leads to a big challenge for recommender system\nthat how to leverage these rich information to get a better performance. Most\ntraditional approaches try to design a specific model for each scenario, which\ndemands great efforts in developing and modifying models. In this technical\nreport, we describe our implementation of feature-based matrix factorization.\nThis model is an abstract of many variants of matrix factorization models, and\nnew types of information can be utilized by simply defining new features,\nwithout modifying any lines of code. Using the toolkit, we built the best\nsingle model reported on track 1 of KDDCup'11.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 01:10:06 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2011 07:32:03 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2011 02:05:01 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Chen", "Tianqi", ""], ["Zheng", "Zhao", ""], ["Lu", "Qiuxia", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1109.2346", "submitter": "A. E. Howe", "authors": "A. E. Howe, J. P. Watson, L. D. Whitley", "title": "Linking Search Space Structure, Run-Time Dynamics, and Problem\n  Difficulty: A Step Toward Demystifying Tabu Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  221-261, 2005", "doi": "10.1613/jair.1576", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabu search is one of the most effective heuristics for locating high-quality\nsolutions to a diverse array of NP-hard combinatorial optimization problems.\nDespite the widespread success of tabu search, researchers have a poor\nunderstanding of many key theoretical aspects of this algorithm, including\nmodels of the high-level run-time dynamics and identification of those search\nspace features that influence problem difficulty. We consider these questions\nin the context of the job-shop scheduling problem (JSP), a domain where tabu\nsearch algorithms have been shown to be remarkably effective. Previously, we\ndemonstrated that the mean distance between random local optima and the nearest\noptimal solution is highly correlated with problem difficulty for a well-known\ntabu search algorithm for the JSP introduced by Taillard. In this paper, we\ndiscuss various shortcomings of this measure and develop a new model of problem\ndifficulty that corrects these deficiencies. We show that Taillards algorithm\ncan be modeled with high fidelity as a simple variant of a straightforward\nrandom walk. The random walk model accounts for nearly all of the variability\nin the cost required to locate both optimal and sub-optimal solutions to random\nJSPs, and provides an explanation for differences in the difficulty of random\nversus structured JSPs. Finally, we discuss and empirically substantiate two\nnovel predictions regarding tabu search algorithm behavior. First, the method\nfor constructing the initial solution is highly unlikely to impact the\nperformance of tabu search. Second, tabu tenure should be selected to be as\nsmall as possible while simultaneously avoiding search stagnation; values\nlarger than necessary lead to significant degradations in performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 20:09:12 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Howe", "A. E.", ""], ["Watson", "J. P.", ""], ["Whitley", "L. D.", ""]]}, {"id": "1109.2347", "submitter": "F. A. Aloul", "authors": "F. A. Aloul, I. L. Markov, A. Ramani, K. A. Sakallah", "title": "Breaking Instance-Independent Symmetries In Exact Graph Coloring", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  289-322, 2006", "doi": "10.1613/jair.1637", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code optimization and high level synthesis can be posed as constraint\nsatisfaction and optimization problems, such as graph coloring used in register\nallocation. Graph coloring is also used to model more traditional CSPs relevant\nto AI, such as planning, time-tabling and scheduling. Provably optimal\nsolutions may be desirable for commercial and defense applications.\nAdditionally, for applications such as register allocation and code\noptimization, naturally-occurring instances of graph coloring are often small\nand can be solved optimally. A recent wave of improvements in algorithms for\nBoolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests\ngeneric problem-reduction methods, rather than problem-specific heuristics,\nbecause (1) heuristics may be upset by new constraints, (2) heuristics tend to\nignore structure, and (3) many relevant problems are provably inapproximable.\n  Problem reductions often lead to highly symmetric SAT instances, and\nsymmetries are known to slow down SAT solvers. In this work, we compare several\navenues for symmetry breaking, in particular when certain kinds of symmetry are\npresent in all generated instances. Our focus on reducing CSPs to SAT allows us\nto leverage recent dramatic improvement in SAT solvers and automatically\nbenefit from future progress. We can use a variety of black-box SAT solvers\nwithout modifying their source code because our symmetry-breaking techniques\nare static, i.e., we detect symmetries and add symmetry breaking predicates\n(SBPs) during pre-processing.\n  An important result of our work is that among the types of\ninstance-independent SBPs we studied and their combinations, the simplest and\nleast complete constructions are the most effective. Our experiments also\nclearly indicate that instance-independent symmetries should mostly be\nprocessed together with instance-specific symmetries rather than at the\nspecification level, contrary to what has been suggested in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 20:09:48 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Aloul", "F. A.", ""], ["Markov", "I. L.", ""], ["Ramani", "A.", ""], ["Sakallah", "K. A.", ""]]}, {"id": "1109.2355", "submitter": "C. Gretton", "authors": "C. Gretton, F. Kabanza, D. Price, J. Slaney, S. Thiebaux", "title": "Decision-Theoretic Planning with non-Markovian Rewards", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  17-74, 2006", "doi": "10.1613/jair.1676", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decision process in which rewards depend on history rather than merely on\nthe current state is called a decision process with non-Markovian rewards\n(NMRDP). In decision-theoretic planning, where many desirable behaviours are\nmore naturally expressed as properties of execution sequences rather than as\nproperties of states, NMRDPs form a more natural model than the commonly\nadopted fully Markovian decision process (MDP) model. While the more tractable\nsolution methods developed for MDPs do not directly apply in the presence of\nnon-Markovian rewards, a number of solution methods for NMRDPs have been\nproposed in the literature. These all exploit a compact specification of the\nnon-Markovian reward function in temporal logic, to automatically translate the\nNMRDP into an equivalent MDP which is solved using efficient MDP solution\nmethods. This paper presents NMRDPP (Non-Markovian Reward Decision Process\nPlanner), a software platform for the development and experimentation of\nmethods for decision-theoretic planning with non-Markovian rewards. The current\nversion of NMRDPP implements, under a single interface, a family of methods\nbased on existing as well as new approaches which we describe in detail. These\ninclude dynamic programming, heuristic search, and structured methods. Using\nNMRDPP, we compare the methods and identify certain problem features that\naffect their performance. NMRDPPs treatment of non-Markovian rewards is\ninspired by the treatment of domain-specific search control knowledge in the\nTLPlan planner, which it incorporates as a special case. In the First\nInternational Probabilistic Planning Competition, NMRDPP was able to compete\nand perform well in both the domain-independent and hand-coded tracks, using\nsearch control knowledge in the latter.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 21:39:21 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Gretton", "C.", ""], ["Kabanza", "F.", ""], ["Price", "D.", ""], ["Slaney", "J.", ""], ["Thiebaux", "S.", ""]]}, {"id": "1109.2752", "submitter": "Ant\\'onio Jos\\'e dos Reis Morgado", "authors": "Antonio Morgado and Joao Marques-Silva", "title": "On Validating Boolean Optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean optimization finds a wide range of application domains, that\nmotivated a number of different organizations of Boolean optimizers since the\nmid 90s. Some of the most successful approaches are based on iterative calls to\nan NP oracle, using either linear search, binary search or the identification\nof unsatisfiable sub-formulas. The increasing use of Boolean optimizers in\npractical settings raises the question of confidence in computed results. For\nexample, the issue of confidence is paramount in safety critical settings. One\nway of increasing the confidence of the results computed by Boolean optimizers\nis to develop techniques for validating the results. Recent work studied the\nvalidation of Boolean optimizers based on branch-and-bound search. This paper\ncomplements existing work, and develops methods for validating Boolean\noptimizers that are based on iterative calls to an NP oracle. This entails\nimplementing solutions for validating both satisfiable and unsatisfiable\nanswers from the NP oracle. The work described in this paper can be applied to\na wide range of Boolean optimizers, that find application in Pseudo-Boolean\nOptimization and in Maximum Satisfiability. Preliminary experimental results\nindicate that the impact of the proposed method in overall performance is\nnegligible.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2011 11:48:32 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Morgado", "Antonio", ""], ["Marques-Silva", "Joao", ""]]}, {"id": "1109.3094", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger and Marc Sevaux", "title": "On the use of reference points for the biobjective Inventory Routing\n  Problem", "comments": null, "journal-ref": "Proceedings of the 9th Metaheuristics International Conference MIC\n  2011, July 25-28, 2011, Udine, Italy, Pages 141-149", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a study on the biobjective inventory routing problem.\nContrary to most previous research, the problem is treated as a true\nmulti-objective optimization problem, with the goal of identifying\nPareto-optimal solutions. Due to the hardness of the problem at hand, a\nreference point based optimization approach is presented and implemented into\nan optimization and decision support system, which allows for the computation\nof a true subset of the optimal outcomes. Experimental investigation involving\nlocal search metaheuristics are conducted on benchmark data, and numerical\nresults are reported and analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 14:36:41 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Sevaux", "Marc", ""]]}, {"id": "1109.3313", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger, Marc Sevaux, Stefan Voss", "title": "Neigborhood Selection in Variable Neighborhood Search", "comments": "ISBN 978-88-900984-3-7", "journal-ref": "Proceedings of the 9th Metaheuristics International Conference MIC\n  2011, July 25-28, 2011, Udine, Italy, Pages 571-573", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable neighborhood search (VNS) is a metaheuristic for solving\noptimization problems based on a simple principle: systematic changes of\nneighborhoods within the search, both in the descent to local minima and in the\nescape from the valleys which contain them. Designing these neighborhoods and\napplying them in a meaningful fashion is not an easy task. Moreover, an\nappropriate order in which they are applied must be determined. In this paper\nwe attempt to investigate this issue. Assume that we are given an optimization\nproblem that is intended to be solved by applying the VNS scheme, how many and\nwhich types of neighborhoods should be investigated and what could be\nappropriate selection criteria to apply these neighborhoods. More specifically,\ndoes it pay to \"look ahead\" (see, e.g., in the context of VNS and GRASP) when\nattempting to switch from one neighborhood to another?\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 10:53:32 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Sevaux", "Marc", ""], ["Voss", "Stefan", ""]]}, {"id": "1109.3532", "submitter": "Misha Denil", "authors": "Misha Denil and Thomas Trappenberg", "title": "A Characterization of the Combined Effects of Overlap and Imbalance on\n  the SVM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate that two common problems in Machine\nLearning---imbalanced and overlapping data distributions---do not have\nindependent effects on the performance of SVM classifiers. This result is\nnotable since it shows that a model of either of these factors must account for\nthe presence of the other. Our study of the relationship between these problems\nhas lead to the discovery of a previously unreported form of \"covert\"\noverfitting which is resilient to commonly used empirical regularization\ntechniques. We demonstrate the existance of this covert phenomenon through\nseveral methods based around the parametric regularization of trained SVMs. Our\nfindings in this area suggest a possible approach to quantifying overlap in\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 06:46:39 GMT"}], "update_date": "2011-09-19", "authors_parsed": [["Denil", "Misha", ""], ["Trappenberg", "Thomas", ""]]}, {"id": "1109.3650", "submitter": "Rohan  Agrawal", "authors": "Rohan Agrawal", "title": "Bi-Objective Community Detection (BOCD) in Networks using Genetic\n  Algorithm", "comments": "11 pages, 3 Figures, 3 Tables. arXiv admin note: substantial text\n  overlap with arXiv:0906.0612", "journal-ref": null, "doi": "10.1007/978-3-642-22606-9_5", "report-no": null, "categories": "cs.SI cs.AI cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of research effort has been put into community detection from all\ncorners of academic interest such as physics, mathematics and computer science.\nIn this paper I have proposed a Bi-Objective Genetic Algorithm for community\ndetection which maximizes modularity and community score. Then the results\nobtained for both benchmark and real life data sets are compared with other\nalgorithms using the modularity and MNI performance metrics. The results show\nthat the BOCD algorithm is capable of successfully detecting community\nstructure in both real life and synthetic datasets, as well as improving upon\nthe performance of previous techniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 15:48:29 GMT"}], "update_date": "2011-09-19", "authors_parsed": [["Agrawal", "Rohan", ""]]}, {"id": "1109.3700", "submitter": "Arnaud Martin", "authors": "Florentin Smarandache (UNM), Arnaud Martin (IRISA), Christophe Osswald\n  (E3I2)", "title": "Contradiction measures and specificity degrees of basic belief\n  assignments", "comments": null, "journal-ref": "International Conference on Information Fusion, Chicago : United\n  States (2011)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the theory of belief functions, many measures of uncertainty have been\nintroduced. However, it is not always easy to understand what these measures\nreally try to represent. In this paper, we re-interpret some measures of\nuncertainty in the theory of belief functions. We present some interests and\ndrawbacks of the existing measures. On these observations, we introduce a\nmeasure of contradiction. Therefore, we present some degrees of non-specificity\nand Bayesianity of a mass. We propose a degree of specificity based on the\ndistance between a mass and its most specific associated mass. We also show how\nto use the degree of specificity to measure the specificity of a fusion rule.\nIllustrations on simple examples are given.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 19:34:47 GMT"}], "update_date": "2011-09-19", "authors_parsed": [["Smarandache", "Florentin", "", "UNM"], ["Martin", "Arnaud", "", "IRISA"], ["Osswald", "Christophe", "", "E3I2"]]}, {"id": "1109.3737", "submitter": "Misha Denil", "authors": "Misha Denil, Loris Bazzani, Hugo Larochelle and Nando de Freitas", "title": "Learning where to Attend with Deep Architectures for Image Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an attentional model for simultaneous object tracking and\nrecognition that is driven by gaze data. Motivated by theories of perception,\nthe model consists of two interacting pathways: identity and control, intended\nto mirror the what and where pathways in neuroscience models. The identity\npathway models object appearance and performs classification using deep\n(factored)-Restricted Boltzmann Machines. At each point in time the\nobservations consist of foveated images, with decaying resolution toward the\nperiphery of the gaze. The control pathway models the location, orientation,\nscale and speed of the attended object. The posterior distribution of these\nstates is estimated with particle filtering. Deeper in the control pathway, we\nencounter an attentional mechanism that learns to select gazes so as to\nminimize tracking uncertainty. Unlike in our previous work, we introduce gaze\nselection strategies which operate in the presence of partial information and\non a continuous action space. We show that a straightforward extension of the\nexisting approach to the partial information setting results in poor\nperformance, and we propose an alternative method based on modeling the reward\nsurface as a Gaussian Process. This approach gives good performance in the\npresence of partial information and allows us to expand the action space from a\nsmall, discrete set of fixation points to a continuous domain.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 22:32:51 GMT"}], "update_date": "2011-09-20", "authors_parsed": [["Denil", "Misha", ""], ["Bazzani", "Loris", ""], ["Larochelle", "Hugo", ""], ["de Freitas", "Nando", ""]]}, {"id": "1109.3940", "submitter": "Yuan Shi", "authors": "Yuan Shi, Yung-Kyun Noh, Fei Sha, Daniel D. Lee", "title": "Learning Discriminative Metrics via Generative Models and Kernel\n  Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics specifying distances between data points can be learned in a\ndiscriminative manner or from generative models. In this paper, we show how to\nunify generative and discriminative learning of metrics via a kernel learning\nframework. Specifically, we learn local metrics optimized from parametric\ngenerative models. These are then used as base kernels to construct a global\nkernel that minimizes a discriminative training criterion. We consider both\nlinear and nonlinear combinations of local metric kernels. Our empirical\nresults show that these combinations significantly improve performance on\nclassification tasks. The proposed learning algorithm is also very efficient,\nachieving order of magnitude speedup in training time compared to previous\ndiscriminative baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 04:19:30 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Shi", "Yuan", ""], ["Noh", "Yung-Kyun", ""], ["Sha", "Fei", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1109.3989", "submitter": "J\\\"org P\\\"uhrer", "authors": "Johannes Oetsch and J\\\"org P\\\"uhrer and Hans Tompits", "title": "The SeaLion has Landed: An IDE for Answer-Set Programming---Preliminary\n  Report", "comments": "Proceedings of the 19th International Conference on Applications of\n  Declarative Programming and Knowledge Management (INAP 2011) and 25th\n  Workshop on Logic Programming (WLP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report about the current state and designated features of the tool\nSeaLion, aimed to serve as an integrated development environment (IDE) for\nanswer-set programming (ASP). A main goal of SeaLion is to provide a\nuser-friendly environment for supporting a developer to write, evaluate, debug,\nand test answer-set programs. To this end, new support techniques have to be\ndeveloped that suit the requirements of the answer-set semantics and meet the\nconstraints of practical applicability. In this respect, SeaLion benefits from\nthe research results of a project on methods and methodologies for answer-set\nprogram development in whose context SeaLion is realised. Currently, the tool\nprovides source-code editors for the languages of Gringo and DLV that offer\nsyntax highlighting, syntax checking, and a visual program outline. Further\nimplemented features are support for external solvers and visualisation as well\nas visual editing of answer sets. SeaLion comes as a plugin of the popular\nEclipse platform and provides itself interfaces for future extensions of the\nIDE.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 10:24:28 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2011 10:03:44 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Oetsch", "Johannes", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Tompits", "Hans", ""]]}, {"id": "1109.4095", "submitter": "J\\\"org P\\\"uhrer", "authors": "Christian Kloim\\\"ullner, Johannes Oetsch, J\\\"org P\\\"uhrer, and Hans\n  Tompits", "title": "Kara: A System for Visualising and Visual Editing of Interpretations for\n  Answer-Set Programs", "comments": "Proceedings of the 19th International Conference on Applications of\n  Declarative Programming and Knowledge Management (INAP 2011) and 25th\n  Workshop on Logic Programming (WLP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In answer-set programming (ASP), the solutions of a problem are encoded in\ndedicated models, called answer sets, of a logical theory. These answer sets\nare computed from the program that represents the theory by means of an ASP\nsolver and returned to the user as sets of ground first-order literals. As this\ntype of representation is often cumbersome for the user to interpret, tools\nlike ASPVIZ and IDPDraw were developed that allow for visualising answer sets.\nThe tool Kara, introduced in this paper, follows these approaches, using ASP\nitself as a language for defining visualisations of interpretations. Unlike\nexisting tools that position graphic primitives according to static coordinates\nonly, Kara allows for more high-level specifications, supporting graph\nstructures, grids, and relative positioning of graphical elements. Moreover,\ngeneralising the functionality of previous tools, Kara provides modifiable\nvisualisations such that interpretations can be manipulated by graphically\nediting their visualisations. This is realised by resorting to abductive\nreasoning techniques. Kara is part of SeaLion, a forthcoming integrated\ndevelopment environment (IDE) for ASP.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 17:09:21 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2011 10:03:57 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Kloim\u00fcllner", "Christian", ""], ["Oetsch", "Johannes", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Tompits", "Hans", ""]]}, {"id": "1109.4335", "submitter": "Xavier Mora", "authors": "Rosa Camps, Xavier Mora, Laia Saumell", "title": "Social choice rules driven by propositional logic", "comments": "The title has been changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several rules for social choice are examined from a unifying point of view\nthat looks at them as procedures for revising a system of degrees of belief in\naccordance with certain specified logical constraints. Belief is here a social\nattribute, its degrees being measured by the fraction of people who share a\ngiven opinion. Different known rules and some new ones are obtained depending\non which particular constraints are assumed. These constraints allow to model\ndifferent notions of choiceness. In particular, we give a new method to deal\nwith approval-disapproval-preferential voting.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 10:44:23 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 13:45:53 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 17:04:10 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Camps", "Rosa", ""], ["Mora", "Xavier", ""], ["Saumell", "Laia", ""]]}, {"id": "1109.4603", "submitter": "Andrew Cotter", "authors": "Andrew Cotter, Joseph Keshet and Nathan Srebro", "title": "Explicit Approximations of the Gaussian Kernel", "comments": "11 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate training and using Gaussian kernel SVMs by approximating the\nkernel with an explicit finite- dimensional polynomial feature representation\nbased on the Taylor expansion of the exponential. Although not as efficient as\nthe recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms\nof the number of features, we show how this polynomial representation can\nprovide a better approximation in terms of the computational cost involved.\nThis makes our \"Taylor features\" especially attractive for use on very large\ndata sets, in conjunction with online or stochastic training.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 18:11:05 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Cotter", "Andrew", ""], ["Keshet", "Joseph", ""], ["Srebro", "Nathan", ""]]}, {"id": "1109.4609", "submitter": "Farnood Merrikh-Bayat", "authors": "Farnood Merrikh-Bayat and Saeed Bagheri Shouraki", "title": "Memristive fuzzy edge detector", "comments": "21 pages, 6 figures, submitted to IET Image Processing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy inference systems always suffer from the lack of efficient structures\nor platforms for their hardware implementation. In this paper, we tried to\novercome this problem by proposing new method for the implementation of those\nfuzzy inference systems which use fuzzy rule base to make inference. To achieve\nthis goal, we have designed a multi-layer neuro-fuzzy computing system based on\nthe memristor crossbar structure by introducing some new concepts like fuzzy\nminterms. Although many applications can be realized through the use of our\nproposed system, in this study we show how the fuzzy XOR function can be\nconstructed and how it can be used to extract edges from grayscale images. Our\nmemristive fuzzy edge detector (implemented in analog form) compared with other\ncommon edge detectors has this advantage that it can extract edges of any given\nimage all at once in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 18:45:03 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Merrikh-Bayat", "Farnood", ""], ["Shouraki", "Saeed Bagheri", ""]]}, {"id": "1109.4623", "submitter": "Fabrizio Angiulli", "authors": "F. Angiulli, R. Ben-Eliyahu-Zohary, L. Palopoli", "title": "Outlier detection in default logics: the tractability/intractability\n  frontier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In default theories, outliers denote sets of literals featuring unexpected\nproperties. In previous papers, we have defined outliers in default logics and\ninvestigated their formal properties. Specifically, we have looked into the\ncomputational complexity of outlier detection problems and proved that while\nthey are generally intractable, interesting tractable cases can be singled out.\nFollowing those results, we study here the tractability frontier in outlier\ndetection problems, by analyzing it with respect to (i) the considered outlier\ndetection problem, (ii) the reference default logic fragment, and (iii) the\nadopted notion of outlier. As for point (i), we shall consider three problems\nof increasing complexity, called Outlier-Witness Recognition, Outlier\nRecognition and Outlier Existence, respectively. As for point (ii), as we look\nfor conditions under which outlier detection can be done efficiently, attention\nwill be limited to subsets of Disjunction-free propositional default theories.\nAs for point (iii), we shall refer to both the notion of outlier of [ABP08] and\na new and more restrictive one, called strong outlier. After complexity\nresults, we present a polynomial time algorithm for enumerating all strong\noutliers of bounded size in an quasi-acyclic normal unary default theory. Some\nof our tractability results rely on the Incremental Lemma that provides\nconditions for a deafult logic fragment to have a monotonic behavior. Finally,\nin order to show that the simple fragments of DL we deal with are still rich\nenough to solve interesting problems and, therefore, the tractability results\nthat we prove are interesting not only on the mere theoretical side, insights\ninto the expressive capabilities of these fragments are provided, by showing\nthat normal unary theories express all NL queries, hereby indirectly answering\na question raised by Kautz and Selman.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 19:54:55 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 10:56:29 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Angiulli", "F.", ""], ["Ben-Eliyahu-Zohary", "R.", ""], ["Palopoli", "L.", ""]]}, {"id": "1109.4684", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Horace H.S. Ip, Yuxin Peng", "title": "Exhaustive and Efficient Constraint Propagation: A Semi-Supervised\n  Learning Perspective and Its Applications", "comments": "The short version of this paper appears as oral paper in ECCV 2010", "journal-ref": "International Journal of Computer Vision (IJCV), 2012", "doi": "10.1007/s11263-012-0602-z", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel pairwise constraint propagation approach by\ndecomposing the challenging constraint propagation problem into a set of\nindependent semi-supervised learning subproblems which can be solved in\nquadratic time using label propagation based on k-nearest neighbor graphs.\nConsidering that this time cost is proportional to the number of all possible\npairwise constraints, our approach actually provides an efficient solution for\nexhaustively propagating pairwise constraints throughout the entire dataset.\nThe resulting exhaustive set of propagated pairwise constraints are further\nused to adjust the similarity matrix for constrained spectral clustering. Other\nthan the traditional constraint propagation on single-source data, our approach\nis also extended to more challenging constraint propagation on multi-source\ndata where each pairwise constraint is defined over a pair of data points from\ndifferent sources. This multi-source constraint propagation has an important\napplication to cross-modal multimedia retrieval. Extensive results have shown\nthe superior performance of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 00:56:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lu", "Zhiwu", ""], ["Ip", "Horace H. S.", ""], ["Peng", "Yuxin", ""]]}, {"id": "1109.4979", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Yuxin Peng", "title": "Latent Semantic Learning with Structured Sparse Representation for Human\n  Action Recognition", "comments": "The short version of this paper appears in ICCV 2011", "journal-ref": null, "doi": "10.1016/j.patcog.2012.09.027", "report-no": null, "categories": "cs.MM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel latent semantic learning method for extracting\nhigh-level features (i.e. latent semantics) from a large vocabulary of abundant\nmid-level features (i.e. visual keywords) with structured sparse\nrepresentation, which can help to bridge the semantic gap in the challenging\ntask of human action recognition. To discover the manifold structure of\nmidlevel features, we develop a spectral embedding approach to latent semantic\nlearning based on L1-graph, without the need to tune any parameter for graph\nconstruction as a key step of manifold learning. More importantly, we construct\nthe L1-graph with structured sparse representation, which can be obtained by\nstructured sparse coding with its structured sparsity ensured by novel L1-norm\nhypergraph regularization over mid-level features. In the new embedding space,\nwe learn latent semantics automatically from abundant mid-level features\nthrough spectral clustering. The learnt latent semantics can be readily used\nfor human action recognition with SVM by defining a histogram intersection\nkernel. Different from the traditional latent semantic analysis based on topic\nmodels, our latent semantic learning method can explore the manifold structure\nof mid-level features in both L1-graph construction and spectral embedding,\nwhich results in compact but discriminative high-level features. The\nexperimental results on the commonly used KTH action dataset and unconstrained\nYouTube action dataset show the superior performance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 00:39:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lu", "Zhiwu", ""], ["Peng", "Yuxin", ""]]}, {"id": "1109.5072", "submitter": "Jose Hernandez-Orallo", "authors": "Javier Insa-Cabrera and Jose Hernandez-Orallo", "title": "Analysis of first prototype universal intelligence tests: evaluating and\n  comparing AI algorithms and humans", "comments": "114 pages, master thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, available methods that assess AI systems are focused on using\nempirical techniques to measure the performance of algorithms in some specific\ntasks (e.g., playing chess, solving mazes or land a helicopter). However, these\nmethods are not appropriate if we want to evaluate the general intelligence of\nAI and, even less, if we compare it with human intelligence. The ANYNT project\nhas designed a new method of evaluation that tries to assess AI systems using\nwell known computational notions and problems which are as general as possible.\nThis new method serves to assess general intelligence (which allows us to learn\nhow to solve any new kind of problem we face) and not only to evaluate\nperformance on a set of specific tasks. This method not only focuses on\nmeasuring the intelligence of algorithms, but also to assess any intelligent\nsystem (human beings, animals, AI, aliens?,...), and letting us to place their\nresults on the same scale and, therefore, to be able to compare them. This new\napproach will allow us (in the future) to evaluate and compare any kind of\nintelligent system known or even to build/find, be it artificial or biological.\nThis master thesis aims at ensuring that this new method provides consistent\nresults when evaluating AI algorithms, this is done through the design and\nimplementation of prototypes of universal intelligence tests and their\napplication to different intelligent systems (AI algorithms and humans beings).\nFrom the study we analyze whether the results obtained by two different\nintelligent systems are properly located on the same scale and we propose\nchanges and refinements to these prototypes in order to, in the future, being\nable to achieve a truly universal intelligence test.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 13:36:10 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Insa-Cabrera", "Javier", ""], ["Hernandez-Orallo", "Jose", ""]]}, {"id": "1109.5370", "submitter": "Jia Zeng", "authors": "Jia Zeng, Wei Feng, William K. Cheung, Chun-Hung Li", "title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the topic modeling problem of tagged documents and images.\nHigher-order relations among tagged documents and images are major and\nubiquitous characteristics, and play positive roles in extracting reliable and\ninterpretable topics. In this paper, we propose the tag-topic models (TTM) to\ndepict such higher-order topic structural dependencies within the Markov random\nfield (MRF) framework. First, we use the novel factor graph representation of\nlatent Dirichlet allocation (LDA)-based topic models from the MRF perspective,\nand present an efficient loopy belief propagation (BP) algorithm for\napproximate inference and parameter estimation. Second, we propose the factor\nhypergraph representation of TTM, and focus on both pairwise and higher-order\nrelation modeling among tagged documents and images. Efficient loopy BP\nalgorithm is developed to learn TTM, which encourages the topic labeling\nsmoothness among tagged documents and images. Extensive experimental results\nconfirm the incorporation of higher-order relations to be effective in\nenhancing the overall topic modeling performance, when compared with current\nstate-of-the-art topic models, in many text and image mining tasks of broad\ninterests such as word and link prediction, document classification, and tag\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 16:58:06 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Zeng", "Jia", ""], ["Feng", "Wei", ""], ["Cheung", "William K.", ""], ["Li", "Chun-Hung", ""]]}, {"id": "1109.5404", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Towards Optimal Learning of Chain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend Meek's conjecture (Meek 1997) from directed and\nacyclic graphs to chain graphs, and prove that the extended conjecture is true.\nSpecifically, we prove that if a chain graph H is an independence map of the\nindependence model induced by another chain graph G, then (i) G can be\ntransformed into H by a sequence of directed and undirected edge additions and\nfeasible splits and mergings, and (ii) after each operation in the sequence H\nremains an independence map of the independence model induced by G. Our result\nhas the same important consequence for learning chain graphs from data as the\nproof of Meek's conjecture in (Chickering 2002) had for learning Bayesian\nnetworks from data: It makes it possible to develop efficient and\nasymptotically correct learning algorithms under mild assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 21:53:44 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1109.5663", "submitter": "S.  Edelkamp", "authors": "S. Edelkamp, J. Hoffmann", "title": "The Deterministic Part of IPC-4: An Overview", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  519-579, 2005", "doi": "10.1613/jair.1677", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of the organization and results of the deterministic\npart of the 4th International Planning Competition, i.e., of the part concerned\nwith evaluating systems doing deterministic planning. IPC-4 attracted even more\ncompeting systems than its already large predecessors, and the competition\nevent was revised in several important respects. After giving an introduction\nto the IPC, we briefly explain the main differences between the deterministic\npart of IPC-4 and its predecessors. We then introduce formally the language\nused, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed\ninitial literals. We list the competing systems and overview the results of the\ncompetition. The entire set of data is far too large to be presented in full.\nWe provide a detailed summary; the complete data is available in an online\nappendix. We explain how we awarded the competition prizes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 18:27:26 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Edelkamp", "S.", ""], ["Hoffmann", "J.", ""]]}, {"id": "1109.5665", "submitter": "D. McDermott", "authors": "D. McDermott", "title": "PDDL2.1 - The Art of the Possible? Commentary on Fox and Long", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  145-148, 2003", "doi": "10.1613/jair.1996", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PDDL2.1 was designed to push the envelope of what planning algorithms can do,\nand it has succeeded. It adds two important features: durative actions,which\ntake time (and may have continuous effects); and objective functions for\nmeasuring the quality of plans. The concept of durative actions is flawed; and\nthe treatment of their semantics reveals too strong an attachment to the way\nmany contemporary planners work. Future PDDL innovators should focus on\nproducing a clean semantics for additions to the language, and let planner\nimplementers worry about coupling their algorithms to problems expressed in the\nlatest version of the language.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 18:44:25 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["McDermott", "D.", ""]]}, {"id": "1109.5666", "submitter": "D. E. Smith", "authors": "D. E. Smith", "title": "The Case for Durative Actions: A Commentary on PDDL2.1", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  149-154, 2003", "doi": "10.1613/jair.1997", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The addition of durative actions to PDDL2.1 sparked some controversy. Fox and\nLong argued that actions should be considered as instantaneous, but can start\nand stop processes. Ultimately, a limited notion of durative actions was\nincorporated into the language. I argue that this notion is still impoverished,\nand that the underlying philosophical position of regarding durative actions as\nbeing a shorthand for a start action, process, and stop action ignores the\nrealities of modelling and execution for complex systems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 18:44:29 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Smith", "D. E.", ""]]}, {"id": "1109.5711", "submitter": "L. Li", "authors": "L. Li, N. Onder, G. C. Whelan", "title": "Engineering a Conformant Probabilistic Planner", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  1-15, 2006", "doi": "10.1613/jair.1701", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a partial-order, conformant, probabilistic planner, Probapop which\ncompeted in the blind track of the Probabilistic Planning Competition in IPC-4.\nWe explain how we adapt distance based heuristics for use with probabilistic\ndomains. Probapop also incorporates heuristics based on probability of success.\nWe explain the successes and difficulties encountered during the design and\nimplementation of Probapop.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 20:20:27 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Li", "L.", ""], ["Onder", "N.", ""], ["Whelan", "G. C.", ""]]}, {"id": "1109.5713", "submitter": "J. Hoffmann", "authors": "J. Hoffmann", "title": "Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning\n  Benchmarks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  685-758, 2005", "doi": "10.1613/jair.1747", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between 1998 and 2004, the planning community has seen vast progress in terms\nof the sizes of benchmark examples that domain-independent planners can tackle\nsuccessfully. The key technique behind this progress is the use of heuristic\nfunctions based on relaxing the planning task at hand, where the relaxation is\nto assume that all delete lists are empty. The unprecedented success of such\nmethods, in many commonly used benchmark examples, calls for an understanding\nof what classes of domains these methods are well suited for. In the\ninvestigation at hand, we derive a formal background to such an understanding.\nWe perform a case study covering a range of 30 commonly used STRIPS and ADL\nbenchmark domains, including all examples used in the first four international\nplanning competitions. We *prove* connections between domain structure and\nlocal search topology -- heuristic cost surface properties -- under an\nidealized version of the heuristic functions used in modern planners. The\nidealized heuristic function is called h^+, and differs from the practically\nused functions in that it returns the length of an *optimal* relaxed plan,\nwhich is NP-hard to compute. We identify several key characteristics of the\ntopology under h^+, concerning the existence/non-existence of unrecognized dead\nends, as well as the existence/non-existence of constant upper bounds on the\ndifficulty of escaping local minima and benches. These distinctions divide the\n(set of all) planning domains into a taxonomy of classes of varying h^+\ntopology. As it turns out, many of the 30 investigated domains lie in classes\nwith a relatively easy topology. Most particularly, 12 of the domains lie in\nclasses where FFs search algorithm, provided with h^+, is a polynomial solving\nmechanism. We also present results relating h^+ to its approximation as\nimplemented in FF. The behavior regarding dead ends is provably the same. We\nsummarize the results of an empirical investigation showing that, in many\ndomains, the topological qualities of h^+ are largely inherited by the\napproximation. The overall investigation gives a rare example of a successful\nanalysis of the connections between typical-case problem structure, and search\nperformance. The theoretical investigation also gives hints on how the\ntopological phenomena might be automatically recognizable by domain analysis\ntechniques. We outline some preliminary steps we made into that direction.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 20:22:39 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Hoffmann", "J.", ""]]}, {"id": "1109.5714", "submitter": "N. Samaras", "authors": "N. Samaras, K. Stergiou", "title": "Binary Encodings of Non-binary Constraint Satisfaction Problems:\n  Algorithms and Experimental Results", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  641-684, 2005", "doi": "10.1613/jair.1776", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-binary Constraint Satisfaction Problem (CSP) can be solved directly\nusing extended versions of binary techniques. Alternatively, the non-binary\nproblem can be translated into an equivalent binary one. In this case, it is\ngenerally accepted that the translated problem can be solved by applying\nwell-established techniques for binary CSPs. In this paper we evaluate the\napplicability of the latter approach. We demonstrate that the use of standard\ntechniques for binary CSPs in the encodings of non-binary problems is\nproblematic and results in models that are very rarely competitive with the\nnon-binary representation. To overcome this, we propose specialized arc\nconsistency and search algorithms for binary encodings, and we evaluate them\ntheoretically and empirically. We consider three binary representations; the\nhidden variable encoding, the dual encoding, and the double encoding.\nTheoretical and empirical results show that, for certain classes of non-binary\nconstraints, binary encodings are a competitive option, and in many cases, a\nbetter one than the non-binary representation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 20:23:01 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Samaras", "N.", ""], ["Stergiou", "K.", ""]]}, {"id": "1109.5716", "submitter": "P. Adjiman", "authors": "P. Adjiman, P. Chatalic, F. Goasdoue, M. C. Rousset, L. Simon", "title": "Distributed Reasoning in a Peer-to-Peer Setting: Application to the\n  Semantic Web", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  269-314, 2006", "doi": "10.1613/jair.1785", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a peer-to-peer inference system, each peer can reason locally but can also\nsolicit some of its acquaintances, which are peers sharing part of its\nvocabulary. In this paper, we consider peer-to-peer inference systems in which\nthe local theory of each peer is a set of propositional clauses defined upon a\nlocal vocabulary. An important characteristic of peer-to-peer inference systems\nis that the global theory (the union of all peer theories) is not known (as\nopposed to partition-based reasoning systems). The main contribution of this\npaper is to provide the first consequence finding algorithm in a peer-to-peer\nsetting: DeCA. It is anytime and computes consequences gradually from the\nsolicited peer to peers that are more and more distant. We exhibit a sufficient\ncondition on the acquaintance graph of the peer-to-peer inference system for\nguaranteeing the completeness of this algorithm. Another important contribution\nis to apply this general distributed reasoning setting to the setting of the\nSemantic Web through the Somewhere semantic peer-to-peer data management\nsystem. The last contribution of this paper is to provide an experimental\nanalysis of the scalability of the peer-to-peer infrastructure that we propose,\non large networks of 1000 peers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 20:23:24 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Adjiman", "P.", ""], ["Chatalic", "P.", ""], ["Goasdoue", "F.", ""], ["Rousset", "M. C.", ""], ["Simon", "L.", ""]]}, {"id": "1109.5717", "submitter": "H. H. Hoos", "authors": "H. H. Hoos, W. Pullan", "title": "Dynamic Local Search for the Maximum Clique Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  159-185, 2006", "doi": "10.1613/jair.1815", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DLS-MC, a new stochastic local search algorithm\nfor the maximum clique problem. DLS-MC alternates between phases of iterative\nimprovement, during which suitable vertices are added to the current clique,\nand plateau search, during which vertices of the current clique are swapped\nwith vertices not contained in the current clique. The selection of vertices is\nsolely based on vertex penalties that are dynamically adjusted during the\nsearch, and a perturbation mechanism is used to overcome search stagnation. The\nbehaviour of DLS-MC is controlled by a single parameter, penalty delay, which\ncontrols the frequency at which vertex penalties are reduced. We show\nempirically that DLS-MC achieves substantial performance improvements over\nstate-of-the-art algorithms for the maximum clique problem over a large range\nof the commonly used DIMACS benchmark instances.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 20:24:56 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Hoos", "H. H.", ""], ["Pullan", "W.", ""]]}, {"id": "1109.5732", "submitter": "G. Gutnik", "authors": "G. Gutnik, G. A. Kaminka", "title": "Representing Conversations for Scalable Overhearing", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  349-387, 2006", "doi": "10.1613/jair.1829", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open distributed multi-agent systems are gaining interest in the academic\ncommunity and in industry. In such open settings, agents are often coordinated\nusing standardized agent conversation protocols. The representation of such\nprotocols (for analysis, validation, monitoring, etc) is an important aspect of\nmulti-agent applications. Recently, Petri nets have been shown to be an\ninteresting approach to such representation, and radically different approaches\nusing Petri nets have been proposed. However, their relative strengths and\nweaknesses have not been examined. Moreover, their scalability and suitability\nfor different tasks have not been addressed. This paper addresses both these\nchallenges. First, we analyze existing Petri net representations in terms of\ntheir scalability and appropriateness for overhearing, an important task in\nmonitoring open multi-agent systems. Then, building on the insights gained, we\nintroduce a novel representation using Colored Petri nets that explicitly\nrepresent legal joint conversation states and messages. This representation\napproach offers significant improvements in scalability and is particularly\nsuitable for overhearing. Furthermore, we show that this new representation\noffers a comprehensive coverage of all conversation features of FIPA\nconversation standards. We also present a procedure for transforming AUML\nconversation protocol diagrams (a standard human-readable representation), to\nour Colored Petri net representation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 21:56:16 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Gutnik", "G.", ""], ["Kaminka", "G. A.", ""]]}, {"id": "1109.5750", "submitter": "P. Haslum", "authors": "P. Haslum", "title": "Improving Heuristics Through Relaxed Search - An Analysis of TP4 and\n  HSP*a in the 2004 Planning Competition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  233-267, 2006", "doi": "10.1613/jair.1885", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hm admissible heuristics for (sequential and temporal) regression\nplanning are defined by a parameterized relaxation of the optimal cost function\nin the regression search space, where the parameter m offers a trade-off\nbetween the accuracy and computational cost of theheuristic. Existing methods\nfor computing the hm heuristic require time exponential in m, limiting them to\nsmall values (m andlt= 2). The hm heuristic can also be viewed as the optimal\ncost function in a relaxation of the search space: this paper presents relaxed\nsearch, a method for computing this function partially by searching in the\nrelaxed space. The relaxed search method, because it computes hm only\npartially, is computationally cheaper and therefore usable for higher values of\nm. The (complete) hm heuristic is combined with partial hm heuristics, for m =\n3,..., computed by relaxed search, resulting in a more accurate heuristic.\n  This use of the relaxed search method to improve on the hm heuristic is\nevaluated by comparing two optimal temporal planners: TP4, which does not use\nit, and HSP*a, which uses it but is otherwise identical to TP4. The comparison\nis made on the domains used in the 2004 International Planning Competition, in\nwhich both planners participated. Relaxed search is found to be cost effective\nin some of these domains, but not all. Analysis reveals a characterization of\nthe domains in which relaxed search can be expected to be cost effective, in\nterms of two measures on the original and relaxed search spaces. In the domains\nwhere relaxed search is cost effective, expanding small states is\ncomputationally cheaper than expanding large states and small states tend to\nhave small successor states.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 00:19:20 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Haslum", "P.", ""]]}, {"id": "1109.5920", "submitter": "Emmanuel Hebrard", "authors": "Diarmuid Grimes (4C UCC), Emmanuel Hebrard (LAAS)", "title": "Models and Strategies for Variants of the Job Shop Scheduling Problem", "comments": "Principles and Practice of Constraint Programming - CP 2011, Perugia\n  : Italy (2011)", "journal-ref": null, "doi": "10.1007/978-3-642-23786-7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a variety of constraint programming and Boolean satisfiability\napproaches to scheduling problems have been introduced. They have in common the\nuse of relatively simple propagation mechanisms and an adaptive way to focus on\nthe most constrained part of the problem. In some cases, these methods compare\nfavorably to more classical constraint programming methods relying on\npropagation algorithms for global unary or cumulative resource constraints and\ndedicated search heuristics. In particular, we described an approach that\ncombines restarting, with a generic adaptive heuristic and solution guided\nbranching on a simple model based on a decomposition of disjunctive\nconstraints. In this paper, we introduce an adaptation of this technique for an\nimportant subclass of job shop scheduling problems (JSPs), where the objective\nfunction involves minimization of earliness/tardiness costs. We further show\nthat our technique can be improved by adding domain specific information for\none variant of the JSP (involving time lag constraints). In particular we\nintroduce a dedicated greedy heuristic, and an improved model for the case\nwhere the maximal time lag is 0 (also referred to as no-wait JSPs).\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 14:53:01 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Grimes", "Diarmuid", "", "4C UCC"], ["Hebrard", "Emmanuel", "", "LAAS"]]}, {"id": "1109.5951", "submitter": "Shane Legg Dr", "authors": "Shane Legg, Joel Veness", "title": "An Approximation of the Universal Intelligence Measure", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Universal Intelligence Measure is a recently proposed formal definition\nof intelligence. It is mathematically specified, extremely general, and\ncaptures the essence of many informal definitions of intelligence. It is based\non Hutter's Universal Artificial Intelligence theory, an extension of Ray\nSolomonoff's pioneering work on universal induction. Since the Universal\nIntelligence Measure is only asymptotically computable, building a practical\nintelligence test from it is not straightforward. This paper studies the\npractical issues involved in developing a real-world UIM-based performance\nmetric. Based on our investigation, we develop a prototype implementation which\nwe use to evaluate a number of different artificial agents.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 16:09:27 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2011 21:38:56 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Legg", "Shane", ""], ["Veness", "Joel", ""]]}, {"id": "1109.6029", "submitter": "S. Schroedl", "authors": "S. Schroedl", "title": "An Improved Search Algorithm for Optimal Multiple-Sequence Alignment", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  587-623, 2005", "doi": "10.1613/jair.1534", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sequence alignment (MSA) is a ubiquitous problem in computational\nbiology. Although it is NP-hard to find an optimal solution for an arbitrary\nnumber of sequences, due to the importance of this problem researchers are\ntrying to push the limits of exact algorithms further. Since MSA can be cast as\na classical path finding problem, it is attracting a growing number of AI\nresearchers interested in heuristic search algorithms as a challenge with\nactual practical relevance. In this paper, we first review two previous,\ncomplementary lines of research. Based on Hirschbergs algorithm, Dynamic\nProgramming needs O(kN^(k-1)) space to store both the search frontier and the\nnodes needed to reconstruct the solution path, for k sequences of length N.\nBest first search, on the other hand, has the advantage of bounding the search\nspace that has to be explored using a heuristic. However, it is necessary to\nmaintain all explored nodes up to the final solution in order to prevent the\nsearch from re-expanding them at higher cost. Earlier approaches to reduce the\nClosed list are either incompatible with pruning methods for the Open list, or\nmust retain at least the boundary of the Closed list. In this article, we\npresent an algorithm that attempts at combining the respective advantages; like\nA* it uses a heuristic for pruning the search space, but reduces both the\nmaximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The\nunderlying idea is to conduct a series of searches with successively increasing\nupper bounds, but using the DP ordering as the key for the Open priority queue.\nWith a suitable choice of thresholds, in practice, a running time below four\ntimes that of A* can be expected. In our experiments we show that our algorithm\noutperforms one of the currently most successful algorithms for optimal\nmultiple sequence alignments, Partial Expansion A*, both in time and memory.\nMoreover, we apply a refined heuristic based on optimal alignments not only of\npairs of sequences, but of larger subsets. This idea is not new; however, to\nmake it practically relevant we show that it is equally important to bound the\nheuristic computation appropriately, or the overhead can obliterate any\npossible gain. Furthermore, we discuss a number of improvements in time and\nspace efficiency with regard to practical implementations. Our algorithm, used\nin conjunction with higher-dimensional heuristics, is able to calculate for the\nfirst time the optimal alignment for almost all of the problems in Reference 1\nof the benchmark database BAliBASE.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 20:40:06 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Schroedl", "S.", ""]]}, {"id": "1109.6030", "submitter": "M. Beetz", "authors": "M. Beetz, H. Grosskreutz", "title": "Probabilistic Hybrid Action Models for Predicting Concurrent\n  Percept-driven Robot Behavior", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  799-849, 2005", "doi": "10.1613/jair.1565", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic\ncausal model for predicting the behavior generated by modern percept-driven\nrobot plans. PHAMs represent aspects of robot behavior that cannot be\nrepresented by most action models used in AI planning: the temporal structure\nof continuous control processes, their non-deterministic effects, several modes\nof their interferences, and the achievement of triggering conditions in\nclosed-loop robot plans.\n  The main contributions of this article are: (1) PHAMs, a model of concurrent\npercept-driven behavior, its formalization, and proofs that the model generates\nprobably, qualitatively accurate predictions; and (2) a resource-efficient\ninference method for PHAMs based on sampling projections from probabilistic\naction models and state descriptions. We show how PHAMs can be applied to\nplanning the course of action of an autonomous robot office courier based on\nanalytical and experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 20:41:47 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Beetz", "M.", ""], ["Grosskreutz", "H.", ""]]}, {"id": "1109.6033", "submitter": "G. DeJong", "authors": "G. DeJong, A. Epshteyn", "title": "Generative Prior Knowledge for Discriminative Classification", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  25-53, 2006", "doi": "10.1613/jair.1934", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for integrating prior knowledge into\ndiscriminative classifiers. Our framework allows discriminative classifiers\nsuch as Support Vector Machines (SVMs) to utilize prior knowledge specified in\nthe generative setting. The dual objective of fitting the data and respecting\nprior knowledge is formulated as a bilevel program, which is solved\n(approximately) via iterative application of second-order cone programming. To\ntest our approach, we consider the problem of using WordNet (a semantic\ndatabase of English language) to improve low-sample classification accuracy of\nnewsgroup categorization. WordNet is viewed as an approximate, but readily\navailable source of background knowledge, and our framework is capable of\nutilizing it in a flexible way.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 20:53:29 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["DeJong", "G.", ""], ["Epshteyn", "A.", ""]]}, {"id": "1109.6051", "submitter": "M. Helmert", "authors": "M. Helmert", "title": "The Fast Downward Planning System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  191-246, 2006", "doi": "10.1613/jair.1705", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Downward is a classical planning system based on heuristic search. It\ncan deal with general deterministic planning problems encoded in the\npropositional fragment of PDDL2.2, including advanced features like ADL\nconditions and effects and derived predicates (axioms). Like other well-known\nplanners such as HSP and FF, Fast Downward is a progression planner, searching\nthe space of world states of a planning task in the forward direction. However,\nunlike other PDDL planning systems, Fast Downward does not use the\npropositional PDDL representation of a planning task directly. Instead, the\ninput is first translated into an alternative representation called\nmulti-valued planning tasks, which makes many of the implicit constraints of a\npropositional planning task explicit. Exploiting this alternative\nrepresentation, Fast Downward uses hierarchical decompositions of planning\ntasks for computing its heuristic function, called the causal graph heuristic,\nwhich is very different from traditional HSP-like heuristics based on ignoring\nnegative interactions of operators.\n  In this article, we give a full account of Fast Downwards approach to solving\nmulti-valued planning tasks. We extend our earlier discussion of the causal\ngraph heuristic to tasks involving axioms and conditional effects and present\nsome novel techniques for search control that are used within Fast Downwards\nbest-first search algorithm: preferred operators transfer the idea of helpful\nactions from local search to global best-first search, deferred evaluation of\nheuristic functions mitigates the negative effect of large branching factors on\nsearch performance, and multi-heuristic best-first search combines several\nheuristic evaluation functions within a single search algorithm in an\northogonal way. We also describe efficient data structures for fast state\nexpansion (successor generators and axiom evaluators) and present a new\nnon-heuristic search algorithm called focused iterative-broadening search,\nwhich utilizes the information encoded in causal graphs in a novel way.\n  Fast Downward has proven remarkably successful: It won the \"classical (i.e.,\npropositional, non-optimising) track of the 4th International Planning\nCompetition at ICAPS 2004, following in the footsteps of planners such as FF\nand LPG. Our experiments show that it also performs very well on the benchmarks\nof the earlier planning competitions and provide some insights about the\nusefulness of the new search enhancements.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 22:04:43 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Helmert", "M.", ""]]}, {"id": "1109.6052", "submitter": "V. R. Lesser", "authors": "V. R. Lesser, R. Mailler", "title": "Asynchronous Partial Overlay: A New Algorithm for Solving Distributed\n  Constraint Satisfaction Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  529-576, 2006", "doi": "10.1613/jair.1786", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Constraint Satisfaction (DCSP) has long been considered an\nimportant problem in multi-agent systems research. This is because many\nreal-world problems can be represented as constraint satisfaction and these\nproblems often present themselves in a distributed form. In this article, we\npresent a new complete, distributed algorithm called Asynchronous Partial\nOverlay (APO) for solving DCSPs that is based on a cooperative mediation\nprocess. The primary ideas behind this algorithm are that agents, when acting\nas a mediator, centralize small, relevant portions of the DCSP, that these\ncentralized subproblems overlap, and that agents increase the size of their\nsubproblems along critical paths within the DCSP as the problem solving\nunfolds. We present empirical evidence that shows that APO outperforms other\nknown, complete DCSP techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 22:05:46 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Lesser", "V. R.", ""], ["Mailler", "R.", ""]]}, {"id": "1109.6112", "submitter": "Carmen Gervet", "authors": "Islam Abdelraouf, Slim Abdennadher, Carmen Gervet", "title": "A Visual Entity-Relationship Model for Constraint-Based University\n  Timetabling", "comments": "12 pages, 7 figures, INAP 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  University timetabling (UTT) is a complex problem due to its combinatorial\nnature but also the type of constraints involved. The holy grail of\n(constraint) programming: \"the user states the problem the program solves it\"\nremains a challenge since solution quality is tightly coupled with deriving\n\"effective models\", best handled by technology experts. In this paper, focusing\non the field of university timetabling, we introduce a visual graphic\ncommunication tool that lets the user specify her problem in an abstract\nmanner, using a visual entity-relationship model. The entities are nodes of\nmainly two types: resource nodes (lecturers, assistants, student groups) and\nevents nodes (lectures, lab sessions, tutorials). The links between the nodes\nsignify a desired relationship between them. The visual modeling abstraction\nfocuses on the nature of the entities and their relationships and abstracts\nfrom an actual constraint model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 06:52:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Abdelraouf", "Islam", ""], ["Abdennadher", "Slim", ""], ["Gervet", "Carmen", ""]]}, {"id": "1109.6344", "submitter": "R. Booth", "authors": "R. Booth, T. Meyer", "title": "Admissible and Restrained Revision", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  127-151, 2006", "doi": "10.1613/jair.1874", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As partial justification of their framework for iterated belief revision\nDarwiche and Pearl convincingly argued against Boutiliers natural revision and\nprovided a prototypical revision operator that fits into their scheme. We show\nthat the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller\nclass of operators which we refer to as admissible. Admissible revision ensures\nthat the penultimate input is not ignored completely, thereby eliminating\nnatural revision, but includes the Darwiche-Pearl operator, Nayaks\nlexicographic revision operator, and a newly introduced operator called\nrestrained revision. We demonstrate that restrained revision is the most\nconservative of admissible revision operators, effecting as few changes as\npossible, while lexicographic revision is the least conservative, and point out\nthat restrained revision can also be viewed as a composite operator, consisting\nof natural revision preceded by an application of a \"backwards revision\"\noperator previously studied by Papini. Finally, we propose the establishment of\na principled approach for choosing an appropriate revision operator in\ndifferent contexts and discuss future work.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:26:49 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Booth", "R.", ""], ["Meyer", "T.", ""]]}, {"id": "1109.6345", "submitter": "R. I. Brafman", "authors": "R. I. Brafman, C. Domshlak, S. E. Shimony", "title": "On Graphical Modeling of Preference and Importance", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  389-424, 2006", "doi": "10.1613/jair.1895", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, CP-nets have emerged as a useful tool for supporting\npreference elicitation, reasoning, and representation. CP-nets capture and\nsupport reasoning with qualitative conditional preference statements,\nstatements that are relatively natural for users to express. In this paper, we\nextend the CP-nets formalism to handle another class of very natural\nqualitative statements one often uses in expressing preferences in daily life -\nstatements of relative importance of attributes. The resulting formalism,\nTCP-nets, maintains the spirit of CP-nets, in that it remains focused on using\nonly simple and natural preference statements, uses the ceteris paribus\nsemantics, and utilizes a graphical representation of this information to\nreason about its consistency and to perform, possibly constrained, optimization\nusing it. The extra expressiveness it provides allows us to better model\ntradeoffs users would like to make, more faithfully representing their\npreferences.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:29:41 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Brafman", "R. I.", ""], ["Domshlak", "C.", ""], ["Shimony", "S. E.", ""]]}, {"id": "1109.6346", "submitter": "M. Pistore", "authors": "M. Pistore, M. Y. Vardi", "title": "The Planning Spectrum - One, Two, Three, Infinity", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  101-132, 2007", "doi": "10.1613/jair.1909", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Temporal Logic (LTL) is widely used for defining conditions on the\nexecution paths of dynamic systems. In the case of dynamic systems that allow\nfor nondeterministic evolutions, one has to specify, along with an LTL formula\nf, which are the paths that are required to satisfy the formula. Two extreme\ncases are the universal interpretation A.f, which requires that the formula be\nsatisfied for all execution paths, and the existential interpretation E.f,\nwhich requires that the formula be satisfied for some execution path.\n  When LTL is applied to the definition of goals in planning problems on\nnondeterministic domains, these two extreme cases are too restrictive. It is\noften impossible to develop plans that achieve the goal in all the\nnondeterministic evolutions of a system, and it is too weak to require that the\ngoal is satisfied by some execution.\n  In this paper we explore alternative interpretations of an LTL formula that\nare between these extreme cases. We define a new language that permits an\narbitrary combination of the A and E quantifiers, thus allowing, for instance,\nto require that each finite execution can be extended to an execution\nsatisfying an LTL formula (AE.f), or that there is some finite execution whose\nextensions all satisfy an LTL formula (EA.f). We show that only eight of these\ncombinations of path quantifiers are relevant, corresponding to an alternation\nof the quantifiers of length one (A and E), two (AE and EA), three (AEA and\nEAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for\nthe new language that is based on an automata-theoretic approach, and study its\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:35:31 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Pistore", "M.", ""], ["Vardi", "M. Y.", ""]]}, {"id": "1109.6348", "submitter": "A. Roy", "authors": "A. Roy", "title": "Fault Tolerant Boolean Satisfiability", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 25, pages\n  503-527, 2006", "doi": "10.1613/jair.1914", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A delta-model is a satisfying assignment of a Boolean formula for which any\nsmall alteration, such as a single bit flip, can be repaired by flips to some\nsmall number of other bits, yielding a new satisfying assignment. These\nsatisfying assignments represent robust solutions to optimization problems\n(e.g., scheduling) where it is possible to recover from unforeseen events\n(e.g., a resource becoming unavailable). The concept of delta-models was\nintroduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that\nfinding delta-models for general Boolean formulas is NP-complete. In this\npaper, we extend that result by studying the complexity of finding delta-models\nfor classes of Boolean formulas which are known to have polynomial time\nsatisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT,\ndual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the\ncomplexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have\npolynomial time tests for delta-models, testing whether a Horn-SAT formula has\none is NP-complete.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:37:53 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Roy", "A.", ""]]}, {"id": "1109.6361", "submitter": "J. Y. Chai", "authors": "J. Y. Chai, Z. Prasov, S. Qu", "title": "Cognitive Principles in Robust Multimodal Interpretation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  55-83, 2006", "doi": "10.1613/jair.1936", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal conversational interfaces provide a natural means for users to\ncommunicate with computer systems through multiple modalities such as speech\nand gesture. To build effective multimodal interfaces, automated interpretation\nof user multimodal inputs is important. Inspired by the previous investigation\non cognitive status in multimodal human machine interaction, we have developed\na greedy algorithm for interpreting user referring expressions (i.e.,\nmultimodal reference resolution). This algorithm incorporates the cognitive\nprinciples of Conversational Implicature and Givenness Hierarchy and applies\nconstraints from various sources (e.g., temporal, semantic, and contextual) to\nresolve references. Our empirical results have shown the advantage of this\nalgorithm in efficiently resolving a variety of user references. Because of its\nsimplicity and generality, this approach has the potential to improve the\nrobustness of multimodal input interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 21:45:34 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Chai", "J. Y.", ""], ["Prasov", "Z.", ""], ["Qu", "S.", ""]]}, {"id": "1109.6401", "submitter": "Frederic Dambreville", "authors": "Frederic Dambreville (ENSIETA)", "title": "An Interpretation of Belief Functions by means of a Probabilistic\n  Multi-modal Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While belief functions may be seen formally as a generalization of\nprobabilistic distributions, the question of the interactions between belief\nfunctions and probability is still an issue in practice. This question is\ndifficult, since the contexts of use of these theory are notably different and\nthe semantics behind these theories are not exactly the same. A prominent issue\nis increasingly regarded by the community, that is the management of the\nconflicting information. Recent works have introduced new rules for handling\nthe conflict redistribution while combining belief functions. The notion of\nconflict, or its cancellation by an hypothesis of open world, seems by itself\nto prevent a direct interpretation of belief function in a probabilistic\nframework. This paper addresses the question of a probabilistic interpretation\nof belief functions. It first introduces and implements a theoretically\ngrounded rule, which is in essence an adaptive conjunctive rule. It is shown,\nhow this rule is derived from a logical interpretation of the belief functions\nby means of a probabilistic multimodal logic; in addition, a concept of source\nindependence is introduced, based on a principle of entropy maximization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 05:24:51 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Dambreville", "Frederic", "", "ENSIETA"]]}, {"id": "1109.6402", "submitter": "Frederic Dambreville", "authors": "Frederic Dambreville (ENSIETA)", "title": "Extension of Boolean algebra by a Bayesian operator; application to the\n  definition of a Deterministic Bayesian Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work contributes to the domains of Boolean algebra and of Bayesian\nprobability, by proposing an algebraic extension of Boolean algebras, which\nimplements an operator for the Bayesian conditional inference and is closed\nunder this operator. It is known since the work of Lewis (Lewis' triviality)\nthat it is not possible to construct such conditional operator within the space\nof events. Nevertheless, this work proposes an answer which complements Lewis'\ntriviality, by the construction of a conditional operator outside the space of\nevents, thus resulting in an algebraic extension. In particular, it is proved\nthat any probability defined on a Boolean algebra may be extended to its\nalgebraic extension in compliance with the multiplicative definition of the\nconditional probability. In the last part of this paper, a new bivalent logic\nis introduced on the basis of this algebraic extension, and basic properties\nare derived.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 05:25:40 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2011 09:33:02 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Dambreville", "Frederic", "", "ENSIETA"]]}, {"id": "1109.6618", "submitter": "D. Davidov", "authors": "D. Davidov, S. Markovitch", "title": "Multiple-Goal Heuristic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  417-451, 2006", "doi": "10.1613/jair.1940", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for anytime heuristic search where the\ntask is to achieve as many goals as possible within the allocated resources. We\nshow the inadequacy of traditional distance-estimation heuristics for tasks of\nthis type and present alternative heuristics that are more appropriate for\nmultiple-goal search. In particular, we introduce the marginal-utility\nheuristic, which estimates the cost and the benefit of exploring a subtree\nbelow a search node. We developed two methods for online learning of the\nmarginal-utility heuristic. One is based on local similarity of the partial\nmarginal utility of sibling nodes, and the other generalizes marginal-utility\nover the state feature space. We apply our adaptive and non-adaptive\nmultiple-goal search algorithms to several problems, including focused\ncrawling, and show their superiority over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 18:50:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Davidov", "D.", ""], ["Markovitch", "S.", ""]]}, {"id": "1109.6621", "submitter": "S. Hoelldobler", "authors": "S. Hoelldobler, E. Karabaev, O. Skvortsova", "title": "FluCaP: A Heuristic Search Planner for First-Order MDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  419-439, 2006", "doi": "10.1613/jair.1965", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a heuristic search algorithm for solving first-order Markov\nDecision Processes (FOMDPs). Our approach combines first-order state\nabstraction that avoids evaluating states individually, and heuristic search\nthat avoids evaluating all states. Firstly, in contrast to existing systems,\nwhich start with propositionalizing the FOMDP and then perform state\nabstraction on its propositionalized version we apply state abstraction\ndirectly on the FOMDP avoiding propositionalization. This kind of abstraction\nis referred to as first-order state abstraction. Secondly, guided by an\nadmissible heuristic, the search is restricted to those states that are\nreachable from the initial state. We demonstrate the usefulness of the above\ntechniques for solving FOMDPs with a system, referred to as FluCaP (formerly,\nFCPlanner), that entered the probabilistic track of the 2004 International\nPlanning Competition (IPC2004) and demonstrated an advantage over other\nplanners on the problems represented in first-order terms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 18:58:54 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Hoelldobler", "S.", ""], ["Karabaev", "E.", ""], ["Skvortsova", "O.", ""]]}, {"id": "1109.6638", "submitter": "James Bergstra", "authors": "James Bergstra, Aaron Courville, Yoshua Bengio", "title": "The Statistical Inefficiency of Sparse Coding for Images (or, One Gabor\n  to Rule them All)", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a proven principle for learning compact representations of\nimages. However, sparse coding by itself often leads to very redundant\ndictionaries. With images, this often takes the form of similar edge detectors\nwhich are replicated many times at various positions, scales and orientations.\nAn immediate consequence of this observation is that the estimation of the\ndictionary components is not statistically efficient. We propose a factored\nmodel in which factors of variation (e.g. position, scale and orientation) are\nuntangled from the underlying Gabor-like filters. There is so much redundancy\nin sparse codes for natural images that our model requires only a single\ndictionary element (a Gabor-like edge detector) to outperform standard sparse\ncoding. Our model scales naturally to arbitrary-sized images while achieving\nmuch greater statistical efficiency during learning. We validate this claim\nwith a number of experiments showing, in part, superior compression of\nout-of-sample data using a sparse coding dictionary learned with only a single\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 19:47:00 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 15:27:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bergstra", "James", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1109.6838", "submitter": "Sarvesh Nikumbh", "authors": "Sarvesh Nikumbh, Joeprakash Nathaman and Rahul Vartak", "title": "Distributed Air Traffic Control : A Human Safety Perspective", "comments": "Extended Abstract, 3 pages, Accepted at IBM Collaborative Academia\n  Research Exchange (I-CARE)-2011, uses ACM-Proceeding style file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issues in air traffic control have so far been addressed with the intent\nto improve resource utilization and achieve an optimized solution with respect\nto fuel comsumption of aircrafts, efficient usage of the available airspace\nwith minimal congestion related losses under various dynamic constraints. So\nthe focus has almost always been more on smarter management of traffic to\nincrease profits while human safety, though achieved in the process, we\nbelieve, has remained less seriously attended. This has become all the more\nimportant given that we have overburdened and overstressed air traffic\ncontrollers managing hundreds of airports and thousands of aircrafts per day.\n  We propose a multiagent system based distributed approach to handle air\ntraffic ensuring complete human (passenger) safety without removing any humans\n(ground controllers) from the loop thereby also retaining the earlier\nadvantages in the new solution. The detailed design of the agent system, which\nwill be easily interfacable with the existing environment, is described. Based\non our initial findings from simulations, we strongly believe the system to be\ncapable of handling the nuances involved, to be extendable and customizable at\nany later point in time.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 14:42:14 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Nikumbh", "Sarvesh", ""], ["Nathaman", "Joeprakash", ""], ["Vartak", "Rahul", ""]]}, {"id": "1109.6841", "submitter": "Percy Liang", "authors": "Percy Liang and Michael I. Jordan and Dan Klein", "title": "Learning Dependency-Based Compositional Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we want to build a system that answers a natural language question by\nrepresenting its semantics as a logical form and computing the answer given a\nstructured database of facts. The core part of such a system is the semantic\nparser that maps questions to logical forms. Semantic parsers are typically\ntrained from examples of questions annotated with their target logical forms,\nbut this type of annotation is expensive.\n  Our goal is to learn a semantic parser from question-answer pairs instead,\nwhere the logical form is modeled as a latent variable. Motivated by this\nchallenging learning problem, we develop a new semantic formalism,\ndependency-based compositional semantics (DCS), which has favorable linguistic,\nstatistical, and computational properties. We define a log-linear distribution\nover DCS logical forms and estimate the parameters using a simple procedure\nthat alternates between beam search and numerical optimization. On two standard\nsemantic parsing benchmarks, our system outperforms all existing\nstate-of-the-art systems, despite using no annotated logical forms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 14:49:30 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Liang", "Percy", ""], ["Jordan", "Michael I.", ""], ["Klein", "Dan", ""]]}]