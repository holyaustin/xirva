[{"id": "1102.0079", "submitter": "Ping Zhu", "authors": "Ping Zhu and Qiaoyan Wen", "title": "Information-theoretic measures associated with rough set approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although some information-theoretic measures of uncertainty or granularity\nhave been proposed in rough set theory, these measures are only dependent on\nthe underlying partition and the cardinality of the universe, independent of\nthe lower and upper approximations. It seems somewhat unreasonable since the\nbasic idea of rough set theory aims at describing vague concepts by the lower\nand upper approximations. In this paper, we thus define new\ninformation-theoretic entropy and co-entropy functions associated to the\npartition and the approximations to measure the uncertainty and granularity of\nan approximation space. After introducing the novel notions of entropy and\nco-entropy, we then examine their properties. In particular, we discuss the\nrelationship of co-entropies between different universes. The theoretical\ndevelopment is accompanied by illustrative numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 05:38:33 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Zhu", "Ping", ""], ["Wen", "Qiaoyan", ""]]}, {"id": "1102.0183", "submitter": "Dan Ciresan", "authors": "Dan C. Cire\\c{s}an, Ueli Meier, Jonathan Masci, Luca M. Gambardella\n  and J\\\"urgen Schmidhuber", "title": "High-Performance Neural Networks for Visual Object Classification", "comments": "12 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "IDSIA 1-11", "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast, fully parameterizable GPU implementation of Convolutional\nNeural Network variants. Our feature extractors are neither carefully designed\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\narchitectures achieve the best published results on benchmarks for object\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\nback-propagation perform better than more shallow ones. Learning is\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 15:34:43 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Cire\u015fan", "Dan C.", ""], ["Meier", "Ueli", ""], ["Masci", "Jonathan", ""], ["Gambardella", "Luca M.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1102.0230", "submitter": "Arup Kumar Ghosh", "authors": "Arup Kumar Ghosh (School of EECS, University of Central Florida)", "title": "Speeding up SAT solver by exploring CNF symmetries : Revisited", "comments": "12 pages, Forty-First Southeastern International Conference on\n  Combinatorics, Graph Theory, and Computing (USA, 2010)", "journal-ref": "Congressus Numerantium 206 (2010), pp. 73-84", "doi": null, "report-no": null, "categories": "math.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean Satisfiability solvers have gone through dramatic improvements in\ntheir performances and scalability over the last few years by considering\nsymmetries. It has been shown that by using graph symmetries and generating\nsymmetry breaking predicates (SBPs) it is possible to break symmetries in\nConjunctive Normal Form (CNF). The SBPs cut down the search space to the\nnonsymmetric regions of the space without affecting the satisfiability of the\nCNF formula. The symmetry breaking predicates are created by representing the\nformula as a graph, finding the graph symmetries and using some symmetry\nextraction mechanism (Crawford et al.). Here in this paper we take one\nnon-trivial CNF and explore its symmetries. Finally, we generate the SBPs and\nadding it to CNF we show how it helps to prune the search tree, so that SAT\nsolver would take short time. Here we present the pruning procedure of the\nsearch tree from scratch, starting from the CNF and its graph representation.\nAs we explore the whole mechanism by a non-trivial example, it would be easily\ncomprehendible. Also we have given a new idea of generating symmetry breaking\npredicates for breaking symmetry in CNF, not derived from Crawford's\nconditions. At last we propose a backtrack SAT solver with inbuilt SBP\ngenerator.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 17:37:17 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Ghosh", "Arup Kumar", "", "School of EECS, University of Central Florida"]]}, {"id": "1102.0257", "submitter": "Walter Quattrociocchi", "authors": "Walter Quattrociocchi and Frederic Amblard", "title": "Emergence through Selection: The Evolution of a Scientific Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most interesting scientific challenges nowadays deals with the\nanalysis and the understanding of complex networks' dynamics and how their\nprocesses lead to emergence according to the interactions among their\ncomponents. In this paper we approach the definition of new methodologies for\nthe visualization and the exploration of the dynamics at play in real dynamic\nsocial networks. We present a recently introduced formalism called TVG (for\ntime-varying graphs), which was initially developed to model and analyze\nhighly-dynamic and infrastructure-less communication networks such as mobile\nad-hoc networks, wireless sensor networks, or vehicular networks. We discuss\nits applicability to complex networks in general, and social networks in\nparticular, by showing how it enables the specification and analysis of complex\ndynamic phenomena in terms of temporal interactions, and allows to easily\nswitch the perspective between local and global dynamics. As an example, we\nchose the case of scientific communities by analyzing portion of the ArXiv\nrepository (ten years of publications in physics) focusing on the social\ndeterminants (e.g. goals and potential interactions among individuals) behind\nthe emergence and the resilience of scientific communities. We consider that\nscientific communities are at the same time communities of practice (through\nco-authorship) and that they exist also as representations in the scientists'\nmind, since references to other scientists' works is not merely an objective\nlink to a relevant work, but it reveals social objects that one manipulates,\nselect and refers to. In the paper we show the emergence/selection of a\ncommunity as a goal-driven preferential attachment toward a set of authors\namong which there are some key scientists (Nobel prizes).\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 19:44:09 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Quattrociocchi", "Walter", ""], ["Amblard", "Frederic", ""]]}, {"id": "1102.0629", "submitter": "Walter Quattrociocchi", "authors": "Nicola Santoro, Walter Quattrociocchi, Paola Flocchini, Arnaud\n  Casteigts, and Frederic Amblard", "title": "Time-Varying Graphs and Social Network Analysis: Temporal Indicators and\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most instruments - formalisms, concepts, and metrics - for social networks\nanalysis fail to capture their dynamics. Typical systems exhibit different\nscales of dynamics, ranging from the fine-grain dynamics of interactions (which\nrecently led researchers to consider temporal versions of distance,\nconnectivity, and related indicators), to the evolution of network properties\nover longer periods of time. This paper proposes a general approach to study\nthat evolution for both atemporal and temporal indicators, based respectively\non sequences of static graphs and sequences of time-varying graphs that cover\nsuccessive time-windows. All the concepts and indicators, some of which are\nnew, are expressed using a time-varying graph formalism.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 09:26:50 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Santoro", "Nicola", ""], ["Quattrociocchi", "Walter", ""], ["Flocchini", "Paola", ""], ["Casteigts", "Arnaud", ""], ["Amblard", "Frederic", ""]]}, {"id": "1102.0714", "submitter": "Jose Hernandez-Orallo", "authors": "Javier Insa-Cabrera, Jose Hernandez-Orallo", "title": "An architecture for the evaluation of intelligent systems", "comments": "112 pages. In Spanish. Final Project Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main research areas in Artificial Intelligence is the coding of\nagents (programs) which are able to learn by themselves in any situation. This\nmeans that agents must be useful for purposes other than those they were\ncreated for, as, for example, playing chess. In this way we try to get closer\nto the pristine goal of Artificial Intelligence. One of the problems to decide\nwhether an agent is really intelligent or not is the measurement of its\nintelligence, since there is currently no way to measure it in a reliable way.\nThe purpose of this project is to create an interpreter that allows for the\nexecution of several environments, including those which are generated\nrandomly, so that an agent (a person or a program) can interact with them. Once\nthe interaction between the agent and the environment is over, the interpreter\nwill measure the intelligence of the agent according to the actions, states and\nrewards the agent has undergone inside the environment during the test. As a\nresult we will be able to measure agents' intelligence in any possible\nenvironment, and to make comparisons between several agents, in order to\ndetermine which of them is the most intelligent. In order to perform the tests,\nthe interpreter must be able to randomly generate environments that are really\nuseful to measure agents' intelligence, since not any randomly generated\nenvironment will serve that purpose.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 15:58:18 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Insa-Cabrera", "Javier", ""], ["Hernandez-Orallo", "Jose", ""]]}, {"id": "1102.0831", "submitter": "Madhu G", "authors": "G.Madhu (1), Dr.A.Govardhan (2), Dr.T.V.Rajinikanth (3)", "title": "Intelligent Semantic Web Search Engines: A Brief Survey", "comments": null, "journal-ref": "International journal of Web & Semantic Technology (IJWesT) Vol.2,\n  No.1, January 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web (WWW) allows the people to share the information (data)\nfrom the large database repositories globally. The amount of information grows\nbillions of databases. We need to search the information will specialize tools\nknown generically search engine. There are many of search engines available\ntoday, retrieving meaningful information is difficult. However to overcome this\nproblem in search engines to retrieve meaningful information intelligently,\nsemantic web technologies are playing a major role. In this paper we present\nsurvey on the search engine generations and the role of search engines in\nintelligent web and semantic search technologies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 03:56:09 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["Madhu", "G.", ""], ["Govardhan", "Dr. A.", ""], ["Rajinikanth", "Dr. T. V.", ""]]}, {"id": "1102.0899", "submitter": "Michael Del Rose", "authors": "Michael DelRose, Christian Wagner, Philip Frederick", "title": "Evidence Feed Forward Hidden Markov Model: A New Type of Hidden Markov\n  Model", "comments": "19 pages, International Journal of Artificial Intelligence and\n  Applications", "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol. 2, No. 1, Jan 2011", "doi": "10.5121/ijaia.2011.2101", "report-no": null, "categories": "cs.AI cs.CV cs.LG math.NA math.PR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ability to predict the intentions of people based solely on their visual\nactions is a skill only performed by humans and animals. The intelligence of\ncurrent computer algorithms has not reached this level of complexity, but there\nare several research efforts that are working towards it. With the number of\nclassification algorithms available, it is hard to determine which algorithm\nworks best for a particular situation. In classification of visual human intent\ndata, Hidden Markov Models (HMM), and their variants, are leading candidates.\n  The inability of HMMs to provide a probability in the observation to\nobservation linkages is a big downfall in this classification technique. If a\nperson is visually identifying an action of another person, they monitor\npatterns in the observations. By estimating the next observation, people have\nthe ability to summarize the actions, and thus determine, with pretty good\naccuracy, the intention of the person performing the action. These visual cues\nand linkages are important in creating intelligent algorithms for determining\nhuman actions based on visual observations.\n  The Evidence Feed Forward Hidden Markov Model is a newly developed algorithm\nwhich provides observation to observation linkages. The following research\naddresses the theory behind Evidence Feed Forward HMMs, provides mathematical\nproofs of their learning of these parameters to optimize the likelihood of\nobservations with a Evidence Feed Forwards HMM, which is important in all\ncomputational intelligence algorithm, and gives comparative examples with\nstandard HMMs in classification of both visual action data and measurement\ndata; thus providing a strong base for Evidence Feed Forward HMMs in\nclassification of many types of problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 13:00:06 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["DelRose", "Michael", ""], ["Wagner", "Christian", ""], ["Frederick", "Philip", ""]]}, {"id": "1102.1027", "submitter": "Alaa Abi Haidar", "authors": "Alaa Abi-Haidar and Luis M. Rocha", "title": "Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics", "comments": null, "journal-ref": "Evolutionary Intelligence. 2011. Volume 4, Number 2, 69-80", "doi": "10.1007/s12065-011-0052-5", "report-no": null, "categories": "cs.IR cs.AI cs.LG nlin.AO q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 22:10:45 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Abi-Haidar", "Alaa", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1102.1536", "submitter": "Nabil Belgasmi", "authors": "Nabil Belgasmi (SOIE), Lamjed Ben Said (SOIE), Khaled Gh\\'edira (SOIE)", "title": "Evolutionary multiobjective optimization of the multi-location\n  transshipment problem", "comments": null, "journal-ref": "Operational Research 8, 2 (2008) 167-183", "doi": "10.1007/s12351-008-0015-5", "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-location inventory system where inventory choices at each\nlocation are centrally coordinated. Lateral transshipments are allowed as\nrecourse actions within the same echelon in the inventory system to reduce\ncosts and improve service level. However, this transshipment process usually\ncauses undesirable lead times. In this paper, we propose a multiobjective model\nof the multi-location transshipment problem which addresses optimizing three\nconflicting objectives: (1) minimizing the aggregate expected cost, (2)\nmaximizing the expected fill rate, and (3) minimizing the expected\ntransshipment lead times. We apply an evolutionary multiobjective optimization\napproach using the strength Pareto evolutionary algorithm (SPEA2), to\napproximate the optimal Pareto front. Simulation with a wide choice of model\nparameters shows the different trades-off between the conflicting objectives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2011 08:54:32 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Belgasmi", "Nabil", "", "SOIE"], ["Said", "Lamjed Ben", "", "SOIE"], ["Gh\u00e9dira", "Khaled", "", "SOIE"]]}, {"id": "1102.1691", "submitter": "Manuel Marques-Pita PhD", "authors": "Manuel Marques-Pita and Luis M. Rocha", "title": "Schema Redescription in Cellular Automata: Revisiting Emergence in\n  Complex Systems", "comments": "paper submitted to the 2011 IEEE Symposium on Artificial Life", "journal-ref": "The 2011 IEEE Symposium on Artificial Life, at the IEEE Symposium\n  Series on Computational Intelligence 2011. April 11 - 15, 201, Paris, France,\n  pp: 233-240", "doi": null, "report-no": null, "categories": "nlin.CG cs.AI cs.FL cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to eliminate redundancy in the transition tables of\nBoolean automata: schema redescription with two symbols. One symbol is used to\ncapture redundancy of individual input variables, and another to capture\npermutability in sets of input variables: fully characterizing the canalization\npresent in Boolean functions. Two-symbol schemata explain aspects of the\nbehaviour of automata networks that the characterization of their emergent\npatterns does not capture. We use our method to compare two well-known cellular\nautomata for the density classification task: the human engineered CA GKL, and\nanother obtained via genetic programming (GP). We show that despite having very\ndifferent collective behaviour, these rules are very similar. Indeed, GKL is a\nspecial case of GP. Therefore, we demonstrate that it is more feasible to\ncompare cellular automata via schema redescriptions of their rules, than by\nlooking at their emergent behaviour, leading us to question the tendency in\ncomplexity research to pay much more attention to emergent patterns than to\nlocal interactions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2011 19:13:53 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 15:44:38 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Marques-Pita", "Manuel", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1102.1745", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Restructuring in Combinatorial Optimization", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses a new class of combinatorial problems which consist in\nrestructuring of solutions (as structures) in combinatorial optimization. Two\nmain features of the restructuring process are examined: (i) a cost of the\nrestructuring, (ii) a closeness to a goal solution. This problem corresponds to\nredesign (improvement, upgrade) of modular systems or solutions. The\nrestructuring approach is described and illustrated for the following\ncombinatorial optimization problems: knapsack problem, multiple choice problem,\nassignment problem, spanning tree problems. Examples illustrate the\nrestructuring processes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2011 23:05:59 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1102.1747", "submitter": "Maria Polukarov", "authors": "Thomas D. Voice, Maria Polukarov, Nicholas R. Jennings", "title": "Graph Coalition Structure Generation", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first analysis of the computational complexity of {\\it coalition\nstructure generation over graphs}. Given an undirected graph $G=(N,E)$ and a\nvaluation function $v:2^N\\rightarrow\\RR$ over the subsets of nodes, the problem\nis to find a partition of $N$ into connected subsets, that maximises the sum of\nthe components' values. This problem is generally NP--complete; in particular,\nit is hard for a defined class of valuation functions which are {\\it\nindependent of disconnected members}---that is, two nodes have no effect on\neach other's marginal contribution to their vertex separator. Nonetheless, for\nall such functions we provide bounds on the complexity of coalition structure\ngeneration over general and minor free graphs. Our proof is constructive and\nyields algorithms for solving corresponding instances of the problem.\nFurthermore, we derive polynomial time bounds for acyclic, $K_{2,3}$ and $K_4$\nminor free graphs. However, as we show, the problem remains NP--complete for\nplanar graphs, and hence, for any $K_k$ minor free graphs where $k\\geq 5$.\nMoreover, our hardness result holds for a particular subclass of valuation\nfunctions, termed {\\it edge sum}, where the value of each subset of nodes is\nsimply determined by the sum of given weights of the edges in the induced\nsubgraph.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2011 23:13:58 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Voice", "Thomas D.", ""], ["Polukarov", "Maria", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "1102.1803", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed", "title": "Proposing LT based Search in PDM Systems for Better Information\n  Retrieval", "comments": "15 pages, 31 figures", "journal-ref": "International Journal of Computer Science & Emerging Technologies\n  (E-ISSN: 2044-6004), Volume 1, Issue 4, P86-100, December 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  PDM Systems contain and manage heavy amount of data but the search mechanism\nof most of the systems is not intelligent which can process user\"s natural\nlanguage based queries to extract desired information. Currently available\nsearch mechanisms in almost all of the PDM systems are not very efficient and\nbased on old ways of searching information by entering the relevant information\nto the respective fields of search forms to find out some specific information\nfrom attached repositories. Targeting this issue, a thorough research was\nconducted in fields of PDM Systems and Language Technology. Concerning the PDM\nSystem, conducted research provides the information about PDM and PDM Systems\nin detail. Concerning the field of Language Technology, helps in implementing a\nsearch mechanism for PDM Systems to search user\"s needed information by\nanalyzing user\"s natural language based requests. The accomplished goal of this\nresearch was to support the field of PDM with a new proposition of a conceptual\nmodel for the implementation of natural language based search. The proposed\nconceptual model is successfully designed and partially implementation in the\nform of a prototype. Describing the proposition in detail the main concept,\nimplementation designs and developed prototype of proposed approach is\ndiscussed in this paper. Implemented prototype is compared with respective\nfunctions of existing PDM systems .i.e., Windchill and CIM to evaluate its\neffectiveness against targeted challenges.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 08:16:50 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Ahmed", "Zeeshan", ""]]}, {"id": "1102.1808", "submitter": "L\\'eon Bottou", "authors": "Leon Bottou", "title": "From Machine Learning to Machine Reasoning", "comments": "15 pages - fix broken pagination in v2", "journal-ref": null, "doi": null, "report-no": "tr-2011-02-08", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plausible definition of \"reasoning\" could be \"algebraically manipulating\npreviously acquired knowledge in order to answer a new question\". This\ndefinition covers first-order logical inference or probabilistic inference. It\nalso includes much simpler manipulations commonly used to build large learning\nsystems. For instance, we can build an optical character recognition system by\nfirst training a character segmenter, an isolated character recognizer, and a\nlanguage model, using appropriate labeled training sets. Adequately\nconcatenating these modules and fine tuning the resulting system can be viewed\nas an algebraic operation in a space of models. The resulting model answers a\nnew question, that is, converting the image of a text page into a computer\nreadable text.\n  This observation suggests a conceptual continuity between algebraically rich\ninference systems, such as logical or probabilistic inference, and simple\nmanipulations, such as the mere concatenation of trainable learning systems.\nTherefore, instead of trying to bridge the gap between machine learning systems\nand sophisticated \"all-purpose\" inference mechanisms, we can instead\nalgebraically enrich the set of manipulations applicable to training systems,\nand build reasoning capabilities from the ground up.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 08:25:36 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 03:18:15 GMT"}, {"version": "v3", "created": "Fri, 11 Feb 2011 05:10:57 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Bottou", "Leon", ""]]}, {"id": "1102.1889", "submitter": "David Spivak", "authors": "David I. Spivak, Robert E. Kent", "title": "Ologs: a categorical framework for knowledge representation", "comments": "38 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0024274", "report-no": null, "categories": "cs.LO cs.AI math.CT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we introduce the olog, or ontology log, a category-theoretic\nmodel for knowledge representation (KR). Grounded in formal mathematics, ologs\ncan be rigorously formulated and cross-compared in ways that other KR models\n(such as semantic networks) cannot. An olog is similar to a relational database\nschema; in fact an olog can serve as a data repository if desired. Unlike\ndatabase schemas, which are generally difficult to create or modify, ologs are\ndesigned to be user-friendly enough that authoring or reconfiguring an olog is\na matter of course rather than a difficult chore. It is hoped that learning to\nauthor ologs is much simpler than learning a database definition language,\ndespite their similarity. We describe ologs carefully and illustrate with many\nexamples. As an application we show that any primitive recursive function can\nbe described by an olog. We also show that ologs can be aligned or connected\ntogether into a larger network using functors. The various methods of\ninformation flow and institutions can then be used to integrate local and\nglobal world-views. We finish by providing several different avenues for future\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 15:49:49 GMT"}, {"version": "v2", "created": "Sun, 7 Aug 2011 20:02:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Spivak", "David I.", ""], ["Kent", "Robert E.", ""]]}, {"id": "1102.2125", "submitter": "Marcello Balduccini", "authors": "Marcello Balduccini", "title": "Improving DPLL Solver Performance with Domain-Specific Heuristics: the\n  ASP Case", "comments": "Presented at the ASPOCP10 workshop of ICLP10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the recent improvements in the performance of the solvers based\non the DPLL procedure, it is still possible for the search algorithm to focus\non the wrong areas of the search space, preventing the solver from returning a\nsolution in an acceptable amount of time. This prospect is a real concern e.g.\nin an industrial setting, where users typically expect consistent performance.\nTo overcome this problem, we propose a framework that allows learning and using\ndomain-specific heuristics in solvers based on the DPLL procedure. The learning\nis done off-line, on representative instances from the target domain, and the\nlearned heuristics are then used for choice-point selection. In this paper we\nfocus on Answer Set Programming (ASP) solvers. In our experiments, the\nintroduction of domain-specific heuristics improved performance on hard\ninstances by up to 3 orders of magnitude (and 2 on average), nearly completely\neliminating the cases in which the solver had to be terminated because the wait\nfor an answer had become unacceptable.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 14:21:57 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Balduccini", "Marcello", ""]]}, {"id": "1102.2174", "submitter": "Vincent Aravantinos", "authors": "Vincent Aravantinos, Ricardo Caferra, Nicolas Peltier", "title": "Linear Temporal Logic and Propositional Schemata, Back and Forth\n  (extended version)", "comments": "Extended version of a paper submitted at TIME 2011: contains proofs,\n  additional examples & figures, additional comparison between classical\n  LTL/schemata algorithms up to the provided translations, and an example of\n  how to do model checking with schemata; 36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper relates the well-known Linear Temporal Logic with the logic of\npropositional schemata introduced by the authors. We prove that LTL is\nequivalent to a class of schemata in the sense that polynomial-time reductions\nexist from one logic to the other. Some consequences about complexity are\ngiven. We report about first experiments and the consequences about possible\nimprovements in existing implementations are analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Feb 2011 17:08:41 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 13:15:36 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Aravantinos", "Vincent", ""], ["Caferra", "Ricardo", ""], ["Peltier", "Nicolas", ""]]}, {"id": "1102.2336", "submitter": "Walter Quattrociocchi", "authors": "Walter Quattrociocchi, Rosaria Conte, Elena Lodi", "title": "Opinions within Media, Power and Gossip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing diffusion of the Internet technology, TV remains the\nprincipal medium of communication. People's perceptions, knowledge, beliefs and\nopinions about matter of facts get (in)formed through the information reported\non by the mass-media. However, a single source of information (and consensus)\ncould be a potential cause of anomalies in the structure and evolution of a\nsociety. Hence, as the information available (and the way it is reported) is\nfundamental for our perceptions and opinions, the definition of conditions\nallowing for a good information to be disseminated is a pressing challenge. In\nthis paper starting from a report on the last Italian political campaign in\n2008, we derive a socio-cognitive computational model of opinion dynamics where\nagents get informed by different sources of information. Then, a what-if\nanalysis, performed trough simulations on the model's parameters space, is\nshown. In particular, the scenario implemented includes three main streams of\ninformation acquisition, differing in both the contents and the perceived\nreliability of the messages spread. Agents' internal opinion is updated either\nby accessing one of the information sources, namely media and experts, or by\nexchanging information with one another. They are also endowed with cognitive\nmechanisms to accept, reject or partially consider the acquired information.\n", "versions": [{"version": "v1", "created": "Fri, 11 Feb 2011 12:20:40 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Quattrociocchi", "Walter", ""], ["Conte", "Rosaria", ""], ["Lodi", "Elena", ""]]}, {"id": "1102.2468", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Algorithmic Randomness as Foundation of Inductive Reasoning and\n  Artificial Intelligence", "comments": "9 LaTeX pages", "journal-ref": "Chapter 12 in Randomness through Computation: Some Answers, More\n  Questions (2011) pages 159-169", "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a brief personal account of the past, present, and future of\nalgorithmic randomness, emphasizing its role in inductive inference and\nartificial intelligence. It is written for a general audience interested in\nscience and philosophy. Intuitively, randomness is a lack of order or\npredictability. If randomness is the opposite of determinism, then algorithmic\nrandomness is the opposite of computability. Besides many other things, these\nconcepts have been used to quantify Ockham's razor, solve the induction\nproblem, and define intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 12 Feb 2011 01:48:49 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "1102.2524", "submitter": "Mark Levin", "authors": "Mark Sh. Levin, Rustem I. Nuriakhmetov", "title": "Multicriteria Steiner Tree Problem for Communication Network", "comments": "11 pages, 7 figures", "journal-ref": "Information Processes 9(3) (2009) 199-209", "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses combinatorial optimization scheme for solving the\nmulticriteria Steiner tree problem for communication network topology design\n(e.g., wireless mesh network). The solving scheme is based on several models:\nmulticriteria ranking, clustering, minimum spanning tree, and minimum Steiner\ntree problem. An illustrative numerical example corresponds to designing a\ncovering long-distance Wi-Fi network (static Ad-Hoc network). The set of\ncriteria (i.e., objective functions) involves the following: total cost, total\nedge length, overall throughput (capacity), and estimate of QoS. Obtained\ncomputing results show the suggested solving scheme provides good network\ntopologies which can be compared with minimum spanning trees.\n", "versions": [{"version": "v1", "created": "Sat, 12 Feb 2011 15:37:19 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Levin", "Mark Sh.", ""], ["Nuriakhmetov", "Rustem I.", ""]]}, {"id": "1102.2670", "submitter": "Yasin Abbasi-Yadkori Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori, David Pal, Csaba Szepesvari", "title": "Online Least Squares Estimation with Self-Normalized Processes: An\n  Application to Bandit Problems", "comments": "Submitted to the 24th Annual Conference on Learning Theory (COLT\n  2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of online least squares estimation is at the heart of many\nstochastic sequential decision making problems. We employ tools from the\nself-normalized processes to provide a simple and self-contained proof of a\ntail bound of a vector-valued martingale. We use the bound to construct a new\ntighter confidence sets for the least squares estimate.\n  We apply the confidence sets to several online decision problems, such as the\nmulti-armed and the linearly parametrized bandit problems. The confidence sets\nare potentially applicable to other problems such as sleeping bandits,\ngeneralized linear bandits, and other linear control problems.\n  We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of\nAuer et al. (2002) and show that its regret is with high-probability a problem\ndependent constant. In the case of linear bandits (Dani et al., 2008), we\nimprove the problem dependent bound in the dimension and number of time steps.\nFurthermore, as opposed to the previous result, we prove that our bound holds\nfor small sample sizes, and at the same time the worst case bound is improved\nby a logarithmic factor and the constant is improved.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 04:06:31 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Pal", "David", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1102.2738", "submitter": "Vyacheslav Yukalov", "authors": "V.I. Yukalov and D. Sornette", "title": "Decision Theory with Prospect Interference and Entanglement", "comments": "Latex file, 42 pages", "journal-ref": "Theor. Dec. 70 (2011) 283-328", "doi": null, "report-no": null, "categories": "math-ph cs.AI math.MP physics.soc-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel variant of decision making based on the mathematical\ntheory of separable Hilbert spaces. This mathematical structure captures the\neffect of superposition of composite prospects, including many incorporated\nintentions, which allows us to describe a variety of interesting fallacies and\nanomalies that have been reported to particularize the decision making of real\nhuman beings. The theory characterizes entangled decision making,\nnon-commutativity of subsequent decisions, and intention interference. We\ndemonstrate how the violation of the Savage's sure-thing principle, known as\nthe disjunction effect, can be explained quantitatively as a result of the\ninterference of intentions, when making decisions under uncertainty. The\ndisjunction effects, observed in experiments, are accurately predicted using a\ntheorem on interference alternation that we derive, which connects\naversion-to-uncertainty to the appearance of negative interference terms\nsuppressing the probability of actions. The conjunction fallacy is also\nexplained by the presence of the interference terms. A series of experiments\nare analysed and shown to be in excellent agreement with a priori evaluation of\ninterference effects. The conjunction fallacy is also shown to be a sufficient\ncondition for the disjunction effect and novel experiments testing the combined\ninterplay between the two effects are suggested.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 11:39:19 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Yukalov", "V. I.", ""], ["Sornette", "D.", ""]]}, {"id": "1102.2739", "submitter": "Sergey Tarasenko", "authors": "Sergey S. Tarasenko", "title": "A General Framework for Development of the Cortex-like Visual Object\n  Recognition System: Waves of Spikes, Predictive Coding and Universal\n  Dictionary of Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is focused on the development of the cortex-like visual object\nrecognition system. We propose a general framework, which consists of three\nhierarchical levels (modules). These modules functionally correspond to the V1,\nV4 and IT areas. Both bottom-up and top-down connections between the\nhierarchical levels V4 and IT are employed. The higher the degree of matching\nbetween the input and the preferred stimulus, the shorter the response time of\nthe neuron. Therefore information about a single stimulus is distributed in\ntime and is transmitted by the waves of spikes. The reciprocal connections and\nwaves of spikes implement predictive coding: an initial hypothesis is generated\non the basis of information delivered by the first wave of spikes and is tested\nwith the information carried by the consecutive waves. The development is\nconsidered as extraction and accumulation of features in V4 and objects in IT.\nOnce stored a feature can be disposed, if rarely activated. This cause update\nof feature repository. Consequently, objects in IT are also updated. This\nillustrates the growing process and dynamical change of topological structures\nof V4, IT and connections between these areas.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 11:40:08 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Tarasenko", "Sergey S.", ""]]}, {"id": "1102.2748", "submitter": "Yixiong Liang", "authors": "Yixiong Liang and Lei Wang and Yao Xiang and Beiji Zou", "title": "Feature Selection via Sparse Approximation for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by biological vision systems, the over-complete local features with\nhuge cardinality are increasingly used for face recognition during the last\ndecades. Accordingly, feature selection has become more and more important and\nplays a critical role for face data description and recognition. In this paper,\nwe propose a trainable feature selection algorithm based on the regularized\nframe for face recognition. By enforcing a sparsity penalty term on the minimum\nsquared error (MSE) criterion, we cast the feature selection problem into a\ncombinatorial sparse approximation problem, which can be solved by greedy\nmethods or convex relaxation methods. Moreover, based on the same frame, we\npropose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal\nsparse solution and the corresponding margin vector of the MSE criterion. The\nproposed methods are used for selecting the most informative Gabor features of\nface images for recognition and the experimental results on benchmark face\ndatabases demonstrate the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 12:05:47 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Liang", "Yixiong", ""], ["Wang", "Lei", ""], ["Xiang", "Yao", ""], ["Zou", "Beiji", ""]]}, {"id": "1102.2749", "submitter": "Yixiong Liang", "authors": "Yixiong Liang and Lingbo Liu and Ying Xu and Yao Xiang and Beiji Zou", "title": "Multi-task GLOH feature selection for human age estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel age estimation method based on GLOH feature\ndescriptor and multi-task learning (MTL). The GLOH feature descriptor, one of\nthe state-of-the-art feature descriptor, is used to capture the age-related\nlocal and spatial information of face image. As the exacted GLOH features are\noften redundant, MTL is designed to select the most informative feature bins\nfor age estimation problem, while the corresponding weights are determined by\nridge regression. This approach largely reduces the dimensions of feature,\nwhich can not only improve performance but also decrease the computational\nburden. Experiments on the public available FG-NET database show that the\nproposed method can achieve comparable performance over previous approaches\nwhile using much fewer features.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 12:12:56 GMT"}, {"version": "v2", "created": "Fri, 6 May 2011 17:19:24 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["Liang", "Yixiong", ""], ["Liu", "Lingbo", ""], ["Xu", "Ying", ""], ["Xiang", "Yao", ""], ["Zou", "Beiji", ""]]}, {"id": "1102.2984", "submitter": "Andreas Baldi", "authors": "Rjab Hajlaoui, Mariem Gzara, Abdelaziz Dammak", "title": "Hybrid Model for Solving Multi-Objective Problems Using Evolutionary\n  Algorithm and Tabu Search", "comments": "5 pages", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT),ISSN: 2221-0741,Vol. 1, No. 1, 5-9, Feb. 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new multi-objective hybrid model that makes cooperation\nbetween the strength of research of neighborhood methods presented by the tabu\nsearch (TS) and the important exploration capacity of evolutionary algorithm.\nThis model was implemented and tested in benchmark functions (ZDT1, ZDT2, and\nZDT3), using a network of computers.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 07:43:03 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Hajlaoui", "Rjab", ""], ["Gzara", "Mariem", ""], ["Dammak", "Abdelaziz", ""]]}, {"id": "1102.3129", "submitter": "Georg Moser", "authors": "Nao Hirokawa, Georg Moser", "title": "Automated Complexity Analysis Based on the Dependency Pair Method", "comments": "37 pages, submitted to Information & Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This article is concerned with automated complexity analysis of term rewrite\nsystems. Since these systems underlie much of declarative programming, time\ncomplexity of functions defined by rewrite systems is of particular interest.\nAmong other results, we present a variant of the dependency pair method for\nanalysing runtime complexities of term rewrite systems automatically. The\nestablished results significantly extent previously known techniques: we give\nexamples of rewrite systems subject to our methods that could previously not\nbeen analysed automatically. Furthermore, the techniques have been implemented\nin the Tyrolean Complexity Tool. We provide ample numerical data for assessing\nthe viability of the method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 17:16:23 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2011 18:19:59 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Hirokawa", "Nao", ""], ["Moser", "Georg", ""]]}, {"id": "1102.3680", "submitter": "Muralidhar Ravuri", "authors": "Muralidhar Ravuri", "title": "Foundations for Understanding and Building Conscious Systems using\n  Stable Parallel Looped Dynamics", "comments": "37 pages, 9 figures - revised for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of consciousness faced several challenges for a few reasons: (a)\na lack of necessary and sufficient conditions, without which we would not know\nhow close we are to the solution, (b) a lack of a synthesis framework to build\nconscious systems and (c) a lack of mechanisms explaining the transition\nbetween the lower-level chemical dynamics and the higher-level abstractions. In\nthis paper, I address these issues using a new framework. The central result is\nthat a person is 'minimally' conscious if and only if he knows at least one\ntruth. This lets us move away from the vagueness surrounding consciousness and\ninstead focus equivalently on: (i) what truths are and how our brain\nrepresents/relates them to each other and (ii) how we attain a feeling of\nknowing for a truth. For the former problem, since truths are things that do\nnot change, I replace the abstract notion with a dynamical one called fixed\nsets. These sets are guaranteed to exist for our brain and other stable\nparallel looped systems. The relationships between everyday events are now\nbuilt using relationships between fixed sets, until our brain creates a unique\ndynamical state called the self-sustaining threshold 'membrane' of fixed sets.\nFor the latter problem, I present necessary and sufficient conditions for\nattaining a feeling of knowing using a definition of continuity applied to\nabstractions. Combining these results, I now say that a person is minimally\nconscious if and only if his brain has a self-sustaining dynamical membrane\nwith abstract continuous paths. A synthetic system built to satisfy this\nequivalent self-sustaining membrane condition appears indistinguishable from\nhuman consciousness.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 20:29:17 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 19:15:09 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Ravuri", "Muralidhar", ""]]}, {"id": "1102.3830", "submitter": "Thomas Schoenemann", "authors": "Thomas Schoenemann, Fredrik Kahl, Simon Masnou and Daniel Cremers", "title": "A linear framework for region-based image segmentation and inpainting\n  involving curvature penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method to handle curvature regularity in region-based\nimage segmentation and inpainting that is independent of initialization.\n  To this end we start from a new formulation of length-based optimization\nschemes, based on surface continuation constraints, and discuss the connections\nto existing schemes. The formulation is based on a \\emph{cell complex} and\nconsiders basic regions and boundary elements. The corresponding optimization\nproblem is cast as an integer linear program.\n  We then show how the method can be extended to include curvature regularity,\nagain cast as an integer linear program. Here, we are considering pairs of\nboundary elements to reflect curvature. Moreover, a constraint set is derived\nto ensure that the boundary variables indeed reflect the boundary of the\nregions described by the region variables.\n  We show that by solving the linear programming relaxation one gets quite\nclose to the global optimum, and that curvature regularity is indeed much\nbetter suited in the presence of long and thin objects compared to standard\nlength regularity.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 13:20:10 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Schoenemann", "Thomas", ""], ["Kahl", "Fredrik", ""], ["Masnou", "Simon", ""], ["Cremers", "Daniel", ""]]}, {"id": "1102.3868", "submitter": "Valmir Barbosa", "authors": "Luis O. Rigo Jr, Valmir C. Barbosa", "title": "Evolved preambles for MAX-SAT heuristics", "comments": null, "journal-ref": "Proceedings of the International Conference on Evolutionary\n  Computation Theory and Applications, 23-31, 2011", "doi": "10.5220/0003660400230031", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MAX-SAT heuristics normally operate from random initial truth assignments to\nthe variables. We consider the use of what we call preambles, which are\nsequences of variables with corresponding single-variable assignment actions\nintended to be used to determine a more suitable initial truth assignment for a\ngiven problem instance and a given heuristic. For a number of well established\nMAX-SAT heuristics and benchmark instances, we demonstrate that preambles can\nbe evolved by a genetic algorithm such that the heuristics are outperformed in\na significant fraction of the cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 16:37:59 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Rigo", "Luis O.", "Jr"], ["Barbosa", "Valmir C.", ""]]}, {"id": "1102.3919", "submitter": "TaeHyun Hwang", "authors": "TaeHyun Hwang, Wei Zhang, Maoqiang Xie, Rui Kuang", "title": "Inferring Disease and Gene Set Associations with Rank Coherence in\n  Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational challenge to validate the candidate disease genes identified\nin a high-throughput genomic study is to elucidate the associations between the\nset of candidate genes and disease phenotypes. The conventional gene set\nenrichment analysis often fails to reveal associations between disease\nphenotypes and the gene sets with a short list of poorly annotated genes,\nbecause the existing annotations of disease causative genes are incomplete. We\npropose a network-based computational approach called rcNet to discover the\nassociations between gene sets and disease phenotypes. Assuming coherent\nassociations between the genes ranked by their relevance to the query gene set,\nand the disease phenotypes ranked by their relevance to the hidden target\ndisease phenotypes of the query gene set, we formulate a learning framework\nmaximizing the rank coherence with respect to the known disease phenotype-gene\nassociations. An efficient algorithm coupling ridge regression with label\npropagation, and two variants are introduced to find the optimal solution of\nthe framework. We evaluated the rcNet algorithms and existing baseline methods\nwith both leave-one-out cross-validation and a task of predicting recently\ndiscovered disease-gene associations in OMIM. The experiments demonstrated that\nthe rcNet algorithms achieved the best overall rankings compared to the\nbaselines. To further validate the reproducibility of the performance, we\napplied the algorithms to identify the target diseases of novel candidate\ndisease genes obtained from recent studies of GWAS, DNA copy number variation\nanalysis, and gene expression profiling. The algorithms ranked the target\ndisease of the candidate genes at the top of the rank list in many cases across\nall the three case studies. The rcNet algorithms are available as a webtool for\ndisease and gene set association analysis at\nhttp://compbio.cs.umn.edu/dgsa_rcNet.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 21:01:38 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Hwang", "TaeHyun", ""], ["Zhang", "Wei", ""], ["Xie", "Maoqiang", ""], ["Kuang", "Rui", ""]]}, {"id": "1102.4498", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Digraph description of k-interchange technique for optimization over\n  permutations and adaptive algorithm system", "comments": "11 pages, 6 figures", "journal-ref": "Foundations of Computing and Decision Sciences 26(3) (2001)\n  225-235", "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a general glance to the use of element exchange\ntechniques for optimization over permutations. A multi-level description of\nproblems is proposed which is a fundamental to understand nature and complexity\nof optimization problems over permutations (e.g., ordering, scheduling,\ntraveling salesman problem). The description is based on permutation\nneighborhoods of several kinds (e.g., by improvement of an objective function).\nOur proposed operational digraph and its kinds can be considered as a way to\nunderstand convexity and polynomial solvability for combinatorial optimization\nproblems over permutations. Issues of an analysis of problems and a design of\nhierarchical heuristics are discussed. The discussion leads to a multi-level\nadaptive algorithm system which analyzes an individual problem and\nselects/designs a solving strategy (trajectory).\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 13:27:40 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1102.4922", "submitter": "Minghao Yin", "authors": "Minghao Yin and Ping Huang", "title": "Counting Solutions of Constraint Satisfiability Problems:Exact Phase\n  Transitions and Approximate Algorithm", "comments": "submitted to AAAI-11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of phase transition phenomenon of NP complete problems plays an\nimportant role in understanding the nature of hard problems. In this paper, we\nfollow this line of research by considering the problem of counting solutions\nof Constraint Satisfaction Problems (#CSP). We consider the random model, i.e.\nRB model. We prove that phase transition of #CSP does exist as the number of\nvariables approaches infinity and the critical values where phase transitions\noccur are precisely located. Preliminary experimental results also show that\nthe critical point coincides with the theoretical derivation. Moreover, we\npropose an approximate algorithm to estimate the expectation value of the\nsolutions number of a given CSP instance of RB model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 08:07:54 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Yin", "Minghao", ""], ["Huang", "Ping", ""]]}, {"id": "1102.4924", "submitter": "Minghao Yin", "authors": "Junping Zhou, Minghao Yin", "title": "New Worst-Case Upper Bound for #XSAT", "comments": "submitted to AAAI-10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm running in O(1.1995n) is presented for counting models for exact\nsatisfiability formulae(#XSAT). This is faster than the previously best\nalgorithm which runs in O(1.2190n). In order to improve the efficiency of the\nalgorithm, a new principle, i.e. the common literals principle, is addressed to\nsimplify formulae. This allows us to eliminate more common literals. In\naddition, we firstly inject the resolution principles into solving #XSAT\nproblem, and therefore this further improves the efficiency of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 08:16:59 GMT"}], "update_date": "2011-02-25", "authors_parsed": [["Zhou", "Junping", ""], ["Yin", "Minghao", ""]]}, {"id": "1102.4925", "submitter": "Minghao Yin", "authors": "Minghao Yin", "title": "Worst-Case Upper Bound for (1, 2)-QSAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rigorous theoretical analysis of the algorithm for a subclass of QSAT,\ni.e. (1, 2)-QSAT, has been proposed in the literature. (1, 2)-QSAT, first\nintroduced in SAT'08, can be seen as quantified extended 2-CNF formulas. Until\nnow, within our knowledge, there exists no algorithm presenting the worst upper\nbound for (1, 2)-QSAT. Therefore in this paper, we present an exact algorithm\nto solve (1, 2)-QSAT. By analyzing the algorithms, we obtain a worst-case upper\nbound O(1.4142m), where m is the number of clauses.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 08:24:04 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2011 03:20:14 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Yin", "Minghao", ""]]}, {"id": "1102.4926", "submitter": "Minghao Yin", "authors": "Junping Zhou, Minghao Yin", "title": "New Worst-Case Upper Bound for X3SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rigorous theoretical analyses of algorithms for exact 3-satisfiability\n(X3SAT) have been proposed in the literature. As we know, previous algorithms\nfor solving X3SAT have been analyzed only regarding the number of variables as\nthe parameter. However, the time complexity for solving X3SAT instances depends\nnot only on the number of variables, but also on the number of clauses.\nTherefore, it is significant to exploit the time complexity from the other\npoint of view, i.e. the number of clauses. In this paper, we present algorithms\nfor solving X3SAT with rigorous complexity analyses using the number of clauses\nas the parameter. By analyzing the algorithms, we obtain the new worst-case\nupper bounds O(1.15855m), where m is the number of clauses.\n", "versions": [{"version": "v1", "created": "Thu, 24 Feb 2011 08:26:15 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2011 03:19:01 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Zhou", "Junping", ""], ["Yin", "Minghao", ""]]}, {"id": "1102.5185", "submitter": "Victor Gluzberg", "authors": "Victor Gluzberg", "title": "Universal Higher Order Grammar", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the class of languages that can be defined entirely in terms of\nprovability in an extension of the sorted type theory (Ty_n) by embedding the\nlogic of phonologies, without introduction of special types for syntactic\nentities. This class is proven to precisely coincide with the class of\nlogically closed languages that may be thought of as functions from expressions\nto sets of logically equivalent Ty_n terms. For a specific sub-class of\nlogically closed languages that are described by finite sets of rules or rule\nschemata, we find effective procedures for building a compact Ty_n\nrepresentation, involving a finite number of axioms or axiom schemata. The\nproposed formalism is characterized by some useful features unavailable in a\ntwo-component architecture of a language model. A further specialization and\nextension of the formalism with a context type enable effective account of\nintensional and dynamic semantics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Feb 2011 08:13:10 GMT"}], "update_date": "2011-02-28", "authors_parsed": [["Gluzberg", "Victor", ""]]}, {"id": "1102.5385", "submitter": "Martin Slota", "authors": "Martin Slota and Jo\\~ao Leite", "title": "Back and Forth Between Rules and SE-Models (Extended Version)", "comments": "25 pages; extended version of the paper accepted for LPNMR 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rules in logic programming encode information about mutual interdependencies\nbetween literals that is not captured by any of the commonly used semantics.\nThis information becomes essential as soon as a program needs to be modified or\nfurther manipulated.\n  We argue that, in these cases, a program should not be viewed solely as the\nset of its models. Instead, it should be viewed and manipulated as the set of\nsets of models of each rule inside it. With this in mind, we investigate and\nhighlight relations between the SE-model semantics and individual rules. We\nidentify a set of representatives of rule equivalence classes induced by\nSE-models, and so pinpoint the exact expressivity of this semantics with\nrespect to a single rule. We also characterise the class of sets of\nSE-interpretations representable by a single rule. Finally, we discuss the\nintroduction of two notions of equivalence, both stronger than strong\nequivalence [1] and weaker than strong update equivalence [2], which seem more\nsuitable whenever the dependency information found in rules is of interest.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 03:06:55 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2011 18:08:20 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Slota", "Martin", ""], ["Leite", "Jo\u00e3o", ""]]}, {"id": "1102.5451", "submitter": "Miroslav \\'Ciri\\'c", "authors": "Aleksandar Stamenkovi\\'c, Miroslav \\'Ciri\\'c, Jelena Ignjatovi\\'c", "title": "Reduction of fuzzy automata by means of fuzzy quasi-orders", "comments": "42 pages, submitted to Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our recent paper we have established close relationships between state\nreduction of a fuzzy recognizer and resolution of a particular system of fuzzy\nrelation equations. In that paper we have also studied reductions by means of\nthose solutions which are fuzzy equivalences. In this paper we will see that in\nsome cases better reductions can be obtained using the solutions of this system\nthat are fuzzy quasi-orders. Generally, fuzzy quasi-orders and fuzzy\nequivalences are equally good in the state reduction, but we show that right\nand left invariant fuzzy quasi-orders give better reductions than right and\nleft invariant fuzzy equivalences. We also show that alternate reductions by\nmeans of fuzzy quasi-orders give better results than alternate reductions by\nmeans of fuzzy equivalences. Furthermore we study a more general type of fuzzy\nquasi-orders, weakly right and left invariant ones, and we show that they are\nclosely related to determinization of fuzzy recognizers. We also demonstrate\nsome applications of weakly left invariant fuzzy quasi-orders in conflict\nanalysis of fuzzy discrete event systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 21:37:24 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Stamenkovi\u0107", "Aleksandar", ""], ["\u0106iri\u0107", "Miroslav", ""], ["Ignjatovi\u0107", "Jelena", ""]]}, {"id": "1102.5452", "submitter": "Miroslav \\'Ciri\\'c", "authors": "Miroslav \\'Ciri\\'c, Jelena Ignjatovi\\'c, Nada Damljanovi\\'c, Milan\n  Ba\\v{s}i\\'c", "title": "Bisimulations for fuzzy automata", "comments": "41 pages, submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bisimulations have been widely used in many areas of computer science to\nmodel equivalence between various systems, and to reduce the number of states\nof these systems, whereas uniform fuzzy relations have recently been introduced\nas a means to model the fuzzy equivalence between elements of two possible\ndifferent sets. Here we use the conjunction of these two concepts as a powerful\ntool in the study of equivalence between fuzzy automata. We prove that a\nuniform fuzzy relation between fuzzy automata $\\cal A$ and $\\cal B$ is a\nforward bisimulation if and only if its kernel and co-kernel are forward\nbisimulation fuzzy equivalences on $\\cal A$ and $\\cal B$ and there is a special\nisomorphism between factor fuzzy automata with respect to these fuzzy\nequivalences. As a consequence we get that fuzzy automata $\\cal A$ and $\\cal B$\nare UFB-equivalent, i.e., there is a uniform forward bisimulation between them,\nif and only if there is a special isomorphism between the factor fuzzy automata\nof $\\cal A$ and $\\cal B$ with respect to their greatest forward bisimulation\nfuzzy equivalences. This result reduces the problem of testing UFB-equivalence\nto the problem of testing isomorphism of fuzzy automata, which is closely\nrelated to the well-known graph isomorphism problem. We prove some similar\nresults for backward-forward bisimulations, and we point to fundamental\ndifferences. Because of the duality with the studied concepts, backward and\nforward-backward bisimulations are not considered separately. Finally, we give\na comprehensive overview of various concepts on deterministic,\nnondeterministic, fuzzy, and weighted automata, which are related to\nbisimulations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 21:37:51 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 20:11:04 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["\u0106iri\u0107", "Miroslav", ""], ["Ignjatovi\u0107", "Jelena", ""], ["Damljanovi\u0107", "Nada", ""], ["Ba\u0161i\u0107", "Milan", ""]]}, {"id": "1102.5549", "submitter": "Gagan Sidhu", "authors": "Gagan Sidhu", "title": "Instant Replay: Investigating statistical Analysis in Sports", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has had an unquestionable impact on the way people watch sports.\nAlong with this technological evolution has come a higher standard to ensure a\ngood viewing experience for the casual sports fan. It can be argued that the\npervasion of statistical analysis in sports serves to satiate the fan's desire\nfor detailed sports statistics. The goal of statistical analysis in sports is a\nsimple one: to eliminate subjective analysis. In this paper, we review previous\nwork that attempts to analyze various aspects in sports by using ideas from\nMarkov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods.\nThe unifying goal of these works is to achieve an accurate representation of\nthe player's ability, the sport, or the environmental effects on the player's\nperformance. With the prevalence of cheap computation, it is possible that\nusing techniques in Artificial Intelligence could improve the result of\nstatistical analysis in sport. This is best illustrated when evaluating\nfootball using Neuro Dynamic Programming, a Control Theory paradigm heavily\nbased on theory in Stochastic processes. The results from this method suggest\nthat statistical analysis in sports may benefit from using ideas from the area\nof Control Theory or Machine Learning\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 21:16:56 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 20:40:42 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2011 16:56:19 GMT"}, {"version": "v4", "created": "Tue, 11 Oct 2011 04:20:03 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Sidhu", "Gagan", ""]]}, {"id": "1102.5561", "submitter": "Andras Lorincz", "authors": "Gabor Matuz and Andras Lorincz", "title": "Decision Making Agent Searching for Markov Models in Near-Deterministic\n  World", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has solid foundations, but becomes inefficient in\npartially observed (non-Markovian) environments. Thus, a learning agent -born\nwith a representation and a policy- might wish to investigate to what extent\nthe Markov property holds. We propose a learning architecture that utilizes\ncombinatorial policy optimization to overcome non-Markovity and to develop\nefficient behaviors, which are easy to inherit, tests the Markov property of\nthe behavioral states, and corrects against non-Markovity by running a\ndeterministic factored Finite State Model, which can be learned. We illustrate\nthe properties of architecture in the near deterministic Ms. Pac-Man game. We\nanalyze the architecture from the point of view of evolutionary, individual,\nand social learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 23:47:13 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2011 07:35:59 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Matuz", "Gabor", ""], ["Lorincz", "Andras", ""]]}, {"id": "1102.5635", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger, Marc Sevaux", "title": "Practical inventory routing: A problem definition and an optimization\n  method", "comments": null, "journal-ref": "Proceedings of the EU/MEeting 2011 - Workshop on Client-Centered\n  Logistics and International Aid, February 21-22, 2011, pages 32-35", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global objective of this work is to provide practical optimization\nmethods to companies involved in inventory routing problems, taking into\naccount this new type of data. Also, companies are sometimes not able to deal\nwith changing plans every period and would like to adopt regular structures for\nserving customers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 10:42:29 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Sevaux", "Marc", ""]]}]