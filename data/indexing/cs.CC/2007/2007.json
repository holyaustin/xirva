[{"id": "2007.00204", "submitter": "Wenpin Tang", "authors": "Wenpin Tang", "title": "Learning an arbitrary mixture of two multinomial logits", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.LG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider mixtures of multinomial logistic models (MNL),\nwhich are known to $\\epsilon$-approximate any random utility model. Despite its\nlong history and broad use, rigorous results are only available for learning a\nuniform mixture of two MNLs. Continuing this line of research, we study the\nproblem of learning an arbitrary mixture of two MNLs. We show that the\nidentifiability of the mixture models may only fail on an algebraic variety of\na negligible measure. This is done by reducing the problem of learning a\nmixture of two MNLs to the problem of solving a system of univariate quartic\nequations. We also devise an algorithm to learn any mixture of two MNLs using a\npolynomial number of samples and a linear number of queries, provided that a\nmixture of two MNLs over some finite universe is identifiable. Several\nnumerical experiments and conjectures are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:33:52 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 09:28:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tang", "Wenpin", ""]]}, {"id": "2007.00373", "submitter": "Juanping Zhu", "authors": "Juanping Zhu, Hairong Gu", "title": "Can Global Optimization Strategy Outperform Myopic Strategy for Bayesian\n  Parameter Estimation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian adaptive inference is widely used in psychophysics to estimate\npsychometric parameters. Most applications used myopic one-step ahead strategy\nwhich only optimizes the immediate utility. The widely held expectation is that\nglobal optimization strategies that explicitly optimize over some horizon can\nlargely improve the performance of the myopic strategy. With limited studies\nthat compared myopic and global strategies, the expectation was not challenged\nand researchers are still investing heavily to achieve global optimization. Is\nthat really worthwhile? This paper provides a discouraging answer based on\nexperimental simulations comparing the performance improvement and computation\nburden between global and myopic strategies in parameter estimation of multiple\nmodels. The finding is that the added horizon in global strategies has\nnegligible contributions to the improvement of optimal global utility other\nthan the most immediate next steps (of myopic strategy). Mathematical recursion\nis derived to prove that the contribution of utility improvement of each added\nhorizon step diminishes fast as that step moves further into the future.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:31:16 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhu", "Juanping", ""], ["Gu", "Hairong", ""]]}, {"id": "2007.00512", "submitter": "Zeyu Guo", "authors": "Zeyu Guo", "title": "Factoring Polynomials over Finite Fields with Linear Galois Groups: An\n  Additive Combinatorics Approach", "comments": "To be published in the proceedings of MFCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\tilde{f}(X)\\in\\mathbb{Z}[X]$ be a degree-$n$ polynomial such that\n$f(X):=\\tilde{f}(X)\\bmod p$ factorizes into $n$ distinct linear factors over\n$\\mathbb{F}_p$. We study the problem of deterministically factoring $f(X)$ over\n$\\mathbb{F}_p$ given $\\tilde{f}(X)$. Under the generalized Riemann hypothesis\n(GRH), we give an improved deterministic algorithm that computes the complete\nfactorization of $f(X)$ in the case that the Galois group of $\\tilde{f}(X)$ is\n(permutation isomorphic to) a linear group $G\\leq \\mathrm{GL}(V)$ on the set\n$S$ of roots of $\\tilde{f}(X)$, where $V$ is a finite-dimensional vector space\nover a finite field $\\mathbb{F}$ and $S$ is identified with a subset of $V$. In\nparticular, when $|S|=|V|^{\\Omega(1)}$, the algorithm runs in time polynomial\nin $n^{\\log n/(\\log\\log\\log\\log n)^{1/3}}$ and the size of the input, improving\nEvdokimov's algorithm. Our result also applies to a general Galois group $G$\nwhen combined with a recent algorithm of the author.\n  To prove our main result, we introduce a family of objects called linear\n$m$-schemes and reduce the problem of factoring $f(X)$ to a combinatorial\nproblem about these objects. We then apply techniques from additive\ncombinatorics to obtain an improved bound. Our techniques may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:22:31 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 02:20:21 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Guo", "Zeyu", ""]]}, {"id": "2007.00662", "submitter": "Andrew Guo", "authors": "Andrew Y. Guo, Abhinav Deshpande, Su-Kuan Chu, Zachary Eldredge,\n  Przemyslaw Bienias, Dhruv Devulapalli, Yuan Su, Andrew M. Childs, and Alexey\n  V. Gorshkov", "title": "Implementing a Fast Unbounded Quantum Fanout Gate Using Power-Law\n  Interactions", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard circuit model for quantum computation presumes the ability to\ndirectly perform gates between arbitrary pairs of qubits, which is unlikely to\nbe practical for large-scale experiments. Power-law interactions with strength\ndecaying as $1/r^\\alpha$ in the distance $r$ provide an experimentally\nrealizable resource for information processing, whilst still retaining\nlong-range connectivity. We leverage the power of these interactions to\nimplement a fast quantum fanout gate with an arbitrary number of targets. Our\nimplementation allows the quantum Fourier transform (QFT) and Shor's algorithm\nto be performed on a $D$-dimensional lattice in time logarithmic in the number\nof qubits for interactions with $\\alpha \\le D$. As a corollary, we show that\npower-law systems with $\\alpha \\le D$ are difficult to simulate classically\neven for short times, under a standard assumption that factoring is classically\nintractable. Complementarily, we develop a new technique to give a general\nlower bound, linear in the size of the system, on the time required to\nimplement the QFT and the fanout gate in systems that are constrained by a\nlinear light cone. This allows us to prove an asymptotically tighter lower\nbound for long-range systems than is possible with previously available\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 18:00:00 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Guo", "Andrew Y.", ""], ["Deshpande", "Abhinav", ""], ["Chu", "Su-Kuan", ""], ["Eldredge", "Zachary", ""], ["Bienias", "Przemyslaw", ""], ["Devulapalli", "Dhruv", ""], ["Su", "Yuan", ""], ["Childs", "Andrew M.", ""], ["Gorshkov", "Alexey V.", ""]]}, {"id": "2007.00870", "submitter": "Kuan Cheng", "authors": "Kuan Cheng and Xin Li", "title": "Efficient Document Exchange and Error Correcting Codes with Asymmetric\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two fundamental problems in communication, Document Exchange (DE)\nand Error Correcting Code (ECC). In the first problem, two parties hold two\nstrings, and one party tries to learn the other party's string through\ncommunication. In the second problem, one party tries to send a message to\nanother party through a noisy channel, by adding some redundant information to\nprotect the message. Two important goals in both problems are to minimize the\ncommunication complexity or redundancy, and to design efficient protocols or\ncodes.\n  Both problems have been studied extensively. In this paper we study whether\nasymmetric partial information can help in these two problems. We focus on the\ncase of Hamming distance/errors, and the asymmetric partial information is\nmodeled by one party having a vector of disjoint subsets $\\vec{S}=(S_1, \\cdots,\nS_t)$ of indices and a vector of integers $\\vec{k}=(k_1, \\cdots, k_t)$, such\nthat in each $S_i$ the Hamming distance/errors is at most $k_i$. We establish\nboth lower bounds and upper bounds in this model, and provide efficient\nrandomized constructions that achieve a $\\min\\lbrace O(t^2), O\\left((\\log \\log\nn)^2\\right) \\rbrace $ factor within the optimum, with almost linear running\ntime.\n  We further show a connection between the above document exchange problem and\nthe problem of document exchange under edit distance, and use our techniques to\ngive an efficient randomized protocol with optimal communication complexity and\n\\emph{exponentially} small error for the latter. This improves the previous\nresult by Haeupler \\cite{haeupler2018optimal} (FOCS'19) and that by Belazzougui\nand Zhang \\cite{BelazzouguiZ16} (FOCS'16). Our techniques are based on a\ngeneralization of the celebrated expander codes by Sipser and Spielman\n\\cite{sipser1996expander}, which may be of independent interests.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:31:54 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 00:19:13 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 10:15:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cheng", "Kuan", ""], ["Li", "Xin", ""]]}, {"id": "2007.00912", "submitter": "Marius Costandin", "authors": "Marius-Simion Costandin, Bogdan Gavrea and Beniamin Costandin", "title": "Maximizing The Distance To A \"Far Enough\" Point Over The Intersection Of\n  Hyper-Disks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel feasibility criteria for the finite intersection of convex\nsets given by inequalities. This criteria allows us to easily assert the\nfeasibility by analyzing the unconstrained minimum of a speci?fic convex\nfunction, that we form with the given sets. Next an algorithm is presented\nwhich extends the idea to a particular non-convex case: assert the inclusion of\nthe fi?nite intersection of a set of hyper-disks with equal radii in another\nhyper-disk with a different radius.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:37:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 20:00:32 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Costandin", "Marius-Simion", ""], ["Gavrea", "Bogdan", ""], ["Costandin", "Beniamin", ""]]}, {"id": "2007.01147", "submitter": "Paul Rolland", "authors": "Paul Rolland, Armin Eftekhari, Ali Kavis and Volkan Cevher", "title": "Double-Loop Unadjusted Langevin Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known first-order method for sampling from log-concave probability\ndistributions is the Unadjusted Langevin Algorithm (ULA). This work proposes a\nnew annealing step-size schedule for ULA, which allows to prove new convergence\nguarantees for sampling from a smooth log-concave distribution, which are not\ncovered by existing state-of-the-art convergence guarantees. To establish this\nresult, we derive a new theoretical bound that relates the Wasserstein distance\nto total variation distance between any two log-concave distributions that\ncomplements the reach of Talagrand T2 inequality. Moreover, applying this new\nstep size schedule to an existing constrained sampling algorithm, we show\nstate-of-the-art convergence rates for sampling from a constrained log-concave\ndistribution, as well as improved dimension dependence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:31:04 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Rolland", "Paul", ""], ["Eftekhari", "Armin", ""], ["Kavis", "Ali", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.01673", "submitter": "Cl\\'ement Dallard", "authors": "Valentin Bartier, Nicolas Bousquet, Cl\\'ement Dallard, Kyle Lomer,\n  Amer E. Mouawad", "title": "On girth and the parameterized complexity of token sliding and token\n  jumping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Token Jumping problem we are given a graph $G = (V,E)$ and two\nindependent sets $S$ and $T$ of $G$, each of size $k \\geq 1$. The goal is to\ndetermine whether there exists a sequence of $k$-sized independent sets in $G$,\n$\\langle S_0, S_1, \\ldots, S_\\ell \\rangle$, such that for every $i$, $|S_i| =\nk$, $S_i$ is an independent set, $S = S_0$, $S_\\ell = T$, and $|S_i \\Delta\nS_{i+1}| = 2$. In other words, if we view each independent set as a collection\nof tokens placed on a subset of the vertices of $G$, then the problem asks for\na sequence of independent sets which transforms $S$ to $T$ by individual token\njumps which maintain the independence of the sets. This problem is known to be\nPSPACE-complete on very restricted graph classes, e.g., planar bounded degree\ngraphs and graphs of bounded bandwidth. A closely related problem is the Token\nSliding problem, where instead of allowing a token to jump to any vertex of the\ngraph we instead require that a token slides along an edge of the graph. Token\nSliding is also known to be PSPACE-complete on the aforementioned graph\nclasses. We investigate the parameterized complexity of both problems on\nseveral graph classes, focusing on the effect of excluding certain cycles from\nthe input graph. In particular, we show that both Token Sliding and Token\nJumping are fixed-parameter tractable on $C_4$-free bipartite graphs when\nparameterized by $k$. For Token Jumping, we in fact show that the problem\nadmits a polynomial kernel on $\\{C_3,C_4\\}$-free graphs. In the case of Token\nSliding, we also show that the problem admits a polynomial kernel on bipartite\ngraphs of bounded degree. We believe both of these results to be of independent\ninterest. We complement these positive results by showing that, for any\nconstant $p \\geq 4$, both problems are W[1]-hard on $\\{C_4, \\dots, C_p\\}$-free\ngraphs and Token Sliding remains W[1]-hard even on bipartite graphs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:28:30 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 12:36:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Bartier", "Valentin", ""], ["Bousquet", "Nicolas", ""], ["Dallard", "Cl\u00e9ment", ""], ["Lomer", "Kyle", ""], ["Mouawad", "Amer E.", ""]]}, {"id": "2007.01779", "submitter": "Stanislav Zivny", "authors": "Caterina Viola and Stanislav Zivny", "title": "The combined basic LP and affine IP relaxation for promise VCSPs on\n  infinite domains", "comments": "Full version of an MFCS'20 paper", "journal-ref": "ACM Transactions on Algorithms 17(3) Article No. 21 (2021)", "doi": "10.1145/3458041", "report-no": null, "categories": "cs.CC cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex relaxations have been instrumental in solvability of constraint\nsatisfaction problems (CSPs), as well as in the three different generalisations\nof CSPs: valued CSPs, infinite-domain CSPs, and most recently promise CSPs. In\nthis work, we extend an existing tractability result to the three\ngeneralisations of CSPs combined: We give a sufficient condition for the\ncombined basic linear programming and affine integer programming relaxation for\nexact solvability of promise valued CSPs over infinite-domains. This extends a\nresult of Brakensiek and Guruswami [SODA'20] for promise (non-valued) CSPs (on\nfinite domains).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:06:43 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 15:12:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Viola", "Caterina", ""], ["Zivny", "Stanislav", ""]]}, {"id": "2007.02330", "submitter": "Marius Zimand", "authors": "Bruno Bauwens and Marius Zimand", "title": "Universal codes in the shared-randomness model for channels with general\n  distortion capabilities", "comments": "Removed the mentioning of online matching, which is not used here", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forth new models for universal channel coding. Unlike standard codes\nwhich are designed for a specific type of channel, our most general universal\ncode makes communication resilient on every channel, provided the noise level\nis below the tolerated bound, where the noise level t of a channel is the\nlogarithm of its ambiguity (the maximum number of strings that can be distorted\ninto a given one). The other more restricted universal codes still work for\nlarge classes of natural channels. In a universal code, encoding is\nchannel-independent, but the decoding function knows the type of channel. We\nallow the encoding and the decoding functions to share randomness, which is\nunavailable to the channel. There are two scenarios for the type of attack that\na channel can perform. In the oblivious scenario, codewords belong to an\nadditive group and the channel distorts a codeword by adding a vector from a\nfixed set. The selection is based on the message and the encoding function, but\nnot on the codeword. In the Hamming scenario, the channel knows the codeword\nand is fully adversarial. For a universal code, there are two parameters of\ninterest: the rate, which is the ratio between the message length k and the\ncodeword length n, and the number of shared random bits. We show the existence\nin both scenarios of universal codes with rate 1-t/n - o(1), which is optimal\nmodulo the o(1) term. The number of shared random bits is O(log n) in the\noblivious scenario, and O(n) in the Hamming scenario, which, for typical values\nof the noise level, we show to be optimal, modulo the constant hidden in the\nO() notation. In both scenarios, the universal encoding is done in time\npolynomial in n, but the channel-dependent decoding procedures are in general\nnot efficient. For some weaker classes of channels we construct universal codes\nwith polynomial-time encoding and decoding.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 13:05:14 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 22:28:09 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 20:24:28 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 14:57:43 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bauwens", "Bruno", ""], ["Zimand", "Marius", ""]]}, {"id": "2007.02370", "submitter": "Adel Nabli", "authors": "Adel Nabli, Margarida Carvalho, Pierre Hosteins", "title": "Complexity of the Multilevel Critical Node Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze a sequential game played in a graph called the\nMultilevel Critical Node problem (MCN). A defender and an attacker are the\nplayers of this game. The defender starts by preventively interdicting vertices\n(vaccination) from being attacked. Then, the attacker infects a subset of\nnon-vaccinated vertices and, finally, the defender reacts with a protection\nstrategy. We provide the first computational complexity results associated with\nMCN and its subgames. Moreover, by considering unitary, weighted, undirected,\nand directed graphs, we clarify how the theoretical tractability of those\nproblems vary. Our findings contribute with new NP-complete,\n$\\Sigma_2^p$-complete and $\\Sigma_3^p$-complete problems. Furthermore, for the\nlast level of the game, the protection stage, we build polynomial time\nalgorithms for certain graph classes.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:54:53 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 14:22:48 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Nabli", "Adel", ""], ["Carvalho", "Margarida", ""], ["Hosteins", "Pierre", ""]]}, {"id": "2007.02431", "submitter": "Xinyu Wu", "authors": "Xinyu Wu", "title": "A stochastic calculus approach to the oracle separation of BQP and PH", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After presentations of Raz and Tal's oracle separation of BQP and PH result,\nseveral people (e.g. Ryan O'Donnell, James Lee, Avishay Tal) suggested that the\nproof may be simplified by stochastic calculus. In this short note, we describe\nsuch a simplification.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 19:39:38 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wu", "Xinyu", ""]]}, {"id": "2007.02509", "submitter": "Erhan Oztop", "authors": "Erhan Oztop and Minoru Asada", "title": "On the weight and density bounds of polynomial threshold functions", "comments": "To be submitted to SIAM Journal on Discrete Mathematics or Discrete\n  Mathematics. Main author: Erhan Oztop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we show that all n-variable Boolean function can be\nrepresented as polynomial threshold functions (PTF) with at most $0.75 \\times\n2^n$ non-zero integer coefficients and give an upper bound on the absolute\nvalue of these coefficients. To our knowledge this provides the best known\nbound on both the PTF density (number of monomials) and weight (sum of the\ncoefficient magnitudes) of general Boolean functions. The special case of Bent\nfunctions is also analyzed and shown that any n-variable Bent function can be\nrepresented with integer coefficients less than $2^n$ while also obeying the\naforementioned density bound. Finally, sparse Boolean functions, which are\nalmost constant except for $m << 2^n$ number of variable assignments, are shown\nto have small weight PTFs with density at most $m+2^{n-1}$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:16:13 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Oztop", "Erhan", ""], ["Asada", "Minoru", ""]]}, {"id": "2007.02533", "submitter": "Ahmed Sunny", "authors": "Lin Chen (1), Ahmed Sunny (1), Lei Xu (2), Shouhuai Xu (3), Zhimin Gao\n  (4), Yang Lu (5), Weidong Shi (5) and Nolan Shah (6) ((1) Texas Tech\n  University, (2) University of Texas Rio Grande Valley, (3) University of\n  Texas San Antonio, (4) Auburn University at Montgomery, (5) University of\n  Houston, (6) Amazon Web Services)", "title": "Computational Complexity Characterization of Protecting Elections from\n  Bribery", "comments": "28 Pages. The Article has been accepted in the 26th International\n  Computing and Combinatorics Conference (COCOON 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bribery problem in election has received considerable attention in the\nliterature, upon which various algorithmic and complexity results have been\nobtained. It is thus natural to ask whether we can protect an election from\npotential bribery. We assume that the protector can protect a voter with some\ncost (e.g., by isolating the voter from potential bribers). A protected voter\ncannot be bribed. Under this setting, we consider the following bi-level\ndecision problem: Is it possible for the protector to protect a proper subset\nof voters such that no briber with a fixed budget on bribery can alter the\nelection result? The goal of this paper is to give a full picture on the\ncomplexity of protection problems. We give an extensive study on the protection\nproblem and provide algorithmic and complexity results. Comparing our results\nwith that on the bribery problems, we observe that the protection problem is in\ngeneral significantly harder. Indeed, it becomes $\\sum_{p}^2$-complete even for\nvery restricted special cases, while most bribery problems lie in NP. However,\nit is not necessarily the case that the protection problem is always harder.\nSome of the protection problems can still be solved in polynomial time, while\nsome of them remain as hard as the bribery problem under the same setting.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 05:40:17 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Lin", ""], ["Sunny", "Ahmed", ""], ["Xu", "Lei", ""], ["Xu", "Shouhuai", ""], ["Gao", "Zhimin", ""], ["Lu", "Yang", ""], ["Shi", "Weidong", ""], ["Shah", "Nolan", ""]]}, {"id": "2007.02730", "submitter": "Pierre-Jean Spaenlehauer", "authors": "Aude Le Gluher and Pierre-Jean Spaenlehauer and Emmanuel Thom\\'e", "title": "Refined Analysis of the Asymptotic Complexity of the Number Field Sieve", "comments": "Accepted for publication in Mathematical Cryptology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical heuristic complexity of the Number Field Sieve (NFS) is the\nsolution of an optimization problem that involves an unknown function, usually\nnoted $o(1)$ and called $\\xi(N)$ throughout this paper, which tends to zero as\nthe entry $N$ grows. The aim of this paper is to find optimal asymptotic\nchoices of the parameters of NFS as $N$ grows, in order to minimize its\nheuristic asymptotic computational cost. This amounts to minimizing a function\nof the parameters of NFS bound together by a non-linear constraint. We provide\nprecise asymptotic estimates of the minimizers of this optimization problem,\nwhich yield refined formulas for the asymptotic complexity of NFS. One of the\nmain outcomes of this analysis is that $\\xi(N)$ has a very slow rate of\nconvergence: We prove that it is equivalent to\n$4{\\log}{\\log}{\\log}\\,N/(3{\\log}{\\log}\\,N)$. Moreover, $\\xi(N)$ has an\nunpredictable behavior for practical estimates of the complexity. Indeed, we\nprovide an asymptotic series expansion of $\\xi$ and numerical experiments\nindicate that this series starts converging only for $N>\\exp(\\exp(25))$, far\nbeyond the practical range of NFS. This raises doubts on the relevance of NFS\nrunning time estimates that are based on setting $\\xi=0$ in the asymptotic\nformula.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:10:01 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:32:28 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gluher", "Aude Le", ""], ["Spaenlehauer", "Pierre-Jean", ""], ["Thom\u00e9", "Emmanuel", ""]]}, {"id": "2007.02740", "submitter": "Or Meir", "authors": "Susanna F. de Rezende, Or Meir, Jakob Nordstr\\\"om, Toniann Pitassi,\n  Robert Robere", "title": "KRW Composition Theorems via Lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major open problems in complexity theory is proving\nsuper-logarithmic lower bounds on the depth of circuits (i.e.,\n$\\mathbf{P}\\not\\subseteq\\mathbf{NC}^1$). Karchmer, Raz, and Wigderson\n(Computational Complexity 5(3/4), 1995) suggested to approach this problem by\nproving that depth complexity behaves \"as expected\" with respect to the\ncomposition of functions $f\\diamond g$. They showed that the validity of this\nconjecture would imply that $\\mathbf{P}\\not\\subseteq\\mathbf{NC}^1$.\n  Several works have made progress toward resolving this conjecture by proving\nspecial cases. In particular, these works proved the KRW conjecture for every\nouter function $f$, but only for few inner functions $g$. Thus, it is an\nimportant challenge to prove the KRW conjecture for a wider range of inner\nfunctions.\n  In this work, we extend significantly the range of inner functions that can\nbe handled. First, we consider the $\\textit{monotone}$ version of the KRW\nconjecture. We prove it for every monotone inner function $g$ whose depth\ncomplexity can be lower bounded via a query-to-communication lifting theorem.\nThis allows us to handle several new and well-studied functions such as the\n$s\\textbf{-}t$-connectivity, clique, and generation functions.\n  In order to carry this progress back to the $\\textit{non-monotone}$ setting,\nwe introduce a new notion of $\\textit{semi-monotone}$ composition, which\ncombines the non-monotone complexity of the outer function $f$ with the\nmonotone complexity of the inner function $g$. In this setting, we prove the\nKRW conjecture for a similar selection of inner functions $g$, but only for a\nspecific choice of the outer function $f$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:22:23 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 20:37:19 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["de Rezende", "Susanna F.", ""], ["Meir", "Or", ""], ["Nordstr\u00f6m", "Jakob", ""], ["Pitassi", "Toniann", ""], ["Robere", "Robert", ""]]}, {"id": "2007.03402", "submitter": "Yixin Shen", "authors": "Andris Ambainis, Kaspars Balodis, J\\=anis Iraids, Kamil Khadiev,\n  Vladislavs K\\c{l}evickis, Kri\\v{s}j\\=anis Pr\\=usis, Yixin Shen, Juris\n  Smotrovs, Jevg\\=enijs Vihrovs", "title": "Quantum Lower and Upper Bounds for 2D-Grid and Dyck Language", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.12638", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantum query complexity of two problems.\n  First, we consider the problem of determining if a sequence of parentheses is\na properly balanced one (a Dyck word), with a depth of at most $k$. We call\nthis the $Dyck_{k,n}$ problem. We prove a lower bound of $\\Omega(c^k\n\\sqrt{n})$, showing that the complexity of this problem increases exponentially\nin $k$. Here $n$ is the length of the word. When $k$ is a constant, this is\ninteresting as a representative example of star-free languages for which a\nsurprising $\\tilde{O}(\\sqrt{n})$ query quantum algorithm was recently\nconstructed by Aaronson et al. Their proof does not give rise to a general\nalgorithm. When $k$ is not a constant, $Dyck_{k,n}$ is not context-free. We\ngive an algorithm with $O\\left(\\sqrt{n}(\\log{n})^{0.5k}\\right)$ quantum queries\nfor $Dyck_{k,n}$ for all $k$. This is better than the trival upper bound $n$\nfor $k=o\\left(\\frac{\\log(n)}{\\log\\log n}\\right)$.\n  Second, we consider connectivity problems on grid graphs in 2 dimensions, if\nsome of the edges of the grid may be missing. By embedding the \"balanced\nparentheses\" problem into the grid, we show a lower bound of\n$\\Omega(n^{1.5-\\epsilon})$ for the directed 2D grid and\n$\\Omega(n^{2-\\epsilon})$ for the undirected 2D grid. The directed problem is\ninteresting as a black-box model for a class of classical dynamic programming\nstrategies including the one that is usually used for the well-known edit\ndistance problem. We also show a generalization of this result to more than 2\ndimensions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:51:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 10:37:53 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ambainis", "Andris", ""], ["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""], ["Khadiev", "Kamil", ""], ["K\u013cevickis", "Vladislavs", ""], ["Pr\u016bsis", "Kri\u0161j\u0101nis", ""], ["Shen", "Yixin", ""], ["Smotrovs", "Juris", ""], ["Vihrovs", "Jevg\u0113nijs", ""]]}, {"id": "2007.03631", "submitter": "Uma Girish", "authors": "Uma Girish, Ran Raz, Wei Zhan", "title": "Lower Bounds for XOR of Forrelations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Forrelation problem, introduced by Aaronson [A10] and Aaronson and\nAmbainis [AA15], is a well studied problem in the context of separating quantum\nand classical models. Variants of this problem were used to give exponential\nseparations between quantum and classical query complexity [A10, AA15]; quantum\nquery complexity and bounded-depth circuits [RT19]; and quantum and classical\ncommunication complexity [GRT19]. In all these separations, the lower bound for\nthe classical model only holds when the advantage of the protocol (over a\nrandom guess) is more than $\\approx 1/\\sqrt{N}$, that is, the success\nprobability is larger than $\\approx 1/2 + 1/\\sqrt{N}$. To achieve separations\nwhen the classical protocol has smaller advantage, we study in this work the\nXOR of $k$ independent copies of the Forrelation function (where $k\\ll N$). We\nprove a very general result that shows that any family of Boolean functions\nthat is closed under restrictions, whose Fourier mass at level $2k$ is bounded\nby $\\alpha^k$, cannot compute the XOR of $k$ independent copies of the\nForrelation function with advantage better than\n$O\\left(\\frac{\\alpha^k}{{N^{k/2}}}\\right)$. This is a strengthening of a result\nof [CHLT19], that gave a similar result for $k=1$, using the technique of\n[RT19]. As an application of our result, we give the first example of a partial\nBoolean function that can be computed by a simultaneous-message quantum\nprotocol of cost $\\mbox{polylog}(N)$ (when players share $\\mbox{polylog}(N)$\nEPR pairs), however, any classical interactive randomized protocol of cost at\nmost $\\tilde{o}(N^{1/4})$, has quasipolynomially small advantage over a random\nguess. We also give the first example of a partial Boolean function that has a\nquantum query algorithm of cost $\\mbox{polylog}(N)$, and such that, any\nconstant-depth circuit of quasipolynomial size has quasipolynomially small\nadvantage over a random guess.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:05:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Girish", "Uma", ""], ["Raz", "Ran", ""], ["Zhan", "Wei", ""]]}, {"id": "2007.03633", "submitter": "Collin Burns", "authors": "Alexandr Andoni, Collin Burns, Yi Li, Sepideh Mahabadi, David P.\n  Woodruff", "title": "Streaming Complexity of SVMs", "comments": "APPROX 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the space complexity of solving the bias-regularized SVM problem in\nthe streaming model. This is a classic supervised learning problem that has\ndrawn lots of attention, including for developing fast algorithms for solving\nthe problem approximately. One of the most widely used algorithms for\napproximately optimizing the SVM objective is Stochastic Gradient Descent\n(SGD), which requires only $O(\\frac{1}{\\lambda\\epsilon})$ random samples, and\nwhich immediately yields a streaming algorithm that uses\n$O(\\frac{d}{\\lambda\\epsilon})$ space. For related problems, better streaming\nalgorithms are only known for smooth functions, unlike the SVM objective that\nwe focus on in this work. We initiate an investigation of the space complexity\nfor both finding an approximate optimum of this objective, and for the related\n``point estimation'' problem of sketching the data set to evaluate the function\nvalue $F_\\lambda$ on any query $(\\theta, b)$. We show that, for both problems,\nfor dimensions $d=1,2$, one can obtain streaming algorithms with space\npolynomially smaller than $\\frac{1}{\\lambda\\epsilon}$, which is the complexity\nof SGD for strongly convex functions like the bias-regularized SVM, and which\nis known to be tight in general, even for $d=1$. We also prove polynomial lower\nbounds for both point estimation and optimization. In particular, for point\nestimation we obtain a tight bound of $\\Theta(1/\\sqrt{\\epsilon})$ for $d=1$ and\na nearly tight lower bound of $\\widetilde{\\Omega}(d/{\\epsilon}^2)$ for $d =\n\\Omega( \\log(1/\\epsilon))$. Finally, for optimization, we prove a\n$\\Omega(1/\\sqrt{\\epsilon})$ lower bound for $d = \\Omega( \\log(1/\\epsilon))$,\nand show similar bounds when $d$ is constant.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:10:00 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Andoni", "Alexandr", ""], ["Burns", "Collin", ""], ["Li", "Yi", ""], ["Mahabadi", "Sepideh", ""], ["Woodruff", "David P.", ""]]}, {"id": "2007.03829", "submitter": "Mingyu Xiao", "authors": "Huairui Chu, Mingyu Xiao and Zhe Zhang", "title": "An Improved Upper Bound for SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the CNF satisfiability problem can be solved $O^*(1.2226^m)$\ntime, where $m$ is the number of clauses in the formula, improving the known\nupper bounds $O^*(1.234^m)$ given by Yamamoto 15 years ago and $O^*(1.239^m)$\ngiven by Hirsch 22 years ago. By using an amortized technique and careful case\nanalysis, we successfully avoid the bottlenecks in previous algorithms and get\nthe improvement.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:18:49 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Chu", "Huairui", ""], ["Xiao", "Mingyu", ""], ["Zhang", "Zhe", ""]]}, {"id": "2007.03867", "submitter": "Jonni Virtema", "authors": "Miika Hannula, Juha Kontinen, Martin L\\\"uck and Jonni Virtema", "title": "On the Complexity of Horn and Krom Fragments of Second-Order Boolean\n  Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order Boolean logic is a generalization of QBF, whose constant\nalternation fragments are known to be complete for the levels of the\nexponential time hierarchy. We consider two types of restriction of this logic:\n1) restrictions to term constructions, 2) restrictions to the form of the\nBoolean matrix. Of the first sort, we consider two kinds of restrictions:\nfirstly, disallowing nested use of proper function variables, and secondly\nstipulating that each function variable must appear with a fixed sequence of\narguments. Of the second sort, we consider Horn, Krom, and core fragments of\nthe Boolean matrix. We classify the complexity of logics obtained by combining\nthese two types of restrictions. We show that, in most cases, logics with k\nalternating blocks of function quantifiers are complete for the kth or (k-1)th\nlevel of the exponential time hierarchy. Furthermore, we establish\nNL-completeness for the Krom and core fragments, when k=1 and both restrictions\nof the first sort are in effect.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:50:43 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hannula", "Miika", ""], ["Kontinen", "Juha", ""], ["L\u00fcck", "Martin", ""], ["Virtema", "Jonni", ""]]}, {"id": "2007.03976", "submitter": "Amandeep Bhatia", "authors": "Amandeep Singh Bhatia, Shenggen Zheng", "title": "A Quantum Finite Automata Approach to Modeling the Chemical Reactions", "comments": "8 figures, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the modeling interest has increased significantly from the\nmolecular level to the atomic and quantum scale. The field of computational\nchemistry plays a significant role in designing computational models for the\noperation and simulation of systems ranging from atoms and molecules to\nindustrial-scale processes. It is influenced by a tremendous increase in\ncomputing power and the efficiency of algorithms. The representation of\nchemical reactions using classical automata theory in thermodynamic terms had a\ngreat influence on computer science. The study of chemical information\nprocessing with quantum computational models is a natural goal. In this paper,\nwe have modeled chemical reactions using two-way quantum finite automata, which\nare halted in linear time. Additionally, classical pushdown automata can be\ndesigned for such chemical reactions with multiple stacks. It has been proven\nthat computational versatility can be increased by combining chemical\naccept/reject signatures and quantum automata models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:15:33 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bhatia", "Amandeep Singh", ""], ["Zheng", "Shenggen", ""]]}, {"id": "2007.04513", "submitter": "Tesshu Hanaka", "authors": "Gabriel L. Duarte, Hiroshi Eto, Tesshu Hanaka, Yasuaki Kobayashi,\n  Yusuke Kobayashi, Daniel Lokshtanov, Lehilton L. C. Pedrosa, Rafael C. S.\n  Schouery, and U\\'everton S. Souza", "title": "Computing the Largest Bond and the Maximum Connected Cut of a Graph", "comments": "This paper resulted from a merge of two papers submitted to arXiv\n  (arXiv:1908.03389 and arXiv:1910.01071). Both preliminary versions were\n  presented at the 14th International Symposium on Parameterized and Exact\n  Computation (IPEC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cut-set $\\partial(S)$ of a graph $G=(V,E)$ is the set of edges that have\none endpoint in $S\\subset V$ and the other endpoint in $V\\setminus S$, and\nwhenever $G[S]$ is connected, the cut $[S,V\\setminus S]$ of $G$ is called a\nconnected cut. A bond of a graph $G$ is an inclusion-wise minimal disconnecting\nset of $G$, i.e., bonds are cut-sets that determine cuts $[S,V\\setminus S]$ of\n$G$ such that $G[S]$ and $G[V\\setminus S]$ are both connected. Contrasting with\na large number of studies related to maximum cuts, there exist very few results\nregarding the largest bond of general graphs. In this paper, we aim to reduce\nthis gap on the complexity of computing the largest bond, and the maximum\nconnected cut of a graph. Although cuts and bonds are similar, we remark that\ncomputing the largest bond and the maximum connected cut of a graph tends to be\nharder than computing its maximum cut. We show that it does not exist a\nconstant-factor approximation algorithm to compute the largest bond, unless P =\nNP. Also, we show that {\\sc Largest Bond} and {\\sc Maximum Connected Cut} are\nNP-hard even for planar bipartite graphs, whereas \\textsc{Maximum Cut} is\ntrivial on bipartite graphs and polynomial-time solvable on planar graphs. In\naddition, we show that {\\sc Largest Bond} and {\\sc Maximum Connected Cut} are\nNP-hard on split graphs, and restricted to graphs of clique-width $w$ they can\nnot be solved in time $f(w)\\times n^{o(w)}$ unless the Exponential Time\nHypothesis fails, but they can be solved in time $f(w)\\times n^{O(w)}$.\nFinally, we show that both problems are fixed-parameter tractable when\nparameterized by the size of the solution, the treewidth, and the twin-cover\nnumber.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:25:25 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Duarte", "Gabriel L.", ""], ["Eto", "Hiroshi", ""], ["Hanaka", "Tesshu", ""], ["Kobayashi", "Yasuaki", ""], ["Kobayashi", "Yusuke", ""], ["Lokshtanov", "Daniel", ""], ["Pedrosa", "Lehilton L. C.", ""], ["Schouery", "Rafael C. S.", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "2007.04620", "submitter": "Markus Hecher", "authors": "Markus Hecher, Jorge Fandinno", "title": "Treewidth-Aware Complexity in ASP: Not all Positive Cycles are Equally\n  Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-know that deciding consistency for normal answer set programs\n(ASP) is NP-complete, thus, as hard as the satisfaction problem for classical\npropositional logic (SAT). The best algorithms to solve these problems take\nexponential time in the worst case. The exponential time hypothesis (ETH)\nimplies that this result is tight for SAT, that is, SAT cannot be solved in\nsubexponential time. This immediately establishes that the result is also tight\nfor the consistency problem for ASP. However, accounting for the treewidth of\nthe problem, the consistency problem for ASP is slightly harder than SAT: while\nSAT can be solved by an algorithm that runs in exponential time in the\ntreewidth k, it was recently shown that ASP requires exponential time in k\n\\cdot log(k). This extra cost is due checking that there are no self-supported\ntrue atoms due to positive cycles in the program. In this paper, we refine the\nabove result and show that the consistency problem for ASP can be solved in\nexponential time in k \\cdot log({\\lambda}) where {\\lambda} is the minimum\nbetween the treewidth and the size of the largest strongly-connected component\nin the positive dependency graph of the program. We provide a dynamic\nprogramming algorithm that solves the problem and a treewidth-aware reduction\nfrom ASP to SAT that adhere to the above limit.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:09:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Hecher", "Markus", ""], ["Fandinno", "Jorge", ""]]}, {"id": "2007.05020", "submitter": "Alexander Ponomarenko", "authors": "Alexander A. Ponomarenko and Dmitry V. Sirotkin", "title": "Dota Underlords game is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how the problem of the optimal team choice in\nthe popular computer game Dota Underlords can be reduced to the problem of\nlinear integer programming. We propose a model and solve it for the real data.\nWe also prove that this problem belongs to the NP-complete class and show that\nit reduces to the maximum edge weighted clique problem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 18:28:47 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Ponomarenko", "Alexander A.", ""], ["Sirotkin", "Dmitry V.", ""]]}, {"id": "2007.05246", "submitter": "Ignasi Sau", "authors": "Lucas Keiler, Carlos Vinicius G. C. Lima, Ana Karolinna Maia, Rudini\n  Sampaio, Ignasi Sau", "title": "Target set selection with maximum activation time", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A target set selection model is a graph $G$ with a threshold function\n$\\tau:V\\to \\mathbb{N}$ upper-bounded by the vertex degree. For a given model, a\nset $S_0\\subseteq V(G)$ is a target set if $V(G)$ can be partitioned into\nnon-empty subsets $S_0,S_1,\\dotsc,S_t$ such that, for $i \\in \\{1, \\ldots, t\\}$,\n$S_i$ contains exactly every vertex $v$ having at least $\\tau(v)$ neighbors in\n$S_0\\cup\\dots\\cup S_{i-1}$. We say that $t$ is the activation time\n$t_{\\tau}(S_0)$ of the target set $S_0$. The problem of, given such a model,\nfinding a target set of minimum size has been extensively studied in the\nliterature. In this article, we investigate its variant, which we call\nTSS-time, in which the goal is to find a target set $S_0$ that maximizes\n$t_{\\tau}(S_0)$. That is, given a graph $G$, a threshold function $\\tau$ in\n$G$, and an integer $k$, the objective of the TSS-time problem is to decide\nwhether $G$ contains a target set $S_0$ such that $t_{\\tau}(S_0)\\geq k$. Let\n$\\tau^* = \\max_{v \\in V(G)} \\tau(v)$. Our main result is the following\ndichotomy about the complexity of TSS-time when $G$ belongs to a minor-closed\ngraph class ${\\cal C}$: if ${\\cal C}$ has bounded local treewidth, the problem\nis FPT parameterized by $k$ and $\\tau^{\\star}$; otherwise, it is NP-complete\neven for fixed $k=4$ and $\\tau^{\\star}=2$. We also prove that, with $\\tau^*=2$,\nthe problem is NP-hard in bipartite graphs for fixed $k=5$, and from previous\nresults we observe that TSS-time is NP-hard in planar graphs and W[1]-hard\nparameterized by treewidth. Finally, we present a linear-time algorithm to find\na target set $S_0$ in a given tree maximizing $t_{\\tau}(S_0)$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 08:46:03 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Keiler", "Lucas", ""], ["Lima", "Carlos Vinicius G. C.", ""], ["Maia", "Ana Karolinna", ""], ["Sampaio", "Rudini", ""], ["Sau", "Ignasi", ""]]}, {"id": "2007.05346", "submitter": "Thekla Hamm", "authors": "Eduard Eiben, Robert Ganian, Thekla Hamm, Fabian Klute, Martin\n  N\\\"ollenburg", "title": "Extending Nearly Complete 1-Planar Drawings in Polynomial Time", "comments": "arXiv admin note: text overlap with arXiv:2004.12222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of extending partial geometric graph representations such as\nplane graphs has received considerable attention in recent years. In\nparticular, given a graph $G$, a connected subgraph $H$ of $G$ and a drawing\n$\\mathcal{H}$ of $H$, the extension problem asks whether $\\mathcal{H}$ can be\nextended into a drawing of $G$ while maintaining some desired property of the\ndrawing (e.g., planarity).\n  In their breakthrough result, Angelini et al. [ACM TALG 2015] showed that the\nextension problem is polynomial-time solvable when the aim is to preserve\nplanarity. Very recently we considered this problem for partial 1-planar\ndrawings [ICALP 2020], which are drawings in the plane that allow each edge to\nhave at most one crossing. The most important question identified and left open\nin that work is whether the problem can be solved in polynomial time when $H$\ncan be obtained from $G$ by deleting a bounded number of vertices and edges. In\nthis work, we answer this question positively by providing a constructive\npolynomial-time decision algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:03:11 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Hamm", "Thekla", ""], ["Klute", "Fabian", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2007.05458", "submitter": "Fulvio Gesmundo", "authors": "Matthias Christandl, Fulvio Gesmundo, Mateusz Micha{\\l}ek, Jeroen\n  Zuiddam", "title": "Border rank non-additivity for higher order tensors", "comments": "26 pages, 5 figures. Final version accepted in SIMAX", "journal-ref": "SIAM J. Matrix Anal. Appl., 42(2), 503-527, 2021", "doi": "10.1137/20M1357366", "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas matrix rank is additive under direct sum, in 1981 Sch\\\"onhage showed\nthat one of its generalizations to the tensor setting, tensor border rank, can\nbe strictly subadditive for tensors of order three. Whether border rank is\nadditive for higher order tensors has remained open. In this work, we settle\nthis problem by providing analogs of Sch\\\"onhage's construction for tensors of\norder four and higher. Sch\\\"onhage's work was motivated by the study of the\ncomputational complexity of matrix multiplication; we discuss implications of\nour results for the asymptotic rank of higher order generalizations of the\nmatrix multiplication tensor.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:57:31 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 09:34:33 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Christandl", "Matthias", ""], ["Gesmundo", "Fulvio", ""], ["Micha\u0142ek", "Mateusz", ""], ["Zuiddam", "Jeroen", ""]]}, {"id": "2007.05469", "submitter": "Dmitri Maslov", "authors": "Sergey Bravyi, Theodore J. Yoder, and Dmitri Maslov", "title": "Efficient ancilla-free reversible and quantum circuits for the Hidden\n  Weighted Bit function", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hidden Weighted Bit function plays an important role in the study of\nclassical models of computation. A common belief is that this function is\nexponentially hard for the implementation by reversible ancilla-free circuits,\neven though introducing a small number of ancillae allows a very efficient\nimplementation. In this paper, we refute the exponential hardness conjecture by\ndeveloping a polynomial-size reversible ancilla-free circuit computing the\nHidden Weighted Bit function. Our circuit has size $O(n^{6.42})$, where $n$ is\nthe number of input bits. We also show that the Hidden Weighted Bit function\ncan be computed by a quantum ancilla-free circuit of size $O(n^2)$. The\ntechnical tools employed come from a combination of Theoretical Computer\nScience (Barrington's theorem) and Physics (simulation of fermionic\nHamiltonians) techniques.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:30:58 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bravyi", "Sergey", ""], ["Yoder", "Theodore J.", ""], ["Maslov", "Dmitri", ""]]}, {"id": "2007.05580", "submitter": "Joshua Brody", "authors": "Joshua Brody, Jae Tak Kim, Peem Lerdputtipongporn, Hariharan\n  Srinivasulu", "title": "A Strong XOR Lemma for Randomized Query Complexity", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a strong direct sum theorem for computing $xor \\circ g$.\nSpecifically, we show that for every function g and every $k\\geq 2$, the\nrandomized query complexity of computing the xor of k instances of g satisfies\n$\\overline{R}_\\eps(xor\\circ g) = \\Theta(k \\overline{R}_{\\eps/k}(g))$. This\nmatches the naive success amplification upper bound and answers a conjecture of\nBlais and Brody (CCC19).\n  As a consequence of our strong direct sum theorem, we give a total function g\nfor which $R(xor \\circ g) = \\Theta(k \\log(k)\\cdot R(g))$, answering an open\nquestion from Ben-David et al.(arxiv:2006.10957v1).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 19:29:52 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:07:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Brody", "Joshua", ""], ["Kim", "Jae Tak", ""], ["Lerdputtipongporn", "Peem", ""], ["Srinivasulu", "Hariharan", ""]]}, {"id": "2007.06754", "submitter": "Warut Suksompong", "authors": "Paul W. Goldberg, Alexandros Hollender, Ayumi Igarashi, Pasin\n  Manurangsi, Warut Suksompong", "title": "Consensus Halving for Sets of Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus halving refers to the problem of dividing a resource into two parts\nso that every agent values both parts equally. Prior work has shown that when\nthe resource is represented by an interval, a consensus halving with at most\n$n$ cuts always exists, but is hard to compute even for agents with simple\nvaluation functions. In this paper, we study consensus halving in a natural\nsetting where the resource consists of a set of items without a linear\nordering. When agents have additive utilities, we present a polynomial-time\nalgorithm that computes a consensus halving with at most $n$ cuts, and show\nthat $n$ cuts are almost surely necessary when the agents' utilities are drawn\nfrom probabilistic distributions. On the other hand, we show that for a simple\nclass of monotonic utilities, the problem already becomes PPAD-hard.\nFurthermore, we compare and contrast consensus halving with the more general\nproblem of consensus $k$-splitting, where we wish to divide the resource into\n$k$ parts in possibly unequal ratios, and provide some consequences of our\nresults on the problem of computing small agreeable sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:20:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Goldberg", "Paul W.", ""], ["Hollender", "Alexandros", ""], ["Igarashi", "Ayumi", ""], ["Manurangsi", "Pasin", ""], ["Suksompong", "Warut", ""]]}, {"id": "2007.06819", "submitter": "Christian Engels", "authors": "Christian Engels", "title": "Lower Bounds of Algebraic Branching Programs and Layerization", "comments": "The current version has some serious gaps which I need to address\n  before the results stand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we improve the lower bound of Chatterjee et al.\\ (ECCC 2019) to\nan $\\Omega(n^2)$ lower bound for unlayered Algebraic Branching Programs. We\nalso\n  study the impact layerization has on Algebraic Branching Programs. We exhibit\na polynomial that has an unlayered ABP of size $O(n)$ but any layered ABP has\nsize at least $\\Omega(n\\sqrt{n})$.\n  We exhibit a similar dichotomy in the non-commutative setting where the\nunlayered ABP has size $O(n)$ and any layered ABP has size at least\n$\\Omega(n\\log n -\\log^2 n)$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:06:36 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 02:14:27 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Engels", "Christian", ""]]}, {"id": "2007.06896", "submitter": "Gregory Gutin", "authors": "J. Bang-Jensen, E. Eiben, G. Gutin, M. Wahlstrom, A. Yeo", "title": "Component Order Connectivity in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph $D$ is semicomplete if for every pair $x,y$ of vertices of\n$D,$ there is at least one arc between $x$ and $y.$ \\viol{Thus, a tournament is\na semicomplete digraph.} In the Directed Component Order Connectivity (DCOC)\nproblem, given a digraph $D=(V,A)$ and a pair of natural numbers $k$ and\n$\\ell$, we are to decide whether there is a subset $X$ of $V$ of size $k$ such\nthat the largest strong connectivity component in $D-X$ has at most $\\ell$\nvertices. Note that DCOC reduces to the Directed Feedback Vertex Set problem\nfor $\\ell=1.$ We study parametered complexity of DCOC for general and\nsemicomplete digraphs with the following parameters: $k, \\ell,\\ell+k$ and\n$n-\\ell$. In particular, we prove that DCOC with parameter $k$ on semicomplete\ndigraphs can be solved in time $O^*(2^{16k})$ but not in time $O^*(2^{o(k)})$\nunless the Exponential Time Hypothesis (ETH) fails. \\gutin{The upper bound\n$O^*(2^{16k})$ implies the upper bound $O^*(2^{16(n-\\ell)})$ for the parameter\n$n-\\ell.$ We complement the latter by showing that there is no algorithm of\ntime complexity $O^*(2^{o({n-\\ell})})$ unless ETH fails.} Finally, we improve\n\\viol{(in dependency on $\\ell$)} the upper bound of G{\\\"{o}}ke, Marx and Mnich\n(2019) for the time complexity of DCOC with parameter $\\ell+k$ on general\ndigraphs from $O^*(2^{O(k\\ell\\log (k\\ell))})$ to $O^*(2^{O(k\\log (k\\ell))}).$\nNote that Drange, Dregi and van 't Hof (2016) proved that even for the\nundirected version of DCOC on split graphs there is no algorithm of running\ntime $O^*(2^{o(k\\log \\ell)})$ unless ETH fails and it is a long-standing\nproblem to decide whether Directed Feedback Vertex Set admits an algorithm of\ntime complexity $O^*(2^{o(k\\log k)}).$\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:12:51 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 05:48:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bang-Jensen", "J.", ""], ["Eiben", "E.", ""], ["Gutin", "G.", ""], ["Wahlstrom", "M.", ""], ["Yeo", "A.", ""]]}, {"id": "2007.06979", "submitter": "Tristan St\\'erin", "authors": "Tristan St\\'erin and Damien Woods", "title": "The Collatz process embeds a base conversion algorithm", "comments": "28 pages. 8 figures. 2 appendices. Short version accepted to the 14th\n  International Conference on Reachability Problems (RP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Collatz process is defined on natural numbers by iterating the map $T(x)\n= T_0(x) = x/2$ when $x\\in\\mathbb{N}$ is even and $T(x)=T_1(x) =(3x+1)/2$ when\n$x$ is odd. In an effort to understand its dynamics, and since Generalised\nCollatz Maps are known to simulate Turing Machines [Conway, 1972], it seems\nnatural to ask what kinds of algorithmic behaviours it embeds. We define a\nquasi-cellular automaton that exactly simulates the Collatz process on the\nsquare grid: on input $x\\in\\mathbb{N}$, written horizontally in base 2,\nsuccessive rows give the Collatz sequence of $x$ in base 2. We show that\nvertical columns simultaneously iterate the map in base 3. This leads to our\nmain result: the Collatz process embeds an algorithm that converts any natural\nnumber from base 3 to base 2. We also find that the evolution of our automaton\ncomputes the parity of the number of 1s in any ternary input. It follows that\npredicting about half of the bits of the iterates $T^i(x)$, for $i = O(\\log\nx)$, is in the complexity class NC$^1$ but outside AC$^0$. Finally, we show\nthat in the extension of the Collatz process to numbers with infinite binary\nexpansions ($2$-adic integers) [Lagarias, 1985], our automaton encodes the\ncyclic Collatz conjecture as a natural reachability problem. These results show\nthat the Collatz process is capable of some simple, but non-trivial,\ncomputation in bases 2 and 3, suggesting an algorithmic approach to thinking\nabout existence, prediction and structure of cycles in the Collatz process.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 11:51:42 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 12:43:28 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 15:05:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["St\u00e9rin", "Tristan", ""], ["Woods", "Damien", ""]]}, {"id": "2007.07449", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms, Yuichi Yoshida", "title": "Downsampling for Testing and Learning in Product Distributions", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the domain reduction problem of eliminating dependence on $n$ from\nthe complexity of property testing and learning algorithms on domain $[n]^d$,\nand the related problem of establishing testing and learning results for\nproduct distributions over $\\mathbb{R}^d$. Our method, which we call\ndownsampling, gives conceptually simple proofs for several results:\n  1. A 1-page proof of the recent $o(d)$-query monotonicity tester for the\nhypergrid (Black, Chakrabarty & Seshadhri, SODA 2020), and an improvement from\n$O(d^7)$ to $\\widetilde O(d^4)$ in the sample complexity of their\ndistribution-free monotonicity tester for product distributions over\n$\\mathbb{R}^d$;\n  2. An $\\exp(\\widetilde O(kd))$-time agnostic learning algorithm for functions\nof $k$ convex sets in product distributions;\n  3. A polynomial-time agnostic learning algorithm for functions of a constant\nnumber of halfspaces in product distributions;\n  4. A polynomial-time agnostic learning algorithm for constant-degree\npolynomial threshold functions in product distributions;\n  5. An $\\exp(\\widetilde O(k \\sqrt d))$-time agnostic learning algorithm for\n$k$-alternating functions in product distributions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:46:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Harms", "Nathaniel", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "2007.07496", "submitter": "Christian Engels", "authors": "Christian Engels", "title": "Observations on Symmetric Circuits", "comments": "While I believe the results still hold, the current proofs are not\n  correct. I will work on a fixed version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study symmetric arithmetic circuits and improve on lower bounds given by\nDawar and Wilsenach (ArXiv 2020). Their result showed an exponential lower\nbound of the permanent computed by symmetric circuits. We extend this result to\nshow a simpler proof of the permanent lower bound and show that a large class\nof polynomials have exponential lower bounds in this model. In fact, we prove\nthat all polynomials that contain at least one monomial of the permanent have\nexponential size lower bounds in the symmetric computation model. We also show\nsuper-polynomial lower bounds for smaller groups. We support our conclusion\nthat the group is much more important than the polynomial by showing that on a\nrandom process of choosing polynomials, the probability of not encountering a\nsuper-polynomial lower bound is exponentially low.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:04:03 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 04:36:36 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 05:06:31 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Engels", "Christian", ""]]}, {"id": "2007.07738", "submitter": "Ignasi Sau", "authors": "Victor Campos, Raul Lopes, Ana Karolinna Maia, Ignasi Sau", "title": "Adapting the Directed Grid Theorem into an FPT Algorithm", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Grid Theorem of Robertson and Seymour [JCTB, 1986], is one of the most\nimportant tools in the field of structural graph theory, finding numerous\napplications in the design of algorithms for undirected graphs. An analogous\nversion of the Grid Theorem in digraphs was conjectured by Johnson et al.\n[JCTB, 2001], and proved by Kawarabayashi and Kreutzer [STOC, 2015]. Namely,\nthey showed that there is a function $f(k)$ such that every digraph of directed\ntree-width at least $f(k)$ contains a cylindrical grid of size $k$ as a\nbutterfly minor and stated that their proof can be turned into an XP algorithm,\nwith parameter $k$, that either constructs a decomposition of the appropriate\nwidth, or finds the claimed large cylindrical grid as a butterfly minor. In\nthis paper, we adapt some of the steps of the proof of Kawarabayashi and\nKreutzer to improve this XP algorithm into an FPT algorithm. Towards this, our\nmain technical contributions are two FPT algorithms with parameter $k$. The\nfirst one either produces an arboreal decomposition of width $3k-2$ or finds a\nhaven of order $k$ in a digraph $D$, improving on the original result for\narboreal decompositions by Johnson et al. The second algorithm finds a\nwell-linked set of order $k$ in a digraph $D$ of large directed tree-width. As\ntools to prove these results, we show how to solve a generalized version of the\nproblem of finding balanced separators for a given set of vertices $T$ in FPT\ntime with parameter $|T|$, a result that we consider to be of its own interest.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:10:59 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Campos", "Victor", ""], ["Lopes", "Raul", ""], ["Maia", "Ana Karolinna", ""], ["Sau", "Ignasi", ""]]}, {"id": "2007.07772", "submitter": "Jesse Goodman", "authors": "Eshan Chattopadhyay, Jesse Goodman", "title": "Explicit Designs and Extractors", "comments": "32 pages; additional main theorem (small-space extractors for\n  polylogarithmic entropy); other additional smaller results; improved\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give significantly improved explicit constructions of three related\npseudorandom objects.\n  1. Extremal designs: An $(n,r,s)$-design, or $(n,r,s)$-partial Steiner\nsystem, is an $r$-uniform hypergraph over $n$ vertices with pairwise hyperedge\nintersections of size $<s$. For all constants $r\\geq s\\in\\mathbb{N}$ with $r$\neven, we explicitly construct $(n,r,s)$-designs $(G_n)_{n\\in\\mathbb{N}}$ with\nindependence number $\\alpha(G_n)\\leq O(n^{\\frac{2(r-s)}{r}})$. This gives the\nfirst derandomization of a result by R\\\"odl and \\v{S}inajov\\'a (Random\nStructures & Algorithms, 1994).\n  2. Extractors for adversarial sources: By combining our designs with\nleakage-resilient extractors (Chattopadhyay et al., FOCS 2020), we establish a\nnew, simple framework for extracting from adversarial sources of locality $0$.\nAs a result, we obtain significantly improved low-error extractors for these\nsources. For any constant $\\delta>0$, we extract from $(N,K,n,$\npolylog$(n))$-adversarial sources of locality $0$, given just $K\\geq N^\\delta$\ngood sources. The previous best result (Chattopadhyay et al., STOC 2020)\nrequired $K\\geq N^{1/2+o(1)}$.\n  3. Extractors for small-space sources: Using a known reduction to adversarial\nsources, we immediately obtain improved low-error extractors for space $s$\nsources over $n$ bits that require entropy $k\\geq n^{1/2+\\delta}\\cdot\ns^{1/2-\\delta}$, whereas the previous best result (Chattopadhyay et al., STOC\n2020) required $k\\geq n^{2/3+\\delta}\\cdot s^{1/3-\\delta}$. On the other hand,\nusing a new reduction from small-space sources to affine sources, we obtain\nnear-optimal extractors for small-space sources in the polynomial error regime.\nOur extractors require just $k\\geq s\\cdot\\log^Cn$ entropy for some constant\n$C$, which is an exponential improvement over the previous best result, which\nrequired $k\\geq s^{1.1}\\cdot2^{\\log^{0.51}n}$ (Chattopadhyay and Li, STOC\n2016).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:45:27 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 23:58:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chattopadhyay", "Eshan", ""], ["Goodman", "Jesse", ""]]}, {"id": "2007.07808", "submitter": "Lukas Graf", "authors": "Lukas Graf and Tobias Harks", "title": "A Finite Time Combinatorial Algorithm for Instantaneous Dynamic\n  Equilibrium Flows", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instantaneous dynamic equilibrium (IDE) is a standard game-theoretic concept\nin dynamic traffic assignment in which individual flow particles myopically\nselect en route currently shortest paths towards their destination. We analyze\nIDE within the Vickrey bottleneck model, where current travel times along a\npath consist of the physical travel times plus the sum of waiting times in all\nthe queues along a path. Although IDE have been studied for decades, several\nfundamental questions regarding equilibrium computation and complexity are not\nwell understood. In particular, all existence results and computational methods\nare based on fixed-point theorems and numerical discretization schemes and no\nexact finite time algorithm for equilibrium computation is known to date. As\nour main result we show that a natural extension algorithm needs only finitely\nmany phases to converge leading to the first finite time combinatorial\nalgorithm computing an IDE. We complement this result by several hardness\nresults showing that computing IDE with natural properties is NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:34:10 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 07:19:36 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 13:00:42 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Graf", "Lukas", ""], ["Harks", "Tobias", ""]]}, {"id": "2007.08069", "submitter": "Bhaskar DasGupta", "authors": "Abolfazl Asudeh and Tanya Berger-Wolf and Bhaskar DasGupta and\n  Anastasios Sidiropoulos", "title": "Maximizing coverage while ensuring fairness: a tale of conflicting\n  objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensuring fairness in computational problems has emerged as a $key$ topic\nduring recent years, buoyed by considerations for equitable resource\ndistributions and social justice. It $is$ possible to incorporate fairness in\ncomputational problems from several perspectives, such as using optimization,\ngame-theoretic or machine learning frameworks. In this paper we address the\nproblem of incorporation of fairness from a $combinatorial$ $optimization$\nperspective. We formulate a combinatorial optimization framework, suitable for\nanalysis by researchers in approximation algorithms and related areas, that\nincorporates fairness in maximum coverage problems as an interplay between\n$two$ conflicting objectives. Fairness is imposed in coverage by using coloring\nconstraints that $minimizes$ the discrepancies between number of elements of\ndifferent colors covered by selected sets; this is in contrast to the usual\ndiscrepancy minimization problems studied extensively in the literature where\n(usually two) colors are $not$ given $a$ $priori$ but need to be selected to\nminimize the maximum color discrepancy of $each$ individual set. Our main\nresults are a set of randomized and deterministic approximation algorithms that\nattempts to $simultaneously$ approximate both fairness and coverage in this\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:45:02 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 19:21:22 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Berger-Wolf", "Tanya", ""], ["DasGupta", "Bhaskar", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "2007.09023", "submitter": "Marieke van der Wegen", "authors": "Hans L. Bodlaender and Marieke van der Wegen", "title": "Parameterized Complexity of Scheduling Chains of Jobs with Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the parameterized complexity of the following\nscheduling problem. We must schedule a number of jobs on $m$ machines, where\neach job has unit length, and the graph of precedence constraints consists of a\nset of chains. Each precedence constraint is labelled with an integer that\ndenotes the exact (or minimum) delay between the jobs. We study different\ncases; delays can be given in unary and in binary, and the case that we have a\nsingle machine is discussed separately. We consider the complexity of this\nproblem parameterized by the number of chains, and by the thickness of the\ninstance, which is the maximum number of chains whose intervals between release\ndate and deadline overlap.\n  We show that this scheduling problem with exact delays in unary is\n$W[t]$-hard for all $t$, when parameterized by the thickness, even when we have\na single machine ($m = 1$). When parameterized by the number of chains, this\nproblem is $W[1]$-complete when we have a single or a constant number of\nmachines, and $W[2]$-complete when the number of machines is a variable. The\nproblem with minimum delays, given in unary, parameterized by the number of\nchains (and as a simple corollary, also when parameterized by the thickness) is\n$W[1]$-hard for a single or a constant number of machines, and $W[2]$-hard when\nthe number of machines is variable.\n  With a dynamic programming algorithm, one can show membership in XP for exact\nand minimum delays in unary, for any number of machines, when parameterized by\nthickness or number of chains. For a single machine, with exact delays in\nbinary, parameterized by the number of chains, membership in XP can be shown\nwith branching and solving a system of difference constraints. For all other\ncases for delays in binary, membership in XP is open.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:32:24 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["van der Wegen", "Marieke", ""]]}, {"id": "2007.09045", "submitter": "Claudio Telha", "authors": "Andreas S. Schulz and Claudio Telha", "title": "Integer factorization and Riemann's hypothesis: Why two-item joint\n  replenishment is hard", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution networks with periodically repeating events often hold great\npromise to exploit economies of scale. Joint replenishment problems are a\nfundamental model in inventory management, manufacturing, and logistics that\ncapture these effects. However, finding an efficient algorithm that optimally\nsolves these models, or showing that none may exist, has long been open,\nregardless of whether empty joint orders are possible or not. In either case,\nwe show that finding optimal solutions to joint replenishment instances with\njust two products is at least as difficult as integer factorization. To the\nbest of the authors' knowledge, this is the first time that integer\nfactorization is used to explain the computational hardness of any kind of\noptimization problem. Under the assumption that Riemann's Hypothesis is\ncorrect, we can actually prove that the two-item joint replenishment problem\nwith possibly empty joint ordering points is NP-complete under randomized\nreductions, which implies that not even quantum computers may be able to solve\nit efficiently. By relating the computational complexity of joint replenishment\nto cryptography, prime decomposition, and other aspects of prime numbers, a\nsimilar approach may help to establish (integer factorization) hardness of\nadditional open periodic problems in supply chain management and beyond, whose\nsolution has eluded standard methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:18:44 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Schulz", "Andreas S.", ""], ["Telha", "Claudio", ""]]}, {"id": "2007.09099", "submitter": "Andrei Bulatov", "authors": "Andrei A. Bulatov", "title": "A dichotomy theorem for nonuniform CSPs simplified", "comments": "This is an updated and improved version of the proof of the CSP\n  dichotomy from CoRR abs/1703.03021, 2017. arXiv admin note: text overlap with\n  arXiv:1703.03021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a non-uniform Constraint Satisfaction problem CSP(G), where G is a set of\nrelations on a finite set A, the goal is to find an assignment of values to\nvariables subject to constraints imposed on specified sets of variables using\nthe relations from G. The Dichotomy Conjecture for the non-uniform CSP states\nthat for every constraint language G the problem CSP(G) is either solvable in\npolynomial time or is NP-complete. It was proposed by Feder and Vardi in their\nseminal 1993 paper. In this paper we confirm the Dichotomy Conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:43:28 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Bulatov", "Andrei A.", ""]]}, {"id": "2007.09281", "submitter": "Tae Hyung Kim", "authors": "Tae Hyung Kim, Justin P. Haldar", "title": "Efficient Iterative Solutions to Complex-Valued Nonlinear Least-Squares\n  Problems with Mixed Linear and Antilinear Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.NA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setting in which it is desired to find an optimal complex\nvector $\\mathbf{x}\\in\\mathbb{C}^N$ that satisfies $\\mathcal{A}(\\mathbf{x})\n\\approx \\mathbf{b}$ in a least-squares sense, where $\\mathbf{b} \\in\n\\mathbb{C}^M$ is a data vector (possibly noise-corrupted), and\n$\\mathcal{A}(\\cdot): \\mathbb{C}^N \\rightarrow \\mathbb{C}^M$ is a measurement\noperator. If $\\mathcal{A}(\\cdot)$ were linear, this reduces to the classical\nlinear least-squares problem, which has a well-known analytic solution as well\nas powerful iterative solution algorithms. However, instead of linear\nleast-squares, this work considers the more complicated scenario where\n$\\mathcal{A}(\\cdot)$ is nonlinear, but can be represented as the summation\nand/or composition of some operators that are linear and some operators that\nare antilinear. Some common nonlinear operations that have this structure\ninclude complex conjugation or taking the real-part or imaginary-part of a\ncomplex vector. Previous literature has shown that this kind of mixed\nlinear/antilinear least-squares problem can be mapped into a linear\nleast-squares problem by considering $\\mathbf{x}$ as a vector in\n$\\mathbb{R}^{2N}$ instead of $\\mathbb{C}^N$. While this approach is valid, the\nreplacement of the original complex-valued optimization problem with a\nreal-valued optimization problem can be complicated to implement, and can also\nbe associated with increased computational complexity. In this work, we\ndescribe theory and computational methods that enable mixed linear/antilinear\nleast-squares problems to be solved iteratively using standard linear\nleast-squares tools, while retaining all of the complex-valued structure of the\noriginal inverse problem. An illustration is provided to demonstrate that this\napproach can simplify the implementation and reduce the computational\ncomplexity of iterative solution algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:48:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Tae Hyung", ""], ["Haldar", "Justin P.", ""]]}, {"id": "2007.09318", "submitter": "Yinzhan Xu", "authors": "Virginia Vassilevska Williams, Yinzhan Xu", "title": "Monochromatic Triangles, Triangle Listing and APSP", "comments": "To appear in FOCS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main hypotheses in fine-grained complexity is that All-Pairs\nShortest Paths (APSP) for $n$-node graphs requires $n^{3-o(1)}$ time. Another\nfamous hypothesis is that the $3$SUM problem for $n$ integers requires\n$n^{2-o(1)}$ time. Although there are no direct reductions between $3$SUM and\nAPSP, it is known that they are related: there is a problem,\n$(\\min,+)$-convolution that reduces in a fine-grained way to both, and a\nproblem Exact Triangle that both fine-grained reduce to.\n  In this paper we find more relationships between these two problems and other\nbasic problems. P\\u{a}tra\\c{s}cu had shown that under the $3$SUM hypothesis the\nAll-Edges Sparse Triangle problem in $m$-edge graphs requires $m^{4/3-o(1)}$\ntime. The latter problem asks to determine for every edge $e$, whether $e$ is\nin a triangle. It is equivalent to the problem of listing $m$ triangles in an\n$m$-edge graph where $m=\\tilde{O}(n^{1.5})$, and can be solved in $O(m^{1.41})$\ntime [Alon et al.'97] with the current matrix multiplication bounds, and in\n$\\tilde{O}(m^{4/3})$ time if $\\omega=2$.\n  We show that one can reduce Exact Triangle to All-Edges Sparse Triangle,\nshowing that All-Edges Sparse Triangle (and hence Triangle Listing) requires\n$m^{4/3-o(1)}$ time also assuming the APSP hypothesis. This allows us to\nprovide APSP-hardness for many dynamic problems that were previously known to\nbe hard under the $3$SUM hypothesis.\n  We also consider the previously studied All-Edges Monochromatic Triangle\nproblem. Via work of [Lincoln et al.'20], our result on All-Edges Sparse\nTriangle implies that if the All-Edges Monochromatic Triangle problem has an\n$O(n^{2.5-\\epsilon})$ time algorithm for $\\epsilon>0$, then both the APSP and\n$3$SUM hypotheses are false. We also connect the problem to other\n``intermediate'' problems, whose runtimes are between $O(n^\\omega)$ and\n$O(n^3)$, such as the Max-Min product problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 03:29:55 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 20:08:57 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Williams", "Virginia Vassilevska", ""], ["Xu", "Yinzhan", ""]]}, {"id": "2007.09461", "submitter": "Sergiu Ivanov", "authors": "Sergiu Ivanov (1) and Ion Petre (2,3) ((1) IBISC, Universit\\'e \\'Evry,\n  Universit\\'e Paris-Saclay, France, (2) Department of Mathematics and\n  Statistics, University of Turku, Finland, (3) National Institute for Research\n  and Development in Biological Sciences, Romania)", "title": "Controllability of reaction systems", "comments": "20 pages, submitted to SECIS SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controlling a dynamical system is the ability of changing its configuration\narbitrarily through a suitable choice of inputs. It is a very well studied\nconcept in control theory, with wide ranging applications in medicine, biology,\nsocial sciences, engineering. We introduce in this article the concept of\ncontrollability of reaction systems as the ability of transitioning between any\ntwo states through a suitable choice of context sequences. We show that the\nproblem is PSPACE-hard. We also introduce a model of oncogenic signalling based\non reaction systems and use it to illustrate the intricacies of the\ncontrollability of reaction systems.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 15:46:27 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:59:17 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Ivanov", "Sergiu", ""], ["Petre", "Ion", ""]]}, {"id": "2007.09946", "submitter": "Kees Middelburg", "authors": "C. A. Middelburg", "title": "Program algebra for random access machine programs", "comments": "25 pages, Sect. 2--4 are largely shortened versions of Sect. 2--4 of\n  arXiv:1808.04264, which, in turn, draw from preliminary sections of several\n  other papers. arXiv admin note: substantial text overlap with\n  arXiv:1901.08840", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algebraic theory of instruction sequences with\ninstructions for a random access machine (RAM) as basic instructions, the\nbehaviours produced by the instruction sequences concerned under execution, and\nthe interaction between such behaviours and RAM memories. This theory provides\na setting for the development of theory in areas such as computational\ncomplexity and analysis of algorithm that distinguishes itself by offering the\npossibility of equational reasoning to establish whether an instruction\nsequence computes a given function and being more general than the setting\nprovided by any known version of the RAM model of computation. In this setting,\na semi-realistic version of the RAM model of computation and a bit-oriented\ntime complexity measure for this version are introduced.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:50:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Middelburg", "C. A.", ""]]}, {"id": "2007.10331", "submitter": "Ioannis Avramopoulos", "authors": "Ioannis Avramopoulos", "title": "Evolution toward a Nash equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the dynamic behavior of Hedge, a well-known algorithm\nin theoretical machine learning and algorithmic game theory. The empirical\naverage (arithmetic mean) of the iterates Hedge generates is known to converge\nto a minimax equilibrium in zero-sum games. We generalize that result to show\nconvergence of the empirical average to Nash equilibrium in symmetric bimatrix\ngames (that is bimatrix games where the payoff matrix of each player is the\ntranspose of that of the other) in the sense that every limit point of the\nsequence of averages is an $\\epsilon$-approximate symmetric equilibrium\nstrategy for any desirable $\\epsilon$. Our analysis gives rise to a symmetric\nequilibrium fully polynomial-time approximation scheme, implying P = PPAD.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 11:21:26 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Avramopoulos", "Ioannis", ""]]}, {"id": "2007.10673", "submitter": "Zhaohui Wei", "authors": "Xiaodie Lin and Zhaohui Wei and Penghui Yao", "title": "Quantum and Classical Hybrid Generations for Classical Correlations", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-stage hybrid protocols that combine quantum resource and\nclassical resource to generate classical correlations shared by two separated\nplayers. Our motivation is twofold. First, in the near future the scale of\nquantum information processing is quite limited, and when quantum resource\navailable is not sufficient for certain tasks, a possible way to strengthen the\ncapability of quantum schemes is introducing extra classical resource. We\nanalyze the mathematical structures of these hybrid protocols, and characterize\nthe relation between the amount of quantum resource and classical resource\nneeded. Second, a fundamental open problem in communication complexity theory\nis to describe the advantages of sharing prior quantum entanglement over\nsharing prior randomness, which is still widely open. It turns out that our\nquantum and classical hybrid protocols provide new insight into this important\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:23:45 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Lin", "Xiaodie", ""], ["Wei", "Zhaohui", ""], ["Yao", "Penghui", ""]]}, {"id": "2007.10824", "submitter": "David Harris", "authors": "David G. Harris, Vladimir Kolmogorov", "title": "Parameter estimation for Gibbs distributions", "comments": "This is a significantly extended version of a paper \"A Faster\n  Approximation Algorithm for the Gibbs Partition Function\" (arXiv:1608.04223),\n  which was published in COLT 2018. It covers many additional topics; most\n  importantly, algorithms to estimate counts and algorithm specialized for\n  integer-valued distributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider \\emph{Gibbs distributions}, which are families of probability\ndistributions over a discrete space $\\Omega$ with probability mass function of\nthe form $\\mu^\\Omega_\\beta(\\omega) \\propto e^{\\beta H(\\omega)}$ for $\\beta$ in\nan interval $[\\beta_{\\min}, \\beta_{\\max}]$ and $H( \\omega ) \\in \\{0 \\} \\cup [1,\nn]$. The \\emph{partition function} is the normalization factor\n$Z(\\beta)=\\sum_{\\omega \\in\\Omega}e^{\\beta H(\\omega)}$.\n  Two important parameters of these distributions are the log partition ratio\n$q = \\log \\tfrac{Z(\\beta_{\\max})}{Z(\\beta_{\\min})}$ and the counts $c_x =\n|H^{-1}(x)|$. These are correlated with system parameters in a number of\nphysical applications and sampling algorithms. Our first main result is to\nestimate the counts $c_x$ using roughly $\\tilde O( \\frac{q}{\\varepsilon^2})$\nsamples for general Gibbs distributions and $\\tilde O(\n\\frac{n^2}{\\varepsilon^2} )$ samples for integer-valued distributions (ignoring\nsome second-order terms and parameters), and we show this is optimal up to\nlogarithmic factors. We illustrate with improved algorithms for counting\nconnected subgraphs and perfect matchings in a graph.\n  We develop a key subroutine to estimate the partition function $Z$.\nSpecifically, it generates a data structure to estimate $Z(\\beta)$ for\n\\emph{all} values $\\beta$, without further samples. Constructing the data\nstructure requires $O(\\frac{q \\log n}{\\varepsilon^2})$ samples for general\nGibbs distributions and $O(\\frac{n^2 \\log n}{\\varepsilon^2} + n \\log q)$\nsamples for integer-valued distributions. This improves over a prior algorithm\nof Huber (2015) which computes a single point estimate $Z(\\beta_\\max)$ using\n$O( q \\log n( \\log q + \\log \\log n + \\varepsilon^{-2}))$ samples. We show\nmatching lower bounds, demonstrating that this complexity is optimal as a\nfunction of $n$ and $q$ up to logarithmic terms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 11:27:08 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:03:27 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 20:54:19 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 20:41:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Harris", "David G.", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "2007.10857", "submitter": "Joshua Brakensiek", "authors": "Shant Boodaghians and Joshua Brakensiek and Samuel B. Hopkins and\n  Aviad Rubinstein", "title": "Smoothed Complexity of 2-player Nash Equilibria", "comments": "21 pages, 1 figure; FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that computing a Nash equilibrium of a two-player ($n \\times n$)\ngame with payoffs in $[-1,1]$ is PPAD-hard (under randomized reductions) even\nin the smoothed analysis setting, smoothing with noise of constant magnitude.\nThis gives a strong negative answer to conjectures of Spielman and Teng [ST06]\nand Cheng, Deng, and Teng [CDT09].\n  In contrast to prior work proving PPAD-hardness after smoothing by noise of\nmagnitude $1/\\operatorname{poly}(n)$ [CDT09], our smoothed complexity result is\nnot proved via hardness of approximation for Nash equilibria. This is by\nnecessity, since Nash equilibria can be approximated to constant error in\nquasi-polynomial time [LMM03]. Our results therefore separate smoothed\ncomplexity and hardness of approximation for Nash equilibria in two-player\ngames.\n  The key ingredient in our reduction is the use of a random zero-sum game as a\ngadget to produce two-player games which remain hard even after smoothing. Our\nanalysis crucially shows that all Nash equilibria of random zero-sum games are\nfar from pure (with high probability), and that this remains true even after\nsmoothing.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 14:33:43 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Boodaghians", "Shant", ""], ["Brakensiek", "Joshua", ""], ["Hopkins", "Samuel B.", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "2007.10924", "submitter": "Daowen Qiu", "authors": "Guoliang Xu and Daowen Qiu", "title": "Partial Boolean functions with exact quantum 1-query complexity", "comments": "11pages; comments are welcome", "journal-ref": null, "doi": "10.3390/e23020189", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two sufficient and necessary conditions to characterize any\n$n$-bit partial Boolean function with exact quantum 1-query complexity. Using\nthe first characterization, we present all $n$-bit partial Boolean functions\nthat depend on $n$ bits and have exact quantum 1-query complexity. Due to the\nsecond characterization, we construct a function $F$ that maps any $n$-bit\npartial Boolean function to some integer, and if an $n$-bit partial Boolean\nfunction $f$ depends on $k$ bits and has exact quantum 1-query complexity, then\n$F(f)$ is non-positive. In addition, we show that the number of all $n$-bit\npartial Boolean functions that depend on $k$ bits and have exact quantum\n1-query complexity is not bigger than $n^{2}2^{2^{n-1}(1+2^{2-k})+2n^{2}}$ for\nall $n\\geq 3$ and $k\\geq 2$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:34:08 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 06:26:02 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 06:22:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xu", "Guoliang", ""], ["Qiu", "Daowen", ""]]}, {"id": "2007.11398", "submitter": "Peter Chini", "authors": "Peter Chini, Prakash Saivasan", "title": "A Framework for Consistency Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that provides deterministic consistency algorithms for\ngiven memory models. Such an algorithm checks whether the executions of a\nshared-memory concurrent program are consistent under the axioms defined by a\nmodel. For memory models like SC and TSO, checking consistency is NP-complete.\nOur framework shows, that despite the hardness, fast deterministic consistency\nalgorithms can be obtained by employing tools from fine-grained complexity. The\nframework is based on a universal consistency problem which can be instantiated\nby different memory models. We construct an algorithm for the problem running\nin time O*(2^k), where k is the number of write accesses in the execution that\nis checked for consistency. Each instance of the framework then admits an\nO*(2^k)-time consistency algorithm. By applying the framework, we obtain\ncorresponding consistency algorithms for SC, TSO, PSO, and RMO. Moreover, we\nshow that the obtained algorithms for SC, TSO, and PSO are optimal in the\nfine-grained sense: there is no consistency algorithm for these running in time\n2^o(k) unless the exponential time hypothesis fails.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:18:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Chini", "Peter", ""], ["Saivasan", "Prakash", ""]]}, {"id": "2007.11582", "submitter": "Abhinav Deshpande", "authors": "Abhinav Deshpande, Alexey V. Gorshkov, and Bill Fefferman", "title": "The importance of the spectral gap in estimating ground-state energies", "comments": "32 pages, 4 figures. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of quantum Hamiltonian complexity lies at the intersection of\nquantum many-body physics and computational complexity theory, with deep\nimplications to both fields. The main object of study is the LocalHamiltonian\nproblem, which is concerned with estimating the ground-state energy of a local\nHamiltonian and is complete for the class QMA, a quantum generalization of the\nclass NP. A major challenge in the field is to understand the complexity of the\nLocalHamiltonian problem in more physically natural parameter regimes. One\ncrucial parameter in understanding the ground space of any Hamiltonian in\nmany-body physics is the spectral gap, which is the difference between the\nsmallest two eigenvalues. Despite its importance in quantum many-body physics,\nthe role played by the spectral gap in the complexity of the LocalHamiltonian\nis less well-understood. In this work, we make progress on this question by\nconsidering the precise regime, in which one estimates the ground-state energy\nto within inverse exponential precision. Computing ground-state energies\nprecisely is a task that is important for quantum chemistry and quantum\nmany-body physics.\n  In the setting of inverse-exponential precision, there is a surprising result\nthat the complexity of LocalHamiltonian is magnified from QMA to PSPACE, the\nclass of problems solvable in polynomial space. We clarify the reason behind\nthis boost in complexity. Specifically, we show that the full complexity of the\nhigh precision case only comes about when the spectral gap is exponentially\nsmall. As a consequence of the proof techniques developed to show our results,\nwe uncover important implications for the representability and circuit\ncomplexity of ground states of local Hamiltonians, the theory of uniqueness of\nquantum witnesses, and techniques for the amplification of quantum witnesses in\nthe presence of postselection.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:00:00 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Deshpande", "Abhinav", ""], ["Gorshkov", "Alexey V.", ""], ["Fefferman", "Bill", ""]]}, {"id": "2007.11964", "submitter": "Marios Ioannou", "authors": "Marios Ioannou, Stephen Piddock, Milad Marvian, Joel Klassen and\n  Barbara M. Terhal", "title": "Sign-curing local Hamiltonians: termwise versus global stoquasticity and\n  the use of Clifford transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We elucidate the distinction between global and termwise stoquasticity for\nlocal Hamiltonians and prove several complexity results. We prove coNP-hardness\nof deciding global stoquasticity in a fixed basis and $\\Sigma_2^p$-hardness of\ndeciding global stoquasticity under single-qubit transformations. We expand the\nclass of sign-curing transformations by showing how Clifford transformations\ncan sign-cure a class of disordered 1D XYZ Hamiltonians.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:29:46 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ioannou", "Marios", ""], ["Piddock", "Stephen", ""], ["Marvian", "Milad", ""], ["Klassen", "Joel", ""], ["Terhal", "Barbara M.", ""]]}, {"id": "2007.11997", "submitter": "Sangram Kishor Jena Mr", "authors": "Sangram K. Jena and Gautam K. Das", "title": "Total Domination in Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph. We call $D_t \\subseteq V$ as a total\ndominating set (TDS) of $G$ if each vertex $v \\in V$ has a dominator in $D$\nother than itself. Here we consider the TDS problem in unit disk graphs, where\nthe objective is to find a minimum cardinality total dominating set for an\ninput graph. We prove that the TDS problem is NP-hard in unit disk graphs.\nNext, we propose an 8-factor approximation algorithm for the problem. The\nrunning time of the proposed approximation algorithm is $O(n \\log k)$, where\n$n$ is the number of vertices of the input graph and $k$ is output size. We\nalso show that TDS problem admits a PTAS in unit disk graphs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:11:19 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jena", "Sangram K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2007.12048", "submitter": "Augusto Modanese", "authors": "Augusto Modanese", "title": "Lower Bounds and Hardness Magnification for Sublinear-Time Shrinking\n  Cellular Automata", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum circuit size problem (MCSP) is a string compression problem with\na parameter $s$ in which, given the truth table of a Boolean function over\ninputs of length $n$, one must answer whether it can be computed by a Boolean\ncircuit of size at most $s(n) \\ge n$. Recently, McKay, Murray, and Williams\n(STOC, 2019) proved a hardness magnification result for MCSP involving\n(one-pass) streaming algorithms: For any reasonable $s$, if there is no\n$\\mathsf{poly}(s(n))$-space streaming algorithm with $\\mathsf{poly}(s(n))$\nupdate time for $\\mathsf{MCSP}[s]$, then $\\mathsf{P} \\neq \\mathsf{NP}$. We\nprove an analogous result for the (provably) strictly less capable model of\nshrinking cellular automata (SCAs), which are cellular automata whose cells can\nspontaneously delete themselves. We show every language accepted by an SCA can\nalso be accepted by a streaming algorithm of similar complexity, and we\nidentify two different aspects in which SCAs are more restricted than streaming\nalgorithms. We also show there is a language which cannot be accepted by any\nSCA in $o(n / \\log n)$ time, even though it admits an $O(\\log n)$-space\nstreaming algorithm with $O(\\log n)$ update time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:53:02 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 13:10:43 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 18:02:55 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Modanese", "Augusto", ""]]}, {"id": "2007.12307", "submitter": "Alex Gavryushkin", "authors": "Lena Collienne and Alex Gavryushkin", "title": "Computing nearest neighbour interchange distances between ranked\n  phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular algorithms for searching the space of leaf-labelled trees are\nbased on tree rearrangement operations. Under any such operation, the problem\nis reduced to searching a graph where vertices are trees and (undirected) edges\nare given by pairs of trees connected by one rearrangement operation (sometimes\ncalled a move). Most popular are the classical nearest neighbour interchange,\nsubtree prune and regraft, and tree bisection and reconnection moves. The\nproblem of computing distances, however, is NP-hard in each of these graphs,\nmaking tree inference and comparison algorithms challenging to design in\npractice.\n  Although ranked phylogenetic trees are one of the central objects of interest\nin applications such as cancer research, immunology, and epidemiology, the\ncomputational complexity of the shortest path problem for these trees remained\nunsolved for decades. In this paper, we settle this problem for the ranked\nnearest neighbour interchange operation by establishing that the complexity\ndepends on the weight difference between the two types of tree rearrangements\n(rank moves and edge moves), and varies from quadratic, which is the lowest\npossible complexity for this problem, to NP-hard, which is the highest. In\nparticular, our result provides the first example of a phylogenetic tree\nrearrangement operation for which shortest paths, and hence the distance, can\nbe computed efficiently. Specifically, our algorithm scales to trees with\nthousands of leaves (and likely hundreds of thousands if implemented\nefficiently).\n  We also connect the problem of computing distances in our graph of ranked\ntrees with the well-known version of this problem on unranked trees by\nintroducing a parameter for the weight difference between move types. We\npropose to study a family of shortest path problems indexed by this parameter\nwith computational complexity varying from quadratic to NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 00:50:45 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Collienne", "Lena", ""], ["Gavryushkin", "Alex", ""]]}, {"id": "2007.12372", "submitter": "Ronny Tredup", "authors": "Ronny Tredup and Evgeny Erofeev", "title": "On the Parameterized Complexity of Synthesizing Boolean Petri Nets With\n  Restricted Dependency (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $\\tau$-synthesis consists in deciding whether a given directed\nlabeled graph $A$ is isomorphic to the reachability graph of a Boolean Petri\nnet $N$ of type $\\tau$. In case of a positive decision, $N$ should be\nconstructed. For many Boolean types of nets, the problem is NP-complete. This\npaper deals with a special variant of $\\tau$-synthesis that imposes\nrestrictions for the target net $N$: we investigate \\emph{dependency\n$d$-restricted $\\tau$-synthesis (DR$\\tau$S)} where each place of $N$ can\ninfluence and be influenced by at most $d$ transitions. For a type $\\tau$, if\n$\\tau$-synthesis is NP-complete then DR$\\tau$S is also NP-complete. In this\npaper, we show that DR$\\tau$S parameterized by $d$ is in XP. Furthermore, we\nprove that it is $W[2]$-hard, for many Boolean types that allow unconditional\ninteractions $set$ and $reset$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:37:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Tredup", "Ronny", ""], ["Erofeev", "Evgeny", ""]]}, {"id": "2007.12651", "submitter": "Marcos Villagra", "authors": "Javier T. Akagi and Eduardo A. Canale and Marcos Villagra", "title": "Tromino Tilings with Pegs via Flow Networks", "comments": "9 pages, 7 figures. Lemmas 4.1 and 4.2 were revised, but the the\n  final result is unaffected. To appear in Proceedings of LAGOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tromino tiling problem is a packing puzzle where we are given a region of\nconnected lattice squares and we want to decide whether there exists a tiling\nof the region using trominoes with the shape of an L. In this work we study a\nslight variation of the tromino tiling problem where some positions of the\nregion have pegs and each tromino comes with a hole that can only be placed on\ntop of the pegs. We present a characterization of this tiling problem with pegs\nusing flow networks and show that (i) there exists a linear-time parsimonious\nreduction to the maximum-flow problem, and (ii) counting the number of such\ntilings can be done in linear-time. The proofs of both results contain\nalgorithms that can then be used to decide the tiling of a region with pegs in\n$O(n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:06:06 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 13:28:11 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Akagi", "Javier T.", ""], ["Canale", "Eduardo A.", ""], ["Villagra", "Marcos", ""]]}, {"id": "2007.12940", "submitter": "Aleksander Mendoza-Drosik", "authors": "Aleksander Mendoza-Drosik", "title": "Multitape automata and finite state transducers with lexicographic\n  weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite state transducers, multitape automata and weighted automata have a lot\nin common. By studying their universal foundations, one can discover some new\ninsights into all of them. The main result presented here is the introduction\nof lexicographic finite state transducers, that could be seen as intermediate\nmodel between multitape automata and weighted transducers. Their most\nsignificant advantage is being equivalent, but often exponentially smaller than\neven smallest nondeterministic automata without weights. Lexicographic\ntransducers were discovered by taking inspiration from Eilenberg's algebraic\napproach to automata and Solomonoff's treatment of a priori probability.\nTherefore, a quick and concise survey of those topics is presented, prior to\nintroducing lexicographic transducers.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 14:28:38 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 11:26:29 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 11:42:07 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 13:44:10 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mendoza-Drosik", "Aleksander", ""]]}, {"id": "2007.13179", "submitter": "Mahsa Shirmohammadi", "authors": "Nikhil Balaji, Sylvain Perifel, Mahsa Shirmohammadi, James Worrell", "title": "Cyclotomic Identity Testing and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the cyclotomic identity testing (CIT) problem: given a polynomial\n$f(x_1,\\ldots,x_k)$, decide whether $f(\\zeta_n^{e_1},\\ldots,\\zeta_n^{e_k})$ is\nzero, where $\\zeta_n = e^{2\\pi i/n}$ is a primitive complex $n$-th root of\nunity and $e_1,\\ldots,e_k$ are integers, represented in binary. When $f$ is\ngiven by an algebraic circuit, we give a randomized polynomial-time algorithm\nfor CIT assuming the generalised Riemann hypothesis (GRH), and show that the\nproblem is in coNP unconditionally. When $f$ is given by a circuit of\npolynomially bounded degree, we give a randomized NC algorithm. In case $f$ is\na linear form we show that the problem lies in NC. Towards understanding when\nCIT can be solved in deterministic polynomial-time, we consider so-called\ndiagonal depth-3 circuits, i.e., polynomials $f=\\sum_{i=1}^m g_i^{d_i}$, where\n$g_i$ is a linear form and $d_i$ a positive integer given in unary. We observe\nthat a polynomial-time algorithm for CIT on this class would yield a\nsub-exponential-time algorithm for polynomial identity testing. However,\nassuming GRH, we show that if the linear forms~$g_i$ are all identical then CIT\ncan be solved in polynomial time. Finally, we use our results to give a new\nproof that equality of compressed strings, i.e., strings presented using\ncontext-free grammars, can be decided in randomized NC.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 17:02:30 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:51:53 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Balaji", "Nikhil", ""], ["Perifel", "Sylvain", ""], ["Shirmohammadi", "Mahsa", ""], ["Worrell", "James", ""]]}, {"id": "2007.13214", "submitter": "Qi Cheng", "authors": "Qi Cheng and J. Maurice Rojas and Daqing Wan", "title": "Computing zeta functions of large polynomial systems over finite fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve the algorithms of Lauder-Wan \\cite{LW} and Harvey\n\\cite{Ha} to compute the zeta function of a system of $m$ polynomial equations\nin $n$ variables over the finite field $\\FF_q$ of $q$ elements, for $m$ large.\nThe dependence on $m$ in the original algorithms was exponential in $m$. Our\nmain result is a reduction of the exponential dependence on $m$ to a polynomial\ndependence on $m$. As an application, we speed up a doubly exponential time\nalgorithm from a software verification paper \\cite{BJK} (on universal\nequivalence of programs over finite fields) to singly exponential time. One key\nnew ingredient is an effective version of the classical Kronecker theorem which\n(set-theoretically) reduces the number of defining equations for a \"large\"\npolynomial system over $\\FF_q$ when $q$ is suitably large.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:38:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cheng", "Qi", ""], ["Rojas", "J. Maurice", ""], ["Wan", "Daqing", ""]]}, {"id": "2007.13564", "submitter": "Nikolajs Nahimovs", "authors": "Nikolajs Nahimovs", "title": "Lackadaisical quantum walks on triangular and honeycomb 2D grids", "comments": "arXiv admin note: text overlap with arXiv:1808.00672", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the typical model, a discrete-time coined quantum walk search has the same\nrunning time of $O(\\sqrt{N} \\log{N})$ for 2D rectangular, triangular and\nhoneycomb grids. It is known that for 2D rectangular grid the running time can\nbe improved to $O(\\sqrt{N \\log{N}})$ using several different techniques. One of\nsuch techniques is adding a self-loop of weight $4/N$ to each vertex (i.e.\nmaking the walk lackadaisical).\n  In this paper we apply lackadaisical approach to quantum walk search on\ntriangular and honeycomb 2D grids. We show that for both types of grids adding\na self-loop of weight $6/N$ and $3/N$ for triangular and honeycomb grids,\nrespectively, results in $O(\\sqrt{N \\log{N}})$ running time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 13:01:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Nahimovs", "Nikolajs", ""]]}, {"id": "2007.13594", "submitter": "Silvia Butti", "authors": "Silvia Butti, Victor Dalmau", "title": "The Complexity of the Distributed Constraint Satisfaction Problem", "comments": "Full version of a STACS'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Distributed Constraint Satisfaction Problem\n(DCSP) on a synchronous, anonymous network from a theoretical standpoint. In\nthis setting, variables and constraints are controlled by agents which\ncommunicate with each other by sending messages through fixed communication\nchannels. Our results endorse the well-known fact from classical CSPs that the\ncomplexity of fixed-template computational problems depends on the template's\ninvariance under certain operations. Specifically, we show that DCSP($\\Gamma$)\nis polynomial-time tractable if and only if $\\Gamma$ is invariant under\nsymmetric polymorphisms of all arities. Otherwise, there are no algorithms that\nsolve DCSP($\\Gamma$) in finite time. We also show that the same condition holds\nfor the search variant of DCSP. Collaterally, our results unveil a feature of\nthe processes' neighbourhood in a distributed network, its iterated degree,\nwhich plays a major role in the analysis. We explore this notion establishing a\ntight connection with the basic linear programming relaxation of a CSP.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:23:26 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 13:16:37 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Butti", "Silvia", ""], ["Dalmau", "Victor", ""]]}, {"id": "2007.14045", "submitter": "Marcelo Arenas", "authors": "Marcelo Arenas, Pablo Barcel\\'o Leopoldo Bertossi, Mika\\\"el Monet", "title": "The Tractability of SHAP-Score-Based Explanations over Deterministic and\n  Decomposable Boolean Circuits", "comments": "17 pages, including 8 pages of main text. arXiv version of the\n  AAAI'21 conference paper. Except from the addition of the technical appendix,\n  the content is the same as the AAAI one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scores based on Shapley values are widely used for providing explanations to\nclassification results over machine learning models. A prime example of this is\nthe influential SHAP-score, a version of the Shapley value that can help\nexplain the result of a learned model on a specific entity by assigning a score\nto every feature. While in general computing Shapley values is a\ncomputationally intractable problem, it has recently been claimed that the\nSHAP-score can be computed in polynomial time over the class of decision trees.\nIn this paper, we provide a proof of a stronger result over Boolean models: the\nSHAP-score can be computed in polynomial time over deterministic and\ndecomposable Boolean circuits. Such circuits, also known as tractable Boolean\ncircuits, generalize a wide range of Boolean circuits and binary decision\ndiagrams classes, including binary decision trees, Ordered Binary Decision\nDiagrams (OBDDs) and Free Binary Decision Diagrams (FBDDs). We also establish\nthe computational limits of the notion of SHAP-score by observing that, under a\nmild condition, computing it over a class of Boolean models is always\npolynomially as hard as the model counting problem for that class. This implies\nthat both determinism and decomposability are essential properties for the\ncircuits that we consider, as removing one or the other renders the problem of\ncomputing the SHAP-score intractable (namely, #P-hard).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:04:28 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 11:38:00 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 16:34:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Arenas", "Marcelo", ""], ["Bertossi", "Pablo Barcel\u00f3 Leopoldo", ""], ["Monet", "Mika\u00ebl", ""]]}, {"id": "2007.14092", "submitter": "Petteri Kaski", "authors": "Andreas Bj\\\"orklund, Petteri Kaski", "title": "Counting Short Vector Pairs by Inner Product and Relations to the\n  Permanent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given as input two $n$-element sets $\\mathcal A,\\mathcal B\\subseteq\\{0,1\\}^d$\nwith $d=c\\log n\\leq(\\log n)^2/(\\log\\log n)^4$ and a target $t\\in\n\\{0,1,\\ldots,d\\}$, we show how to count the number of pairs $(x,y)\\in \\mathcal\nA\\times \\mathcal B$ with integer inner product $\\langle x,y \\rangle=t$\ndeterministically, in $n^2/2^{\\Omega\\bigl(\\!\\sqrt{\\log n\\log \\log n/(c\\log^2\nc)}\\bigr)}$ time. This demonstrates that one can solve this problem in\ndeterministic subquadratic time almost up to $\\log^2 n$ dimensions, nearly\nmatching the dimension bound of a subquadratic randomized detection algorithm\nof Alman and Williams [FOCS 2015]. We also show how to modify their randomized\nalgorithm to count the pairs w.h.p., to obtain a fast randomized algorithm. Our\ndeterministic algorithm builds on a novel technique of reconstructing a\nfunction from sum-aggregates by prime residues, which can be seen as an {\\em\nadditive} analog of the Chinese Remainder Theorem. As our second contribution,\nwe relate the fine-grained complexity of the task of counting of vector pairs\nby inner product to the task of computing a zero-one matrix permanent over the\nintegers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:55:29 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""], ["Kaski", "Petteri", ""]]}, {"id": "2007.14161", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Colin Geniet, Eun Jung Kim, St\\'ephan Thomass\\'e,\n  R\\'emi Watrigant", "title": "Twin-width III: Max Independent Set, Min Dominating Set, and Coloring", "comments": "38 pages, 6 figures. This version contains more results, notably the\n  approximation for Min Dominating Set, and the title has been edited\n  accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently introduced the graph invariant twin-width, and showed that\nfirst-order model checking can be solved in time $f(d,k)n$ for $n$-vertex\ngraphs given with a witness that the twin-width is at most $d$, called\n$d$-contraction sequence or $d$-sequence, and formulas of size $k$ [Bonnet et\nal., FOCS '20]. The inevitable price to pay for such a general result is that\n$f$ is a tower of exponentials of height roughly $k$. In this paper, we show\nthat algorithms based on twin-width need not be impractical. We present\n$2^{O(k)}n$-time algorithms for $k$-Independent Set, $r$-Scattered Set,\n$k$-Clique, and $k$-Dominating Set when an $O(1)$-sequence is provided. We\nfurther show how to solve weighted $k$-Independent Set, Subgraph Isomorphism,\nand Induced Subgraph Isomorphism, in time $2^{O(k \\log k)}n$. These algorithms\nare based on a dynamic programming scheme following the sequence of\ncontractions forward. We then show a second algorithmic use of the contraction\nsequence, by starting at its end and rewinding it. As an example, we establish\nthat bounded twin-width classes are $\\chi$-bounded. This significantly extends\nthe $\\chi$-boundedness of bounded rank-width classes, and does so with a very\nconcise proof. The third algorithmic use of twin-width builds on the second\none. Playing the contraction sequence backward, we show that bounded twin-width\ngraphs can be edge-partitioned into a linear number of bicliques, such that\nboth sides of the bicliques are on consecutive vertices, in a fixed vertex\nordering. Given that biclique edge-partition, we show how to solve the\nunweighted Single-Source Shortest Paths and hence All-Pairs Shortest Paths in\nsublinear time $O(n \\log n)$ and time $O(n^2 \\log n)$, respectively. Finally we\nshow that Min Dominating Set and related problems have constant integrality\ngaps on bounded twin-width classes, thereby getting constant approximations on\nthese classes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:36:03 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:32:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Geniet", "Colin", ""], ["Kim", "Eun Jung", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2007.14169", "submitter": "Matthias Lanzinger", "authors": "Hubie Chen, Georg Gottlob, Matthias Lanzinger, Reinhard Pichler", "title": "Semantic Width and the Fixed-Parameter Tractability of Constraint\n  Satisfaction Problems", "comments": "Full and extended version of the IJCAI2020 paper with the same title", "journal-ref": null, "doi": "10.24963/ijcai.2020/239", "report-no": null, "categories": "cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint satisfaction problems (CSPs) are an important formal framework for\nthe uniform treatment of various prominent AI tasks, e.g., coloring or\nscheduling problems. Solving CSPs is, in general, known to be NP-complete and\nfixed-parameter intractable when parameterized by their constraint scopes. We\ngive a characterization of those classes of CSPs for which the problem becomes\nfixed-parameter tractable.\n  Our characterization significantly increases the utility of the CSP framework\nby making it possible to decide the fixed-parameter tractability of problems\nvia their CSP formulations.\n  We further extend our characterization to the evaluation of unions of\nconjunctive queries, a fundamental problem in databases. Furthermore, we\nprovide some new insight on the frontier of PTIME solvability of CSPs.\n  In particular, we observe that bounded fractional hypertree width is more\ngeneral than bounded hypertree width only for classes that exhibit a certain\ntype of exponential growth.\n  The presented work resolves a long-standing open problem and yields powerful\nnew tools for complexity research in AI and database theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:44:03 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Hubie", ""], ["Gottlob", "Georg", ""], ["Lanzinger", "Matthias", ""], ["Pichler", "Reinhard", ""]]}, {"id": "2007.14179", "submitter": "\\'Edouard Bonnet", "authors": "Benjamin Bergougnoux, \\'Edouard Bonnet, Nick Brettell, O-joung Kwon", "title": "Close relatives of Feedback Vertex Set without single-exponential\n  algorithms parameterized by treewidth", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cut & Count technique and the rank-based approach have lead to\nsingle-exponential FPT algorithms parameterized by treewidth, that is, running\nin time $2^{O(tw)}n^{O(1)}$, for Feedback Vertex Set and connected versions of\nthe classical graph problems (such as Vertex Cover and Dominating Set). We show\nthat Subset Feedback Vertex Set, Subset Odd Cycle Transversal, Restricted\nEdge-Subset Feedback Edge Set, Node Multiway Cut, and Multiway Cut are unlikely\nto have such running times. More precisely, we match algorithms running in time\n$2^{O(tw \\log tw)}n^{O(1)}$ with tight lower bounds under the Exponential-Time\nHypothesis (ETH), ruling out $2^{o(tw \\log tw)}n^{O(1)}$, where $n$ is the\nnumber of vertices and $tw$ is the treewidth of the input graph. Our algorithms\nextend to the weighted case, while our lower bounds also hold for the larger\nparameter pathwidth and do not require weights. We also show that, in contrast\nto Odd Cycle Transversal, there is no $2^{o(tw \\log tw)}n^{O(1)}$-time\nalgorithm for Even Cycle Transversal under the ETH.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:06:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Bonnet", "\u00c9douard", ""], ["Brettell", "Nick", ""], ["Kwon", "O-joung", ""]]}, {"id": "2007.14225", "submitter": "Mingyu Xiao", "authors": "Zhenyu Guo, Mingyu Xiao and Yi Zhou", "title": "The Complexity of the Partition Coloring Problem", "comments": "To appear in TAMC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple undirected graph $G=(V,E)$ and a partition of the vertex set\n$V$ into $p$ parts, the \\textsc{Partition Coloring Problem} asks if we can\nselect one vertex from each part of the partition such that the chromatic\nnumber of the subgraph induced on the $p$ selected vertices is bounded by $k$.\nPCP is a generalized problem of the classical \\textsc{Vertex Coloring Problem}\nand has applications in many areas, such as scheduling and encoding etc.\n  In this paper, we show the complexity status of the \\textsc{Partition\nColoring Problem} with three parameters: the number of colors, the number of\nparts of the partition, and the maximum size of each part of the partition.\n  Furthermore, we give a new exact algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:51:11 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Guo", "Zhenyu", ""], ["Xiao", "Mingyu", ""], ["Zhou", "Yi", ""]]}, {"id": "2007.14339", "submitter": "Ajinkya Ramdas Gaikwad", "authors": "Ajinkya Gaikwad, Soumen Maity, Shuvam Kant Tripathi", "title": "The Satisfactory Partition Problem", "comments": "1-15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Satisfactory Partition problem consists in deciding if the set of\nvertices of a given undirected graph can be partitioned into two nonempty parts\nsuch that each vertex has at least as many neighbours in its part as in the\nother part. This problem was introduced by Gerber and Kobler [European J. Oper.\nRes. 125 (2000) 283-291] and further studied by other authors, but its\nparameterized complexity remains open until now. It is known that the\nSatisfactory Partition problem, as well as a variant where the parts are\nrequired to be of the same cardinality, are NP-complete. We enhance our\nunderstanding of the problem from the viewpoint of parameterized complexity by\nshowing that (1) the problem is FPT when parameterized by the neighbourhood\ndiversity of the input graph, (2) it can be solved in $O(n^{8 {\\tt cw}})$ where\n${\\tt cw}$ is the clique-width,(3) a generalized version of the problem is\nW[1]-hard when parameterized by the treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:15:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Gaikwad", "Ajinkya", ""], ["Maity", "Soumen", ""], ["Tripathi", "Shuvam Kant", ""]]}, {"id": "2007.14346", "submitter": "Evira Mayordomo", "authors": "Jack H. Lutz and Elvira Mayordomo", "title": "Algorithmic Fractal Dimensions in Geometric Measure Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of algorithmic fractal dimensions in this century has had\nmany fruitful interactions with geometric measure theory, especially fractal\ngeometry in Euclidean spaces. We survey these developments, with emphasis on\nconnections with computable functions on the reals, recent uses of algorithmic\ndimensions in proving new theorems in classical (non-algorithmic) fractal\ngeometry, and directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:24:40 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Lutz", "Jack H.", ""], ["Mayordomo", "Elvira", ""]]}, {"id": "2007.15125", "submitter": "Alexandros Hollender", "authors": "Argyrios Deligkas, Aris Filos-Ratsikas, Alexandros Hollender", "title": "Two's Company, Three's a Crowd: Consensus-Halving for a Constant Number\n  of Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $\\varepsilon$-Consensus-Halving problem, in which a set of\nheterogeneous agents aim at dividing a continuous resource into two (not\nnecessarily contiguous) portions that all of them simultaneously consider to be\nof approximately the same value (up to $\\varepsilon$). This problem was\nrecently shown to be PPA-complete, for $n$ agents and $n$ cuts, even for very\nsimple valuation functions. In a quest to understand the root of the complexity\nof the problem, we consider the setting where there is only a constant number\nof agents, and we consider both the computational complexity and the query\ncomplexity of the problem.\n  For agents with monotone valuation functions, we show a dichotomy: for two\nagents the problem is polynomial-time solvable, whereas for three or more\nagents it becomes PPA-complete. Similarly, we show that for two monotone agents\nthe problem can be solved with polynomially-many queries, whereas for three or\nmore agents, we provide exponential query complexity lower bounds. These\nresults are enabled via an interesting connection to a monotone Borsuk-Ulam\nproblem, which may be of independent interest. For agents with general\nvaluations, we show that the problem is PPA-complete and admits exponential\nquery complexity lower bounds, even for two agents.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:46:01 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Filos-Ratsikas", "Aris", ""], ["Hollender", "Alexandros", ""]]}, {"id": "2007.15220", "submitter": "Pasin Manurangsi", "authors": "Ilias Diakonikolas, Daniel M. Kane, Pasin Manurangsi", "title": "The Complexity of Adversarially Robust Proper Learning of Halfspaces\n  with Agnostic Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of adversarially robust proper learning\nof halfspaces in the distribution-independent agnostic PAC model, with a focus\non $L_p$ perturbations. We give a computationally efficient learning algorithm\nand a nearly matching computational hardness result for this problem. An\ninteresting implication of our findings is that the $L_{\\infty}$ perturbations\ncase is provably computationally harder than the case $2 \\leq p < \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:18:51 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Manurangsi", "Pasin", ""]]}]