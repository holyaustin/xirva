[{"id": "1707.00118", "submitter": "Bernd Schuh", "authors": "Bernd R. Schuh", "title": "A criterion for \"easiness\" of certain SAT problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized 1-in-3SAT problem is defined and found to be in complexity\nclass P when restricted to a certain subset of CNF expressions. In particular,\n1-in-kSAT with no restrictions on the number of literals per clause can be\ndecided in polynomial time when restricted to exact READ-3 formulas with equal\nnumber of clauses (m) and variables (n), and no pure literals. Also individual\ninstances can be checked for easiness with respect to a given SAT problem. By\nidentifying whole classes of formulas as being solvable efficiently the\napproach might be of interest also in the complementary search for hard\ninstances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:03:59 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Schuh", "Bernd R.", ""]]}, {"id": "1707.00337", "submitter": "Mohammadreza Samadi", "authors": "Frank E. Curtis, Daniel P. Robinson, Mohammadreza Samadi", "title": "Complexity Analysis of a Trust Funnel Algorithm for Equality Constrained\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "16T-013 (ISE Department, Lehigh University, Bethlehem, PA, USA)", "categories": "math.NA cs.CC cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is proposed for solving equality constrained nonlinear optimization\nproblems involving twice continuously differentiable functions. The method\nemploys a trust funnel approach consisting of two phases: a first phase to\nlocate an $\\epsilon$-feasible point and a second phase to seek optimality while\nmaintaining at least $\\epsilon$-feasibility. A two-phase approach of this kind\nbased on a cubic regularization methodology was recently proposed along with a\nsupporting worst-case iteration complexity analysis. Unfortunately, however, in\nthat approach, the objective function is completely ignored in the first phase\nwhen $\\epsilon$-feasibility is sought. The main contribution of the method\nproposed in this paper is that the same worst-case iteration complexity is\nachieved, but with a first phase that also accounts for improvements in the\nobjective function. As such, the method typically requires fewer iterations in\nthe second phase, as the results of numerical experiments demonstrate.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 19:25:38 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Curtis", "Frank E.", ""], ["Robinson", "Daniel P.", ""], ["Samadi", "Mohammadreza", ""]]}, {"id": "1707.00362", "submitter": "Josh Alman", "authors": "Josh Alman, Matthias Mnich, Virginia Vassilevska Williams", "title": "Dynamic Parameterized Problems and Algorithms", "comments": "40 pages, appears in ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-parameter algorithms and kernelization are two powerful methods to\nsolve $\\mathsf{NP}$-hard problems. Yet, so far those algorithms have been\nlargely restricted to static inputs.\n  In this paper we provide fixed-parameter algorithms and kernelizations for\nfundamental $\\mathsf{NP}$-hard problems with dynamic inputs. We consider a\nvariety of parameterized graph and hitting set problems which are known to have\n$f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider the\nquestion of whether there is a data structure that supports small updates (such\nas edge/vertex/set/element insertions and deletions) with an update time of\n$g(k)n^{o(1)}$; such an update time would be essentially optimal. Update and\nquery times independent of $n$ are particularly desirable. Among many other\nresults, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithms\nwith $f(k)\\log^{O(1)}n$ update and query times for some function $f$ depending\non the solution size $k$ only.\n  We complement our positive results by several conditional and unconditional\nlower bounds. For example, we show that unlike their undirected counterparts,\nDirected Feedback Vertex Set and Directed $k$-Path do not admit dynamic\nalgorithms with $n^{o(1)}$ update and query times even for constant solution\nsizes $k\\leq 3$, assuming popular hardness hypotheses. We also show that\nunconditionally, in the cell probe model, Directed Feedback Vertex Set cannot\nbe solved with update time that is purely a function of $k$.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 22:34:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Alman", "Josh", ""], ["Mnich", "Matthias", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1707.00373", "submitter": "Zhiguo Fu", "authors": "Zhiguo Fu", "title": "On Blockwise Symmetric Matchgate Signatures and Higher Domain \\#CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any $n\\geq 3$ and $ q\\geq 3$, we prove that the {\\sc Equality} function\n$(=_n)$ on $n$ variables over a domain of size $q$ cannot be realized by\nmatchgates under holographic transformations. This is a consequence of our\ntheorem on the structure of blockwise symmetric matchgate signatures. %due to\nthe rank of the matrix form of the blockwise symmetric standard signatures,\n%where $(=_n)$ is an equality signature on domain $\\{0, 1, \\cdots, q-1\\}$. This\nhas the implication that the standard holographic algorithms based on\nmatchgates, a methodology known to be universal for \\#CSP over the Boolean\ndomain, cannot produce P-time algorithms for planar \\#CSP over any higher\ndomain $q\\geq 3$.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 00:20:17 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Fu", "Zhiguo", ""]]}, {"id": "1707.01204", "submitter": "Santosh Vempala", "authors": "Manuel Blum and Santosh Vempala", "title": "The Complexity of Human Computation: A Concrete Model with an\n  Application to Passwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What can humans compute in their heads? We are thinking of a variety of\nCrypto Protocols, games like Sudoku, Crossword Puzzles, Speed Chess, and so on.\nThe intent of this paper is to apply the ideas and methods of theoretical\ncomputer science to better understand what humans can compute in their heads.\nFor example, can a person compute a function in their head so that an\neavesdropper with a powerful computer --- who sees the responses to random\ninput --- still cannot infer responses to new inputs? To address such\nquestions, we propose a rigorous model of human computation and associated\nmeasures of complexity. We apply the model and measures first and foremost to\nthe problem of (1) humanly computable password generation, and then consider\nrelated problems of (2) humanly computable \"one-way functions\" and (3) humanly\ncomputable \"pseudorandom generators\".\n  The theory of Human Computability developed here plays by different rules\nthan standard computability, and this takes some getting used to. For reasons\nto be made clear, the polynomial versus exponential time divide of modern\ncomputability theory is irrelevant to human computation. In human\ncomputability, the step-counts for both humans and computers must be more\nconcrete. Specifically, we restrict the adversary to at most 10^24 (Avogadro\nnumber of) steps. An alternate view of this work is that it deals with the\nanalysis of algorithms and counting steps for the case that inputs are small as\nopposed to the usual case of inputs large-in-the-limit.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:25:52 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Blum", "Manuel", ""], ["Vempala", "Santosh", ""]]}, {"id": "1707.01242", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Learning Geometric Concepts with Nasty Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient learnability of geometric concept classes -\nspecifically, low-degree polynomial threshold functions (PTFs) and\nintersections of halfspaces - when a fraction of the data is adversarially\ncorrupted. We give the first polynomial-time PAC learning algorithms for these\nconcept classes with dimension-independent error guarantees in the presence of\nnasty noise under the Gaussian distribution. In the nasty noise model, an\nomniscient adversary can arbitrarily corrupt a small fraction of both the\nunlabeled data points and their labels. This model generalizes well-studied\nnoise models, including the malicious noise model and the agnostic (adversarial\nlabel noise) model. Prior to our work, the only concept class for which\nefficient malicious learning algorithms were known was the class of\norigin-centered halfspaces.\n  Specifically, our robust learning algorithm for low-degree PTFs succeeds\nunder a number of tame distributions -- including the Gaussian distribution\nand, more generally, any log-concave distribution with (approximately) known\nlow-degree moments. For LTFs under the Gaussian distribution, we give a\npolynomial-time algorithm that achieves error $O(\\epsilon)$, where $\\epsilon$\nis the noise rate. At the core of our PAC learning results is an efficient\nalgorithm to approximate the low-degree Chow-parameters of any bounded function\nin the presence of nasty noise. To achieve this, we employ an iterative\nspectral method for outlier detection and removal, inspired by recent work in\nrobust unsupervised learning. Our aforementioned algorithm succeeds for a range\nof distributions satisfying mild concentration bounds and moment assumptions.\nThe correctness of our robust learning algorithm for intersections of\nhalfspaces makes essential use of a novel robust inverse independence lemma\nthat may be of broader interest.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 07:41:40 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1707.01470", "submitter": "Arkadiusz Socala", "authors": "Marthe Bonamy, {\\L}ukasz Kowalik, Jesper Nederlof, Micha{\\l}\n  Pilipczuk, Arkadiusz Soca{\\l}a, Marcin Wrochna", "title": "On Directed Feedback Vertex Set parameterized by treewidth", "comments": "20p", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Directed Feedback Vertex Set problem parameterized by the\ntreewidth of the input graph. We prove that unless the Exponential Time\nHypothesis fails, the problem cannot be solved in time $2^{o(t\\log t)}\\cdot\nn^{\\mathcal{O}(1)}$ on general directed graphs, where $t$ is the treewidth of\nthe underlying undirected graph. This is matched by a dynamic programming\nalgorithm with running time $2^{\\mathcal{O}(t\\log t)}\\cdot n^{\\mathcal{O}(1)}$.\nOn the other hand, we show that if the input digraph is planar, then the\nrunning time can be improved to $2^{\\mathcal{O}(t)}\\cdot n^{\\mathcal{O}(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:07:50 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 19:48:39 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Bonamy", "Marthe", ""], ["Kowalik", "\u0141ukasz", ""], ["Nederlof", "Jesper", ""], ["Pilipczuk", "Micha\u0142", ""], ["Soca\u0142a", "Arkadiusz", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1707.01702", "submitter": "Fabrizio Grandoni", "authors": "Marek Adamczyk, Fabrizio Grandoni, Stefano Leonardi, MIchal Wlodarczyk", "title": "When the Optimum is also Blind: a New Perspective on Universal\n  Optimization", "comments": "Full version of ICALP'17 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following variant of the set cover problem. We are given a\nuniverse $U=\\{1,...,n\\}$ and a collection of subsets $\\mathcal{C} =\n\\{S_1,...,S_m\\}$ where $S_i \\subseteq U$. For every element $u \\in U$ we need\nto find a set $\\phi(u) \\in \\mathcal C$ such that $u\\in \\phi(u)$. Once we\nconstruct and fix the mapping $\\phi:U \\rightarrow \\mathcal{C}$ a subset $X\n\\subseteq U$ of the universe is revealed, and we need to cover all elements\nfrom $X$ with exactly $\\phi(X):=\\cup_{u\\in X} \\phi(u)$. The goal is to find a\nmapping such that the cover $\\phi(X)$ is as cheap as possible.\n  This is an example of a universal problem where the solution has to be\ncreated before the actual instance to deal with is revealed. Such problems\nappear naturally in some settings when we need to optimize under uncertainty\nand it may be actually too expensive to begin finding a good solution once the\ninput starts being revealed.\n  A rich body of work was devoted to investigate the approximability of such\nproblems under the regime of worst case analysis or when the input instance is\ndrawn randomly from some probability distribution. Here one typically compares\nthe quality of the produced solution with the optimal offline solution.\n  In this paper we consider a different viewpoint: What if we would compare our\napproximate universal solution against an optimal universal solution that obeys\nthe same rules as we do? We show that under this viewpoint it is possible to\nachieve improved approximation algorithms for the stochastic version of\nuniversal set cover. Our result is based on rounding a proper configuration IP\nthat captures the optimal universal solution, and using tools from submodular\noptimization. The same basic approach leads to improved approximation\nalgorithms also for other related problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:30:22 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Adamczyk", "Marek", ""], ["Grandoni", "Fabrizio", ""], ["Leonardi", "Stefano", ""], ["Wlodarczyk", "MIchal", ""]]}, {"id": "1707.01795", "submitter": "Rishi Saket", "authors": "Arnab Bhattacharyya, Suprovat Ghoshal, Rishi Saket", "title": "Hardness of learning noisy halfspaces using polynomial thresholds", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the hardness of weakly learning halfspaces in the presence of\nadversarial noise using polynomial threshold functions (PTFs). In particular,\nwe prove that for any constants $d \\in \\mathbb{Z}^+$ and $\\varepsilon > 0$, it\nis NP-hard to decide: given a set of $\\{-1,1\\}$-labeled points in\n$\\mathbb{R}^n$ whether (YES Case) there exists a halfspace that classifies\n$(1-\\varepsilon)$-fraction of the points correctly, or (NO Case) any degree-$d$\nPTF classifies at most $(1/2 + \\varepsilon)$-fraction of the points correctly.\nThis strengthens to all constant degrees the previous NP-hardness of learning\nusing degree-$2$ PTFs shown by Diakonikolas et al. (2011). The latter result\nhad remained the only progress over the works of Feldman et al. (2006) and\nGuruswami et al. (2006) ruling out weakly proper learning adversarially noisy\nhalfspaces.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 13:56:35 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Ghoshal", "Suprovat", ""], ["Saket", "Rishi", ""]]}, {"id": "1707.01797", "submitter": "Marcin Wrochna", "authors": "Bart M. P. Jansen, Marcin Pilipczuk, Marcin Wrochna", "title": "Turing Kernelization for Finding Long Paths in Graph Classes Excluding a\n  Topological Minor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of Turing kernelization investigates whether a polynomial-time\nalgorithm can solve an NP-hard problem, when it is aided by an oracle that can\nbe queried for the answers to bounded-size subproblems. One of the main open\nproblems in this direction is whether k-Path admits a polynomial Turing kernel:\ncan a polynomial-time algorithm determine whether an undirected graph has a\nsimple path of length k, using an oracle that answers queries of size poly(k)?\n  We show this can be done when the input graph avoids a fixed graph H as a\ntopological minor, thereby significantly generalizing an earlier result for\nbounded-degree and $K_{3,t}$-minor-free graphs. Moreover, we show that k-Path\neven admits a polynomial Turing kernel when the input graph is not\nH-topological-minor-free itself, but contains a known vertex modulator of size\nbounded polynomially in the parameter, whose deletion makes it so. To obtain\nour results, we build on the graph minors decomposition to show that any\nH-topological-minor-free graph that does not contain a k-path, has a separation\nthat can safely be reduced after communication with the oracle.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:00:17 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pilipczuk", "Marcin", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1707.02638", "submitter": "Amer Mouawad", "authors": "Daniel Lokshtanov and Amer E. Mouawad", "title": "The complexity of independent set reconfiguration on bipartite graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We settle the complexity of the Independent Set Reconfiguration problem on\nbipartite graphs under all three commonly studied reconfiguration models. We\nshow that under the token jumping or token addition/removal model the problem\nis NP-complete. For the token sliding model, we show that the problem remains\nPSPACE-complete.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 21:13:35 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Mouawad", "Amer E.", ""]]}, {"id": "1707.03125", "submitter": "Zhaohui Wei", "authors": "Zhaohui Wei and Jamie Sikora", "title": "Device-independent dimension test in a multiparty Bell experiment", "comments": "10 pages, 2 figures", "journal-ref": "New J. Phys. 21, 043021 (2019)", "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A device-independent dimension test for a Bell experiment aims to estimate\nthe underlying Hilbert space dimension that is required to produce given\nmeasurement statistical data without any other assumptions concerning the\nquantum apparatus. Previous work mostly deals with the two-party version of\nthis problem. In this paper, we propose a very general and robust approach to\ntest the dimension of any subsystem in a multiparty Bell experiment. Our\ndimension test stems from the study of a new multiparty scenario which we call\nprepare-and-distribute. This is like the prepare-and-measure scenario, but the\nquantum state is sent to multiple, non-communicating parties. Through specific\nexamples, we show that our test results can be tight. Furthermore, we compare\nthe performance of our test to results based on known bipartite tests, and\nwitness remarkable advantage, which indicates that our test is of a true\nmultiparty nature. We conclude by pointing out that with some partial\ninformation about the quantum states involved in the experiment, it is possible\nto learn other interesting properties beyond dimension.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 04:20:26 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 02:50:50 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Wei", "Zhaohui", ""], ["Sikora", "Jamie", ""]]}, {"id": "1707.03146", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Mikhail Rudoy", "title": "A simple proof that the $(n^2-1)$-puzzle is hard", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.tcs.2018.04.031", "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 15 puzzle is a classic reconfiguration puzzle with fifteen uniquely\nlabeled unit squares within a $4 \\times 4$ board in which the goal is to slide\nthe squares (without ever overlapping) into a target configuration. By\ngeneralizing the puzzle to an $n \\times n$ board with $n^2-1$ squares, we can\nstudy the computational complexity of problems related to the puzzle; in\nparticular, we consider the problem of determining whether a given end\nconfiguration can be reached from a given start configuration via at most a\ngiven number of moves. This problem was shown NP-complete in Ratner and Warmuth\n(1990). We provide an alternative simpler proof of this fact by reduction from\nthe rectilinear Steiner tree problem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 06:46:16 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1707.03324", "submitter": "Zhiqiang Zhou", "authors": "Guanghui Lan and Zhiqiang Zhou", "title": "Dynamic Stochastic Approximation for Multi-stage Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multi-stage stochastic optimization problems with\nconvex objectives and conic constraints at each stage. We present a new\nstochastic first-order method, namely the dynamic stochastic approximation\n(DSA) algorithm, for solving these types of stochastic optimization problems.\nWe show that DSA can achieve an optimal ${\\cal O}(1/\\epsilon^4)$ rate of\nconvergence in terms of the total number of required scenarios when applied to\na three-stage stochastic optimization problem. We further show that this rate\nof convergence can be improved to ${\\cal O}(1/\\epsilon^2)$ when the objective\nfunction is strongly convex. We also discuss variants of DSA for solving more\ngeneral multi-stage stochastic optimization problems with the number of stages\n$T > 3$. The developed DSA algorithms only need to go through the scenario tree\nonce in order to compute an $\\epsilon$-solution of the multi-stage stochastic\noptimization problem. As a result, the memory required by DSA only grows\nlinearly with respect to the number of stages. To the best of our knowledge,\nthis is the first time that stochastic approximation type methods are\ngeneralized for multi-stage stochastic optimization with $T \\ge 3$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 15:29:55 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 22:30:13 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Zhiqiang", ""]]}, {"id": "1707.03584", "submitter": "Benjamin Bergougnoux", "authors": "Benjamin Bergougnoux and Mamadou Moustapha Kant\\'e", "title": "Fast exact algorithms for some connectivity problems parametrized by\n  clique-width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a clique-width $k$-expression of a graph $G$, we provide $2^{O(k)}\\cdot\nn$ time algorithms for connectivity constraints on locally checkable properties\nsuch as Node-Weighted Steiner Tree, Connected Dominating Set, or Connected\nVertex Cover. We also propose a $2^{O(k)}\\cdot n$ time algorithm for Feedback\nVertex Set. The best running times for all the considered cases were either\n$2^{O(k\\cdot \\log(k))}\\cdot n^{O(1)}$ or worse.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:59:47 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 08:07:47 GMT"}, {"version": "v3", "created": "Sat, 18 Aug 2018 08:50:34 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Kant\u00e9", "Mamadou Moustapha", ""]]}, {"id": "1707.03811", "submitter": "Greg Kuperberg", "authors": "Greg Kuperberg (UC Davis) and Eric Samperton (UC Davis)", "title": "Computational complexity and 3-manifolds and zombies", "comments": "20 pages", "journal-ref": "Geom. Topol. 22 (2018) 3623-3670", "doi": "10.2140/gt.2018.22.3623", "report-no": null, "categories": "math.GT cs.CC math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the problem of counting homomorphisms from the fundamental group of a\nhomology $3$-sphere $M$ to a finite, non-abelian simple group $G$ is\n#P-complete, in the case that $G$ is fixed and $M$ is the computational input.\nSimilarly, deciding if there is a non-trivial homomorphism is NP-complete. In\nboth reductions, we can guarantee that every non-trivial homomorphism is a\nsurjection. As a corollary, for any fixed integer $m \\ge 5$, it is NP-complete\nto decide whether $M$ admits a connected $m$-sheeted covering.\n  Our construction is inspired by universality results in topological quantum\ncomputation. Given a classical reversible circuit $C$, we construct $M$ so that\nevaluations of $C$ with certain initialization and finalization conditions\ncorrespond to homomorphisms $\\pi_1(M) \\to G$. An intermediate state of $C$\nlikewise corresponds to a homomorphism $\\pi_1(\\Sigma_g) \\to G$, where\n$\\Sigma_g$ is a pointed Heegaard surface of $M$ of genus $g$. We analyze the\naction on these homomorphisms by the pointed mapping class group\n$\\text{MCG}_*(\\Sigma_g)$ and its Torelli subgroup $\\text{Tor}_*(\\Sigma_g)$. By\nresults of Dunfield-Thurston, the action of $\\text{MCG}_*(\\Sigma_g)$ is as\nlarge as possible when $g$ is sufficiently large; we can pass to the Torelli\ngroup using the congruence subgroup property of $\\text{Sp}(2g,\\mathbb{Z})$. Our\nresults can be interpreted as a sharp classical universality property of an\nassociated combinatorial $(2+1)$-dimensional TQFT.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:39:46 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Kuperberg", "Greg", "", "UC Davis"], ["Samperton", "Eric", "", "UC Davis"]]}, {"id": "1707.04251", "submitter": "Bernhard Bliem", "authors": "Bernhard Bliem and Stefan Woltran", "title": "Defensive Alliances in Graphs of Bounded Treewidth", "comments": "18 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1411.6549", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A set S of vertices of a graph is a defensive alliance if, for each element\nof S, the majority of its neighbors is in S. The problem of finding a defensive\nalliance of minimum size in a given graph is NP-hard and there are\npolynomial-time algorithms if certain parameters are bounded by a fixed\nconstant. In particular, fixed-parameter tractability results have been\nobtained for some structural parameters such as the vertex cover number.\nHowever, for the parameter treewidth, the question of whether the problem is\nFPT has remained open. This is unfortunate because treewidth is perhaps the\nmost prominent graph parameter and has proven successful for many problems. In\nthis work, we give a negative answer by showing that the problem is W[1]-hard\nwhen parameterized by treewidth, which rules out FPT algorithms under common\nassumptions. This is surprising since the problem is known to be FPT when\nparameterized by solution size and \"subset problems\" that satisfy this property\nusually tend to be FPT for bounded treewidth as well. We prove W[1]-hardness by\nusing techniques from a recent hardness result for the problem of finding\nso-called secure sets in a graph.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:13:31 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Bliem", "Bernhard", ""], ["Woltran", "Stefan", ""]]}, {"id": "1707.04310", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Charles Paperman", "title": "Topological Sorting under Regular Constraints", "comments": "45 pages, 31 references in the main text. This is the full version\n  with proofs of the ICALP'18 paper, and is the same as the ICALP proceedings\n  version up to minor publisher-dependent changes. Several important changes\n  with respect to version 1, including fixing some errors. Title changed with\n  respect to version 2", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2018.115", "report-no": null, "categories": "cs.DS cs.CC cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the constrained topological sorting problem (CTS): given a\nregular language K and a directed acyclic graph G with labeled vertices,\ndetermine if G has a topological sort that forms a word in K. This natural\nproblem applies to several settings, e.g., scheduling with costs or verifying\nconcurrent programs. We consider the problem CTS[K] where the target language K\nis fixed, and study its complexity depending on K. We show that CTS[K] is\ntractable when K falls in several language families, e.g., unions of monomials,\nwhich can be used for pattern matching. However, we show that CTS[K] is NP-hard\nfor K = (ab)^* and introduce a shuffle reduction technique to show hardness for\nmore languages. We also study the special case of the constrained shuffle\nproblem (CSh), where the input graph is a disjoint union of strings, and show\nthat CSh[K] is additionally tractable when K is a group language or a union of\ndistrict group monomials. We conjecture that a dichotomy should hold on the\ncomplexity of CTS[K] or CSh[K] depending on K, and substantiate this by proving\na coarser dichotomy under a different problem phrasing which ensures that\ntractable languages are closed under common operators.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:35:48 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 10:40:29 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 18:57:06 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Amarilli", "Antoine", ""], ["Paperman", "Charles", ""]]}, {"id": "1707.04316", "submitter": "Jiehua Chen", "authors": "Jiehua Chen and Danny Hermelin and Manuel Sorge and Harel Yedidsion", "title": "How hard is it to satisfy (almost) all roommates?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic Stable Roommates problem (which is the non-bipartite\ngeneralization of the well-known Stable Marriage problem) asks whether there is\na stable matching for a given set of agents, i.e. a partitioning of the agents\ninto disjoint pairs such that no two agents induce a blocking pair. Herein,\neach agent has a preference list denoting who it prefers to have as a partner,\nand two agents are blocking if they prefer to be with each other rather than\nwith their assigned partners. Since stable matchings may not be unique, we\nstudy an NP-hard optimization variant of Stable Roommates, called Egal Stable\nRoommates, which seeks to find a stable matching with a minimum egalitarian\ncost {\\gamma}, i.e. the sum of the dissatisfaction of the agents is minimum.\nThe dissatisfaction of an agent is the number of agents that this agent prefers\nover its partner if it is matched; otherwise it is the length of its preference\nlist. We also study almost stable matchings, called Min-Block-Pair Stable\nRoommates, which seeks to find a matching with a minimum number {\\beta} of\nblocking pairs. Our main result is that Egal Stable Roommates parameterized by\n{\\gamma} is fixed-parameter tractable, while Min-Block-Pair Stable Roommates\nparameterized by {\\beta} is W[1]-hard, even if the length of each preference\nlist is at most five.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:53:33 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 20:28:44 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 10:02:28 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chen", "Jiehua", ""], ["Hermelin", "Danny", ""], ["Sorge", "Manuel", ""], ["Yedidsion", "Harel", ""]]}, {"id": "1707.04609", "submitter": "Holger Dell", "authors": "Holger Dell and John Lapinskas", "title": "Fine-grained reductions from approximate counting to decision", "comments": "An extended abstract was presented at STOC 2018", "journal-ref": null, "doi": "10.1145/3188745.3188920", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we introduce a general framework for fine-grained reductions\nof approximate counting problems to their decision versions. (Thus we use an\noracle that decides whether any witness exists to multiplicatively approximate\nthe number of witnesses with minimal overhead.) This mirrors a foundational\nresult of Sipser (STOC 1983) and Stockmeyer (SICOMP 1985) in the\npolynomial-time setting, and a similar result of M\\\"uller (IWPEC 2006) in the\nFPT setting. Using our framework, we obtain such reductions for some of the\nmost important problems in fine-grained complexity: the Orthogonal Vectors\nproblem, 3SUM, and the Negative-Weight Triangle problem (which is closely\nrelated to All-Pairs Shortest Path).\n  We also provide a fine-grained reduction from approximate #SAT to SAT.\nSuppose the Strong Exponential Time Hypothesis (SETH) is false, so that for\nsome $1<c<2$ and all $k$ there is an $O(c^n)$-time algorithm for k-SAT. Then we\nprove that for all $k$, there is an $O((c+o(1))^n)$-time algorithm for\napproximate #$k$-SAT. In particular, our result implies that the Exponential\nTime Hypothesis (ETH) is equivalent to the seemingly-weaker statement that\nthere is no algorithm to approximate #3-SAT to within a factor of $1+\\epsilon$\nin time $2^{o(n)}/\\epsilon^2$ (taking $\\epsilon > 0$ as part of the input).\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:02:42 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 12:56:27 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 19:42:20 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Dell", "Holger", ""], ["Lapinskas", "John", ""]]}, {"id": "1707.04615", "submitter": "John Wilmes", "authors": "Le Song, Santosh Vempala, John Wilmes, and Bo Xie", "title": "On the Complexity of Learning Neural Networks", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stunning empirical successes of neural networks currently lack rigorous\ntheoretical explanation. What form would such an explanation take, in the face\nof existing complexity-theoretic lower bounds? A first step might be to show\nthat data generated by neural networks with a single hidden layer, smooth\nactivation functions and benign input distributions can be learned efficiently.\nWe demonstrate here a comprehensive lower bound ruling out this possibility:\nfor a wide class of activation functions (including all currently used), and\ninputs drawn from any logconcave distribution, there is a family of\none-hidden-layer functions whose output is a sum gate, that are hard to learn\nin a precise sense: any statistical query algorithm (which includes all known\nvariants of stochastic gradient descent with any loss function) needs an\nexponential number of queries even using tolerance inversely proportional to\nthe input dimensionality. Moreover, this hard family of functions is realizable\nwith a small (sublinear in dimension) number of activation units in the single\nhidden layer. The lower bound is also robust to small perturbations of the true\nweights. Systematic experiments illustrate a phase transition in the training\nerror as predicted by the analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:23:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Song", "Le", ""], ["Vempala", "Santosh", ""], ["Wilmes", "John", ""], ["Xie", "Bo", ""]]}, {"id": "1707.05016", "submitter": "Guillaume Ducoffe", "authors": "David Coudert (1), Guillaume Ducoffe (1,2), Alexandru Popa ((1) COATI,\n  (2) ICI Bucharest)", "title": "Fully polynomial FPT algorithms for some classes of bounded clique-width\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity theory has enabled a refined classification of the\ndifficulty of NP-hard optimization problems on graphs with respect to key\nstructural properties, and so to a better understanding of their true\ndifficulties. More recently, hardness results for problems in P were achieved\nusing reasonable complexity theoretic assumptions such as: Strong Exponential\nTime Hypothesis (SETH), 3SUM and All-Pairs Shortest-Paths (APSP). According to\nthese assumptions, many graph theoretic problems do not admit truly\nsubquadratic algorithms, nor even truly subcubic algorithms (Williams and\nWilliams, FOCS 2010 and Abboud, Grandoni, Williams, SODA 2015). A central\ntechnique used to tackle the difficulty of the above mentioned problems is\nfixed-parameter algorithms for polynomial-time problems with polynomial\ndependency in the fixed parameter (P-FPT). This technique was introduced by\nAbboud, Williams and Wang in SODA 2016 and continued by Husfeldt (IPEC 2016)\nand Fomin et al. (SODA 2017), using the treewidth as a parameter. Applying this\ntechnique to clique-width, another important graph parameter, remained to be\ndone. In this paper we study several graph theoretic problems for which\nhardness results exist such as cycle problems (triangle detection, triangle\ncounting, girth, diameter), distance problems (diameter, eccentricities, Gromov\nhyperbolicity, betweenness centrality) and maximum matching. We provide\nhardness results and fully polynomial FPT algorithms, using clique-width and\nsome of its upper-bounds as parameters (split-width, modular-width and\n$P\\_4$-sparseness). We believe that our most important result is an ${\\cal\nO}(k^4 \\cdot n + m)$-time algorithm for computing a maximum matching where $k$\nis either the modular-width or the $P\\_4$-sparseness. The latter generalizes\nmany algorithms that have been introduced so far for specific subclasses such\nas cographs, $P\\_4$-lite graphs, $P\\_4$-extendible graphs and $P\\_4$-tidy\ngraphs. Our algorithms are based on preprocessing methods using modular\ndecomposition, split decomposition and primeval decomposition. Thus they can\nalso be generalized to some graph classes with unbounded clique-width.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:57:27 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 12:37:56 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Coudert", "David", ""], ["Ducoffe", "Guillaume", ""], ["Popa", "Alexandru", ""]]}, {"id": "1707.05062", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI)", "title": "Speeding up the K\\\"ohler's method of contrast thresholding", "comments": "IEEE CopyrightProceedings of the IEEE International Conference on\n  Image Processing ICIP 2017", "journal-ref": "IEEE. IEEE International Conference on Image Processing ICIP 2017,\n  Sep 2017, Beijing, China. IEEE, 2017, http://2017.ieeeicip.org", "doi": "10.1109/ICIP.2017.8296295", "report-no": null, "categories": "cs.CV cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K{\\\"o}hler's method is a useful multi-thresholding technique based on\nboundary contrast. However, the direct algorithm has a too high complexity-O(N\n2) i.e. quadratic with the pixel numbers N-to process images at a sufficient\nspeed for practical applications. In this paper, a new algorithm to speed up\nK{\\\"o}hler's method is introduced with a complexity in O(N M), M is the number\nof grey levels. The proposed algorithm is designed for parallelisation and\nvector processing , which are available in current processors, using OpenMP\n(Open Multi-Processing) and SIMD instructions (Single Instruction on Multiple\nData). A fast implementation allows a gain factor of 405 in an image of 18\nmillion pixels and a video processing in real time (gain factor of 96).\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 09:41:04 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 10:43:48 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"]]}, {"id": "1707.05404", "submitter": "Sushmita Gupta", "authors": "Sushmita Gupta, Saket Saurabh and Meirav Zehavi", "title": "On Treewidth and Stable Marriage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable Marriage is a fundamental problem to both computer science and\neconomics. Four well-known NP-hard optimization versions of this problem are\nthe Sex-Equal Stable Marriage (SESM), Balanced Stable Marriage (BSM),\nmax-Stable Marriage with Ties (max-SMT) and min-Stable Marriage with Ties\n(min-SMT) problems. In this paper, we analyze these problems from the viewpoint\nof Parameterized Complexity. We conduct the first study of these problems with\nrespect to the parameter treewidth. First, we study the treewidth $\\mathtt{tw}$\nof the primal graph. We establish that all four problems are W[1]-hard. In\nparticular, while it is easy to show that all four problems admit algorithms\nthat run in time $n^{O(\\mathtt{tw})}$, we prove that all of these algorithms\nare likely to be essentially optimal. Next, we study the treewidth\n$\\mathtt{tw}$ of the rotation digraph. In this context, the max-SMT and min-SMT\nare not defined. For both SESM and BSM, we design (non-trivial) algorithms that\nrun in time $2^{\\mathtt{tw}}n^{O(1)}$. Then, for both SESM and BSM, we also\nprove that unless SETH is false, algorithms that run in time\n$(2-\\epsilon)^{\\mathtt{tw}}n^{O(1)}$ do not exist for any fixed $\\epsilon>0$.\nWe thus present a comprehensive, complete picture of the behavior of central\noptimization versions of Stable Marriage with respect to treewidth.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 22:13:53 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Gupta", "Sushmita", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1707.05808", "submitter": "Dhiraj Holden", "authors": "Dhiraj Holden", "title": "A Note on Unconditional Subexponential-time Pseudo-deterministic\n  Algorithms for BPP Search Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We show the first unconditional pseudo-determinism result for all of\nsearch-BPP. Specifically, we show that every BPP search problem can be computed\npseudo-deterministically on average for infinitely many input lengths. In other\nwords, for infinitely many input lengths and for any polynomial-time samplable\ndistribution our algorithm succeeds in producing a unique answer (if one\nexists) with high probability over the distribution and the coins tossed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:19:34 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Holden", "Dhiraj", ""]]}, {"id": "1707.05846", "submitter": "Diego Coelho", "authors": "Vassil Dimitrov and Diego Coelho", "title": "On the Computation of Neumann Series", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new factorizations for computing the Neumann series. The\nfactorizations are based on fast algorithms for small prime sizes series and\nthe splitting of large sizes into several smaller ones. We propose a different\nbasis for factorizations other than the well-known binary and ternary basis. We\nshow that is possible to reduce the overall complexity for the usual binary\ndecomposition from 2log2(N)-2 multiplications to around 1.72log2(N)-2 using a\nbasis of size five. Merging different basis we can demonstrate that we can\nbuild fast algorithms for particular sizes. We also show the asymptotic case\nwhere one can reduce the number of multiplications to around 1.70log2(N)-2.\nSimulations are performed for applications in the context of wireless\ncommunications and image rendering, where is necessary perform large sized\nmatrices inversion.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:32:43 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Dimitrov", "Vassil", ""], ["Coelho", "Diego", ""]]}, {"id": "1707.05941", "submitter": "Jun Zhang", "authors": "Jun Zhang and Qi Cheng", "title": "An Efficient Version of the Bombieri-Vaaler Lemma", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their celebrated paper \"On Siegel's Lemma\", Bombieri and Vaaler found an\nupper bound on the height of integer solutions of systems of linear Diophantine\nequations. Calculating the bound directly, however, requires exponential time.\nIn this paper, we present the bound in a different form that can be computed in\npolynomial time. We also give an elementary (and arguably simpler) proof for\nthe bound.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 05:59:20 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Zhang", "Jun", ""], ["Cheng", "Qi", ""]]}, {"id": "1707.06343", "submitter": "Quanquan Liu", "authors": "Erik D. Demaine, Quanquan C. Liu", "title": "Inapproximability of the Standard Pebble Game and Hard to Pebble Graphs", "comments": "Preliminary version in WADS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pebble games are single-player games on DAGs involving placing and moving\npebbles on nodes of the graph according to a certain set of rules. The goal is\nto pebble a set of target nodes using a minimum number of pebbles. In this\npaper, we present a possibly simpler proof of the result in [CLNV15] and\nstrengthen the result to show that it is PSPACE-hard to determine the minimum\nnumber of pebbles to an additive $n^{1/3-\\epsilon}$ term for all $\\epsilon >\n0$, which improves upon the currently known additive constant hardness of\napproximation [CLNV15] in the standard pebble game. We also introduce a family\nof explicit, constant indegree graphs with $n$ nodes where there exists a graph\nin the family such that using constant $k$ pebbles requires $\\Omega(n^k)$ moves\nto pebble in both the standard and black-white pebble games. This independently\nanswers an open question summarized in [Nor15] of whether a family of DAGs\nexists that meets the upper bound of $O(n^k)$ moves using constant $k$ pebbles\nwith a different construction than that presented in [AdRNV17].\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 02:18:28 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Demaine", "Erik D.", ""], ["Liu", "Quanquan C.", ""]]}, {"id": "1707.06499", "submitter": "Andreas Emil Feldmann", "authors": "Rajesh Chitnis, Andreas Emil Feldmann, Pasin Manurangsi", "title": "Parameterized Approximation Algorithms for Bidirected Steiner Network\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Directed Steiner Network (DSN) problem takes as input a directed\nedge-weighted graph $G=(V,E)$ and a set $\\mathcal{D}\\subseteq V\\times V$ of $k$\ndemand pairs. The aim is to compute the cheapest network $N\\subseteq G$ for\nwhich there is an $s\\to t$ path for each $(s,t)\\in\\mathcal{D}$. It is known\nthat this problem is notoriously hard as there is no\n$k^{1/4-o(1)}$-approximation algorithm under Gap-ETH, even when parametrizing\nthe runtime by $k$ [Dinur & Manurangsi, ITCS 2018]. In light of this, we\nsystematically study several special cases of DSN and determine their\nparameterized approximability for the parameter $k$.\n  For the bi-DSN$_\\text{Planar}$ problem, the aim is to compute a solution\n$N\\subseteq G$ whose cost is at most that of an optimum planar solution in a\nbidirected graph $G$, i.e., for every edge $uv$ of $G$ the reverse edge $vu$\nexists and has the same weight. This problem is a generalization of several\nwell-studied special cases. Our main result is that this problem admits a\nparameterized approximation scheme (PAS) for $k$. We also prove that our result\nis tight in the sense that (a) the runtime of our PAS cannot be significantly\nimproved, and (b) it is unlikely that a PAS exists for any generalization of\nbi-DSN$_\\text{Planar}$, unless FPT=W[1].\n  One important special case of DSN is the Strongly Connected Steiner Subgraph\n(SCSS) problem, for which the solution network $N\\subseteq G$ needs to strongly\nconnect a given set of $k$ terminals. It has been observed before that for SCSS\na parameterized $2$-approximation exists when parameterized by $k$ [Chitnis et\nal., IPEC 2013]. We give a tight inapproximability result by showing that for\n$k$ no parameterized $(2-\\varepsilon)$-approximation algorithm exists under\nGap-ETH. Additionally we show that when restricting the input of SCSS to\nbidirected graphs, the problem remains NP-hard but becomes FPT for $k$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 13:33:43 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 12:01:14 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:46:39 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 18:07:04 GMT"}, {"version": "v5", "created": "Wed, 2 Sep 2020 13:08:58 GMT"}, {"version": "v6", "created": "Wed, 13 Jan 2021 12:46:55 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Feldmann", "Andreas Emil", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "1707.06860", "submitter": "Alexandre Sedoglavic", "authors": "Alexandre Sedoglavic (CRIStAL)", "title": "A non-commutative algorithm for multiplying 5x5 matrices using 99\n  multiplications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-commutative algorithm for multiplying 5x5 matrices using 99\nmultiplications. This algorithm is a minor modification of Makarov's algorithm\nwhich exhibit the previous best known bound with 100 multiplications.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 08:57:28 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 12:28:38 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Sedoglavic", "Alexandre", "", "CRIStAL"]]}, {"id": "1707.06992", "submitter": "Hossein Hosseini", "authors": "S. Hossein Hosseini and Afshin Ebrahimi", "title": "Ideological Sublations: Resolution of Dialectic in Population-based\n  Optimization", "comments": "An antenna selection model for massive MIMO was considered at the\n  current version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population-based optimization algorithm was designed, inspired by two main\nthinking modes in philosophy, both based on dialectic concept and\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\nIdealistic and materialistic antitheses are formulated as optimization models.\nBased on the models, the population is coordinated for dialectical\ninteractions. At the population-based context, the formulated optimization\nmodels are reduced to a simple detection problem for each thinker (particle).\nAccording to the assigned thinking mode to each thinker and her/his\nmeasurements of corresponding dialectic with other candidate particles, they\ndeterministically decide to interact with a thinker in maximum dialectic with\ntheir theses. The position of a thinker at maximum dialectic is known as an\navailable antithesis among the existing solutions. The dialectical interactions\nat each ideological community are distinguished by meaningful distributions of\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\nexploration and exploitation elements of the proposed algorithm. The result is\na delicate balance without any requirement for adjustment of step-size\ncoefficients. Main parameter of the proposed algorithm is the number of\nparticles appointed to each thinking modes, or equivalently for each kind of\nmotions. An additional integer parameter is defined to boost the stability of\nthe final algorithm in some particular problems. The proposed algorithm is\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\nMoreover, its performance and speed were highlighted in sparse reconstruction\nand antenna selection problems, at the context of compressed sensing and\nmassive MIMO, respectively. The results indicate fast and efficient performance\nin comparison with well-known evolutionary algorithms and dedicated\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:53:04 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 13:33:09 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hosseini", "S. Hossein", ""], ["Ebrahimi", "Afshin", ""]]}, {"id": "1707.08086", "submitter": "T.S. Jayram", "authors": "Badih Ghazi and T.S. Jayram", "title": "Resource-Efficient Common Randomness and Secret-Key Schemes", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study common randomness where two parties have access to i.i.d. samples\nfrom a known random source, and wish to generate a shared random key using\nlimited (or no) communication with the largest possible probability of\nagreement. This problem is at the core of secret key generation in\ncryptography, with connections to communication under uncertainty and locality\nsensitive hashing. We take the approach of treating correlated sources as a\ncritical resource, and ask whether common randomness can be generated\nresource-efficiently.\n  We consider two notable sources in this setup arising from correlated bits\nand correlated Gaussians. We design the first explicit schemes that use only a\npolynomial number of samples (in the key length) so that the players can\ngenerate shared keys that agree with constant probability using optimal\ncommunication. The best previously known schemes were both non-constructive and\nused an exponential number of samples. In the amortized setting, we\ncharacterize the largest achievable ratio of key length to communication in\nterms of the external and internal information costs, two well-studied\nquantities in theoretical computer science. In the relaxed setting where the\ntwo parties merely wish to improve the correlation between the generated keys\nof length $k$, we show that there are no interactive protocols using $o(k)$\nbits of communication having agreement probability even as small as\n$2^{-o(k)}$. For the related communication problem where the players wish to\ncompute a joint function $f$ of their inputs using i.i.d. samples from a known\nsource, we give a zero-communication protocol using $2^{O(c)}$ bits where $c$\nis the interactive randomized public-coin communication complexity of $f$. This\nmatches the lower bound shown previously while the best previously known upper\nbound was doubly exponential in $c$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:57:42 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Ghazi", "Badih", ""], ["Jayram", "T. S.", ""]]}, {"id": "1707.08109", "submitter": "George Barmpalias Dr", "authors": "George Barmpalias", "title": "Aspects of Chaitin's Omega", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The halting probability of a Turing machine,also known as Chaitin's Omega, is\nan algorithmically random number with many interesting properties. Since\nChaitin's seminal work, many popular expositions have appeared, mainly focusing\non the metamathematical or philosophical significance of Omega (or debating\nagainst it). At the same time, a rich mathematical theory exploring the\nproperties of Chaitin's Omega has been brewing in various technical papers,\nwhich quietly reveals the significance of this number to many aspects of\ncontemporary algorithmic information theory. The purpose of this survey is to\nexpose these developments and tell a story about Omega, which outlines its\nmultifaceted mathematical properties and roles in algorithmic randomness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:59:15 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 03:58:02 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 06:23:38 GMT"}, {"version": "v4", "created": "Sat, 24 Feb 2018 12:01:08 GMT"}, {"version": "v5", "created": "Fri, 21 Sep 2018 03:00:23 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Barmpalias", "George", ""]]}, {"id": "1707.08389", "submitter": "Sang-Ki Ko", "authors": "Sang-Ki Ko and Igor Potapov", "title": "Composition problems for braids: Membership, Identity and Freeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the decidability and complexity of problems\nrelated to braid composition. While all known problems for a class of braids\nwith three strands, $B_3$, have polynomial time solutions we prove that a very\nnatural question for braid composition, the membership problem, is NP-complete\nfor braids with only three strands. The membership problem is decidable in NP\nfor $B_3$, but it becomes harder for a class of braids with more strands. In\nparticular we show that fundamental problems about braid compositions are\nundecidable for braids with at least five strands, but decidability of these\nproblems for $B_4$ remains open. Finally we show that the freeness problem for\nsemigroups of braids from $B_3$ is also decidable in NP.\n  The paper introduces a few challenging algorithmic problems about topological\nbraids opening new connections between braid groups, combinatorics on words,\ncomplexity theory and provides solutions for some of these problems by\napplication of several techniques from automata theory, matrix semigroups and\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:49:46 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ko", "Sang-Ki", ""], ["Potapov", "Igor", ""]]}, {"id": "1707.08730", "submitter": "Ammar Daskin", "authors": "Ammar Daskin", "title": "A Quantum Approach to Subset-Sum and Similar Problems", "comments": "Missing references added, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the subset-sum problem by using a quantum heuristic\napproach similar to the verification circuit of quantum Arthur-Merlin games.\nUnder described certain assumptions, we show that the exact solution of the\nsubset sum problem my be obtained in polynomial time and the exponential\nspeed-up over the classical algorithms may be possible. We give a numerical\nexample and discuss the complexity of the approach and its further application\nto the knapsack problem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:33:05 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 07:31:56 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 16:21:05 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 08:34:56 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Daskin", "Ammar", ""]]}, {"id": "1707.09327", "submitter": "Nerio Borges", "authors": "Edwin Pin and Nerio Borges", "title": "A syntactic tool for proving hardness in the Second Level of the\n  Polynomial-Time Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the nineties Immerman and Medina initiated the search for syn- tactic\ntools to prove NP-completeness. In their work, amongst several results, they\nconjecture that the NP-completeness of a problem defined by the conjunction of\na sentence in Existential Second Order Logic with a First Order sentence,\nnecessarily imply the NP-completeness of the problem defined by the Existential\nSecond Order sentence alone. This is interesting because if true it would\njustify the restriction heuristic pro- posed in Garey and Johnson in his\nclassical book on NP completeness, which roughly says that in some cases one\ncan prove NP- complete a problem A by proving NP-complete a problem B contained\nin A. Borges and Bonet extend some results from Immerman and Medina and they\nalso prove for a host of complexity classes that the Immerman- Medina\nconjecture is true when the First Order sentence in the conjunc- tion is\nuniversal. Our work extends that result to the Second Level of the\nPolynomial-Time Hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 16:55:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Pin", "Edwin", ""], ["Borges", "Nerio", ""]]}, {"id": "1707.09440", "submitter": "Ross Willard", "authors": "Ross Willard", "title": "Refuting Feder, Kinne and Rafiey", "comments": "This note is in response to arXiv:1701.02409 versions (v1)-(v3) by T.\n  Feder, J. Kinne, and A. Rafiey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I give an example showing that the recent claimed solution by Feder, Kinne\nand Rafiey to the CSP Dichotomy Conjecture is not correct.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 00:17:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Willard", "Ross", ""]]}, {"id": "1707.09553", "submitter": "James P. Crutchfield", "authors": "C. Aghamohammadi, S. P. Loomis, J. R. Mahoney, and J. P. Crutchfield", "title": "Extreme Quantum Advantage for Rare-Event Sampling", "comments": "11 pages, 9 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/eqafbs.htm", "journal-ref": "Phys. Rev. X 8, 011025 (2018)", "doi": "10.1103/PhysRevX.8.011025", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a quantum algorithm for efficient biased sampling of the rare\nevents generated by classical memoryful stochastic processes. We show that this\nquantum algorithm gives an extreme advantage over known classical biased\nsampling algorithms in terms of the memory resources required. The quantum\nmemory advantage ranges from polynomial to exponential and when sampling the\nrare equilibrium configurations of spin systems the quantum advantage diverges.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 20:10:59 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Aghamohammadi", "C.", ""], ["Loomis", "S. P.", ""], ["Mahoney", "J. R.", ""], ["Crutchfield", "J. P.", ""]]}]