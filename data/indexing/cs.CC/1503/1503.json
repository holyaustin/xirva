[{"id": "1503.00035", "submitter": "Steffen Kopecki", "authors": "Lila Kari, Stavros Konstantinidis, and Steffen Kopecki", "title": "Transducer Descriptions of DNA Code Properties and Undecidability of\n  Antimorphic Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns formal descriptions of DNA code properties, and builds on\nprevious work on transducer descriptions of classic code properties and on\ntrajectory descriptions of DNA code properties. This line of research allows us\nto give a property as input to an algorithm, in addition to any regular\nlanguage, which can then answer questions about the language and the property.\nHere we define DNA code properties via transducers and show that this method is\nstrictly more expressive than that of trajectories, without sacrificing the\nefficiency of deciding the satisfaction question. We also show that the\nmaximality question can be undecidable. Our undecidability results hold not\nonly for the fixed DNA involution but also for any fixed antimorphic\npermutation. Moreover, we also show the undecidability of the antimorphic\nversion of the Post Corresponding Problem, for any fixed antimorphic\npermutation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:45:25 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kari", "Lila", ""], ["Konstantinidis", "Stavros", ""], ["Kopecki", "Steffen", ""]]}, {"id": "1503.00138", "submitter": "Saugata Basu", "authors": "Saugata Basu and Cordian Riener", "title": "On the isotypic decomposition of cohomology modules of symmetric\n  semi-algebraic sets: polynomial bounds on multiplicities", "comments": "42 pages, 1 figure. Reorganized for readability. Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC math.AT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider symmetric (under the action of products of finite symmetric\ngroups) real algebraic varieties and semi-algebraic sets, as well as symmetric\ncomplex varieties in affine and projective spaces, defined by polynomials of\ndegrees bounded by a fixed constant $d$. We prove that if a Specht module,\n$\\mathbb{S}^\\lambda$, appears with positive multiplicity in the isotypic\ndecomposition of the cohomology modules of such sets, then the rank of the\npartition $\\lambda$ is bounded by $O(d)$. This implies a polynomial (in the\ndimension of the ambient space) bound on the number of such modules.\nFurthermore, we prove a polynomial bound on the multiplicities of those that do\nappear with positive multiplicity in the isotypic decomposition of the above\nmentioned cohomology modules.\n  We give some applications of our methods in proving lower bounds on the\ndegrees of defining polynomials of certain symmetric semi-algebraic sets, as\nwell as improved bounds on the Betti numbers of the images under projections of\n(not necessarily symmetric) bounded real algebraic sets, improving in certain\nsituations prior results of Gabrielov, Vorobjov and Zell.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 15:10:02 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 20:08:14 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 18:41:47 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Basu", "Saugata", ""], ["Riener", "Cordian", ""]]}, {"id": "1503.00141", "submitter": "Jonathan Weed", "authors": "Jonathan Weed", "title": "Multinational War is Hard", "comments": "10 pages [fixed references]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the problem of determining whether one player can\nforce a win in a multiplayer version of the children's card game War is\nPSPACE-hard. The same reduction shows that a related problem, asking whether a\nplayer can survive k rounds, is PSPACE-complete when k is polynomial in the\nsize of the input.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 15:27:24 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 18:37:44 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 17:57:06 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Weed", "Jonathan", ""]]}, {"id": "1503.00260", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "Parameter Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In resolving instances of a computational problem, if multiple instances of\ninterest share a feature in common, it may be fruitful to compile this feature\ninto a format that allows for more efficient resolution, even if the\ncompilation is relatively expensive. In this article, we introduce a formal\nframework for classifying problems according to their compilability. The basic\nobject in our framework is that of a parameterized problem, which here is a\nlanguage along with a parameterization---a map which provides, for each\ninstance, a so-called parameter on which compilation may be performed. Our\nframework is positioned within the paradigm of parameterized complexity, and\nour notions are relatable to established concepts in the theory of\nparameterized complexity. Indeed, we view our framework as playing a unifying\nrole, integrating together parameterized complexity and compilability theory.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 12:23:06 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "1503.00275", "submitter": "Balagopal Komarath", "authors": "Balagopal Komarath, Jayalal Sarma and K.S. Sunil", "title": "Comparator Circuits over Finite Bounded Posets", "comments": "21 pages, previous version incorrectly claimed NL = L-SkewCC when in\n  fact P = L-SkewCC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparator circuit model was originally introduced by Mayr and Subramanian\n(1992) (and further studied by Cook, Filmus and Le (2012)) to capture problems\nwhich are not known to be P-complete but still not known to admit efficient\nparallel algorithms. The class CC is the complexity class of problems many-one\nlogspace reducible to the Comparator Circuit Value Problem and we know that NL\nis contained in CC which is inturn contained in P. Cook, Filmus and Le (2012)\nshowed that CC is also the class of languages decided by polynomial size\ncomparator circuits.\n  We study generalizations of the comparator circuit model that work over fixed\nfinite bounded posets. We observe that there are universal comparator circuits\neven over arbitrary fixed finite bounded posets. Building on this, we show that\ngeneral (resp. skew) comparator circuits of polynomial size over fixed finite\ndistributive lattices characterizes CC (resp. L). Complementing this, we show\nthat general comparator circuits of polynomial size over arbitrary fixed finite\nlattices exactly characterizes P even when the comparator circuit is skew. In\naddition, we show a characterization of the class NP by a family of polynomial\nsized comparator circuits over fixed {\\em finite bounded posets}. These results\ngeneralize the results by Cook, Filmus and Le (2012) regarding the power of\ncomparator circuits. As an aside, we consider generalizations of Boolean\nformulae over arbitrary lattices. We show that Spira's theorem (1971) can be\nextended to this setting as well and show that polynomial sized Boolean\nformulae over finite fixed lattices capture exactly NC^1.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 13:46:11 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 09:49:10 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Komarath", "Balagopal", ""], ["Sarma", "Jayalal", ""], ["Sunil", "K. S.", ""]]}, {"id": "1503.00321", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Jaikumar Radhakrishnan", "title": "A Sampling Technique of Proving Lower Bounds for Noisy Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique of proving lower bounds for noisy computations. This\nis achieved by a theorem connecting computations on a kind of randomized\ndecision trees and sampling based algorithms. This approach is surprisingly\npowerful, and applicable to several models of computation previously studied.\n  As a first illustration we show how all the results of Evans and Pippenger\n(SIAM J. Computing, 1999) for noisy decision trees, some of which were derived\nusing Fourier analysis, follow immediately if we consider the sampling-based\nalgorithms that naturally arise from these decision trees.\n  Next, we show a tight lower bound of $\\Omega(N \\log\\log N)$ on the number of\ntransmissions required to compute several functions (including the parity\nfunction and the majority function) in a network of $N$ randomly placed\nsensors, communicating using local transmissions, and operating with power near\nthe connectivity threshold. This result considerably simplifies and strengthens\nan earlier result of Dutta, Kanoria Manjunath and Radhakrishnan (SODA 08) that\nsuch networks cannot compute the parity function reliably with significantly\nfewer than $N\\log \\log N$ transmissions. The lower bound for parity shown\nearlier made use of special properties of the parity function and is\ninapplicable, e.g., to the majority function. In this paper, we use our\napproach to develop an interesting connection between computation of boolean\nfunctions on noisy networks that make few transmissionss, and algorithms that\nwork by sampling only a part of the input. It is straightforward to verify that\nsuch sampling-based algorithms cannot compute the majority function.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 18:14:23 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Radhakrishnan", "Jaikumar", ""]]}, {"id": "1503.00362", "submitter": "Antonis Achilleos", "authors": "Antonis Achilleos", "title": "NEXP-completeness and Universal Hardness Results for Justification Logic", "comments": "Shorter version has been accepted for publication by CSR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a lower complexity bound for the satisfiability problem of a\nmulti-agent justification logic, establishing that the general NEXP upper bound\nfrom our previous work is tight. We then use a simple modification of the\ncorresponding reduction to prove that satisfiability for all multi-agent\njustification logics from there is hard for the Sigma 2 p class of the second\nlevel of the polynomial hierarchy - given certain reasonable conditions. Our\nmethods improve on these required conditions for the same lower bound for the\nsingle-agent justification logics, proven by Buss and Kuznets in 2009, thus\nanswering one of their open questions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 22:07:21 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Achilleos", "Antonis", ""]]}, {"id": "1503.00448", "submitter": "Qiang Li", "authors": "Wei Chen, Qiang Li, Xiaoming Sun, Jialin Zhang", "title": "The Routing of Complex Contagion in Kleinberg's Small-World Networks", "comments": "Conference version will appear in COCOON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Kleinberg's small-world network model, strong ties are modeled as\ndeterministic edges in the underlying base grid and weak ties are modeled as\nrandom edges connecting remote nodes. The probability of connecting a node $u$\nwith node $v$ through a weak tie is proportional to $1/|uv|^\\alpha$, where\n$|uv|$ is the grid distance between $u$ and $v$ and $\\alpha\\ge 0$ is the\nparameter of the model. Complex contagion refers to the propagation mechanism\nin a network where each node is activated only after $k \\ge 2$ neighbors of the\nnode are activated.\n  In this paper, we propose the concept of routing of complex contagion (or\ncomplex routing), where we can activate one node at one time step with the goal\nof activating the targeted node in the end. We consider decentralized routing\nscheme where only the weak ties from the activated nodes are revealed. We study\nthe routing time of complex contagion and compare the result with simple\nrouting and complex diffusion (the diffusion of complex contagion, where all\nnodes that could be activated are activated immediately in the same step with\nthe goal of activating all nodes in the end).\n  We show that for decentralized complex routing, the routing time is lower\nbounded by a polynomial in $n$ (the number of nodes in the network) for all\nrange of $\\alpha$ both in expectation and with high probability (in particular,\n$\\Omega(n^{\\frac{1}{\\alpha+2}})$ for $\\alpha \\le 2$ and\n$\\Omega(n^{\\frac{\\alpha}{2(\\alpha+2)}})$ for $\\alpha > 2$ in expectation),\nwhile the routing time of simple contagion has polylogarithmic upper bound when\n$\\alpha = 2$. Our results indicate that complex routing is harder than complex\ndiffusion and the routing time of complex contagion differs exponentially\ncompared to simple contagion at sweetspot.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 09:20:00 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 11:50:27 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Chen", "Wei", ""], ["Li", "Qiang", ""], ["Sun", "Xiaoming", ""], ["Zhang", "Jialin", ""]]}, {"id": "1503.00484", "submitter": "Maciej  Skorski", "authors": "Maciej Skorski", "title": "Simulating Auxiliary Inputs, Revisited", "comments": "Some typos present in the previous version have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any pair $(X,Z)$ of correlated random variables we can think of $Z$ as a\nrandomized function of $X$. Provided that $Z$ is short, one can make this\nfunction computationally efficient by allowing it to be only approximately\ncorrect. In folklore this problem is known as \\emph{simulating auxiliary\ninputs}. This idea of simulating auxiliary information turns out to be a\npowerful tool in computer science, finding applications in complexity theory,\ncryptography, pseudorandomness and zero-knowledge. In this paper we revisit\nthis problem, achieving the following results:\n  \\begin{enumerate}[(a)] We discuss and compare efficiency of known results,\nfinding the flaw in the best known bound claimed in the TCC'14 paper \"How to\nFake Auxiliary Inputs\". We present a novel boosting algorithm for constructing\nthe simulator. Our technique essentially fixes the flaw. This boosting proof is\nof independent interest, as it shows how to handle \"negative mass\" issues when\nconstructing probability measures in descent algorithms. Our bounds are much\nbetter than bounds known so far. To make the simulator\n$(s,\\epsilon)$-indistinguishable we need the complexity $O\\left(s\\cdot\n2^{5\\ell}\\epsilon^{-2}\\right)$ in time/circuit size, which is better by a\nfactor $\\epsilon^{-2}$ compared to previous bounds. In particular, with our\ntechnique we (finally) get meaningful provable security for the EUROCRYPT'09\nleakage-resilient stream cipher instantiated with a standard 256-bit block\ncipher, like $\\mathsf{AES256}$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 11:13:42 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 14:59:35 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 21:35:35 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "1503.00753", "submitter": "Abbas Bazzi", "authors": "Abbas Bazzi, Samuel Fiorini, Sebastian Pokutta and Ola Svensson", "title": "No Small Linear Program Approximates Vertex Cover within a Factor $2 -\n  \\epsilon$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertex cover problem is one of the most important and intensively studied\ncombinatorial optimization problems. Khot and Regev (2003) proved that the\nproblem is NP-hard to approximate within a factor $2 - \\epsilon$, assuming the\nUnique Games Conjecture (UGC). This is tight because the problem has an easy\n2-approximation algorithm. Without resorting to the UGC, the best\ninapproximability result for the problem is due to Dinur and Safra (2002):\nvertex cover is NP-hard to approximate within a factor 1.3606. We prove the\nfollowing unconditional result about linear programming (LP) relaxations of the\nproblem: every LP relaxation that approximates vertex cover within a factor\n$2-\\epsilon$ has super-polynomially many inequalities. As a direct consequence\nof our methods, we also establish that LP relaxations (as well as SDP\nrelaxations) that approximate the independent set problem within any constant\nfactor have super-polynomial size.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:26:23 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 13:14:52 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Bazzi", "Abbas", ""], ["Fiorini", "Samuel", ""], ["Pokutta", "Sebastian", ""], ["Svensson", "Ola", ""]]}, {"id": "1503.01170", "submitter": "John Kim", "authors": "John Y. Kim", "title": "Integer Addition and Hamming Weight", "comments": "21 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of addition on the Hamming weight of a positive integer.\nConsider the first $2^n$ positive integers, and fix an $\\alpha$ among them. We\nshow that if the binary representation of $\\alpha$ consists of $\\Theta(n)$\nblocks of zeros and ones, then addition by $\\alpha$ causes a constant fraction\nof low Hamming weight integers to become high Hamming weight integers. This\nresult has applications in complexity theory to the hardness of computing\npowering maps using bounded-depth arithmetic circuits over $\\mathbb{F}_2$. Our\nresult implies that powering by $\\alpha$ composed of many blocks require\nexponential-size, bounded-depth arithmetic circuits over $\\mathbb{F}_2$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 00:23:29 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 23:29:04 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Kim", "John Y.", ""]]}, {"id": "1503.01613", "submitter": "Patrick Bennett", "authors": "Patrick Bennett, Ilario Bonacina, Nicola Galesi, Tony Huynh, Mike\n  Molloy, Paul Wollan", "title": "Space proof complexity for random 3-CNFs", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.1619", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the space complexity of refuting $3$-CNFs in Resolution and\nalgebraic systems. We prove that every Polynomial Calculus with Resolution\nrefutation of a random $3$-CNF $\\phi$ in $n$ variables requires, with high\nprobability, $\\Omega(n)$ distinct monomials to be kept simultaneously in\nmemory. The same construction also proves that every Resolution refutation\n$\\phi$ requires, with high probability, $\\Omega(n)$ clauses each of width\n$\\Omega(n)$ to be kept at the same time in memory. This gives a $\\Omega(n^2)$\nlower bound for the total space needed in Resolution to refute $\\phi$. These\nresults are best possible (up to a constant factor).\n  The main technical innovation is a variant of Hall's Lemma. We show that in\nbipartite graphs $G$ with bipartition $(L,R)$ and left-degree at most 3, $L$\ncan be covered by certain families of disjoint paths, called VW-matchings,\nprovided that $L$ expands in $R$ by a factor of $(2-\\epsilon)$, for $\\epsilon <\n1/23$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 11:46:47 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 19:22:24 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 18:56:03 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Bennett", "Patrick", ""], ["Bonacina", "Ilario", ""], ["Galesi", "Nicola", ""], ["Huynh", "Tony", ""], ["Molloy", "Mike", ""], ["Wollan", "Paul", ""]]}, {"id": "1503.02217", "submitter": "Martin Haenggi", "authors": "Roxana Smarandache and Martin Haenggi", "title": "Bounding the Bethe and the Degree-$M$ Bethe Permanents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math-ph math.CO math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently conjectured that the permanent of a ${P}$-lifting\n$\\theta^{\\uparrow{P}}$ of a matrix $\\theta$ of degree $M$ is less than or equal\nto the $M$th power of the permanent perm$(\\theta)$, i.e.,\nperm$(\\theta^{\\uparrow{P}})\\leq(\\text{perm}(\\theta))^M$ and, consequently, that\nthe degree-$M$ Bethe permanent $\\text{perm}_{M,\\mathrm{B}} (\\theta)$ of a\nmatrix $\\theta$ is less than or equal to the permanent perm$(\\theta)$ of\n$\\theta$, i.e., perm$_{M, \\mathrm{B}} (\\theta)\\leq \\text{perm}(\\theta)$. In\nthis paper, we prove these related conjectures and show in addition a few\nproperties of the permanent of block matrices that are lifts of a matrix. As a\ncorollary, we obtain an alternative proof of the inequality perm$_{\\mathrm{B}}\n(\\theta)\\leq \\text{perm}(\\theta)$ on the Bethe permanent of the base matrix\n$\\theta$ that uses only the combinatorial definition of the Bethe permanent.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 21:55:43 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Smarandache", "Roxana", ""], ["Haenggi", "Martin", ""]]}, {"id": "1503.02286", "submitter": "Xin Li", "authors": "Xin Li", "title": "Three-Source Extractors for Polylogarithmic Min-Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the study of constructing explicit extractors for independent\ngeneral weak random sources. The ultimate goal is to give a construction that\nmatches what is given by the probabilistic method --- an extractor for two\nindependent $n$-bit weak random sources with min-entropy as small as $\\log\nn+O(1)$. Previously, the best known result in the two-source case is an\nextractor by Bourgain \\cite{Bourgain05}, which works for min-entropy $0.49n$;\nand the best known result in the general case is an earlier work of the author\n\\cite{Li13b}, which gives an extractor for a constant number of independent\nsources with min-entropy $\\mathsf{polylog(n)}$. However, the constant in the\nconstruction of \\cite{Li13b} depends on the hidden constant in the best known\nseeded extractor, and can be large; moreover the error in that construction is\nonly $1/\\mathsf{poly(n)}$.\n  In this paper, we make two important improvements over the result in\n\\cite{Li13b}. First, we construct an explicit extractor for \\emph{three}\nindependent sources on $n$ bits with min-entropy $k \\geq \\mathsf{polylog(n)}$.\nIn fact, our extractor works for one independent source with poly-logarithmic\nmin-entropy and another independent block source with two blocks each having\npoly-logarithmic min-entropy. Thus, our result is nearly optimal, and the next\nstep would be to break the $0.49n$ barrier in two-source extractors. Second, we\nimprove the error of the extractor from $1/\\mathsf{poly(n)}$ to\n$2^{-k^{\\Omega(1)}}$, which is almost optimal and crucial for cryptographic\napplications. Some of the techniques developed here may be of independent\ninterests.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 15:25:15 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Li", "Xin", ""]]}, {"id": "1503.03594", "submitter": "Nika Haghtalab", "authors": "Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Ruth Urner", "title": "Efficient Learning of Linear Separators under Bounded Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the learnability of linear separators in $\\Re^d$ in the presence of\nbounded (a.k.a Massart) noise. This is a realistic generalization of the random\nclassification noise model, where the adversary can flip each example $x$ with\nprobability $\\eta(x) \\leq \\eta$. We provide the first polynomial time algorithm\nthat can learn linear separators to arbitrarily small excess error in this\nnoise model under the uniform distribution over the unit ball in $\\Re^d$, for\nsome constant value of $\\eta$. While widely studied in the statistical learning\ntheory community in the context of getting faster convergence rates,\ncomputationally efficient algorithms in this model had remained elusive. Our\nwork provides the first evidence that one can indeed design algorithms\nachieving arbitrarily small excess error in polynomial time under this\nrealistic noise model and thus opens up a new and exciting line of research.\n  We additionally provide lower bounds showing that popular algorithms such as\nhinge loss minimization and averaging cannot lead to arbitrarily small excess\nerror under Massart noise, even under the uniform distribution. Our work\ninstead, makes use of a margin based technique developed in the context of\nactive learning. As a result, our algorithm is also an active learning\nalgorithm with label complexity that is only a logarithmic the desired excess\nerror $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 05:38:19 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria-Florina", ""], ["Haghtalab", "Nika", ""], ["Urner", "Ruth", ""]]}, {"id": "1503.03888", "submitter": "Andrey Nikolaev", "authors": "Jeremy Macdonald, Alexei Myasnikov, Andrey Nikolaev, Svetla Vassileva", "title": "Logspace and compressed-word computations in nilpotent groups", "comments": "Updated bibliography, improved results and exposition in Section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For finitely generated nilpotent groups, we employ Mal'cev coordinates to\nsolve several classical algorithmic problems efficiently. Computation of normal\nforms, the membership problem, the conjugacy problem, and computation of\npresentations for subgroups are solved using only logarithmic space and\nquasilinear time. Logarithmic space presentation-uniform versions of these\nalgorithms are provided. Compressed-word versions of the same problems, in\nwhich each input word is provided as a straight-line program, are solved in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:59:43 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 20:41:55 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Macdonald", "Jeremy", ""], ["Myasnikov", "Alexei", ""], ["Nikolaev", "Andrey", ""], ["Vassileva", "Svetla", ""]]}, {"id": "1503.04099", "submitter": "Jonathan Spreer", "authors": "Benjamin A. Burton, Cl\\'ement Maria, Jonathan Spreer", "title": "Algorithms and complexity for Turaev-Viro invariants", "comments": "17 pages, 5 figures", "journal-ref": "Journal of Applied and Computational Topology, 2018", "doi": "10.1007/s41468-018-0016-2", "report-no": null, "categories": "math.GT cs.CC cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 15:21:06 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Burton", "Benjamin A.", ""], ["Maria", "Cl\u00e9ment", ""], ["Spreer", "Jonathan", ""]]}, {"id": "1503.04426", "submitter": "Carlo Comin", "authors": "Carlo Comin, Romeo Rizzi", "title": "An Improved Pseudo-Polynomial Upper Bound for the Value Problem and\n  Optimal Strategy Synthesis in Mean Payoff Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we offer an $O(|V|^2 |E|\\, W)$ pseudo-polynomial time\ndeterministic algorithm for solving the Value Problem and Optimal Strategy\nSynthesis in Mean Payoff Games. This improves by a factor $\\log(|V|\\, W)$ the\nbest previously known pseudo-polynomial time upper bound due to Brim,~\\etal The\nimprovement hinges on a suitable characterization of values, and a description\nof optimal positional strategies, in terms of reweighted Energy Games and Small\nEnergy-Progress Measures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 13:48:06 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 09:09:21 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 10:08:35 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2015 20:57:29 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2015 16:18:38 GMT"}, {"version": "v6", "created": "Wed, 23 Dec 2015 20:43:43 GMT"}, {"version": "v7", "created": "Sun, 24 Apr 2016 16:35:44 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Comin", "Carlo", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1503.04486", "submitter": "Amey Bhangale", "authors": "Amey Bhangale and Swastik Kopparty", "title": "The complexity of computing the minimum rank of a sign pattern matrix", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that computing the minimum rank of a sign pattern matrix is NP hard.\nOur proof is based on a simple but useful connection between minimum ranks of\nsign pattern matrices and the stretchability problem for pseudolines\narrangements. In fact, our hardness result shows that it is already hard to\ndetermine if the minimum rank of a sign pattern matrix is $\\leq 3$. We\ncomplement this by giving a polynomial time algorithm for determining if a\ngiven sign pattern matrix has minimum rank $\\leq 2$.\n  Our result answers one of the open problems from Linial et al.\n[Combinatorica, 27(4):439--463, 2007].\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 22:53:13 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 23:52:14 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bhangale", "Amey", ""], ["Kopparty", "Swastik", ""]]}, {"id": "1503.04610", "submitter": "Jean-Camille Birget", "authors": "J.C. Birget", "title": "Infinitely generated semigroups and polynomial complexity", "comments": "17 pages. To appear in International J. of Algebra and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the functional approach to the P-versus-NP problem,\nbegun in [1]. Here we focus on the monoid RM_2^P of right-ideal morphisms of\nthe free monoid, that have polynomial input balance and polynomial\ntime-complexity. We construct a machine model for the functions in RM_2^P, and\nevaluation functions. We prove that RM_2^P is not finitely generated, and use\nthis to show separation results for time-complexity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 11:28:06 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 14:46:19 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Birget", "J. C.", ""]]}, {"id": "1503.05847", "submitter": "Abhishek Bose-Kolanu", "authors": "Abhishek Bose-Kolanu", "title": "Hypercomputation, Frege, Deleuze: Solving Thomson's Lamp", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first known solution to the original supertask, the Thomson\nLamp Paradox.\n  We also offer preliminary resources for classifying computational complexity\nof various supertasks. In so doing we consider a newly apparent paradox between\nthe metrical limit and the ordinal limit. We use this distinction between the\nmetrical and ordinal limits to explain the shortcomings both of Thomson's\noriginal formulation of the Lamp Paradox and Benacerraf's consequent critique.\n  We resolve this paradox through a careful consideration of transfinite\nordinals and locate its ambiguity as inherent to the identity relation under\nlogic with a close reading of Frege's Begriffsschrift. With this close reading\nin hand we expose how the identity relation is counter-intuitively polyvalent\nand, with supertasks, how the logico-mathematical field operates on the basis\nof Deleuzian point-folds. Our results combine resources from philosophy,\nmathematics, and computer science to ground the field of hypercomputation for\nlogically rigorous study.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:20:09 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Bose-Kolanu", "Abhishek", ""]]}, {"id": "1503.05879", "submitter": "Alexander Rubtsov", "authors": "Alexander A. Rubtsov", "title": "Regular realizability problems and regular languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate regular realizability (RR) problems, which are the problems of\nverifying whether intersection of a regular language -- the input of the\nproblem -- and fixed language called filter is non-empty. We consider two kind\nof problems depending on representation of regular language. If a regular\nlanguage on input is represented by a DFA, then we obtain (deterministic)\nregular realizability problem and we show that in this case the complexity of\nregular realizability problem for an arbitrary regular filter is either\nL-complete or NL-complete. We also show that in case of representation regular\nlanguage on input by NFA the problem is always NL-complete.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 18:49:15 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rubtsov", "Alexander A.", ""]]}, {"id": "1503.06321", "submitter": "Christian Komusiewicz", "authors": "Danny Hermelin, Moshe Kaspi, Christian Komusiewicz, Barak Navon", "title": "Parameterized Complexity of Critical Node Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following natural graph cut problem called Critical Node Cut\n(CNC): Given a graph $G$ on $n$ vertices, and two positive integers $k$ and\n$x$, determine whether $G$ has a set of $k$ vertices whose removal leaves $G$\nwith at most $x$ connected pairs of vertices. We analyze this problem in the\nframework of parameterized complexity. That is, we are interested in whether or\nnot this problem is solvable in $f(\\kappa) \\cdot n^{O(1)}$ time (i.e., whether\nor not it is fixed-parameter tractable), for various natural parameters\n$\\kappa$. We consider four such parameters:\n  - The size $k$ of the required cut.\n  - The upper bound $x$ on the number of remaining connected pairs.\n  - The lower bound $y$ on the number of connected pairs to be removed.\n  - The treewidth $w$ of $G$.\n  We determine whether or not CNC is fixed-parameter tractable for each of\nthese parameters. We determine this also for all possible aggregations of these\nfour parameters, apart from $w+k$. Moreover, we also determine whether or not\nCNC admits a polynomial kernel for all these parameterizations. That is,\nwhether or not there is an algorithm that reduces each instance of CNC in\npolynomial time to an equivalent instance of size $\\kappa^{O(1)}$, where\n$\\kappa$ is the given parameter.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 17:03:30 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 19:29:14 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Hermelin", "Danny", ""], ["Kaspi", "Moshe", ""], ["Komusiewicz", "Christian", ""], ["Navon", "Barak", ""]]}, {"id": "1503.06447", "submitter": "Raghu Meka", "authors": "Raghu Meka, Aaron Potechin, Avi Wigderson", "title": "Sum-of-squares lower bounds for planted clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding cliques in random graphs and the closely related \"planted\" clique\nvariant, where a clique of size k is planted in a random G(n, 1/2) graph, have\nbeen the focus of substantial study in algorithm design. Despite much effort,\nthe best known polynomial-time algorithms only solve the problem for k ~\nsqrt(n).\n  In this paper we study the complexity of the planted clique problem under\nalgorithms from the Sum-of-squares hierarchy. We prove the first average case\nlower bound for this model: for almost all graphs in G(n,1/2), r rounds of the\nSOS hierarchy cannot find a planted k-clique unless k > n^{1/2r} (up to\nlogarithmic factors). Thus, for any constant number of rounds planted cliques\nof size n^{o(1)} cannot be found by this powerful class of algorithms. This is\nshown via an integrability gap for the natural formulation of maximum clique\nproblem on random graphs for SOS and Lasserre hierarchies, which in turn follow\nfrom degree lower bounds for the Positivestellensatz proof system.\n  We follow the usual recipe for such proofs. First, we introduce a natural\n\"dual certificate\" (also known as a \"vector-solution\" or \"pseudo-expectation\")\nfor the given system of polynomial equations representing the problem for every\nfixed input graph. Then we show that the matrix associated with this dual\ncertificate is PSD (positive semi-definite) with high probability over the\nchoice of the input graph.This requires the use of certain tools. One is the\ntheory of association schemes, and in particular the eigenspaces and\neigenvalues of the Johnson scheme. Another is a combinatorial method we develop\nto compute (via traces) norm bounds for certain random matrices whose entries\nare highly dependent; we hope this method will be useful elsewhere.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 17:48:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Meka", "Raghu", ""], ["Potechin", "Aaron", ""], ["Wigderson", "Avi", ""]]}, {"id": "1503.06572", "submitter": "Georgiana Ifrim", "authors": "Bichen Shi, Michel Schellekens, Georgiana Ifrim", "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of\n  Sorting Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a framework for analyzing the complexity of an\nalgorithm, acting as a bridge between average and worst-case behaviour. For\nexample, Quicksort and the Simplex algorithm are widely used in practical\napplications, despite their heavy worst-case complexity. Smoothed complexity\naims to better characterize such algorithms. Existing theoretical bounds for\nthe smoothed complexity of sorting algorithms are still quite weak.\nFurthermore, empirically computing the smoothed complexity via its original\ndefinition is computationally infeasible, even for modest input sizes. In this\npaper, we focus on accurately predicting the smoothed complexity of sorting\nalgorithms, using machine learning techniques. We propose two regression models\nthat take into account various properties of sorting algorithms and some of the\nknown theoretical results in smoothed analysis to improve prediction quality.\nWe show experimental results for predicting the smoothed complexity of\nQuicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore\nfilling the gap between known theoretical and empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:37:33 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Shi", "Bichen", ""], ["Schellekens", "Michel", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1503.06929", "submitter": "Asahi Takaoka", "authors": "Asahi Takaoka", "title": "Graph isomorphism completeness for trapezoid graphs", "comments": "4 pages, 3 Postscript figures", "journal-ref": null, "doi": "10.1587/transfun.E98.A.1838", "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of the graph isomorphism problem for trapezoid graphs has been\nopen over a decade. This paper shows that the problem is GI-complete. More\nprecisely, we show that the graph isomorphism problem is GI-complete for\ncomparability graphs of partially ordered sets with interval dimension 2 and\nheight 3. In contrast, the problem is known to be solvable in polynomial time\nfor comparability graphs of partially ordered sets with interval dimension at\nmost 2 and height at most 2.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 06:56:38 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Takaoka", "Asahi", ""]]}, {"id": "1503.07261", "submitter": "Mark Bun", "authors": "Mark Bun and Justin Thaler", "title": "Dual Polynomials for Collision and Element Distinctness", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate degree of a Boolean function $f: \\{-1, 1\\}^n \\to \\{-1, 1\\}$\nis the minimum degree of a real polynomial that approximates $f$ to within\nerror $1/3$ in the $\\ell_\\infty$ norm. In an influential result, Aaronson and\nShi (J. ACM 2004) proved tight $\\tilde{\\Omega}(n^{1/3})$ and\n$\\tilde{\\Omega}(n^{2/3})$ lower bounds on the approximate degree of the\nCollision and Element Distinctness functions, respectively. Their proof was\nnon-constructive, using a sophisticated symmetrization argument and tools from\napproximation theory.\n  More recently, several open problems in the study of approximate degree have\nbeen resolved via the construction of dual polynomials. These are explicit dual\nsolutions to an appropriate linear program that captures the approximate degree\nof any function. We reprove Aaronson and Shi's results by constructing explicit\ndual polynomials for the Collision and Element Distinctness functions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 02:14:41 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Bun", "Mark", ""], ["Thaler", "Justin", ""]]}, {"id": "1503.07310", "submitter": "Trung Van Pham", "authors": "Manuel Bodirsky, Peter Jonsson, Trung Van Pham", "title": "The Complexity of Phylogeny Constraint Satisfaction Problems", "comments": "48 pages, 2 figures. In this version we fix several bugs in the\n  proofs of the previous versions", "journal-ref": "ACM Transactions on Computational Logic (TOCL), 18(3), 2017", "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically study the computational complexity of a broad class of\ncomputational problems in phylogenetic reconstruction. The class contains for\nexample the rooted triple consistency problem, forbidden subtree problems, the\nquartet consistency problem, and many other problems studied in the\nbioinformatics literature. The studied problems can be described as\n\\emph{constraint satisfaction problems} where the constraints have a\nfirst-order definition over the rooted triple relation. We show that every such\nphylogeny problem can be solved in polynomial time or is NP-complete. On the\nalgorithmic side, we generalize a well-known polynomial-time algorithm of Aho,\nSagiv, Szymanski, and Ullman for the rooted triple consistency problem. Our\nalgorithm repeatedly solves linear equation systems to construct a solution in\npolynomial time. We then show that every phylogeny problem that cannot be\nsolved by our algorithm is NP-complete. Our classification establishes a\ndichotomy for a large class of infinite structures that we believe is of\nindependent interest in universal algebra, model theory, and topology. The\nproof of our main result combines results and techniques from various research\nareas: a recent classification of the model-complete cores of the reducts of\nthe homogeneous binary branching C-relation, Leeb's Ramsey theorem for rooted\ntrees, and universal algebra.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 09:18:40 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 13:05:26 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 11:09:25 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 13:55:07 GMT"}, {"version": "v5", "created": "Mon, 14 Aug 2017 15:19:37 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Jonsson", "Peter", ""], ["Van Pham", "Trung", ""]]}, {"id": "1503.07691", "submitter": "Jevg\\=enijs Vihrovs", "authors": "Andris Ambainis and Kri\\v{s}j\\=anis Pr\\=usis and Jevg\\=enijs Vihrovs", "title": "Sensitivity versus Certificate Complexity of Boolean Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity, block sensitivity and certificate complexity are basic\ncomplexity measures of Boolean functions. The famous sensitivity conjecture\nclaims that sensitivity is polynomially related to block sensitivity. However,\nit has been notoriously hard to obtain even exponential bounds. Since block\nsensitivity is known to be polynomially related to certificate complexity, an\nequivalent of proving this conjecture would be showing that certificate\ncomplexity is polynomially related to sensitivity. Previously, it has been\nshown that $bs(f) \\leq C(f) \\leq 2^{s(f)-1} s(f) - (s(f)-1)$. In this work, we\ngive a better upper bound of $bs(f) \\leq C(f) \\leq\n\\max\\left(2^{s(f)-1}\\left(s(f)-\\frac 1 3\\right), s(f)\\right)$ using a recent\ntheorem limiting the structure of function graphs. We also examine relations\nbetween these measures for functions with small 1-sensitivity $s_1(f)$ and\narbitrary 0-sensitivity $s_0(f)$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:24:58 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 09:34:11 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ambainis", "Andris", ""], ["Pr\u016bsis", "Kri\u0161j\u0101nis", ""], ["Vihrovs", "Jevg\u0113nijs", ""]]}, {"id": "1503.07705", "submitter": "Ignacio Garcia-Marco", "authors": "Ignacio Garc\\'ia-Marco (LIP), Pascal Koiran (LIP), S\\'ebastien Tavenas", "title": "Log-concavity and lower bounds for arithmetic circuits", "comments": null, "journal-ref": "MFCS 2015, Part II, LNCS 9235, pp. 361-371 (2015)", "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One question that we investigate in this paper is, how can we build\nlog-concave polynomials using sparse polynomials as building blocks? More\nprecisely, let $f = \\sum\\_{i = 0}^d a\\_i X^i \\in \\mathbb{R}^+[X]$ be a\npolynomial satisfying the log-concavity condition $a\\_i^2 \\textgreater{} \\tau\na\\_{i-1}a\\_{i+1}$ for every $i \\in \\{1,\\ldots,d-1\\},$ where $\\tau\n\\textgreater{} 0$. Whenever $f$ can be written under the form $f = \\sum\\_{i =\n1}^k \\prod\\_{j = 1}^m f\\_{i,j}$ where the polynomials $f\\_{i,j}$ have at most\n$t$ monomials, it is clear that $d \\leq k t^m$. Assuming that the $f\\_{i,j}$\nhave only non-negative coefficients, we improve this degree bound to $d =\n\\mathcal O(k m^{2/3} t^{2m/3} {\\rm log^{2/3}}(kt))$ if $\\tau \\textgreater{} 1$,\nand to $d \\leq kmt$ if $\\tau = d^{2d}$.\n  This investigation has a complexity-theoretic motivation: we show that a\nsuitable strengthening of the above results would imply a separation of the\nalgebraic complexity classes VP and VNP. As they currently stand, these results\nare strong enough to provide a new example of a family of polynomials in VNP\nwhich cannot be computed by monotone arithmetic circuits of polynomial size.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 12:29:43 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Garc\u00eda-Marco", "Ignacio", "", "LIP"], ["Koiran", "Pascal", "", "LIP"], ["Tavenas", "S\u00e9bastien", ""]]}, {"id": "1503.08572", "submitter": "Antoine Mottet", "authors": "Manuel Bodirsky, Barnaby Martin, and Antoine Mottet", "title": "Discrete Temporal Constraint Satisfaction Problems", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A discrete temporal constraint satisfaction problem is a constraint\nsatisfaction problem (CSP) whose constraint language consists of relations that\nare first-order definable over $(\\Bbb Z,<)$. Our main result says that every\ndistance CSP is in Ptime or NP-complete, unless it can be formulated as a\nfinite domain CSP in which case the computational complexity is not known in\ngeneral.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 07:44:20 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 14:22:48 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 07:32:20 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Martin", "Barnaby", ""], ["Mottet", "Antoine", ""]]}, {"id": "1503.08643", "submitter": "Y. Ben-Aryeh", "authors": "Y. Ben-Aryeh", "title": "Strong and weak separability conditions for two-qubits density matrices", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit separable density matrices, for mixed two qubits states, are derived\nby the use of Hilbert Schmidt decompositions and Peres Horodecki criterion. A\nstrongly separable two qubits mixed state is defined by multiplications of two\ndensity matrices given with pure states while weakly separable two qubits state\nis defined by multiplications of two density matrices which includes non-pure\nstates. We find the sufficient and necessary condition for separability of\ntwo-qubits density matrices and show that under this condition the two-qubit\ndensity matrices are strongly separable.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 11:25:57 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 06:44:00 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Ben-Aryeh", "Y.", ""]]}, {"id": "1503.08659", "submitter": "Sophie Spirkl", "authors": "Stephan Held and Sophie Theresa Spirkl", "title": "Binary Adder Circuits of Asymptotically Minimum Depth, Linear Size, and\n  Fan-Out Two", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing fast and small binary adder circuits.\nAmong widely-used adders, the Kogge-Stone adder is often considered the\nfastest, because it computes the carry bits for two $n$-bit numbers (where $n$\nis a power of two) with a depth of $2\\log_2 n$ logic gates, size $4 n\\log_2 n$,\nand all fan-outs bounded by two. Fan-outs of more than two are avoided, because\nthey lead to the insertion of repeaters for repowering the signal and\nadditional depth in the physical implementation. However, the depth bound of\nthe Kogge-Stone adder is off by a factor of two from the lower bound of $\\log_2\nn$. This bound is achieved asymptotically in two separate constructions by\nBrent and Krapchenko. Brent's construction gives neither a bound on the fan-out\nnor the size, while Krapchenko's adder has linear size, but can have up to\nlinear fan-out. With a fan-out bound of two, neither construction achieves a\ndepth of less than $2 \\log_2 n$. In a further approach, Brent and Kung proposed\nan adder with linear size and fan-out two, but twice the depth of the\nKogge-Stone adder. These results are 33-43 years old and no substantial\ntheoretical improvement for has been made since then.\n  In this paper we integrate the individual advantages of all previous adder\ncircuits into a new family of full adders, the first to improve on the depth\nbound of $2\\log_2 n$ while maintaining a fan-out bound of two. Our adders\nachieve an asymptotically optimum logic gate depth of $\\log_2 n + o(\\log_2 n)$\nand linear size $\\mathcal {O}(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 13:02:53 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 09:56:26 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 21:53:09 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Held", "Stephan", ""], ["Spirkl", "Sophie Theresa", ""]]}, {"id": "1503.09016", "submitter": "G\\'abor Ivanyos", "authors": "Gabor Ivanyos, Miklos Santha", "title": "On solving systems of diagonal polynomial equations over finite fields", "comments": "A preliminary extended abstract of this paper has appeared in\n  Proceedings of FAW 2015, Springer LNCS vol. 9130, pp. 125-137 (2015)", "journal-ref": null, "doi": "10.1016/j.tcs.2016.04.045", "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to solve a system of diagonal polynomial equations\nover finite fields when the number of variables is greater than some fixed\npolynomial of the number of equations whose degree depends only on the degree\nof the polynomial equations. Our algorithm works in time polynomial in the\nnumber of equations and the logarithm of the size of the field, whenever the\ndegree of the polynomial equations is constant. As a consequence we design\npolynomial time quantum algorithms for two algebraic hidden structure problems:\nfor the hidden subgroup problem in certain semidirect product p-groups of\nconstant nilpotency class, and for the multi-dimensional univariate hidden\npolynomial graph problem when the degree of the polynomials is constant.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 12:06:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:54:48 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Ivanyos", "Gabor", ""], ["Santha", "Miklos", ""]]}]