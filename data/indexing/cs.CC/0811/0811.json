[{"id": "0811.0037", "submitter": "Martin Dyer", "authors": "Martin Dyer, Leslie Ann Goldberg and Mark Jerrum", "title": "A complexity dichotomy for hypergraph partition functions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of counting homomorphisms from an $r$-uniform\nhypergraph $G$ to a symmetric $r$-ary relation $H$. We give a dichotomy theorem\nfor $r>2$, showing for which $H$ this problem is in FP and for which $H$ it is\n#P-complete. This generalises a theorem of Dyer and Greenhill (2000) for the\ncase $r=2$, which corresponds to counting graph homomorphisms. Our dichotomy\ntheorem extends to the case in which the relation $H$ is weighted, and the goal\nis to compute the \\emph{partition function}, which is the sum of weights of the\nhomomorphisms. This problem is motivated by statistical physics, where it\narises as computing the partition function for particle models in which certain\ncombinations of $r$ sites interact symmetrically. In the weighted case, our\ndichotomy theorem generalises a result of Bulatov and Grohe (2005) for graphs,\nwhere $r=2$. When $r=2$, the polynomial time cases of the dichotomy correspond\nsimply to rank-1 weights. Surprisingly, for all $r>2$ the polynomial time cases\nof the dichotomy have rather more structure. It turns out that the weights must\nbe superimposed on a combinatorial structure defined by solutions of an\nequation over an Abelian group. Our result also gives a dichotomy for a closely\nrelated constraint satisfaction problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2008 22:42:57 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2010 15:39:33 GMT"}], "update_date": "2010-01-04", "authors_parsed": [["Dyer", "Martin", ""], ["Goldberg", "Leslie Ann", ""], ["Jerrum", "Mark", ""]]}, {"id": "0811.0463", "submitter": "Stefan Jaeger", "authors": "Stefan Jaeger", "title": "Solving the P/NP Problem under Intrinsic Uncertainty", "comments": "typos corrected, figure added, statements clarified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heisenberg's uncertainty principle states that it is not possible to compute\nboth the position and momentum of an electron with absolute certainty. However,\nthis computational limitation, which is central to quantum mechanics, has no\ncounterpart in theoretical computer science. Here, I will show that we can\ndistinguish between the complexity classes P and NP when we consider intrinsic\nuncertainty in our computations, and take uncertainty about whether a bit\nbelongs to the program code or machine input into account. Given intrinsic\nuncertainty, every output is uncertain, and computations become meaningful only\nin combination with a confidence level. In particular, it is impossible to\ncompute solutions with absolute certainty as this requires infinite run-time.\nConsidering intrinsic uncertainty, I will present a function that is in NP but\nnot in P, and thus prove that P is a proper subset of NP. I will also show that\nall traditional hard decision problems have polynomial-time algorithms that\nprovide solutions with confidence under uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 10:08:47 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2008 07:47:38 GMT"}], "update_date": "2008-11-10", "authors_parsed": [["Jaeger", "Stefan", ""]]}, {"id": "0811.0475", "submitter": "Manoj Prabhakaran", "authors": "Yuval Ishai, Manoj Prabhakaran and Amit Sahai", "title": "Secure Arithmetic Computation with No Honest Majority", "comments": "minor editorial changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of securely evaluating arithmetic circuits over\nfinite rings. This question is motivated by natural secure computation tasks.\nFocusing mainly on the case of two-party protocols with security against\nmalicious parties, our main goals are to: (1) only make black-box calls to the\nring operations and standard cryptographic primitives, and (2) minimize the\nnumber of such black-box calls as well as the communication overhead.\n  We present several solutions which differ in their efficiency, generality,\nand underlying intractability assumptions. These include:\n  1. An unconditionally secure protocol in the OT-hybrid model which makes a\nblack-box use of an arbitrary ring $R$, but where the number of ring operations\ngrows linearly with (an upper bound on) $\\log|R|$.\n  2. Computationally secure protocols in the OT-hybrid model which make a\nblack-box use of an underlying ring, and in which the number of ring operations\ndoes not grow with the ring size. These results extend a previous approach of\nNaor and Pinkas for secure polynomial evaluation (SIAM J. Comput., 35(5),\n2006).\n  3. A protocol for the rings $\\mathbb{Z}_m=\\mathbb{Z}/m\\mathbb{Z}$ which only\nmakes a black-box use of a homomorphic encryption scheme. When $m$ is prime,\nthe (amortized) number of calls to the encryption scheme for each gate of the\ncircuit is constant.\n  All of our protocols are in fact UC-secure in the OT-hybrid model and can be\ngeneralized to multiparty computation with an arbitrary number of malicious\nparties.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 11:18:06 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2008 16:40:42 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2008 17:54:42 GMT"}], "update_date": "2008-11-08", "authors_parsed": [["Ishai", "Yuval", ""], ["Prabhakaran", "Manoj", ""], ["Sahai", "Amit", ""]]}, {"id": "0811.0623", "submitter": "Joel Ratsaby", "authors": "J. Ratsaby and J. Chaskalovic", "title": "Algorithmic complexity and randomness in elastic solids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system comprised of an elastic solid and its response to an external random\nforce sequence is shown to behave based on the principles of the theory of\nalgorithmic complexity and randomness. The solid distorts the randomness of an\ninput force sequence in a way proportional to its algorithmic complexity. We\ndemonstrate this by numerical analysis of a one-dimensional vibrating elastic\nsolid (the system) on which we apply a maximally random input force. The level\nof complexity of the system is controlled via external parameters. The output\nresponse is the field of displacements observed at several positions on the\nbody. The algorithmic complexity and stochasticity of the resulting output\ndisplacement sequence is measured and compared against the complexity of the\nsystem. The results show that the higher the system complexity the more\nrandom-deficient the output sequence. This agrees with the theory introduced in\n[16] which states that physical systems such as this behave as algorithmic\nselection-rules which act on random actions in their surroundings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 21:34:54 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Ratsaby", "J.", ""], ["Chaskalovic", "J.", ""]]}, {"id": "0811.0699", "submitter": "Hiroki Morizumi", "authors": "Hiroki Morizumi", "title": "A Note on the Inversion Complexity of Boolean Functions in Boolean\n  Formulas", "comments": "5 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we consider the minimum number of NOT operators in a Boolean\nformula representing a Boolean function. In circuit complexity theory, the\nminimum number of NOT gates in a Boolean circuit computing a Boolean function\n$f$ is called the inversion complexity of $f$. In 1958, Markov determined the\ninversion complexity of every Boolean function and particularly proved that\n$\\lceil \\log_2(n+1) \\rceil$ NOT gates are sufficient to compute any Boolean\nfunction on $n$ variables. As far as we know, no result is known for inversion\ncomplexity in Boolean formulas, i.e., the minimum number of NOT operators in a\nBoolean formula representing a Boolean function. The aim of this note is\nshowing that we can determine the inversion complexity of every Boolean\nfunction in Boolean formulas by arguments based on the study of circuit\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 11:22:15 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Morizumi", "Hiroki", ""]]}, {"id": "0811.0881", "submitter": "Arnab Das", "authors": "Arnab Das", "title": "Non-classical Role of Potential Energy in Adiabatic Quantum Annealing", "comments": "10 pages, 2 figures (for the Proceedings of the International\n  Workshop on Statis tical-Mechanical Informatics 2008, Sendai, Japan). Journal\n  of Physics: Conference Series (to be publised)", "journal-ref": "J. Phys.: Conf. Ser. vol. 143 012001 (2009)", "doi": "10.1088/1742-6596/143/1/012001", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adiabatic quantum annealing is a paradigm of analog quantum computation,\nwhere a given computational job is converted to the task of finding the global\nminimum of some classical potential energy function and the search for the\nglobal potential minimum is performed by employing external kinetic quantum\nfluctuations and subsequent slow reduction (annealing) of them. In this method,\nthe entire potential energy landscape (PEL) may be accessed simultaneously\nthrough a delocalized wave-function, in contrast to a classical search, where\nthe searcher has to visit different points in the landscape (i.e., individual\nclassical configurations) sequentially. Thus in such searches, the role of the\npotential energy might be significantly different in the two cases. Here we\ndiscuss this in the context of searching of a single isolated hole (potential\nminimum) in a golf-course type gradient free PEL. We show, that the quantum\nparticle would be able to locate the hole faster if the hole is deeper, while\nthe classical particle of course would have no scope to exploit the depth of\nthe hole. We also discuss the effect of the underlying quantum phase transition\non the adiabatic dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2008 15:54:36 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Das", "Arnab", ""]]}, {"id": "0811.0959", "submitter": "Michael Thomas", "authors": "Olaf Beyersdorff, Arne Meier, Michael Thomas, Heribert Vollmer", "title": "The Complexity of Propositional Implication", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipl.2009.06.015", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question whether a set of formulae G implies a formula f is fundamental.\nThe present paper studies the complexity of the above implication problem for\npropositional formulae that are built from a systematically restricted set of\nBoolean connectives. We give a complete complexity classification for all sets\nof Boolean functions in the meaning of Post's lattice and show that the\nimplication problem is efficentily solvable only if the connectives are\ndefinable using the constants {false,true} and only one of {and,or,xor}. The\nproblem remains coNP-complete in all other cases. We also consider the\nrestriction of G to singletons.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2008 14:44:57 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2008 15:01:30 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2009 06:43:54 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Beyersdorff", "Olaf", ""], ["Meier", "Arne", ""], ["Thomas", "Michael", ""], ["Vollmer", "Heribert", ""]]}, {"id": "0811.0987", "submitter": "Andreas Blass", "authors": "Nikolaj Bj{\\o}rner (1), Andreas Blass (2), Yuri Gurevich (1), and\n  Madan Musuvathi (1)", "title": "Modular difference logic is hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In connection with machine arithmetic, we are interested in systems of\nconstraints of the form x + k \\leq y + k'. Over integers, the satisfiability\nproblem for such systems is polynomial time. The problem becomes NP complete if\nwe restrict attention to the residues for a fixed modulus N.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2008 19:52:45 GMT"}], "update_date": "2008-11-07", "authors_parsed": [["Bj\u00f8rner", "Nikolaj", ""], ["Blass", "Andreas", ""], ["Gurevich", "Yuri", ""], ["Musuvathi", "Madan", ""]]}, {"id": "0811.1075", "submitter": "Jan Johannsen", "authors": "Samuel R. Buss, Jan Hoffmann, Jan Johannsen", "title": "Resolution Trees with Lemmas: Resolution Refinements that Characterize\n  DLL Algorithms with Clause Learning", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 4 (December\n  5, 2008) lmcs:860", "doi": "10.2168/LMCS-4(4:13)2008", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution refinements called w-resolution trees with lemmas (WRTL) and with\ninput lemmas (WRTI) are introduced. Dag-like resolution is equivalent to both\nWRTL and WRTI when there is no regularity condition. For regular proofs, an\nexponential separation between regular dag-like resolution and both regular\nWRTL and regular WRTI is given.\n  It is proved that DLL proof search algorithms that use clause learning based\non unit propagation can be polynomially simulated by regular WRTI. More\ngenerally, non-greedy DLL algorithms with learning by unit propagation are\nequivalent to regular WRTI. A general form of clause learning, called\nDLL-Learn, is defined that is equivalent to regular WRTL.\n  A variable extension method is used to give simulations of resolution by\nregular WRTI, using a simplified form of proof trace extensions. DLL-Learn and\nnon-greedy DLL algorithms with learning by unit propagation can use variable\nextensions to simulate general resolution without doing restarts.\n  Finally, an exponential lower bound for WRTL where the lemmas are restricted\nto short clauses is shown.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 15:31:58 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2008 10:34:34 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Buss", "Samuel R.", ""], ["Hoffmann", "Jan", ""], ["Johannsen", "Jan", ""]]}, {"id": "0811.1103", "submitter": "Matthew Hague", "authors": "Matthew Hague and C.-H. Luke Ong", "title": "Symbolic Backwards-Reachability Analysis for Higher-Order Pushdown\n  Systems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 4 (December\n  5, 2008) lmcs:831", "doi": "10.2168/LMCS-4(4:14)2008", "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order pushdown systems (PDSs) generalise pushdown systems through the\nuse of higher-order stacks, that is, a nested \"stack of stacks\" structure.\nThese systems may be used to model higher-order programs and are closely\nrelated to the Caucal hierarchy of infinite graphs and safe higher-order\nrecursion schemes.\n  We consider the backwards-reachability problem over higher-order Alternating\nPDSs (APDSs), a generalisation of higher-order PDSs. This builds on and extends\nprevious work on pushdown systems and context-free higher-order processes in a\nnon-trivial manner. In particular, we show that the set of configurations from\nwhich a regular set of higher-order APDS configurations is reachable is regular\nand computable in n-EXPTIME. In fact, the problem is n-EXPTIME-complete.\n  We show that this work has several applications in the verification of\nhigher-order PDSs, such as linear-time model-checking, alternation-free\nmu-calculus model-checking and the computation of winning regions of\nreachability games.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 10:29:11 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2008 11:41:12 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Hague", "Matthew", ""], ["Ong", "C. -H. Luke", ""]]}, {"id": "0811.1305", "submitter": "Ryan Williams", "authors": "Ryan Williams", "title": "Applying Practice to Theory", "comments": "16 pages, 1 figure; ACM SIGACT News, December 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can complexity theory and algorithms benefit from practical advances in\ncomputing? We give a short overview of some prior work using practical\ncomputing to attack problems in computational complexity and algorithms,\ninformally describe how linear program solvers may be used to help prove new\nlower bounds for satisfiability, and suggest a research program for developing\nnew understanding in circuit complexity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2008 00:49:41 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Williams", "Ryan", ""]]}, {"id": "0811.1825", "submitter": "Xiaoyang Gu", "authors": "Jack H. Lutz", "title": "A Divergence Formula for Randomness and Dimension", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If $S$ is an infinite sequence over a finite alphabet $\\Sigma$ and $\\beta$ is\na probability measure on $\\Sigma$, then the {\\it dimension} of $ S$ with\nrespect to $\\beta$, written $\\dim^\\beta(S)$, is a constructive version of\nBillingsley dimension that coincides with the (constructive Hausdorff)\ndimension $\\dim(S)$ when $\\beta$ is the uniform probability measure. This paper\nshows that $\\dim^\\beta(S)$ and its dual $\\Dim^\\beta(S)$, the {\\it strong\ndimension} of $S$ with respect to $\\beta$, can be used in conjunction with\nrandomness to measure the similarity of two probability measures $\\alpha$ and\n$\\beta$ on $\\Sigma$. Specifically, we prove that the {\\it divergence formula}\n\\[\n  \\dim^\\beta(R) = \\Dim^\\beta(R) =\\frac{\\CH(\\alpha)}{\\CH(\\alpha) + \\D(\\alpha ||\n\\beta)} \\] holds whenever $\\alpha$ and $\\beta$ are computable, positive\nprobability measures on $\\Sigma$ and $R \\in \\Sigma^\\infty$ is random with\nrespect to $\\alpha$. In this formula, $\\CH(\\alpha)$ is the Shannon entropy of\n$\\alpha$, and $\\D(\\alpha||\\beta)$ is the Kullback-Leibler divergence between\n$\\alpha$ and $\\beta$. We also show that the above formula holds for all\nsequences $R$ that are $\\alpha$-normal (in the sense of Borel) when\n$\\dim^\\beta(R)$ and $\\Dim^\\beta(R)$ are replaced by the more effective\nfinite-state dimensions $\\dimfs^\\beta(R)$ and $\\Dimfs^\\beta(R)$. In the course\nof proving this, we also prove finite-state compression characterizations of\n$\\dimfs^\\beta(S)$ and $\\Dimfs^\\beta(S)$.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2008 06:30:55 GMT"}], "update_date": "2008-11-13", "authors_parsed": [["Lutz", "Jack H.", ""]]}, {"id": "0811.2497", "submitter": "Haris Aziz", "authors": "Haris Aziz and Mike Paterson", "title": "Computing voting power in easy weighted voting games", "comments": "12 pages, Presented at the International Symposium on Combinatorial\n  Optimization 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted voting games are ubiquitous mathematical models which are used in\neconomics, political science, neuroscience, threshold logic, reliability theory\nand distributed systems. They model situations where agents with variable\nvoting weight vote in favour of or against a decision. A coalition of agents is\nwinning if and only if the sum of weights of the coalition exceeds or equals a\nspecified quota. The Banzhaf index is a measure of voting power of an agent in\na weighted voting game. It depends on the number of coalitions in which the\nagent is the difference in the coalition winning or losing. It is well known\nthat computing Banzhaf indices in a weighted voting game is NP-hard. We give a\ncomprehensive classification of weighted voting games which can be solved in\npolynomial time. Among other results, we provide a polynomial\n($O(k{(\\frac{n}{k})}^k)$) algorithm to compute the Banzhaf indices in weighted\nvoting games in which the number of weight values is bounded by $k$.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2008 14:55:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2010 22:27:38 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Aziz", "Haris", ""], ["Paterson", "Mike", ""]]}, {"id": "0811.2586", "submitter": "Michael Vyalyi", "authors": "M. N. Vyalyi", "title": "On models of a nondeterministic computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a nondeterministic computation by deterministic\nmulti-head 2-way automata having a read-only access to an auxiliary memory. The\nmemory contains additional data (a guess) and computation is successful iff it\nis successful for some memory content. Also we consider the case of restricted\nguesses in which a guess should satisfy some constraint. We show that the\nstandard complexity classes such as L, NL, P, NP, PSPACE can be characterized\nin terms of these models of nondeterministic computation. These\ncharacterizations differ from the well-known ones by absence of alternation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2008 16:22:11 GMT"}], "update_date": "2008-11-18", "authors_parsed": [["Vyalyi", "M. N.", ""]]}, {"id": "0811.2731", "submitter": "Guillaume Theyssier", "authors": "Mathieu Sablik (LATP), Guillaume Theyssier (LAMA)", "title": "Topological Dynamics of Cellular Automata: Dimension Matters", "comments": "to appear in Theory of Computing Systems (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological dynamics of cellular automata (CA), inherited from classical\ndynamical systems theory, has been essentially studied in dimension 1. This\npaper focuses on higher dimensional CA and aims at showing that the situation\nis different and more complex starting from dimension 2. The main results are\nthe existence of non sensitive CA without equicontinuous points, the\nnon-recursivity of sensitivity constants, the existence of CA having only\nnon-recursive equicontinuous points and the existence of CA having only\ncountably many equicontinuous points. They all show a difference between\ndimension 1 and higher dimensions. Thanks to these new constructions, we also\nextend undecidability results concerning topological classification previously\nobtained in the 1D case. Finally, we show that the set of sensitive CA is only\nPi_2 in dimension 1, but becomes Sigma_3-hard for dimension 3.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2008 15:30:44 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2009 19:27:14 GMT"}], "update_date": "2009-09-03", "authors_parsed": [["Sablik", "Mathieu", "", "LATP"], ["Theyssier", "Guillaume", "", "LAMA"]]}, {"id": "0811.3116", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate", "title": "Geometric properties of satisfying assignments of random\n  $\\epsilon$-1-in-k SAT", "comments": null, "journal-ref": "International Journal of Computer Mathematics, 86(12), pp.\n  2029-2039, 2009", "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the geometric structure of the set of solutions of random\n$\\epsilon$-1-in-k SAT problem. For $l\\geq 1$, two satisfying assignments $A$\nand $B$ are $l$-connected if there exists a sequence of satisfying assignments\nconnecting them by changing at most $l$ bits at a time.\n  We first prove that w.h.p. two assignments of a random $\\epsilon$-1-in-$k$\nSAT instance are $O(\\log n)$-connected, conditional on being satisfying\nassignments. Also, there exists $\\epsilon_{0}\\in (0,\\frac{1}{k-2})$ such that\nw.h.p. no two satisfying assignments at distance at least $\\epsilon_{0}\\cdot n$\nform a \"hole\" in the set of assignments. We believe that this is true for all\n$\\epsilon >0$, and thus satisfying assignments of a random 1-in-$k$ SAT\ninstance form a single cluster.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 15:35:42 GMT"}], "update_date": "2010-03-18", "authors_parsed": [["Istrate", "Gabriel", ""]]}, {"id": "0811.3161", "submitter": "Nitin Saxena", "authors": "Nitin Saxena and C. Seshadhri", "title": "An Almost Optimal Rank Bound for Depth-3 Identities", "comments": "25 pages, preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the rank of a depth-3 circuit (over any field) that is simple,\nminimal and zero is at most k^3\\log d. The previous best rank bound known was\n2^{O(k^2)}(\\log d)^{k-2} by Dvir and Shpilka (STOC 2005). This almost resolves\nthe rank question first posed by Dvir and Shpilka (as we also provide a simple\nand minimal identity of rank \\Omega(k\\log d)).\n  Our rank bound significantly improves (dependence on k exponentially reduced)\nthe best known deterministic black-box identity tests for depth-3 circuits by\nKarnin and Shpilka (CCC 2008). Our techniques also shed light on the\nfactorization pattern of nonzero depth-3 circuits, most strikingly: the rank of\nlinear factors of a simple, minimal and nonzero depth-3 circuit (over any\nfield) is at most k^3\\log d.\n  The novel feature of this work is a new notion of maps between sets of linear\nforms, called \"ideal matchings\", used to study depth-3 circuits. We prove\ninteresting structural results about depth-3 identities using these techniques.\nWe believe that these can lead to the goal of a deterministic polynomial time\nidentity test for these circuits.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 17:41:06 GMT"}], "update_date": "2008-11-20", "authors_parsed": [["Saxena", "Nitin", ""], ["Seshadhri", "C.", ""]]}, {"id": "0811.3165", "submitter": "Nitin Saxena", "authors": "G\\'abor Ivanyos, Marek Karpinski, Lajos R\\'onyai and Nitin Saxena", "title": "Trading GRH for algebra: algorithms for factoring polynomials and\n  related structures", "comments": "35 pages, preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop techniques that eliminate the need of the\nGeneralized Riemann Hypothesis (GRH) from various (almost all) known results\nabout deterministic polynomial factoring over finite fields. Our main result\nshows that given a polynomial f(x) of degree n over a finite field k, we can\nfind in deterministic poly(n^{\\log n},\\log |k|) time \"either\" a nontrivial\nfactor of f(x) \"or\" a nontrivial automorphism of k[x]/(f(x)) of order n. This\nmain tool leads to various new GRH-free results, most striking of which are:\n  (1) Given a noncommutative algebra over a finite field, we can find a zero\ndivisor in deterministic subexponential time.\n  (2) Given a positive integer r such that either 8|r or r has at least two\ndistinct odd prime factors. There is a deterministic polynomial time algorithm\nto find a nontrivial factor of the r-th cyclotomic polynomial over a finite\nfield.\n  In this paper, following the seminal work of Lenstra (1991) on constructing\nisomorphisms between finite fields, we further generalize classical Galois\ntheory constructs like cyclotomic extensions, Kummer extensions, Teichmuller\nsubgroups, to the case of commutative semisimple algebras with automorphisms.\nThese generalized constructs help eliminate the dependence on GRH.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 17:57:25 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2009 17:14:49 GMT"}], "update_date": "2009-02-08", "authors_parsed": [["Ivanyos", "G\u00e1bor", ""], ["Karpinski", "Marek", ""], ["R\u00f3nyai", "Lajos", ""], ["Saxena", "Nitin", ""]]}, {"id": "0811.3208", "submitter": "Martin Roetteler", "authors": "Martin Roetteler", "title": "Quantum algorithms for highly non-linear Boolean functions", "comments": "15 pages, 1 figure, to appear in Proceedings of the 21st Annual\n  ACM-SIAM Symposium on Discrete Algorithms (SODA'10). This updated version of\n  the paper contains a new exponential separation between classical and quantum\n  query complexity", "journal-ref": "Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete\n  Algorithms (SODA'10), pp. 448-457, 2010", "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to separate the power of classical and quantum models of computation\nhave a long history. The ultimate goal is to find exponential separations for\ncomputational problems. However, such separations do not come a dime a dozen:\nwhile there were some early successes in the form of hidden subgroup problems\nfor abelian groups--which generalize Shor's factoring algorithm perhaps most\nfaithfully--only for a handful of non-abelian groups efficient quantum\nalgorithms were found. Recently, problems have gotten increased attention that\nseek to identify hidden sub-structures of other combinatorial and algebraic\nobjects besides groups. In this paper we provide new examples for exponential\nseparations by considering hidden shift problems that are defined for several\nclasses of highly non-linear Boolean functions. These so-called bent functions\narise in cryptography, where their property of having perfectly flat Fourier\nspectra on the Boolean hypercube gives them resilience against certain types of\nattack. We present new quantum algorithms that solve the hidden shift problems\nfor several well-known classes of bent functions in polynomial time and with a\nconstant number of queries, while the classical query complexity is shown to be\nexponential. Our approach uses a technique that exploits the duality between\nbent functions and their Fourier transforms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 21:13:00 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2009 21:13:50 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Roetteler", "Martin", ""]]}, {"id": "0811.3648", "submitter": "Jelani Nelson", "authors": "Daniel M. Kane, Jelani Nelson, David P. Woodruff", "title": "Revisiting Norm Estimation in Data Streams", "comments": "added content; modified L_0 algorithm -- ParityLogEstimator in\n  version 1 contained an error, and the new algorithm uses slightly more space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the pth moment F_p (p nonnegative and real) in data\nstreams is as follows. There is a vector x which starts at 0, and many updates\nof the form x_i <-- x_i + v come sequentially in a stream. The algorithm also\nreceives an error parameter 0 < eps < 1. The goal is then to output an\napproximation with relative error at most eps to F_p = ||x||_p^p.\n  Previously, it was known that polylogarithmic space (in the vector length n)\nwas achievable if and only if p <= 2. We make several new contributions in this\nregime, including:\n  (*) An optimal space algorithm for 0 < p < 2, which, unlike previous\nalgorithms which had optimal dependence on 1/eps but sub-optimal dependence on\nn, does not rely on a generic pseudorandom generator.\n  (*) A near-optimal space algorithm for p = 0 with optimal update and query\ntime.\n  (*) A near-optimal space algorithm for the \"distinct elements\" problem (p = 0\nand all updates have v = 1) with optimal update and query time.\n  (*) Improved L_2 --> L_2 dimensionality reduction in a stream.\n  (*) New 1-pass lower bounds to show optimality and near-optimality of our\nalgorithms, as well as of some previous algorithms (the \"AMS sketch\" for p = 2,\nand the L_1-difference algorithm of Feigenbaum et al.).\n  As corollaries of our work, we also obtain a few separations in the\ncomplexity of moment estimation problems: F_0 in 1 pass vs. 2 passes, p = 0 vs.\np > 0, and F_0 with strictly positive updates vs. arbitrary updates.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 22:55:07 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 02:45:30 GMT"}], "update_date": "2009-04-09", "authors_parsed": [["Kane", "Daniel M.", ""], ["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "0811.3704", "submitter": "Olivier Finkel", "authors": "Olivier Finkel (LIP, Elm)", "title": "Highly Undecidable Problems about Recognizability by Tiling Systems", "comments": "to appear in a Special Issue of the journal Fundamenta Informaticae\n  on Machines, Computations and Universality", "journal-ref": "Fundamenta Informaticae 2, 91 (2009) 305-323", "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Altenbernd, Thomas and W\\\"ohrle have considered acceptance of languages of\ninfinite two-dimensional words (infinite pictures) by finite tiling systems,\nwith usual acceptance conditions, such as the B\\\"uchi and Muller ones [1]. It\nwas proved in [9] that it is undecidable whether a B\\\"uchi-recognizable\nlanguage of infinite pictures is E-recognizable (respectively, A-recognizable).\nWe show here that these two decision problems are actually $\\Pi_2^1$-complete,\nhence located at the second level of the analytical hierarchy, and \"highly\nundecidable\". We give the exact degree of numerous other undecidable problems\nfor B\\\"uchi-recognizable languages of infinite pictures. In particular, the\nnon-emptiness and the infiniteness problems are $\\Sigma^1_1$-complete, and the\nuniversality problem, the inclusion problem, the equivalence problem, the\ndeterminizability problem, the complementability problem, are all\n$\\Pi^1_2$-complete. It is also $\\Pi^1_2$-complete to determine whether a given\nB\\\"uchi recognizable language of infinite pictures can be accepted row by row\nusing an automaton model over ordinal words of length $\\omega^2$.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2008 17:41:28 GMT"}], "update_date": "2009-08-04", "authors_parsed": [["Finkel", "Olivier", "", "LIP, Elm"]]}, {"id": "0811.3760", "submitter": "Sebastien Tixeuil", "authors": "St\\'ephane Devismes, Toshimitsu Masuzawa, S\\'ebastien Tixeuil (LIP6)", "title": "Communication Efficiency in Self-stabilizing Silent Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-6731", "categories": "cs.DS cs.CC cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization is a general paradigm to provide forward recovery\ncapabilities to distributed systems and networks. Intuitively, a protocol is\nself-stabilizing if it is able to recover without external intervention from\nany catastrophic transient failure. In this paper, our focus is to lower the\ncommunication complexity of self-stabilizing protocols \\emph{below} the need of\nchecking every neighbor forever. In more details, the contribution of the paper\nis threefold: (i) We provide new complexity measures for communication\nefficiency of self-stabilizing protocols, especially in the stabilized phase or\nwhen there are no faults, (ii) On the negative side, we show that for\nnon-trivial problems such as coloring, maximal matching, and maximal\nindependent set, it is impossible to get (deterministic or probabilistic)\nself-stabilizing solutions where every participant communicates with less than\nevery neighbor in the stabilized phase, and (iii) On the positive side, we\npresent protocols for coloring, maximal matching, and maximal independent set\nsuch that a fraction of the participants communicates with exactly one neighbor\nin the stabilized phase.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2008 17:29:25 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Devismes", "St\u00e9phane", "", "LIP6"], ["Masuzawa", "Toshimitsu", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "0811.3782", "submitter": "Martin Ziegler", "authors": "Martin Ziegler", "title": "Real Computation with Least Discrete Advice: A Complexity Theory of\n  Nonuniform Computability", "comments": "added Sections 5.1 and 5.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is folklore particularly in numerical and computer sciences that, instead\nof solving some general problem f:A->B, additional structural information about\nthe input x in A (that is any kind of promise that x belongs to a certain\nsubset A' of A) should be taken advantage of. Some examples from real number\ncomputation show that such discrete advice can even make the difference between\ncomputability and uncomputability. We turn this into a both topological and\ncombinatorial complexity theory of information, investigating for several\npractical problems how much advice is necessary and sufficient to render them\ncomputable.\n  Specifically, finding a nontrivial solution to a homogeneous linear equation\nA*x=0 for a given singular real NxN-matrix A is possible when knowing\nrank(A)=0,1,...,N-1; and we show this to be best possible. Similarly,\ndiagonalizing (i.e. finding a BASIS of eigenvectors of) a given real symmetric\nNxN-matrix is possible when knowing the number of distinct eigenvalues: an\ninteger between 1 and N (the latter corresponding to the nondegenerate case).\nAnd again we show that N-fold (i.e. roughly log N bits of) additional\ninformation is indeed necessary in order to render this problem (continuous\nand) computable; whereas for finding SOME SINGLE eigenvector of A, providing\nthe truncated binary logarithm of the least-dimensional eigenspace of A--i.e.\nTheta(log N)-fold advice--is sufficient and optimal.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2008 00:06:40 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2008 08:35:49 GMT"}, {"version": "v3", "created": "Mon, 15 Dec 2008 16:10:56 GMT"}, {"version": "v4", "created": "Tue, 23 Dec 2008 11:08:27 GMT"}, {"version": "v5", "created": "Wed, 2 Sep 2009 14:52:59 GMT"}], "update_date": "2009-09-02", "authors_parsed": [["Ziegler", "Martin", ""]]}, {"id": "0811.3859", "submitter": "Raghavendra  Rao", "authors": "Raghavendra Rao B.V. and Jayalal M.N. Sarma", "title": "On the Complexity of Matroid Isomorphism Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We study the complexity of testing if two given matroids are isomorphic. The\nproblem is easily seen to be in $\\Sigma_2^p$. In the case of linear matroids,\nwhich are represented over polynomially growing fields, we note that the\nproblem is unlikely to be $\\Sigma_2^p$-complete and is $\\co\\NP$-hard. We show\nthat when the rank of the matroid is bounded by a constant, linear matroid\nisomorphism, and matroid isomorphism are both polynomial time many-one\nequivalent to graph isomorphism. We give a polynomial time Turing reduction\nfrom graphic matroid isomorphism problem to the graph isomorphism problem.\nUsing this, we are able to show that graphic matroid isomorphism testing for\nplanar graphs can be done in deterministic polynomial time. We then give a\npolynomial time many-one reduction from bounded rank matroid isomorphism\nproblem to graphic matroid isomorphism, thus showing that all the above\nproblems are polynomial time equivalent. Further, for linear and graphic\nmatroids, we prove that the automorphism problem is polynomial time equivalent\nto the corresponding isomorphism problems. In addition, we give a polynomial\ntime membership test algorithm for the automorphism group of a graphic matroid.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2008 14:19:02 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["V.", "Raghavendra Rao B.", ""], ["Sarma", "Jayalal M. N.", ""]]}, {"id": "0811.3958", "submitter": "Daniil Musatov", "authors": "Daniil Musatov", "title": "Extractors and an efficient variant of Muchnik's theorem", "comments": "37 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Muchnik's theorem about simple conditional descriprion states that for all\nwords $a$ and $b$ there exists a short program $p$ transforming $a$ to $b$ that\nhas the least possible length and is simple conditional on $b$. This paper\npresents a new proof of this theorem, based on extractors. Employing the\nextractor technique, two new versions of Muchnik's theorem for space- and\ntime-bounded Kolmogorov complexity are proven.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2008 20:49:50 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Musatov", "Daniil", ""]]}, {"id": "0811.3959", "submitter": "Michael Soltys", "authors": "Grzegorz Herman and Michael Soltys", "title": "A polytime proof of correctness of the Rabin-Miller algorithm from\n  Fermat's little theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a deterministic polytime algorithm for primality testing is now\nknown, the Rabin-Miller randomized test of primality continues being the most\nefficient and widely used algorithm.\n  We prove the correctness of the Rabin-Miller algorithm in the theory V1 for\npolynomial time reasoning, from Fermat's little theorem. This is interesting\nbecause the Rabin-Miller algorithm is a polytime randomized algorithm, which\nruns in the class RP (i.e., the class of polytime Monte-Carlo algorithms), with\na sampling space exponential in the length of the binary encoding of the input\nnumber. (The class RP contains polytime P.) However, we show how to express the\ncorrectness in the language of V1, and we also show that we can prove the\nformula expressing correctness with polytime reasoning from Fermat's Little\ntheorem, which is generally expected to be independent of V1.\n  Our proof is also conceptually very basic in the sense that we use the\nextended Euclid's algorithm, for computing greatest common divisors, as the\nmain workhorse of the proof. For example, we make do without proving the\nChinese Reminder theorem, which is used in the standard proofs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2008 20:34:32 GMT"}], "update_date": "2008-11-25", "authors_parsed": [["Herman", "Grzegorz", ""], ["Soltys", "Michael", ""]]}, {"id": "0811.4376", "submitter": "Soubhik Chakraborty", "authors": "Suman Kumar Sourabh and Soubhik Chakraborty", "title": "How robust is quicksort average complexity?", "comments": "15 pages;12figures;2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper questions the robustness of average case time complexity of the\nfast and popular quicksort algorithm. Among the six standard probability\ndistributions examined in the paper, only continuous uniform, exponential and\nstandard normal are supporting it whereas the others are supporting the worst\ncase complexity measure. To the question -why are we getting the worst case\ncomplexity measure each time the average case measure is discredited? -- one\nlogical answer is average case complexity under the universal distribution\nequals worst case complexity. This answer, which is hard to challenge, however\ngives no idea as to which of the standard probability distributions come under\nthe umbrella of universality. The morale is that average case complexity\nmeasures, in cases where they are different from those in worst case, should be\ndeemed as robust provided only they get the support from at least the standard\nprobability distributions, both discrete and continuous. Regretfully, this is\nnot the case with quicksort.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 17:23:22 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Sourabh", "Suman Kumar", ""], ["Chakraborty", "Soubhik", ""]]}]