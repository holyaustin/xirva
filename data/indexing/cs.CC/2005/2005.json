[{"id": "2005.00593", "submitter": "Dmitriy Zhuk", "authors": "Dmitriy Zhuk", "title": "Strong subalgebras and the Constraint Satisfaction Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2007 it was conjectured that the Constraint Satisfaction Problem (CSP)\nover a constraint language $\\Gamma$ is tractable if and only if $\\Gamma$ is\npreserved by a weak near-unanimity (WNU) operation. After many efforts and\npartial results, this conjecture was independently proved by Andrei Bulatov and\nthe author in 2017. In this paper we consider one of two main ingredients of my\nproof, that is, strong subalgebras that allow us to reduce domains of the\nvariables iteratively. To explain how this idea works we show the algebraic\nproperties of strong subalgebras and provide self-contained proof of two\nimportant facts about the complexity of the CSP. First, we prove that if a\nconstraint language is not preserved by a WNU operation then the corresponding\nCSP is NP-hard. Second, we characterize all constraint languages that can be\nsolved by local consistency checking. Additionally, we characterize all\nidempotent algebras not having a WNU term of a concrete arity $n$, not having a\nWNU term, having WNU terms of all arities greater than 2. Most of the results\npresented in the paper are not new, but I believe this paper can help to\nunderstand my approach to CSP and the new self-contained proof of known facts\nwill be also useful.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:19:00 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhuk", "Dmitriy", ""]]}, {"id": "2005.00809", "submitter": "Lev Gordeev", "authors": "Lev Gordeev", "title": "On P Versus NP", "comments": "Upper bounds on negative deviations in section 3.1.2 are wrong due to\n  errernous calculation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I generalize a well-known result that P = NP fails for monotone polynomial\ncircuits - more precisely, that the clique problem CLIQUE(k^4,k) is not\nsolvable by Boolean (AND,OR)-circuits of the size polynomial in k. In the other\nwords, there is no Boolean (AND,OR)-formula F expressing that a given graph\nwith k^4 vertices contains a clique of k elements, provided that the circuit\nlength of F, cl(F), is polynomial in k. In fact, for any solution F in\nquestion, cl(F) must be exponential in k. Moreover this holds also for DeMorgan\nnormal (abbr.: DMN) (AND,OR)-formulas F that allow negated variables. Based on\nthe latter observation I consider an arbitrary (AND,OR,NOT)-formula F and\nrecall that standard NOT-conversions to DMN at most double its circuit length.\nHence for any Boolean solution F of CLIQUE(k^4,k), cl(F) is exponential in k. I\nconclude that CLIQUE(k^4,k) is not solvable by polynomial-size Boolean\ncircuits, and hence P is not NP. The entire proof is formalizable by standard\nmethods in the exponential function arithmetic EFA.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:03:50 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 05:19:05 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 16:02:21 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2021 09:23:54 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gordeev", "Lev", ""]]}, {"id": "2005.01045", "submitter": "Yotam Dikstein", "authors": "Yotam Dikstein, Irit Dinur, Prahladh Harsha, Noga Ron-Zewi", "title": "Locally testable codes via high-dimensional expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally testable codes (LTC) are error-correcting codes that have a local\ntester which can distinguish valid codewords from words that are \"far\" from all\ncodewords by probing a given word only at a very few (sublinear, typically\nconstant) number of locations. Such codes form the combinatorial backbone of\nPCPs. A major open problem is whether there exist LTCs with positive rate,\nconstant relative distance and testable with a constant number of queries.\n  In this paper, we present a new approach towards constructing such LTCs using\nthe machinery of high-dimensional expanders. To this end, we consider the\nTanner representation of a code, which is specified by a graph and a base code.\nInformally, our result states that if this graph is part of a high-dimensional\nexpander then the local testability of the code follows from the local\ntestability of the base code.\n  This work unifies and generalizes the known results on testability of the\nHadamard, Reed-Muller and lifted codes on the Subspace Complex, all of which\nare proved via local self correction. However, unlike previous results,\nconstant rounds of self correction do not suffice as the diameter of the\nunderlying test graph can be logarithmically large in a high-dimensional\nexpander and not constant as in all known earlier results. We overcome this\ntechnical hurdle by performing iterative self correction with logarithmically\nmany rounds and tightly controlling the error in each iteration using\nproperties of the high-dimensional expander.\n  Given this result, the missing ingredient towards constructing a\nconstant-query LTC with positive rate and constant relative distance is an\ninstantiation of a base code that interacts well with a constant-degree\nhigh-dimensional expander.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 10:43:47 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Dikstein", "Yotam", ""], ["Dinur", "Irit", ""], ["Harsha", "Prahladh", ""], ["Ron-Zewi", "Noga", ""]]}, {"id": "2005.01076", "submitter": "Andr\\'es Herrera-Poyatos", "authors": "Andreas Galanis, Leslie Ann Goldberg and Andr\\'es Herrera-Poyatos", "title": "The complexity of approximating the complex-valued Potts model", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating the partition function of the\n$q$-state Potts model and the closely related Tutte polynomial for complex\nvalues of the underlying parameters. Apart from the classical connections with\nquantum computing and phase transitions in statistical physics, recent work in\napproximate counting has shown that the behaviour in the complex plane, and\nmore precisely the location of zeros, is strongly connected with the complexity\nof the approximation problem, even for positive real-valued parameters.\nPrevious work in the complex plane by Goldberg and Guo focused on $q=2$, which\ncorresponds to the case of the Ising model; for $q>2$, the behaviour in the\ncomplex plane is not as well understood and most work applies only to the\nreal-valued Tutte plane.\n  Our main result is a complete classification of the complexity of the\napproximation problems for all non-real values of the parameters, by\nestablishing \\#P-hardness results that apply even when restricted to planar\ngraphs. Our techniques apply to all $q\\geq 2$ and further complement/refine\nprevious results both for the Ising model and the Tutte plane, answering in\nparticular a question raised by Bordewich, Freedman, Lov\\'{a}sz and Welsh in\nthe context of quantum computations.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 12:59:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Herrera-Poyatos", "Andr\u00e9s", ""]]}, {"id": "2005.01323", "submitter": "M\\=aris Ozols", "authors": "Arjan Cornelissen and Stacey Jeffery and Maris Ozols and Alvaro\n  Piedrafita", "title": "Span programs and quantum time complexity", "comments": "54 pages, 2 figures", "journal-ref": "Proceedings of MFCS 2020, LIPIcs, vol. 170, pp. 26:1--26:14,\n  978-3-95977-159-7 (2020)", "doi": "10.4230/LIPIcs.MFCS.2020.26", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Span programs are an important model of quantum computation due to their\ntight correspondence with quantum query complexity. For any decision problem\n$f$, the minimum complexity of a span program for $f$ is equal, up to a\nconstant factor, to the quantum query complexity of $f$. Moreover, this\ncorrespondence is constructive. A span program for $f$ with complexity $C$ can\nbe compiled into a bounded error quantum algorithm for $f$ with query\ncomplexity $O(C)$, and vice versa.\n  In this work, we prove an analogous connection for quantum time complexity.\nIn particular, we show how to convert a quantum algorithm for $f$ with time\ncomplexity $T$ into a span program for $f$ such that it compiles back into a\nquantum algorithm for $f$ with time complexity $\\widetilde{O}(T)$. While the\nquery complexity of quantum algorithms obtained from span programs is\nwell-understood, it is not generally clear how to implement certain\nquery-independent operations in a time-efficient manner. We show that for span\nprograms derived from algorithms with a time-efficient implementation, we can\npreserve the time efficiency when implementing the span program. This means in\nparticular that span programs not only fully capture quantum query complexity,\nbut also quantum time complexity.\n  One practical advantage of being able to convert quantum algorithms to span\nprograms in a way that preserves time complexity is that span programs compose\nvery nicely. We demonstrate this by improving Ambainis's variable-time quantum\nsearch result using our construction through a span program composition for the\nOR function.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:43:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Cornelissen", "Arjan", ""], ["Jeffery", "Stacey", ""], ["Ozols", "Maris", ""], ["Piedrafita", "Alvaro", ""]]}, {"id": "2005.01374", "submitter": "Petra Wolf", "authors": "Henning Fernau and Petra Wolf", "title": "Synchronization of Deterministic Visibly Push-Down Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the concept of synchronizing words for finite automata, which\nmap all states of the automata to the same state, to deterministic visibly\npush-down automata. Here, a synchronizing word w does not only map all states\nto the same state but also fulfills some conditions on the stack content of\neach run after reading w. We consider three types of these stack constraints:\nafter reading w, the stack (1) is empty in each run, (2) contains the same\nsequence of stack symbols in each run, or (3) contains an arbitrary sequence\nwhich is independent of the other runs. We show that in contrast to general\ndeterministic push-down automata, it is decidable for deterministic visibly\npush-down automata whether there exists a synchronizing word with each of these\nstack constraints, i.e., the problems are in EXPTIME. Under the constraint (1)\nthe problem is even in P. For the sub-classes of deterministic very visibly\npush-down automata the problem is in P for all three types of constraints. We\nfurther study variants of the synchronization problem where the number of turns\nin the stack height behavior caused by a synchronizing word is restricted, as\nwell as the problem of synchronizing a variant of a sequential transducer,\nwhich shows some visibly behavior, by a word that synchronizes the states and\nproduces the same output on all runs.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:39:00 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:17:21 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 15:41:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Fernau", "Henning", ""], ["Wolf", "Petra", ""]]}, {"id": "2005.01381", "submitter": "Petra Wolf", "authors": "Henning Fernau and Petra Wolf and Tomoyuki Yamakami", "title": "Synchronizing Deterministic Push-Down Automata Can Be Really Hard", "comments": "arXiv admin note: text overlap with arXiv:2005.01374", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question if a deterministic finite automaton admits a software reset in\nthe form of a so-called synchronizing word can be answered in polynomial time.\nIn this paper, we extend this algorithmic question to deterministic automata\nbeyond finite automata. We prove that the question of synchronizability becomes\nundecidable even when looking at deterministic one-counter automata. This is\nalso true for another classical mild extension of regularity, namely that of\ndeterministic one-turn push-down automata. However, when we combine both\nrestrictions, we arrive at scenarios with a PSPACE-complete (and hence\ndecidable) synchronizability problem. Likewise, we arrive at a decidable\nsynchronizability problem for (partially) blind deterministic counter automata.\n  There are several interpretations of what synchronizability should mean for\ndeterministic push-down automata. This is depending on the role of the stack:\nshould it be empty on synchronization, should it be always the same or is it\narbitrary? For the automata classes studied in this paper, the complexity or\ndecidability status of the synchronizability problem is mostly independent of\nthis technicality, but we also discuss one class of automata where this makes a\ndifference.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:54:45 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:23:33 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 15:26:25 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Fernau", "Henning", ""], ["Wolf", "Petra", ""], ["Yamakami", "Tomoyuki", ""]]}, {"id": "2005.01412", "submitter": "Vasil Penchev", "authors": "Vasil Penchev", "title": "A class of examples demonstrating that P is different from NP in the \"P\n  vs NP\" problem", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CMI Millennium \"P vs NP Problem\" can be resolved e.g. if one shows at\nleast one counterexample to the conjecture \"P is equal to NP\". A certain class\nof problems being such counterexamples is formulated. This implies the\nrejection of the hypothesis \"P is equal to NP\" for any conditions satisfying\nthe formulation of the problem. Thus, the solution \"P is different from NP\" of\nthe problem is proved. The class of counterexamples can be interpreted as any\nquantum superposition of any finite set of quantum states. The Kochen-Specker\ntheorem is involved. Any fundamentally random choice among a finite set of\nalternatives belong to NP, but not to P. The conjecture that the set complement\nof P to NP can be described by that kind of choice is formulated exhaustively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:32:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Penchev", "Vasil", ""]]}, {"id": "2005.01460", "submitter": "Ignasi Sau", "authors": "Paloma T. Lima, Vinicius F. dos Santos, Ignasi Sau, U\\'everton S.\n  Souza", "title": "Reducing graph transversals via edge contractions", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph invariant $\\pi$, the Contraction($\\pi$) problem consists in,\ngiven a graph $G$ and two positive integers $k,d$, deciding whether one can\ncontract at most $k$ edges of $G$ to obtain a graph in which $\\pi$ has dropped\nby at least $d$. Galby et al. [ISAAC 2019, MFCS 2019] recently studied the case\nwhere $\\pi$ is the size of a minimum dominating set. We focus on graph\ninvariants defined as the minimum size of a vertex set that hits all the\noccurrences of graphs in a collection ${\\cal H}$ according to a fixed\ncontainment relation. We prove co-NP-hardness results under some assumptions on\nthe graphs in ${\\cal H}$, which in particular imply that Contraction($\\pi$) is\nco-NP-hard even for fixed $k=d=1$ when $\\pi$ is the size of a minimum feedback\nvertex set or an odd cycle transversal. In sharp contrast, we show that when\n$\\pi$ is the size of a minimum vertex cover, the problem is in XP parameterized\nby $d$.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:18:35 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 09:59:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lima", "Paloma T.", ""], ["Santos", "Vinicius F. dos", ""], ["Sau", "Ignasi", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "2005.01867", "submitter": "Fabian Frei", "authors": "Hans-Joachim B\\\"ockenhauer and Jan Dreier and Fabian Frei and Peter\n  Rossmanith", "title": "Advice for Online Knapsack With Removable Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the proportional knapsack problem, we are given a knapsack of some\ncapacity and a set of variably sized items. The goal is to pack some of these\nitems such that they fill the knapsack as much as possible without ever\nexceeding the capacity. The online version of this problem reveals the items\nand their sizes not all at once but one by one. For each item, the algorithm\nhas to decide immediately whether to pack it or not. We consider a natural\nvariant of this online knapsack problem, which has been coined removable\nknapsack and we denote by RemKnap. It differs from the classical variant by\nallowing the removal of any packed item from the knapsack. Repacking is\nimpossible, however: Once an item is removed, it is gone for good.\n  We analyze the advice complexity of this problem. It measures how many advice\nbits an omniscient oracle needs to provide for an online algorithm to reach any\ngiven competitive ratio, which is--understood in its strict sense--just the\nalgorithm's approximation factor. The online knapsack problem without\nremovability is known for its peculiar advice behavior involving three jumps in\ncompetitivity. We show that the advice complexity of RemKnap is quite different\nbut just as interesting. The competitivity starts from the golden ratio when no\nadvice is given. It then drops down in small increments to (1 + epsilon) for a\nconstant amount of advice already, which requires logarithmic advice in the\nclassical version. Removability comes as no relief to the perfectionist,\nhowever: Optimality still requires one full advice bit for every single item in\nthe instance as before.\n  These results are particularly noteworthy from a structural viewpoint for the\nexceptionally slow transition from near-optimality to optimality; such a steep\njump up from constant to full linear advice for just an infinitesimally small\nimprovement is unique among the online problems examined so far.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:00:11 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["B\u00f6ckenhauer", "Hans-Joachim", ""], ["Dreier", "Jan", ""], ["Frei", "Fabian", ""], ["Rossmanith", "Peter", ""]]}, {"id": "2005.01982", "submitter": "Guillaume Cheze", "authors": "Guillaume Ch\\`eze (IMT)", "title": "Envy-free cake cutting: A polynomial number of queries with high\n  probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.MA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a probabilistic framework in order to study the\nfair division of a divisible good, e.g. a cake, between n players. Our\nframework follows the same idea than the ''Full independence model'' used in\nthe study of fair division of indivisible goods. We show that, in this\nframework, there exists an envy-free division algorithm satisfying the\nfollowing probability estimate:$$\\mathbb{P}\\big( C(\\mu_1, \\ldots,\\mu_n) \\geq\nn^{7+b}\\big) = \\mathcal{O}\\Big(n^{-\\frac{b-1}{3}+1+o(1)}\\Big),$$where\n$\\mu_1,\\ldots, \\mu_n$ correspond to the preferences of the $n$\nplayers,$C(\\mu_1, \\ldots,\\mu_n)$ is the number of queries used by the algorithm\nand $b>4$.In particular, this gives$$\\lim_{n \\rightarrow +\n\\infty}\\mathbb{P}\\big( C(\\mu_1, \\ldots,\\mu_n) \\geq n^{12}\\big) = 0.$$It must be\nnoticed that nowadays few things are known about the complexity of envy-free\ndivision algorithms. Indeed, Procaccia has given a lower bound in $\\Omega(n^2)$\nand Aziz and Mackenzie have given an upper bound in $n^{n^{n^{n^{n^{n}}}}}$. As\nour estimate means that we have $C(\\mu_1, \\ldots, \\mu_n)<n^{12}$ with a high\nprobability, this gives a new insight on the complexity of envy-free cake\ncutting algorithms.\\\\Our result follows from a study of Webb's algorithm and a\ntheorem of Tao and Vu about the smallest singular value of a random matrix.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 07:35:18 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Ch\u00e8ze", "Guillaume", "", "IMT"]]}, {"id": "2005.02238", "submitter": "Pavel Dvo\\v{r}\\'ak", "authors": "Pavel Dvo\\v{r}\\'ak, Bruno Loff", "title": "Lower Bounds for Semi-adaptive Data Structures via Corruption", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic data structure problem we wish to maintain an encoding of some\ndata in memory, in such a way that we may efficiently carry out a sequence of\nqueries and updates to the data. A long-standing open problem in this area is\nto prove an unconditional polynomial lower bound of a trade-off between the\nupdate time and the query time of an adaptive dynamic data structure computing\nsome explicit function. Ko and Weinstein provided such lower bound for a\nrestricted class of {\\em semi-adaptive\\} data structures, which compute the\nDisjointness function. There, the data are subsets $x_1,\\dots,x_k$ and $y$ of\n$\\{1,\\dots,n\\}$, the updates can modify $y$ (by inserting and removing\nelements), and the queries are an index $i \\in \\{1,\\dots,k\\}$ (query $i$ should\nanswer whether $x_i$ and $y$ are disjoint, i.e., it should compute the\nDisjointness function applied to $(x_i, y)$). The semi-adaptiveness places a\nrestriction in how the data structure can be accessed in order to answer a\nquery. We generalize the lower bound of Ko and Weinstein to work not just for\nthe Disjointness, but for any function having high complexity under the smooth\ncorruption bound.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:36:31 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 18:27:54 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Dvo\u0159\u00e1k", "Pavel", ""], ["Loff", "Bruno", ""]]}, {"id": "2005.02300", "submitter": "Till Fluschnik", "authors": "Robert Bredereck, Till Fluschnik, Andrzej Kaczmarczyk", "title": "Multistage Committee Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electing a single committee of a small size is a classical and\nwell-understood voting situation. Being interested in a sequence of committees,\nwe introduce and study two time-dependent multistage models based on simple\nPlurality voting. Therein, we are given a sequence of voting profiles (stages)\nover the same set of agents and candidates, and our task is to find a small\ncommittee for each stage of high score. In the conservative model we\nadditionally require that any two consecutive committees have a small symmetric\ndifference. Analogously, in the revolutionary model we require large symmetric\ndifferences. We prove both models to be NP-hard even for a constant number of\nagents, and, based on this, initiate a parameterized complexity analysis for\nthe most natural parameters and combinations thereof. Among other results, we\nprove both models to be in XP yet W[1]-hard regarding the number of stages, and\nthat being revolutionary seems to be \"easier\" than being conservative: If the\n(upper- resp. lower-) bound on the size of symmetric differences is constant,\nthe conservative model remains NP-hard while the revolutionary model becomes\npolynomial-time solvable.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:55:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bredereck", "Robert", ""], ["Fluschnik", "Till", ""], ["Kaczmarczyk", "Andrzej", ""]]}, {"id": "2005.02421", "submitter": "Chi-Ning Chou", "authors": "Boaz Barak, Chi-Ning Chou, Xun Gao", "title": "Spoofing Linear Cross-Entropy Benchmarking in Shallow Quantum Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear cross-entropy benchmark (Linear XEB) has been used as a test for\nprocedures simulating quantum circuits. Given a quantum circuit $C$ with $n$\ninputs and outputs and purported simulator whose output is distributed\naccording to a distribution $p$ over $\\{0,1\\}^n$, the linear XEB fidelity of\nthe simulator is $\\mathcal{F}_{C}(p) = 2^n \\mathbb{E}_{x \\sim p} q_C(x) -1$\nwhere $q_C(x)$ is the probability that $x$ is output from the distribution\n$C|0^n\\rangle$. A trivial simulator (e.g., the uniform distribution) satisfies\n$\\mathcal{F}_C(p)=0$, while Google's noisy quantum simulation of a 53 qubit\ncircuit $C$ achieved a fidelity value of $(2.24\\pm0.21)\\times10^{-3}$ (Arute\net. al., Nature'19).\n  In this work we give a classical randomized algorithm that for a given\ncircuit $C$ of depth $d$ with Haar random 2-qubit gates achieves in expectation\na fidelity value of $\\Omega(\\tfrac{n}{L} \\cdot 15^{-d})$ in running time\n$\\textsf{poly}(n,2^L)$. Here $L$ is the size of the \\emph{light cone} of $C$:\nthe maximum number of input bits that each output bit depends on. In\nparticular, we obtain a polynomial-time algorithm that achieves large fidelity\nof $\\omega(1)$ for depth $O(\\sqrt{\\log n})$ two-dimensional circuits. To our\nknowledge, this is the first such result for two dimensional circuits of\nsuper-constant depth. Our results can be considered as an evidence that fooling\nthe linear XEB test might be easier than achieving a full simulation of the\nquantum circuit.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:01:48 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Barak", "Boaz", ""], ["Chou", "Chi-Ning", ""], ["Gao", "Xun", ""]]}, {"id": "2005.02478", "submitter": "Aditya Potukuchi", "authors": "Ben Lund and Aditya Potukuchi", "title": "On the list recoverability of randomly punctured codes", "comments": "13 pages, several changes in introductory sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a random puncturing of a code with good distance is list\nrecoverable beyond the Johnson bound. In particular, this implies that there\nare Reed-Solomon codes that are list recoverable beyond the Johnson bound. It\nwas previously known that there are Reed-Solomon codes that do not have this\nproperty. As an immediate corollary to our main theorem, we obtain better\ndegree bounds on unbalanced expanders that come from Reed-Solomon codes.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:42:56 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 05:40:31 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 23:01:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lund", "Ben", ""], ["Potukuchi", "Aditya", ""]]}, {"id": "2005.02607", "submitter": "Casper Gyurik", "authors": "Casper Gyurik, Chris Cade and Vedran Dunjko", "title": "Towards quantum advantage via topological data analysis", "comments": "26 pages, 3 figures. New algorithms and results added, improved\n  exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even after decades of quantum computing development, examples of generally\nuseful quantum algorithms with exponential speedups over classical counterparts\nare scarce. Recent progress in quantum algorithms for linear-algebra positioned\nquantum machine learning (QML) as a potential source of such useful exponential\nimprovements. Yet, in an unexpected development, a recent series of\n\"dequantization\" results has equally rapidly removed the promise of exponential\nspeedups for several QML algorithms. This raises the critical question whether\nexponential speedups of other linear-algebraic QML algorithms persist. In this\npaper, we study the quantum-algorithmic methods behind the algorithm for\ntopological data analysis of Lloyd, Garnerone and Zanardi through this lens. We\nprovide evidence that the problem solved by this algorithm is classically\nintractable by showing that its natural generalization is as hard as simulating\nthe one clean qubit model -- which is widely believed to require\nsuperpolynomial time on a classical computer -- and is thus very likely immune\nto dequantizations. Based on this result, we provide a number of new quantum\nalgorithms for problems such as rank estimation and complex network analysis,\nalong with complexity-theoretic evidence for their classical intractability.\nFurthermore, we analyze the suitability of the proposed quantum algorithms for\nnear-term implementations. Our results provide a number of useful applications\nfor full-blown, and restricted quantum computers with a guaranteed exponential\nspeedup over classical methods, recovering some of the potential for\nlinear-algebraic QML to become a killer application of quantum computers.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 06:31:24 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:23:27 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gyurik", "Casper", ""], ["Cade", "Chris", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2005.03123", "submitter": "Orr Paradise", "authors": "Amey Bhangale, Prahladh Harsha, Orr Paradise and Avishay Tal", "title": "Rigid Matrices From Rectangular PCPs", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of PCPs, that we refer to as rectangular PCPs, wherein\nproofs are thought of as square matrices, and the random coins used by the\nverifier can be partitioned into two disjoint sets, one determining the row of\neach query and the other determining the column.\n  We construct PCPs that are efficient, short, smooth and (almost-)rectangular.\nAs a key application, we show that proofs for hard languages in $NTIME(2^n)$,\nwhen viewed as matrices, are rigid infinitely often. This strengthens and\nsimplifies a recent result of Alman and Chen [FOCS, 2019] constructing explicit\nrigid matrices in FNP. Namely, we prove the following theorem:\n  - There is a constant $\\delta \\in (0,1)$ such that there is an FNP-machine\nthat, for infinitely many $N$, on input $1^N$ outputs $N \\times N$ matrices\nwith entries in $\\mathbb{F}_2$ that are $\\delta N^2$-far (in Hamming distance)\nfrom matrices of rank at most $2^{\\log N/\\Omega(\\log \\log N)}$.\n  Our construction of rectangular PCPs starts with an analysis of how\nrandomness yields queries in the Reed--Muller-based outer PCP of Ben-Sasson,\nGoldreich, Harsha, Sudan and Vadhan [SICOMP, 2006; CCC, 2005]. We then show how\nto preserve rectangularity under PCP composition and a smoothness-inducing\ntransformation. This warrants refined and stronger notions of rectangularity,\nwhich we prove for the outer PCP and its transforms.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 20:34:55 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 21:52:25 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 06:28:59 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 04:42:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bhangale", "Amey", ""], ["Harsha", "Prahladh", ""], ["Paradise", "Orr", ""], ["Tal", "Avishay", ""]]}, {"id": "2005.03176", "submitter": "Kishen N Gowda", "authors": "Kishen N. Gowda, Neeldhara Misra and Vraj Patel", "title": "A Parameterized Perspective on Attacking and Defending Elections", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-48966-3_21", "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of protecting and manipulating elections by\nrecounting and changing ballots, respectively. Our setting involves a\nplurality-based election held across multiple districts, and the problem\nformulations are based on the model proposed recently by~[Elkind et al, IJCAI\n2019]. It turns out that both of the manipulation and protection problems are\nNP-complete even in fairly simple settings. We study these problems from a\nparameterized perspective with the goal of establishing a more detailed\ncomplexity landscape. The parameters we consider include the number of voters,\nand the budgets of the attacker and the defender. While we observe\nfixed-parameter tractability when parameterizing by number of voters, our main\ncontribution is a demonstration of parameterized hardness when working with the\nbudgets of the attacker and the defender.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 23:53:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gowda", "Kishen N.", ""], ["Misra", "Neeldhara", ""], ["Patel", "Vraj", ""]]}, {"id": "2005.03192", "submitter": "Jayson Lynch", "authors": "Joshua Ani, Erik D. Demaine, Dylan H. Hendrickson, Jayson Lynch", "title": "Trains, Games, and Complexity: 0/1/2-Player Motion Planning through\n  Input/Output Gadgets", "comments": "37 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational complexity of motion planning through local\n\"input/output\" gadgets with separate entrances and exits, and a subset of\nallowed traversals from entrances to exits, each of which changes the state of\nthe gadget and thereby the allowed traversals. We study such gadgets in the 0-,\n1-, and 2-player settings, in particular extending past\nmotion-planning-through-gadgets work to 0-player games for the first time, by\nconsidering \"branchless\" connections between gadgets that route every gadget's\nexit to a unique gadget's entrance. Our complexity results include containment\nin L, NL, P, NP, and PSPACE; as well as hardness for NL, P, NP, and PSPACE. We\napply these results to show PSPACE-completeness for certain mechanics in\nFactorio, [the Sequence], and a restricted version of Trainyard, improving\nprior results. This work strengthens prior results on switching graphs and\nreachability switching games.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 01:12:29 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ani", "Joshua", ""], ["Demaine", "Erik D.", ""], ["Hendrickson", "Dylan H.", ""], ["Lynch", "Jayson", ""]]}, {"id": "2005.03256", "submitter": "Brendon Pon", "authors": "Brendon Pon", "title": "Critique of Boyu Sima's Proof that ${\\rm P}\\neq{\\rm NP}$", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review and critique Boyu Sima's paper, \"A solution of the P versus NP\nproblem based on specific property of clique function,\" (arXiv:1911.00722)\nwhich claims to prove that ${\\rm P}\\neq{\\rm NP}$ by way of removing the gap\nbetween the nonmonotone circuit complexity and the monotone circuit complexity\nof the clique function. We first describe Sima's argument, and then we describe\nwhere and why it fails. Finally, we present a simple example that clearly\ndemonstrates the failure.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 05:33:00 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Pon", "Brendon", ""]]}, {"id": "2005.03497", "submitter": "Mark Giesbrecht", "authors": "Mark Giesbrecht, Armin Jamshidpey, \\'Eric Schost", "title": "Subquadratic-Time Algorithms for Normal Bases", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.03278", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any finite Galois field extension $\\mathsf{K}/\\mathsf{F}$, with Galois\ngroup $G = \\mathrm{Gal}(\\mathsf{K}/\\mathsf{F})$, there exists an element\n$\\alpha \\in \\mathsf{K}$ whose orbit $G\\cdot\\alpha$ forms an $\\mathsf{F}$-basis\nof $\\mathsf{K}$. Such a $\\alpha$ is called a normal element and $G\\cdot\\alpha$\nis a normal basis. We introduce a probabilistic algorithm for testing whether a\ngiven $\\alpha \\in \\mathsf{K}$ is normal, when $G$ is either a finite abelian or\na metacyclic group. The algorithm is based on the fact that deciding whether\n$\\alpha$ is normal can be reduced to deciding whether $\\sum_{g \\in G}\ng(\\alpha)g \\in \\mathsf{K}[G]$ is invertible; it requires a slightly\nsubquadratic number of operations. Once we know that $\\alpha$ is normal, we\nshow how to perform conversions between the power basis of\n$\\mathsf{K}/\\mathsf{F}$ and the normal basis with the same asymptotic cost.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 19:54:39 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 18:12:53 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Giesbrecht", "Mark", ""], ["Jamshidpey", "Armin", ""], ["Schost", "\u00c9ric", ""]]}, {"id": "2005.04042", "submitter": "Stefan Hoffmann", "authors": "Stefan Hoffmann", "title": "Computational Complexity of Synchronization under Regular Commutative\n  Constraints", "comments": "Published in COCOON 2020 (The 26th International Computing and\n  Combinatorics Conference); 2nd version is update of the published version and\n  1st version; both contain a minor error, the assumption of maximality in the\n  NP-c and PSPACE-c results (propositions 5 & 6) is missing, and of\n  incomparability of the vectors in main theorem; fixed in this version. See\n  (new) discussion after main theorem", "journal-ref": "Computing and Combinatorics, 26th International Conference, COCOON\n  2020, Proceedings, pages 460-471", "doi": "10.1007/978-3-030-58150-3_37", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study the computational complexity of the constrained synchronization\nproblem for the class of regular commutative constraint languages. Utilizing a\nvector representation of regular commutative constraint languages, we give a\nfull classification of the computational complexity of the constraint\nsynchronization problem. Depending on the constraint language, our problem\nbecomes PSPACE-complete, NP-complete or polynomial time solvable. In addition,\nwe derive a polynomial time decision procedure for the complexity of the\nconstraint synchronization problem, given some constraint automaton accepting a\ncommutative language as input.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:43:23 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 20:12:21 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hoffmann", "Stefan", ""]]}, {"id": "2005.04068", "submitter": "Srinivasan Arunachalam", "authors": "Srinivasan Arunachalam and Supartha Podder", "title": "Communication memento: Memoryless communication complexity", "comments": "30 Pages; several improvements to the presentations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the communication complexity of computing functions\n$F:\\{0,1\\}^n\\times \\{0,1\\}^n \\rightarrow \\{0,1\\}$ in the memoryless\ncommunication model. Here, Alice is given $x\\in \\{0,1\\}^n$, Bob is given $y\\in\n\\{0,1\\}^n$ and their goal is to compute F(x,y) subject to the following\nconstraint: at every round, Alice receives a message from Bob and her reply to\nBob solely depends on the message received and her input x; the same applies to\nBob. The cost of computing F in this model is the maximum number of bits\nexchanged in any round between Alice and Bob (on the worst case input x,y). In\nthis paper, we also consider variants of our memoryless model wherein one party\nis allowed to have memory, the parties are allowed to communicate quantum bits,\nonly one player is allowed to send messages. We show that our memoryless\ncommunication model capture the garden-hose model of computation by Buhrman et\nal. (ITCS'13), space bounded communication complexity by Brody et al. (ITCS'13)\nand the overlay communication complexity by Papakonstantinou et al. (CCC'14).\nThus the memoryless communication complexity model provides a unified framework\nto study space-bounded communication models. We establish the following: (1) We\nshow that the memoryless communication complexity of F equals the logarithm of\nthe size of the smallest bipartite branching program computing F (up to a\nfactor 2); (2) We show that memoryless communication complexity equals\ngarden-hose complexity; (3) We exhibit various exponential separations between\nthese memoryless communication models.\n  We end with an intriguing open question: can we find an explicit function F\nand universal constant c>1 for which the memoryless communication complexity is\nat least $c \\log n$? Note that $c\\geq 2+\\varepsilon$ would imply a\n$\\Omega(n^{2+\\varepsilon})$ lower bound for general formula size, improving\nupon the best lower bound by Ne\\v{c}iporuk in 1966.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:33:25 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 12:36:47 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Arunachalam", "Srinivasan", ""], ["Podder", "Supartha", ""]]}, {"id": "2005.04123", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore", "title": "The ghosts of forgotten things: A study on size after forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forgetting is removing variables from a logical formula while preserving the\nconstraints on the other variables. In spite of being a form of reduction, it\ndoes not always decrease the size of the formula and may sometimes increase it.\nThis article discusses the implications of such an increase and analyzes the\ncomputational properties of the phenomenon. Given a propositional Horn formula,\na set of variables and a maximum allowed size, deciding whether forgetting the\nvariables from the formula can be expressed in that size is $D^p$-hard in\n$\\Sigma^p_2$. The same problem for unrestricted propositional formulae is\n$D^p_2$-hard in $\\Sigma^p_3$. The hardness results employ superredundancy: a\nsuperirredundant clause is in all formulae of minimal size equivalent to a\ngiven one. This concept may be useful outside forgetting.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:56:01 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 07:39:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Liberatore", "Paolo", ""]]}, {"id": "2005.04530", "submitter": "Zhong Sun", "authors": "Zhong Sun, Giacomo Pedretti, Piergiulio Mannocci, Elia Ambrosi,\n  Alessandro Bricalli, Daniele Ielmini", "title": "Time complexity of in-memory solution of linear systems", "comments": "Accepted by IEEE Trans. Electron Devices. The authors thank Scott\n  Aaronson for helpful discussion about time complexity", "journal-ref": "IEEE Trans. Electron Devices (2020)", "doi": "10.1109/TED.2020.2992435", "report-no": null, "categories": "cs.CC cs.ET cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-memory computing with crosspoint resistive memory arrays has been shown to\naccelerate data-centric computations such as the training and inference of deep\nneural networks, thanks to the high parallelism endowed by physical rules in\nthe electrical circuits. By connecting crosspoint arrays with negative feedback\namplifiers, it is possible to solve linear algebraic problems such as linear\nsystems and matrix eigenvectors in just one step. Based on the theory of\nfeedback circuits, we study the dynamics of the solution of linear systems\nwithin a memory array, showing that the time complexity of the solution is free\nof any direct dependence on the problem size N, rather it is governed by the\nminimal eigenvalue of an associated matrix of the coefficient matrix. We show\nthat, when the linear system is modeled by a covariance matrix, the time\ncomplexity is O(logN) or O(1). In the case of sparse positive-definite linear\nsystems, the time complexity is solely determined by the minimal eigenvalue of\nthe coefficient matrix. These results demonstrate the high speed of the circuit\nfor solving linear systems in a wide range of applications, thus supporting\nin-memory computing as a strong candidate for future big data and machine\nlearning accelerators.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 23:48:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sun", "Zhong", ""], ["Pedretti", "Giacomo", ""], ["Mannocci", "Piergiulio", ""], ["Ambrosi", "Elia", ""], ["Bricalli", "Alessandro", ""], ["Ielmini", "Daniele", ""]]}, {"id": "2005.04531", "submitter": "Zhong Sun", "authors": "Zhong Sun, Giacomo Pedretti, Elia Ambrosi, Alessandro Bricalli,\n  Daniele Ielmini", "title": "In-memory eigenvector computation in time O(1)", "comments": "Accepted by Adv. Intell. Syst", "journal-ref": "Advanced Intelligent Systems (2020)", "doi": "10.1002/aisy.202000042", "report-no": null, "categories": "cs.CC cs.ET cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-memory computing with crosspoint resistive memory arrays has gained\nenormous attention to accelerate the matrix-vector multiplication in the\ncomputation of data-centric applications. By combining a crosspoint array and\nfeedback amplifiers, it is possible to compute matrix eigenvectors in one step\nwithout algorithmic iterations. In this work, time complexity of the\neigenvector computation is investigated, based on the feedback analysis of the\ncrosspoint circuit. The results show that the computing time of the circuit is\ndetermined by the mismatch degree of the eigenvalues implemented in the\ncircuit, which controls the rising speed of output voltages. For a dataset of\nrandom matrices, the time for computing the dominant eigenvector in the circuit\nis constant for various matrix sizes, namely the time complexity is O(1). The\nO(1) time complexity is also supported by simulations of PageRank of real-world\ndatasets. This work paves the way for fast, energy-efficient accelerators for\neigenvector computation in a wide range of practical applications.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 23:54:23 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sun", "Zhong", ""], ["Pedretti", "Giacomo", ""], ["Ambrosi", "Elia", ""], ["Bricalli", "Alessandro", ""], ["Ielmini", "Daniele", ""]]}, {"id": "2005.04598", "submitter": "Klaus-Dieter Schewe", "authors": "Klaus-Dieter Schewe", "title": "Insignificant Choice Polynomial Time", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the late 1980s Gurevich conjectured that there is no logic capturing\nPTIME, where logic has to be understood in a very general way comprising\ncomputation models over structures. In this article we first refute Gurevich's\nconjecture. For this we extend the seminal research of Blass, Gurevich and\nShelah on {\\em choiceless polynomial time} (CPT), which exploits deterministic\nAbstract State Machines (ASMs) supporting unbounded parallelism to capture the\nchoiceless fragment of PTIME. CPT is strictly included in PTIME. We observe\nthat choice is unavoidable, but that a restricted version suffices, which\nguarantees that the final result is independent from the choice. Such a version\nof polynomially bounded ASMs, which we call {\\em insignificant choice\npolynomial time} (ICPT) will indeed capture PTIME. Even more, insignificant\nchoice can be captured by ASMs with choice restricted to atoms such that a {\\em\nlocal insignificance condition} is satisfied. As this condition can be\nexpressed in the logic of non-deterministic ASMs, we obtain a logic capturing\nPTIME. Furthermore, using inflationary fixed-points we can capture problems in\nPTIME by fixed-point formulae in a fragment of the logic of non-deterministic\nASMs plus inflationary fixed-points. We use this result for our second\ncontribution showing that PTIME differs from NP. For the proof we build again\non the research on CPT first establishing a limitation on permutation classes\nof the sets that can be activated by an ICPT computation. We then prove an\ninseparability theorem, which characterises classes of structures that cannot\nbe separated by the logic. In particular, this implies that SAT cannot be\ndecided by an ICPT computation.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 07:36:19 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 00:44:13 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 08:36:44 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 14:14:38 GMT"}, {"version": "v5", "created": "Tue, 8 Sep 2020 08:44:59 GMT"}, {"version": "v6", "created": "Tue, 22 Sep 2020 12:23:25 GMT"}, {"version": "v7", "created": "Mon, 1 Feb 2021 07:57:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Schewe", "Klaus-Dieter", ""]]}, {"id": "2005.04733", "submitter": "Lars Jaffke", "authors": "Lars Jaffke, Paloma T. Lima, Geevarghese Philip", "title": "Structural Parameterizations of Clique Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clique coloring of a graph is an assignment of colors to its vertices such\nthat no maximal clique is monochromatic. We initiate the study of structural\nparameterizations of the Clique Coloring problem which asks whether a given\ngraph has a clique coloring with $q$ colors. For fixed $q \\ge 2$, we give an\n$\\mathcal{O}^{\\star}(q^{tw})$-time algorithm when the input graph is given\ntogether with one of its tree decompositions of width $tw$. We complement this\nresult with a matching lower bound under the Strong Exponential Time\nHypothesis. We furthermore show that (when the number of colors is unbounded)\nClique Coloring is XP parameterized by clique-width.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:58:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Jaffke", "Lars", ""], ["Lima", "Paloma T.", ""], ["Philip", "Geevarghese", ""]]}, {"id": "2005.04916", "submitter": "Timon Barlag", "authors": "Timon Barlag and Heribert Vollmer", "title": "A Logical Characterization of Constant-Depth Circuits over the Reals", "comments": "24 pages, submitted to WoLLIC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we give an Immerman's Theorem for real-valued computation. We\ndefine circuits operating over real numbers and show that families of such\ncircuits of polynomial size and constant depth decide exactly those sets of\nvectors of reals that can be defined in first-order logic on R-structures in\nthe sense of Cucker and Meer. Our characterization holds both non-uniformily as\nwell as for many natural uniformity conditions.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:18:15 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 13:44:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Barlag", "Timon", ""], ["Vollmer", "Heribert", ""]]}, {"id": "2005.05052", "submitter": "Tuhin Sahai", "authors": "Tuhin Sahai", "title": "Dynamical Systems Theory and Algorithms for NP-hard Problems", "comments": "Accepted for Workshop on Set Oriented Numerics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article surveys the burgeoning area at the intersection of dynamical\nsystems theory and algorithms for NP-hard problems. Traditionally,\ncomputational complexity and the analysis of non-deterministic polynomial-time\n(NP)-hard problems have fallen under the purview of computer science and\ndiscrete optimization. However, over the past few years, dynamical systems\ntheory has increasingly been used to construct new algorithms and shed light on\nthe hardness of problem instances. We survey a range of examples that\nillustrate the use of dynamical systems theory in the context of computational\ncomplexity analysis and novel algorithm construction. In particular, we\nsummarize a) a novel approach for clustering graphs using the wave equation\npartial differential equation, b) invariant manifold computations for the\ntraveling salesman problem, c) novel approaches for building quantum networks\nof Duffing oscillators to solve the MAX-CUT problem, d) applications of the\nKoopman operator for analyzing optimization algorithms, and e) the use of\ndynamical systems theory to analyze computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 00:54:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sahai", "Tuhin", ""]]}, {"id": "2005.05143", "submitter": "Kevin Pratt", "authors": "Cornelius Brand, Kevin Pratt", "title": "An Algorithmic Method of Partial Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem and its applications: given a homogeneous\ndegree-$d$ polynomial $g$ as an arithmetic circuit, and a $d \\times d$ matrix\n$X$ whose entries are homogeneous linear polynomials, compute\n$g(\\partial/\\partial x_1, \\ldots, \\partial/\\partial x_n) \\det X$. By\nconsidering special cases of this problem we obtain faster parameterized\nalgorithms for several problems, including the matroid $k$-parity and\n$k$-matroid intersection problems, faster \\emph{deterministic} algorithms for\ntesting if a linear space of matrices contains an invertible matrix (Edmonds's\nproblem) and detecting $k$-internal outbranchings, and more. We also match the\nruntime of the fastest known deterministic algorithm for detecting subgraphs of\nbounded pathwidth, while using a new approach.\n  Our approach raises questions in algebraic complexity related to Waring rank\nand the exponent of matrix multiplication $\\omega$. In particular, we study a\nnew complexity measure on the space of homogeneous polynomials, namely the\nbilinear complexity of a polynomial's apolar algebra. Our algorithmic\nimprovements are reflective of the fact that for the degree-$n$ determinant\npolynomial this quantity is at most $O(n 2^{\\omega n})$, whereas all known\nupper bounds on the Waring rank of this polynomial exceed $n!$.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:36:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Brand", "Cornelius", ""], ["Pratt", "Kevin", ""]]}, {"id": "2005.05547", "submitter": "Jan Bok", "authors": "Jan Bok, Richard Brewster, Tom\\'as Feder, Pavol Hell and Nikola\n  Jedli\\v{c}kov\\'a", "title": "List homomorphism problems for signed graphs", "comments": "various enhancements, full graph-theoretical classification of\n  complexity for cycle-separable graphs, 44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider homomorphisms of signed graphs from a computational perspective.\nIn particular, we study the list homomorphism problem seeking a homomorphism of\nan input signed graph $(G,\\sigma)$, equipped with lists $L(v) \\subseteq V(H), v\n\\in V(G)$, of allowed images, to a fixed target signed graph $(H,\\pi)$. The\ncomplexity of the similar homomorphism problem without lists (corresponding to\nall lists being $L(v)=V(H)$) has been previously classified by Brewster and\nSiggers, but the list version remains open and appears difficult. We illustrate\nthis difficulty by classifying the complexity of the problem when $H$ is a tree\n(with possible loops). The tools we develop will be useful for classifications\nof other classes of signed graphs, and we illustrate this by classifying the\ncomplexity of irreflexive signed graphs in which the unicoloured edges form\nsome simple structures, namely paths or cycles. The structure of the signed\ngraphs in the polynomial cases is interesting, suggesting they may constitute a\nnice class of signed graphs analogous to the so-called bi-arc graphs (which\ncharacterized the polynomial cases of list homomorphisms to unsigned graphs).\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 04:57:23 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 08:54:18 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:56:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bok", "Jan", ""], ["Brewster", "Richard", ""], ["Feder", "Tom\u00e1s", ""], ["Hell", "Pavol", ""], ["Jedli\u010dkov\u00e1", "Nikola", ""]]}, {"id": "2005.05773", "submitter": "Yunhao Yang", "authors": "Yunhao Yang, Andrew Tan", "title": "Approximating Boolean Functions with Disjunctive Normal Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theorem states that: Every Boolean function can be $\\epsilon\n-approximated$ by a Disjunctive Normal Form (DNF) of size\n$O_{\\epsilon}(2^{n}/\\log{n})$. This paper will demonstrate this theorem in\ndetail by showing how this theorem is generated and proving its correctness. We\nwill also dive into some specific Boolean functions and explore how these\nBoolean functions can be approximated by a DNF whose size is within the\nuniversal bound $O_{\\epsilon}(2^{n}/\\log{n})$. The Boolean functions we\ninterested in are: Parity Function: the parity function can be\n$\\epsilon-approximated$ by a DNF of width $(1 - 2\\epsilon)n$ and size $2^{(1 -\n2\\epsilon)n}$. Furthermore, we will explore the lower bounds on the DNF's size\nand width. Majority Function: for every constant $1/2 < \\epsilon < 1$, there is\na DNF of size $2^{O(\\sqrt{n})}$ that can $\\epsilon-approximated$ the Majority\nFunction on n bits. Monotone Functions: every monotone function f can be\n$\\epsilon-approximated$ by a DNF g of size $2^{n - \\Omega\\epsilon(n)}$\nsatisfying $g(x) \\le f(x)$ for all x.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:51:22 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Yang", "Yunhao", ""], ["Tan", "Andrew", ""]]}, {"id": "2005.05907", "submitter": "Stefan Hoffmann", "authors": "Stefan Hoffmann", "title": "Ideal Separation and General Theorems for Constrained Synchronization\n  and their Application to Small Constraint Automata", "comments": "Complete reworking of the 1st version. The classification of the 1st\n  version is achieved by identifying and utilising more general theorems, which\n  are also stated in the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the constrained synchronization problem we ask if a given automaton admits\na synchronizing word coming from a fixed regular constraint language. We show\nthat intersecting a given constraint language with an ideal language decreases\nthe computational complexity. Additionally, we state a theorem giving\nPSPACE-hardness that broadly generalizes previously used constructions and a\nresult on how to combine languages by concatenation to get polynomial time\nsolvable constrained synchronization problems. We use these results to give a\nclassification of the complexity landscape for small constraint automata of up\nto three states.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:35:30 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:53:00 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hoffmann", "Stefan", ""]]}, {"id": "2005.06419", "submitter": "Rahul Jain", "authors": "Chetan Gupta, Rahul Jain and Raghunath Tewari", "title": "Time Space Optimal Algorithm for Computing Separators in Bounded Genus\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph separator is a subset of vertices of a graph whose removal divides\nthe graph into small components. Computing small graph separators for various\nclasses of graphs is an important computational task. In this paper, we present\na polynomial time algorithm that uses $O(g^{1/2}n^{1/2}\\log n)$-space to find\nan $O(g^{1/2}n^{1/2})$-sized separator of a graph having $n$ vertices and\nembedded on a surface of genus $g$.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:49:55 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Gupta", "Chetan", ""], ["Jain", "Rahul", ""], ["Tewari", "Raghunath", ""]]}, {"id": "2005.06436", "submitter": "Leonid A. Levin", "authors": "Leonid A. Levin", "title": "Fundamentals of Computing", "comments": "22 pages; extended", "journal-ref": "SIGACT News. 27(3):89-110, 1996", "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are notes for the course CS-172 I first taught in the Fall 1986 at UC\nBerkeley and subsequently at Boston University. The goal was to introduce the\nundergraduates to basic concepts of Theory of Computation and to provoke their\ninterest in further study. Model-dependent effects were systematically ignored.\nConcrete computational problems were considered only as illustrations of\ngeneral principles. The notes are skeletal: they do have (terse) proofs, but\nexercises, references, intuitive comments, examples are missing or inadequate.\nThe notes can be used for designing a course or by students who want to refresh\nthe known material or are bright and have access to an instructor for\nquestions. Each subsection takes about a week of the course.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:21:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 17:07:57 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 11:39:39 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Levin", "Leonid A.", ""]]}, {"id": "2005.06827", "submitter": "Stefan Neubert", "authors": "Katrin Casel, Tobias Friedrich, Stefan Neubert and Markus L. Schmid", "title": "Shortest Distances as Enumeration Problem", "comments": "Updated version adds the study of space complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the single source shortest distance (SSSD) and all pairs\nshortest distance (APSD) problems as enumeration problems (on unweighted and\ninteger weighted graphs), meaning that the elements $(u, v, d(u, v))$ -- where\n$u$ and $v$ are vertices with shortest distance $d(u, v)$ -- are produced and\nlisted one by one without repetition. The performance is measured in the RAM\nmodel of computation with respect to preprocessing time and delay, i.e., the\nmaximum time that elapses between two consecutive outputs. This point of view\nreveals that specific types of output (e.g., excluding the non-reachable pairs\n$(u, v, \\infty)$, or excluding the self-distances $(u, u, 0)$) and the order of\nenumeration (e.g., sorted by distance, sorted row-wise with respect to the\ndistance matrix) have a huge impact on the complexity of APSD while they appear\nto have no effect on SSSD.\n  In particular, we show for APSD that enumeration without output restrictions\nis possible with delay in the order of the average degree. Excluding\nnon-reachable pairs, or requesting the output to be sorted by distance,\nincreases this delay to the order of the maximum degree. Further, for weighted\ngraphs, a delay in the order of the average degree is also not possible without\npreprocessing or considering self-distances as output. In contrast, for SSSD we\nfind that a delay in the order of the maximum degree without preprocessing is\nattainable and unavoidable for any of these requirements.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:14:58 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 12:50:20 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Casel", "Katrin", ""], ["Friedrich", "Tobias", ""], ["Neubert", "Stefan", ""], ["Schmid", "Markus L.", ""]]}, {"id": "2005.07906", "submitter": "Shuai Shao", "authors": "Shuai Shao and Jin-Yi Cai", "title": "A Dichotomy for Real Boolean Holant Problems", "comments": "91 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a complexity dichotomy for Holant problems on the boolean domain\nwith arbitrary sets of real-valued constraint functions. These constraint\nfunctions need not be symmetric nor do we assume any auxiliary functions as in\nprevious results. It is proved that for every set $\\mathcal{F}$ of real-valued\nconstraint functions, Holant$(\\mathcal{F})$ is either P-time computable or\n#P-hard. The classification has an explicit criterion. This is the culmination\nof much research on this problem, and it uses previous results and techniques\nfrom many researchers. Some particularly intriguing concrete functions $f_6$,\n$f_8$ and their associated families with extraordinary closure properties\nrelated to Bell states in quantum information theory play an important role in\nthis proof.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:31:49 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Shao", "Shuai", ""], ["Cai", "Jin-Yi", ""]]}, {"id": "2005.07944", "submitter": "Mark Jerrum", "authors": "Martin Dyer, Marc Heinrich, Mark Jerrum and Haiko M\\\"uller", "title": "Polynomial-time approximation algorithms for the antiferromagnetic Ising\n  model on line graphs", "comments": "Minor revisions. The version is accepted for publication in\n  Combinatorics, Probability and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-time Markov chain Monte Carlo algorithm for\nestimating the partition function of the antiferromagnetic Ising model on any\nline graph. The analysis of the algorithm exploits the \"winding\" technology\ndevised by McQuillan [CoRR abs/1301.2880 (2013)] and developed by Huang, Lu and\nZhang [Proc. 27th Symp. on Disc. Algorithms (SODA16), 514-527]. We show that\nexact computation of the partition function is #P-hard, even for line graphs,\nindicating that an approximation algorithm is the best that can be expected. We\nalso show that Glauber dynamics for the Ising model is rapidly mixing on line\ngraphs, an example being the kagome lattice.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:42:54 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 13:25:21 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dyer", "Martin", ""], ["Heinrich", "Marc", ""], ["Jerrum", "Mark", ""], ["M\u00fcller", "Haiko", ""]]}, {"id": "2005.08099", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler", "title": "Reducibility and Statistical-Computational Gaps from Secret Leakage", "comments": "175 pages; subsumes preliminary draft arXiv:1908.06130; accepted for\n  presentation at the Conference on Learning Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference problems with conjectured statistical-computational gaps are\nubiquitous throughout modern statistics, computer science and statistical\nphysics. While there has been success evidencing these gaps from the failure of\nrestricted classes of algorithms, progress towards a more traditional\nreduction-based approach to computational complexity in statistical inference\nhas been limited. Existing reductions have largely been limited to inference\nproblems with similar structure -- primarily mapping among problems\nrepresentable as a sparse submatrix signal plus a noise matrix, which are\nsimilar to the common hardness assumption of planted clique.\n  The insight in this work is that a slight generalization of the planted\nclique conjecture -- secret leakage planted clique -- gives rise to a variety\nof new average-case reduction techniques, yielding a web of reductions among\nproblems with very different structure. Using variants of the planted clique\nconjecture for specific forms of secret leakage planted clique, we deduce tight\nstatistical-computational tradeoffs for a diverse range of problems including\nrobust sparse mean estimation, mixtures of sparse linear regressions, robust\nsparse linear regression, tensor PCA, variants of dense $k$-block stochastic\nblock models, negatively correlated sparse PCA, semirandom planted dense\nsubgraph, detection in hidden partition models and a universality principle for\nlearning sparse mixtures. In particular, a $k$-partite hypergraph variant of\nthe planted clique conjecture is sufficient to establish all of our\ncomputational lower bounds. Our techniques also reveal novel connections to\ncombinatorial designs and to random matrix theory. This work gives the first\nevidence that an expanded set of hardness assumptions, such as for secret\nleakage planted clique, may be a key first step towards a more complete theory\nof reductions among statistical problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:56:09 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:59:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""]]}, {"id": "2005.08609", "submitter": "P\\'al Andr\\'as Papp", "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer", "title": "On the Hardness of Red-Blue Pebble Games", "comments": null, "journal-ref": null, "doi": "10.1145/3350755.3400278", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Red-blue pebble games model the computation cost of a two-level memory\nhierarchy. We present various hardness results in different red-blue pebbling\nvariants, with a focus on the oneshot model. We first study the relationship\nbetween previously introduced red-blue pebble models (base, oneshot, nodel). We\nalso analyze a new variant (compcost) to obtain a more realistic model of\ncomputation. We then prove that red-blue pebbling is NP-hard in all of these\nmodel variants. Furthermore, we show that in the oneshot model, a\n$\\delta$-approximation algorithm for $\\delta<2$ is only possible if the unique\ngames conjecture is false. Finally, we show that greedy algorithms are not good\ncandidates for approximation, since they can return significantly worse\nsolutions than the optimum.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:44:59 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Papp", "P\u00e1l Andr\u00e1s", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2005.08887", "submitter": "Oleg Verbitsky", "authors": "Frank Fuhlbr\\\"uck, Johannes K\\\"obler, Ilia Ponomarenko, Oleg Verbitsky", "title": "The Weisfeiler-Leman Algorithm and Recognition of Graph Properties", "comments": "30 pages, 2 figures. This paper supersedes Section 5 in the first\n  version of arXiv:2002.04590. The 3rd version is a thorough revision of the\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) is a very useful\ncombinatorial tool in graph isomorphism testing. We address the applicability\nof $k$-WL to recognition of graph properties. Let $G$ be an input graph with\n$n$ vertices. We show that, if $n$ is prime, then vertex-transitivity of $G$\ncan be seen in a straightforward way from the output of 2-WL on $G$ and on the\nvertex-individualized copies of $G$. However, if $n$ is divisible by 16, then\n$k$-WL is unable to distinguish between vertex-transitive and\nnon-vertex-transitive graphs with $n$ vertices as long as $k=o(\\sqrt n)$.\nSimilar results are obtained for recognition of arc-transitivity.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:57:42 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 10:19:43 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 12:35:38 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Fuhlbr\u00fcck", "Frank", ""], ["K\u00f6bler", "Johannes", ""], ["Ponomarenko", "Ilia", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "2005.08918", "submitter": "Suprovat Ghoshal", "authors": "Suprovat Ghoshal and Anand Louis", "title": "Approximation Algorithms and Hardness for Strong Unique Games", "comments": "67 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UNIQUE GAMES problem is a central problem in algorithms and complexity\ntheory. Given an instance of UNIQUE GAMES, the STRONG UNIQUE GAMES problem asks\nto find the largest subset of vertices, such that the UNIQUE GAMES instance\ninduced on them is completely satisfiable. In this paper, we give new\nalgorithmic and hardness results for STRONG UNIQUE GAMES. Given an instance\nwith label set size $k$ where a set of $(1 - \\epsilon)$-fraction of the\nvertices induce an instance that is completely satisfiable, our first algorithm\nproduces a set of $1 - \\widetilde{O}({k^2}) \\epsilon \\sqrt{\\log n}$ fraction of\nthe vertices such that the UNIQUE GAMES induced on them is completely\nsatisfiable. In the same setting, our second algorithm produces a set of $1 -\n\\widetilde{O}({k^2}) \\sqrt{\\epsilon \\log d}$ (here $d$ is the largest vertex\ndegree of the graph) fraction of the vertices such that the UNIQUE GAMES\ninduced on them is completely satisfiable. The technical core of our results is\na new connection between STRONG UNIQUE GAMES and Small-Set-Vertex-Expansion in\ngraphs. Complementing this, assuming the Unique Games Conjecture, we prove that\nit is NP-hard to compute a set of size larger than $1 - \\Omega( \\sqrt{\\epsilon\n\\log k \\log d})$ for which all the constraints induced on this set are\nsatisfied.\n  Given an undirected graph $G(V,E)$ the ODD CYCLE TRANSVERSAL problem asks to\ndelete the least fraction of vertices to make the induced graph on the\nremaining vertices bipartite. As a corollary to our main algorithmic results,\nwe obtain an algorithm that outputs a set $S$ such the graph induced on $V\n\\setminus S$ is bipartite, and $|S|/n \\leq O(\\sqrt{\\epsilon \\log d})$ (here $d$\nis the largest vertex degree and $\\epsilon$ is the optimal fraction of vertices\nthat need to be deleted). Assuming the Unique Games Conjecture, we prove a\nmatching (up to constant factors) hardness.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:44:58 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ghoshal", "Suprovat", ""], ["Louis", "Anand", ""]]}, {"id": "2005.08962", "submitter": "Aviram Imber", "authors": "Aviram Imber, Benny Kimelfeld", "title": "Computing the Extremal Possible Ranks with Incomplete Preferences", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.09212", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various voting rules are based on ranking the candidates by scores induced by\naggregating voter preferences. A winner (respectively, unique winner) is a\ncandidate who receives a score not smaller than (respectively, strictly greater\nthan) the remaining candidates. Examples of such rules include the positional\nscoring rules and the Bucklin, Copeland, and Maximin rules. When voter\npreferences are known in an incomplete manner as partial orders, a candidate\ncan be a possible/necessary winner based on the possibilities of completing the\npartial votes. Past research has studied in depth the computational problems of\ndetermining the possible and necessary winners and unique winners.\n  These problems are all special cases of reasoning about the range of possible\npositions of a candidate under different tiebreakers. We investigate the\ncomplexity of determining this range, and particularly the extremal positions.\nAmong our results, we establish that finding each of the minimal and maximal\npositions is NP-hard for each of the above rules, including all positional\nscoring rules, pure or not. Hence, none of the tractable variants of\nnecessary/possible winner determination remain tractable for extremal position\ndetermination. Tractability can be retained when reasoning about the top-$k$\npositions for a fixed $k$. Yet, exceptional is Maximin where it is tractable to\ndecide whether the maximal rank is $k$ for $k=1$ (necessary winning) but it\nbecomes intractable for all $k>1$.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:40:33 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 09:41:59 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 11:43:28 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Imber", "Aviram", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2005.09507", "submitter": "Daniel Krenn", "authors": "Daniel Krenn and Jeffrey Shallit", "title": "Decidability and k-Regular Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a number of natural decision problems involving\nk-regular sequences. Specifically, they arise from - lower and upper bounds on\ngrowth rate; in particular boundedness, - images, - regularity (recognizability\nby a deterministic finite automaton) of preimages, and - factors, such as\nsquares and palindromes of such sequences. We show that the decision problems\nare undecidable.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:05:07 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 16:02:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Krenn", "Daniel", ""], ["Shallit", "Jeffrey", ""]]}, {"id": "2005.09595", "submitter": "Min Jae Song", "authors": "Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang", "title": "Continuous LWE", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous analogue of the Learning with Errors (LWE) problem,\nwhich we name CLWE. We give a polynomial-time quantum reduction from worst-case\nlattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees\nto those of LWE. Alternatively, our result can also be seen as opening new\navenues of (quantum) attacks on lattice problems. Our work resolves an open\nproblem regarding the computational complexity of learning mixtures of\nGaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As\nan additional motivation, (a slight variant of) CLWE was considered in the\ncontext of robust machine learning (Diakonikolas et al.~FOCS 2017), where\nhardness in the statistical query (SQ) model was shown; our work addresses the\nopen question regarding its computational hardness (Bubeck et al.~ICML 2019).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:16:12 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 20:55:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bruna", "Joan", ""], ["Regev", "Oded", ""], ["Song", "Min Jae", ""], ["Tang", "Yi", ""]]}, {"id": "2005.09836", "submitter": "Qi Qi", "authors": "Chuangyin Dang, Qi Qi, Yinyu Ye", "title": "Computations and Complexities of Tarski's Fixed Points and Supermodular\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two models of computation for Tarski's order preserving function\nf related to fixed points in a complete lattice: the oracle function model and\nthe polynomial function model. In both models, we find the first polynomial\ntime algorithm for finding a Tarski's fixed point. In addition, we provide a\nmatching oracle bound for determining the uniqueness in the oracle function\nmodel and prove it is Co-NP hard in the polynomial function model. The\nexistence of the pure Nash equilibrium in supermodular games is proved by\nTarski's fixed point theorem. Exploring the difference between supermodular\ngames and Tarski's fixed point, we also develop the computational results for\nfinding one pure Nash equilibrium and determining the uniqueness of the\nequilibrium in supermodular games.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 03:32:37 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Dang", "Chuangyin", ""], ["Qi", "Qi", ""], ["Ye", "Yinyu", ""]]}, {"id": "2005.10080", "submitter": "Rupert McCallum", "authors": "Rupert McCallum", "title": "A proof of P!=NP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is provable in PA that there is an arithmetically definable\nsequence $\\{\\phi_{n}:n \\in \\omega\\}$ of $\\Pi^{0}_{2}$-sentences, such that\n  - PRA+$\\{\\phi_{n}:n \\in \\omega\\}$ is $\\Pi^{0}_{2}$-sound and\n$\\Pi^{0}_{1}$-complete\n  - the length of $\\phi_{n}$ is bounded above by a polynomial function of $n$\nwith positive leading coefficient\n  - PRA+$\\phi_{n+1}$ always proves 1-consistency of PRA+$\\phi_{n}$.\n  One has that the growth in logical strength is in some sense \"as fast as\npossible\", manifested in the fact that the total general recursive functions\nwhose totality is asserted by the true $\\Pi^{0}_{2}$-sentences in the sequence\nare cofinal growth-rate-wise in the set of all total general recursive\nfunctions. We then develop an argument which makes use of a sequence of\nsentences constructed by an application of the diagonal lemma, which are\ngeneralisations in a broad sense of Hugh Woodin's \"Tower of Hanoi\" construction\nas outlined in his essay \"Tower of Hanoi\" in Chapter 18 of the anthology \"Truth\nin Mathematics\". The argument establishes the result that it is provable in PA\nthat $P \\neq NP$. We indicate how to pull the argument all the way down into\nEFA.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:56:15 GMT"}, {"version": "v10", "created": "Tue, 9 Mar 2021 09:22:29 GMT"}, {"version": "v11", "created": "Fri, 2 Apr 2021 06:20:13 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 06:55:17 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 19:12:08 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 20:26:12 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 09:08:07 GMT"}, {"version": "v6", "created": "Wed, 17 Jun 2020 20:30:33 GMT"}, {"version": "v7", "created": "Tue, 23 Jun 2020 20:28:23 GMT"}, {"version": "v8", "created": "Tue, 7 Jul 2020 14:44:36 GMT"}, {"version": "v9", "created": "Fri, 16 Oct 2020 17:19:59 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["McCallum", "Rupert", ""]]}, {"id": "2005.10182", "submitter": "Sandra Kiefer", "authors": "Sandra Kiefer, Brendan D. McKay", "title": "The Iteration Number of Colour Refinement", "comments": "22 pages, 3 figures, full version of a paper accepted at ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Colour Refinement procedure and its generalisation to higher dimensions,\nthe Weisfeiler-Leman algorithm, are central subroutines in approaches to the\ngraph isomorphism problem. In an iterative fashion, Colour Refinement computes\na colouring of the vertices of its input graph.\n  A trivial upper bound on the iteration number of Colour Refinement on graphs\nof order n is n-1. We show that this bound is tight. More precisely, we prove\nvia explicit constructions that there are infinitely many graphs G on which\nColour Refinement takes |G|-1 iterations to stabilise. Modifying the infinite\nfamilies that we present, we show that for every natural number n >= 10, there\nare graphs on n vertices on which Colour Refinement requires at least n-2\niterations to reach stabilisation.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 16:43:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Kiefer", "Sandra", ""], ["McKay", "Brendan D.", ""]]}, {"id": "2005.10328", "submitter": "Luiz Alberto Do Carmo Viana", "authors": "Luiz Alberto do Carmo Viana, Manoel Camp\\^elo, Ignasi Sau and Ana\n  Silva", "title": "A Unifying Model for Locally Constrained Spanning Tree Problems", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ and a digraph $D$ whose vertices are the edges of $G$, we\ninvestigate the problem of finding a spanning tree of $G$ that satisfies the\nconstraints imposed by $D$. The restrictions to add an edge in the tree depend\non its neighborhood in $D$. Here, we generalize previously investigated\nproblems by also considering as input functions $\\ell$ and $u$ on $E(G)$ that\ngive a lower and an upper bound, respectively, on the number of constraints\nthat must be satisfied by each edge. The produced feasibility problem is\ndenoted by \\texttt{G-DCST}, while the optimization problem is denoted by\n\\texttt{G-DCMST}. We show that \\texttt{G-DCST} is NP-complete even under strong\nassumptions on the structures of $G$ and $D$, as well as on functions $\\ell$\nand $u$. On the positive side, we prove two polynomial results, one for\n\\texttt{G-DCST} and another for \\texttt{G-DCMST}, and also give a simple\nexponential-time algorithm along with a proof that it is asymptotically optimal\nunder the \\ETH. Finally, we prove that other previously studied constrained\nspanning tree (\\textsc{CST}) problems can be modeled within our framework,\nnamely, the \\textsc{Conflict CST}, the \\textsc{Forcing CS, the \\textsc{At Least\nOne/All Dependency CST}, the \\textsc{Maximum Degree CST}, the \\textsc{Minimum\nDegree CST}, and the \\textsc{Fixed-Leaves Minimum Degree CST}.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:42:31 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Viana", "Luiz Alberto do Carmo", ""], ["Camp\u00ealo", "Manoel", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "2005.10506", "submitter": "Lu\\'is M. S. Russo", "authors": "Diogo M. Costa, Alexandre P. Francisco, Lu\\'is M. S. Russo", "title": "Hardness of Modern Games", "comments": "The work reported in this article was supported by national funds\n  through Funda\\c{c}\\~ao para a Ci\\^encia e Tecnologia (FCT) through projects\n  NGPHYLO PTDC/CCI-BIO/29676/2017 and UID/CEC/50021/2019. Funded in part by\n  European Union Horizon 2020 research and innovation programme under the Marie\n  Sk{\\l}odowska-Curie Actions grant agreement No 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity properties of modern puzzle games, Hexiom, Cut the\nRope and Back to Bed. The complexity of games plays an important role in the\ntype of experience they provide to players. Back to Bed is shown to be\nPSPACE-Hard and the first two are shown to be NP-Hard. These results give\nfurther insight into the structure of these games and the resulting\nconstructions may be useful in further complexity studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:55:18 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Costa", "Diogo M.", ""], ["Francisco", "Alexandre P.", ""], ["Russo", "Lu\u00eds M. S.", ""]]}, {"id": "2005.10743", "submitter": "Anru R. Zhang", "authors": "Yuetian Luo and Anru R. Zhang", "title": "Tensor Clustering with Planted Structures: Statistical Optimality and\n  Computational Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the statistical and computational limits of high-order\nclustering with planted structures. We focus on two clustering models, constant\nhigh-order clustering (CHC) and rank-one higher-order clustering (ROHC), and\nstudy the methods and theory for testing whether a cluster exists (detection)\nand identifying the support of cluster (recovery).\n  Specifically, we identify the sharp boundaries of signal-to-noise ratio for\nwhich CHC and ROHC detection/recovery are statistically possible. We also\ndevelop the tight computational thresholds: when the signal-to-noise ratio is\nbelow these thresholds, we prove that polynomial-time algorithms cannot solve\nthese problems under the computational hardness conjectures of hypergraphic\nplanted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)\nrecovery. We also propose polynomial-time tensor algorithms that achieve\nreliable detection and recovery when the signal-to-noise ratio is above these\nthresholds. Both sparsity and tensor structures yield the computational\nbarriers in high-order tensor clustering. The interplay between them results in\nsignificant differences between high-order tensor clustering and matrix\nclustering in literature in aspects of statistical and computational phase\ntransition diagrams, algorithmic approaches, hardness conjecture, and proof\ntechniques. To our best knowledge, we are the first to give a thorough\ncharacterization of the statistical and computational trade-off for such a\ndouble computational-barrier problem. Finally, we provide evidence for the\ncomputational hardness conjectures of HPC detection and HPDS recovery.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:53:44 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 01:57:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Luo", "Yuetian", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2005.10749", "submitter": "Nagaganesh Jaladanki", "authors": "Nagaganesh Jaladanki and Wilson Wu", "title": "Distributed Verifiers in PCP", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional proof systems involve a resource-bounded verifier communicating\nwith a powerful (but untrusted) prover. Distributed verifier proof systems are\na new family of proof models that involve a network of verifier nodes\ncommunicating with a single independent prover that has access to the complete\nnetwork structure of the verifiers. The prover is tasked with convincing all\nverifiers of some global property of the network graph. In addition, each\nindividual verifier may be given some input string they will be required to\nverify during the course of computation. Verifier nodes are allowed to exchange\nmessaged with nodes a constant distance away, and accept / reject the input\nafter some computation.\n  Because individual nodes are limited to a local view, communication with the\nprover is potentially necessary to prove global properties about the network\ngraph of nodes, which only the prover has access to. In this system of models,\nthe entire model accepts the input if and only if every individual node has\naccepted.\n  There are three models in the distributed verifier proof system family:\n$\\mathsf{LCP}$, $\\mathsf{dIP}$, and our proposed $\\mathsf{dPCP}$, with the\nfundamental difference between these coming from the type of communication\nestablished between the verifiers and the prover. In this paper, we will first\ngo over the past work in the $\\mathsf{LCP}$ and $\\mathsf{dIP}$ space before\nshowing properties and proofs in our $\\mathsf{dPCP}$ system.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:01:09 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jaladanki", "Nagaganesh", ""], ["Wu", "Wilson", ""]]}, {"id": "2005.10801", "submitter": "Farhad Pakdaman", "authors": "Farhad Pakdaman, Mohammad Ali Adelimanesh, Moncef Gabbouj, Mahmoud\n  Reza Hashemi", "title": "Complexity Analysis Of Next-Generation VVC Encoding and Decoding", "comments": "IEEE ICIP 2020", "journal-ref": "Proceedings of International Conference on Image Processing\n  (ICIP), (2020) 3134-3138", "doi": "10.1109/ICIP40778.2020.9190983", "report-no": null, "categories": "cs.MM cs.CC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the next generation video compression standard, Versatile Video Coding\n(VVC), provides a superior compression efficiency, its computational complexity\ndramatically increases. This paper thoroughly analyzes this complexity for both\nencoder and decoder of VVC Test Model 6, by quantifying the complexity\nbreak-down for each coding tool and measuring the complexity and memory\nrequirements for VVC encoding/decoding. These extensive analyses are performed\nfor six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD),\nRandom-Access (RA), and All-Intra (AI) conditions (a total of 320\nencoding/decoding). Results indicate that the VVC encoder and decoder are 5x\nand 1.5x more complex compared to HEVC in LD, and 31x and 1.8x in AI,\nrespectively. Detailed analysis of coding tools reveals that in LD on average,\nmotion estimation tools with 53%, transformation and quantization with 22%, and\nentropy coding with 7% dominate the encoding complexity. In decoding, loop\nfilters with 30%, motion compensation with 20%, and entropy decoding with 16%,\nare the most complex modules. Moreover, the required memory bandwidth for VVC\nencoding/decoding are measured through memory profiling, which are 30x and 3x\nof HEVC. The reported results and insights are a guide for future research and\nimplementations of energy-efficient VVC encoder/decoder.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:30:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Pakdaman", "Farhad", ""], ["Adelimanesh", "Mohammad Ali", ""], ["Gabbouj", "Moncef", ""], ["Hashemi", "Mahmoud Reza", ""]]}, {"id": "2005.10817", "submitter": "Alexander Wein", "authors": "Matthias L\\\"offler, Alexander S. Wein, Afonso S. Bandeira", "title": "Computationally efficient sparse clustering", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical and computational limits of clustering when the means of\nthe centres are sparse and their dimension is possibly much larger than the\nsample size. Our theoretical analysis focuses on the model $X_i = z_i \\theta +\n\\varepsilon_i, ~z_i \\in \\{-1,1\\}, ~\\varepsilon_i \\thicksim \\mathcal{N}(0,I)$,\nwhich has two clusters with centres $\\theta$ and $-\\theta$. We provide a finite\nsample analysis of a new sparse clustering algorithm based on sparse PCA and\nshow that it achieves the minimax optimal misclustering rate in the regime\n$\\|\\theta\\| \\rightarrow \\infty$.\n  Our results require the sparsity to grow slower than the square root of the\nsample size. Using a recent framework for computational lower bounds -- the\nlow-degree likelihood ratio -- we give evidence that this condition is\nnecessary for any polynomial-time clustering algorithm to succeed below the BBP\nthreshold. This complements existing evidence based on reductions and\nstatistical query lower bounds. Compared to these existing results, we cover a\nwider set of parameter regimes and give a more precise understanding of the\nruntime required and the misclustering error achievable. Our results imply that\na large class of tests based on low-degree polynomials fail to solve even the\nweak testing task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:51:30 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:21:09 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 17:38:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["L\u00f6ffler", "Matthias", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "2005.10830", "submitter": "Yaqiao Li", "authors": "Lianna Hambardzumyan, Yaqiao Li", "title": "Chang's lemma via Pinsker's inequality", "comments": "4 pages", "journal-ref": "Discrete Mathematics 343.1 (2020): 111496", "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the idea in [Impagliazzo, R., Moore, C. and Russell, A., An\nentropic proof of Chang's inequality. SIAM Journal on Discrete Mathematics,\n28(1), pp.173-176.] we give a short information theoretic proof for Chang's\nlemma that is based on Pinsker's inequality.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:39:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hambardzumyan", "Lianna", ""], ["Li", "Yaqiao", ""]]}, {"id": "2005.10885", "submitter": "Robert Andrews", "authors": "Robert Andrews", "title": "Algebraic Hardness versus Randomness in Low Characteristic", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that lower bounds for explicit constant-variate polynomials over\nfields of characteristic $p > 0$ are sufficient to derandomize polynomial\nidentity testing over fields of characteristic $p$. In this setting, existing\nwork on hardness-randomness tradeoffs for polynomial identity testing requires\neither the characteristic to be sufficiently large or the notion of hardness to\nbe stronger than the standard syntactic notion of hardness used in algebraic\ncomplexity. Our results make no restriction on the characteristic of the field\nand use standard notions of hardness.\n  We do this by combining the Kabanets-Impagliazzo generator with a white-box\nprocedure to take $p$-th roots of circuits computing a $p$-th power over fields\nof characteristic $p$. When the number of variables appearing in the circuit is\nbounded by some constant, this procedure turns out to be efficient, which\nallows us to bypass difficulties related to factoring circuits in\ncharacteristic $p$.\n  We also combine the Kabanets-Impagliazzo generator with recent\n\"bootstrapping\" results in polynomial identity testing to show that a\nsufficiently-hard family of explicit constant-variate polynomials yields a\nnear-complete derandomization of polynomial identity testing. This result holds\nover fields of both zero and positive characteristic and complements a recent\nwork of Guo, Kumar, Saptharishi, and Solomon, who obtained a slightly stronger\nstatement over fields of characteristic zero.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:21:55 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Andrews", "Robert", ""]]}, {"id": "2005.11116", "submitter": "Christian Konrad", "authors": "Jacques Dark, Christian Konrad", "title": "Optimal Lower Bounds for Matching and Vertex Cover in Dynamic Graph\n  Streams", "comments": "to appear in CCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give simple optimal lower bounds on the one-way two-party\ncommunication complexity of approximate Maximum Matching and Minimum Vertex\nCover with deletions. In our model, Alice holds a set of edges and sends a\nsingle message to Bob. Bob holds a set of edge deletions, which form a subset\nof Alice's edges, and needs to report a large matching or a small vertex cover\nin the graph spanned by the edges that are not deleted. Our results imply\noptimal space lower bounds for insertion-deletion streaming algorithms for\nMaximum Matching and Minimum Vertex Cover.\n  Previously, Assadi et al. [SODA 2016] gave an optimal space lower bound for\ninsertion-deletion streaming algorithms for Maximum Matching via the\nsimultaneous model of communication. Our lower bound is simpler and stronger in\nseveral aspects: The lower bound of Assadi et al. only holds for algorithms\nthat (1) are able to process streams that contain a triple exponential number\nof deletions in $n$, the number of vertices of the input graph; (2) are able to\nprocess multi-graphs; and (3) never output edges that do not exist in the input\ngraph when the randomized algorithm errs. In contrast, our lower bound even\nholds for algorithms that (1) rely on short ($O(n^2)$-length) input streams;\n(2) are only able to process simple graphs; and (3) may output non-existing\nedges when the algorithm errs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 11:34:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Dark", "Jacques", ""], ["Konrad", "Christian", ""]]}, {"id": "2005.11270", "submitter": "Yunzi Ding", "authors": "Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "The Average-Case Time Complexity of Certifying the Restricted Isometry\n  Property", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, the restricted isometry property (RIP) on $M \\times N$\nsensing matrices (where $M < N$) guarantees efficient reconstruction of sparse\nvectors. A matrix has the $(s,\\delta)$-$\\mathsf{RIP}$ property if behaves as a\n$\\delta$-approximate isometry on $s$-sparse vectors. It is well known that an\n$M\\times N$ matrix with i.i.d. $\\mathcal{N}(0,1/M)$ entries is\n$(s,\\delta)$-$\\mathsf{RIP}$ with high probability as long as $s\\lesssim\n\\delta^2 M/\\log N$. On the other hand, most prior works aiming to\ndeterministically construct $(s,\\delta)$-$\\mathsf{RIP}$ matrices have failed\nwhen $s \\gg \\sqrt{M}$. An alternative way to find an RIP matrix could be to\ndraw a random gaussian matrix and certify that it is indeed RIP. However, there\nis evidence that this certification task is computationally hard when $s \\gg\n\\sqrt{M}$, both in the worst case and the average case.\n  In this paper, we investigate the exact average-case time complexity of\ncertifying the RIP property for $M\\times N$ matrices with i.i.d.\n$\\mathcal{N}(0,1/M)$ entries, in the \"possible but hard\" regime $\\sqrt{M} \\ll\ns\\lesssim M/\\log N$. Based on analysis of the low-degree likelihood ratio, we\ngive rigorous evidence that subexponential runtime $N^{\\tilde\\Omega(s^2/M)}$ is\nrequired, demonstrating a smooth tradeoff between the maximum tolerated\nsparsity and the required computational power. This lower bound is essentially\ntight, matching the runtime of an existing algorithm due to Koiran and Zouzias.\nOur hardness result allows $\\delta$ to take any constant value in $(0,1)$,\nwhich captures the relevant regime for compressed sensing. This improves upon\nthe existing average-case hardness result of Wang, Berthet, and Plan, which is\nlimited to $\\delta = o(1)$.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:55:01 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 15:44:33 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 16:00:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ding", "Yunzi", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "2005.11307", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "Algebraic Global Gadgetry for Surjective Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) on a finite relational structure B\nis to decide, given a set of constraints on variables where the relations come\nfrom B, whether or not there is a assignment to the variables satisfying all of\nthe constraints; the surjective CSP is the variant where one decides the\nexistence of a surjective satisfying assignment onto the universe of B.\n  We present an algebraic framework for proving hardness results on surjective\nCSPs; essentially, this framework computes global gadgetry that permits one to\npresent a reduction from a classical CSP to a surjective CSP. We show how to\nderive a number of hardness results for surjective CSP in this framework,\nincluding the hardness of the disconnected cut problem, of the no-rainbow\n3-coloring problem, and of the surjective CSP on all 2-element structures known\nto be intractable (in this setting). Our framework thus allows us to unify\nthese hardness results, and reveal common structure among them; we believe that\nour hardness proof for the disconnected cut problem is more succinct than the\noriginal. In our view, the framework also makes very transparent a way in which\nclassical CSPs can be reduced to surjective CSPs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:52:24 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:01:09 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 07:26:45 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 16:13:33 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "2005.11541", "submitter": "Marvin K\\\"unnemann", "authors": "Marvin K\\\"unnemann and D\\'aniel Marx", "title": "Finding Small Satisfying Assignments Faster Than Brute Force: A\n  Fine-grained Perspective into Boolean Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the question under which circumstances small solutions can be found\nfaster than by exhaustive search (and by how much), we study the fine-grained\ncomplexity of Boolean constraint satisfaction with size constraint exactly $k$.\nMore precisely, we aim to determine, for any finite constraint family, the\noptimal running time $f(k)n^{g(k)}$ required to find satisfying assignments\nthat set precisely $k$ of the $n$ variables to $1$.\n  Under central hardness assumptions on detecting cliques in graphs and\n3-uniform hypergraphs, we give an almost tight characterization of $g(k)$ into\nfour regimes: (1) Brute force is essentially best-possible, i.e., $g(k) = (1\\pm\no(1))k$, (2) the best algorithms are as fast as current $k$-clique algorithms,\ni.e., $g(k)=(\\omega/3\\pm o(1))k$, (3) the exponent has sublinear dependence on\n$k$ with $g(k) \\in [\\Omega(\\sqrt[3]{k}), O(\\sqrt{k})]$, or (4) the problem is\nfixed-parameter tractable, i.e., $g(k) = O(1)$.\n  This yields a more fine-grained perspective than a previous FPT/W[1]-hardness\ndichotomy (Marx, Computational Complexity 2005). Our most interesting technical\ncontribution is a $f(k)n^{4\\sqrt{k}}$-time algorithm for SubsetSum with\nprecedence constraints parameterized by the target $k$ -- particularly the\napproach, based on generalizing a bound on the Frobenius coin problem to a\nsetting with precedence constraints, might be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:05:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["K\u00fcnnemann", "Marvin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "2005.11589", "submitter": "Gaurav Sood", "authors": "Yuval Filmus, Meena Mahajan, Gaurav Sood, Marc Vinyals", "title": "MaxSAT Resolution and Subcube Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the MaxRes rule in the context of certifying unsatisfiability. We\nshow that it can be exponentially more powerful than tree-like resolution, and\nwhen augmented with weakening (the system MaxResW), p-simulates tree-like\nresolution. In devising a lower bound technique specific to MaxRes (and not\nmerely inheriting lower bounds from Res), we define a new proof system called\nthe SubCubeSums proof system. This system, which p-simulates MaxResW, can be\nviewed as a special case of the semialgebraic Sherali-Adams proof system. In\nexpressivity, it is the integral restriction of conical juntas studied in the\ncontexts of communication complexity and extension complexity. We show that it\nis not simulated by Res. Using a proof technique qualitatively different from\nthe lower bounds that MaxResW inherits from Res, we show that Tseitin\ncontradictions on expander graphs are hard to refute in SubCubeSums. We also\nestablish a lower bound technique via lifting: for formulas requiring large\ndegree in SubCubeSums, their XOR-ification requires large size in SubCubeSums.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 19:16:36 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 05:49:37 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Filmus", "Yuval", ""], ["Mahajan", "Meena", ""], ["Sood", "Gaurav", ""], ["Vinyals", "Marc", ""]]}, {"id": "2005.11654", "submitter": "Eldon Chung", "authors": "Divesh Aggarwal and Eldon Chung", "title": "A Note on the Concrete Hardness of the Shortest Independent Vectors\n  Problem in Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bl\\\"omer and Seifert showed that $\\mathsf{SIVP}_2$ is NP-hard to approximate\nby giving a reduction from $\\mathsf{CVP}_2$ to $\\mathsf{SIVP}_2$ for constant\napproximation factors as long as the $\\mathsf{CVP}$ instance has a certain\nproperty. In order to formally define this requirement on the $\\mathsf{CVP}$\ninstance, we introduce a new computational problem called the Gap Closest\nVector Problem with Bounded Minima. We adapt the proof of Bl\\\"omer and Seifert\nto show a reduction from the Gap Closest Vector Problem with Bounded Minima to\n$\\mathsf{SIVP}$ for any $\\ell_p$ norm for some constant approximation factor\ngreater than $1$.\n  In a recent result, Bennett, Golovnev and Stephens-Davidowitz showed that\nunder Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating\n$\\mathsf{CVP}_p$ up to some constant factor $\\gamma \\geq 1$ for any $1 \\leq p\n\\leq \\infty$. We observe that the reduction in their paper can be viewed as a\nreduction from $\\mathsf{Gap3SAT}$ to the Gap Closest Vector Problem with\nBounded Minima. This, together with the above mentioned reduction, implies\nthat, under Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating\n$\\mathsf{SIVP}_p$ up to some constant factor $\\gamma \\geq 1$ for any $1 \\leq p\n\\leq \\infty$.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 04:22:02 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 05:26:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Chung", "Eldon", ""]]}, {"id": "2005.11668", "submitter": "Amandeep Bhatia", "authors": "Amandeep Singh Bhatia, Ajay Kumar", "title": "Quadratic Sieve Factorization Quantum Algorithm and its Simulation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computing is a winsome field that concerns with the behaviour and\nnature of energy at the quantum level to improve the efficiency of\ncomputations. In recent years, quantum computation is receiving much attention\nfor its capability to solve difficult problems efficiently in contrast to\nclassical computers. Specifically, some well-known public-key cryptosystems\ndepend on the difficulty of factoring large numbers, which takes a very long\ntime. It is expected that the emergence of a quantum computer has the potential\nto break such cryptosystems by 2020 due to the discovery of powerful quantum\nalgorithms (Shor's factoring, Grover's search algorithm and many more). In this\npaper, we have designed a quantum variant of the second fastest classical\nfactorization algorithm named \"Quadratic Sieve\". We have constructed the\nsimulation framework of quantized quadratic sieve algorithm using high-level\nprogramming language Mathematica. Further, the simulation results are performed\non a classical computer to get a feel of the quantum system and proved that it\nis more efficient than its classical variants from computational complexity\npoint of view.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:14:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bhatia", "Amandeep Singh", ""], ["Kumar", "Ajay", ""]]}, {"id": "2005.11758", "submitter": "Pedro Montealegre", "authors": "Eric Goles, Pedro Montealegre, Mart\\'in R\\'ios-Wilson, Guillaume\n  Theyssier", "title": "On the impact of treewidth in the computational complexity of freezing\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automata network is a network of entities, each holding a state from a\nfinite set and evolving according to a local update rule which depends only on\nits neighbors in the network's graph. It is freezing if there is an order on\nstates such that the state evolution of any node is non-decreasing in any\norbit. They are commonly used to model epidemic propagation, diffusion\nphenomena like bootstrap percolation or cristal growth. In this paper we\nestablish how treewidth and maximum degree of the underlying graph are key\nparameters which influence the overall computational complexity of finite\nfreezing automata networks. First, we define a general model checking formalism\nthat captures many classical decision problems: prediction, nilpotency,\npredecessor, asynchronous reachability. Then, on one hand, we present an\nefficient parallel algorithm that solves the general model checking problem in\nNC for any graph with bounded degree and bounded treewidth. On the other hand,\nwe show that these problems are hard in their respective classes when\nrestricted to families of graph with polynomially growing treewidth. For\nprediction, predecessor and asynchronous reachability, we establish the\nhardness result with a fixed set-defiend update rule that is universally hard\non any input graph of such families.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 14:38:00 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 16:05:29 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Goles", "Eric", ""], ["Montealegre", "Pedro", ""], ["R\u00edos-Wilson", "Mart\u00edn", ""], ["Theyssier", "Guillaume", ""]]}, {"id": "2005.11766", "submitter": "Alexander Gavrilyuk", "authors": "Alexander L. Gavrilyuk, Roman Nedela, Ilia Ponomarenko", "title": "The Weisfeiler-Leman dimension of distance-hereditary graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is said to be distance-hereditary if the distance function in every\nconnected induced subgraph is the same as in the graph itself. We prove that\nthe ordinary Weisfeiler-Leman algorithm correctly tests the isomorphism of any\ntwo graphs if one of them is distance-hereditary; more precisely, the\nWeisfeiler-Leman dimension of the class of finite distance-hereditary graphs is\nequal to $2$. The previously best known upper bound for the dimension was $7$.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 14:59:39 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Gavrilyuk", "Alexander L.", ""], ["Nedela", "Roman", ""], ["Ponomarenko", "Ilia", ""]]}, {"id": "2005.11796", "submitter": "Themistoklis Melissourgos", "authors": "Argyrios Deligkas, Themistoklis Melissourgos, Paul G. Spirakis", "title": "Walrasian Equilibria in Markets with Small Demands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of finding a Walrasian equilibrium in markets where\nthe agents have $k$-demand valuations. These valuations are an extension of\nunit-demand valuations where a bundle's value is the maximum of its\n$k$-subsets' values. For unit-demand agents, where the existence of a Walrasian\nequilibrium is guaranteed, we show that the problem is in quasi-NC. For $k=2$,\nwe show that it is NP-hard to decide if a Walrasian equilibrium exists even if\nthe valuations are fractionally subadditive (XOS), while for $k=3$ the hardness\ncarries over to budget-additive valuations. In addition, we give a\npolynomial-time algorithm for markets with 2-demand single-minded valuations,\nor unit-demand valuations.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 16:39:29 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 01:38:04 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 16:11:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Melissourgos", "Themistoklis", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "2005.12064", "submitter": "Shivesh Kumar", "authors": "Felix Wiebe and Shivesh Kumar and Daniel Harnack and Malte Langosz and\n  Hendrik W\\\"ohrle and Frank Kirchner", "title": "Combinatorics of a Discrete Trajectory Space for Robot Motion Planning", "comments": "8 pages, 3 figures, to be published in the proceedings of 2nd IMA\n  Conference on Mathematics of Robotics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion planning is a difficult problem in robot control. The complexity of\nthe problem is directly related to the dimension of the robot's configuration\nspace. While in many theoretical calculations and practical applications the\nconfiguration space is modeled as a continuous space, we present a discrete\nrobot model based on the fundamental hardware specifications of a robot. Using\nlattice path methods, we provide estimates for the complexity of motion\nplanning by counting the number of possible trajectories in a discrete robot\nconfiguration space.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:14:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wiebe", "Felix", ""], ["Kumar", "Shivesh", ""], ["Harnack", "Daniel", ""], ["Langosz", "Malte", ""], ["W\u00f6hrle", "Hendrik", ""], ["Kirchner", "Frank", ""]]}, {"id": "2005.12169", "submitter": "Stephen A. Fenner", "authors": "Daniel Pad\\'e (University of South Carolina), Stephen Fenner\n  (University of South Carolina), Daniel Grier (IQC), Thomas Thierauf (Aalen\n  University)", "title": "Depth-2 QAC circuits cannot simulate quantum parity", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the quantum parity gate on $n > 3$ qubits cannot be cleanly\nsimulated by a quantum circuit with two layers of arbitrary C-SIGN gates of any\narity and arbitrary 1-qubit unitary gates, regardless of the number of allowed\nancilla qubits. This is the best known and first nontrivial separation between\nthe parity gate and circuits of this form. The same bounds also apply to the\nquantum fanout gate. Our results are incomparable with those of Fang et al.\n[3], which apply to any constant depth but require a sublinear number of\nancilla qubits on the simulating circuit.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:32:16 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Pad\u00e9", "Daniel", "", "University of South Carolina"], ["Fenner", "Stephen", "", "University of South Carolina"], ["Grier", "Daniel", "", "IQC"], ["Thierauf", "Thomas", "", "Aalen\n  University"]]}, {"id": "2005.13634", "submitter": "Fabio Henrique Viduani Martinez", "authors": "Diego P Rubert, Eloi Araujo, Marco A Stefanes, Jens Stoye, F\\'abio V\n  Martinez", "title": "On motifs in colored graphs", "comments": "28 pages, 9 figures, to be published in Journal of Combinatorial\n  Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important concepts in biological network analysis is that of\nnetwork motifs, which are patterns of interconnections that occur in a given\nnetwork at a frequency higher than expected in a random network. In this work\nwe are interested in searching and inferring network motifs in a class of\nbiological networks that can be represented by vertex-colored graphs. We show\nthe computational complexity for many problems related to colorful topological\nmotifs and present efficient algorithms for special cases. We also present a\nprobabilistic strategy to detect highly frequent motifs in vertex-colored\ngraphs. Experiments on real data sets show that our algorithms are very\ncompetitive both in efficiency and in quality of the solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:27:44 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Rubert", "Diego P", ""], ["Araujo", "Eloi", ""], ["Stefanes", "Marco A", ""], ["Stoye", "Jens", ""], ["Martinez", "F\u00e1bio V", ""]]}, {"id": "2005.13710", "submitter": "Fabian Frei", "authors": "Elisabet Burjons and Fabian Frei and Martin Raszyk", "title": "From Finite-Valued Nondeterministic Transducers to Deterministic\n  Two-Tape Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question whether P equals NP revolves around the discrepancy between\nactive production and mere verification by Turing machines. In this paper, we\nexamine the analogous problem for finite transducers and automata. Every\nnondeterministic finite transducer defines a binary relation associating each\ninput word with all output words that the transducer can successfully produce\non the given input. Finite-valued transducers are those for which there is a\nfinite upper bound on the number of output words that the relation associates\nwith every input word. We characterize finite-valued, functional, and\nunambiguous nondeterministic transducers whose relations can be verified by a\ndeterministic two-tape automaton, show how to construct such an automaton if\none exists, and prove the undecidability of the criterion.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:38:29 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 14:25:22 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 15:08:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Burjons", "Elisabet", ""], ["Frei", "Fabian", ""], ["Raszyk", "Martin", ""]]}, {"id": "2005.13913", "submitter": "Sangram Kishor Jena Mr", "authors": "Ramesh K. Jallu, Sangram K. Jena and Gautam K. Das", "title": "Liar's Domination in Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a variant of the minimum dominating set problem\nknown as the minimum liar's dominating set (MLDS) problem. We prove that the\nMLDS problem is NP-hard in unit disk graphs. Next, we show that the recent\nsub-quadratic time $\\frac{11}{2}$-factor approximation algorithm \\cite{bhore}\nfor the MLDS problem is erroneous and propose a simple $O(n + m)$ time\n7.31-factor approximation algorithm, where $n$ and $m$ are the number of\nvertices and edges in the input unit disk graph, respectively. Finally, we\nprove that the MLDS problem admits a polynomial-time approximation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:27:40 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Jallu", "Ramesh K.", ""], ["Jena", "Sangram K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2005.14105", "submitter": "Christoph Hertrich", "authors": "Christoph Hertrich and Martin Skutella", "title": "Provably Good Solutions to the Knapsack Problem via Neural Networks of\n  Bounded Size", "comments": "A short version of this paper appears in the proceedings of AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a satisfying and rigorous mathematical understanding of\nthe performance of neural networks is a major challenge in artificial\nintelligence. Against this background, we study the expressive power of neural\nnetworks through the example of the classical NP-hard Knapsack Problem. Our\nmain contribution is a class of recurrent neural networks (RNNs) with rectified\nlinear units that are iteratively applied to each item of a Knapsack instance\nand thereby compute optimal or provably good solution values. We show that an\nRNN of depth four and width depending quadratically on the profit of an optimum\nKnapsack solution is sufficient to find optimum Knapsack solutions. We also\nprove the following tradeoff between the size of an RNN and the quality of the\ncomputed Knapsack solution: for Knapsack instances consisting of $n$ items, an\nRNN of depth five and width $w$ computes a solution of value at least\n$1-\\mathcal{O}(n^2/\\sqrt{w})$ times the optimum solution value. Our results\nbuild upon a classical dynamic programming formulation of the Knapsack Problem\nas well as a careful rounding of profit values that are also at the core of the\nwell-known fully polynomial-time approximation scheme for the Knapsack Problem.\nA carefully conducted computational study qualitatively supports our\ntheoretical size bounds. Finally, we point out that our results can be\ngeneralized to many other combinatorial optimization problems that admit\ndynamic programming solution methods, such as various Shortest Path Problems,\nthe Longest Common Subsequence Problem, and the Traveling Salesperson Problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:55:37 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 10:26:12 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hertrich", "Christoph", ""], ["Skutella", "Martin", ""]]}]