[{"id": "1107.0056", "submitter": "Rudini Sampaio", "authors": "Victor Campos, Cl\\'audia Linhares-Sales, Ana Karolinna Maia, Nicolas\n  Martins, Rudini Menezes Sampaio", "title": "Fixed parameter algorithms for restricted coloring problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain polynomial time algorithms to determine the acyclic\nchromatic number, the star chromatic number, the Thue chromatic number, the\nharmonious chromatic number and the clique chromatic number of $P_4$-tidy\ngraphs and $(q,q-4)$-graphs, for every fixed $q$. These classes include\ncographs, $P_4$-sparse and $P_4$-lite graphs. All these coloring problems are\nknown to be NP-hard for general graphs. These algorithms are fixed parameter\ntractable on the parameter $q(G)$, which is the minimum $q$ such that $G$ is a\n$(q,q-4)$-graph. We also prove that every connected $(q,q-4)$-graph with at\nleast $q$ vertices is 2-clique-colorable and that every acyclic coloring of a\ncograph is also nonrepetitive.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:45:34 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 14:20:44 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Campos", "Victor", ""], ["Linhares-Sales", "Cl\u00e1udia", ""], ["Maia", "Ana Karolinna", ""], ["Martins", "Nicolas", ""], ["Sampaio", "Rudini Menezes", ""]]}, {"id": "1107.0098", "submitter": "Alexander Davydov", "authors": "Alexander Y. Davydov", "title": "A Probabilistic Attack on NP-complete Problems", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the probability theory-based approach, this paper reveals the\nequivalence of an arbitrary NP-complete problem to a problem of checking\nwhether a level set of a specifically constructed harmonic cost function (with\nall diagonal entries of its Hessian matrix equal to zero) intersects with a\nunit hypercube in many-dimensional Euclidean space. This connection suggests\nthe possibility that methods of continuous mathematics can provide crucial\ninsights into the most intriguing open questions in modern complexity theory.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 03:43:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2011 01:57:22 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2012 21:14:17 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Davydov", "Alexander Y.", ""]]}, {"id": "1107.0336", "submitter": "Hugues Randriam", "authors": "Hugues Randriambololona", "title": "Bilinear complexity of algebras and the Chudnovsky-Chudnovsky\n  interpolation method", "comments": "40 pages; difference with previous version: modified Lemma 5.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new improvements to the Chudnovsky-Chudnovsky method that provides\nupper bounds on the bilinear complexity of multiplication in extensions of\nfinite fields through interpolation on algebraic curves. Our approach features\nthree independent key ingredients:\n  (1) We allow asymmetry in the interpolation procedure. This allows to prove,\nvia the usual cardinality argument, the existence of auxiliary divisors needed\nfor the bounds, up to optimal degree.\n  (2) We give an alternative proof for the existence of these auxiliary\ndivisors, which is constructive, and works also in the symmetric case, although\nit requires the curves to have sufficiently many points.\n  (3) We allow the method to deal not only with extensions of finite fields,\nbut more generally with monogenous algebras over finite fields. This leads to\nsharper bounds, and is designed also to combine well with base field descent\narguments in case the curves do not have sufficiently many points.\n  As a main application of these techniques, we fix errors in, improve, and\ngeneralize, previous works of Shparlinski-Tsfasman-Vladut, Ballet, and\nCenk-Ozbudak. Besides, generalities on interpolation systems, as well as on\nsymmetric and asymmetric bilinear complexity, are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 20:57:31 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2011 15:09:32 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2011 15:57:55 GMT"}, {"version": "v4", "created": "Fri, 13 Jan 2012 19:05:45 GMT"}, {"version": "v5", "created": "Fri, 16 Mar 2012 13:47:09 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Randriambololona", "Hugues", ""]]}, {"id": "1107.0634", "submitter": "Christian Reitwiessner", "authors": "Christian Gla{\\ss}er, Christian Reitwie{\\ss}ner, Maximilian Witek", "title": "Applications of Discrepancy Theory in Multiobjective Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a multi-color extension of the Beck-Fiala theorem to show that the\nmultiobjective maximum traveling salesman problem is randomized\n1/2-approximable on directed graphs and randomized 2/3-approximable on\nundirected graphs. Using the same technique we show that the multiobjective\nmaximum satisfiablilty problem is 1/2-approximable.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 14:22:07 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Gla\u00dfer", "Christian", ""], ["Reitwie\u00dfner", "Christian", ""], ["Witek", "Maximilian", ""]]}, {"id": "1107.0919", "submitter": "Markus Lohrey", "authors": "Stefan G\\\"oller (University of Bremen), Markus Lohrey (University of\n  Leipzig)", "title": "The First-Order Theory of Ground Tree Rewrite Graphs", "comments": "accepted for Logical Methods in Computer Science", "journal-ref": "Logical Methods in Computer Science, Volume 10, Issue 1 (February\n  12, 2014) lmcs:1223", "doi": "10.2168/LMCS-10(1:7)2014", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the complexity of the uniform first-order theory of ground tree\nrewrite graphs is in ATIME(2^{2^{poly(n)}},O(n)). Providing a matching lower\nbound, we show that there is some fixed ground tree rewrite graph whose\nfirst-order theory is hard for ATIME(2^{2^{poly(n)}},poly(n)) with respect to\nlogspace reductions. Finally, we prove that there exists a fixed ground tree\nrewrite graph together with a single unary predicate in form of a regular tree\nlanguage such that the resulting structure has a non-elementary first-order\ntheory.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 16:32:12 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2011 22:30:54 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2014 09:22:16 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2014 10:39:12 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["G\u00f6ller", "Stefan", "", "University of Bremen"], ["Lohrey", "Markus", "", "University of\n  Leipzig"]]}, {"id": "1107.1052", "submitter": "Ren\\'e Sitters", "authors": "Sylvia Boyd, Ren\\'e Sitters, Suzanne van der Ster, and Leen Stougie", "title": "The traveling salesman problem on cubic and subcubic graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Travelling Salesman Problem (TSP) on the metric completion of\ncubic and subcubic graphs, which is known to be NP-hard. The problem is of\ninterest because of its relation to the famous 4/3 conjecture for metric TSP,\nwhich says that the integrality gap, i.e., the worst case ratio between the\noptimal values of the TSP and its linear programming relaxation (the subtour\nelimination relaxation), is 4/3. We present the first algorithm for cubic\ngraphs with approximation ratio 4/3. The proof uses polyhedral techniques in a\nsurprising way, which is of independent interest. In fact we prove\nconstructively that for any cubic graph on $n$ vertices a tour of length 4n/3-2\nexists, which also implies the 4/3 conjecture, as an upper bound, for this\nclass of graph-TSP.\n  Recently, M\\\"omke and Svensson presented a randomized algorithm that gives a\n1.461-approximation for graph-TSP on general graphs and as a side result a\n4/3-approximation algorithm for this problem on subcubic graphs, also settling\nthe 4/3 conjecture for this class of graph-TSP. We will present a way to\nderandomize their algorithm which leads to a smaller running time than the\nobvious derandomization. All of the latter also works for multi-graphs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 08:14:51 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Boyd", "Sylvia", ""], ["Sitters", "Ren\u00e9", ""], ["van der Ster", "Suzanne", ""], ["Stougie", "Leen", ""]]}, {"id": "1107.1358", "submitter": "Omri Weinstein", "authors": "Zohar Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz, Omri\n  Weinstein", "title": "On the Furthest Hyperplane Problem and Maximal Margin Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Furthest Hyperplane Problem (FHP), which is an\nunsupervised counterpart of Support Vector Machines. Given a set of n points in\nRd, the objective is to produce the hyperplane (passing through the origin)\nwhich maximizes the separation margin, that is, the minimal distance between\nthe hyperplane and any input point. To the best of our knowledge, this is the\nfirst paper achieving provable results regarding FHP. We provide both lower and\nupper bounds to this NP-hard problem. First, we give a simple randomized\nalgorithm whose running time is n^O(1/{\\theta}^2) where {\\theta} is the optimal\nseparation margin. We show that its exponential dependency on 1/{\\theta}^2 is\ntight, up to sub-polynomial factors, assuming SAT cannot be solved in\nsub-exponential time. Next, we give an efficient approxima- tion algorithm. For\nany {\\alpha} \\in [0, 1], the algorithm produces a hyperplane whose distance\nfrom at least 1 - 5{\\alpha} fraction of the points is at least {\\alpha} times\nthe optimal separation margin. Finally, we show that FHP does not admit a PTAS\nby presenting a gap preserving reduction from a particular version of the PCP\ntheorem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 11:58:52 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2012 21:40:04 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Karnin", "Zohar", ""], ["Liberty", "Edo", ""], ["Lovett", "Shachar", ""], ["Schwartz", "Roy", ""], ["Weinstein", "Omri", ""]]}, {"id": "1107.1434", "submitter": "Bruno Grenet", "authors": "Bruno Grenet, Pascal Koiran, Natacha Portier, Yann Strozecki", "title": "The Limited Power of Powering: Polynomial Identity Testing and a\n  Depth-four Lower Bound for the Permanent", "comments": "16 pages", "journal-ref": "IARCS Annual Conference on Foundations of Software Technology and\n  Theoretical Computer Science (FSTTCS'11), Mumbai : India (2011)", "doi": "10.4230/LIPIcs.FSTTCS.2011.127", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial identity testing and arithmetic circuit lower bounds are two\ncentral questions in algebraic complexity theory. It is an intriguing fact that\nthese questions are actually related. One of the authors of the present paper\nhas recently proposed a \"real {\\tau}-conjecture\" which is inspired by this\nconnection. The real {\\tau}-conjecture states that the number of real roots of\na sum of products of sparse univariate polynomials should be polynomially\nbounded. It implies a superpolynomial lower bound on the size of arithmetic\ncircuits computing the permanent polynomial. In this paper we show that the\nreal {\\tau}-conjecture holds true for a restricted class of sums of products of\nsparse polynomials. This result yields lower bounds for a restricted class of\ndepth-4 circuits: we show that polynomial size circuits from this class cannot\ncompute the permanent, and we also give a deterministic polynomial identity\ntesting algorithm for the same class of circuits.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 15:48:17 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Grenet", "Bruno", ""], ["Koiran", "Pascal", ""], ["Portier", "Natacha", ""], ["Strozecki", "Yann", ""]]}, {"id": "1107.1458", "submitter": "Leonid A. Levin", "authors": "Samuel Epstein, Leonid A. Levin", "title": "Sets Have Simple Members", "comments": "This paper has been withdrawn by the authors. Withdrawn (v8) by S.\n  Epstein. Reposted as arXiv:1403.4539 by the coauthor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combined Universal Probability M(D) of strings x in sets D is close to\nmax M({x}) over x in D: their ~logs differ by at most D's information j=I(D:H)\nabout the halting sequence H. Thus if all x have complexity K(x) >k, D carries\n>i bits of information on each its x where i+j ~ k. Note that there are no ways\nto generate D with significant I(D:H).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 17:19:53 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2011 18:34:08 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2011 19:45:00 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2011 19:51:53 GMT"}, {"version": "v5", "created": "Tue, 20 Sep 2011 17:27:29 GMT"}, {"version": "v6", "created": "Tue, 3 Jul 2012 16:00:43 GMT"}, {"version": "v7", "created": "Mon, 31 Dec 2012 20:05:18 GMT"}, {"version": "v8", "created": "Fri, 14 Feb 2014 11:48:28 GMT"}, {"version": "v9", "created": "Thu, 20 Mar 2014 18:39:47 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Epstein", "Samuel", ""], ["Levin", "Leonid A.", ""]]}, {"id": "1107.1940", "submitter": "David Meyer", "authors": "David A. Meyer (1) and James Pommersheim (1 and 2) ((1) Mathematics\n  Department, UCSD, (2) Mathematics Department, Reed)", "title": "Multi-query quantum sums", "comments": "11 pages, 1 figure; presented at TQC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PARITY is the problem of determining the parity of a string $f$ of $n$ bits\ngiven access to an oracle that responds to a query $x\\in\\{0,1,...,n-1\\}$ with\nthe $x^{\\rm th}$ bit of the string, $f(x)$. Classically, $n$ queries are\nrequired to succeed with probability greater than 1/2 (assuming equal prior\nprobabilities for all length $n$ bitstrings), but only $\\lceil n/2\\rceil$\nquantum queries suffice to determine the parity with probability 1. We consider\na generalization to strings $f$ of $n$ elements of $\\Z_k$ and the problem of\ndetermining $\\sum f(x)$. By constructing an explicit algorithm, we show that\n$n-r$ ($n\\ge r\\in\\N$) entangled quantum queries suffice to compute the sum\ncorrectly with worst case probability $\\min\\{\\lfloor n/r\\rfloor/k,1\\}$. This\nquantum algorithm utilizes the $n-r$ queries sequentially and adaptively, like\nGrover's algorithm, but in a different way that is not amplitude amplification.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 05:55:09 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Meyer", "David A.", "", "1 and 2"], ["Pommersheim", "James", "", "1 and 2"]]}, {"id": "1107.1963", "submitter": "Felix Weiss", "authors": "Martin Mundhenk, Felix Weiss", "title": "Intuitionistic implication makes model checking hard", "comments": "29 pages, 10 figures", "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 2 (April 27,\n  2012) lmcs:1160", "doi": "10.2168/LMCS-8(2:3)2012", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of the model checking problem for\nintuitionistic and modal propositional logics over transitive Kripke models.\nMore specific, we consider intuitionistic logic IPC, basic propositional logic\nBPL, formal propositional logic FPL, and Jankov's logic KC. We show that the\nmodel checking problem is P-complete for the implicational fragments of all\nthese intuitionistic logics. For BPL and FPL we reach P-hardness even on the\nimplicational fragment with only one variable. The same hardness results are\nobtained for the strictly implicational fragments of their modal companions.\nMoreover, we investigate whether formulas with less variables and additional\nconnectives make model checking easier. Whereas for variable free formulas\noutside of the implicational fragment, FPL model checking is shown to be in\nLOGCFL, the problem remains P-complete for BPL.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 08:41:55 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 14:52:04 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2012 06:18:46 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Mundhenk", "Martin", ""], ["Weiss", "Felix", ""]]}, {"id": "1107.2001", "submitter": "Marc Thurley", "authors": "Marc Thurley", "title": "An Approximation Algorithm for #k-SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple randomized algorithm that approximates the number of\nsatisfying assignments of Boolean formulas in conjunctive normal form. To the\nbest of our knowledge this is the first algorithm which approximates #k-SAT for\nany k >= 3 within a running time that is not only non-trivial, but also\nsignificantly better than that of the currently fastest exact algorithms for\nthe problem. More precisely, our algorithm is a randomized approximation scheme\nwhose running time depends polynomially on the error tolerance and is mildly\nexponential in the number n of variables of the input formula. For example,\neven stipulating sub-exponentially small error tolerance, the number of\nsolutions to 3-CNF input formulas can be approximated in time O(1.5366^n). For\n4-CNF input the bound increases to O(1.6155^n).\n  We further show how to obtain upper and lower bounds on the number of\nsolutions to a CNF formula in a controllable way. Relaxing the requirements on\nthe quality of the approximation, on k-CNF input we obtain significantly\nreduced running times in comparison to the above bounds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 11:27:34 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Thurley", "Marc", ""]]}, {"id": "1107.2183", "submitter": "Anindya De", "authors": "Anindya De", "title": "Lower bounds in differential privacy", "comments": "Corrected some minor errors and typos. To appear in Theory of\n  Cryptography Conference (TCC) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a paper about private data analysis, in which a trusted curator\nholding a confidential database responds to real vector-valued queries. A\ncommon approach to ensuring privacy for the database elements is to add\nappropriately generated random noise to the answers, releasing only these {\\em\nnoisy} responses. In this paper, we investigate various lower bounds on the\nnoise required to maintain different kind of privacy guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 02:51:42 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2011 23:59:15 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["De", "Anindya", ""]]}, {"id": "1107.2256", "submitter": "Erik Jan van Leeuwen", "authors": "Josep Diaz and Olli Pottonen and Maria Serna and Erik Jan van Leeuwen", "title": "Complexity of Metric Dimension on Planar Graphs", "comments": "v5: minor modifications. to appear in JCSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metric dimension of a graph $G$ is the size of a smallest subset $L\n\\subseteq V(G)$ such that for any $x,y \\in V(G)$ with $x\\not= y$ there is a $z\n\\in L$ such that the graph distance between $x$ and $z$ differs from the graph\ndistance between $y$ and $z$. Even though this notion has been part of the\nliterature for almost 40 years, prior to our work the computational complexity\nof determining the metric dimension of a graph was still very unclear. In this\npaper, we show tight complexity boundaries for the Metric Dimension problem. We\nachieve this by giving two complementary results. First, we show that the\nMetric Dimension problem on planar graphs of maximum degree $6$ is NP-complete.\nThen, we give a polynomial-time algorithm for determining the metric dimension\nof outerplanar graphs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 12:07:33 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2012 17:00:40 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2012 09:55:50 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 09:14:48 GMT"}, {"version": "v5", "created": "Tue, 12 Jul 2016 16:39:44 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Diaz", "Josep", ""], ["Pottonen", "Olli", ""], ["Serna", "Maria", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1107.2321", "submitter": "Jean-Fran\\c{c}ois Biasse", "authors": "Jean-Fran\\c{c}ois Biasse and Guillaume Quintin", "title": "An algorithm for list decoding number field codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for list decoding codewords of algebraic number field\ncodes in polynomial time. This is the first explicit procedure for decoding\nnumber field codes whose construction were previously described by Lenstra and\nGuruswami. We rely on an equivalent of the LLL reduction algorithm for\n$\\OK$-modules due to Fieker and Stehl\\'e and on algorithms due to Cohen for\ncomputing the Hermite normal form of matrices representing modules over\nDedekind domains.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 15:19:54 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 17:16:43 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Biasse", "Jean-Fran\u00e7ois", ""], ["Quintin", "Guillaume", ""]]}, {"id": "1107.2368", "submitter": "Piyush Srivastava", "authors": "Alistair Sinclair and Piyush Srivastava and Marc Thurley", "title": "Approximation algorithms for two-state anti-ferromagnetic spin systems\n  on bounded degree graphs", "comments": "1 figure. Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper (Weitz, 2006), Weitz gave a deterministic fully polynomial\napproximation scheme for count- ing exponentially weighted independent sets\n(equivalently, approximating the partition function of the hard-core model from\nstatistical physics) on graphs of degree at most d, up to the critical activity\nfor the uniqueness of the Gibbs measure on the infinite d-regular tree. More\nrecently Sly (Sly, 2010) showed that this is optimal in the sense that if there\nis an FPRAS for the hard-core partition function on graphs of maximum degree d\nfor activities larger than the critical activity on the infinite d-regular tree\nthen NP = RP. In this paper, we extend Weitz's approach to derive a\ndeterministic fully polynomial approx- imation scheme for the partition\nfunction of the anti-ferromagnetic Ising model with arbitrary field on graphs\nof maximum degree d, up to the corresponding critical point on the d-regular\ntree. The main ingredient of our result is a proof that for two-state\nanti-ferromagnetic spin systems on the d-regular tree, weak spatial mixing\nimplies strong spatial mixing. This in turn uses a message-decay argument which\nextends a similar approach proposed recently for the hard-core model by\nRestrepo et al (Restrepo et al, 2011) to the case of the anti-ferromagnetic\nIsing model with arbitrary field. By a standard correspondence, these results\ntranslate to arbitrary two-state anti-ferromagnetic spin systems with soft\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 18:42:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2011 18:10:05 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2011 05:54:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""], ["Thurley", "Marc", ""]]}, {"id": "1107.2444", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Guy N. Rothblum and Rocco A. Servedio", "title": "Private Data Release via Learning Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers computationally efficient privacy-preserving data\nrelease. We study the task of analyzing a database containing sensitive\ninformation about individual participants. Given a set of statistical queries\non the data, we want to release approximate answers to the queries while also\nguaranteeing differential privacy---protecting each participant's sensitive\ndata.\n  Our focus is on computationally efficient data release algorithms; we seek\nalgorithms whose running time is polynomial, or at least sub-exponential, in\nthe data dimensionality. Our primary contribution is a computationally\nefficient reduction from differentially private data release for a class of\ncounting queries, to learning thresholded sums of predicates from a related\nclass.\n  We instantiate this general reduction with a variety of algorithms for\nlearning thresholds. These instantiations yield several new results for\ndifferentially private data release. As two examples, taking {0,1}^d to be the\ndata domain (of dimension d), we obtain differentially private algorithms for:\n  (*) Releasing all k-way conjunctions. For any given k, the resulting data\nrelease algorithm has bounded error as long as the database is of size at least\nd^{O(\\sqrt{k\\log(k\\log d)})}. The running time is polynomial in the database\nsize.\n  (*) Releasing a (1-\\gamma)-fraction of all parity queries. For any \\gamma\n\\geq \\poly(1/d), the algorithm has bounded error as long as the database is of\nsize at least \\poly(d). The running time is polynomial in the database size.\n  Several other instantiations yield further results for privacy-preserving\ndata release. Of the two results highlighted above, the first learning\nalgorithm uses techniques for representing thresholded sums of predicates as\nlow-degree polynomial threshold functions. The second learning algorithm is\nbased on Jackson's Harmonic Sieve algorithm [Jackson 1997].\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 00:53:23 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Hardt", "Moritz", ""], ["Rothblum", "Guy N.", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1107.2559", "submitter": "Qin Zhang", "authors": "Jeff M. Phillips and Elad Verbin and Qin Zhang", "title": "Lower Bounds for Number-in-Hand Multiparty Communication Complexity,\n  Made Easy", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove lower bounds on randomized multiparty communication\ncomplexity, both in the \\emph{blackboard model} (where each message is written\non a blackboard for all players to see) and (mainly) in the\n\\emph{message-passing model}, where messages are sent player-to-player. We\nintroduce a new technique for proving such bounds, called\n\\emph{symmetrization}, which is natural, intuitive, and often easy to use.\n  For example, for the problem where each of $k$ players gets a bit-vector of\nlength $n$, and the goal is to compute the coordinate-wise XOR of these\nvectors, we prove a tight lower bounds of $\\Omega(nk)$ in the blackboard model.\nFor the same problem with AND instead of XOR, we prove a lower bounds of\nroughly $\\Omega(nk)$ in the message-passing model (assuming $k \\le n/3200$) and\n$\\Omega(n \\log k)$ in the blackboard model. We also prove lower bounds for\nbit-wise majority, for a graph-connectivity problem, and for other problems;\nthe technique seems applicable to a wide range of other problems as well. All\nof our lower bounds allow randomized communication protocols with two-sided\nerror.\n  We also use the symmetrization technique to prove several direct-sum-like\nresults for multiparty communication.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 14:29:18 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 16:53:22 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Verbin", "Elad", ""], ["Zhang", "Qin", ""]]}, {"id": "1107.2722", "submitter": "Luke Mathieson", "authors": "Arnaud Casteigts, Bernard Mans and Luke Mathieson", "title": "On the Feasibility of Maintenance Algorithms in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near ubiquitous mobile computing has led to intense interest in dynamic graph\ntheory. This provides a new and challenging setting for algorithmics and\ncomplexity theory. For any graph-based problem, the rapid evolution of a\n(possibly disconnected) graph over time naturally leads to the important\ncomplexity question: is it better to calculate a new solution from scratch or\nto adapt the known solution on the prior graph to quickly provide a solution of\nguaranteed quality for the changed graph?\n  In this paper, we demonstrate that the former is the best approach in some\ncases, but that there are cases where the latter is feasible. We prove that,\nunder certain conditions, hard problems cannot even be approximated in any\nreasonable complexity bound --- i.e., even with a large amount of time, having\na solution to a very similar graph does not help in computing a solution to the\ncurrent graph. To achieve this, we formalize the idea as a maintenance\nalgorithm. Using r-Regular Subgraph as the primary example we show that\nW[1]-hardness for the parameterized approximation problem implies the\nnon-existence of a maintenance algorithm for the given approximation ratio.\nConversely we show that Vertex Cover, which is fixed-parameter tractable, has a\n2-approximate maintenance algorithm. The implications of NP-hardness and\nNPO-hardness are also explored.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 03:23:38 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2011 02:03:08 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2012 00:36:29 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2012 06:33:36 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Mans", "Bernard", ""], ["Mathieson", "Luke", ""]]}, {"id": "1107.3090", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Michael L. Littman, David Barber", "title": "On the Computational Complexity of Stochastic Controller Optimization in\n  POMDPs", "comments": "Corrected error in the proof of Theorem 2, and revised Section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of finding an optimal stochastic 'blind' controller\nin a Markov decision process is an NP-hard problem. The corresponding decision\nproblem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP would\nimply breakthroughs in long-standing open problems in computer science. Our\nresult establishes that the more general problem of stochastic controller\noptimization in POMDPs is also NP-hard. Nonetheless, we outline a special case\nthat is convex and admits efficient global solutions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 15:33:15 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 13:54:42 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Vlassis", "Nikos", ""], ["Littman", "Michael L.", ""], ["Barber", "David", ""]]}, {"id": "1107.3127", "submitter": "Ramamohan Paturi", "authors": "Russell Impagliazzo, William Matthews, Ramamohan Paturi", "title": "A Satisfiability Algorithm for AC$^0$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently enumerating the satisfying assignments\nto $\\AC^0$ circuits. We give a zero-error randomized algorithm which takes an\n$\\AC^0$ circuit as input and constructs a set of restrictions which partition\n$\\{0,1\\}^n$ so that under each restriction the value of the circuit is\nconstant. Let $d$ denote the depth of the circuit and $cn$ denote the number of\ngates. This algorithm runs in time $|C| 2^{n(1-\\mu_{c.d})}$ where $|C|$ is the\nsize of the circuit for $\\mu_{c,d} \\ge 1/\\bigO[\\lg c + d \\lg d]^{d-1}$ with\nprobability at least $1-2^{-n}$.\n  As a result, we get improved exponential time algorithms for $\\AC^0$ circuit\nsatisfiability and for counting solutions. In addition, we get an improved\nbound on the correlation of $\\AC^0$ circuits with parity.\n  As an important component of our analysis, we extend the H{\\aa}stad Switching\nLemma to handle multiple $\\kcnf$s and $\\kdnf$s.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 18:37:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Impagliazzo", "Russell", ""], ["Matthews", "William", ""], ["Paturi", "Ramamohan", ""]]}, {"id": "1107.3226", "submitter": "HuiMin Zheng", "authors": "Huimin Zheng, HaiXing Hu, Nan Wu and Fangmin Song", "title": "On Measurement and Computation", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Inspired by the work of Feynman, Deutsch, We formally propose the theory of\nphysical computability and accordingly, the physical complexity theory. To\nachieve this, a framework that can evaluate almost all forms of computation\nusing various physical mechanisms is discussed. Here, we focus on using it to\nreview the theory of Quantum Computation. As a preliminary study on more\ngeneral problems, some examples of other physical mechanism are also given in\nthis paper.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2011 12:52:20 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2011 14:04:23 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Zheng", "Huimin", ""], ["Hu", "HaiXing", ""], ["Wu", "Nan", ""], ["Song", "Fangmin", ""]]}, {"id": "1107.3271", "submitter": "Vikram Dhillon", "authors": "Vikram Dhillon", "title": "On the Simulation of Adaptive Measurements via Postselection", "comments": "3 pgs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this note we address the question of whether any any quantum computational\nmodel that allows adaptive measurements can be simulated by a model that allows\npostselected measurements. We argue in the favor of this question and prove\nthat adaptive measurements can be simulated by postselection. We also discuss\nsome potentially stunning consequences of this result such as the ability to\nsolve #P problems.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2011 02:49:01 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Dhillon", "Vikram", ""]]}, {"id": "1107.3746", "submitter": "Kohtaro Tadaki", "authors": "Kohtaro Tadaki", "title": "A Computational Complexity-Theoretic Elaboration of Weak Truth-Table\n  Reducibility", "comments": "25 pages, LaTeX2e, no figures", "journal-ref": "In: Dinneen M.J., Khoussainov B., Nies A. (eds) Computation,\n  Physics and Beyond. WTCS 2012. LNCS, vol 7160. Springer (2012)", "doi": "10.1007/978-3-642-27654-5_16", "report-no": null, "categories": "math.LO cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of weak truth-table reducibility plays an important role in\nrecursion theory. In this paper, we introduce an elaboration of this notion,\nwhere a computable bound on the use function is explicitly specified. This\nelaboration enables us to deal with the notion of asymptotic behavior in a\nmanner like in computational complexity theory, while staying in computability\ntheory. We apply the elaboration to sets which appear in the statistical\nmechanical interpretation of algorithmic information theory. We demonstrate the\npower of the elaboration by revealing a critical phenomenon, i.e., a phase\ntransition, in the statistical mechanical interpretation, which cannot be\ncaptured by the original notion of weak truth-table reducibility.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 15:36:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tadaki", "Kohtaro", ""]]}, {"id": "1107.3767", "submitter": "Andrea Jimenez", "authors": "Andrea Jim\\'enez, Marcos Kiwi", "title": "Computational Hardness of Enumerating Satisfying Spin-Assignments in\n  Triangulations", "comments": "20 pages,25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisfying spin-assignments in triangulations of a surface are states of\nminimum energy of the antiferromagnetic Ising model on triangulations which\ncorrespond (via geometric duality) to perfect matchings in cubic bridgeless\ngraphs. In this work we show that it is NP-complete to decide whether or not a\nsurface triangulation admits a satisfying spin-assignment, and that it is\n#P-complete to determine the number of such assignments. Both results are\nderived via an elaborate (and atypical) reduction that maps a Boolean formula\nin 3-conjunctive normal form into a triangulation of an orientable closed\nsurface.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 16:34:07 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Jim\u00e9nez", "Andrea", ""], ["Kiwi", "Marcos", ""]]}, {"id": "1107.4021", "submitter": "Petros Elia", "authors": "Arun Singh, Petros Elia and Joakim Jalden", "title": "Achieving a vanishing SNR-gap to exact lattice decoding at a\n  subexponential complexity", "comments": "16 pages - submission for IEEE Trans. Inform. Theory", "journal-ref": null, "doi": "10.1109/TIT.2012.2190709", "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work identifies the first lattice decoding solution that achieves, in the\ngeneral outage-limited MIMO setting and in the high-rate and high-SNR limit,\nboth a vanishing gap to the error-performance of the (DMT optimal) exact\nsolution of preprocessed lattice decoding, as well as a computational\ncomplexity that is subexponential in the number of codeword bits. The proposed\nsolution employs lattice reduction (LR)-aided regularized (lattice) sphere\ndecoding and proper timeout policies. These performance and complexity\nguarantees hold for most MIMO scenarios, all reasonable fading statistics, all\nchannel dimensions and all full-rate lattice codes.\n  In sharp contrast to the above manageable complexity, the complexity of other\nstandard preprocessed lattice decoding solutions is shown here to be extremely\nhigh. Specifically the work is first to quantify the complexity of these\nlattice (sphere) decoding solutions and to prove the surprising result that the\ncomplexity required to achieve a certain rate-reliability performance, is\nexponential in the lattice dimensionality and in the number of codeword bits,\nand it in fact matches, in common scenarios, the complexity of ML-based\nsolutions. Through this sharp contrast, the work was able to, for the first\ntime, rigorously quantify the pivotal role of lattice reduction as a special\ncomplexity reducing ingredient.\n  Finally the work analytically refines transceiver DMT analysis which\ngenerally fails to address potentially massive gaps between theory and\npractice. Instead the adopted vanishing gap condition guarantees that the\ndecoder's error curve is arbitrarily close, given a sufficiently high SNR, to\nthe optimal error curve of exact solutions, which is a much stronger condition\nthan DMT optimality which only guarantees an error gap that is subpolynomial in\nSNR, and can thus be unbounded and generally unacceptable in practical\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 16:14:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Singh", "Arun", ""], ["Elia", "Petros", ""], ["Jalden", "Joakim", ""]]}, {"id": "1107.4113", "submitter": "Nicholas Pippenger", "authors": "Patrick Eschenfeldt, Ben Gross and Nicholas Pippenger", "title": "Stochastic Service Systems, Random Interval Graphs and Search Algorithms", "comments": "i+21 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several stochastic service systems, and study the asymptotic\nbehavior of the moments of various quantities that have application to models\nfor random interval graphs and algorithms for searching for an idle server or\nempty waiting station. In two cases the moments turn out to involve Lambert\nseries for the generating functions for the sums of powers of divisors of\npositive integers. For these cases we are able to obtain complete asymptotic\nexpansions for the moments of the quantities in question.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 20:27:15 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2011 21:46:00 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Eschenfeldt", "Patrick", ""], ["Gross", "Ben", ""], ["Pippenger", "Nicholas", ""]]}, {"id": "1107.4150", "submitter": "Kun He Dr.", "authors": "Wenqi Huang and Kun He", "title": "Analysis on the computability over the efficient utilization problem of\n  the four-dimensional space-time", "comments": "13 pages, 3 figures", "journal-ref": "Theoretical Computer Science, vol. 501(27): 1-10, 2013", "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formally proposes a problem about the efficient utilization of the\nfour dimensional space-time. Given a cuboid container, a finite number of rigid\ncuboid items, and the time length that each item should be continuous baked in\nthe container, the problem asks to arrange the starting time for each item\nbeing placed into the container and to arrange the position and orientation for\neach item at each instant during its continuous baking period such that the\ntotal time length the container be utilized is as short as possible. Here all\nside dimensions of the container and of the items are positive real numbers\narbitrarily given. Differs from the classical packing problems, the position\nand orientation of each item in the container could be changed over time.\nTherefore, according to above mathematical model, the four-dimensional\nspace-time can be utilized more truly and more fully. This paper then proves\nthat there exists an exact algorithm that could solve the problem by finite\noperations, so we say this problem is weak computable. Based on the\nunderstanding of this computability proof, it is expected to design effective\napproximate algorithms in the near future. A piggyback work completed is a\nstrict proof on the weak computability over general and natural case of the\nthree-dimensional cuboid packing decision problem that all parameters are\npositive real numbers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 03:19:09 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Huang", "Wenqi", ""], ["He", "Kun", ""]]}, {"id": "1107.4196", "submitter": "Pascal Vontobel", "authors": "Pascal O. Vontobel", "title": "The Bethe Permanent of a Non-Negative Matrix", "comments": "Accepted for IEEE Transactions on Information Theory. Manuscript\n  received July 21, 2011; date of current version October 20, 2012. Changes\n  compared to v1: see v2. Changes compared to v2: changed t to \\tau in Section\n  IV in order to distinguish it from iteration number t in Section V. Fixed\n  some typos", "journal-ref": "IEEE Trans. Inf. Theory, vol. 59, pp. 1866-1901, Mar. 2013", "doi": "10.1109/TIT.2012.2227109", "report-no": null, "categories": "cs.IT cs.CC math-ph math.CO math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been observed that the permanent of a non-negative square\nmatrix, i.e., of a square matrix containing only non-negative real entries, can\nvery well be approximated by solving a certain Bethe free energy function\nminimization problem with the help of the sum-product algorithm. We call the\nresulting approximation of the permanent the Bethe permanent. In this paper we\ngive reasons why this approach to approximating the permanent works well.\nNamely, we show that the Bethe free energy function is convex and that the\nsum-product algorithm finds its minimum efficiently. We then discuss the fact\nthat the permanent is lower bounded by the Bethe permanent, and we comment on\npotential upper bounds on the permanent based on the Bethe permanent. We also\npresent a combinatorial characterization of the Bethe permanent in terms of\npermanents of so-called lifted versions of the matrix under consideration.\nMoreover, we comment on possibilities to modify the Bethe permanent so that it\napproximates the permanent even better, and we conclude the paper with some\nobservations and conjectures about permanent-based pseudo-codewords and\npermanent-based kernels.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 08:19:30 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 00:11:47 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2012 07:45:09 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Vontobel", "Pascal O.", ""]]}, {"id": "1107.4709", "submitter": "Mahdi Cheraghchi", "authors": "Mahdi Cheraghchi", "title": "Applications of Derandomization Theory in Coding", "comments": "EPFL Phd Thesis", "journal-ref": null, "doi": "10.5075/epfl-thesis-4767", "report-no": "EPFL PhD Thesis #4767", "categories": "cs.DM cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized techniques play a fundamental role in theoretical computer science\nand discrete mathematics, in particular for the design of efficient algorithms\nand construction of combinatorial objects. The basic goal in derandomization\ntheory is to eliminate or reduce the need for randomness in such randomized\nconstructions. In this thesis, we explore some applications of the fundamental\nnotions in derandomization theory to problems outside the core of theoretical\ncomputer science, and in particular, certain problems related to coding theory.\n  First, we consider the wiretap channel problem which involves a communication\nsystem in which an intruder can eavesdrop a limited portion of the\ntransmissions, and construct efficient and information-theoretically optimal\ncommunication protocols for this model. Then we consider the combinatorial\ngroup testing problem. In this classical problem, one aims to determine a set\nof defective items within a large population by asking a number of queries,\nwhere each query reveals whether a defective item is present within a specified\ngroup of items. We use randomness condensers to explicitly construct optimal,\nor nearly optimal, group testing schemes for a setting where the query outcomes\ncan be highly unreliable, as well as the threshold model where a query returns\npositive if the number of defectives pass a certain threshold. Finally, we\ndesign ensembles of error-correcting codes that achieve the\ninformation-theoretic capacity of a large class of communication channels, and\nthen use the obtained ensembles for construction of explicit capacity achieving\ncodes.\n  [This is a shortened version of the actual abstract in the thesis.]\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2011 21:07:03 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Cheraghchi", "Mahdi", ""]]}, {"id": "1107.5419", "submitter": "Florian Huc", "authors": "Sebastien Gambs and Rachid Guerraoui and Hamza Harkous and Florian Huc\n  and Anne-Marie Kermarrec", "title": "Scalable and Secure Aggregation in Distributed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing an aggregation function in a\n\\emph{secure} and \\emph{scalable} way. Whereas previous distributed solutions\nwith similar security guarantees have a communication cost of $O(n^3)$, we\npresent a distributed protocol that requires only a communication complexity of\n$O(n\\log^3 n)$, which we prove is near-optimal. Our protocol ensures perfect\nsecurity against a computationally-bounded adversary, tolerates\n$(1/2-\\epsilon)n$ malicious nodes for any constant $1/2 > \\epsilon > 0$ (not\ndepending on $n$), and outputs the exact value of the aggregated function with\nhigh probability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 09:09:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2011 12:10:07 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2011 14:14:26 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Gambs", "Sebastien", ""], ["Guerraoui", "Rachid", ""], ["Harkous", "Hamza", ""], ["Huc", "Florian", ""], ["Kermarrec", "Anne-Marie", ""]]}, {"id": "1107.5478", "submitter": "Santosh Vempala", "authors": "Daniel Dadush and Santosh Vempala", "title": "Deterministic Construction of an Approximate M-Ellipsoid and its\n  Application to Derandomizing Lattice Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic O(log n)^n algorithm for the {\\em Shortest Vector\nProblem (SVP)} of a lattice under {\\em any} norm, improving on the previous\nbest deterministic bound of n^O(n) for general norms and nearly matching the\nbound of 2^O(n) for the standard Euclidean norm established by Micciancio and\nVoulgaris (STOC 2010). Our algorithm can be viewed as a derandomization of the\nAKS randomized sieve algorithm, which can be used to solve SVP for any norm in\n2^O(n) time with high probability. We use the technique of covering a convex\nbody by ellipsoids, as introduced for lattice problems in (Dadush et al., FOCS\n2011).\n  Our main contribution is a deterministic approximation of an M-ellipsoid of\nany convex body. We achieve this via a convex programming formulation of the\noptimal ellipsoid with the objective function being an n-dimensional integral\nthat we show can be approximated deterministically, a technique that appears to\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 14:05:55 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Dadush", "Daniel", ""], ["Vempala", "Santosh", ""]]}, {"id": "1107.5881", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Quantum Private Information Retrieval with Sublinear Communication\n  Complexity", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a quantum protocol for private information retrieval, in\nthe single-server case and with information-theoretical privacy, that has\nO(\\sqrt{n})-qubit communication complexity, where n denotes the size of the\ndatabase. In comparison, it is known that any classical protocol must use\n\\Omega(n) bits of communication in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 06:59:08 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1107.5886", "submitter": "Olivier Finkel", "authors": "Olivier Finkel (ELM)", "title": "Three Applications to Rational Relations of the High Undecidability of\n  the Infinite Post Correspondence Problem in a Regular omega-Language", "comments": "To appear in: Special Issue: Frontier Between Decidability and\n  Undecidability and Related Problems, International Journal of Foundations of\n  Computer Science", "journal-ref": "International Journal of Foundations of Computer Science 23, 7\n  (2012) p. 1481-1497", "doi": "10.1142/S0129054112400606", "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was noticed by Harel in [Har86] that \"one can define $\\Sigma_1^1$-complete\nversions of the well-known Post Correspondence Problem\". We first give a\ncomplete proof of this result, showing that the infinite Post Correspondence\nProblem in a regular $\\omega$-language is $\\Sigma_1^1$-complete, hence located\nbeyond the arithmetical hierarchy and highly undecidable. We infer from this\nresult that it is $\\Pi_1^1$-complete to determine whether two given infinitary\nrational relations are disjoint. Then we prove that there is an amazing gap\nbetween two decision problems about $\\omega$-rational functions realized by\nfinite state B\\\"uchi transducers. Indeed Prieur proved in [Pri01, Pri02] that\nit is decidable whether a given $\\omega$-rational function is continuous, while\nwe show here that it is $\\Sigma_1^1$-complete to determine whether a given\n$\\omega$-rational function has at least one point of continuity. Next we prove\nthat it is $\\Pi_1^1$-complete to determine whether the continuity set of a\ngiven $\\omega$-rational function is $\\omega$-regular. This gives the exact\ncomplexity of two problems which were shown to be undecidable in [CFS08].\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 07:15:04 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Finkel", "Olivier", "", "ELM"]]}]