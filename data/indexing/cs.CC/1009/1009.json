[{"id": "1009.0246", "submitter": "Ketan Mulmuley D", "authors": "Ketan Mulmuley", "title": "Explicit Proofs and The Flip", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a formal strategy of geometric complexity theory (GCT)\nto resolve the {\\em self referential paradox} in the $P$ vs. $NP$ and related\nproblems. The strategy, called the {\\em flip}, is to go for {\\em explicit\nproofs} of these problems. By an explicit proof we mean a proof that constructs\nproof certificates of hardness that are easy to verify, construct and decode.\nThe main result in this paper says that (1) any proof of the arithmetic\nimplication of the $P$ vs. $NP$ conjecture is close to an explicit proof in the\nsense that it can be transformed into an explicit proof by proving in addition\nthat arithmetic circuit identity testing can be derandomized in a blackbox\nfashion, and (2) stronger forms of these arithmetic hardness and\nderandomization conjectures together imply a polynomial time algorithm for a\nformidable explicit construction problem in algebraic geometry. This may\nexplain why these conjectures, which look so elementary at the surface, have\nturned out to be so hard.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 19:03:11 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Mulmuley", "Ketan", ""]]}, {"id": "1009.0300", "submitter": "Arkadii Slinko", "authors": "Edith Elkind, Piotr Faliszewski and Arkadii Slinko", "title": "Rationalizations of Condorcet-Consistent Rules via Distances of Hamming\n  Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of the {\\em distance rationalizability} approach to view the\nvoters' preferences as an imperfect approximation to some kind of consensus is\ndeeply rooted in social choice literature. It allows one to define\n(\"rationalize\") voting rules via a consensus class of elections and a distance:\na candidate is said to be an election winner if she is ranked first in one of\nthe nearest (with respect to the given distance) consensus elections. It is\nknown that many classic voting rules can be distance rationalized. In this\npaper, we provide new results on distance rationalizability of several\nCondorcet-consistent voting rules. In particular, we distance rationalize\nYoung's rule and Maximin rule using distances similar to the Hamming distance.\nWe show that the claim that Young's rule can be rationalized by the Condorcet\nconsensus class and the Hamming distance is incorrect; in fact, these consensus\nclass and distance yield a new rule which has not been studied before. We prove\nthat, similarly to Young's rule, this new rule has a computationally hard\nwinner determination problem.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 17:11:29 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Elkind", "Edith", ""], ["Faliszewski", "Piotr", ""], ["Slinko", "Arkadii", ""]]}, {"id": "1009.0309", "submitter": "Xi Chen", "authors": "Xi Chen and Shang-Hua Teng", "title": "A Complexity View of Markets with Social Influence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, inspired by the work of Megiddo on the formation of\npreferences and strategic analysis, we consider an early market model studied\nin the field of economic theory, in which each trader's utility may be\ninfluenced by the bundles of goods obtained by her social neighbors. The goal\nof this paper is to understand and characterize the impact of social influence\non the complexity of computing and approximating market equilibria.\n  We present complexity-theoretic and algorithmic results for approximating\nmarket equilibria in this model with focus on two concrete influence models\nbased on the traditional linear utility functions. Recall that an Arrow-Debreu\nmarket equilibrium in a conventional exchange market with linear utility\nfunctions can be computed in polynomial time by convex programming. Our\ncomplexity results show that even a bounded-degree, planar influence network\ncan significantly increase the difficulty of equilibrium computation even in\nmarkets with only a constant number of goods. Our algorithmic results suggest\nthat finding an approximate equilibrium in markets with hierarchical influence\nnetworks might be easier than that in markets with arbitrary neighborhood\nstructures. By demonstrating a simple market with a constant number of goods\nand a bounded-degree, planar influence graph whose equilibrium is PPAD-hard to\napproximate, we also provide a counterexample to a common belief, which we\nrefer to as the myth of a constant number of goods, that equilibria in markets\nwith a constant number of goods are easy to compute or easy to approximate.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 00:56:25 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Chen", "Xi", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1009.0358", "submitter": "Juraj Stacho", "authors": "Tom\\'as Feder, Pavol Hell, David G. Schell, Juraj Stacho", "title": "Dichotomy for tree-structured trigraph list homomorphism problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trigraph list homomorphism problems (also known as list matrix partition\nproblems) have generated recent interest, partly because there are concrete\nproblems that are not known to be polynomial time solvable or NP-complete. Thus\nwhile digraph list homomorphism problems enjoy dichotomy (each problem is\nNP-complete or polynomial time solvable), such dichotomy is not necessarily\nexpected for trigraph list homomorphism problems. However, in this paper, we\nidentify a large class of trigraphs for which list homomorphism problems do\nexhibit a dichotomy. They consist of trigraphs with a tree-like structure, and,\nin particular, include all trigraphs whose underlying graphs are trees. In\nfact, we show that for these tree-like trigraphs, the trigraph list\nhomomorphism problem is polynomially equivalent to a related digraph list\nhomomorphism problem. We also describe a few examples illustrating that our\nconditions defining tree-like trigraphs are not unnatural, as relaxing them may\nlead to harder problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 09:05:04 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Feder", "Tom\u00e1s", ""], ["Hell", "Pavol", ""], ["Schell", "David G.", ""], ["Stacho", "Juraj", ""]]}, {"id": "1009.0416", "submitter": "Harumichi Nishimura", "authors": "Kazuo Iwama, Harumichi Nishimura, Rudy Raymond, Junichi Teruyama", "title": "Quantum Counterfeit Coin Problems", "comments": "18 pages", "journal-ref": "Theor. Comput. Sci. 456 (2012) 51-64", "doi": "10.1016/j.tcs.2012.05.039", "report-no": null, "categories": "quant-ph cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The counterfeit coin problem requires us to find all false coins from a given\nbunch of coins using a balance scale. We assume that the balance scale gives us\nonly ``balanced'' or ``tilted'' information and that we know the number k of\nfalse coins in advance. The balance scale can be modeled by a certain type of\noracle and its query complexity is a measure for the cost of weighing\nalgorithms (the number of weighings). In this paper, we study the quantum query\ncomplexity for this problem. Let Q(k,N) be the quantum query complexity of\nfinding all k false coins from the N given coins. We show that for any k and N\nsuch that k < N/2, Q(k,N)=O(k^{1/4}), contrasting with the classical query\ncomplexity, \\Omega(k\\log(N/k)), that depends on N. So our quantum algorithm\nachieves a quartic speed-up for this problem. We do not have a matching lower\nbound, but we show some evidence that the upper bound is tight: any algorithm,\nincluding our algorithm, that satisfies certain properties needs\n\\Omega(k^{1/4}) queries.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 12:58:51 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Iwama", "Kazuo", ""], ["Nishimura", "Harumichi", ""], ["Raymond", "Rudy", ""], ["Teruyama", "Junichi", ""]]}, {"id": "1009.0706", "submitter": "Amir Daneshgar", "authors": "Amir Daneshgar, Ramin Javadi", "title": "On Complexity of Isoperimetric Problems on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is aimed to investigate some computational aspects of different\nisoperimetric problems on weighted trees. In this regard, we consider different\nconnectivity parameters called {\\it minimum normalized cuts}/{\\it isoperimteric\nnumbers} defined through taking minimum of the maximum or the mean of the\nnormalized outgoing flows from a set of subdomains of vertices, where these\nsubdomains constitute a {\\it partition}/{\\it subpartition}. Following the main\nresult of [A. Daneshgar, {\\it et. al.}, {\\it On the isoperimetric spectrum of\ngraphs and its approximations}, JCTB, (2010)], it is known that the\nisoperimetric number and the minimum normalized cut both can be described as\n$\\{0,1\\}$-optimization programs, where the latter one does {\\it not} admit a\nrelaxation to the reals. We show that the decision problem for the case of\ntaking $k$-partitions and the maximum (called the max normalized cut problem\n{\\rm NCP}$^M$) as well as the other two decision problems for the mean version\n(referred to as {\\rm IPP}$^m$ and {\\rm NCP}$^m$) are $NP$-complete problems. On\nthe other hand, we show that the decision problem for the case of taking\n$k$-subpartitions and the maximum (called the max isoperimetric problem {\\rm\nIPP}$^M$) can be solved in {\\it linear time} for any weighted tree and any $k\n\\geq 2$. Based on this fact, we provide polynomial time $O(k)$-approximation\nalgorithms for all different versions of $k$th isoperimetric numbers\nconsidered. Moreover, when the number of partitions/subpartitions, $k$, is a\nfixed constant, as an extension of a result of B. Mohar (1989) for the case\n$k=2$ (usually referred to as the Cheeger constant), we prove that max and mean\nisoperimetric numbers of weighted trees as well as their max normalized cut can\nbe computed in polynomial time. We also prove some hardness results for the\ncase of simple unweighted graphs and trees.\n", "versions": [{"version": "v1", "created": "Fri, 3 Sep 2010 16:00:57 GMT"}], "update_date": "2010-09-06", "authors_parsed": [["Daneshgar", "Amir", ""], ["Javadi", "Ramin", ""]]}, {"id": "1009.0884", "submitter": "Xiao Wen Han", "authors": "Han Xiao Wen", "title": "Knowledge Recognition Algorithm enables P = NP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a knowledge recognition algorithm (KRA) that is both a\nTuring machine algorithm and an Oracle Turing machine algorithm. By definition\nKRA is a non-deterministic language recognition algorithm. Simultaneously it\ncan be implemented as a deterministic Turing machine algorithm. KRA applies\nmirrored perceptual-conceptual languages to learn member-class relations\nbetween the two languages iteratively and retrieve information through\ndeductive and reductive recognition from one language to another. The novelty\nof KRA is that the conventional concept of relation is adjusted. The\ncomputation therefore becomes efficient bidirectional string mapping.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 02:11:50 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Wen", "Han Xiao", ""]]}, {"id": "1009.1174", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Parameterized Complexity Results in Symmetry Breaking", "comments": "Invited talk at IPEC 2010, 5th International Symposium on\n  Parameterized and Exact Computation, December 13-15, 2010, Chennai, India\n  http://www.imsc.res.in/ipec", "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_3", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is a common feature of many combinatorial problems. Unfortunately\neliminating all symmetry from a problem is often computationally intractable.\nThis paper argues that recent parameterized complexity results provide insight\ninto that intractability and help identify special cases in which symmetry can\nbe dealt with more tractably\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 22:54:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1009.1208", "submitter": "Henning Schnoor", "authors": "Elmar B\\~A{\\P}hler (Universit\\\"at W\\\"urzburg), Nadia Creignou\n  (Universit\\'e de la M\\'editerran\\'ee, Marseille), Matthias Galota\n  (Elektrobit), Steffen Reith (Hochschule RheinMain, Wiesbaden), Henning\n  Schnoor (Christian-Albrechts-Universit\\\"at zu Kiel), Heribert Vollmer\n  (Leibniz Universit\\\"at Hannover)", "title": "Complexity classifications for different equivalence and audit problems\n  for Boolean circuits", "comments": "25 pages, 1 figure", "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 3 (September\n  30, 2012) lmcs:1172", "doi": "10.2168/LMCS-8(3:31)2012", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Boolean circuits as a representation of Boolean functions and\nconsider different equivalence, audit, and enumeration problems. For a number\nof restricted sets of gate types (bases) we obtain efficient algorithms, while\nfor all other gate types we show these problems are at least NP-hard.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 07:19:16 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2011 09:29:37 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2012 10:21:13 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2012 08:39:28 GMT"}, {"version": "v5", "created": "Mon, 1 Oct 2012 20:10:58 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["B\u00c3\u00b6hler", "Elmar", "", "Universit\u00e4t W\u00fcrzburg"], ["Creignou", "Nadia", "", "Universit\u00e9 de la M\u00e9diterran\u00e9e, Marseille"], ["Galota", "Matthias", "", "Elektrobit"], ["Reith", "Steffen", "", "Hochschule RheinMain, Wiesbaden"], ["Schnoor", "Henning", "", "Christian-Albrechts-Universit\u00e4t zu Kiel"], ["Vollmer", "Heribert", "", "Leibniz Universit\u00e4t Hannover"]]}, {"id": "1009.1635", "submitter": "Christopher Laumann", "authors": "C. R. Laumann, R. Moessner, A. Scardicchio, S. L. Sondhi", "title": "Statistical mechanics of classical and quantum computational complexity", "comments": "25 pages, 8 figures. Lecture notes for lectures given by R. Moessner\n  at the Les Houches School on \"Modern theories of correlated electron\n  systems\", May 11-29, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for quantum computers is motivated by their potential for solving\nproblems that defy existing, classical, computers. The theory of computational\ncomplexity, one of the crown jewels of computer science, provides a rigorous\nframework for classifying the hardness of problems according to the\ncomputational resources, most notably time, needed to solve them. Its extension\nto quantum computers allows the relative power of quantum computers to be\nanalyzed. This framework identifies families of problems which are likely hard\nfor classical computers (``NP-complete'') and those which are likely hard for\nquantum computers (``QMA-complete'') by indirect methods. That is, they\nidentify problems of comparable worst-case difficulty without directly\ndetermining the individual hardness of any given instance. Statistical\nmechanical methods can be used to complement this classification by directly\nextracting information about particular families of instances---typically those\nthat involve optimization---by studying random ensembles of them. These pose\nunusual and interesting (quantum) statistical mechanical questions and the\nresults shed light on the difficulty of problems for large classes of\nalgorithms as well as providing a window on the contrast between typical and\nworst case complexity. In these lecture notes we present an introduction to\nthis set of ideas with older work on classical satisfiability and recent work\non quantum satisfiability as primary examples. We also touch on the connection\nof computational hardness with the physical notion of glassiness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 20:09:59 GMT"}], "update_date": "2010-09-10", "authors_parsed": [["Laumann", "C. R.", ""], ["Moessner", "R.", ""], ["Scardicchio", "A.", ""], ["Sondhi", "S. L.", ""]]}, {"id": "1009.1990", "submitter": "Michael Thomas", "authors": "Michael Thomas and Heribert Vollmer", "title": "Complexity of Non-Monotonic Logics", "comments": "To appear in Bulletin of the EATCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, non-monotonic reasoning has developed to be one of\nthe most important topics in computational logic and artificial intelligence.\nDifferent ways to introduce non-monotonic aspects to classical logic have been\nconsidered, e.g., extension with default rules, extension with modal belief\noperators, or modification of the semantics. In this survey we consider a\nlogical formalism from each of the above possibilities, namely Reiter's default\nlogic, Moore's autoepistemic logic and McCarthy's circumscription.\nAdditionally, we consider abduction, where one is not interested in inferences\nfrom a given knowledge base but in computing possible explanations for an\nobservation with respect to a given knowledge base.\n  Complexity results for different reasoning tasks for propositional variants\nof these logics have been studied already in the nineties. In recent years,\nhowever, a renewed interest in complexity issues can be observed. One current\nfocal approach is to consider parameterized problems and identify reasonable\nparameters that allow for FPT algorithms. In another approach, the emphasis\nlies on identifying fragments, i.e., restriction of the logical language, that\nallow more efficient algorithms for the most important reasoning tasks. In this\nsurvey we focus on this second aspect. We describe complexity results for\nfragments of logical languages obtained by either restricting the allowed set\nof operators (e.g., forbidding negations one might consider only monotone\nformulae) or by considering only formulae in conjunctive normal form but with\ngeneralized clause types.\n  The algorithmic problems we consider are suitable variants of satisfiability\nand implication in each of the logics, but also counting problems, where one is\nnot only interested in the existence of certain objects (e.g., models of a\nformula) but asks for their number.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 12:03:44 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Thomas", "Michael", ""], ["Vollmer", "Heribert", ""]]}, {"id": "1009.2211", "submitter": "Xiaodi Wu", "authors": "Xiaodi Wu", "title": "Parallelized Solution to Semidefinite Programmings in Quantum Complexity\n  Theory", "comments": "22 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an equilibrium value based framework for solving\nSDPs via the multiplicative weight update method which is different from the\none in Kale's thesis \\cite{Kale07}. One of the main advantages of the new\nframework is that we can guarantee the convertibility from approximate to exact\nfeasibility in a much more general class of SDPs than previous result. Another\nadvantage is the design of the oracle which is necessary for applying the\nmultiplicative weight update method is much simplified in general cases. This\nleads to an alternative and easier solutions to the SDPs used in the previous\nresults \\class{QIP(2)}$\\subseteq$\\class{PSPACE} \\cite{JainUW09} and\n\\class{QMAM}=\\class{PSPACE} \\cite{JainJUW09}. Furthermore, we provide a generic\nform of SDPs which can be solved in the similar way. By parallelizing every\nstep in our solution, we are able to solve a class of SDPs in \\class{NC}.\nAlthough our motivation is from quantum computing, our result will also apply\ndirectly to any SDP which satisfies our conditions. In addition to the new\nframework for solving SDPs, we also provide a novel framework which improves\nthe range of equilibrium value problems that can be solved via the\nmultiplicative weight update method. Before this work we are only able to\ncalculate the equilibrium value where one of the two convex sets needs to be\nthe set of density operators. Our work demonstrates that in the case when one\nset is the set of density operators with further linear constraints, we are\nstill able to approximate the equilibrium value to high precision via the\nmultiplicative weight update method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Sep 2010 02:33:50 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Wu", "Xiaodi", ""]]}, {"id": "1009.2363", "submitter": "Nina Taslaman", "authors": "Thore Husfeldt and Nina Taslaman", "title": "The Exponential Time Complexity of Computing the Probability That a\n  Graph is Connected", "comments": "To appear in 5th International Symposium on Parameterized and Exact\n  Computation (IPEC 2010), December 13-15, 2010, Chennai, India, Springer LNCS,\n  2010", "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_19", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for every probability p with 0 < p < 1, computation of\nall-terminal graph reliability with edge failure probability p requires time\nexponential in Omega(m/ log^2 m) for simple graphs of m edges under the\nExponential Time Hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 12:21:16 GMT"}, {"version": "v2", "created": "Sun, 26 Sep 2010 14:26:22 GMT"}, {"version": "v3", "created": "Sat, 16 Oct 2010 12:18:52 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Husfeldt", "Thore", ""], ["Taslaman", "Nina", ""]]}, {"id": "1009.2577", "submitter": "Praveen Manjunatha", "authors": "M. Praveen", "title": "Small Vertex Cover makes Petri Net Coverability and Boundedness Easier", "comments": "Full version of the paper appearing in IPEC 2010", "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_21", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coverability and boundedness problems for Petri nets are known to be\nExpspace-complete. Given a Petri net, we associate a graph with it. With the\nvertex cover number k of this graph and the maximum arc weight W as parameters,\nwe show that coverability and boundedness are in ParaPspace. This means that\nthese problems can be solved in space O(ef(k,W)poly(n)), where ef(k,W) is some\nexponential function and poly(n) is some polynomial in the size of the input.\nWe then extend the ParaPspace result to model checking a logic that can express\nsome generalizations of coverability and boundedness.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 07:06:30 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Praveen", "M.", ""]]}, {"id": "1009.2706", "submitter": "Sergey Verlan", "authors": "Artiom Alhazov, Sergey Verlan", "title": "Minimization Strategies for Maximally Parallel Multiset Rewriting\n  Systems", "comments": "This article is an improved version of [1]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximally parallel multiset rewriting systems (MPMRS) give a convenient way\nto express relations between unstructured objects. The functioning of various\ncomputational devices may be expressed in terms of MPMRS (e.g., register\nmachines and many variants of P systems). In particular, this means that MPMRS\nare computationally complete; however, a direct translation leads to quite a\nbig number of rules. Like for other classes of computationally complete\ndevices, there is a challenge to find a universal system having the smallest\nnumber of rules. In this article we present different rule minimization\nstrategies for MPMRS based on encodings and structural transformations. We\napply these strategies to the translation of a small universal register machine\n(Korec, 1996) and we show that there exists a universal MPMRS with 23 rules.\nSince MPMRS are identical to a restricted variant of P systems with antiport\nrules, the results we obtained improve previously known results on the number\nof rules for those systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 16:06:02 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Alhazov", "Artiom", ""], ["Verlan", "Sergey", ""]]}, {"id": "1009.3124", "submitter": "Abuzer Yakaryilmaz", "authors": "A. C. Cem Say and Abuzer Yakaryilmaz", "title": "Quantum function computation using sublogarithmic space (abstract &\n  poster)", "comments": "2 pages, poster presented at the 13th Workshop on Quantum Information\n  Processing (QIP2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that quantum Turing machines are strictly superior to probabilistic\nTuring machines in function computation for any space bound $ o(\\log(n)) $.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 09:23:38 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Say", "A. C. Cem", ""], ["Yakaryilmaz", "Abuzer", ""]]}, {"id": "1009.3214", "submitter": "Mark Giesbrecht", "authors": "Mark Giesbrecht and Daniel S. Roche and Hrushikesh Tilak", "title": "Computing sparse multiples of polynomials", "comments": "Extended abstract appears in Proc. ISAAC 2010, pp. 266-278, LNCS 6506", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a sparse multiple of a polynomial. Given f\nin F[x] of degree d over a field F, and a desired sparsity t, our goal is to\ndetermine if there exists a multiple h in F[x] of f such that h has at most t\nnon-zero terms, and if so, to find such an h. When F=Q and t is constant, we\ngive a polynomial-time algorithm in d and the size of coefficients in h. When F\nis a finite field, we show that the problem is at least as hard as determining\nthe multiplicative order of elements in an extension field of F (a problem\nthought to have complexity similar to that of factoring integers), and this\nlower bound is tight when t=2.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 16:21:15 GMT"}, {"version": "v2", "created": "Sat, 1 Jan 2011 21:08:52 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Giesbrecht", "Mark", ""], ["Roche", "Daniel S.", ""], ["Tilak", "Hrushikesh", ""]]}, {"id": "1009.3217", "submitter": "Paul Bonsma", "authors": "Paul Bonsma", "title": "The Complexity of Rerouting Shortest Paths", "comments": "The results on claw-free graphs, chordal graphs and isolated paths\n  have been added in version 2 (april 2012). Version 1 (September 2010) only\n  contained the PSPACE-hardness result. (Version 2 has been submitted.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shortest Path Reconfiguration problem has as input a graph G (with unit\nedge lengths) with vertices s and t, and two shortest st-paths P and Q. The\nquestion is whether there exists a sequence of shortest st-paths that starts\nwith P and ends with Q, such that subsequent paths differ in only one vertex.\nThis is called a rerouting sequence.\n  This problem is shown to be PSPACE-complete. For claw-free graphs and chordal\ngraphs, it is shown that the problem can be solved in polynomial time, and that\nshortest rerouting sequences have linear length. For these classes, it is also\nshown that deciding whether a rerouting sequence exists between all pairs of\nshortest st-paths can be done in polynomial time. Finally, a polynomial time\nalgorithm for counting the number of isolated paths is given.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 16:26:51 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 11:36:42 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Bonsma", "Paul", ""]]}, {"id": "1009.3460", "submitter": "Oded Regev", "authors": "Amit Chakrabarti, Oded Regev", "title": "An Optimal Lower Bound on the Communication Complexity of\n  Gap-Hamming-Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an optimal $\\Omega(n)$ lower bound on the randomized communication\ncomplexity of the much-studied Gap-Hamming-Distance problem. As a consequence,\nwe obtain essentially optimal multi-pass space lower bounds in the data stream\nmodel for a number of fundamental problems, including the estimation of\nfrequency moments.\n  The Gap-Hamming-Distance problem is a communication problem, wherein Alice\nand Bob receive $n$-bit strings $x$ and $y$, respectively. They are promised\nthat the Hamming distance between $x$ and $y$ is either at least $n/2+\\sqrt{n}$\nor at most $n/2-\\sqrt{n}$, and their goal is to decide which of these is the\ncase. Since the formal presentation of the problem by Indyk and Woodruff (FOCS,\n2003), it had been conjectured that the naive protocol, which uses $n$ bits of\ncommunication, is asymptotically optimal. The conjecture was shown to be true\nin several special cases, e.g., when the communication is deterministic, or\nwhen the number of rounds of communication is limited.\n  The proof of our aforementioned result, which settles this conjecture fully,\nis based on a new geometric statement regarding correlations in Gaussian space,\nrelated to a result of C. Borell (1985). To prove this geometric statement, we\nshow that random projections of not-too-small sets in Gaussian space are close\nto a mixture of translated normal variables.\n", "versions": [{"version": "v1", "created": "Fri, 17 Sep 2010 15:53:26 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2011 17:58:39 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2012 19:13:06 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Regev", "Oded", ""]]}, {"id": "1009.3640", "submitter": "Oded Regev", "authors": "Bo'az Klartag and Oded Regev", "title": "Quantum One-Way Communication is Exponentially Stronger Than Classical\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.MG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In STOC 1999, Raz presented a (partial) function for which there is a quantum\nprotocol communicating only $O(\\log n)$ qubits, but for which any classical\n(randomized, bounded-error) protocol requires $\\poly(n)$ bits of communication.\nThat quantum protocol requires two rounds of communication. Ever since Raz's\npaper it was open whether the same exponential separation can be achieved with\na quantum protocol that uses only one round of communication. Here we settle\nthis question in the affirmative.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 14:13:36 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Klartag", "Bo'az", ""], ["Regev", "Oded", ""]]}, {"id": "1009.3687", "submitter": "Xiao Wen Han", "authors": "Xiaowen Han, David Zhu, Cuifeng Zhou", "title": "3-SAT Polynomial Solution of Knowledge Recognition Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a knowledge recognition algorithm (KRA) for solving the\n3SAT problem in polynomial time. KRA learns member-class relations and\nretrieves information through deductive and reductive iterative reasoning. It\napplies the principle of Chinese COVA* (equivalent to a set of eight 3-variable\nconjunctive clauses) and eliminates the \"OR\" operation to solve 3-SAT problem.\nThat is, KRA does not search the assignment directly. It recognizes the\ncomplements as rejections at each level of the set through iterative set\nrelation recognition. KRA recognizes which conjunctive 3-variable-clause is not\nsatisfiable. If all the eight clauses of any set of 3-variable clauses are\nrejected, then there is not an assignment for the formula. If there is at least\none clause in each set that remains, then there is at least one assignment that\nis the union of clauses of each set. If there is more than one clause in each\nset that remains, then there are multiple assignments that are the unions of\nthe clauses of each set respectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 Sep 2010 03:55:13 GMT"}, {"version": "v10", "created": "Thu, 25 Nov 2010 03:05:19 GMT"}, {"version": "v11", "created": "Mon, 6 Dec 2010 01:07:34 GMT"}, {"version": "v12", "created": "Thu, 9 Dec 2010 23:42:06 GMT"}, {"version": "v2", "created": "Mon, 4 Oct 2010 01:03:53 GMT"}, {"version": "v3", "created": "Sat, 9 Oct 2010 17:33:55 GMT"}, {"version": "v4", "created": "Wed, 20 Oct 2010 21:09:49 GMT"}, {"version": "v5", "created": "Mon, 1 Nov 2010 16:28:19 GMT"}, {"version": "v6", "created": "Tue, 2 Nov 2010 17:06:34 GMT"}, {"version": "v7", "created": "Thu, 4 Nov 2010 16:48:41 GMT"}, {"version": "v8", "created": "Mon, 8 Nov 2010 03:02:21 GMT"}, {"version": "v9", "created": "Mon, 22 Nov 2010 02:17:19 GMT"}], "update_date": "2010-12-13", "authors_parsed": [["Han", "Xiaowen", ""], ["Zhu", "David", ""], ["Zhou", "Cuifeng", ""]]}, {"id": "1009.4102", "submitter": "Bjoern Andres", "authors": "Bjoern Andres and Joerg H. Kappes and Ullrich Koethe and Fred A.\n  Hamprecht", "title": "The Lazy Flipper: MAP Inference in Higher-Order Graphical Models by\n  Depth-limited Exhaustive Search", "comments": "C++ Source Code available from\n  http://hci.iwr.uni-heidelberg.de/software.php", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new search algorithm for the NP-hard problem of\noptimizing functions of binary variables that decompose according to a\ngraphical model. It can be applied to models of any order and structure. The\nmain novelty is a technique to constrain the search space based on the topology\nof the model. When pursued to the full search depth, the algorithm is\nguaranteed to converge to a global optimum, passing through a series of\nmonotonously improving local optima that are guaranteed to be optimal within a\ngiven and increasing Hamming distance. For a search depth of 1, it specializes\nto Iterated Conditional Modes. Between these extremes, a useful tradeoff\nbetween approximation quality and runtime is established. Experiments on models\nderived from both illustrative and real problems show that approximations found\nwith limited search depth match or improve those obtained by state-of-the-art\nmethods based on message passing and linear programming.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 14:07:51 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Andres", "Bjoern", ""], ["Kappes", "Joerg H.", ""], ["Koethe", "Ullrich", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1009.4136", "submitter": "Alexander Russell", "authors": "Cristopher Moore and Alexander Russell", "title": "Regarding a Representation-Theoretic Conjecture of Wigderson", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exists a family of irreducible representations R_i (of\nfinite groups G_i) such that, for any constant t, the average of R_i over t\nuniformly random elements g_1, ..., g_t of G_i has operator norm 1 with\nprobability approaching 1 as i limits to infinity. This settles a conjecture of\nWigderson in the negative.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 16:33:35 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Moore", "Cristopher", ""], ["Russell", "Alexander", ""]]}, {"id": "1009.4188", "submitter": "John Wiltshire-Gordon", "authors": "Gene S. Kopp and John D. Wiltshire-Gordon", "title": "Robust Coin Flipping", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": "10.1007/978-3-642-29011-4_12", "report-no": null, "categories": "cs.CC cs.CR cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alice seeks an information-theoretically secure source of private random\ndata. Unfortunately, she lacks a personal source and must use remote sources\ncontrolled by other parties. Alice wants to simulate a coin flip of specified\nbias $\\alpha$, as a function of data she receives from $p$ sources; she seeks\nprivacy from any coalition of $r$ of them. We show: If $p/2 \\leq r < p$, the\nbias can be any rational number and nothing else; if $0 < r < p/2$, the bias\ncan be any algebraic number and nothing else. The proof uses projective\nvarieties, convex geometry, and the probabilistic method. Our results improve\non those laid out by Yao, who asserts one direction of the $r=1$ case in his\nseminal paper [Yao82]. We also provide an application to secure multiparty\ncomputation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 19:59:16 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 21:30:24 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2012 00:58:20 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kopp", "Gene S.", ""], ["Wiltshire-Gordon", "John D.", ""]]}, {"id": "1009.4375", "submitter": "Zeev Dvir", "authors": "Boaz Barak, Zeev Dvir, Avi Wigderson, Amir Yehudayoff", "title": "Rank Bounds for Design Matrices with Applications to Combinatorial\n  Geometry and Locally Correctable Codes", "comments": "31 pages. Added high dimensional SG theorem. Extended abstract to\n  appear in STOC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A (q,k,t)-design matrix is an m x n matrix whose pattern of zeros/non-zeros\nsatisfies the following design-like condition: each row has at most q\nnon-zeros, each column has at least k non-zeros and the supports of every two\ncolumns intersect in at most t rows. We prove that the rank of any\n(q,k,t)-design matrix over a field of characteristic zero (or sufficiently\nlarge finite characteristic) is at least n - (qtn/2k)^2 . Using this result we\nderive the following applications:\n  (1) Impossibility results for 2-query LCCs over the complex numbers: A\n2-query locally correctable code (LCC) is an error correcting code in which\nevery codeword coordinate can be recovered, probabilistically, by reading at\nmost two other code positions. Such codes have numerous applications and\nconstructions (with exponential encoding length) are known over finite fields\nof small characteristic. We show that infinite families of such linear 2-query\nLCCs do not exist over the complex numbers.\n  (2) Generalization of results in combinatorial geometry: We prove a\nquantitative analog of the Sylvester-Gallai theorem: Let $v_1,...,v_m$ be a set\nof points in $\\C^d$ such that for every $i \\in [m]$ there exists at least\n$\\delta m$ values of $j \\in [m]$ such that the line through $v_i,v_j$ contains\na third point in the set. We show that the dimension of $\\{v_1,...,v_m \\}$ is\nat most $O(1/\\delta^2)$. Our results generalize to the high dimensional case\n(replacing lines with planes, etc.) and to the case where the points are\ncolored (as in the Motzkin-Rabin Theorem).\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 14:50:25 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2011 15:06:30 GMT"}], "update_date": "2011-03-11", "authors_parsed": [["Barak", "Boaz", ""], ["Dvir", "Zeev", ""], ["Wigderson", "Avi", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1009.4447", "submitter": "Karol Suchan", "authors": "Florent Becker, Mart\\'in Matamala, Nicolas Nisse, Ivan Rapaport, Karol\n  Suchan, Ioan Todinca", "title": "Adding a referee to an interconnection network: What can(not) be\n  computed in one round", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we ask which properties of a distributed network can be\ncomputed from a little amount of local information provided by its nodes. The\ndistributed model we consider is a restriction of the classical CONGEST\n(distributed) model and it is close to the simultaneous messages (communication\ncomplexity) model defined by Babai, Kimmel and Lokam. More precisely, each of\nthese n nodes -which only knows its own ID and the IDs of its neighbors- is\nallowed to send a message of O(log n) bits to some central entity, called the\nreferee. Is it possible for the referee to decide some basic structural\nproperties of the network topology G? We show that simple questions like, \"does\nG contain a square?\", \"does G contain a triangle?\" or \"Is the diameter of G at\nmost 3? cannot be solved in general. On the other hand, the referee can decode\nthe messages in order to have full knowledge of G when G belongs to many graph\nclasses such as planar graphs, bounded treewidth graphs and, more generally,\nbounded degeneracy graphs. We leave open questions related to the connectivity\nof arbitrary graphs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 19:01:17 GMT"}, {"version": "v2", "created": "Tue, 5 Oct 2010 18:17:01 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Becker", "Florent", ""], ["Matamala", "Mart\u00edn", ""], ["Nisse", "Nicolas", ""], ["Rapaport", "Ivan", ""], ["Suchan", "Karol", ""], ["Todinca", "Ioan", ""]]}, {"id": "1009.4597", "submitter": "Giorgi Pascal", "authors": "Pascal Giorgi (LIRMM)", "title": "On Polynomial Multiplication in Chebyshev Basis", "comments": null, "journal-ref": "IEEE Transactions on Computers 61, 6 (2012) 780-789", "doi": "10.1109/TC.2011.110", "report-no": null, "categories": "cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Lima, Panario and Wang have provided a new method to\nmultiply polynomials in Chebyshev basis which aims at reducing the total number\nof multiplication when polynomials have small degree. Their idea is to use\nKaratsuba's multiplication scheme to improve upon the naive method but without\nbeing able to get rid of its quadratic complexity. In this paper, we extend\ntheir result by providing a reduction scheme which allows to multiply\npolynomial in Chebyshev basis by using algorithms from the monomial basis case\nand therefore get the same asymptotic complexity estimate. Our reduction allows\nto use any of these algorithms without converting polynomials input to monomial\nbasis which therefore provide a more direct reduction scheme then the one using\nconversions. We also demonstrate that our reduction is efficient in practice,\nand even outperform the performance of the best known algorithm for Chebyshev\nbasis when polynomials have large degree. Finally, we demonstrate a linear time\nequivalence between the polynomial multiplication problem under monomial basis\nand under Chebyshev basis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 12:53:25 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2013 11:57:03 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Giorgi", "Pascal", "", "LIRMM"]]}, {"id": "1009.5019", "submitter": "Qi Ge", "authors": "Qi Ge, Daniel Stefankovic", "title": "The Complexity of Counting Eulerian Tours in 4-Regular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of counting Eulerian tours ({\\sc #ET}) and its\nvariations from two perspectives---the complexity of exact counting and the\ncomplexity w.r.t. approximation-preserving reductions (AP-reductions\n\\cite{MR2044886}). We prove that {\\sc #ET} is #P-complete even for planar\n4-regular graphs.\n  A closely related problem is that of counting A-trails ({\\sc #A-trails}) in\ngraphs with rotational embedding schemes (so called maps). Kotzig\n\\cite{MR0248043} showed that {\\sc #A-trails} can be computed in polynomial time\nfor 4-regular plane graphs (embedding in the plane is equivalent to giving a\nrotational embedding scheme). We show that for 4-regular maps the problem is\n#P-hard. Moreover, we show that from the approximation viewpoint {\\sc\n#A-trails} in 4-regular maps captures the essence of {\\sc #ET}, that is, we\ngive an AP-reduction from {\\sc #ET} in general graphs to {\\sc #A-trails} in\n4-regular maps. The reduction uses a fast mixing result for a card shuffling\nproblem \\cite{MR2023023}.\n  In order to understand whether #{\\sc A-trails} in 4-regular maps can\nAP-reduce to #{\\sc ET} in 4-regular graphs, we investigate a problem in which\ntransitions in vertices are weighted (this generalizes both #{\\sc A-trails} and\n#{\\sc ET}). In the 4-regular case we show that {\\sc A-trails} can be used to\nsimulate any vertex weights and provide evidence that {\\sc ET} can simulate\nonly a limited set of vertex weights.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 15:40:39 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Ge", "Qi", ""], ["Stefankovic", "Daniel", ""]]}, {"id": "1009.5029", "submitter": "Sophie Toulouse", "authors": "Sophie Toulouse (LIPN), Roberto Wolfler Calvo (LIPN)", "title": "On the complexity of the multiple stack TSP, kSTSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple Stack Travelling Salesman Problem, STSP, deals with the collect\nand the deliverance of n commodities in two distinct cities. The two cities are\nrepresented by means of two edge-valued graphs (G1,d2) and (G2,d2). During the\npick-up tour, the commodities are stored into a container whose rows are\nsubject to LIFO constraints. As a generalisation of standard TSP, the problem\nobviously is NP-hard; nevertheless, one could wonder about what combinatorial\nstructure of STSP does the most impact its complexity: the arrangement of the\ncommodities into the container, or the tours themselves? The answer is not\nclear. First, given a pair (T1,T2) of pick-up and delivery tours, it is\npolynomial to decide whether these tours are or not compatible. Second, for a\ngiven arrangement of the commodities into the k rows of the container, the\noptimum pick-up and delivery tours w.r.t. this arrangement can be computed\nwithin a time that is polynomial in n, but exponential in k. Finally, we\nprovide instances on which a tour that is optimum for one of three distances\nd1, d2 or d1+d2 lead to solutions of STSP that are arbitrarily far to the\noptimum STSP.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 19:41:35 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Toulouse", "Sophie", "", "LIPN"], ["Calvo", "Roberto Wolfler", "", "LIPN"]]}, {"id": "1009.5030", "submitter": "Sophie Toulouse", "authors": "Sophie Toulouse (LIPN)", "title": "Approximability of the Multiple Stack TSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  STSP seeks a pair of pickup and delivery tours in two distinct networks,\nwhere the two tours are related by LIFO contraints. We address here the problem\napproximability. We notably establish that asymmetric MaxSTSP and MinSTSP12 are\nAPX, and propose a heuristic that yields to a 1/2, 3/4 and 3/2 standard\napproximation for respectively Max2STSP, Max2STSP12 and Min2STSP12.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 19:42:20 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Toulouse", "Sophie", "", "LIPN"]]}, {"id": "1009.5104", "submitter": "Scott Aaronson", "authors": "Scott Aaronson", "title": "The Equivalence of Sampling and Searching", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sampling problem, we are given an input x, and asked to sample\napproximately from a probability distribution D_x. In a search problem, we are\ngiven an input x, and asked to find a member of a nonempty set A_x with high\nprobability. (An example is finding a Nash equilibrium.) In this paper, we use\ntools from Kolmogorov complexity and algorithmic information theory to show\nthat sampling and search problems are essentially equivalent. More precisely,\nfor any sampling problem S, there exists a search problem R_S such that, if C\nis any \"reasonable\" complexity class, then R_S is in the search version of C if\nand only if S is in the sampling version. As one application, we show that\nSampP=SampBQP if and only if FBPP=FBQP: in other words, classical computers can\nefficiently sample the output distribution of every quantum circuit, if and\nonly if they can efficiently solve every search problem that quantum computers\ncan solve. A second application is that, assuming a plausible conjecture, there\nexists a search problem R that can be solved using a simple linear-optics\nexperiment, but that cannot be solved efficiently by a classical computer\nunless the polynomial hierarchy collapses. That application will be described\nin a forthcoming paper with Alex Arkhipov on the computational complexity of\nlinear optics.\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 16:09:31 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Aaronson", "Scott", ""]]}, {"id": "1009.5108", "submitter": "Daniil Musatov", "authors": "Daniil Musatov", "title": "Improving the Space-Bounded Version of Muchnik's Conditional Complexity\n  Theorem via \"Naive\" Derandomization", "comments": "14 pages. Presented at CSR'2011, Yandex Best Student Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theorems about Kolmogorov complexity rely on existence of combinatorial\nobjects with specific properties. Usually the probabilistic method gives such\nobjects with better parameters than explicit constructions do. But the\nprobabilistic method does not give \"effective\" variants of such theorems, i.e.\nvariants for resource-bounded Kolmogorov complexity. We show that a \"naive\nderandomization\" approach of replacing these objects by the output of\nNisan-Wigderson pseudo-random generator may give polynomial-space variants of\nsuch theorems.\n  Specifically, we improve the preceding polynomial-space analogue of Muchnik's\nconditional complexity theorem. I.e., for all $a$ and $b$ there exists a\nprogram $p$ of least possible length that transforms $a$ to $b$ and is simple\nconditional on $b$. Here all programs work in polynomial space and all\ncomplexities are measured with logarithmic accuracy instead of polylogarithmic\none in the previous work.\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 17:07:58 GMT"}, {"version": "v2", "created": "Tue, 28 Sep 2010 08:59:25 GMT"}, {"version": "v3", "created": "Tue, 7 Dec 2010 08:48:28 GMT"}, {"version": "v4", "created": "Fri, 9 Mar 2012 20:20:44 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Musatov", "Daniil", ""]]}, {"id": "1009.5435", "submitter": "Guohun Zhu", "authors": "Guohun Zhu", "title": "Determining All Maximum Uniquely Restricted Matching in Bipartite Graphs", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approach mapping from a matching of bipartite graphs to digraphs has been\nsuccessfully used for forcing set problem, in this paper, it is extended to\nuniquely restricted matching problem. We show to determine a uniquely\nrestricted matching in a bipartite graph is equivalent to recognition a acyclic\ndigraph. Based on these results, it proves that determine the bipartite graphs\nwith all maximum matching are uniquely restricted is polynomial time. This\nanswers an open question of Levit and Mandrescu(Discrete Applied Mathematics\n132(2004) 163-164).\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 03:45:38 GMT"}], "update_date": "2010-09-29", "authors_parsed": [["Zhu", "Guohun", ""]]}, {"id": "1009.6105", "submitter": "Oscar Valero", "authors": "M.A. Cerd\\`a-Uguet, M.P. Schellekens, O. Valero", "title": "The Baire partial quasi-metric space: A mathematical tool for asymptotic\n  complexity analysis in Computer Science", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1994, S.G. Matthews introduced the notion of partial metric space in order\nto obtain a suitable mathematical tool for program verification [Ann. New York\nAcad. Sci. 728 (1994), 183-197]. He gave an application of this new structure\nto parallel computing by means of a partial metric version of the celebrated\nBanach fixed point theorem [Theoret. Comput. Sci. 151 (1995), 195-205]. Later\non, M.P. Schellekens introduced the theory of complexity (quasi-metric) spaces\nas a part of the development of a topological foundation for the asymptotic\ncomplexity analysis of programs and algorithms [Elec- tronic Notes in Theoret.\nComput. Sci. 1 (1995), 211-232]. The applicability of this theory to the\nasymptotic complexity analysis of Divide and Conquer algorithms was also\nillustrated by Schellekens. In particular, he gave a new proof, based on the\nuse of the aforenamed Banach fixed point theorem, of the well-known fact that\nMergesort al- gorithm has optimal asymptotic average running time of computing.\nIn this paper, motivated by the utility of partial metrics in Computer Science,\nwe discuss whether the Matthews fixed point theorem is a suitable tool to\nanalyze the asymptotic complexity of algorithms in the spirit of Schellekens.\nSpecifically, we show that a slight modification of the well-known Baire\npartial metric on the set of all words over an alphabet constitutes an\nappropriate tool to carry out the asymptotic complexity analysis of algorithms\nvia fixed point methods without the need for assuming the convergence condition\ninherent to the defini- tion of the complexity space in the Shellekens\nframework. Finally, in order to illustrate and to validate the developed theory\nwe apply our results to analyze the asymptotic complexity of Quicksort,\nMergesort and Largesort algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Sep 2010 11:43:33 GMT"}], "update_date": "2010-10-01", "authors_parsed": [["Cerd\u00e0-Uguet", "M. A.", ""], ["Schellekens", "M. P.", ""], ["Valero", "O.", ""]]}]