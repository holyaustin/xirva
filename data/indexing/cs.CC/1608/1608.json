[{"id": "1608.00127", "submitter": "Xin Li", "authors": "Xin Li", "title": "Improved Non-Malleable Extractors, Non-Malleable Codes and Independent\n  Source Extractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give improved constructions of several central objects in\nthe literature of randomness extraction and tamper-resilient cryptography. Our\nmain results are:\n  (1) An explicit seeded non-malleable extractor with error $\\epsilon$ and seed\nlength $d=O(\\log n)+O(\\log(1/\\epsilon)\\log \\log (1/\\epsilon))$, that supports\nmin-entropy $k=\\Omega(d)$ and outputs $\\Omega(k)$ bits. Combined with the\nprotocol in \\cite{DW09}, this gives a two round privacy amplification protocol\nwith optimal entropy loss in the presence of an active adversary, for all\nsecurity parameters up to $\\Omega(k/\\log k)$.\n  (2) An explicit non-malleable two-source extractor for min-entropy $k \\geq\n(1-\\gamma)n$, some constant $\\gamma>0$, that outputs $\\Omega(k)$ bits with\nerror $2^{-\\Omega(n/\\log n)}$. Combined with the connection in \\cite{CG14b}\nthis gives a non-malleable code in the two-split-state model with relative rate\n$\\Omega(1/\\log n)$. This exponentially improves previous constructions, all of\nwhich only achieve rate $n^{-\\Omega(1)}$.\\footnote{The work of Aggarwal et. al\n\\cite{ADKO15} had a construction which \"achieves\" constant rate, but recently\nthe author found an error in their proof.}\n  (3)A two-source extractor for min-entropy $O(\\log n \\log \\log n)$, which also\nimplies a $K$-Ramsey graph on $N$ vertices with $K=(\\log N)^{O(\\log \\log \\log\nN)}$. We also obtain a seeded non-malleable $9$-source extractor with optimal\nseed length, which in turn gives a $10$-source extractor for min-entropy\n$O(\\log n)$. Previously the best known extractor for such min-entropy requires\n$O(\\log \\log n)$ sources \\cite{CohL16}.\n  Independent of our work, Cohen \\cite{Cohen16} obtained similar results to (1)\nand the two-source extractor, except the dependence on $\\epsilon$ is\n$\\log(1/\\epsilon)(\\log \\log (1/\\epsilon))^{O(1)}$ and the two-source extractor\nrequires min-entropy $\\log n (\\log \\log n)^{O(1)}$.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 14:35:24 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Li", "Xin", ""]]}, {"id": "1608.00135", "submitter": "Amaury Pouly", "authors": "Amaury Pouly", "title": "Computational complexity of solving polynomial differential equations\n  over unbounded domains with non-rational coefficients", "comments": "This is a note that extends arXiv:1409.0451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we extend the result of \\cite{PoulyG16} about the complexity of\nsolving polynomial differential equations over unbounded domains to work with\nnon-rational input. In order to deal with arbitrary input, we phrase the result\nin framework of Conputable Analysis \\cite{Ko91}. As a side result, we also get\na uniform result about complexity of the operator, and not just about the\nsolution.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 15:57:16 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Pouly", "Amaury", ""]]}, {"id": "1608.00180", "submitter": "Venkata Gandikota", "authors": "Karthekeyan Chandrasekaran, Mahdi Cheraghchi, Venkata Gandikota, Elena\n  Grigorescu", "title": "Local Testing for Membership in Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the structural analogies between point lattices and linear\nerror-correcting codes, and by the mature theory on locally testable codes, we\ninitiate a systematic study of local testing for membership in lattices.\nTesting membership in lattices is also motivated in practice, by applications\nto integer programming, error detection in lattice-based communication, and\ncryptography.\n  Apart from establishing the conceptual foundations of lattice testing, our\nresults include the following:\n  1. We demonstrate upper and lower bounds on the query complexity of local\ntesting for the well-known family of code formula lattices. Furthermore, we\ninstantiate our results with code formula lattices constructed from Reed-Muller\ncodes, and obtain nearly-tight bounds.\n  2. We show that in order to achieve low query complexity, it is sufficient to\ndesign one-sided non-adaptive canonical tests. This result is akin to, and\nbased on an analogous result for error-correcting codes due to Ben-Sasson et\nal. (SIAM J. Computing 35(1) pp1-21).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 03:48:22 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Chandrasekaran", "Karthekeyan", ""], ["Cheraghchi", "Mahdi", ""], ["Gandikota", "Venkata", ""], ["Grigorescu", "Elena", ""]]}, {"id": "1608.00417", "submitter": "Abuzer Yakaryilmaz", "authors": "Maksims Dimitrijevs and Abuzer Yakary{\\i}lmaz", "title": "Uncountable classical and quantum complexity classes", "comments": "19 pages. Accepted to NCMA2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial--time constant--space quantum Turing machines (QTMs) and\nlogarithmic--space probabilistic Turing machines (PTMs) recognize uncountably\nmany languages with bounded error (Say and Yakary\\i lmaz 2014,\narXiv:1411.7647). In this paper, we investigate more restricted cases for both\nmodels to recognize uncountably many languages with bounded error. We show that\ndouble logarithmic space is enough for PTMs on unary languages in sweeping\nreading mode or logarithmic space for one-way head. On unary languages, for\nquantum models, we obtain middle logarithmic space for counter machines. For\nbinary languages, arbitrary small non-constant space is enough for PTMs even\nusing only counter as memory. For counter machines, when restricted to\npolynomial time, we can obtain the same result for linear space. For\nconstant--space QTMs, we follow the result for a restricted sweeping head,\nknown as restarting realtime.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 13:20:16 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Dimitrijevs", "Maksims", ""], ["Yakary\u0131lmaz", "Abuzer", ""]]}, {"id": "1608.00497", "submitter": "Madhur Tulsiani", "authors": "Mrinalkanti Ghosh, Madhur Tulsiani", "title": "From Weak to Strong LP Gaps for all CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximability of constraint satisfaction problems (CSPs) by\nlinear programming (LP) relaxations. We show that for every CSP, the\napproximation obtained by a basic LP relaxation, is no weaker than the\napproximation obtained using relaxations given by $\\Omega\\left(\\frac{\\log\nn}{\\log \\log n}\\right)$ levels of the Sherali-Adams hierarchy on instances of\nsize $n$.\n  It was proved by Chan et al. [FOCS 2013] that any polynomial size LP extended\nformulation is no stronger than relaxations obtained by a super-constant levels\nof the Sherali-Adams hierarchy.. Combining this with our result also implies\nthat any polynomial size LP extended formulation is no stronger than the basic\nLP.\n  Using our techniques, we also simplify and strengthen the result by Khot et\nal. [STOC 2014] on (strong) approximation resistance for LPs. They provided a\nnecessary and sufficient condition under which $\\Omega(\\log \\log n)$ levels of\nthe Sherali-Adams hierarchy cannot achieve an approximation better than a\nrandom assignment. We simplify their proof and strengthen the bound to\n$\\Omega\\left(\\frac{\\log n}{\\log \\log n}\\right)$ levels.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:14:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ghosh", "Mrinalkanti", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1608.00692", "submitter": "George Barmpalias Dr", "authors": "George Barmpalias and Rodney G. Downey", "title": "Kobayashi compressibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kobayashi introduced a uniform notion of compressibility of infinite binary\nsequences in terms of relative Turing computations with sub-identity use of the\noracle. Kobayashi compressibility has remained a relatively obscure notion,\nwith the exception of some work on resource bounded Kolmogorov complexity. The\nmain goal of this note is to show that it is relevant to a number of topics in\ncurrent research on algorithmic randomness. We prove that Kobayashi\ncompressibility can be used in order to define Martin-Loef randomness, a strong\nversion of finite randomness and Kurtz randomness, strictly in terms of Turing\nreductions. Moreover these randomness notions naturally correspond to Turing\nreducibility, weak truth-table reducibility and truth-table reducibility\nrespectively. Finally we discuss Kobayashi's main result from his 1981\ntechnical report regarding the compressibility of computably enumerable sets,\nand provide additional related original results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 04:12:08 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 00:17:52 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Barmpalias", "George", ""], ["Downey", "Rodney G.", ""]]}, {"id": "1608.00889", "submitter": "Andrew Ryzhikov", "authors": "Andrew Ryzhikov", "title": "Approximating the Maximum Number of Synchronizing States in Automata", "comments": "8 pages, 1 figure; a mistake in the class of automata in the main\n  result is fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem {\\sc Max Sync Set} of finding a maximum synchronizing\nset of states in a given automaton. We show that the decision version of this\nproblem is PSPACE-complete and investigate the approximability of {\\sc Max Sync\nSet} for binary and weakly acyclic automata (an automaton is called weakly\nacyclic if it contains no cycles other than self-loops). We prove that,\nassuming $P \\ne NP$, for any $\\varepsilon > 0$, the {\\sc Max Sync Set} problem\ncannot be approximated in polynomial time within a factor of $O(n^{1 -\n\\varepsilon})$ for weakly acyclic $n$-state automata with alphabet of linear\nsize, within a factor of $O(n^{\\frac{1}{2} - \\varepsilon})$ for binary\n$n$-state automata, and within a factor of $O(n^{\\frac{1}{3} - \\varepsilon})$\nfor binary weakly acyclic $n$-state automata. Finally, we prove that for unary\nautomata the problem becomes solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:33:21 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 23:04:57 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ryzhikov", "Andrew", ""]]}, {"id": "1608.00931", "submitter": "Roberto Canogar", "authors": "Alberto Borobia, Roberto Canogar", "title": "The real nonnegative inverse eigenvalue problem is NP-hard", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": "10.1016/j.laa.2017.02.010", "report-no": null, "categories": "cs.CC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A list of complex numbers is realizable if it is the spectrum of a\nnonnegative matrix. In 1949 Suleimanova posed the nonnegative inverse\neigenvalue problem (NIEP): the problem of determining which lists of complex\nnumbers are realizable. The version for reals of the NIEP (RNIEP) asks for\nrealizable lists of real numbers. In the literature there are many sufficient\nconditions or criteria for lists of real numbers to be realizable. We will\npresent an unified account of these criteria. Then we will see that the\ndecision problem associated to the RNIEP is NP-hard and we will study the\ncomplexity for the decision problems associated to known criteria.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 11:58:51 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 17:28:17 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Borobia", "Alberto", ""], ["Canogar", "Roberto", ""]]}, {"id": "1608.01292", "submitter": "Marton Naszodi", "authors": "M\\'arton Nasz\\'odi and Alexandr Polyanskii", "title": "Approximating set multi-covers", "comments": "THE TITLE CHANGED! This is the final version. 7 pages", "journal-ref": "European Journal of Combinatorics, Volume 67, January 2018, Pages\n  174-180;", "doi": "10.1016/j.ejc.2017.08.001", "report-no": null, "categories": "math.CO cs.CC math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Johnson and Lov\\'asz and Stein proved independently that any hypergraph\nsatisfies $\\tau\\leq (1+\\ln \\Delta)\\tau^{\\ast}$, where $\\tau$ is the transversal\nnumber, $\\tau^{\\ast}$ is its fractional version, and $\\Delta$ denotes the\nmaximum degree. We prove $\\tau_f\\leq c \\tau^{\\ast}\\max\\{\\ln \\Delta, f\\}$ for\nthe $f$-fold transversal number $\\tau_f$. Similarly to Johnson, Lov\\'asz and\nStein, we also show that this bound can be achieved non-probabilistically,\nusing a greedy algorithm.\n  As a combinatorial application, we prove an estimate on how fast $\\tau_f/f$\nconverges to $\\tau^{\\ast}$. As a geometric application, we obtain an upper\nbound on the minimal density of an $f$-fold covering of the $d$-dimensional\nEuclidean space by translates of any convex body.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 19:12:23 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 07:30:55 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Nasz\u00f3di", "M\u00e1rton", ""], ["Polyanskii", "Alexandr", ""]]}, {"id": "1608.01426", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Solving Laplacian Systems in Logarithmic Space", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the space complexity of solving linear systems of equations.\nWhile all known deterministic or randomized algorithms solving a square system\nof $n$ linear equations in $n$ variables require $\\Omega(\\log^2 n)$ space,\nTa-Shma (STOC 2013) recently showed that on a quantum computer an approximate\nsolution can be computed in logarithmic space, giving the first explicit\ncomputational task for which quantum computation seems to outperform classical\ncomputation with respect to space complexity. In this paper we show that for\nsystems of linear equations in the Laplacian matrix of graphs, the same\nlogarithmic space complexity can actually be achieved by a classical (i.e.,\nnon-quantum) algorithm. More precisely, given a system of linear equations\n$Lx=b$, where $L$ is the (normalized) Laplacian matrix of a graph on $n$\nvertices and $b$ is a unit-norm vector, our algorithm outputs a vector $\\tilde\nx$ such that $\\left\\lVert\\tilde x -x\\right\\rVert\\le 1/\\mathrm{poly}(n)$ and\nuses only $O(\\log n)$ space if the underlying graph has polynomially bounded\nweights. We also show how to estimate, again in logarithmic space, the smallest\nnon-zero eigenvalue of $L$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 04:41:48 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.01594", "submitter": "K. Tuncay Tekle", "authors": "K. Tuncay Tekle, Yanhong A. Liu", "title": "Precise Complexity Guarantees for Pointer Analysis via Datalog with\n  Extensions", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 19 pages,\n  LaTeX", "journal-ref": "Theory and Practice of Logic Programming, Volume 16, Special Issue\n  5-6, September 2016, pp. 916-932", "doi": "10.1017/S1471068416000405", "report-no": null, "categories": "cs.PL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointer analysis is a fundamental static program analysis for computing the\nset of objects that an expression can refer to. Decades of research has gone\ninto developing methods of varying precision and efficiency for pointer\nanalysis for programs that use different language features, but determining\nprecisely how efficient a particular method is has been a challenge in itself.\n  For programs that use different language features, we consider methods for\npointer analysis using Datalog and extensions to Datalog. When the rules are in\nDatalog, we present the calculation of precise time complexities from the rules\nusing a new algorithm for decomposing rules for obtaining the best\ncomplexities. When extensions such as function symbols and universal\nquantification are used, we describe algorithms for efficiently implementing\nthe extensions and the complexities of the algorithms.\n  This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 16:05:13 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 15:46:20 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Tekle", "K. Tuncay", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "1608.01628", "submitter": "Stanislav Zivny", "authors": "David A. Cohen and Martin C. Cooper and Peter G. Jeavons and Andrei\n  Krokhin and Robert Powell and Stanislav Zivny", "title": "Binarisation for Valued Constraint Satisfaction Problems", "comments": "Subsumes 1507.01776", "journal-ref": "SIAM Journal on Discrete Mathematics 31(4) (2017) 2279-2300", "doi": "10.1137/16M1088107", "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for transforming valued constraint satisfaction problems\n(VCSPs) to binary VCSPs. First, we show that the standard dual encoding\npreserves many aspects of the algebraic properties that capture the\ncomputational complexity of VCSPs. Second, we extend the reduction of CSPs to\nbinary CSPs described by Bulin et al. [LMCS'15] to VCSPs. This reduction\nestablishes that VCSPs over a fixed valued constraint language are\npolynomial-time equivalent to Minimum-Cost Homomorphism Problems over a fixed\ndigraph.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 17:55:52 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 20:08:59 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 16:17:07 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Jeavons", "Peter G.", ""], ["Krokhin", "Andrei", ""], ["Powell", "Robert", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1608.01921", "submitter": "Yannik Stein", "authors": "Fr\\'ed\\'eric Meunier, Wolfgang Mulzer, Pauline Sarrabezolles, Yannik\n  Stein", "title": "The Rainbow at the End of the Line --- A PPAD Formulation of the\n  Colorful Carath\\'eodory Theorem with Applications", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $C_1,...,C_{d+1}$ be $d+1$ point sets in $\\mathbb{R}^d$, each containing\nthe origin in its convex hull. A subset $C$ of $\\bigcup_{i=1}^{d+1} C_i$ is\ncalled a colorful choice (or rainbow) for $C_1, \\dots, C_{d+1}$, if it contains\nexactly one point from each set $C_i$. The colorful Carath\\'eodory theorem\nstates that there always exists a colorful choice for $C_1,\\dots,C_{d+1}$ that\nhas the origin in its convex hull. This theorem is very general and can be used\nto prove several other existence theorems in high-dimensional discrete\ngeometry, such as the centerpoint theorem or Tverberg's theorem. The colorful\nCarath\\'eodory problem (CCP) is the computational problem of finding such a\ncolorful choice. Despite several efforts in the past, the computational\ncomplexity of CCP in arbitrary dimension is still open.\n  We show that CCP lies in the intersection of the complexity classes PPAD and\nPLS. This makes it one of the few geometric problems in PPAD and PLS that are\nnot known to be solvable in polynomial time. Moreover, it implies that the\nproblem of computing centerpoints, computing Tverberg partitions, and computing\npoints with large simplicial depth is contained in $\\text{PPAD} \\cap\n\\text{PLS}$. This is the first nontrivial upper bound on the complexity of\nthese problems. Finally, we show that our PPAD formulation leads to a\npolynomial-time algorithm for a special case of CCP in which we have only two\ncolor classes $C_1$ and $C_2$ in $d$ dimensions, each with the origin in its\nconvex hull, and we would like to find a set with half the points from each\ncolor class that contains the origin in its convex hull.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 15:45:28 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Meunier", "Fr\u00e9d\u00e9ric", ""], ["Mulzer", "Wolfgang", ""], ["Sarrabezolles", "Pauline", ""], ["Stein", "Yannik", ""]]}, {"id": "1608.01932", "submitter": "Nathan Grosshans", "authors": "Paul Beame, Nathan Grosshans, Pierre McKenzie and Luc Segoufin", "title": "Nondeterminism and an abstract formulation of Ne\\v{c}iporuk's lower\n  bound method", "comments": null, "journal-ref": null, "doi": "10.1145/3013516", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formulation of \"Ne\\v{c}iporuk's lower bound method\" slightly more inclusive\nthan the usual complexity-measure-specific formulation is presented. Using this\ngeneral formulation, limitations to lower bounds achievable by the method are\nobtained for several computation models, such as branching programs and Boolean\nformulas having access to a sublinear number of nondeterministic bits. In\nparticular, it is shown that any lower bound achievable by the method of\nNe\\v{c}iporuk for the size of nondeterministic and parity branching programs is\nat most $O(n^{3/2}/\\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 16:37:12 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Beame", "Paul", ""], ["Grosshans", "Nathan", ""], ["McKenzie", "Pierre", ""], ["Segoufin", "Luc", ""]]}, {"id": "1608.02003", "submitter": "Nai-Hui Chia", "authors": "Nai-Hui Chia and Sean Hallgren", "title": "How hard is deciding trivial versus nontrivial in the dihedral coset\n  problem?", "comments": "16 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.TQC.2016.89", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the hardness of the dihedral hidden subgroup problem. It is known\nthat lattice problems reduce to it, and that it reduces to random subset sum\nwith density $> 1$ and also to quantum sampling subset sum solutions. We\nexamine a decision version of the problem where the question asks whether the\nhidden subgroup is trivial or order two. The decision problem essentially asks\nif a given vector is in the span of all coset states. We approach this by first\ncomputing an explicit basis for the coset space and the perpendicular space. We\nthen look at the consequences of having efficient unitaries that use this\nbasis. We show that if a unitary maps the basis to the standard basis in any\nway, then that unitary can be used to solve random subset sum with constant\ndensity $>1$. We also show that if a unitary can exactly decide membership in\nthe coset subspace, then the collision problem for subset sum can be solved for\ndensity $>1$ but approaching $1$ as the problem size increases. This\nstrengthens the previous hardness result that implementing the optimal POVM in\na specific way is as hard as quantum sampling subset sum solutions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:01:14 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chia", "Nai-Hui", ""], ["Hallgren", "Sean", ""]]}, {"id": "1608.02081", "submitter": "George Barmpalias Dr", "authors": "George Barmpalias and Andrew Lewis-Pye and Angsheng Li", "title": "Pointed computations and Martin-L\\\"of randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schnorr showed that a real is Martin-Loef random if and only if all of its\ninitial segments are incompressible with respect to prefix-free complexity.\nFortnow and independently Nies, Stephan and Terwijn noticed that this statement\nremains true if we can merely require that the initial segments of the real\ncorresponding to a computable increasing sequence of lengths are\nincompressible. The purpose of this note is to establish the following\ngeneralization of this fact. We show that a real is X Martin-Loef random if and\nonly if its initial segments corresponding to a pointedly X-computable sequence\n(r_n) (where r_n is computable from X in a self-delimiting way, so that at most\nthe first r_n bits of X are queried in the computation) of lengths are\nincompressible. On the other hand we also show that there are reals which are\nvery far from being Martin-Loef random, yet they compute an increasing sequence\nof lengths at which their initial segments are incompressible.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 08:40:02 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 21:14:48 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Barmpalias", "George", ""], ["Lewis-Pye", "Andrew", ""], ["Li", "Angsheng", ""]]}, {"id": "1608.02198", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "A General Characterization of the Statistical Query Complexity", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical query (SQ) algorithms are algorithms that have access to an {\\em\nSQ oracle} for the input distribution $D$ instead of i.i.d.~ samples from $D$.\nGiven a query function $\\phi:X \\rightarrow [-1,1]$, the oracle returns an\nestimate of ${\\bf E}_{ x\\sim D}[\\phi(x)]$ within some tolerance $\\tau_\\phi$\nthat roughly corresponds to the number of samples.\n  In this work we demonstrate that the complexity of solving general problems\nover distributions using SQ algorithms can be captured by a relatively simple\nnotion of statistical dimension that we introduce. SQ algorithms capture a\nbroad spectrum of algorithmic approaches used in theory and practice, most\nnotably, convex optimization techniques. Hence our statistical dimension allows\nto investigate the power of a variety of algorithmic approaches by analyzing a\nsingle linear-algebraic parameter. Such characterizations were investigated\nover the past 20 years in learning theory but prior characterizations are\nrestricted to the much simpler setting of classification problems relative to a\nfixed distribution on the domain (Blum et al., 1994; Bshouty and Feldman, 2002;\nYang, 2001; Simon, 2007; Feldman, 2012; Szorenyi, 2009). Our characterization\nis also the first to precisely characterize the necessary tolerance of queries.\nWe give applications of our techniques to two open problems in learning theory\nand to algorithms that are subject to memory and communication constraints.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 09:35:44 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 19:55:31 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 06:12:23 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1608.02282", "submitter": "Piyush Srivastava", "authors": "Nicholas J. A. Harvey, Piyush Srivastava, Jan Vondr\\'ak", "title": "Computing the Independence Polynomial: from the Tree Threshold down to\n  the Roots", "comments": "35 pages. Extended abstract to appear in Proceedings of ACM-SIAM\n  SODA, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an algorithm for approximating the multivariate independence\npolynomial $Z(\\mathbf{z})$, with negative and complex arguments, an object that\nhas strong connections to combinatorics and to statistical physics. In\nparticular, the independence polynomial with negative arguments,\n$Z(-\\mathbf{p})$, determines the Shearer region, the maximal region of\nprobabilities to which the Lovasz Local Lemma (LLL) can be extended (Shearer\n1985). In statistical physics, complex zeros of the independence polynomial\nrelate to existence of phase transitions.\n  Our main result is a deterministic algorithm to compute approximately the\nindependence polynomial in any root-free complex polydisc centered at the\norigin. Our algorithm is essentially the same as Weitz's algorithm for positive\nparameters up to the tree uniqueness threshold, and the core of our analysis is\na novel multivariate form of the correlation decay technique, which can handle\nnon-uniform complex parameters. In particular, in the univariate real setting\nour work implies that Weitz's algorithm works in an interval between two\ncritical points $(\\lambda'_c(d), \\lambda_c(d))$, and outside of this interval\nan approximation of $Z(\\mathbf{z})$ is known to be NP-hard.\n  As an application, we give a sub-exponential time algorithm for testing\napproximate membership in the Shearer region. We also give a new rounding based\ndeterministic algorithm for Shearer's lemma (an extension of the LLL), which,\nhowever, runs in sub-exponential time. On the hardness side, we prove that\nevaluating $Z(\\mathbf{z})$ at an arbitrary point in Shearer's region, and\ntesting membership in Shearer's region, are #P-hard problems. We also establish\nthe best possible dependence of the exponent of the run time of Weitz's\ncorrelation decay technique in the negative regime on the distance to the\nboundary of the Shearer region.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 23:49:42 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 20:46:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Harvey", "Nicholas J. A.", ""], ["Srivastava", "Piyush", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1608.02374", "submitter": "J\\=anis Iraids", "authors": "Andris Ambainis, J\\=anis Iraids, Daniel Nagaj", "title": "Exact quantum query complexity of $\\rm{EXACT}_{k,l}^n$", "comments": "19 pages, fixed some typos and constraints", "journal-ref": "SOFSEM 2017: Theory and Practice of Computer Science: 43rd\n  International Conference on Current Trends in Theory and Practice of Computer\n  Science, Limerick, Ireland, January 16-20, 2017, Proceedings (pp.243-255)", "doi": "10.1007/978-3-319-51963-0_19", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the exact quantum query model a successful algorithm must always output\nthe correct function value. We investigate the function that is true if exactly\n$k$ or $l$ of the $n$ input bits given by an oracle are 1. We find an optimal\nalgorithm (for some cases), and a nontrivial general lower and upper bound on\nthe minimum number of queries to the black box.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 10:34:45 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 10:43:54 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Ambainis", "Andris", ""], ["Iraids", "J\u0101nis", ""], ["Nagaj", "Daniel", ""]]}, {"id": "1608.02406", "submitter": "Ronald de Haan", "authors": "Ronald de Haan", "title": "Complexity Results for Manipulation, Bribery and Control of the Kemeny\n  Procedure in Judgment Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of several scenarios of strategic\nbehavior for the Kemeny procedure in the setting of judgment aggregation. In\nparticular, we investigate (1) manipulation, where an individual aims to\nachieve a better group outcome by reporting an insincere individual opinion,\n(2) bribery, where an external agent aims to achieve an outcome with certain\nproperties by bribing a number of individuals, and (3) control (by adding or\ndeleting issues), where an external agent aims to achieve an outcome with\ncertain properties by influencing the set of issues in the judgment aggregation\nsituation. We show that determining whether these types of strategic behavior\nare possible (and if so, computing a policy for successful strategic behavior)\nis complete for the second level of the Polynomial Hierarchy. That is, we show\nthat these problems are $\\Sigma^p_2$-complete.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 12:24:05 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["de Haan", "Ronald", ""]]}, {"id": "1608.02530", "submitter": "Luke Oeding", "authors": "Luke Oeding", "title": "Border Ranks of Monomials", "comments": "Completely re-written. Changes are summarized in Remark 1.11", "journal-ref": null, "doi": null, "report-no": "BCSim-2018-s09", "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Young flattenings, introduced by Landsberg and Ottaviani, give determinantal\nequations for secant varieties and their non-vanishing provides lower bounds\nfor border ranks of tensors and in particular polynomials. We study\nmonomial-optimal shapes for Young flattenings, which exhibit the limits of the\nYoung flattening method. In particular, they provide the best possible lower\nbound for large classes of monomials including all monomials up to degree 6,\nmonomials in 3 variables, and any power of the product of variables. On the\nother hand, for degree 7 and higher there are monomials for which no Young\nflattening can give a lower bound that matches the conjecturally tight upper\nbound of Landsberg and Teitler.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 17:49:40 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 16:22:28 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 18:26:03 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Oeding", "Luke", ""]]}, {"id": "1608.02709", "submitter": "Feng Shi", "authors": "Feng Shi, Jianer Chen, Qilong Feng and Jianxin Wang", "title": "Parameterized Algorithms for the Maximum Agreement Forest Problem on\n  Multiple Rooted Multifurcating Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Agreement Forest problem has been extensively studied in\nphylogenetics. Most previous work is on two binary phylogenetic trees. In this\npaper, we study a generalized version of the problem: the Maximum Agreement\nForest problem on multiple rooted multifurcating phylogenetic trees, from the\nperspective of fixed-parameter algorithms. By taking advantage of a new\nbranch-and-bound strategy, two parameterized algorithms, with running times\n$O(2.42^k m^3 n^4)$ and $O(2.74^k m^3 n^5)$, respectively, are presented for\nthe hard version and the soft version of the problem, which correspond to two\ndifferent biological meanings to the polytomies in multifurcating phylogenetic\ntrees.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 07:39:01 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 06:57:11 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Shi", "Feng", ""], ["Chen", "Jianer", ""], ["Feng", "Qilong", ""], ["Wang", "Jianxin", ""]]}, {"id": "1608.03123", "submitter": "Per Kristian Lehre", "authors": "Duc-Cuong Dang and Tobias Friedrich and Timo K\\\"otzing and Martin S.\n  Krejca and Per Kristian Lehre and Pietro S. Oliveto and Dirk Sudholt and\n  Andrew M. Sutton", "title": "Escaping Local Optima using Crossover with Emergent or Reinforced\n  Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population diversity is essential for avoiding premature convergence in\nGenetic Algorithms (GAs) and for the effective use of crossover. Yet the\ndynamics of how diversity emerges in populations are not well understood. We\nuse rigorous runtime analysis to gain insight into population dynamics and GA\nperformance for the ($\\mu$+1) GA and the $\\text{Jump}_k$ test function. We show\nthat the interplay of crossover and mutation may serve as a catalyst leading to\na sudden burst of diversity. This leads to improvements of the expected\noptimisation time of order $\\Omega(n/\\log n)$ compared to mutation-only\nalgorithms like (1+1) EA. Moreover, increasing the mutation rate by an\narbitrarily small constant factor can facilitate the generation of diversity,\nleading to speedups of order $\\Omega(n)$. We also compare seven commonly used\ndiversity mechanisms and evaluate their impact on runtime bounds for the\n($\\mu$+1) GA. All previous results in this context only hold for\nunrealistically low crossover probability $p_c=O(k/n)$, while we give analyses\nfor the setting of constant $p_c < 1$ in all but one case.\n  For the typical case of constant $k > 2$ and constant $p_c$, we can compare\nthe resulting expected runtimes for different diversity mechanisms assuming an\noptimal choice of $\\mu$: $O(n^{k-1})$ for duplicate elimination/minim.,\n$O(n^2\\log n)$ for maximising the convex hull, $O(n\\log n)$ for deterministic\ncrowding (assuming $p_c = k/n$), $O(n\\log n)$ for maximising Hamming distance,\n$O(n\\log n)$ for fitness sharing, $O(n\\log n)$ for single-receiver island\nmodel.\n  This proves a sizeable advantage of all variants of the ($\\mu$+1) GA compared\nto (1+1) EA, which requires time $\\Theta(n^k)$. Experiments complement our\ntheoretical findings and further highlight the benefits of crossover and\ndiversity on $\\text{Jump}_k$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:00:15 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Friedrich", "Tobias", ""], ["K\u00f6tzing", "Timo", ""], ["Krejca", "Martin S.", ""], ["Lehre", "Per Kristian", ""], ["Oliveto", "Pietro S.", ""], ["Sudholt", "Dirk", ""], ["Sutton", "Andrew M.", ""]]}, {"id": "1608.03245", "submitter": "Karthik C. S.", "authors": "Roee David, Karthik C. S., and Bundit Laekhanukit", "title": "On the Complexity of Closest Pair via Polar-Pair of Point-Sets", "comments": "The paper was previously titled, \"The Curse of Medium Dimension for\n  Geometric Problems in Almost Every Norm\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every graph $G$ can be represented by a collection of equi-radii spheres in a\n$d$-dimensional metric $\\Delta$ such that there is an edge $uv$ in $G$ if and\nonly if the spheres corresponding to $u$ and $v$ intersect. The smallest\ninteger $d$ such that $G$ can be represented by a collection of spheres (all of\nthe same radius) in $\\Delta$ is called the sphericity of $G$, and if the\ncollection of spheres are non-overlapping, then the value $d$ is called the\ncontact-dimension of $G$. In this paper, we study the sphericity and contact\ndimension of the complete bipartite graph $K_{n,n}$ in various $L^p$-metrics\nand consequently connect the complexity of the monochromatic closest pair and\nbichromatic closest pair problems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 18:13:00 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2016 19:16:36 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 17:32:11 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 10:39:05 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["David", "Roee", ""], ["S.", "Karthik C.", ""], ["Laekhanukit", "Bundit", ""]]}, {"id": "1608.03313", "submitter": "Atri Rudra", "authors": "Arkadev Chattopadhyay and Michael Langberg and Shi Li and Atri Rudra", "title": "Tight Network Topology Dependent Bounds on Rounds of Communication", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove tight network topology dependent bounds on the round complexity of\ncomputing well studied $k$-party functions such as set disjointness and element\ndistinctness. Unlike the usual case in the CONGEST model in distributed\ncomputing, we fix the function and then vary the underlying network topology.\nThis complements the recent such results on total communication that have\nreceived some attention. We also present some applications to distributed graph\ncomputation problems.\n  Our main contribution is a proof technique that allows us to reduce the\nproblem on a general graph topology to a relevant two-party communication\ncomplexity problem. However, unlike many previous works that also used the same\nhigh level strategy, we do not reason about a two-party communication problem\nthat is induced by a cut in the graph. To `stitch' back the various lower\nbounds from the two party communication problems, we use the notion of timed\ngraph that has seen prior use in network coding. Our reductions use some tools\nfrom Steiner tree packing and multi-commodity flow problems that have a delay\nconstraint.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 23:57:44 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 15:05:03 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Chattopadhyay", "Arkadev", ""], ["Langberg", "Michael", ""], ["Li", "Shi", ""], ["Rudra", "Atri", ""]]}, {"id": "1608.03368", "submitter": "Akbar Rafiey", "authors": "Pavol Hell and Akbar Rafiey and Arash Rafiey", "title": "Bi-Arc Digraphs and Conservative Polymorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the class of bi-arc digraphs, important from two\nseemingly unrelated perspectives. On the one hand, they are precisely the\ndigraphs that admit certain polymorphisms of interest in the study of\nconstraint satisfaction problems; on the other hand, they are a very broad\ngeneralization of interval graphs.\n  Bi-arc digraphs is the class of digraphs that admit conservative semilattice\npolymorphisms. There is much interest in understanding structures that admit\nparticular types of polymorphisms, and especially in their recognition\nalgorithms. (Such problems are referred to as metaproblems.) Surprisingly, the\nclass of bi-arc digraphs also describes the class of digraphs that admit\ncertain other kinds of conservative polymorphisms. Thus solving the recognition\nproblem for bi-arc digraphs solves the metaproblem for digraphs for several\ntypes of conservative polymorphisms. The complexity of the recognition problem\nfor digraphs with conservative semilattice polymorphisms was an open problem,\nwhile it was known to be NP-complete for certain more complex relational\nstructures. We complement our result by providing a complete dichotomy\nclassification of which general relational structures have polynomial or\nNP-complete recognition problems for the existence of conservative semilattice\npolymorphisms.\n  Bi-arc digraphs also generalizes the class of interval graphs; in fact it\nreduces to the class of interval graphs for symmetric and reflexive digraphs.\nIt is much broader than interval graphs and includes other generalizations of\ninterval graphs such as co-threshold tolerance graphs and adjusted interval\ndigraphs. Yet, it is still a reasonable extension of interval graphs, in the\nsense that it keeps much of the appeal of interval graphs.\n  Our main result is a forbidden obstruction characterization of, and a\npolynomial recognition for, the class of bi-arc digraphs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 04:53:53 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 23:57:57 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 02:53:32 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 20:24:57 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2020 09:27:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hell", "Pavol", ""], ["Rafiey", "Akbar", ""], ["Rafiey", "Arash", ""]]}, {"id": "1608.03439", "submitter": "Jesper Nederlof", "authors": "Jesper Nederlof", "title": "Finding Large Set Covers Faster via the Representation Method", "comments": "20 pages, to appear at ESA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worst-case fastest known algorithm for the Set Cover problem on universes\nwith $n$ elements still essentially is the simple $O^*(2^n)$-time dynamic\nprogramming algorithm, and no non-trivial consequences of an $O^*(1.01^n)$-time\nalgorithm are known. Motivated by this chasm, we study the following natural\nquestion: Which instances of Set Cover can we solve faster than the simple\ndynamic programming algorithm? Specifically, we give a Monte Carlo algorithm\nthat determines the existence of a set cover of size $\\sigma n$ in\n$O^*(2^{(1-\\Omega(\\sigma^4))n})$ time. Our approach is also applicable to Set\nCover instances with exponentially many sets: By reducing the task of finding\nthe chromatic number $\\chi(G)$ of a given $n$-vertex graph $G$ to Set Cover in\nthe natural way, we show there is an $O^*(2^{(1-\\Omega(\\sigma^4))n})$-time\nrandomized algorithm that given integer $s=\\sigma n$, outputs NO if $\\chi(G) >\ns$ and YES with constant probability if $\\chi(G)\\leq s-1$.\n  On a high level, our results are inspired by the `representation method' of\nHowgrave-Graham and Joux~[EUROCRYPT'10] and obtained by only evaluating a\nrandomly sampled subset of the table entries of a dynamic programming\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 12:48:39 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Nederlof", "Jesper", ""]]}, {"id": "1608.03580", "submitter": "Erik Waingarten", "authors": "Alexandr Andoni and Thijs Laarhoven and Ilya Razenshteyn and Erik\n  Waingarten", "title": "Optimal Hashing-based Time-Space Trade-offs for Approximate Near\n  Neighbors", "comments": "62 pages, 5 figures; a merger of arXiv:1511.07527 [cs.DS] and\n  arXiv:1605.02701 [cs.DS], which subsumes both of the preprints. New version\n  contains more elaborated proofs and fixed some typos", "journal-ref": null, "doi": "10.1137/1.9781611974782.4", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n$c$-Approximate Near Neighbor Search problem. For the $d$-dimensional Euclidean\nspace and $n$-point datasets, we develop a data structure with space $n^{1 +\n\\rho_u + o(1)} + O(dn)$ and query time $n^{\\rho_q + o(1)} + d n^{o(1)}$ for\nevery $\\rho_u, \\rho_q \\geq 0$ such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor $c > 1$, improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for $\\rho_q = 0$, improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:50:00 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 16:57:47 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Andoni", "Alexandr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1608.03999", "submitter": "William S Zwicker", "authors": "William S. Zwicker", "title": "Cycles and Intractability in a Large Class of Aggregation Rules", "comments": "25 pages, 8 figures. Uses jair.sty style file. Two earlier versions\n  had a slightly different title, \"Cycles and Intractability in Social Choice\n  Theory.\" The first of these did not include Theorem 1.2; it was presented in\n  COMSOC 2016 (Sixth International Workshop on Computational Social Choice).\n  The second version was posted to arXiv, and this is the final (published)\n  version", "journal-ref": "Journal of Artificial Intelligence Research 61 (2018), 407-431", "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the $(j,k)$-Kemeny rule -- a generalization of Kemeny's voting\nrule that aggregates $j$-chotomous weak orders into a $k$-chotomous weak order.\nSpecial cases of $(j,k)$-Kemeny include approval voting, the mean rule and\nBorda mean rule, as well as the Borda count and plurality voting. Why, then, is\nthe winner problem computationally tractable for each of these other rules, but\nintractable for Kemeny? We show that intractability of winner determination for\nthe $(j,k)$-Kemeny rule first appears at the $j=3$, $k=3$ level. The proof\nrests on a reduction of max cut to a related problem on weighted tournaments,\nand reveals that computational complexity arises from the cyclic part in the\nfundamental decomposition of a weighted tournament into cyclic and cocyclic\ncomponents. Thus the existence of majority cycles -- the engine driving both\nArrow's impossibility theorem and the Gibbard-Satterthwaite theorem -- also\nserves as a source of computational complexity in social choice.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 16:20:33 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 19:36:21 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Zwicker", "William S.", ""]]}, {"id": "1608.04112", "submitter": "Vanessa Kosoy", "authors": "Vanessa Kosoy, Alexander Appel", "title": "Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation\n  Algorithm", "comments": "86 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new concept of approximation applicable to decision problems\nand functions, inspired by Bayesian probability. From the perspective of a\nBayesian reasoner with limited computational resources, the answer to a problem\nthat cannot be solved exactly is uncertain and therefore should be described by\na random variable. It thus should make sense to talk about the expected value\nof this random variable, an idea we formalize in the language of average-case\ncomplexity theory by introducing the concept of \"optimal polynomial-time\nestimators.\" We prove some existence theorems and completeness results, and\nshow that optimal polynomial-time estimators exhibit many parallels with\n\"classical\" probability theory.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 15:34:24 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 07:18:29 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 07:32:47 GMT"}, {"version": "v4", "created": "Wed, 28 Dec 2016 19:57:53 GMT"}, {"version": "v5", "created": "Fri, 31 May 2019 18:06:43 GMT"}, {"version": "v6", "created": "Tue, 4 Jun 2019 19:53:27 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Kosoy", "Vanessa", ""], ["Appel", "Alexander", ""]]}, {"id": "1608.04355", "submitter": "Josh Alman", "authors": "Josh Alman, Timothy M. Chan and Ryan Williams", "title": "Polynomial Representations of Threshold Functions and Algorithmic\n  Applications", "comments": "30 pages. To appear in 57th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design new polynomials for representing threshold functions in three\ndifferent regimes: probabilistic polynomials of low degree, which need far less\nrandomness than previous constructions, polynomial threshold functions (PTFs)\nwith \"nice\" threshold behavior and degree almost as low as the probabilistic\npolynomials, and a new notion of probabilistic PTFs where we combine the above\ntechniques to achieve even lower degree with similar \"nice\" threshold behavior.\nUtilizing these polynomial constructions, we design faster algorithms for a\nvariety of problems:\n  $\\bullet$ Offline Hamming Nearest (and Furthest) Neighbors: Given $n$ red and\n$n$ blue points in $d$-dimensional Hamming space for $d=c\\log n$, we can find\nan (exact) nearest (or furthest) blue neighbor for every red point in\nrandomized time $n^{2-1/O(\\sqrt{c}\\log^{2/3}c)}$ or deterministic time\n$n^{2-1/O(c\\log^2c)}$. These also lead to faster MAX-SAT algorithms for sparse\nCNFs.\n  $\\bullet$ Offline Approximate Nearest (and Furthest) Neighbors: Given $n$ red\nand $n$ blue points in $d$-dimensional $\\ell_1$ or Euclidean space, we can find\na $(1+\\epsilon)$-approximate nearest (or furthest) blue neighbor for each red\npoint in randomized time near\n$dn+n^{2-\\Omega(\\epsilon^{1/3}/\\log(1/\\epsilon))}$.\n  $\\bullet$ SAT Algorithms and Lower Bounds for Circuits With Linear Threshold\nFunctions: We give a satisfiability algorithm for $AC^0[m]\\circ LTF\\circ LTF$\ncircuits with a subquadratic number of linear threshold gates on the bottom\nlayer, and a subexponential number of gates on the other layers, that runs in\ndeterministic $2^{n-n^\\epsilon}$ time. This also implies new circuit lower\nbounds for threshold circuits. We also give a randomized\n$2^{n-n^\\epsilon}$-time SAT algorithm for subexponential-size $MAJ\\circ\nAC^0\\circ LTF\\circ AC^0\\circ LTF$ circuits, where the top $MAJ$ gate and middle\n$LTF$ gates have $O(n^{6/5-\\delta})$ fan-in.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:28:47 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alman", "Josh", ""], ["Chan", "Timothy M.", ""], ["Williams", "Ryan", ""]]}, {"id": "1608.04633", "submitter": "Atul Mantri", "authors": "Atul Mantri, Tommaso F. Demarie, Nicolas C. Menicucci and Joseph F.\n  Fitzsimons", "title": "Flow Ambiguity: A Path Towards Classically Driven Blind Quantum\n  Computation", "comments": "(v3) 14 pages, 6 figures. expands introduction and definition of\n  flow, corrects typos to increase readability; contains a new figure to\n  illustrate example run of CDBQC protocol; minor changes to match the\n  published version.(v2) 12 pages, 5 figures. Corrects motivation for\n  quantities used in blindness analysis", "journal-ref": "Phys. Rev. X 7, 031004 (2017)", "doi": "10.1103/PhysRevX.7.031004", "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind quantum computation protocols allow a user to delegate a computation to\na remote quantum computer in such a way that the privacy of their computation\nis preserved, even from the device implementing the computation. To date, such\nprotocols are only known for settings involving at least two quantum devices:\neither a user with some quantum capabilities and a remote quantum server or two\nor more entangled but noncommunicating servers. In this work, we take the first\nstep towards the construction of a blind quantum computing protocol with a\ncompletely classical client and single quantum server. Specifically, we show\nhow a classical client can exploit the ambiguity in the flow of information in\nmeasurement-based quantum computing to construct a protocol for hiding critical\naspects of a computation delegated to a remote quantum computer. This ambiguity\narises due to the fact that, for a fixed graph, there exist multiple choices of\nthe input and output vertex sets that result in deterministic measurement\npatterns consistent with the same fixed total ordering of vertices. This allows\na classical user, computing only measurement angles, to drive a\nmeasurement-based computation performed on a remote device while hiding\ncritical aspects of the computation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:23:00 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 15:41:33 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 14:47:57 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Mantri", "Atul", ""], ["Demarie", "Tommaso F.", ""], ["Menicucci", "Nicolas C.", ""], ["Fitzsimons", "Joseph F.", ""]]}, {"id": "1608.04764", "submitter": "Adam Case", "authors": "Adam Case", "title": "Bounded Turing Reductions and Data Processing Inequalities for Sequences", "comments": "This article is 12 pages. A preliminary version of part of this work\n  was presented at the 11th International Conference on Computability,\n  Complexity, and Randomness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data processing inequality states that the quantity of shared information\nbetween two entities (e.g. signals, strings) cannot be significantly increased\nwhen one of the entities is processed by certain kinds of transformations. In\nthis paper, we prove several data processing inequalities for sequences, where\nthe transformations are bounded Turing functionals and the shared information\nis measured by the lower and upper mutual dimensions between sequences.\n  We show that, for all sequences $X,Y,$ and $Z$, if $Z$ is computable\nLipschitz reducible to $X$, then \\[ mdim(Z:Y) \\leq mdim(X:Y) \\text{ and }\nMdim(Z:Y) \\leq Mdim(X:Y). \\] We also show how to derive different data\nprocessing inequalities by making adjustments to the computable bounds of the\nuse of a Turing functional.\n  The yield of a Turing functional $\\Phi^S$ with access to at most $n$ bits of\nthe oracle $S$ is the smallest input $m \\in \\mathbb{N}$ such that $\\Phi^{S\n\\upharpoonright n}(m)\\uparrow$. We show how to derive reverse data processing\ninequalities (i.e., data processing inequalities where the transformation may\nsignificantly increase the shared information between two entities) for\nsequences by applying computable bounds to the yield of a Turing functional.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:10:23 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Case", "Adam", ""]]}, {"id": "1608.04829", "submitter": "Tomoyuki Morimae", "authors": "Tomoyuki Morimae, Keisuke Fujii, Harumichi Nishimura", "title": "Quantum Merlin-Arthur with noisy channel", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What happens if in QMA the quantum channel between Merlin and Arthur is\nnoisy? It is not difficult to show that such a modification does not change the\ncomputational power as long as the noise is not too strong so that errors are\ncorrectable with high probability, since if Merlin encodes the witness state in\na quantum error-correction code and sends it to Arthur, Arthur can correct the\nerror caused by the noisy channel. If we further assume that Arthur can do only\nsingle-qubit measurements, however, the problem becomes nontrivial, since in\nthis case Arthur cannot do the universal quantum computation by himself. In\nthis paper, we show that such a restricted complexity class is still equivalent\nto QMA. To show it, we use measurement-based quantum computing: honest Merlin\nsends the graph state to Arthur, and Arthur does fault-tolerant\nmeasurement-based quantum computing on the noisy graph state with only\nsingle-qubit measurements. By measuring stabilizer operators, Arthur also\nchecks the correctness of the graph state. Although this idea itself was\nalready used in several previous papers, these results cannot be directly used\nto the present case, since the test that checks the graph state used in these\npapers is so strict that even honest Merlin is rejected with high probability\nif the channel is noisy. We therefore introduce a more relaxed test that can\naccept not only the ideal graph state but also noisy graph states that are\nerror-correctable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 01:33:00 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Morimae", "Tomoyuki", ""], ["Fujii", "Keisuke", ""], ["Nishimura", "Harumichi", ""]]}, {"id": "1608.05043", "submitter": "Sergey Fomin", "authors": "Sergey Fomin, Dima Grigoriev, Dorian Nogneng, Eric Schost", "title": "On semiring complexity of Schur polynomials", "comments": "22 pages, final version, to appear in Computational Complexity.\n  Section 4 rewritten per referee's suggestion, to make the argument more\n  explicit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiring complexity is the version of arithmetic circuit complexity that\nallows only two operations: addition and multiplication. We show that when the\nnumber of variables is fixed, the semiring complexity of a Schur polynomial\n$s_\\lambda$ is $O(log(\\lambda_1))$; here $\\lambda_1$ is the largest part of the\npartition $\\lambda$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 18:47:38 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 16:04:48 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Fomin", "Sergey", ""], ["Grigoriev", "Dima", ""], ["Nogneng", "Dorian", ""], ["Schost", "Eric", ""]]}, {"id": "1608.05358", "submitter": "Stanislav Zivny", "authors": "David A. Cohen, Martin C. Cooper, Peter G. Jeavons, Stanislav Zivny", "title": "Binary Constraint Satisfaction Problems Defined by Excluded Topological\n  Minors", "comments": "Full version of an IJCAI'15 paper", "journal-ref": "Information and Computation 264 12-31 (2019)", "doi": "10.1016/j.ic.2018.09.013", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary Constraint Satisfaction Problem (CSP) is to decide whether there\nexists an assignment to a set of variables which satisfies specified\nconstraints between pairs of variables. A binary CSP instance can be presented\nas a labelled graph encoding both the forms of the constraints and where they\nare imposed. We consider subproblems defined by restricting the allowed form of\nthis graph. One type of restriction that has previously been considered is to\nforbid certain specified substructures (patterns). This captures some tractable\nclasses of the CSP, but does not capture classes defined by language\nrestrictions, or the well-known structural property of acyclicity.\n  In this paper we extend the notion of pattern and introduce the notion of a\ntopological minor of a binary CSP instance. By forbidding a finite set of\npatterns from occurring as topological minors we obtain a compact mechanism for\nexpressing novel tractable subproblems of the binary CSP, including new\ngeneralisations of the class of acyclic instances. Forbidding a finite set of\npatterns as topological minors also captures all other tractable structural\nrestrictions of the binary CSP. Moreover, we show that several patterns give\nrise to tractable subproblems if forbidden as topological minors but not if\nforbidden as sub-patterns. Finally, we introduce the idea of augmented patterns\nthat allows for the identification of more tractable classes, including all\nlanguage restrictions of the binary CSP.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 18:17:38 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 11:35:47 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Jeavons", "Peter G.", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1608.05972", "submitter": "Hector Zenil", "authors": "Hector Zenil, Narsis Kiani and Jesper Tegn\\'er", "title": "Low Algorithmic Complexity Entropy-deceiving Graphs", "comments": "28 pages", "journal-ref": "Phys. Rev. E 96, 012308 (2017)", "doi": "10.1103/PhysRevE.96.012308", "report-no": null, "categories": "cs.IT cs.CC math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In estimating the complexity of objects, in particular of graphs, it is\ncommon practice to rely on graph- and information-theoretic measures. Here,\nusing integer sequences with properties such as Borel normality, we explain how\nthese measures are not independent of the way in which an object, such as a\ngraph, can be described or observed. From observations that can reconstruct the\nsame graph and are therefore essentially translations of the same description,\nwe will see that when applying a computable measure such as Shannon Entropy,\nnot only is it necessary to pre-select a feature of interest where there is\none, and to make an arbitrary selection where there is not, but also that more\ngeneral properties, such as the causal likelihood of a graph as a measure\n(opposed to randomness), can be largely misrepresented by computable measures\nsuch as Entropy and Entropy rate. We introduce recursive and non-recursive\n(uncomputable) graphs and graph constructions based on these integer sequences,\nwhose different lossless descriptions have disparate Entropy values, thereby\nenabling the study and exploration of a measure's range of applications and\ndemonstrating the weaknesses of computable measures of complexity.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 17:37:26 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 20:19:22 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 06:18:11 GMT"}, {"version": "v4", "created": "Sat, 3 Sep 2016 17:16:21 GMT"}, {"version": "v5", "created": "Tue, 11 Oct 2016 20:58:41 GMT"}, {"version": "v6", "created": "Wed, 15 Mar 2017 21:47:54 GMT"}, {"version": "v7", "created": "Sat, 18 Mar 2017 02:13:46 GMT"}, {"version": "v8", "created": "Wed, 10 May 2017 10:46:17 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Zenil", "Hector", ""], ["Kiani", "Narsis", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1608.06113", "submitter": "Jeroen Zuiddam", "authors": "Jop Bri\\\"et, Jeroen Zuiddam", "title": "On the orthogonal rank of Cayley graphs and impossibility of quantum\n  round elimination", "comments": "13 pages", "journal-ref": "Quantum Information and Computation, Vol. 17, No. 1&2 (2017)\n  0106-0116", "doi": null, "report-no": null, "categories": "quant-ph cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After Bob sends Alice a bit, she responds with a lengthy reply. At the cost\nof a factor of two in the total communication, Alice could just as well have\ngiven the two possible replies without listening and have Bob select which\napplies to him. Motivated by a conjecture stating that this form of \"round\nelimination\" is impossible in exact quantum communication complexity, we study\nthe orthogonal rank and a symmetric variant thereof for a certain family of\nCayley graphs. The orthogonal rank of a graph is the smallest number $d$ for\nwhich one can label each vertex with a nonzero $d$-dimensional complex vector\nsuch that adjacent vertices receive orthogonal vectors.\n  We show an exp$(n)$ lower bound on the orthogonal rank of the graph on\n$\\{0,1\\}^n$ in which two strings are adjacent if they have Hamming distance at\nleast $n/2$. In combination with previous work, this implies an affirmative\nanswer to the above conjecture.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 10:33:41 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Bri\u00ebt", "Jop", ""], ["Zuiddam", "Jeroen", ""]]}, {"id": "1608.06318", "submitter": "Thomas Vidick", "authors": "Gil Cohen, Thomas Vidick", "title": "Privacy Amplification Against Active Quantum Adversaries", "comments": "The result is invalidated due to a mistake, pointed out by an\n  anonymous referee, in the use of the Markov condition at the beginning of the\n  proof of Theorem 31", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy amplification is the task by which two cooperating parties transform\na shared weak secret, about which an eavesdropper may have side information,\ninto a uniformly random string uncorrelated from the eavesdropper. Privacy\namplification against passive adversaries, where it is assumed that the\ncommunication is over a public but authenticated channel, can be achieved in\nthe presence of classical as well as quantum side information by a\nsingle-message protocol based on strong extractors.\n  In 2009 Dodis and Wichs devised a two-message protocol to achieve privacy\namplification against active adversaries, where the public communication\nchannel is no longer assumed to be authenticated, through the use of a\nstrengthening of strong extractors called non-malleable extractors which they\nintroduced. Dodis and Wichs only analyzed the case of classical side\ninformation.\n  We consider the task of privacy amplification against active adversaries with\nquantum side information. Our main result is showing that the Dodis-Wichs\nprotocol remains secure in this scenario provided its main building block, the\nnon-malleable extractor, satisfies a notion of quantum-proof non-malleability\nwhich we introduce. We show that an adaptation of a recent construction of\nnon-malleable extractors due to Chattopadhyay et al. is quantum proof, thereby\nproviding the first protocol for privacy amplification that is secure against\nactive quantum adversaries. Our protocol is quantitatively comparable to the\nnear-optimal protocols known in the classical setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 21:33:30 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 18:45:43 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Cohen", "Gil", ""], ["Vidick", "Thomas", ""]]}, {"id": "1608.06580", "submitter": "Aviad Rubinstein", "authors": "Yakov Babichenko and Aviad Rubinstein", "title": "Communication complexity of approximate Nash equilibria", "comments": "Second revision extends the lower bounds to randomized communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a constant $\\epsilon$, we prove a poly(N) lower bound on the (randomized)\ncommunication complexity of $\\epsilon$-Nash equilibrium in two-player NxN\ngames. For n-player binary-action games we prove an exp(n) lower bound for the\n(randomized) communication complexity of $(\\epsilon,\\epsilon)$-weak approximate\nNash equilibrium, which is a profile of mixed actions such that at least\n$(1-\\epsilon)$-fraction of the players are $\\epsilon$-best replying.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 17:07:52 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 17:53:09 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Babichenko", "Yakov", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1608.06724", "submitter": "Qian Li", "authors": "Qian Li and Xiaoming Sun", "title": "On the Sensitivity Complexity of $k$-Uniform Hypergraph Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the sensitivity complexity of hypergraph\nproperties. We present a $k$-uniform hypergraph property with sensitivity\ncomplexity $O(n^{\\lceil k/3\\rceil})$ for any $k\\geq3$, where $n$ is the number\nof vertices. Moreover, we can do better when $k\\equiv1$ (mod 3) by presenting a\n$k$-uniform hypergraph property with sensitivity $O(n^{\\lceil k/3\\rceil-1/2})$.\nThis result disproves a conjecture of Babai~\\cite{Babai}, which conjectures\nthat the sensitivity complexity of $k$-uniform hypergraph properties is at\nleast $\\Omega(n^{k/2})$. We also investigate the sensitivity complexity of\nother symmetric functions and show that for many classes of transitive Boolean\nfunctions the minimum achievable sensitivity complexity can be $O(N^{1/3})$,\nwhere $N$ is the number of variables. Finally, we give a lower bound for\nsensitivity of $k$-uniform hypergraph properties, which implies the {\\em\nsensitivity conjecture} of $k$-uniform hypergraph properties for any constant\n$k$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 06:12:09 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Li", "Qian", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1608.07020", "submitter": "Yasuhiro Takahashi", "authors": "Yasuhiro Takahashi, Seiichiro Tani", "title": "Power of Uninitialized Qubits in Shallow Quantum Circuits", "comments": "23 pages, 10 figures; v3: Theorem 1 improved, title changed, text\n  substantially rewritten", "journal-ref": "Theoretical Computer Science, Vol. 851, pp. 129-153, 2021;\n  Proceedings of the 35th Symposium on Theoretical Aspects of Computer Science\n  (STACS 2018), pp.57:1-57:13, 2018", "doi": "10.1016/j.tcs.2020.11.039", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational power of shallow quantum circuits with $O(\\log n)$\ninitialized and $n^{O(1)}$ uninitialized ancillary qubits, where $n$ is the\ninput length and the initial state of the uninitialized ancillary qubits is\narbitrary. First, we show that such a circuit can compute any symmetric\nfunction on $n$ bits that is classically computable in polynomial time. Then,\nwe regard such a circuit as an oracle and show that a polynomial-time classical\nalgorithm with the oracle can estimate the elements of any unitary matrix\ncorresponding to a constant-depth quantum circuit on $n$ qubits. Since it seems\nunlikely that these tasks can be done with only $O(\\log n)$ initialized\nancillary qubits, our results give evidences that adding uninitialized\nancillary qubits increases the computational power of shallow quantum circuits\nwith only $O(\\log n)$ initialized ancillary qubits. Lastly, to understand the\nlimitations of uninitialized ancillary qubits, we focus on\nnear-logarithmic-depth quantum circuits with them and show the impossibility of\ncomputing the parity function on $n$ bits.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 05:48:44 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 00:38:10 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 07:47:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Takahashi", "Yasuhiro", ""], ["Tani", "Seiichiro", ""]]}, {"id": "1608.07486", "submitter": "J. M. Landsberg", "authors": "J.M. Landsberg and Mateusz Micha{\\l}ek", "title": "A $2n^2-log(n)-1$ lower bound for the border rank of matrix\n  multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let M_n denote the matrix multiplication tensor for nxn matrices. We use the\nborder substitution method combined with Koszul flattenings to prove the border\nrank lower bound of 2n^2-log(n)-1 for M_n.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 15:06:53 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Landsberg", "J. M.", ""], ["Micha\u0142ek", "Mateusz", ""]]}, {"id": "1608.07505", "submitter": "Tilo Wiedera", "authors": "Markus Chimani, Karsten Klein, and Tilo Wiedera", "title": "A Note on the Practicality of Maximal Planar Subgraph Algorithms", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$, the NP-hard Maximum Planar Subgraph problem (MPS) asks for\na planar subgraph of $G$ with the maximum number of edges. There are several\nheuristic, approximative, and exact algorithms to tackle the problem, but---to\nthe best of our knowledge---they have never been compared competitively in\npractice. We report on an exploratory study on the relative merits of the\ndiverse approaches, focusing on practical runtime, solution quality, and\nimplementation complexity. Surprisingly, a seemingly only theoretically strong\napproximation forms the building block of the strongest choice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 16:21:15 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Chimani", "Markus", ""], ["Klein", "Karsten", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1608.07564", "submitter": "Alexey Milovanov", "authors": "Alexey Milovanov", "title": "#P- and $\\oplus$P- completeness of counting roots of a sparse polynomial", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve and simplify the result of the part 4 of \"Counting curves and\ntheir projections\" (Joachim von zur Gathen, Marek Karpinski, Igor Shparlinski)\nby showing that counting roots of a sparse polynomial over $\\mathbb{F}_{2^n}$\nis #P- and $\\oplus$P-complete under deterministic reductions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 16:56:31 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Milovanov", "Alexey", ""]]}, {"id": "1608.07647", "submitter": "Yu Hin Au", "authors": "Yu Hin Au, Levent Tun\\c{c}el", "title": "Elementary polytopes with high lift-and-project ranks for strong\n  positive semidefinite operators", "comments": null, "journal-ref": "Discrete Optimization 27 (2018), 103-129", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider operators acting on convex subsets of the unit hypercube. These\noperators are used in constructing convex relaxations of combinatorial\noptimization problems presented as a 0,1 integer programming problem or a 0,1\npolynomial optimization problem. Our focus is mostly on operators that, when\nexpressed as a lift-and-project operator, involve the use of semidefiniteness\nconstraints in the lifted space, including operators due to Lasserre and\nvariants of the Sherali--Adams and Bienstock--Zuckerberg operators. We study\nthe performance of these semidefinite-optimization-based lift-and-project\noperators on some elementary polytopes --- hypercubes that are chipped (at\nleast one vertex of the hypercube removed by intersection with a closed\nhalfspace) or cropped (all $2^n$ vertices of the hypercube removed by\nintersection with $2^n$ closed halfspaces) to varying degrees of severity\n$\\rho$. We prove bounds on $\\rho$ where these operators would perform badly on\nthe aforementioned examples. We also show that the integrality gap of the\nchipped hypercube is invariant under the application of several\nlift-and-project operators of varying strengths.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 02:20:21 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Au", "Yu Hin", ""], ["Tun\u00e7el", "Levent", ""]]}, {"id": "1608.08679", "submitter": "Andras Farago", "authors": "Andras Farago", "title": "Roughly Polynomial Time: A Concept of Tractability Covering All Known\n  Natural NP-complete Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a concept of efficiency for which we can prove that it applies\nto all paddable languages, but still does not conflict with potential worst\ncase intractability. Note that the family of paddable languages apparently\nincludes all known natural NP-complete problems. We call our concept Roughly\nPolynomial Time (RoughP). A language $L,$ over an at least 2-symbol alphabet,\nis in RoughP, if the following hold: (1) there exists a bijective encoding\n$\\alpha$ of strings, such that both $\\alpha$ and its inverse are computable in\npolynomial time; (2) there is a polynomial time algorithm $\\cal A$, which is an\nerrorless heuristic for $L,$ with exponentially vanishing failure rate relative\nto the $\\alpha$-spheres $S^{(\\alpha)}_n=\\{\\alpha(x)\\,|\\;\\, |x|=n\\}$. It means,\n$\\cal A$ always correctly decides whether $x\\in L$ or $x\\notin L$, whenever it\noutputs a decision. For some inputs, however, it may not output a decision,\nrather it may return a special sign, meaning \"don't know.\" But the latter can\nhappen only on an exponentially small fraction of each $\\alpha$-sphere. We\nprove that RoughP contains all paddable languages. This may contribute to the\nexplanation of the often observed gap between practical algorithm performance\nand theoretical worst case analysis for hard problems. Furthermore, the proof\nalso provides a general method to construct the desired encoding and the\nerrorless heuristic. Additionally, we also show how to use it for efficiently\ngenerating large, random, guaranteed positive and negative test instances for\nany paddable language, including all known natural NP-complete problems. In\nfact, it appears that every practical decision task (whether in NP or not) can\nbe represented by paddable languages, and, therefore, our RoughP framework\napplies to all of them. We also explore some connections between RoughP and\nother complexity classes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 22:47:43 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Farago", "Andras", ""]]}, {"id": "1608.08687", "submitter": "Mario Ullrich", "authors": "Josef Dick, Friedrich Pillichshammer, Kosuke Suzuki, Mario Ullrich,\n  Takehito Yoshiki", "title": "Lattice based integration algorithms: Kronecker sequences and rank-1\n  lattices", "comments": "19 pages", "journal-ref": "Annali di Matematica (2018) 197: 109", "doi": "10.1007/s10231-017-0670-3", "report-no": null, "categories": "math.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove upper bounds on the order of convergence of lattice based algorithms\nfor numerical integration in function spaces of dominating mixed smoothness on\nthe unit cube with homogeneous boundary condition. More precisely, we study\nworst-case integration errors for Besov spaces of dominating mixed smoothness\n$\\mathring{\\mathbf{B}}^s_{p,\\theta}$, which also comprise the concept of\nSobolev spaces of dominating mixed smoothness $\\mathring{\\mathbf{H}}^s_{p}$ as\nspecial cases. The considered algorithms are quasi-Monte Carlo rules with\nunderlying nodes from $T_N(\\mathbb{Z}^d) \\cap [0,1)^d$, where $T_N$ is a real\ninvertible generator matrix of size $d$. For such rules the worst-case error\ncan be bounded in terms of the Zaremba index of the lattice\n$\\mathbb{X}_N=T_N(\\mathbb{Z}^d)$. We apply this result to Kronecker lattices\nand to rank-1 lattice point sets, which both lead to optimal error bounds up to\n$\\log N$-factors for arbitrary smoothness $s$. The advantage of Kronecker\nlattices and classical lattice point sets is that the run-time of algorithms\ngenerating these point sets is very short.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 23:29:23 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Dick", "Josef", ""], ["Pillichshammer", "Friedrich", ""], ["Suzuki", "Kosuke", ""], ["Ullrich", "Mario", ""], ["Yoshiki", "Takehito", ""]]}, {"id": "1608.08704", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz and Jakob Nordstr\\\"om", "title": "Near-Optimal Lower Bounds on Quantifier Depth and Weisfeiler-Leman\n  Refinement Steps", "comments": "This is the full-length version of a paper with the same title which\n  appeared in Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in\n  Computer Science (LICS '16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove near-optimal trade-offs for quantifier depth versus number of\nvariables in first-order logic by exhibiting pairs of $n$-element structures\nthat can be distinguished by a $k$-variable first-order sentence but where\nevery such sentence requires quantifier depth at least $n^{\\Omega(k/\\log k)}$.\nOur trade-offs also apply to first-order counting logic, and by the known\nconnection to the $k$-dimensional Weisfeiler--Leman algorithm imply\nnear-optimal lower bounds on the number of refinement iterations.\n  A key component in our proof is the hardness condensation technique recently\nintroduced by [Razborov '16] in the context of proof complexity. We apply this\nmethod to reduce the domain size of relational structures while maintaining the\nminimal quantifier depth to distinguish them in finite variable logics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 01:58:56 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 14:53:13 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Berkholz", "Christoph", ""], ["Nordstr\u00f6m", "Jakob", ""]]}]