[{"id": "1504.00143", "submitter": "Christoph Aistleitner", "authors": "Christoph Aistleitner", "title": "Fully explicit large deviation inequalities for empirical processes with\n  applications to information-based complexity", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we obtain fully explicit large deviation inequalities\nfor empirical processes indexed by a Vapnik--Chervonenkis class of sets (or\nfunctions). Furthermore we illustrate the importance of such results for the\ntheory of information-based complexity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 08:32:39 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Aistleitner", "Christoph", ""]]}, {"id": "1504.00151", "submitter": "Akihiro Yabe", "authors": "Akihiro Yabe", "title": "Bi-polynomial rank and determinantal complexity", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The permanent vs. determinant problem is one of the most important problems\nin theoretical computer science, and is the main target of geometric complexity\ntheory proposed by Mulmuley and Sohoni. The current best lower bound for the\ndeterminantal complexity of the d by d permanent polynomial is d^2/2, due to\nMignon and Ressayre in 2004. Inspired by their proof method, we introduce a\nnatural rank concept of polynomials, called the bi-polynomial rank. The\nbi-polynomial rank is related to width of an arithmetic branching program. The\nbi-polynomial rank of a homogeneous polynomial p of even degree 2k is defined\nas the minimum n such that p can be written as a summation of n products of\npolynomials of degree k. We prove that the bi-polynomial rank gives a lower\nbound of the determinantal complexity. As a consequence, the above Mignon and\nRessayre bound is improved to (d-1)^2 + 1 over the field of reals. We show that\nthe computation of the bi-polynomial rank is formulated as a rank minimization\nproblem. Applying the concave minimization technique, we reduce the problem of\nlower-bounding determinantal complexity to that of proving the positive\nsemidefiniteness of matrices, and this is a new approach for the permanent vs.\ndeterminant problem. We propose a computational approach for giving a lower\nbound of this rank minimization, via techniques of the concave minimization.\nThis also yields a new strategy to attack the permanent vs. determinant\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 09:03:00 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Yabe", "Akihiro", ""]]}, {"id": "1504.00169", "submitter": "Maximilien Gadouleau", "authors": "Florian Bridoux and Alonso Castillo-Ramirez and Maximilien Gadouleau", "title": "Complete Simulation of Automata Networks", "comments": "Vastly updated version of the paper previously known as \"Universal\n  simulation of automata networks.\" Florian Bridoux has joined the paper,\n  thanks to his significant contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.DM math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a finite set $A$ and an integer $n \\geq 1$. This paper studies the\nconcept of complete simulation in the context of semigroups of transformations\nof $A^n$, also known as finite state-homogeneous automata networks. For $m \\geq\nn$, a transformation of $A^m$ is \\emph{$n$-complete of size $m$} if it may\nsimulate every transformation of $A^n$ by updating one coordinate (or register)\nat a time. Using tools from memoryless computation, it is established that\nthere is no $n$-complete transformation of size $n$, but there is such a\ntransformation of size $n+1$. By studying the the time of simulation of various\n$n$-complete transformations, it is conjectured that the maximal time of\nsimulation of any $n$-complete transformation is at least $2n$. A\ntransformation of $A^m$ is \\emph{sequentially $n$-complete of size $m$} if it\nmay sequentially simulate every finite sequence of transformations of $A^n$; in\nthis case, minimal examples and bounds for the size and time of simulation are\ndetermined. It is also shown that there is no $n$-complete transformation that\nupdates all the registers in parallel, but that there exists a sequentally\n$n$-complete transformation that updates all but one register in parallel. This\nillustrates the strengths and weaknesses of parallel models of computation,\nsuch as cellular automata.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 10:10:05 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 12:53:53 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 18:55:20 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Bridoux", "Florian", ""], ["Castillo-Ramirez", "Alonso", ""], ["Gadouleau", "Maximilien", ""]]}, {"id": "1504.00337", "submitter": "Alejandro Sanchez Guinea", "authors": "Alejandro Sanchez Guinea", "title": "Understanding SAT is in P", "comments": "10 pages, the paper is completely changed from previous versions\n  while the main idea is the same, correctness and time complexity proofs are\n  included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the idea of an understanding with respect to a set of clauses as\na satisfying truth assignment explained by the contexts of the literals in the\nclauses. Following this idea, we present a mechanical process that obtains, if\nit exists, an understanding with respect to a 3-SAT problem instance based on\nthe contexts of each literal in the instance, otherwise it determines that none\nexists. We demonstrate that our process is correct and efficient in solving\n3-SAT.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 18:54:44 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 21:14:53 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 00:39:37 GMT"}, {"version": "v4", "created": "Fri, 16 Sep 2016 13:23:24 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Guinea", "Alejandro Sanchez", ""]]}, {"id": "1504.00442", "submitter": "Peng Cui", "authors": "Peng Cui", "title": "Refuting Unique Game Conjecture", "comments": "6 pages, short note. arXiv admin note: substantial text overlap with\n  arXiv:1401.6520", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, the author shows that the gap problem of some $k$-CSPs\nwith the support of its predicate the ground of a balanced pairwise independent\ndistribution can be solved by a modified version of Hast's Algorithm BiLin that\ncalls Charikar\\&Wirth's SDP algorithm for two rounds in polynomial time, when\n$k$ is sufficiently large, the support of its predicate is combined by the\ngrounds of three biased homogeneous distributions and the three biases satisfy\ncertain conditions. To conclude, the author refutes Unique Game Conjecture,\nassuming $P\\ne NP$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 03:52:43 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 08:18:20 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 08:44:09 GMT"}, {"version": "v4", "created": "Mon, 11 May 2015 09:00:50 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 09:20:17 GMT"}, {"version": "v6", "created": "Thu, 2 Jul 2015 10:28:35 GMT"}, {"version": "v7", "created": "Tue, 11 Aug 2015 06:06:19 GMT"}, {"version": "v8", "created": "Mon, 9 Nov 2015 16:42:57 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Cui", "Peng", ""]]}, {"id": "1504.00572", "submitter": "Mrinal Kumar", "authors": "Swastik Kopparty, Mrinal Kumar, Michael Saks", "title": "Efficient indexing of necklaces and irreducible polynomials over finite\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of indexing irreducible polynomials over finite fields,\nand give the first efficient algorithm for this problem. Specifically, we show\nthe existence of poly(n, log q)-size circuits that compute a bijection between\n{1, ... , |S|} and the set S of all irreducible, monic, univariate polynomials\nof degree n over a finite field F_q. This has applications in pseudorandomness,\nand answers an open question of Alon, Goldreich, H{\\aa}stad and Peralta[AGHP].\n  Our approach uses a connection between irreducible polynomials and necklaces\n( equivalence classes of strings under cyclic rotation). Along the way, we give\nthe first efficient algorithm for indexing necklaces of a given length over a\ngiven alphabet, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 14:40:36 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Kopparty", "Swastik", ""], ["Kumar", "Mrinal", ""], ["Saks", "Michael", ""]]}, {"id": "1504.00681", "submitter": "Alexandra Kolla", "authors": "Guy Kindler, Alexandra Kolla, Luca Trevisan", "title": "Approximation of non-boolean 2CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a polynomial time $\\Omega\\left ( \\frac 1R \\log R \\right)$\napproximate algorithm for Max 2CSP-$R$, the problem where we are given a\ncollection of constraints, each involving two variables, where each variable\nranges over a set of size $R$, and we want to find an assignment to the\nvariables that maximizes the number of satisfied constraints. Assuming the\nUnique Games Conjecture, this is the best possible approximation up to constant\nfactors.\n  Previously, a $1/R$-approximate algorithm was known, based on linear\nprogramming. Our algorithm is based on semidefinite programming (SDP) and on a\nnovel rounding technique. The SDP that we use has an almost-matching\nintegrality gap.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 20:05:58 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 02:55:27 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Kindler", "Guy", ""], ["Kolla", "Alexandra", ""], ["Trevisan", "Luca", ""]]}, {"id": "1504.00695", "submitter": "Oded Lachish Dr", "authors": "Eldar Fischer, Oded Lachish and Yadu Vasudev", "title": "Trading query complexity for sample-based testing and multi-testing\n  scalability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show here that every non-adaptive property testing algorithm making a\nconstant number of queries, over a fixed alphabet, can be converted to a\nsample-based (as per [Goldreich and Ron, 2015]) testing algorithm whose average\nnumber of queries is a fixed, smaller than $1$, power of $n$. Since the query\ndistribution of the sample-based algorithm is not dependent at all on the\nproperty, or the original algorithm, this has many implications in scenarios\nwhere there are many properties that need to be tested for concurrently, such\nas testing (relatively large) unions of properties, or converting a\nMerlin-Arthur Proximity proof (as per [Gur and Rothblum, 2013]) to a proper\ntesting algorithm.\n  The proof method involves preparing the original testing algorithm for a\ncombinatorial analysis, which in turn involves a new result about the existence\nof combinatorial structures (essentially generalized sunflowers) that allow the\nsample-based tester to replace the original constant query complexity tester.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 21:34:01 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Fischer", "Eldar", ""], ["Lachish", "Oded", ""], ["Vasudev", "Yadu", ""]]}, {"id": "1504.00703", "submitter": "Arefin Huq", "authors": "G\\'abor Braun, Jonah Brown-Cohen, Arefin Huq, Sebastian Pokutta,\n  Prasad Raghavendra, Aurko Roy, Benjamin Weitz, Daniel Zink", "title": "The matching problem has no small symmetric SDP", "comments": "18 pages", "journal-ref": "Proceedings of SODA 2016, 1067-1078", "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yannakakis showed that the matching problem does not have a small symmetric\nlinear program. Rothvo{\\ss} recently proved that any, not necessarily\nsymmetric, linear program also has exponential size. It is natural to ask\nwhether the matching problem can be expressed compactly in a framework such as\nsemidefinite programming (SDP) that is more powerful than linear programming\nbut still allows efficient optimization. We answer this question negatively for\nsymmetric SDPs: any symmetric SDP for the matching problem has exponential\nsize.\n  We also show that an O(k)-round Lasserre SDP relaxation for the metric\ntraveling salesperson problem yields at least as good an approximation as any\nsymmetric SDP relaxation of size $n^k$.\n  The key technical ingredient underlying both these results is an upper bound\non the degree needed to derive polynomial identities that hold over the space\nof matchings or traveling salesperson tours.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 22:31:41 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 13:52:09 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2015 22:50:07 GMT"}, {"version": "v4", "created": "Mon, 19 Sep 2016 05:05:23 GMT"}, {"version": "v5", "created": "Wed, 30 Nov 2016 07:21:44 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Brown-Cohen", "Jonah", ""], ["Huq", "Arefin", ""], ["Pokutta", "Sebastian", ""], ["Raghavendra", "Prasad", ""], ["Roy", "Aurko", ""], ["Weitz", "Benjamin", ""], ["Zink", "Daniel", ""]]}, {"id": "1504.00834", "submitter": "Raphael Clifford", "authors": "Raphael Clifford, Markus Jalsenius, Benjamin Sach", "title": "The complexity of computation in bit streams", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the complexity of online computation in the cell probe model. We\nconsider a class of problems where we are first given a fixed pattern or vector\n$F$ of $n$ symbols and then one symbol arrives at a time in a stream. After\neach symbol has arrived we must output some function of $F$ and the $n$-length\nsuffix of the arriving stream. Cell probe bounds of $\\Omega(\\delta\\lg{n}/w)$\nhave previously been shown for both convolution and Hamming distance in this\nsetting, where $\\delta$ is the size of a symbol in bits and\n$w\\in\\Omega(\\lg{n})$ is the cell size in bits. However, when $\\delta$ is a\nconstant, as it is in many natural situations, these previous results no longer\ngive us non-trivial bounds.\n  We introduce a new lop-sided information transfer proof technique which\nenables us to prove meaningful lower bounds even for constant size input\nalphabets. We use our new framework to prove an amortised cell probe lower\nbound of $\\Omega(\\lg^2 n/(w\\cdot \\lg \\lg n))$ time per arriving bit for an\nonline version of a well studied problem known as pattern matching with address\nerrors. This is the first non-trivial cell probe lower bound for any online\nproblem on bit streams that still holds when the cell sizes are large. We also\nshow the same bound for online convolution conditioned on a new combinatorial\nconjecture related to Toeplitz matrices.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 13:10:58 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Sach", "Benjamin", ""]]}, {"id": "1504.01092", "submitter": "Marek Suchenek", "authors": "Marek A. Suchenek", "title": "Technical Notes on Complexity of the Satisfiability Problem", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes contain, among others, a proof that the average running time of\nan easy solution to the satisfiability problem for propositional calculus is,\nunder some reasonable assumptions, linear (with constant 2) in the size of the\ninput. Moreover, some suggestions are made about criteria for tractability of\ncomplex algorithms. In particular, it is argued that the distribution of\nprobability on the whole input space of an algorithm constitutes an\nnon-negligible factor in estimating whether the algorithm is tractable or not.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 07:45:31 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Suchenek", "Marek A.", ""]]}, {"id": "1504.01130", "submitter": "Radu Grigore", "authors": "Maria Bruna, Radu Grigore, Stefan Kiefer, Jo\\\"el Ouaknine, James\n  Worrell", "title": "Proving the Herman-Protocol Conjecture", "comments": "ICALP 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2016.104", "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herman's self-stabilisation algorithm, introduced 25 years ago, is a\nwell-studied synchronous randomised protocol for enabling a ring of $N$\nprocesses collectively holding any odd number of tokens to reach a stable state\nin which a single token remains. Determining the worst-case expected time to\nstabilisation is the central outstanding open problem about this protocol. It\nis known that there is a constant $h$ such that any initial configuration has\nexpected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan\nestablished a lower bound of $4/27 \\approx 0.148$ for $h$, achieved with three\nequally-spaced tokens, and conjectured this to be the optimal value of $h$. A\nseries of papers over the last decade gradually reduced the upper bound on $h$,\nwith the present record (achieved in 2014) standing at approximately $0.156$.\nIn this paper, we prove McIver and Morgan's conjecture and establish that $h =\n4/27$ is indeed optimal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 15:55:55 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 15:17:53 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 22:34:01 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Bruna", "Maria", ""], ["Grigore", "Radu", ""], ["Kiefer", "Stefan", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""]]}, {"id": "1504.01145", "submitter": "Mikhail Babin", "authors": "Mikhail A. Babin and Sergei O. Kuznetsov", "title": "Dualization in Lattices Given by Ordered Sets of Irreducibles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dualization of a monotone Boolean function on a finite lattice can be\nrepresented by transforming the set of its minimal 1 to the set of its maximal\n0 values. In this paper we consider finite lattices given by ordered sets of\ntheir meet and join irreducibles (i.e., as a concept lattice of a formal\ncontext). We show that in this case dualization is equivalent to the\nenumeration of so-called minimal hypotheses. In contrast to usual dualization\nsetting, where a lattice is given by the ordered set of its elements,\ndualization in this case is shown to be impossible in output polynomial time\nunless P = NP. However, if the lattice is distributive, dualization is shown to\nbe possible in subexponential time.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 18:23:50 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 08:25:07 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Babin", "Mikhail A.", ""], ["Kuznetsov", "Sergei O.", ""]]}, {"id": "1504.01167", "submitter": "Can Eren Sezener", "authors": "Can Eren Sezener and Erhan Oztop", "title": "Heuristic algorithms for obtaining Polynomial Threshold Functions with\n  low densities", "comments": "This paper will appear in the 13th Cologne-Twente Workshop on Graphs\n  & Combinatorial Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present several heuristic algorithms, including a Genetic\nAlgorithm (GA), for obtaining polynomial threshold function (PTF)\nrepresentations of Boolean functions (BFs) with small number of monomials. We\ncompare these among each other and against the algorithm of Oztop via\ncomputational experiments. The results indicate that our heuristic algorithms\nfind more parsimonious representations compared to the those of non-heuristic\nand GA-based algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 23:07:24 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Sezener", "Can Eren", ""], ["Oztop", "Erhan", ""]]}, {"id": "1504.01175", "submitter": "Igor Semaev", "authors": "Igor Semaev", "title": "New algorithm for the discrete logarithm problem on elliptic curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC math.AC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithms for computing discrete logarithms on elliptic curves defined\nover finite fields is suggested. It is based on a new method to find zeroes of\nsummation polynomials. In binary elliptic curves one is to solve a cubic system\nof Boolean equations. Under a first fall degree assumption the regularity\ndegree of the system is at most $4$. Extensive experimental data which supports\nthe assumption is provided. An heuristic analysis suggests a new asymptotical\ncomplexity bound $2^{c\\sqrt{n\\ln n}}, c\\approx 1.69$ for computing discrete\nlogarithms on an elliptic curve over a field of size $2^n$. For several binary\nelliptic curves recommended by FIPS the new method performs better than\nPollard's.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 00:19:59 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Semaev", "Igor", ""]]}, {"id": "1504.01339", "submitter": "Robin Kothari", "authors": "Robin Kothari, David Racicot-Desloges, Miklos Santha", "title": "Separating decision tree complexity from subcube partition complexity", "comments": "16 pages, 1 figure", "journal-ref": "Leibniz International Proceedings in Informatics (LIPIcs) 40, pp.\n  915-930 (2015)", "doi": "10.4230/LIPIcs.APPROX-RANDOM.2015.915", "report-no": "MIT-CTP #4663", "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subcube partition model of computation is at least as powerful as\ndecision trees but no separation between these models was known. We show that\nthere exists a function whose deterministic subcube partition complexity is\nasymptotically smaller than its randomized decision tree complexity, resolving\nan open problem of Friedgut, Kahn, and Wigderson (2002). Our lower bound is\nbased on the information-theoretic techniques first introduced to lower bound\nthe randomized decision tree complexity of the recursive majority function.\n  We also show that the public-coin partition bound, the best known lower bound\nmethod for randomized decision tree complexity subsuming other general\ntechniques such as block sensitivity, approximate degree, randomized\ncertificate complexity, and the classical adversary bound, also lower bounds\nrandomized subcube partition complexity. This shows that all these lower bound\ntechniques cannot prove optimal lower bounds for randomized decision tree\ncomplexity, which answers an open question of Jain and Klauck (2010) and Jain,\nLee, and Vishnoi (2014).\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 17:56:16 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Kothari", "Robin", ""], ["Racicot-Desloges", "David", ""], ["Santha", "Miklos", ""]]}, {"id": "1504.01431", "submitter": "Arturs Backurs", "authors": "Amir Abboud, Arturs Backurs, Virginia Vassilevska Williams", "title": "If the Current Clique Algorithms are Optimal, so is Valiant's Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14).\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 22:21:59 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 18:02:03 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1504.01459", "submitter": "Marek Suchenek", "authors": "Marek A. Suchenek", "title": "A Complete Worst-Case Analysis of Heapsort with Experimental\n  Verification of Its Results, A manuscript (MS)", "comments": "115 pages 41 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous proof is presented that the number of comparisons of keys\nperformed in the worst case by ${\\tt Heapsort}$ on any array of size $N \\geq 2$\nis equal to: $ 2 (N-1)\\, ( \\, \\lg \\frac{N-1}{2} +\\varepsilon \\, ) - 2s_2(N) -\ne_2(N) + \\min (\\lfloor \\lg (N-1) \\rfloor, 2) + 6 + c, $ where $ \\varepsilon $,\ngiven by: $\\varepsilon = 1 + \\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1) -\n2^{\\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1)} ,$ is a function of $ N $ with\nthe minimum value 0 and and the supremum value $\\delta = 1 - \\lg e + \\lg \\lg e\n\\approx 0.0860713320559342$, $s_2(N)$ is the sum of all digits of the binary\nrepresentation of $N$, $e_2(N)$ is the exponent of $2$ in the prime\nfactorization of $N$, and $ c $ is a binary function on the set of integers\ndefined by: $c = 1$, if $N \\leq 2 ^{\\lceil \\lg N \\rceil} - 4$, and $c = 0$,\notherwise. An algorithm that generates worst-case input arrays of any size $ N\n\\geq 2 $ for ${\\tt Heapsort}$ is offered. The algorithm has been implemented in\nJava, runs in $O( N \\log N )$ time, and allows for precise experimental\nverification of the above formula.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 02:07:00 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Suchenek", "Marek A.", ""]]}, {"id": "1504.01649", "submitter": "Ishay Haviv", "authors": "Ishay Haviv and Oded Regev", "title": "The List-Decoding Size of Fourier-Sparse Boolean Functions", "comments": "16 pages, CCC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function defined on the Boolean hypercube is $k$-Fourier-sparse if it has\nat most $k$ nonzero Fourier coefficients. For a function $f: \\mathbb{F}_2^n\n\\rightarrow \\mathbb{R}$ and parameters $k$ and $d$, we prove a strong upper\nbound on the number of $k$-Fourier-sparse Boolean functions that disagree with\n$f$ on at most $d$ inputs. Our bound implies that the number of uniform and\nindependent random samples needed for learning the class of $k$-Fourier-sparse\nBoolean functions on $n$ variables exactly is at most $O(n \\cdot k \\log k)$.\n  As an application, we prove an upper bound on the query complexity of testing\nBooleanity of Fourier-sparse functions. Our bound is tight up to a logarithmic\nfactor and quadratically improves on a result due to Gur and Tamuz (Chicago J.\nTheor. Comput. Sci., 2013).\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 15:46:17 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Haviv", "Ishay", ""], ["Regev", "Oded", ""]]}, {"id": "1504.01656", "submitter": "Massimo Lauria", "authors": "Massimo Lauria and Jakob Nordstr\\\"om", "title": "Tight Size-Degree Bounds for Sums-of-Squares Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We exhibit families of $4$-CNF formulas over $n$ variables that have\nsums-of-squares (SOS) proofs of unsatisfiability of degree (a.k.a. rank) $d$\nbut require SOS proofs of size $n^{\\Omega(d)}$ for values of $d = d(n)$ from\nconstant all the way up to $n^{\\delta}$ for some universal constant$\\delta$.\nThis shows that the $n^{O(d)}$ running time obtained by using the Lasserre\nsemidefinite programming relaxations to find degree-$d$ SOS proofs is optimal\nup to constant factors in the exponent. We establish this result by combining\n$\\mathsf{NP}$-reductions expressible as low-degree SOS derivations with the\nidea of relativizing CNF formulas in [Kraj\\'i\\v{c}ek '04] and [Dantchev and\nRiis'03], and then applying a restriction argument as in [Atserias, M\\\"uller,\nand Oliva '13] and [Atserias, Lauria, and Nordstr\\\"om '14]. This yields a\ngeneric method of amplifying SOS degree lower bounds to size lower bounds, and\nalso generalizes the approach in [ALN14] to obtain size lower bounds for the\nproof systems resolution, polynomial calculus, and Sherali-Adams from lower\nbounds on width, degree, and rank, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 16:15:22 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Lauria", "Massimo", ""], ["Nordstr\u00f6m", "Jakob", ""]]}, {"id": "1504.01836", "submitter": "Kasper Green Larsen", "authors": "Raphael Clifford, Allan Gr{\\o}nlund, Kasper Green Larsen", "title": "New Unconditional Hardness Results for Dynamic and Online Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a resurgence of interest in lower bounds whose truth rests on\nthe conjectured hardness of well known computational problems. These\nconditional lower bounds have become important and popular due to the painfully\nslow progress on proving strong unconditional lower bounds. Nevertheless, the\nlong term goal is to replace these conditional bounds with unconditional ones.\nIn this paper we make progress in this direction by studying the cell probe\ncomplexity of two conjectured to be hard problems of particular importance:\nmatrix-vector multiplication and a version of dynamic set disjointness known as\nPatrascu's Multiphase Problem. We give improved unconditional lower bounds for\nthese problems as well as introducing new proof techniques of independent\ninterest. These include a technique capable of proving strong threshold lower\nbounds of the following form: If we insist on having a very fast query time,\nthen the update time has to be slow enough to compute a lookup table with the\nanswer to every possible query. This is the first time a lower bound of this\ntype has been proven.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 06:03:56 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Clifford", "Raphael", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1504.02072", "submitter": "Jingjin Yu", "authors": "Jingjin Yu", "title": "Intractability of Optimal Multi-Robot Path Planning on Planar Graphs", "comments": "Updated draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of optimally solving multi-robot path\nplanning problems on planar graphs. For four common time- and distance-based\nobjectives, we show that the associated path optimization problems for multiple\nrobots are all NP-complete, even when the underlying graph is planar.\nEstablishing the computational intractability of optimal multi-robot path\nplanning problems on planar graphs has important practical implications. In\nparticular, our result suggests the preferred approach toward solving such\nproblems, when the number of robots is large, is to augment the planar\nenvironment to reduce the sharing of paths among robots traveling in opposite\ndirections on those paths. Indeed, such efficiency boosting structures, such as\nhighways and elevated intersections, are ubiquitous in robotics and\ntransportation applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:39:53 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 04:19:08 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 04:29:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Yu", "Jingjin", ""]]}, {"id": "1504.02146", "submitter": "Patrick Lin", "authors": "Lisa Hellerstein, Devorah Kletenik, Patrick Lin", "title": "Discrete Stochastic Submodular Maximization: Adaptive vs. Non-Adaptive\n  vs. Offline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of stochastic monotone submodular function\nmaximization, subject to constraints. We give results on adaptivity gaps, and\non the gap between the optimal offline and online solutions. We present a\nprocedure that transforms a decision tree (adaptive algorithm) into a\nnon-adaptive chain. We prove that this chain achieves at least ${\\tau}$ times\nthe utility of the decision tree, over a product distribution and binary state\nspace, where ${\\tau} = \\min_{i,j} \\Pr[x_i=j]$. This proves an adaptivity gap of\n$1/{\\tau}$ (which is $2$ in the case of a uniform distribution) for the problem\nof stochastic monotone submodular maximization subject to state-independent\nconstraints. For a cardinality constraint, we prove that a simple adaptive\ngreedy algorithm achieves an approximation factor of $(1-1/e^{\\tau})$ with\nrespect to the optimal offline solution; previously, it has been proven that\nthe algorithm achieves an approximation factor of $(1-1/e)$ with respect to the\noptimal adaptive online solution. Finally, we show that there exists a\nnon-adaptive solution for the stochastic max coverage problem that is within a\nfactor $(1-1/e)$ of the optimal adaptive solution and within a factor of\n${\\tau}(1-1/e)$ of the optimal offline solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:26:28 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 22:30:01 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""], ["Lin", "Patrick", ""]]}, {"id": "1504.02411", "submitter": "Aviad Rubinstein", "authors": "Yakov Babichenko and Christos Papadimitriou and Aviad Rubinstein", "title": "Can Almost Everybody be Almost Happy? PCP for PPAD and the\n  Inapproximability of Nash", "comments": "Revision 2 derandomizes the main reduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conjecture that PPAD has a PCP-like complete problem, seeking a near\nequilibrium in which all but very few players have very little incentive to\ndeviate. We show that, if one assumes that this problem requires exponential\ntime, several open problems in this area are settled. The most important\nimplication, proved via a \"birthday repetition\" reduction, is that the n^O(log\nn) approximation scheme of [LMM03] for the Nash equilibrium of two-player games\nis essentially optimum. Two other open problems in the area are resolved once\none assumes this conjecture, establishing that certain approximate equilibria\nare PPAD-complete: Finding a relative approximation of two-player Nash\nequilibria (without the well-supported restriction of [Das13]), and an\napproximate competitive equilibrium with equal incomes [Bud11] with small\nclearing error and near-optimal Gini coefficient.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 18:23:13 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 18:55:30 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Babichenko", "Yakov", ""], ["Papadimitriou", "Christos", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1504.02444", "submitter": "EPTCS", "authors": "Natalia Kushik (Tomsk State University), Nina Yevtushenko (Tomsk State\n  University)", "title": "Adaptive Homing is in P", "comments": "In Proceedings MBT 2015, arXiv:1504.01928", "journal-ref": "EPTCS 180, 2015, pp. 73-78", "doi": "10.4204/EPTCS.180.5", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homing preset and adaptive experiments with Finite State Machines (FSMs) are\nwidely used when a non-initialized discrete event system is given for testing\nand thus, has to be set to the known state at the first step. The length of a\nshortest homing sequence is known to be exponential with respect to the number\nof states for a complete observable nondeterministic FSM while the problem of\nchecking the existence of such sequence (Homing problem) is PSPACE-complete. In\norder to decrease the complexity of related problems, one can consider adaptive\nexperiments when a next input to be applied to a system under experiment\ndepends on the output responses to the previous inputs. In this paper, we study\nthe problem of the existence of an adaptive homing experiment for complete\nobservable nondeterministic machines. We show that if such experiment exists\nthen it can be constructed with the use of a polynomial-time algorithm with\nrespect to the number of FSM states.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 19:30:58 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Kushik", "Natalia", "", "Tomsk State University"], ["Yevtushenko", "Nina", "", "Tomsk State\n  University"]]}, {"id": "1504.03398", "submitter": "Li-Yang Tan", "authors": "Benjamin Rossman and Rocco A. Servedio and Li-Yang Tan", "title": "An average-case depth hierarchy theorem for Boolean circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an average-case depth hierarchy theorem for Boolean circuits over\nthe standard basis of $\\mathsf{AND}$, $\\mathsf{OR}$, and $\\mathsf{NOT}$ gates.\nOur hierarchy theorem says that for every $d \\geq 2$, there is an explicit\n$n$-variable Boolean function $f$, computed by a linear-size depth-$d$ formula,\nwhich is such that any depth-$(d-1)$ circuit that agrees with $f$ on $(1/2 +\no_n(1))$ fraction of all inputs must have size $\\exp({n^{\\Omega(1/d)}}).$ This\nanswers an open question posed by H{\\aa}stad in his Ph.D. thesis.\n  Our average-case depth hierarchy theorem implies that the polynomial\nhierarchy is infinite relative to a random oracle with probability 1,\nconfirming a conjecture of H{\\aa}stad, Cai, and Babai. We also use our result\nto show that there is no \"approximate converse\" to the results of Linial,\nMansour, Nisan and Boppana on the total influence of small-depth circuits, thus\nanswering a question posed by O'Donnell, Kalai, and Hatami.\n  A key ingredient in our proof is a notion of \\emph{random projections} which\ngeneralize random restrictions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 01:44:39 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Rossman", "Benjamin", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1504.03732", "submitter": "J. M. Landsberg", "authors": "J.M. Landsberg, Mateusz Micha{\\l}ek", "title": "Abelian Tensors", "comments": "to appear in JMPA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze tensors in the tensor product of three m-dimensional vector spaces\nsatisfying Strassen's equations for border rank m. Results include: two purely\ngeometric characterizations of the Coppersmith-Winograd tensor, a reduction to\nthe study of symmetric tensors under a mild genericity hypothesis, and numerous\nadditional equations and examples. This study is closely connected to the study\nof the variety of m-dimensional abelian subspaces of the space of endomorphisms\nof an m-dimensional vector space, and the subvariety consisting of the Zariski\nclosure of the variety of maximal tori, called the variety of reductions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 21:42:12 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 20:25:18 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Landsberg", "J. M.", ""], ["Micha\u0142ek", "Mateusz", ""]]}, {"id": "1504.03761", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi and Raphael Jungers", "title": "Lower Bounds on Complexity of Lyapunov Functions for Switched Linear\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.SY math.DS nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any positive integer $d$, there are families of switched\nlinear systems---in fixed dimension and defined by two matrices only---that are\nstable under arbitrary switching but do not admit (i) a polynomial Lyapunov\nfunction of degree $\\leq d$, or (ii) a polytopic Lyapunov function with $\\leq\nd$ facets, or (iii) a piecewise quadratic Lyapunov function with $\\leq d$\npieces. This implies that there cannot be an upper bound on the size of the\nlinear and semidefinite programs that search for such stability certificates.\nSeveral constructive and non-constructive arguments are presented which connect\nour problem to known (and rather classical) results in the literature regarding\nthe finiteness conjecture, undecidability, and non-algebraicity of the joint\nspectral radius. In particular, we show that existence of an extremal piecewise\nalgebraic Lyapunov function implies the finiteness property of the optimal\nproduct, generalizing a result of Lagarias and Wang. As a corollary, we prove\nthat the finiteness property holds for sets of matrices with an extremal\nLyapunov function belonging to some of the most popular function classes in\ncontrols.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 01:46:24 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Jungers", "Raphael", ""]]}, {"id": "1504.03856", "submitter": "Priyanka Mukhopadhyay Ms", "authors": "Priyanka Mukhopadhyay and Youming Qiao", "title": "Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials", "comments": "20 pages; some typos corrected", "journal-ref": "Computational Complexity, 2017 Dec, 26(4), pp. 881-909", "doi": "10.1007/s00037-016-0142-y", "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 10:41:50 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 00:40:07 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2016 22:02:43 GMT"}, {"version": "v4", "created": "Thu, 29 Sep 2016 08:07:02 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Mukhopadhyay", "Priyanka", ""], ["Qiao", "Youming", ""]]}, {"id": "1504.03923", "submitter": "Sangxia Huang", "authors": "Sangxia Huang", "title": "$2^{(\\log N)^{1/10-o(1)}}$ Hardness for Hypergraph Coloring", "comments": "The main theorem of Section 4 in the previous version contains a bug,\n  replaced with a new construction. This gives a weaker hardness of\n  2^{(logn)^{1/10}} than the 2^{(logn)^{1/4}} in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is quasi-NP-hard to color 2-colorable 8-uniform hypergraphs\nwith $2^{(\\log N)^{1/10-o(1)}}$ colors, where $N$ is the number of vertices.\nThere has been much focus on hardness of hypergraph coloring recently.\nGuruswami, H{\\aa}stad, Harsha, Srinivasan and Varma showed that it is\nquasi-NP-hard to color 2-colorable 8-uniform hypergraphs with\n$2^{2^{\\Omega(\\sqrt{\\log\\log N})}}$ colors. Their result is obtained by\ncomposing standard Label Cover with an inner-verifier based on Low Degree Long\nCode, using Reed-Muller code testing results by Dinur and Guruswami. Using a\ndifferent approach, Khot and Saket constructed a new variant of Label Cover,\nand composed it with Quadratic Code to show quasi-NP-hardness of coloring\n2-colorable 12-uniform hypergraphs with $2^{(\\log N)^c}$ colors, for some $c$\naround 1/20. Their construction of Label Cover is based on a new notion of\nsuperposition complexity for CSP instances. The composition with inner-verifier\nwas subsequently improved by Varma, giving the same hardness result for\n8-uniform hypergraphs.\n  Our construction uses both Quadratic Code and Low Degree Long Code, and\nbuilds upon the work by Khot and Saket. We present a different approach to\nconstruct CSP instances with superposition hardness by observing that when the\nnumber of assignments is odd, satisfying a constraint in superposition is the\nsame as \"odd-covering\" the constraint. We employ Low Degree Long Code in order\nto keep the construction efficient. In the analysis, we also adapt and\ngeneralize one of the key theorems by Dinur and Guruswami in the context of\nanalyzing probabilistically checkable proof systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 14:12:58 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 20:19:00 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 07:42:35 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Huang", "Sangxia", ""]]}, {"id": "1504.04103", "submitter": "Ananda Theertha Suresh", "authors": "Moein Falahatgar and Ashkan Jafarpour and Alon Orlitsky and\n  Venkatadheeraj Pichapathi and Ananda Theertha Suresh", "title": "Faster Algorithms for Testing under Conditional Sampling", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent interest in distribution-tests whose\nrun-time and sample requirements are sublinear in the domain-size $k$. We study\ntwo of the most important tests under the conditional-sampling model where each\nquery specifies a subset $S$ of the domain, and the response is a sample drawn\nfrom $S$ according to the underlying distribution.\n  For identity testing, which asks whether the underlying distribution equals a\nspecific given distribution or $\\epsilon$-differs from it, we reduce the known\ntime and sample complexities from $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ to\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, thereby matching the information\ntheoretic lower bound. For closeness testing, which asks whether two\ndistributions underlying observed data sets are equal or different, we reduce\nexisting complexity from $\\tilde{\\mathcal{O}}(\\epsilon^{-4} \\log^5 k)$ to an\neven sub-logarithmic $\\tilde{\\mathcal{O}}(\\epsilon^{-5} \\log \\log k)$ thus\nproviding a better bound to an open problem in Bertinoro Workshop on Sublinear\nAlgorithms [Fisher, 2004].\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 05:56:34 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Falahatgar", "Moein", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Pichapathi", "Venkatadheeraj", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1504.04181", "submitter": "Barnaby Martin", "authors": "Christian Glasser and Peter Jonsson and Barnaby Martin", "title": "Constraint Satisfaction Problems around Skolem Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study interactions between Skolem Arithmetic and certain classes of\nConstraint Satisfaction Problems (CSPs). We revisit results of Glass er et al.\nin the context of CSPs and settle the major open question from that paper,\nfinding a certain satisfaction problem on circuits to be decidable. This we\nprove using the decidability of Skolem Arithmetic. We continue by studying\nfirst-order expansions of Skolem Arithmetic without constants, (N;*), where *\nindicates multiplication, as CSPs. We find already here a rich landscape of\nproblems with non-trivial instances that are in P as well as those that are\nNP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 11:03:52 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Glasser", "Christian", ""], ["Jonsson", "Peter", ""], ["Martin", "Barnaby", ""]]}, {"id": "1504.04675", "submitter": "Sitan Chen", "authors": "Sitan Chen, Thomas Steinke, and Salil Vadhan", "title": "Pseudorandomness for Read-Once, Constant-Depth Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Boolean functions computed by read-once, depth-$D$ circuits with\nunbounded fan-in over the de Morgan basis, we present an explicit pseudorandom\ngenerator with seed length $\\tilde{O}(\\log^{D+1} n)$. The previous best seed\nlength known for this model was $\\tilde{O}(\\log^{D+4} n)$, obtained by Trevisan\nand Xue (CCC `13) for all of $AC^0$ (not just read-once). Our work makes use of\nFourier analytic techniques for pseudorandomness introduced by Reingold,\nSteinke, and Vadhan (RANDOM `13) to show that the generator of Gopalan et al.\n(FOCS `12) fools read-once $AC^0$. To this end, we prove a new Fourier growth\nbound for read-once circuits, namely that for every $F: \\{0,1\\}^n\\to\\{0,1\\}$\ncomputed by a read-once, depth-$D$ circuit,\n\\begin{equation*}\\sum_{s\\subseteq[n], |s|=k}|\\hat{F}[s]|\\le\nO(\\log^{D-1}n)^k,\\end{equation*} where $\\hat{F}$ denotes the Fourier transform\nof $F$ over $\\mathbb{Z}^n_2$.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 02:46:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 15:06:31 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Chen", "Sitan", ""], ["Steinke", "Thomas", ""], ["Vadhan", "Salil", ""]]}, {"id": "1504.04708", "submitter": "Arne Meier", "authors": "Andreas Krebs and Arne Meier and Martin Mundhenk", "title": "The model checking fingerprints of CTL operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to understand the inherent expressive power of CTL\noperators. We investigate the complexity of model checking for all CTL\nfragments with one CTL operator and arbitrary Boolean operators. This gives us\na fingerprint of each CTL operator. The comparison between the fingerprints\nyields a hierarchy of the operators that mirrors their strength with respect to\nmodel checking.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 10:52:04 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 13:51:02 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Krebs", "Andreas", ""], ["Meier", "Arne", ""], ["Mundhenk", "Martin", ""]]}, {"id": "1504.04813", "submitter": "Ilan Komargodski", "authors": "Badih Ghazi, Ilan Komargodski, Pravesh Kothari and Madhu Sudan", "title": "Communication with Contextual Uncertainty", "comments": "20 pages + 1 title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple model illustrating the role of context in communication\nand the challenge posed by uncertainty of knowledge of context. We consider a\nvariant of distributional communication complexity where Alice gets some\ninformation $x$ and Bob gets $y$, where $(x,y)$ is drawn from a known\ndistribution, and Bob wishes to compute some function $g(x,y)$ (with high\nprobability over $(x,y)$). In our variant, Alice does not know $g$, but only\nknows some function $f$ which is an approximation of $g$. Thus, the function\nbeing computed forms the context for the communication, and knowing it\nimperfectly models (mild) uncertainty in this context.\n  A naive solution would be for Alice and Bob to first agree on some common\nfunction $h$ that is close to both $f$ and $g$ and then use a protocol for $h$\nto compute $h(x,y)$. We show that any such agreement leads to a large overhead\nin communication ruling out such a universal solution.\n  In contrast, we show that if $g$ has a one-way communication protocol with\ncomplexity $k$ in the standard setting, then it has a communication protocol\nwith complexity $O(k \\cdot (1+I))$ in the uncertain setting, where $I$ denotes\nthe mutual information between $x$ and $y$. In the particular case where the\ninput distribution is a product distribution, the protocol in the uncertain\nsetting only incurs a constant factor blow-up in communication and error.\n  Furthermore, we show that the dependence on the mutual information $I$ is\nrequired. Namely, we construct a class of functions along with a non-product\ndistribution over $(x,y)$ for which the communication complexity is a single\nbit in the standard setting but at least $\\Omega(\\sqrt{n})$ bits in the\nuncertain setting.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 08:46:16 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 05:52:05 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ghazi", "Badih", ""], ["Komargodski", "Ilan", ""], ["Kothari", "Pravesh", ""], ["Sudan", "Madhu", ""]]}, {"id": "1504.05155", "submitter": "Scott Aaronson", "authors": "Scott Aaronson and Daniel Grier and Luke Schaeffer", "title": "The Classification of Reversible Bit Operations", "comments": "68 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete classification of all possible sets of classical\nreversible gates acting on bits, in terms of which reversible transformations\nthey generate, assuming swaps and ancilla bits are available for free. Our\nclassification can be seen as the reversible-computing analogue of Post's\nlattice, a central result in mathematical logic from the 1940s. It is a step\ntoward the ambitious goal of classifying all possible quantum gate sets acting\non qubits. Our theorem implies a linear-time algorithm (which we have\nimplemented), that takes as input the truth tables of reversible gates G and H,\nand that decides whether G generates H. Previously, this problem was not even\nknown to be decidable. The theorem also implies that any n-bit reversible\ncircuit can be \"compressed\" to an equivalent circuit, over the same gates, that\nuses at most 2^n*poly(n) gates and O(1) ancilla bits; these are the first upper\nbounds on these quantities known, and are close to optimal. Finally, the\ntheorem implies that every non-degenerate reversible gate can implement either\nevery reversible transformation, or every affine transformation, when\nrestricted to an \"encoded subspace.\" Briefly, the theorem says that every set\nof reversible gates generates either all reversible transformations on n-bit\nstrings (as the Toffoli gate does); no transformations; all transformations\nthat preserve Hamming weight (as the Fredkin gate does); all transformations\nthat preserve Hamming weight mod k for some k; all affine transformations (as\nthe Controlled-NOT gate does); all affine transformations that preserve Hamming\nweight mod 2 or mod 4, inner products mod 2, or a combination thereof; or a\nprevious class augmented by a NOT or NOTNOT gate. Ruling out the possibility of\nadditional classes, not in the list, requires some arguments about polynomials,\nlattices, and Diophantine equations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:50:53 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Aaronson", "Scott", ""], ["Grier", "Daniel", ""], ["Schaeffer", "Luke", ""]]}, {"id": "1504.05171", "submitter": "Henry K. Schenck", "authors": "Klim Efremenko, J.M. Landsberg, Hal Schenck and Jerzy Weyman", "title": "On minimal free resolutions of sub-permanents and other ideals arising\n  in complexity theory", "comments": "Prior version split into two parts; portion on method of shifted\n  partials is now arXiv 1609.02103. Title changed to reflect modification. V2:\n  changes suggested by referee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the linear strand of the minimal free resolution of the ideal\ngenerated by k x k sub-permanents of an n x n generic matrix and of the ideal\ngenerated by square-free monomials of degree k. The latter calculation gives\nthe full minimal free resolution by work of Biagioli-Faridi-Rosas. Our\nmotivation is to lay groundwork for the use of commutative algebra in algebraic\ncomplexity theory. We also compute several Hilbert functions relevant for\ncomplexity theory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 19:44:41 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 15:45:01 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 22:51:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Efremenko", "Klim", ""], ["Landsberg", "J. M.", ""], ["Schenck", "Hal", ""], ["Weyman", "Jerzy", ""]]}, {"id": "1504.05240", "submitter": "Meng-Tsung Tsai", "authors": "Martin Farach-Colton and Meng-Tsung Tsai", "title": "On the complexity of computing prime tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large arithmetic computations rely on tables of all primes less than\n$n$. For example, the fastest algorithms for computing $n!$ takes time\n$O(M(n\\log n) + P(n))$, where $M(n)$ is the time to multiply two $n$-bit\nnumbers, and $P(n)$ is the time to compute a prime table up to $n$. The fastest\nalgorithm to compute $\\binom{n}{n/2}$ also uses a prime table. We show that it\ntakes time $O(M(n) + P(n))$.\n  In various models, the best bound on $P(n)$ is greater than $M(n\\log n)$,\ngiven advances in the complexity of multiplication \\cite{Furer07,De08}. In this\npaper, we give two algorithms to computing prime tables and analyze their\ncomplexity on a multitape Turing machine, one of the standard models for\nanalyzing such algorithms. These two algorithms run in time $O(M(n\\log n))$ and\n$O(n\\log^2 n/\\log \\log n)$, respectively. We achieve our results by speeding up\nAtkin's sieve.\n  Given that the current best bound on $M(n)$ is $n\\log n 2^{O(\\log^*n)}$, the\nsecond algorithm is faster and improves on the previous best algorithm by a\nfactor of $\\log^2\\log n$. Our fast prime-table algorithms speed up both the\ncomputation of $n!$ and $\\binom{n}{n/2}$.\n  Finally, we show that computing the factorial takes $\\Omega(M(n \\log^{4/7 -\n\\epsilon} n))$ for any constant $\\epsilon > 0$ assuming only multiplication is\nallowed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 21:34:17 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Farach-Colton", "Martin", ""], ["Tsai", "Meng-Tsung", ""]]}, {"id": "1504.05476", "submitter": "Micha{\\l} Pilipczuk", "authors": "D\\'aniel Marx and Micha{\\l} Pilipczuk", "title": "Optimal parameterized algorithms for planar facility location problems\n  using Voronoi diagrams", "comments": "64 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general family of facility location problems defined on planar\ngraphs and on the 2-dimensional plane. In these problems, a subset of $k$\nobjects has to be selected, satisfying certain packing (disjointness) and\ncovering constraints. Our main result is showing that, for each of these\nproblems, the $n^{O(k)}$ time brute force algorithm of selecting $k$ objects\ncan be improved to $n^{O(\\sqrt{k})}$ time. The algorithm is based on an idea\nthat was introduced recently in the design of geometric QPTASs, but was not yet\nused for exact algorithms and for planar graphs. We focus on the Voronoi\ndiagram of a hypothetical solution of $k$ objects, guess a balanced separator\ncycle of this Voronoi diagram to obtain a set that separates the solution in a\nbalanced way, and then recurse on the resulting subproblems. We complement our\nstudy by giving evidence that packing problems have $n^{O(\\sqrt{k})}$ time\nalgorithms for a much more general class of objects than covering problems\nhave.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:47:32 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1504.05515", "submitter": "Ignasi Sau", "authors": "Julien Baste, Luerbio Faria, Sulamita Klein, Ignasi Sau", "title": "Parameterized complexity dichotomy for $(r,\\ell)$-Vertex Deletion", "comments": "After the first version of this article appeared in arXiv, we learnt\n  that Kolay and Panolan [abs/1504.08120] obtained simultaneously and\n  independently some of the results of this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two integers $r, \\ell \\geq 0$, a graph $G = (V, E)$ is an\n$(r,\\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\\ell$\ncliques. In the parameterized $(r,\\ell)$-Vertex Deletion problem, given a graph\n$G$ and an integer $k$, one has to decide whether at most $k$ vertices can be\nremoved from $G$ to obtain an $(r,\\ell)$-graph. This problem is NP-hard if\n$r+\\ell \\geq 1$ and encompasses several relevant problems such as Vertex Cover\nand Odd Cycle Transversal. The parameterized complexity of $(r,\\ell)$-Vertex\nDeletion was known for all values of $(r,\\ell)$ except for $(2,1)$, $(1,2)$,\nand $(2,2)$. We prove that each of these three cases is FPT and, furthermore,\nsolvable in single-exponential time, which is asymptotically optimal in terms\nof $k$. We consider as well the version of $(r,\\ell)$-Vertex Deletion where the\nset of vertices to be removed has to induce an independent set, and provide\nalso a parameterized complexity dichotomy for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 17:23:06 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 20:56:49 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Baste", "Julien", ""], ["Faria", "Luerbio", ""], ["Klein", "Sulamita", ""], ["Sau", "Ignasi", ""]]}, {"id": "1504.05556", "submitter": "Amey Bhangale", "authors": "Amey Bhangale, Ramprasad Saptharishi, Girish Varma and Rakesh Venkat", "title": "On Fortification of Projection Games", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent result of Moshkovitz \\cite{Moshkovitz14} presented an ingenious\nmethod to provide a completely elementary proof of the Parallel Repetition\nTheorem for certain projection games via a construction called fortification.\nHowever, the construction used in \\cite{Moshkovitz14} to fortify arbitrary\nlabel cover instances using an arbitrary extractor is insufficient to prove\nparallel repetition. In this paper, we provide a fix by using a stronger graph\nthat we call fortifiers. Fortifiers are graphs that have both $\\ell_1$ and\n$\\ell_2$ guarantees on induced distributions from large subsets. We then show\nthat an expander with sufficient spectral gap, or a bi-regular extractor with\nstronger parameters (the latter is also the construction used in an independent\nupdate \\cite{Moshkovitz15} of \\cite{Moshkovitz14} with an alternate argument),\nis a good fortifier. We also show that using a fortifier (in particular\n$\\ell_2$ guarantees) is necessary for obtaining the robustness required for\nfortification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 19:09:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 15:17:53 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Bhangale", "Amey", ""], ["Saptharishi", "Ramprasad", ""], ["Varma", "Girish", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1504.05653", "submitter": "Noga Ron-Zewi", "authors": "Swastik Kopparty, Or Meir, Noga Ron-Zewi, Shubhangi Saraf", "title": "High rate locally-correctable and locally-testable codes with\n  sub-polynomial query complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we construct the first locally-correctable codes (LCCs), and\nlocally-testable codes (LTCs) with constant rate, constant relative distance,\nand sub-polynomial query complexity. Specifically, we show that there exist\nbinary LCCs and LTCs with block length $n$, constant rate (which can even be\ntaken arbitrarily close to 1), constant relative distance, and query complexity\n$\\exp(\\tilde{O}(\\sqrt{\\log n}))$. Previously such codes were known to exist\nonly with $\\Omega(n^{\\beta})$ query complexity (for constant $\\beta > 0$), and\nthere were several, quite different, constructions known.\n  Our codes are based on a general distance-amplification method of Alon and\nLuby~\\cite{AL96_codes}. We show that this method interacts well with local\ncorrectors and testers, and obtain our main results by applying it to suitably\nconstructed LCCs and LTCs in the non-standard regime of \\emph{sub-constant\nrelative distance}.\n  Along the way, we also construct LCCs and LTCs over large alphabets, with the\nsame query complexity $\\exp(\\tilde{O}(\\sqrt{\\log n}))$, which additionally have\nthe property of approaching the Singleton bound: they have almost the\nbest-possible relationship between their rate and distance. This has the\nsurprising consequence that asking for a large alphabet error-correcting code\nto further be an LCC or LTC with $\\exp(\\tilde{O}(\\sqrt{\\log n}))$ query\ncomplexity does not require any sacrifice in terms of rate and distance! Such a\nresult was previously not known for any $o(n)$ query complexity.\n  Our results on LCCs also immediately give locally-decodable codes (LDCs) with\nthe same parameters.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 04:52:50 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Kopparty", "Swastik", ""], ["Meir", "Or", ""], ["Ron-Zewi", "Noga", ""], ["Saraf", "Shubhangi", ""]]}, {"id": "1504.05666", "submitter": "Himanshu Tyagi", "authors": "Himanshu Tyagi and Shaileshh Venkatakrishnan and Pramod Viswanath and\n  Shun Watanabe", "title": "Information Complexity Density and Simulation of Protocols", "comments": "Submitted to the IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two parties observing correlated random variables seek to run an interactive\ncommunication protocol. How many bits must they exchange to simulate the\nprotocol, namely to produce a view with a joint distribution within a fixed\nstatistical distance of the joint distribution of the input and the transcript\nof the original protocol? We present an information spectrum approach for this\nproblem whereby the information complexity of the protocol is replaced by its\ninformation complexity density. Our single-shot bounds relate the communication\ncomplexity of simulating a protocol to tail bounds for information complexity\ndensity. As a consequence, we obtain a strong converse and characterize the\nsecond-order asymptotic term in communication complexity for indepedent and\nidentically distributed observation sequences. Furthermore, we obtain a general\nformula for the rate of communication complexity which applies to any sequence\nof observations and protocols. Connections with results from theoretical\ncomputer science and implications for the function computation problem are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 06:37:45 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 17:16:19 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 15:43:30 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Tyagi", "Himanshu", ""], ["Venkatakrishnan", "Shaileshh", ""], ["Viswanath", "Pramod", ""], ["Watanabe", "Shun", ""]]}, {"id": "1504.05908", "submitter": "Marco Kuhlmann", "authors": "Peter Jonsson and Marco Kuhlmann", "title": "Maximum Pagenumber-k Subgraph is NP-Complete", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ with a total order defined on its vertices, the Maximum\nPagenumber-$k$ Subgraph Problem asks for a maximum subgraph $G'$ of $G$ such\nthat $G'$ can be embedded into a $k$-book when the vertices are placed on the\nspine according to the specified total order. We show that this problem is\nNP-complete for $k \\geq 2$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 06:36:48 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 10:17:25 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Jonsson", "Peter", ""], ["Kuhlmann", "Marco", ""]]}, {"id": "1504.06135", "submitter": "Jonni Virtema", "authors": "Miika Hannula, Juha Kontinen, Jonni Virtema, Heribert Vollmer", "title": "Complexity of Propositional Logics in Team Semantics", "comments": "15 pages + 1 page appendix, the main result is generalised and the\n  title updated to reflect this", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify the computational complexity of the satisfiability, validity and\nmodel-checking problems for propositional independence, inclusion, and team\nlogic. Our main result shows that the satisfiability and validity problems for\npropositional team logic are complete for alternating exponential-time with\npolynomially many alternations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 12:01:22 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 14:01:13 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Hannula", "Miika", ""], ["Kontinen", "Juha", ""], ["Virtema", "Jonni", ""], ["Vollmer", "Heribert", ""]]}, {"id": "1504.06187", "submitter": "Arne Meier", "authors": "Martin L\\\"uck and Arne Meier", "title": "LTL Fragments are Hard for Standard Parameterisations", "comments": "TIME 2015 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify the complexity of the LTL satisfiability and model checking\nproblems for several standard parameterisations. The investigated parameters\nare temporal depth, number of propositional variables and formula treewidth,\nresp., pathwidth. We show that all operator fragments of LTL under the\ninvestigated parameterisations are intractable in the sense of parameterised\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 13:56:05 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 08:36:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["L\u00fcck", "Martin", ""], ["Meier", "Arne", ""]]}, {"id": "1504.06213", "submitter": "Mrinal Kumar", "authors": "Mrinal Kumar and Shubhangi Saraf", "title": "Sums of products of polynomials in few variables : lower bounds and\n  polynomial identity testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of representing polynomials as a sum of products of\npolynomials in few variables. More precisely, we study representations of the\nform $$P = \\sum_{i = 1}^T \\prod_{j = 1}^d Q_{ij}$$ such that each $Q_{ij}$ is\nan arbitrary polynomial that depends on at most $s$ variables. We prove the\nfollowing results.\n  1. Over fields of characteristic zero, for every constant $\\mu$ such that $0\n\\leq \\mu < 1$, we give an explicit family of polynomials $\\{P_{N}\\}$, where\n$P_{N}$ is of degree $n$ in $N = n^{O(1)}$ variables, such that any\nrepresentation of the above type for $P_{N}$ with $s = N^{\\mu}$ requires $Td\n\\geq n^{\\Omega(\\sqrt{n})}$. This strengthens a recent result of Kayal and Saha\n[KS14a] which showed similar lower bounds for the model of sums of products of\nlinear forms in few variables. It is known that any asymptotic improvement in\nthe exponent of the lower bounds (even for $s = \\sqrt{n}$) would separate VP\nand VNP[KS14a].\n  2. We obtain a deterministic subexponential time blackbox polynomial identity\ntesting (PIT) algorithm for circuits computed by the above model when $T$ and\nthe individual degree of each variable in $P$ are at most $\\log^{O(1)} N$ and\n$s \\leq N^{\\mu}$ for any constant $\\mu < 1/2$. We get quasipolynomial running\ntime when $s < \\log^{O(1)} N$. The PIT algorithm is obtained by combining our\nlower bounds with the hardness-randomness tradeoffs developed in [DSY09, KI04].\nTo the best of our knowledge, this is the first nontrivial PIT algorithm for\nthis model (even for the case $s=2$), and the first nontrivial PIT algorithm\nobtained from lower bounds for small depth circuits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 15:02:44 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Kumar", "Mrinal", ""], ["Saraf", "Shubhangi", ""]]}, {"id": "1504.06240", "submitter": "Hector Zenil", "authors": "Fernando Soler-Toscano and Hector Zenil", "title": "A Computable Measure of Algorithmic Probability by Finite Approximations\n  with an Application to Integer Sequences", "comments": "As accepted by the journal Complexity (Wiley/Hindawi)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.FL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the widespread use of lossless compression algorithms to approximate\nalgorithmic (Kolmogorov-Chaitin) complexity, and that lossless compression\nalgorithms fall short at characterizing patterns other than statistical ones\nnot different to entropy estimations, here we explore an alternative and\ncomplementary approach. We study formal properties of a Levin-inspired measure\n$m$ calculated from the output distribution of small Turing machines. We\nintroduce and justify finite approximations $m_k$ that have been used in some\napplications as an alternative to lossless compression algorithms for\napproximating algorithmic (Kolmogorov-Chaitin) complexity. We provide proofs of\nthe relevant properties of both $m$ and $m_k$ and compare them to Levin's\nUniversal Distribution. We provide error estimations of $m_k$ with respect to\n$m$. Finally, we present an application to integer sequences from the Online\nEncyclopedia of Integer Sequences which suggests that our AP-based measures may\ncharacterize non-statistical patterns, and we report interesting correlations\nwith textual, function and program description lengths of the said sequences.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 16:20:58 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 19:28:55 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 14:14:45 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Soler-Toscano", "Fernando", ""], ["Zenil", "Hector", ""]]}, {"id": "1504.06409", "submitter": "Arne Meier", "authors": "Lauri Hella and Antti Kuusisto and Arne Meier and Heribert Vollmer", "title": "Satisfiability of Modal Inclusion Logic: Lax and Strict Semantics", "comments": "Correction of MFCS 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of the satisfiability problem of\nmodal inclusion logic. We distinguish two variants of the problem: one for the\nstrict and another one for the lax semantics. Both problems turn out to be\nEXPTIME-complete on general structures. Finally, we show how for a specific\nclass of structures NEXPTIME-completeness for these problems under strict\nsemantics can be achieved.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 07:28:02 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 13:10:37 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 10:40:35 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Hella", "Lauri", ""], ["Kuusisto", "Antti", ""], ["Meier", "Arne", ""], ["Vollmer", "Heribert", ""]]}, {"id": "1504.06602", "submitter": "Atri Rudra", "authors": "Arkadev Chattopadhyay and Atri Rudra", "title": "The Range of Topological Effects on Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue the study of communication cost of computing functions when\ninputs are distributed among $k$ processors, each of which is located at one\nvertex of a network/graph called a terminal. Every other node of the network\nalso has a processor, with no input. The communication is point-to-point and\nthe cost is the total number of bits exchanged by the protocol, in the worst\ncase, on all edges.\n  Chattopadhyay, Radhakrishnan and Rudra (FOCS'14) recently initiated a study\nof the effect of topology of the network on the total communication cost using\ntools from $L_1$ embeddings. Their techniques provided tight bounds for simple\nfunctions like Element-Distinctness (ED), which depend on the 1-median of the\ngraph. This work addresses two other kinds of natural functions. We show that\nfor a large class of natural functions like Set-Disjointness the communication\ncost is essentially $n$ times the cost of the optimal Steiner tree connecting\nthe terminals. Further, we show for natural composed functions like $\\text{ED}\n\\circ \\text{XOR}$ and $\\text{XOR} \\circ \\text{ED}$, the naive protocols\nsuggested by their definition is optimal for general networks. Interestingly,\nthe bounds for these functions depend on more involved topological parameters\nthat are a combination of Steiner tree and 1-median costs.\n  To obtain our results, we use some new tools in addition to ones used in\nChattopadhyay et. al. These include (i) viewing the communication constraints\nvia a linear program; (ii) using tools from the theory of tree embeddings to\nprove topology sensitive direct sum results that handle the case of composed\nfunctions and (iii) representing the communication constraints of certain\nproblems as a family of collection of multiway cuts, where each multiway cut\nsimulates the hardness of computing the function on the star topology.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 19:12:30 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Chattopadhyay", "Arkadev", ""], ["Rudra", "Atri", ""]]}, {"id": "1504.06731", "submitter": "Hiroki Morizumi", "authors": "Hiroki Morizumi", "title": "Lower Bounds for the Size of Nondeterministic Circuits", "comments": "the submitted version to COCOON'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondeterministic circuits are a nondeterministic computation model in circuit\ncomplexity theory. In this paper, we prove a $3(n-1)$ lower bound for the size\nof nondeterministic $U_2$-circuits computing the parity function. It is known\nthat the minimum size of (deterministic) $U_2$-circuits computing the parity\nfunction exactly equals $3(n-1)$. Thus, our result means that nondeterministic\ncomputation is useless to compute the parity function by $U_2$-circuits and\ncannot reduce the size from $3(n-1)$. To the best of our knowledge, this is the\nfirst nontrivial lower bound for the size of nondeterministic circuits\n(including formulas, constant depth circuits, and so on) with unlimited\nnondeterminism for an explicit Boolean function. We also discuss an approach to\nproving lower bounds for the size of deterministic circuits via lower bounds\nfor the size of nondeterministic restricted circuits.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 14:32:56 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Morizumi", "Hiroki", ""]]}, {"id": "1504.06830", "submitter": "Omri Weinstein", "authors": "Omri Weinstein", "title": "Information Complexity and the Quest for Interactive Compression (A\n  Survey)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information complexity is the interactive analogue of Shannon's classical\ninformation theory. In recent years this field has emerged as a powerful tool\nfor proving strong communication lower bounds, and for addressing some of the\nmajor open problems in communication complexity and circuit complexity. A\nnotable achievement of information complexity is the breakthrough in\nunderstanding of the fundamental direct sum and direct product conjectures,\nwhich aim to quantify the power of parallel computation. This survey provides a\nbrief introduction to information complexity, and overviews some of the recent\nprogress on these conjectures and their tight relationship with the fascinating\nproblem of compressing interactive protocols.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:30:05 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Weinstein", "Omri", ""]]}, {"id": "1504.06890", "submitter": "Maria Janczak", "authors": "Hector A. Cardenas, Chester Holtz, Maria Janczak, Philip Meyers and\n  Nathaniel S. Potrepka", "title": "A Refutation of the Clique-Based P=NP Proofs of LaPlante and\n  Tamta-Pande-Dhami", "comments": "14 pages, 11 figures arXiv:1403.1178v1 [cs.DS] arXiv:1503.04794v1\n  [cs.DS]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we critique two papers, \"A Polynomial-Time Solution to the\nClique Problem\" by Tamta, Pande, and Dhami, and \"A Polynomial-Time Algorithm\nFor Solving Clique Problems\" by LaPlante. We summarize and analyze both papers,\nnoting that the algorithms presented in both papers are flawed. We conclude\nthat neither author has successfully established that P = NP.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 22:51:32 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Cardenas", "Hector A.", ""], ["Holtz", "Chester", ""], ["Janczak", "Maria", ""], ["Meyers", "Philip", ""], ["Potrepka", "Nathaniel S.", ""]]}, {"id": "1504.07067", "submitter": "Rustem Takhanov", "authors": "Vladimir Kolmogorov, Michal Rolinek, Rustem Takhanov", "title": "Effectiveness of Structural Restrictions for Hybrid CSPs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Satisfaction Problem (CSP) is a fundamental algorithmic problem\nthat appears in many areas of Computer Science. It can be equivalently stated\nas computing a homomorphism $\\mbox{$\\bR \\rightarrow \\bGamma$}$ between two\nrelational structures, e.g.\\ between two directed graphs. Analyzing its\ncomplexity has been a prominent research direction, especially for {\\em fixed\ntemplate CSPs} in which the right side $\\bGamma$ is fixed and the left side\n$\\bR$ is unconstrained.\n  Far fewer results are known for the {\\em hybrid} setting that restricts both\nsides simultaneously. It assumes that $\\bR$ belongs to a certain class of\nrelational structures (called a {\\em structural restriction} in this paper). We\nstudy which structural restrictions are {\\em effective}, i.e.\\ there exists a\nfixed template $\\bGamma$ (from a certain class of languages) for which the\nproblem is tractable when $\\bR$ is restricted, and NP-hard otherwise. We\nprovide a characterization for structural restrictions that are {\\em closed\nunder inverse homomorphisms}. The criterion is based on the {\\em chromatic\nnumber} of a relational structure defined in this paper; it generalizes the\nstandard chromatic number of a graph.\n  As our main tool, we use the algebraic machinery developed for fixed template\nCSPs. To apply it to our case, we introduce a new construction called a \"lifted\nlanguage.\" We also give a characterization for structural restrictions\ncorresponding to minor-closed families of graphs, extend results to certain\nValued CSPs (namely conservative valued languages), and state implications for\nCSPs with ordered variables, (valued) CSPs on structures with large girth, and\nfor the maximum weight independent set problem on some restricted families of\ngraphs including graphs with large girth.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:07:15 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 07:02:31 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2015 08:32:20 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Kolmogorov", "Vladimir", ""], ["Rolinek", "Michal", ""], ["Takhanov", "Rustem", ""]]}, {"id": "1504.07648", "submitter": "Mahdi Cheraghchi", "authors": "Mahdi Cheraghchi, Piotr Indyk", "title": "Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard\n  Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.LG math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every fixed constant $\\alpha > 0$, we design an algorithm for computing\nthe $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \\in\n\\mathbb{R}^N$ in time $k^{1+\\alpha} (\\log N)^{O(1)}$. Specifically, the\nalgorithm is given query access to $x$ and computes a $k$-sparse $\\tilde{x} \\in\n\\mathbb{R}^N$ satisfying $\\|\\tilde{x} - \\hat{x}\\|_1 \\leq c \\|\\hat{x} -\nH_k(\\hat{x})\\|_1$, for an absolute constant $c > 0$, where $\\hat{x}$ is the\ntransform of $x$ and $H_k(\\hat{x})$ is its best $k$-sparse approximation. Our\nalgorithm is fully deterministic and only uses non-adaptive queries to $x$\n(i.e., all queries are determined and performed in parallel when the algorithm\nstarts).\n  An important technical tool that we use is a construction of nearly optimal\nand linear lossless condensers which is a careful instantiation of the GUV\ncondenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a\ndeterministic and non-adaptive $\\ell_1/\\ell_1$ compressed sensing scheme based\non general lossless condensers that is equipped with a fast reconstruction\nalgorithm running in time $k^{1+\\alpha} (\\log N)^{O(1)}$ (for the GUV-based\ncondenser) and is of independent interest. Our scheme significantly simplifies\nand improves an earlier expander-based construction due to Berinde, Gilbert,\nIndyk, Karloff, Strauss (Allerton 2008).\n  Our methods use linear lossless condensers in a black box fashion; therefore,\nany future improvement on explicit constructions of such condensers would\nimmediately translate to improved parameters in our framework (potentially\nleading to $k (\\log N)^{O(1)}$ reconstruction time with a reduced exponent in\nthe poly-logarithmic factor, and eliminating the extra parameter $\\alpha$).\n  Finally, by allowing the algorithm to use randomness, while still using\nnon-adaptive queries, the running time of the algorithm can be improved to\n$\\tilde{O}(k \\log^3 N)$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 20:22:27 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Cheraghchi", "Mahdi", ""], ["Indyk", "Piotr", ""]]}, {"id": "1504.07687", "submitter": "Parikshit Gopalan", "authors": "Parikshit Gopalan, Noam Nisan, Tim Roughgarden", "title": "Public projects, Boolean functions and the borders of Border's theorem", "comments": "Accepted to ACM EC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Border's theorem gives an intuitive linear characterization of the feasible\ninterim allocation rules of a Bayesian single-item environment, and it has\nseveral applications in economic and algorithmic mechanism design. All known\ngeneralizations of Border's theorem either restrict attention to relatively\nsimple settings, or resort to approximation. This paper identifies a\ncomplexity-theoretic barrier that indicates, assuming standard complexity class\nseparations, that Border's theorem cannot be extended significantly beyond the\nstate-of-the-art. We also identify a surprisingly tight connection between\nMyerson's optimal auction theory, when applied to public project settings, and\nsome fundamental results in the analysis of Boolean functions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 00:24:39 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Nisan", "Noam", ""], ["Roughgarden", "Tim", ""]]}, {"id": "1504.07697", "submitter": "Anand Kumar Narayanan", "authors": "Anand Kumar Narayanan", "title": "Polynomial Factorization over Finite Fields By Computing Euler-Poincare\n  Characteristics of Drinfeld Modules", "comments": "Proof of theorem 1.4 and section 4 revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and rigorously analyze two randomized algorithms to factor\nunivariate polynomials over finite fields using rank $2$ Drinfeld modules. The\nfirst algorithm estimates the degree of an irreducible factor of a polynomial\nfrom Euler-Poincare characteristics of random Drinfeld modules. Knowledge of a\nfactor degree allows one to rapidly extract all factors of that degree. As a\nconsequence, the problem of factoring polynomials over finite fields in time\nnearly linear in the degree is reduced to finding Euler-Poincare\ncharacteristics of random Drinfeld modules with high probability. Notably, the\nworst case complexity of polynomial factorization over finite fields is reduced\nto the average case complexity of a problem concerning Drinfeld modules. The\nsecond algorithm is a random Drinfeld module analogue of Berlekamp's algorithm.\nDuring the course of its analysis, we prove a new bound on degree distributions\nin factorization patterns of polynomials over finite fields in certain short\nintervals.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 01:48:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 07:08:01 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Narayanan", "Anand Kumar", ""]]}, {"id": "1504.07830", "submitter": "Yuni Iwamasa", "authors": "Hiroshi Hirai and Yuni Iwamasa", "title": "On k-Submodular Relaxation", "comments": "11 pages, corrected typos, accepted in SIAM Journal on Discrete\n  Mathematics", "journal-ref": "SIAM Journal on Discrete Mathematics, 30(3):1726-1736, 2016", "doi": "10.1137/15M101926X", "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$-submodular functions, introduced by Huber and Kolmogorov, are functions\ndefined on $\\{0, 1, 2, \\dots, k\\}^n$ satisfying certain submodular-type\ninequalities. $k$-submodular functions typically arise as relaxations of\nNP-hard problems, and the relaxations by $k$-submodular functions play key\nroles in design of efficient, approximation, or fixed-parameter tractable\nalgorithms. Motivated by this, we consider the following problem: Given a\nfunction $f : \\{1, 2, \\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$,\ndetermine whether $f$ is extended to a $k$-submodular function $g : \\{0, 1, 2,\n\\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, where $g$ is called a\n$k$-submodular relaxation of $f$.\n  We give a polymorphic characterization of those functions which admit a\n$k$-submodular relaxation, and also give a combinatorial $O((k^n)^2)$-time\nalgorithm to find a $k$-submodular relaxation or establish that a\n$k$-submodular relaxation does not exist. Our algorithm has interesting\nproperties: (1) If the input function is integer valued, then our algorithm\noutputs a half-integral relaxation, and (2) if the input function is binary,\nthen our algorithm outputs the unique optimal relaxation. We present\napplications of our algorithm to valued constraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:27:44 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 14:58:03 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 06:22:10 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Hirai", "Hiroshi", ""], ["Iwamasa", "Yuni", ""]]}, {"id": "1504.07999", "submitter": "Michael Bremner", "authors": "Michael J. Bremner, Ashley Montanaro, Dan J. Shepherd", "title": "Average-case complexity versus approximate simulation of commuting\n  quantum computations", "comments": "This version is arguably easier to read than v1. Trust us, we argued\n  about it. 4+1+5 pages, RevTex 4.1", "journal-ref": "Phys. Rev. Lett. 117, 080501 (2016)", "doi": "10.1103/PhysRevLett.117.080501", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the class of commuting quantum computations known as IQP\n(Instantaneous Quantum Polynomial time) to strengthen the conjecture that\nquantum computers are hard to simulate classically. We show that, if either of\ntwo plausible average-case hardness conjectures holds, then IQP computations\nare hard to simulate classically up to constant additive error. One conjecture\nrelates to the hardness of estimating the complex-temperature partition\nfunction for random instances of the Ising model; the other concerns\napproximating the number of zeroes of random low-degree polynomials. We observe\nthat both conjectures can be shown to be valid in the setting of worst-case\ncomplexity. We arrive at these conjectures by deriving spin-based\ngeneralisations of the Boson Sampling problem that avoid the so-called\npermanent anticoncentration conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:01:21 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 07:50:23 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Bremner", "Michael J.", ""], ["Montanaro", "Ashley", ""], ["Shepherd", "Dan J.", ""]]}, {"id": "1504.08120", "submitter": "Sudeshna Kolay", "authors": "Sudeshna Kolay and Fahad Panolan", "title": "Parameterized Algorithms for Deletion to (r,l)-graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fixed integers $r,\\ell \\geq 0$, a graph $G$ is called an {\\em\n$(r,\\ell)$-graph} if the vertex set $V(G)$ can be partitioned into $r$\nindependent sets and $\\ell$ cliques. This brings us to the following natural\nparameterized questions: {\\sc Vertex $(r,\\ell)$-Partization} and {\\sc Edge\n$(r,\\ell)$-Partization}. An input to these problems consist of a graph $G$ and\na positive integer $k$ and the objective is to decide whether there exists a\nset $S\\subseteq V(G)$ ($S\\subseteq E(G)$) such that the deletion of $S$ from\n$G$ results in an $(r,\\ell)$-graph. These problems generalize well studied\nproblems such as {\\sc Odd Cycle Transversal}, {\\sc Edge Odd Cycle Transversal},\n{\\sc Split Vertex Deletion} and {\\sc Split Edge Deletion}. We do not hope to\nget parameterized algorithms for either {\\sc Vertex $(r,\\ell)$-Partization} or\n{\\sc Edge $(r,\\ell)$-Partization} when either of $r$ or $\\ell$ is at least $3$\nas the recognition problem itself is NP-complete. This leaves the case of\n$r,\\ell \\in \\{1,2\\}$. We almost complete the parameterized complexity dichotomy\nfor these problems. Only the parameterized complexity of {\\sc Edge\n$(2,2)$-Partization} remains open. We also give an approximation algorithm and\na Turing kernelization for {\\sc Vertex $(r,\\ell)$-Partization}. We use an\ninteresting finite forbidden induced graph characterization, for a class of\ngraphs known as $(r,\\ell)$-split graphs, properly containing the class of\n$(r,\\ell)$-graphs. This approach to obtain approximation algorithms could be of\nan independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 08:45:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Kolay", "Sudeshna", ""], ["Panolan", "Fahad", ""]]}, {"id": "1504.08316", "submitter": "Katherine Edwards", "authors": "Emmanuel Abbe, Katherine Edwards", "title": "Concentration of the number of solutions of random planted CSPs and\n  Goldreich's one-way candidates", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that the logarithm of the number of solutions of a random\nplanted $k$-SAT formula concentrates around a deterministic $n$-independent\nthreshold. Specifically, if $F^*_{k}(\\alpha,n)$ is a random $k$-SAT formula on\n$n$ variables, with clause density $\\alpha$ and with a uniformly drawn planted\nsolution, there exists a function $\\phi_k(\\cdot)$ such that, besides for some\n$\\alpha$ in a set of Lesbegue measure zero, we have $ \\frac{1}{n}\\log\nZ(F^*_{k}(\\alpha,n)) \\to \\phi_k(\\alpha)$ in probability, where $Z(F)$ is the\nnumber of solutions of the formula $F$. This settles a problem left open in\nAbbe-Montanari RANDOM 2013, where the concentration is obtained only for the\nexpected logarithm over the clause distribution. The result is also extended to\na more general class of random planted CSPs; in particular, it is shown that\nthe number of pre-images for the Goldreich one-way function model concentrates\nfor some choices of the predicates.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 17:48:21 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Edwards", "Katherine", ""]]}, {"id": "1504.08352", "submitter": "Young Kun Ko", "authors": "Mark Braverman, Young Kun Ko, Aviad Rubinstein, Omri Weinstein", "title": "ETH Hardness for Densest-$k$-Subgraph with Perfect Completeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, assuming the (deterministic) Exponential Time Hypothesis,\ndistinguishing between a graph with an induced $k$-clique and a graph in which\nall k-subgraphs have density at most $1-\\epsilon$, requires $n^{\\tilde\n\\Omega(log n)}$ time. Our result essentially matches the quasi-polynomial\nalgorithms of Feige and Seltser [FS97] and Barman [Bar15] for this problem, and\nis the first one to rule out an additive PTAS for Densest $k$-Subgraph. We\nfurther strengthen this result by showing that our lower bound continues to\nhold when, in the soundness case, even subgraphs smaller by a near-polynomial\nfactor ($k' = k 2^{-\\tilde \\Omega (log n)}$) are assumed to be at most\n($1-\\epsilon$)-dense.\n  Our reduction is inspired by recent applications of the \"birthday repetition\"\ntechnique [AIM14,BKW15]. Our analysis relies on information theoretical\nmachinery and is similar in spirit to analyzing a parallel repetition of\ntwo-prover games in which the provers may choose to answer some challenges\nmultiple times, while completely ignoring other challenges.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:31:16 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Braverman", "Mark", ""], ["Ko", "Young Kun", ""], ["Rubinstein", "Aviad", ""], ["Weinstein", "Omri", ""]]}, {"id": "1504.08361", "submitter": "Shikha Singh", "authors": "Jing Chen, Samuel McCauley and Shikha Singh", "title": "Rational Proofs with Multiple Provers", "comments": "Proceedings of the 2016 ACM Conference on Innovations in Theoretical\n  Computer Science. ACM, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive proofs (IP) model a world where a verifier delegates computation\nto an untrustworthy prover, verifying the prover's claims before accepting\nthem. IP protocols have applications in areas such as verifiable computation\noutsourcing, computation delegation, cloud computing. In these applications,\nthe verifier may pay the prover based on the quality of his work. Rational\ninteractive proofs (RIP), introduced by Azar and Micali (2012), are an\ninteractive-proof system with payments, in which the prover is rational rather\nthan untrustworthy---he may lie, but only to increase his payment. Rational\nproofs leverage the provers' rationality to obtain simple and efficient\nprotocols. Azar and Micali show that RIP=IP(=PSAPCE). They leave the question\nof whether multiple provers are more powerful than a single prover for rational\nand classical proofs as an open problem.\n  In this paper, we introduce multi-prover rational interactive proofs (MRIP).\nHere, a verifier cross-checks the provers' answers with each other and pays\nthem according to the messages exchanged. The provers are cooperative and\nmaximize their total expected payment if and only if the verifier learns the\ncorrect answer to the problem. We further refine the model of MRIP to\nincorporate utility gap, which is the loss in payment suffered by provers who\nmislead the verifier to the wrong answer.\n  We define the class of MRIP protocols with constant, noticeable and\nnegligible utility gaps. We give tight characterization for all three MRIP\nclasses. We show that under standard complexity-theoretic assumptions, MRIP is\nmore powerful than both RIP and MIP ; and this is true even the utility gap is\nrequired to be constant. Furthermore the full power of each MRIP class can be\nachieved using only two provers and three rounds. (A preliminary version of\nthis paper appeared at ITCS 2016. This is the full version that contains new\nresults.)\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:44:10 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 17:48:46 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2016 16:39:09 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 21:02:13 GMT"}, {"version": "v5", "created": "Sat, 11 Nov 2017 20:41:48 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Chen", "Jing", ""], ["McCauley", "Samuel", ""], ["Singh", "Shikha", ""]]}]