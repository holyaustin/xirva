[{"id": "0908.0512", "submitter": "Greg Kuperberg", "authors": "Greg Kuperberg (UC Davis)", "title": "How hard is it to approximate the Jones polynomial?", "comments": "19 pages. Don't miss this major revision! Includes more complete\n  explanations of all results, and refinements of both Aaronson's theorem and\n  the Solovay-Kitaev theorem. To appear in ToC", "journal-ref": "Theory Comput. 11 (2015), 183-219", "doi": null, "report-no": null, "categories": "quant-ph cs.CC math.QA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Freedman, Kitaev, and Wang [arXiv:quant-ph/0001071], and later Aharonov,\nJones, and Landau [arXiv:quant-ph/0511096], established a quantum algorithm to\n\"additively\" approximate the Jones polynomial V(L,t) at any principal root of\nunity t. The strength of this additive approximation depends exponentially on\nthe bridge number of the link presentation. Freedman, Larsen, and Wang\n[arXiv:math/0103200] established that the approximation is universal for\nquantum computation at a non-lattice, principal root of unity; and Aharonov and\nArad [arXiv:quant-ph/0605181] established a uniform version of this result.\n  In this article, we show that any value-dependent approximation of the Jones\npolynomial at these non-lattice roots of unity is #P-hard. If given the power\nto decide whether |V(L,t)| > a or |V(L,t)| < b for fixed constants a > b > 0,\nthere is a polynomial-time algorithm to exactly count the solutions to\narbitrary combinatorial equations. In our argument, the result follows fairly\ndirectly from the universality result and Aaronson's theorem that PostBQP = PP\n[arXiv:quant-ph/0412187].\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2009 18:55:02 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 05:04:49 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Kuperberg", "Greg", "", "UC Davis"]]}, {"id": "0908.1027", "submitter": "Yoshitaka Sasaki", "authors": "Yasuo Ohno, Yoshitaka Sasaki and Chika Yamazaki", "title": "On exponential polynomials and quantum computing", "comments": "7pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We calculate the zeros of an exponential polynomial of three variables by a\nclassical algorithm and quantum algorithms which are based on the method of van\nDam and Shparlinski, they treated the case of two variables, and compare with\nthe time complexity of those cases. Further we compare the case of van Dam and\nShparlinski with our case by considering the ratio (classical/quantum) of the\ntime complexity. Then we can observe the ratio decreases.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2009 11:44:57 GMT"}], "update_date": "2009-08-10", "authors_parsed": [["Ohno", "Yasuo", ""], ["Sasaki", "Yoshitaka", ""], ["Yamazaki", "Chika", ""]]}, {"id": "0908.1159", "submitter": "Norbert B\\'atfai", "authors": "Norbert B\\'atfai", "title": "On the Running Time of the Shortest Programs", "comments": "24 pages, 15 figures; added new values to the table \"The increasing\n  of running time of the shortest programs\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kolmogorov complexity of the word w is equal to the length of the\nshortest concatenation of program Z and its input x with which the word w is\ncomputed by the universal turing machine U. The question introduced in this\npaper is the following: How long do the shortest programs run for?\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2009 14:07:47 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2009 10:46:14 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "0908.1397", "submitter": "Julien Hendrickx", "authors": "Julien M. Hendrickx and Alex Olshevsky", "title": "Matrix P-norms are NP-hard to approximate if p \\neq 1,2,\\infty", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for any rational p \\in [1,\\infty) except p = 1, 2, unless P =\nNP, there is no polynomial-time algorithm for approximating the matrix p-norm\nto arbitrary relative precision. We also show that for any rational p\\in\n[1,\\infty) including p = 1, 2, unless P = NP, there is no polynomial-time\nalgorithm approximates the \\infty, p mixed norm to some fixed relative\nprecision.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2009 20:22:46 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2009 14:00:08 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2010 15:30:30 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Hendrickx", "Julien M.", ""], ["Olshevsky", "Alex", ""]]}, {"id": "0908.1599", "submitter": "Saburo Higuchi", "authors": "Saburo Higuchi and Marc M\\'ezard", "title": "Decimation flows in constraint satisfaction problems", "comments": "14 pages, 2 figures", "journal-ref": "J. Stat. Mech. (2009) P12009", "doi": "10.1088/1742-5468/2009/12/P12009", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study hard constraint satisfaction problems with a decimation approach\nbased on message passing algorithms. Decimation induces a renormalization flow\nin the space of problems, and we exploit the fact that this flow transforms\nsome of the constraints into linear constraints over GF(2). In particular, when\nthe flow hits the subspace of linear problems, one can stop decimation and use\nGaussian elimination. We introduce a new decimation algorithm which uses this\nlinear structure and shows a strongly improved performance with respect to the\nusual decimation methods on some of the hardest locked occupation problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2009 00:18:43 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Higuchi", "Saburo", ""], ["M\u00e9zard", "Marc", ""]]}, {"id": "0908.1932", "submitter": "Ketan Mulmuley D", "authors": "Ketan D. Mulmuley", "title": "On P vs. NP, Geometric Complexity Theory, Explicit Proofs and the\n  Complexity Barrier", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric complexity theory (GCT) is an approach to the P vs. NP and related\nproblems. This article gives its complexity theoretic overview without assuming\nany background in algebraic geometry or representation theory.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2009 18:31:54 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2009 00:22:41 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Mulmuley", "Ketan D.", ""]]}, {"id": "0908.1936", "submitter": "Ketan Mulmuley D", "authors": "Ketan D. Mulmuley", "title": "On P vs. NP, Geometric Complexity Theory, and the Riemann Hypothesis", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and\nrelated problems. A high level overview of this research plan and the results\nobtained so far was presented in a series of three lectures in the Institute of\nAdvanced study, Princeton, Feb 9-11, 2009. This article contains the material\ncovered in those lectures after some revision, and gives a mathematical\noverview of GCT. No background in algebraic geometry, representation theory or\nquantum groups is assumed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2009 18:05:07 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2009 00:32:52 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Mulmuley", "Ketan D.", ""]]}, {"id": "0908.2122", "submitter": "Michael Freedman", "authors": "M. Bordewich, M. Freedman, L. Lov\\'asz and D. Welsh", "title": "Approximate Counting and Quantum Computation", "comments": null, "journal-ref": "Combinatorics, Probability and Computing (2005) 14,737-754", "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the result that an `approximate' evaluation of the Jones\npolynomial of a braid at a $5^{th}$ root of unity can be used to simulate the\nquantum part of any algorithm in the quantum complexity class BQP, and results\nrelating BQP to the counting class GapP, we introduce a form of additive\napproximation which can be used to simulate a function in BQP. We show that all\nfunctions in the classes #P and GapP have such an approximation scheme under\ncertain natural normalisations. However we are unable to determine whether the\nparticular functions we are motivated by, such as the above evaluation of the\nJones polynomial, can be approximated in this way. We close with some open\nproblems motivated by this work.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 19:29:07 GMT"}], "update_date": "2009-08-17", "authors_parsed": [["Bordewich", "M.", ""], ["Freedman", "M.", ""], ["Lov\u00e1sz", "L.", ""], ["Welsh", "D.", ""]]}, {"id": "0908.2128", "submitter": "Toby S. Cubitt", "authors": "Toby S. Cubitt, Jens Eisert and Michael M. Wolf", "title": "The Complexity of Relating Quantum Channels to Master Equations", "comments": "V1: 43 pages, single column, 8 figures. V2: titled changed; added\n  proof-overview and accompanying figure; 50 pages, single column, 9 figures", "journal-ref": "Commun. Math. Phys. 310, 383 (2012)", "doi": null, "report-no": null, "categories": "math-ph cs.CC math.MP quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely positive, trace preserving (CPT) maps and Lindblad master\nequations are both widely used to describe the dynamics of open quantum\nsystems. The connection between these two descriptions is a classic topic in\nmathematical physics. One direction was solved by the now famous result due to\nLindblad, Kossakowski Gorini and Sudarshan, who gave a complete\ncharacterisation of the master equations that generate completely positive\nsemi-groups. However, the other direction has remained open: given a CPT map,\nis there a Lindblad master equation that generates it (and if so, can we find\nit's form)? This is sometimes known as the Markovianity problem. Physically, it\nis asking how one can deduce underlying physical processes from experimental\nobservations.\n  We give a complexity theoretic answer to this problem: it is NP-hard. We also\ngive an explicit algorithm that reduces the problem to integer semi-definite\nprogramming, a well-known NP problem. Together, these results imply that\nresolving the question of which CPT maps can be generated by master equations\nis tantamount to solving P=NP: any efficiently computable criterion for\nMarkovianity would imply P=NP; whereas a proof that P=NP would imply that our\nalgorithm already gives an efficiently computable criterion. Thus, unless P\ndoes equal NP, there cannot exist any simple criterion for determining when a\nCPT map has a master equation description.\n  However, we also show that if the system dimension is fixed (relevant for\ncurrent quantum process tomography experiments), then our algorithm scales\nefficiently in the required precision, allowing an underlying Lindblad master\nequation to be determined efficiently from even a single snapshot in this case.\n  Our work also leads to similar complexity-theoretic answers to a related\nlong-standing open problem in probability theory.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 15:51:17 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 12:46:41 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Cubitt", "Toby S.", ""], ["Eisert", "Jens", ""], ["Wolf", "Michael M.", ""]]}, {"id": "0908.2363", "submitter": "Tsuyoshi Ito", "authors": "Tsuyoshi Ito", "title": "Polynomial-Space Approximation of No-Signaling Provers", "comments": "13 pages. v2: Introduction expanded, references added, some open\n  problems stated, and typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two-prover one-round interactive proof systems, no-signaling provers are\nthose who are allowed to use arbitrary strategies, not limited to local\noperations, as long as their strategies cannot be used for communication\nbetween them. Study of multi-prover interactive proof systems with no-signaling\nprovers is motivated by study of those with provers sharing quantum states. The\nrelation between them is that no-signaling strategies include all the\nstrategies realizable by provers sharing arbitrary entangled quantum states,\nand more.\n  This paper shows that two-prover one-round interactive proof systems with\nno-signaling provers only accept languages in PSPACE. Combined with the\nprotocol for PSPACE by Ito, Kobayashi and Matsumoto (CCC 2009), this implies\nMIPns(2,1)=PSPACE, where MIPns(2,1) is the class of languages having a\ntwo-prover one-round interactive proof system with no-signaling provers. This\nis proved by constructing a fast parallel algorithm which approximates within\nan additive error the maximum value of a two-player one-round game achievable\nby cooperative no-signaling players. The algorithm uses the fast parallel\nalgorithm for the mixed packing and covering problem by Young (FOCS 2001).\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 17:52:16 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2009 19:11:27 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Ito", "Tsuyoshi", ""]]}, {"id": "0908.2468", "submitter": "Seiichiro Tani", "authors": "Andris Ambainis, Kazuo Iwama, Masaki Nakanishi, Harumichi Nishimura,\n  Rudy Raymond, Seiichiro Tani, Shigeru Yamashita", "title": "Average/Worst-Case Gap of Quantum Query Complexities by On-Set Size", "comments": "19 pages", "journal-ref": "Computational Complexity, vol.25, Issue 4, pp.723--735, 2016\n  (entitled \"Quantum Query Complexity of Almost All Functions with Fixed On-Set\n  Size\")", "doi": "10.1007/s00037-016-0139-6", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the query complexity of the functions in the family\nF_{N,M} of N-variable Boolean functions with onset size M, i.e., the number of\ninputs for which the function value is 1, where 1<= M <= 2^{N}/2 is assumed\nwithout loss of generality because of the symmetry of function values, 0 and 1.\nOur main results are as follows: (1) There is a super-linear gap between the\naverage-case and worst-case quantum query complexities over F_{N,M} for a\ncertain range of M. (2) There is no super-linear gap between the average-case\nand worst-case randomized query complexities over F_{N,M} for every M. (3) For\nevery M bounded by a polynomial in N, any function in F_{N,M} has quantum query\ncomplexity Theta (sqrt{N}). (4) For every M=O(2^{cN}) with an arbitrary large\nconstant c<1, any function in F_{N,M} has randomized query complexity Omega\n(N).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 00:13:44 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ambainis", "Andris", ""], ["Iwama", "Kazuo", ""], ["Nakanishi", "Masaki", ""], ["Nishimura", "Harumichi", ""], ["Raymond", "Rudy", ""], ["Tani", "Seiichiro", ""], ["Yamashita", "Shigeru", ""]]}, {"id": "0908.2476", "submitter": "Yunlei Zhao", "authors": "Andrew C. Yao, Moti Yung, Yunlei Zhao", "title": "Concurrent Knowledge-Extraction in the Public-Key Model", "comments": "38 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge extraction is a fundamental notion, modelling machine possession of\nvalues (witnesses) in a computational complexity sense. The notion provides an\nessential tool for cryptographic protocol design and analysis, enabling one to\nargue about the internal state of protocol players without ever looking at this\nsupposedly secret state. However, when transactions are concurrent (e.g., over\nthe Internet) with players possessing public-keys (as is common in\ncryptography), assuring that entities ``know'' what they claim to know, where\nadversaries may be well coordinated across different transactions, turns out to\nbe much more subtle and in need of re-examination. Here, we investigate how to\nformally treat knowledge possession by parties (with registered public-keys)\ninteracting over the Internet. Stated more technically, we look into the\nrelative power of the notion of ``concurrent knowledge-extraction'' (CKE) in\nthe concurrent zero-knowledge (CZK) bare public-key (BPK) model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 01:43:21 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Yao", "Andrew C.", ""], ["Yung", "Moti", ""], ["Zhao", "Yunlei", ""]]}, {"id": "0908.2707", "submitter": "Edward Hirsch", "authors": "Edward A. Hirsch and Dmitry Itsykson", "title": "On optimal heuristic randomized semidecision procedures, with\n  application to proof complexity", "comments": "11 pages, accepted to STACS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The existence of a (p-)optimal propositional proof system is a major open\nquestion in (proof) complexity; many people conjecture that such systems do not\nexist. Krajicek and Pudlak (1989) show that this question is equivalent to the\nexistence of an algorithm that is optimal on all propositional tautologies.\nMonroe (2009) recently gave a conjecture implying that such algorithm does not\nexist.\n  We show that in the presence of errors such optimal algorithms do exist. The\nconcept is motivated by the notion of heuristic algorithms. Namely, we allow\nthe algorithm to claim a small number of false \"theorems\" (according to any\nsamplable distribution on non-tautologies) and err with bounded probability on\nother inputs.\n  Our result can also be viewed as the existence of an optimal proof system in\na class of proof systems obtained by generalizing automatizable proof systems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 09:18:25 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2010 20:14:38 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2010 12:25:24 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Hirsch", "Edward A.", ""], ["Itsykson", "Dmitry", ""]]}, {"id": "0908.2782", "submitter": "J\\'er\\'emie Roland", "authors": "Boris Altshuler, Hari Krovi and Jeremie Roland", "title": "Adiabatic quantum optimization fails for random instances of NP-complete\n  problems", "comments": "34 pages, 5 color figures. Significant changes compared to v1,\n  including a new section about Anderson Localization", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adiabatic quantum optimization has attracted a lot of attention because small\nscale simulations gave hope that it would allow to solve NP-complete problems\nefficiently. Later, negative results proved the existence of specifically\ndesigned hard instances where adiabatic optimization requires exponential time.\nIn spite of this, there was still hope that this would not happen for random\ninstances of NP-complete problems. This is an important issue since random\ninstances are a good model for hard instances that can not be solved by current\nclassical solvers, for which an efficient quantum algorithm would therefore be\ndesirable. Here, we will show that because of a phenomenon similar to Anderson\nlocalization, an exponentially small eigenvalue gap appears in the spectrum of\nthe adiabatic Hamiltonian for large random instances, very close to the end of\nthe algorithm. This implies that unfortunately, adiabatic quantum optimization\nalso fails for these instances by getting stuck in a local minimum, unless the\ncomputation is exponentially long.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 19:47:58 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2009 22:29:18 GMT"}], "update_date": "2009-12-02", "authors_parsed": [["Altshuler", "Boris", ""], ["Krovi", "Hari", ""], ["Roland", "Jeremie", ""]]}, {"id": "0908.2940", "submitter": "Hartmut Klauck", "authors": "Hartmut Klauck", "title": "A Strong Direct Product Theorem for Disjointness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strong direct product theorem states that if we want to compute $k$\nindependent instances of a function, using less than $k$ times the resources\nneeded for one instance, then the overall success probability will be\nexponentially small in $k$. We establish such a theorem for the randomized\ncommunication complexity of the Disjointness problem, i.e., with communication\n$const\\cdot kn$ the success probability of solving $k$ instances of size $n$\ncan only be exponentially small in $k$. We show that this bound even holds for\n$AM$ communication protocols with limited ambiguity. This also implies a new\nlower bound for Disjointness in a restricted 3-player NOF protocol, and optimal\ncommunication-space tradeoffs for Boolean matrix product. Our main result\nfollows from a solution to the dual of a linear programming problem, whose\nfeasibility comes from a so-called Intersection Sampling Lemma that generalizes\na result by Razborov.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2009 14:59:24 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2009 11:03:53 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2010 08:13:39 GMT"}], "update_date": "2010-04-12", "authors_parsed": [["Klauck", "Hartmut", ""]]}, {"id": "0908.3430", "submitter": "Yu. I. Manin", "authors": "Yuri I. Manin", "title": "Renormalization and Computation II: Time Cut-off and the Halting Problem", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.QA cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second installment to the project initiated in [Ma3]. In the\nfirst Part, I argued that both philosophy and technique of the perturbative\nrenormalization in quantum field theory could be meaningfully transplanted to\nthe theory of computation, and sketched several contexts supporting this view.\n  In this second part, I address some of the issues raised in [Ma3] and provide\ntheir development in three contexts: a categorification of the algorithmic\ncomputations; time cut--off and Anytime Algorithms; and finally, a Hopf algebra\nrenormalization of the Halting Problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2009 13:03:38 GMT"}], "update_date": "2009-08-25", "authors_parsed": [["Manin", "Yuri I.", ""]]}, {"id": "0908.3491", "submitter": "Gus Gutoski", "authors": "Gus Gutoski", "title": "Entangled games do not require much entanglement (withdrawn)", "comments": "Withdrawn. I found a mistake in my proof. This one-page replacement\n  note explains the problem in more detail. Sorry, everyone", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an explicit upper bound on the amount of entanglement required by\nany strategy in a two-player cooperative game with classical questions and\nquantum answers. Specifically, we show that every strategy for a game with\nn-bit questions and n-qubit answers can be implemented exactly by players who\nshare an entangled state of no more than 5n qubits--a bound which is optimal to\nwithin a factor of 5/2. Previously, no upper bound at all was known on the\namount of entanglement required even to approximate such a strategy. It follows\nthat the problem of computing the value of these games is in NP, whereas\npreviously this problem was not known to be computable.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2009 20:23:26 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2009 03:40:43 GMT"}], "update_date": "2009-09-03", "authors_parsed": [["Gutoski", "Gus", ""]]}, {"id": "0908.3954", "submitter": "Alexandre Goldsztejn", "authors": "Alexandre Goldsztejn (LINA)", "title": "On the Exponentiation of Interval Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical computation of the exponentiation of a real matrix has been\nintensively studied. The main objective of a good numerical method is to deal\nwith round-off errors and computational cost. The situation is more complicated\nwhen dealing with interval matrices exponentiation: Indeed, the main problem\nwill now be the dependency loss of the different occurrences of the variables\ndue to interval evaluation, which may lead to so wide enclosures that they are\nuseless. In this paper, the problem of computing a sharp enclosure of the\ninterval matrix exponential is proved to be NP-hard. Then the scaling and\nsquaring method is adapted to interval matrices and shown to drastically reduce\nthe dependency loss w.r.t. the interval evaluation of the Taylor series.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 08:49:57 GMT"}], "update_date": "2009-08-28", "authors_parsed": [["Goldsztejn", "Alexandre", "", "LINA"]]}, {"id": "0908.3981", "submitter": "Alexander Mozeika", "authors": "Alexander Mozeika, David Saad and Jack Raymond", "title": "Computing with Noise - Phase Transitions in Boolean Formulas", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": "10.1103/PhysRevLett.103.248701", "report-no": null, "categories": "cond-mat.dis-nn cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing circuits composed of noisy logical gates and their ability to\nrepresent arbitrary Boolean functions with a given level of error are\ninvestigated within a statistical mechanics setting. Bounds on their\nperformance, derived in the information theory literature for specific gates,\nare straightforwardly retrieved, generalized and identified as the\ncorresponding typical-case phase transitions. This framework paves the way for\nobtaining new results on error-rates, function-depth and sensitivity, and their\ndependence on the gate-type and noise model used.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 11:03:32 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Mozeika", "Alexander", ""], ["Saad", "David", ""], ["Raymond", "Jack", ""]]}, {"id": "0908.4013", "submitter": "Norbert B\\'atfai", "authors": "Norbert B\\'atfai", "title": "Recombinations of Busy Beaver Machines", "comments": "15 pages, 11 figures; added new recombinated M_PP(4097) machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many programmers belive that Turing-based machines cannot think. We also\nbelieve in this, however it is interesting to note that the most sophisticated\nmachines are not programmed by human beings. We have only discovered them. In\nthis paper, using well-known Busy Beaver and Placid Platypus machines, we\ngenerate further very similar, but not exactly the same machines. We have found\na recombinated BB_5 machine which can make 70.740.809 steps before halting.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 13:59:27 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2009 10:23:50 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2009 10:51:42 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "0908.4016", "submitter": "Vadim E. Levit", "authors": "Vadim E. Levit and David Tankus", "title": "On Relating Edges in Graphs without Cycles of Length 4", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An edge xy is relating in the graph G if there is an independent set S,\ncontaining neither x nor y, such that S_{x} and S_{y} are both maximal\nindependent sets in G. It is an NP-complete problem to decide whether an edge\nis relating (Brown, Nowakowski, Zverovich, 2007). We show that the problem\nremains NP-complete even for graphs without cycles of length 4 and 5. On the\nother hand, for graphs without cycles of length 4 and 6, the problem can be\nsolved in polynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 14:11:47 GMT"}], "update_date": "2009-08-28", "authors_parsed": [["Levit", "Vadim E.", ""], ["Tankus", "David", ""]]}, {"id": "0908.4041", "submitter": "Alireza Bagheri", "authors": "Alireza Bagheri and Mohammadreza Razzazi", "title": "Complexity of Planar Embeddability of Trees inside Simple Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric embedding of graphs in a point set in the plane is a well known\nproblem. In this paper, the complexity of a variant of this problem, where the\npoint set is bounded by a simple polygon, is considered. Given a point set in\nthe plane bounded by a simple polygon and a free tree, we show that deciding\nwhether there is a planar straight-line embedding of the tree on the point set\ninside the simple polygon is NP-complete. This implies that the straight-line\nconstrained point-set embedding of trees is also NP-complete, which was posed\nas an open problem in [8].\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2009 16:35:26 GMT"}], "update_date": "2009-08-28", "authors_parsed": [["Bagheri", "Alireza", ""], ["Razzazi", "Mohammadreza", ""]]}, {"id": "0908.4309", "submitter": "Li Yan", "authors": "Francis Chin, Marek Chrobak, Li Yan", "title": "Algorithms for Placing Monitors in a Flow Network", "comments": "13 pages, 5 figures. Preliminary version appeared in AAIM 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Flow Edge-Monitor Problem, we are given an undirected graph G=(V,E),\nan integer k > 0 and some unknown circulation \\psi on G. We want to find a set\nof k edges in G, so that if we place k monitors on those edges to measure the\nflow along them, the total number of edges for which the flow can be uniquely\ndetermined is maximized. In this paper, we first show that the Flow\nEdge-Monitor Problem is NP-hard, and then we give two approximation algorithms:\na 3-approximation algorithm with running time O((m+n)^2) and a 2-approximation\nalgorithm with running time O((m+n)^3), where n = |V| and m=|E|.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2009 01:45:44 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Chin", "Francis", ""], ["Chrobak", "Marek", ""], ["Yan", "Li", ""]]}, {"id": "0908.4453", "submitter": "Shengyu Zhang", "authors": "Rahul Jain, Hartmut Klauck, Shengyu Zhang", "title": "Depth-Independent Lower bounds on the Communication Complexity of\n  Read-Once Boolean Formulas", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show lower bounds of $\\Omega(\\sqrt{n})$ and $\\Omega(n^{1/4})$ on the\nrandomized and quantum communication complexity, respectively, of all\n$n$-variable read-once Boolean formulas. Our results complement the recent\nlower bound of $\\Omega(n/8^d)$ by Leonardos and Saks and\n$\\Omega(n/2^{\\Omega(d\\log d)})$ by Jayram, Kopparty and Raghavendra for\nrandomized communication complexity of read-once Boolean formulas with depth\n$d$. We obtain our result by \"embedding\" either the Disjointness problem or its\ncomplement in any given read-once Boolean formula.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 03:40:57 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Jain", "Rahul", ""], ["Klauck", "Hartmut", ""], ["Zhang", "Shengyu", ""]]}, {"id": "0908.4494", "submitter": "Joel Ratsaby", "authors": "Joel Ratsaby", "title": "Learning, complexity and information density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the relationship between the complexity of a learner and the\nrandomness of his mistakes? This question was posed in \\cite{rat0903} who\nshowed that the more complex the learner the higher the possibility that his\nmistakes deviate from a true random sequence. In the current paper we report on\nan empirical investigation of this problem. We investigate two characteristics\nof randomness, the stochastic and algorithmic complexity of the binary sequence\nof mistakes. A learner with a Markov model of order $k$ is trained on a finite\nbinary sequence produced by a Markov source of order $k^{*}$ and is tested on a\ndifferent random sequence. As a measure of learner's complexity we define a\nquantity called the \\emph{sysRatio}, denoted by $\\rho$, which is the ratio\nbetween the compressed and uncompressed lengths of the binary string whose\n$i^{th}$ bit represents the maximum \\emph{a posteriori} decision made at state\n$i$ of the learner's model. The quantity $\\rho$ is a measure of information\ndensity. The main result of the paper shows that this ratio is crucial in\nanswering the above posed question. The result indicates that there is a\ncritical threshold $\\rho^{*}$ such that when $\\rho\\leq\\rho^{*}$ the sequence of\nmistakes possesses the following features: (1)\\emph{}low divergence $\\Delta$\nfrom a random sequence, (2) low variance in algorithmic complexity. When\n$\\rho>\\rho^{*}$, the characteristics of the mistake sequence changes sharply\ntowards a\\emph{}high\\emph{$\\Delta$} and high variance in algorithmic\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 10:05:53 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Ratsaby", "Joel", ""]]}, {"id": "0908.4580", "submitter": "Emanuele Viola", "authors": "Jasmina Hasanhodzic, Andrew W. Lo, Emanuele Viola", "title": "A Computational View of Market Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CC q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study market efficiency from a computational viewpoint.\nBorrowing from theoretical computer science, we define a market to be\n\\emph{efficient with respect to resources $S$} (e.g., time, memory) if no\nstrategy using resources $S$ can make a profit. As a first step, we consider\nmemory-$m$ strategies whose action at time $t$ depends only on the $m$ previous\nobservations at times $t-m,...,t-1$. We introduce and study a simple model of\nmarket evolution, where strategies impact the market by their decision to buy\nor sell. We show that the effect of optimal strategies using memory $m$ can\nlead to \"market conditions\" that were not present initially, such as (1) market\nbubbles and (2) the possibility for a strategy using memory $m' > m$ to make a\nbigger profit than was initially possible. We suggest ours as a framework to\nrationalize the technological arms race of quantitative trading firms.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 16:57:35 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Hasanhodzic", "Jasmina", ""], ["Lo", "Andrew W.", ""], ["Viola", "Emanuele", ""]]}]