[{"id": "1907.00102", "submitter": "Fran\\c{c}ois Schwarzentruber", "authors": "Fran\\c{c}ois Schwarzentruber", "title": "The Complexity of Tiling Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we collected the most important complexity results of\ntilings. We also propose a definition of a so-called deterministic set of tile\ntypes, in order to capture deterministic classes without the notion of games.\nWe also pinpoint tiling problems complete for respectively LOGSPACE and\nNLOGSPACE.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:32:42 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 12:53:46 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Schwarzentruber", "Fran\u00e7ois", ""]]}, {"id": "1907.00227", "submitter": "Veena Ravishankar", "authors": "Christopher Lynch, Andrew M. Marshall, Catherine Meadows, Paliath\n  Narendran, and Veena Ravishankar", "title": "On Asymmetric Unification for the Theory of XOR with a Homomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric unification, or unification with irreducibility constraints, is a\nnewly developed paradigm that arose out of the automated analysis of\ncryptographic protocols. However, there are still relatively few asymmetric\nunification algorithms. In this paper we address this lack by exploring the\napplication of automata-based unification methods. We examine the theory of xor\nwith a homomorphism, ACUNh, from the point of view of asymmetric unification,\nand develop a new automata-based decision procedure. Then, we adapt a recently\ndeveloped asymmetric combination procedure to produce a general asymmetric-\nACUNh decision procedure. Finally, we present a new approach for obtaining a\nsolution-generating asymmetric-ACUNh unification automaton. We also compare our\napproach to the most commonly used form of asymmetric unification available\ntoday, variant unification.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 15:57:54 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lynch", "Christopher", ""], ["Marshall", "Andrew M.", ""], ["Meadows", "Catherine", ""], ["Narendran", "Paliath", ""], ["Ravishankar", "Veena", ""]]}, {"id": "1907.00239", "submitter": "Dmitriy Zhuk", "authors": "Dmitriy Zhuk and Barnaby Martin", "title": "QCSP monsters and the demise of the Chen Conjecture", "comments": "with minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a surprising classification for the computational complexity of the\nQuantified Constraint Satisfaction Problem over a constraint language $\\Gamma$,\nQCSP$(\\Gamma)$, where $\\Gamma$ is a finite language over $3$ elements which\ncontains all constants. In particular, such problems are either in P,\nNP-complete, co-NP-complete or PSpace-complete. Our classification refutes the\nhitherto widely-believed Chen Conjecture.\n  Additionally, we show that already on a 4-element domain there exists a\nconstraint language $\\Gamma$ such that QCSP$(\\Gamma)$ is DP-complete (from\nBoolean Hierarchy), and on a 10-element domain there exists a constraint\nlanguage giving the complexity class $\\Theta_{2}^{P}$.\n  Meanwhile, we prove the Chen Conjecture for finite conservative languages\n$\\Gamma$. If the polymorphism clone of $\\Gamma$ has the polynomially generated\npowers (PGP) property then QCSP$(\\Gamma)$ is in NP. Otherwise, the polymorphism\nclone of $\\Gamma$ has the exponentially generated powers (EGP) property and\nQCSP$(\\Gamma)$ is PSpace-complete.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 17:13:13 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 20:41:38 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 07:35:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhuk", "Dmitriy", ""], ["Martin", "Barnaby", ""]]}, {"id": "1907.00309", "submitter": "Joshua Grochow", "authors": "Joshua A. Grochow and Youming Qiao", "title": "Isomorphism problems for tensors, groups, and cubic forms: completeness\n  and reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG math.GR math.RT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problems of testing isomorphism of tensors,\n$p$-groups, cubic forms, algebras, and more, which arise from a variety of\nareas, including machine learning, group theory, and cryptography. These\nproblems can all be cast as orbit problems on multi-way arrays under different\ngroup actions. Our first two main results are:\n  1. All the aforementioned isomorphism problems are equivalent under\npolynomial-time reductions, in conjunction with the recent results of\nFutorny-Grochow-Sergeichuk (Lin. Alg. Appl., 2019).\n  2. Isomorphism of $d$-tensors reduces to isomorphism of 3-tensors, for any $d\n\\geq 3$.\n  Our results suggest that these isomorphism problems form a rich and robust\nequivalence class, which we call Tensor Isomorphism-complete, or TI-complete.\nWe then leverage the techniques used in the above results to prove two\nfirst-of-their-kind results for Group Isomorphism (GpI):\n  3. We give a reduction from GpI for $p$-groups of exponent $p$ and small\nclass ($c < p$) to GpI for $p$-groups of exponent $p$ and class 2. The latter\nare widely believed to be the hardest cases of GpI, but as far as we know, this\nis the first reduction from any more general class of groups to this class.\n  4. We give a search-to-decision reduction for isomorphism of $p$-groups of\nexponent $p$ and class 2 in time $|G|^{O(\\log \\log |G|)}$. While\nsearch-to-decision reductions for Graph Isomorphism (GI) have been known for\nmore than 40 years, as far as we know this is the first non-trivial\nsearch-to-decision reduction in the context of GpI.\n  Our main technique for (1), (3), and (4) is a linear-algebraic analogue of\nthe classical graph coloring gadget, which was used to obtain the\nsearch-to-decision reduction for GI. This gadget construction may be of\nindependent interest and utility. The technique for (2) gives a method for\nencoding an arbitrary tensor into an algebra.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 03:08:38 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Qiao", "Youming", ""]]}, {"id": "1907.00412", "submitter": "Michael Rathjen", "authors": "Martin Krombholz and Michael Rathjen", "title": "Upper bounds on the graph minor theorem", "comments": "19 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lower bounds on the proof-theoretic strength of the graph minor theorem were\nfound over 30 years ago by Friedman, Robertson and Seymour 1987, but upper\nbounds have always been elusive. We present recently found upper bounds on the\ngraph minor theorem and other theorems appearing in the Graph Minors series.\nFurther, we give some ideas as to how the lower bounds on some of these\ntheorems might be improved.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 16:41:23 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Krombholz", "Martin", ""], ["Rathjen", "Michael", ""]]}, {"id": "1907.00817", "submitter": "Joergen Bang-Jensen", "authors": "J{\\o}rgen Bang-Jensen, Thomas Bellitto, William Lochet, Anders Yeo", "title": "The directed 2-linkage problem with length constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\sc weak 2-linkage} problem for digraphs asks for a given digraph and\nvertices $s_1,s_2,t_1,t_2$ whether $D$ contains a pair of arc-disjoint paths\n$P_1,P_2$ such that $P_i$ is an $(s_i,t_i)$-path. This problem is NP-complete\nfor general digraphs but polynomially solvable for acyclic digraphs\n\\cite{fortuneTCS10}. Recently it was shown \\cite{bercziESA17} that if $D$ is\nequipped with a weight function $w$ on the arcs\n  which satisfies that all edges have positive weight, then there is a\npolynomial algorithm for the variant of the weak-2-linkage problem when both\npaths have to be shortest paths in $D$. In this paper we consider the unit\nweight case and prove that for every pair constants $k_1,k_2$, there is a\npolynomial algorithm which decides whether the input digraph $D$ has a pair of\narc-disjoint paths $P_1,P_2$ such that $P_i$ is an $(s_i,t_i)$-path and the\nlength of $P_i$ is no more than $d(s_i,t_i)+k_i$, for $i=1,2$, where\n$d(s_i,t_i)$ denotes the length of the shortest $(s_i,t_i)$-path. We prove\nthat, unless the exponential time hypothesis (ETH) fails, there is no\npolynomial algorithm for deciding the existence of a solution $P_1,P_2$ to the\n{\\sc weak 2-linkage} problem where each path $P_i$ has length at most\n$d(s_i,t_i)+ c\\log^{1+\\epsilon}{}n$ for some constant $c$.\n  We also prove that the {\\sc weak 2-linkage} problem remains NP-complete if we\nrequire one of the two paths to be a shortest path while the other path has no\nrestriction on the length.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:25:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bang-Jensen", "J\u00f8rgen", ""], ["Bellitto", "Thomas", ""], ["Lochet", "William", ""], ["Yeo", "Anders", ""]]}, {"id": "1907.00847", "submitter": "Hao Huang", "authors": "Hao Huang", "title": "Induced subgraphs of hypercubes and a proof of the Sensitivity\n  Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that every $(2^{n-1}+1)$-vertex induced subgraph of\nthe $n$-dimensional cube graph has maximum degree at least $\\sqrt{n}$. This\nresult is best possible, and improves a logarithmic lower bound shown by Chung,\nF\\\"uredi, Graham and Seymour in 1988. As a direct consequence, we prove that\nthe sensitivity and degree of a boolean function are polynomially related,\nsolving an outstanding foundational problem in theoretical computer science,\nthe Sensitivity Conjecture of Nisan and Szegedy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:14:42 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 18:59:20 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Huang", "Hao", ""]]}, {"id": "1907.00872", "submitter": "Marcin Wrochna", "authors": "Marcin Wrochna, Stanislav \\v{Z}ivn\\'y", "title": "Improved hardness for H-colourings of G-colourable graphs", "comments": "Mention improvement in Proposition 2.5. SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results on approximate colourings of graphs and, more\ngenerally, approximate H-colourings and promise constraint satisfaction\nproblems.\n  First, we show NP-hardness of colouring $k$-colourable graphs with\n$\\binom{k}{\\lfloor k/2\\rfloor}-1$ colours for every $k\\geq 4$. This improves\nthe result of Bul\\'in, Krokhin, and Opr\\v{s}al [STOC'19], who gave NP-hardness\nof colouring $k$-colourable graphs with $2k-1$ colours for $k\\geq 3$, and the\nresult of Huang [APPROX-RANDOM'13], who gave NP-hardness of colouring\n$k$-colourable graphs with $2^{k^{1/3}}$ colours for sufficiently large $k$.\nThus, for $k\\geq 4$, we improve from known linear/sub-exponential gaps to\nexponential gaps.\n  Second, we show that the topology of the box complex of H alone determines\nwhether H-colouring of G-colourable graphs is NP-hard for all (non-bipartite,\nH-colourable) G. This formalises the topological intuition behind the result of\nKrokhin and Opr\\v{s}al [FOCS'19] that 3-colouring of G-colourable graphs is\nNP-hard for all (3-colourable, non-bipartite) G. We use this technique to\nestablish NP-hardness of H-colouring of G-colourable graphs for H that include\nbut go beyond $K_3$, including square-free graphs and circular cliques (leaving\n$K_4$ and larger cliques open).\n  Underlying all of our proofs is a very general observation that adjoint\nfunctors give reductions between promise constraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:35:21 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 20:35:21 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 17:14:22 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1907.01018", "submitter": "Samuel Epstein", "authors": "Samuel Epstein", "title": "On the Conditional Complexity of Sets of Strings", "comments": "18 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1304.3872, arXiv:1511.05006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set X of finite strings, one interesting question to ask is whether\nthere exists a member of X which is simple conditional to all other members of\nX. Conditional simplicity is measured by low conditional Kolmogorov complexity.\nWe prove the affirmative to this question for sets that have low mutual\ninformation with the halting sequence. There are two results with respect to\nthis question. One is dependent on the maximum conditional complexity between\ntwo elements of X, the other is dependent on the maximum expected value of the\nconditional complexity of a member of X relative to each member of X.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:06:39 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 18:31:47 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 20:16:17 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Epstein", "Samuel", ""]]}, {"id": "1907.01076", "submitter": "Florian Zuleger", "authors": "Florian Zuleger", "title": "The Polynomial Complexity of Vector Addition Systems with States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector addition systems are an important model in theoretical computer\nscience and have been used in a variety of areas. In this paper, we consider\nvector addition systems with states over a parameterized initial configuration.\nFor these systems, we are interested in the standard notion of computational\ncomplexity, i.e., we want to understand the length of the longest trace for a\nfixed vector addition system with states depending on the size of the initial\nconfiguration. We show that the asymptotic complexity of a given vector\naddition system with states is either $\\Theta(N^k)$ for some computable integer\n$k$, where $N$ is the size of the initial configuration, or at least\nexponential. We further show that $k$ can be computed in polynomial time in the\nsize of the considered vector addition system. Finally, we show that $1 \\le k\n\\le 2^n$, where $n$ is the dimension of the considered vector addition system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 21:09:19 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:00:00 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 19:34:26 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zuleger", "Florian", ""]]}, {"id": "1907.01129", "submitter": "Cibele Freire", "authors": "Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou", "title": "New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins", "comments": "23 pages, 19 figures, included a new section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resilience of a Boolean query is the minimum number of tuples that need\nto be deleted from the input tables in order to make the query false. A\nsolution to this problem immediately translates into a solution for the more\nwidely known problem of deletion propagation with source-side effects. In this\npaper, we give several novel results on the hardness of the resilience problem\nfor $\\textit{binary conjunctive queries with self-joins}$ (i.e. conjunctive\nqueries with relations of maximal arity 2) with one repeated relation. Unlike\nin the self-join free case, the concept of triad is not enough to fully\ncharacterize the complexity of resilience. We identify new structural\nproperties, namely chains, confluences and permutations, which lead to various\n$NP$-hardness results. We also give novel involved reductions to network flow\nto show certain cases are in $P$. Overall, we give a dichotomy result for the\nrestricted setting when one relation is repeated at most 2 times, and we cover\nmany of the cases for 3. Although restricted, our results provide important\ninsights into the problem of self-joins that we hope can help solve the general\ncase of all conjunctive queries with self-joins in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 02:34:59 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:29:34 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Freire", "Cibele", ""], ["Gatterbauer", "Wolfgang", ""], ["Immerman", "Neil", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1907.01218", "submitter": "Artem Kaznatcheev", "authors": "Artem Kaznatcheev, David A. Cohen, Peter G. Jeavons", "title": "Representing fitness landscapes by valued constraints to understand the\n  complexity of local search", "comments": "26 pages, 9 figures. Extended journal version to appear in Journal of\n  Artificial Intelligence Research; conference version appeared in CP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search is widely used to solve combinatorial optimisation problems and\nto model biological evolution, but the performance of local search algorithms\non different kinds of fitness landscapes is poorly understood. Here we consider\nhow fitness landscapes can be represented using valued constraints, and\ninvestigate what the structure of such representations reveals about the\ncomplexity of local search.\n  First, we show that for fitness landscapes representable by binary Boolean\nvalued constraints there is a minimal necessary constraint graph that can be\neasily computed. Second, we consider landscapes as equivalent if they allow the\nsame (improving) local search moves; we show that a minimal constraint graph\nstill exists, but is NP-hard to compute.\n  We then develop several techniques to bound the length of any sequence of\nlocal search moves. We show that such a bound can be obtained from the\nnumerical values of the constraints in the representation, and show how this\nbound may be tightened by considering equivalent representations. In the binary\nBoolean case, we prove that a degree 2 or tree-structured constraint graph\ngives a quadratic bound on the number of improving moves made by any local\nsearch; hence, any landscape that can be represented by such a model will be\ntractable for any form of local search.\n  Finally, we build two families of examples to show that the conditions in our\ntractability results are essential. With domain size three, even just a path of\nbinary constraints can model a landscape with an exponentially long sequence of\nimproving moves. With a treewidth-two constraint graph, even with a maximum\ndegree of three, binary Boolean constraints can model a landscape with an\nexponentially long sequence of improving moves.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:05:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:56:49 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 12:21:07 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 01:47:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kaznatcheev", "Artem", ""], ["Cohen", "David A.", ""], ["Jeavons", "Peter G.", ""]]}, {"id": "1907.01258", "submitter": "Yimin Ge", "authors": "Yimin Ge and Vedran Dunjko", "title": "A hybrid algorithm framework for small quantum computers with\n  application to finding Hamiltonian cycles", "comments": "20+2 pages", "journal-ref": null, "doi": "10.1063/1.5119235", "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that quantum computers can polynomially speed up\ncertain SAT-solving algorithms even when the number of available qubits is\nsignificantly smaller than the number of variables. Here we generalise this\napproach. We present a framework for hybrid quantum-classical algorithms which\nutilise quantum computers significantly smaller than the problem size. Given an\narbitrarily small ratio of the quantum computer to the instance size, we\nachieve polynomial speedups for classical divide-and-conquer algorithms,\nprovided that certain criteria on the time- and space-efficiency are met. We\ndemonstrate how this approach can be used to enhance Eppstein's algorithm for\nthe cubic Hamiltonian cycle problem, and achieve a polynomial speedup for any\nratio of the number of qubits to the size of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:36:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ge", "Yimin", ""], ["Dunjko", "Vedran", ""]]}, {"id": "1907.01619", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne and Anindya De and Rocco A. Servedio", "title": "Learning from satisfying assignments under continuous distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  What kinds of functions are learnable from their satisfying assignments?\nMotivated by this simple question, we extend the framework of De, Diakonikolas,\nand Servedio [DDS15], which studied the learnability of probability\ndistributions over $\\{0,1\\}^n$ defined by the set of satisfying assignments to\n\"low-complexity\" Boolean functions, to Boolean-valued functions defined over\ncontinuous domains. In our learning scenario there is a known \"background\ndistribution\" $\\mathcal{D}$ over $\\mathbb{R}^n$ (such as a known normal\ndistribution or a known log-concave distribution) and the learner is given\ni.i.d. samples drawn from a target distribution $\\mathcal{D}_f$, where\n$\\mathcal{D}_f$ is $\\mathcal{D}$ restricted to the satisfying assignments of an\nunknown low-complexity Boolean-valued function $f$. The problem is to learn an\napproximation $\\mathcal{D}'$ of the target distribution $\\mathcal{D}_f$ which\nhas small error as measured in total variation distance.\n  We give a range of efficient algorithms and hardness results for this\nproblem, focusing on the case when $f$ is a low-degree polynomial threshold\nfunction (PTF). When the background distribution $\\mathcal{D}$ is log-concave,\nwe show that this learning problem is efficiently solvable for degree-1 PTFs\n(i.e.,~linear threshold functions) but not for degree-2 PTFs. In contrast, when\n$\\mathcal{D}$ is a normal distribution, we show that this learning problem is\nefficiently solvable for degree-2 PTFs but not for degree-4 PTFs. Our hardness\nresults rely on standard assumptions about secure signature schemes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:17:59 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1907.01624", "submitter": "Fabian Frei", "authors": "Fabian Frei and Koichi Wada", "title": "Efficient Circuit Simulation in MapReduce", "comments": "This is the full version of the preliminary paper with the same title\n  presented at the 30th International Symposium on Algorithms and Computation\n  (ISAAC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MapReduce framework has firmly established itself as one of the most\nwidely used parallel computing platforms for processing big data on tera- and\npeta-byte scale. Approaching it from a theoretical standpoint has proved to be\nnotoriously difficult, however. In continuation of Goodrich et al.'s early\nefforts, explicitly espousing the goal of putting the MapReduce framework on\nfooting equal to that of long-established models such as the PRAM, we\ninvestigate the obvious complexity question of how the computational power of\nMapReduce algorithms compares to that of combinational Boolean circuits\ncommonly used for parallel computations. Relying on the standard MapReduce\nmodel introduced by Karloff et al. a decade ago, we develop an intricate\nsimulation technique to show that any problem in NC (i.e., a problem solved by\na logspace-uniform family of Boolean circuits of polynomial size and a depth\npolylogarithmic in the input size) can be solved by a MapReduce computation in\nO(T(n)/ log n) rounds, where n is the input size and T(n) is the depth of the\nwitnessing circuit family. Thus, we are able to closely relate the standard,\nuniform NC hierarchy modeling parallel computations to the deterministic\nMapReduce hierarchy DMRC by proving that NC^(i+1) is contained in DMRC^i for\nall natural i, including 0. Besides the theoretical significance, this result\nthat has important applied aspects as well. In particular, we show for all\nproblems in NC^1---many practically relevant ones such as integer\nmultiplication and division, the parity function, and recognizing balanced\nstrings of parentheses being among these---how to solve them in a constant\nnumber of deterministic MapReduce rounds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:32:26 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 16:12:20 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Frei", "Fabian", ""], ["Wada", "Koichi", ""]]}, {"id": "1907.02251", "submitter": "Nina Stausholm", "authors": "Rasmus Pagh, Nina Stausholm and Mikkel Thorup", "title": "Hardness of Bichromatic Closest Pair with Jaccard Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider collections $\\mathcal{A}$ and $\\mathcal{B}$ of red and blue sets,\nrespectively. Bichromatic Closest Pair is the problem of finding a pair from\n$\\mathcal{A}\\times \\mathcal{B}$ that has similarity higher than a given\nthreshold according to some similarity measure. Our focus here is the classic\nJaccard similarity $|\\textbf{a}\\cap \\textbf{b}|/|\\textbf{a}\\cup \\textbf{b}|$\nfor $(\\textbf{a},\\textbf{b})\\in \\mathcal{A}\\times \\mathcal{B}$.\n  We consider the approximate version of the problem where we are given\nthresholds $j_1>j_2$ and wish to return a pair from $\\mathcal{A}\\times\n\\mathcal{B}$ that has Jaccard similarity higher than $j_2$ if there exists a\npair in $\\mathcal{A}\\times \\mathcal{B}$ with Jaccard similarity at least $j_1$.\nThe classic locality sensitive hashing (LSH) algorithm of Indyk and Motwani\n(STOC '98), instantiated with the MinHash LSH function of Broder et al., solves\nthis problem in $\\tilde O(n^{2-\\delta})$ time if $j_1\\ge j_2^{1-\\delta}$. In\nparticular, for $\\delta=\\Omega(1)$, the approximation ratio\n$j_1/j_2=1/j_2^{\\delta}$ increases polynomially in $1/j_2$.\n  In this paper we give a corresponding hardness result. Assuming the\nOrthogonal Vectors Conjecture (OVC), we show that there cannot be a general\nsolution that solves the Bichromatic Closest Pair problem in\n$O(n^{2-\\Omega(1)})$ time for $j_1/j_2=1/j_2^{o(1)}$. Specifically, assuming\nOVC, we prove that for any $\\delta>0$ there exists an $\\varepsilon>0$ such that\nBichromatic Closest Pair with Jaccard similarity requires time\n$\\Omega(n^{2-\\delta})$ for any choice of thresholds $j_2<j_1<1-\\delta$, that\nsatisfy $j_1\\le j_2^{1-\\varepsilon}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 07:03:53 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Pagh", "Rasmus", ""], ["Stausholm", "Nina", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1907.02319", "submitter": "Jacob Focke", "authors": "Jacob Focke, Leslie Ann Goldberg, Stanislav \\v{Z}ivn\\'y", "title": "The Complexity of Approximately Counting Retractions to Square-Free\n  Graphs", "comments": null, "journal-ref": "ACM Transactions on Algorithms 17(3) Article No. 22 (2021)", "doi": "10.1145/3458040", "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A retraction is a homomorphism from a graph $G$ to an induced subgraph $H$ of\n$G$ that is the identity on $H$. In a long line of research, retractions have\nbeen studied under various algorithmic settings. Recently, the problem of\napproximately counting retractions was considered. We give a complete\ntrichotomy for the complexity of approximately counting retractions to all\nsquare-free graphs (graphs that do not contain a cycle of length $4$). It turns\nout there is a rich and interesting class of graphs for which this problem is\ncomplete in the class $\\#\\mathrm{BIS}$. As retractions generalise\nhomomorphisms, our easiness results extend to the important problem of\napproximately counting homomorphisms. By giving new $\\#\\mathrm{BIS}$-easiness\nresults we now settle the complexity of approximately counting homomorphisms\nfor a whole class of non-trivial graphs which were previously unresolved.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 10:45:22 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 13:41:50 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 17:13:59 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 14:13:20 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Focke", "Jacob", ""], ["Goldberg", "Leslie Ann", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1907.02353", "submitter": "Pierre Berg\\'e", "authors": "Pierre Berg\\'e, Benjamin Mouscadet, Arpad Rimmel, Joanna Tomasik", "title": "Fixed-parameter tractability of counting small minimum $(S,T)$-cuts", "comments": "13 pages, 10 figures, full version of the paper accepted in WG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameterized complexity of counting minimum cuts stands as a natural\nquestion because Ball and Provan showed its #P-completeness. For any undirected\ngraph $G=(V,E)$ and two disjoint sets of its vertices $S,T$, we design a\nfixed-parameter tractable algorithm which counts minimum edge $(S,T)$-cuts\nparameterized by their size $p$. Our algorithm operates on a transformed graph\ninstance. This transformation, called drainage, reveals a collection of at most\n$n=\\left| V \\right|$ successive minimum $(S,T)$-cuts $Z_i$. We prove that any\nminimum $(S,T)$-cut $X$ contains edges of at least one cut $Z_i$. This\nobservation, together with Menger's theorem, allows us to build the algorithm\ncounting all minimum $(S,T)$-cuts with running time $2^{O(p^2)}n^{O(1)}$.\nInitially dedicated to counting minimum cuts, it can be modified to obtain an\nFPT sampling of minimum edge $(S,T)$-cuts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:16:34 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 08:59:25 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Berg\u00e9", "Pierre", ""], ["Mouscadet", "Benjamin", ""], ["Rimmel", "Arpad", ""], ["Tomasik", "Joanna", ""]]}, {"id": "1907.02539", "submitter": "Jess Banks", "authors": "Jess Banks, Luca Trevisan", "title": "Vector Colorings of Random, Ramanujan, and Large-Girth Irregular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.SI math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that in sparse Erd\\H{o}s-R\\'{e}nyi graphs of average degree $d$, the\nvector chromatic number (the relaxation of chromatic number coming from the\nLov\\`{a}sz theta function) is typically $\\tfrac{1}{2}\\sqrt{d} + o_d(1)$. This\nfits with a long-standing conjecture that various refutation and\nhypothesis-testing problems concerning $k$-colorings of sparse\nErd\\H{o}s-R\\'{e}nyi graphs become computationally intractable below the\n`Kesten-Stigum threshold' $d_{KS,k} = (k-1)^2$. Along the way, we use the\ncelebrated Ihara-Bass identity and a carefully constructed non-backtracking\nrandom walk to prove two deterministic results of independent interest: a lower\nbound on the vector chromatic number (and thus the chromatic number) using the\nspectrum of the non-backtracking walk matrix, and an upper bound dependent only\non the girth and universal cover. Our upper bound may be equivalently viewed as\na generalization of the Alon-Boppana theorem to irregular graphs\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 18:00:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Banks", "Jess", ""], ["Trevisan", "Luca", ""]]}, {"id": "1907.02892", "submitter": "Oleg Verbitsky", "authors": "Frank Fuhlbr\\\"uck, Johannes K\\\"obler, Oleg Verbitsky", "title": "Identifiability of Graphs with Small Color Classes by the\n  Weisfeiler-Leman Algorithm", "comments": "74 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As it is well known, the isomorphism problem for vertex-colored graphs with\ncolor multiplicity at most 3 is solvable by the classical 2-dimensional\nWeisfeiler-Leman algorithm (2-WL). On the other hand, the prominent\nCai-F\\\"urer-Immerman construction shows that even the multidimensional version\nof the algorithm does not suffice for graphs with color multiplicity 4. We give\nan efficient decision procedure that, given a graph $G$ of color multiplicity\n4, recognizes whether or not $G$ is identifiable by 2-WL, that is, whether or\nnot 2-WL distinguishes $G$ from any non-isomorphic graph. In fact, we solve the\nmuch more general problem of recognizing whether or not a given coherent\nconfiguration of maximum fiber size 4 is separable. This extends our\nrecognition algorithm to graphs of color multiplicity 4 with directed and\ncolored edges.\n  Our decision procedure is based on an explicit description of the class of\ngraphs with color multiplicity 4 that are not identifiable by 2-WL. The\nCai-F\\\"urer-Immerman graphs of color multiplicity 4 distinctly appear here as a\nnatural subclass, which demonstrates that the Cai-F\\\"urer-Immerman construction\nis not ad hoc. Our classification reveals also other types of graphs that are\nhard for 2-WL. One of them arises from patterns known as $(n_3)$-configurations\nin incidence geometry.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:25:08 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 08:34:00 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 15:59:24 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Fuhlbr\u00fcck", "Frank", ""], ["K\u00f6bler", "Johannes", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "1907.02916", "submitter": "Tomoyuki Yamakami", "authors": "Tomoyuki Yamakami", "title": "Nonuniform Families of Polynomial-Size Quantum Finite Automata and\n  Quantum Logarithmic-Space Computation with Polynomial-Size Advice", "comments": "(10pt, A4, pp.32, 1 figure) This current version corrects and extends\n  the preliminary version that appeared in the Proceedings of the 13th\n  International Conference on Language and Automata Theory and Applications\n  (LATA 2019), Saint Petersburg, Russia, March 26-29, 2019, Lecture Notes in\n  Computer Science, Springer, vol. 11417, pp. 134-145, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state complexity of a finite(-state) automaton intuitively measures the\nsize of the description of the automaton. Sakoda and Sipser [STOC 1972, pp.\n275--286] were concerned with nonuniform families of finite automata and they\ndiscussed the behaviors of the nonuniform complexity classes defined by such\nfamilies of finite automata having polynomial-size state complexity. In a\nsimilar fashion, we introduce nonuniform state complexity classes using\nnonuniform families of quantum finite automata empowered by the flexible use of\ngarbage tapes. We first present general inclusion and separation relationships\namong nonuniform state complexity classes of various one-way finite automata,\nincluding deterministic, nondeterministic, probabilistic, and quantum finite\nautomata having polynomially many inner states. For two-way quantum finite\nautomata equipped with flexible garbage tapes, we show a close relationship\nbetween the nonuniform state complexity of the family of such polynomial-size\nquantum finite automata and the parameterized complexity class induced by\nlogarithmic-space quantum computation assisted by polynomial-size advice. We\nfurther establish a direct connection between space-bounded quantum computation\nwith quantum advice and quantum finite automata whose transitions are dictated\nby superpositions of transition tables.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 16:30:12 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 09:31:29 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 10:16:52 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 13:52:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Yamakami", "Tomoyuki", ""]]}, {"id": "1907.03205", "submitter": "Benjamin Morrison", "authors": "Benjamin Morrison and Adam Groce", "title": "Oracle Separations Between Quantum and Non-interactive Zero-Knowledge\n  Classes", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between problems solvable by quantum algorithms in\npolynomial time and those for which zero-knowledge proofs exist. In prior work,\nAaronson [arxiv:quant-ph/0111102] showed an oracle separation between BQP and\nSZK, i.e. an oracle $A$ such that $\\mathrm{SZK}^A \\not\\subseteq\n\\mathrm{BQP}^A$. In this paper we give a simple extension of Aaronson's result\nto non-interactive zero-knowledge proofs with perfect security. This class,\nNIPZK, is the most restrictive zero-knowledge class. We show that even for this\nclass we can construct an $A$ with $\\mathrm{NIPZK}^A \\not\\subseteq\n\\mathrm{BQP}^A$.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 23:47:48 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Morrison", "Benjamin", ""], ["Groce", "Adam", ""]]}, {"id": "1907.03526", "submitter": "Marten Maack", "authors": "Marten Maack and Klaus Jansen", "title": "Inapproximability Results for Scheduling with Interval and Resource\n  Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the restricted assignment problem, the input consists of a set of machines\nand a set of jobs each with a processing time and a subset of eligible\nmachines. The goal is to find an assignment of the jobs to the machines\nminimizing the makespan, that is, the maximum summed up processing time any\nmachine receives. Herein, jobs should only be assigned to those machines on\nwhich they are eligible. It is well-known that there is no polynomial time\napproximation algorithm with an approximation guarantee of less than 1.5 for\nthe restricted assignment problem unless P=NP. In this work, we show hardness\nresults for variants of the restricted assignment problem with particular types\nof restrictions.\n  In the case of interval restrictions the machines can be totally ordered such\nthat jobs are eligible on consecutive machines. We resolve the open question of\nwhether the problem admits a polynomial time approximation scheme (PTAS) in the\nnegative (unless P=NP). There are several special cases of this problem known\nto admit a PTAS.\n  Furthermore, we consider a variant with resource restriction where each\nmachine has capacities and each job demands for a fixed number of resources. A\njob is eligible on a machine if its demand is at most the capacity of the\nmachine for each resource. For one resource, this problem is known to admit a\nPTAS, for two, the case of interval restrictions is contained, and in general,\nthe problem is closely related to unrelated scheduling with a low rank\nprocessing time matrix. We show that there is no polynomial time approximation\nalgorithm with a rate smaller than 48/47 or 1.5 for scheduling with resource\nrestrictions with 2 or 4 resources, respectively, unless P=NP. All our results\ncan be extended to the so called Santa Claus variants of the problems where the\ngoal is to maximize the minimal processing time any machine receives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:47:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Maack", "Marten", ""], ["Jansen", "Klaus", ""]]}, {"id": "1907.03533", "submitter": "Rasoul Ramezanian", "authors": "Rasoul Ramezanian", "title": "A Formal Axiomatization of Computation", "comments": "13 page. arXiv admin note: substantial text overlap with\n  arXiv:1906.09873, arXiv:1205.5994", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an axiomatization for the notion of computation. Based on the\nidea of Brouwer choice sequences, we construct a model, denoted by $E$, which\nsatisfies our axioms and $E \\models \\mathrm{ P \\neq NP}$. In other words,\nregarding \"effective computability\" in Brouwer intuitionism viewpoint, we show\n$\\mathrm{ P \\neq NP}$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:52:45 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 07:47:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Ramezanian", "Rasoul", ""]]}, {"id": "1907.03850", "submitter": "Philip Wellnitz", "authors": "Marc Roth, Philip Wellnitz", "title": "Counting and Finding Homomorphisms is Universal for Parameterized\n  Complexity Theory", "comments": "42 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting homomorphisms from a graph $H$ into another graph $G$ is a\nfundamental problem of (parameterized) counting complexity theory. In this\nwork, we study the case where \\emph{both} graphs $H$ and $G$ stem from given\nclasses of graphs: $H\\in \\mathcal{H}$ and $G\\in \\mathcal{G}$. By this, we\ncombine the structurally restricted version of this problem, with the\nlanguage-restricted version.\n  Our main result is a construction based on Kneser graphs that associates\nevery problem $\\tt P$ in $\\#\\mathsf{W[1]}$ with two classes of graphs\n$\\mathcal{H}$ and $\\mathcal{G}$ such that the problem $\\tt P$ is\n\\emph{equivalent} to the problem $\\#{\\tt HOM}(\\mathcal{H}\\to \\mathcal{G})$ of\ncounting homomorphisms from a graph in $\\mathcal{H}$ to a graph in\n$\\mathcal{G}$. In view of Ladner's seminal work on the existence of\n$\\mathsf{NP}$-intermediate problems [J.ACM'75] and its adaptations to the\nparameterized setting, a classification of the class $\\#\\mathsf{W[1]}$ in\nfixed-parameter tractable and $\\#\\mathsf{W[1]}$-complete cases is unlikely.\nHence, obtaining a complete classification for the problem $\\#{\\tt\nHOM}(\\mathcal{H}\\to \\mathcal{G})$ seems unlikely. Further, our proofs easily\nadapt to $\\mathsf{W[1]}$.\n  In search of complexity dichotomies, we hence turn to special graph classes.\nThose classes include line graphs, claw-free graphs, perfect graphs, and\ncombinations thereof, and $F$-colorable graphs for fixed graphs $F$: If the\nclass $\\mathcal{G}$ is one of those classes and the class $\\mathcal{H}$ is\nclosed under taking minors, then we establish explicit criteria for the class\n$\\mathcal{H}$ that partition the family of problems $\\#{\\tt\nHOM}(\\mathcal{H}\\to\\mathcal{G})$ into polynomial-time solvable and\n$\\#\\mathsf{W[1]}$-hard cases. In particular, we can drop the condition of\n$\\mathcal{H}$ being minor-closed for $F$-colorable graphs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:18:58 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Roth", "Marc", ""], ["Wellnitz", "Philip", ""]]}, {"id": "1907.04132", "submitter": "Erlend Raa Vaagset", "authors": "Svein H{\\o}gemo, Jan Arne Telle and Erlend Raa V{\\aa}gset", "title": "Linear MIM-Width of Trees", "comments": "19 pages, 7 figures, full version of WG19 paper of same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an $O(n \\log n)$ algorithm computing the linear maximum induced\nmatching width of a tree and an optimal layout.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:14:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["H\u00f8gemo", "Svein", ""], ["Telle", "Jan Arne", ""], ["V\u00e5gset", "Erlend Raa", ""]]}, {"id": "1907.04165", "submitter": "Aleksa Stankovic", "authors": "Per Austrin and Aleksa Stankovic", "title": "Global Cardinality Constraints Make Approximating Some Max-2-CSPs Harder", "comments": "Paper appeared in APPROX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming the Unique Games Conjecture, we show that existing approximation\nalgorithms for some Boolean Max-2-CSPs with cardinality constraints are\noptimal. In particular, we prove that Max-Cut with cardinality constraints is\nUG-hard to approximate within \\approx 0.858, and that Max-2-Sat with\ncardinality constraints is UG-hard to approximate within \\approx 0.929. In both\ncases, the previous best hardness results were the same as the hardness of the\ncorresponding unconstrained Max-2-CSP (\\approx 0.878 for Max-Cut, and \\approx\n0.940 for Max-2-Sat). The hardness obtained for Max-2-Sat applies to monotone\nMax-2-Sat instances, meaning that we also obtain tight inapproximability for\nthe Max-k-Vertex-Cover problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:45:43 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 13:53:07 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Austrin", "Per", ""], ["Stankovic", "Aleksa", ""]]}, {"id": "1907.04302", "submitter": "Saeid Sahraei", "authors": "Saeid Sahraei, Mohammad Ali Maddah-Ali and Salman Avestimehr", "title": "Interactive Verifiable Polynomial Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing platforms have created the possibility for computationally\nlimited users to delegate demanding tasks to strong but untrusted servers.\nVerifiable computing algorithms help build trust in such interactions by\nenabling the server to provide a proof of correctness of his results which the\nuser can check very efficiently. In this paper, we present a doubly-efficient\ninteractive algorithm for verifiable polynomial evaluation. Unlike the\nmainstream literature on verifiable computing, the soundness of our algorithm\nis information-theoretic and cannot be broken by a computationally unbounded\nserver. By relying on basic properties of error correcting codes, our algorithm\nenforces a dishonest server to provide false results to problems which become\nprogressively easier to verify. After roughly $\\log d$ rounds, the user can\nverify the response of the server against a look-up table that has been\npre-computed during an initialization phase. For a polynomial of degree $d$, we\nachieve a user complexity of $O(d^{\\epsilon})$, a server complexity of\n$O(d^{1+\\epsilon})$, a round complexity of $O(\\log d)$ and an initialization\ncomplexity of $O(d^{1+\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:40:46 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Sahraei", "Saeid", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "1907.04383", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Venkatesan Guruswami, Marcin Wrochna, and Stanislav\n  \\v{Z}ivn\\'y", "title": "The Power of the Combined Basic LP and Affine Relaxation for Promise\n  CSPs", "comments": "17 pages, to appear in SICOMP", "journal-ref": "SIAM Journal on Computing 49(6) (2020) 1232-1248", "doi": "10.1137/20M1312745", "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of constraint satisfaction problems (CSP), promise CSPs are an\nexciting new direction of study. In a promise CSP, each constraint comes in two\nforms: \"strict\" and \"weak,\" and in the associated decision problem one must\ndistinguish between being able to satisfy all the strict constraints versus not\nbeing able to satisfy all the weak constraints. The most commonly cited example\nof a promise CSP is the approximate graph coloring problem--which has recently\nseen exciting progress [BKO19, WZ20] benefiting from a systematic algebraic\napproach to promise CSPs based on \"polymorphisms,\" operations that map tuples\nin the strict form of each constraint to tuples in the corresponding weak form.\n  In this work, we present a simple algorithm which in polynomial time solves\nthe decision problem for all promise CSPs that admit infinitely many symmetric\npolymorphisms, which are invariant under arbitrary coordinate permutations.\nThis generalizes previous work of the first two authors [BG19]. We also extend\nthis algorithm to a more general class of block-symmetric polymorphisms. As a\ncorollary, this single algorithm solves all polynomial-time tractable Boolean\nCSPs simultaneously. These results give a new perspective on Schaefer's classic\ndichotomy theorem and shed further light on how symmetries of polymorphisms\nenable algorithms. Finally, we show that block symmetric polymorphisms are not\nonly sufficient but also necessary for this algorithm to work, thus\nestablishing its precise power\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 21:35:32 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 14:35:12 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Guruswami", "Venkatesan", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1907.04442", "submitter": "Ignasi Sau", "authors": "Julien Baste, Ignasi Sau, Dimitrios M. Thilikos", "title": "Hitting minors on bounded treewidth graphs. IV. An optimal algorithm", "comments": "51 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem asks, given an $n$-vertex input graph $G,$ for the minimum number of\nvertices that intersect all minor models in $G$ of the graphs in ${\\cal F}$. by\nCourcelle Theorem, this problem can be solved in time $f_{{\\cal F}}(tw)\\cdot\nn^{O(1)},$ where $tw$ is the treewidth of $G$, for some function $f_{{\\cal F}}$\ndepending on ${\\cal F}$ In a recent series of articles, we have initiated the\nprogramme of optimizing asymptotically the function $f_{{\\cal F}}$. Here we\nprovide an algorithm showing that $f_{{\\cal F}}(tw) = 2^{O(tw\\cdot \\log tw)}$\nfor every collection ${\\cal F}$. Prior to this work, the best known function\n$f_{{\\cal F}}$ was double-exponential in $tw$. In particular, our algorithm\nvastly extends the results of Jansen et al. [SODA 2014] for the particular case\n${\\cal F}=\\{K_5,K_{3,3}\\}$ and of Kociumaka and Pilipczuk [Algorithmica 2019]\nfor graphs of bounded genus, and answers an open problem posed by Cygan et al.\n[Inf Comput 2017]. We combine several ingredients such as the machinery of\nboundaried graphs in dynamic programming via representatives, the Flat Wall\nTheorem, Bidimensionality, the irrelevant vertex technique, treewidth\nmodulators, and protrusion replacement. Together with our previous results\nproviding single-exponential algorithms for particular collections ${\\cal F}$\n[Theor Comput Sci 2020] and general lower bounds [J Comput Syst Sci 2020], our\nalgorithm yields the following complexity dichotomy when ${\\cal F} = \\{H\\}$\ncontains a single connected graph $H,$ assuming the Exponential Time\nHypothesis: $f_H(tw)=2^{\\Theta(tw)}$ if $H$ is a contraction of the chair or\nthe banner, and $f_H(tw)=2^{\\Theta(tw\\cdot \\log tw)}$ otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:29:50 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:18:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Baste", "Julien", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1907.04450", "submitter": "Songtao Lu", "authors": "Songtao Lu and Meisam Razaviyayn and Bo Yang and Kejun Huang and\n  Mingyi Hong", "title": "SNAP: Finding Approximate Second-Order Stationary Solutions Efficiently\n  for Non-convex Linearly Constrained Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes low-complexity algorithms for finding approximate\nsecond-order stationary points (SOSPs) of problems with smooth non-convex\nobjective and linear constraints. While finding (approximate) SOSPs is\ncomputationally intractable, we first show that generic instances of the\nproblem can be solved efficiently. More specifically, for a generic problem\ninstance, certain strict complementarity (SC) condition holds for all\nKarush-Kuhn-Tucker (KKT) solutions (with probability one). The SC condition is\nthen used to establish an equivalence relationship between two different\nnotions of SOSPs, one of which is computationally easy to verify. Based on this\nparticular notion of SOSP, we design an algorithm named the Successive\nNegative-curvature grAdient Projection (SNAP), which successively performs\neither conventional gradient projection or some negative curvature based\nprojection steps to find SOSPs. SNAP and its first-order extension SNAP$^+$,\nrequire $\\mathcal{O}(1/\\epsilon^{2.5})$ iterations to compute an $(\\epsilon,\n\\sqrt{\\epsilon})$-SOSP, and their per-iteration computational complexities are\npolynomial in the number of constraints and problem dimension. To our\nknowledge, this is the first time that first-order algorithms with polynomial\nper-iteration complexity and global sublinear rate have been designed to find\nSOSPs of the important class of non-convex problems with linear constraints.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:46:41 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Lu", "Songtao", ""], ["Razaviyayn", "Meisam", ""], ["Yang", "Bo", ""], ["Huang", "Kejun", ""], ["Hong", "Mingyi", ""]]}, {"id": "1907.04451", "submitter": "Aaron Potechin", "authors": "Neng Huang and Aaron Potechin", "title": "On the Approximability of Presidential Type Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a predicate $P: \\{-1, 1\\}^k \\to \\{-1, 1\\}$, let $CSP(P)$ be the set of\nconstraint satisfaction problems whose constraints are of the form $P$. We say\nthat $P$ is approximable if given a nearly satisfiable instance of $CSP(P)$,\nthere exists a probabilistic polynomial time algorithm that does better than a\nrandom assignment. Otherwise, we say that $P$ is approximation resistant.\n  In this paper, we analyze presidential type predicates, which are balanced\nlinear threshold functions where all of the variables except the first variable\n(the president) have the same weight. We show that almost all presidential-type\npredicates $P$ are approximable. More precisely, we prove the following result:\nfor any $\\delta_0 > 0$, there exists a $k_0$ such that if $k \\geq k_0$, $\\delta\n\\in (\\delta_0,1 - 2/k]$, and ${\\delta}k + k - 1$ is an odd integer then the\npresidential type predicate $P(x) = sign({\\delta}k{x_1} + \\sum_{i=2}^{k}{x_i})$\nis approximable. To prove this, we construct a rounding scheme that makes use\nof biases and pairwise biases. We also give evidence that using pairwise biases\nis necessary for such rounding schemes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 23:00:11 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 23:31:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Huang", "Neng", ""], ["Potechin", "Aaron", ""]]}, {"id": "1907.04521", "submitter": "Ivan Latkin", "authors": "Ivan V. Latkin", "title": "The complexity of the first-order theory of pure equality", "comments": "40 pages, 19 references bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will find a lower bound on the recognition complexity of the theories that\nare nontrivial relative to some equivalence relation (this relation may be\nequality), namely, each of these theories is consistent with the formula, whose\nsense is that there exist two non-equivalent elements. However, at first, we\nwill obtain a lower bound on the computational complexity for the first-order\ntheory of Boolean algebra that has only two elements. For this purpose, we will\ncode the long-continued deterministic Turing machine computations by the\nrelatively short-length quantified Boolean formulae; the modified Stockmeyer\nand Meyer method will appreciably be used for this simulation. Then, we will\ntransform the modeling formulae of the theory of this Boolean algebra to the\nsimulation ones of the first-order theory of the only equivalence relation in\npolynomial time. Since the computational complexity of these theories is not\npolynomial, we obtain that the class $\\mathbf{P}$ is a proper subclass of\n$\\mathbf{PSPACE}$ (Polynomial Time is a proper subset of Polynomial Space).\n  Keywords: Computational complexity, the theory of equality, the coding of\ncomputations, simulation by means formulae, polynomial time, polynomial space,\nlower complexity bound\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 05:56:10 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 13:07:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Latkin", "Ivan V.", ""]]}, {"id": "1907.04628", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Polytopes, lattices, and spherical codes for the nearest neighbor\n  problem", "comments": "This is the full version of the paper published in the proceedings of\n  ICALP 2020 under the same title, which only contains Section 1", "journal-ref": "ICALP 2020", "doi": "10.4230/LIPIcs.ICALP.2020.76", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study locality-sensitive hash methods for the nearest neighbor problem for\nthe angular distance, focusing on the approach of first projecting down onto a\nlow-dimensional subspace, and then partitioning the projected vectors according\nto Voronoi cells induced by a suitable spherical code. This approach\ngeneralizes and interpolates between the fast but suboptimal hyperplane hashing\nof Charikar [STOC'02] and the asymptotically optimal but practically often\nslower hash families of Andoni-Indyk [FOCS'06], Andoni-Indyk-Nguyen-Razenshteyn\n[SODA'14] and Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt [NIPS'15]. We set up a\nframework for analyzing the performance of any spherical code in this context,\nand we provide results for various codes from the literature, such as those\nrelated to regular polytopes and root lattices. Similar to hyperplane hashing,\nand unlike cross-polytope hashing, our analysis of collision probabilities and\nquery exponents is exact and does not hide order terms which vanish only for\nlarge $d$, facilitating an easy parameter selection.\n  For the two-dimensional case, we derive closed-form expressions for arbitrary\nspherical codes, and we show that the equilateral triangle is optimal,\nachieving a better performance than the two-dimensional analogues of hyperplane\nand cross-polytope hashing. In three and four dimensions, we numerically find\nthat the tetrahedron, $5$-cell, and $16$-cell achieve the best query exponents,\nwhile in five or more dimensions orthoplices appear to outperform regular\nsimplices, as well as the root lattice families $A_k$ and $D_k$. We argue that\nin higher dimensions, larger spherical codes will likely exist which will\noutperform orthoplices in theory, and we argue why using the $D_k$ root\nlattices will likely lead to better results in practice, due to a better\ntrade-off between the asymptotic query exponent and the concrete costs of\nhashing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:38:27 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 03:50:06 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1907.04640", "submitter": "Dmitry Gavinsky", "authors": "Dmitry Gavinsky, Pavel Pudl\\'ak", "title": "Santha-Vazirani sources, deterministic condensers and very strong\n  extractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of semi-random sources, also known as Santha-Vazirani (SV)\nsources, stands for a sequence of n bits, where the dependence of the i'th bit\non the previous i-1 bits is limited for every $i\\in[n]$. If the dependence of\nthe i'th bit on the remaining n-1 bits is limited, then this is a strong\nSV-source. Even the strong SV-sources are known not to admit (universal)\ndeterministic extractors, but they have seeded extractors, as their min-entropy\nis $\\Omega(n)$. It is intuitively obvious that strong SV-sources are more than\njust high-min-entropy sources, and this work explores the intuition.\nDeterministic condensers are known not to exist for general high-min-entropy\nsources, and we construct for any constants $\\epsilon, \\delta \\in (0,1)$ a\ndeterministic condenser that maps n bits coming from a strong SV-source with\nbias at most $\\delta$ to $\\Omega(n)$ bits of min-entropy rate at least\n$1-\\epsilon$. In conclusion we observe that deterministic condensers are\nclosely related to very strong extractors - a proposed strengthening of the\nnotion of strong (seeded) extractors: in particular, our constructions can be\nviewed as very strong extractors for the family of strong Santha-Vazirani\ndistributions. The notion of very strong extractors requires that the output\nremains unpredictable even to someone who knows not only the seed value (as in\nthe case of strong extractors), but also the extractor's outputs corresponding\nto the same input value with each of the preceding seed values (say, under the\nlexicographic ordering). Very strong extractors closely resemble the original\nnotion of SV-sources, except that the bits must satisfy the unpredictability\nrequirement only on average.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 22:58:04 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 18:41:11 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gavinsky", "Dmitry", ""], ["Pudl\u00e1k", "Pavel", ""]]}, {"id": "1907.04645", "submitter": "Tillmann Miltzow", "authors": "Ivor van der Hoog, Tillmann Miltzow, Martijn van Schaik", "title": "Smoothed Analysis of Order Types", "comments": "15 pages, 6 figures, long introduction and short proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an ordered point set $P = (p_1,\\ldots,p_n)$, its order type (denoted\nby $\\chi_P$) is a map which assigns to every triple of points a value in\n$\\{+,-,0\\}$ based on whether the points are collinear(0), oriented clockwise(-)\nor counter-clockwise(+). An abstract order type is a map $\\chi :\n\\left[\\substack{n\\\\3}\\right] \\rightarrow \\{+,-,0\\}$ (where\n$\\left[\\substack{n\\\\3}\\right]$ is the collection of all triples of a set of $n$\nelements) that satisfies the following condition: for every set of five\nelements $S\\subset [n]$ its induced order type $\\chi_{|S}$ is realizable by a\npoint set. To be precise, a point set $P$ realizes an order type $\\chi$,if\n$\\chi_P(p_i,p_j,p_k) = \\chi(i,j,k)$, for all $i<j<k$. Planar point sets are\namong the most basic and natural geometric objects of study in Discrete and\nComputational Geometry. Properties of point sets are relevant in theory and\npractice alike. It is known, that deciding if an abstract order type is\nrealizable is complete for the existential theory of the reals. Our results\nshow that order type realizability is much easier for realistic instances than\nin the worst case. In particular, we can recognize instances in \"expected\n\\NP-time\". This is one of the first $\\exists\\mathbb{R}$-complete problems\nanalyzed under the lens of Smoothed Analysis.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:10:09 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["van der Hoog", "Ivor", ""], ["Miltzow", "Tillmann", ""], ["van Schaik", "Martijn", ""]]}, {"id": "1907.04776", "submitter": "Samuel Epstein", "authors": "Samuel Epstein", "title": "On the Algorithmic Probability of Sets", "comments": "22 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combined universal probability m(D) of strings x in sets D is close to\nmax \\m(x) over x in D: their logs differ by at most D's information I(D:H)\nabout the halting sequence H. As a result of this, given a binary predicate P,\nthe length of the smallest program that computes a complete extension of P is\nless than the size of the domain of P plus the amount of information that P has\nwith the halting sequence.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:06:32 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 13:21:01 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Epstein", "Samuel", ""]]}, {"id": "1907.04826", "submitter": "Holger Dell", "authors": "Holger Dell, John Lapinskas, Kitty Meeks", "title": "Approximately counting and sampling small witnesses using a colourful\n  decision oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we prove \"black box\" results for turning algorithms which\ndecide whether or not a witness exists into algorithms to approximately count\nthe number of witnesses, or to sample from the set of witnesses approximately\nuniformly, with essentially the same running time. We do so by extending the\nframework of Dell and Lapinskas (STOC 2018), which covers decision problems\nthat can be expressed as edge detection in bipartite graphs given limited\noracle access; our framework covers problems which can be expressed as edge\ndetection in arbitrary $k$-hypergraphs given limited oracle access. (Simulating\nthis oracle generally corresponds to invoking a decision algorithm.) This\nincludes many key problems in both the fine-grained setting (such as $k$-SUM,\n$k$-OV and weighted $k$-Clique) and the parameterised setting (such as induced\nsubgraphs of size $k$ or weight-$k$ solutions to CSPs). From an algorithmic\nstandpoint, our results will make the development of new approximate counting\nalgorithms substantially easier; indeed, it already yields a new\nstate-of-the-art algorithm for approximately counting graph motifs, improving\non Jerrum and Meeks (JCSS 2015) unless the input graph is very dense and the\ndesired motif very small. Our $k$-hypergraph reduction framework generalises\nand strengthens results in the graph oracle literature due to Beame et al.\n(ITCS 2018) and Bhattacharya et al. (CoRR abs/1808.00691).\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:11:58 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Dell", "Holger", ""], ["Lapinskas", "John", ""], ["Meeks", "Kitty", ""]]}, {"id": "1907.05087", "submitter": "Jiaqing Jiang", "authors": "Jiaqing Jiang, Xiaoming Sun, Shang-Hua Teng, Bujiao Wu, Kewen Wu and\n  Jialin Zhang", "title": "Optimal Space-Depth Trade-Off of CNOT Circuits in Quantum Logic\n  Synthesis", "comments": "25 pages, 5 figures. Fixed several minor typos and a mistake about\n  CNOT+Rz circuit", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the decoherence of the state-of-the-art physical implementations of\nquantum computers, it is essential to parallelize the quantum circuits to\nreduce their depth. Two decades ago, Moore et al. demonstrated that additional\nqubits (or ancillae) could be used to design \"shallow\" parallel circuits for\nquantum operators. They proved that any $n$-qubit CNOT circuit could be\nparallelized to $O(\\log n)$ depth, with $O(n^2)$ ancillae. However, the\nnear-term quantum technologies can only support limited amount of qubits,\nmaking space-depth trade-off a fundamental research subject for quantum-circuit\nsynthesis.\n  In this work, we establish an asymptotically optimal space-depth trade-off\nfor the design of CNOT circuits. We prove that for any $m\\geq0$, any $n$-qubit\nCNOT circuit can be parallelized to $O\\left(\\max \\left\\{\\log n,\n\\frac{n^{2}}{(n+m)\\log (n+m)}\\right\\} \\right)$ depth, with $O(m)$ ancillae. We\nshow that this bound is tight by a counting argument, and further show that\neven with arbitrary two-qubit quantum gates to approximate CNOT circuits, the\ndepth lower bound still meets our construction, illustrating the robustness of\nour result. Our work improves upon two previous results, one by Moore et al.\nfor $O(\\log n)$-depth quantum synthesis, and one by Patel et al. for $m = 0$:\nfor the former, we reduce the need of ancillae by a factor of $\\log^2 n$ by\nshowing that $m=O(n^2/\\log^2 n)$ additional qubits suffice to build $O(\\log\nn)$-depth, $O(n^2/\\log n)$ size --- which is asymptotically optimal --- CNOT\ncircuits; for the later, we reduce the depth by a factor of $n$ to the\nasymptotically optimal bound $O(n/\\log n)$. Our results can be directly\nextended to stabilizer circuits using an earlier result by Aaronson et al. In\naddition, we provide relevant hardness evidences for synthesis optimization of\nCNOT circuits in term of both size and depth.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:19:13 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 07:30:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Jiang", "Jiaqing", ""], ["Sun", "Xiaoming", ""], ["Teng", "Shang-Hua", ""], ["Wu", "Bujiao", ""], ["Wu", "Kewen", ""], ["Zhang", "Jialin", ""]]}, {"id": "1907.05296", "submitter": "Johannes Zink", "authors": "Joachim Spoerhase, Sabine Storandt, Johannes Zink", "title": "Simplification of Polyline Bundles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a generalization to the well-known problem of polyline\nsimplification. Instead of a single polyline, we are given a set of $\\ell$\npolylines possibly sharing some line segments and bend points. Our goal is to\nminimize the number of bend points in the simplified bundle with respect to\nsome error tolerance $\\delta$ (measuring Fr\\'echet distance) but under the\nadditional constraint that shared parts have to be simplified consistently. We\nshow that polyline bundle simplification is NP-hard to approximate within a\nfactor $n^{1/3 - \\varepsilon}$ for any $\\varepsilon > 0$ where $n$ is the\nnumber of bend points in the polyline bundle. This inapproximability even\napplies to instances with only $\\ell=2$ polylines. However, we identify the\nsensitivity of the solution to the choice of $\\delta$ as a reason for this\nstrong inapproximability. In particular, we prove that if we allow $\\delta$ to\nbe exceeded by a factor of $2$ in our solution, we can find a simplified\npolyline bundle with no more than $O(\\log (\\ell + n)) \\cdot OPT$ bend points in\npolytime, providing us with an efficient bi-criteria approximation. As a\nfurther result, we show fixed-parameter tractability in the number of shared\nbend points.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:11:30 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 23:43:18 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Spoerhase", "Joachim", ""], ["Storandt", "Sabine", ""], ["Zink", "Johannes", ""]]}, {"id": "1907.05401", "submitter": "Omid Etesami", "authors": "Omid Etesami, Saeed Mahloujifar, Mohammad Mahmoody", "title": "Computational Concentration of Measure: Optimal Bounds, Reductions, and\n  More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product measures of dimension $n$ are known to be concentrated in Hamming\ndistance: for any set $S$ in the product space of probability $\\epsilon$, a\nrandom point in the space, with probability $1-\\delta$, has a neighbor in $S$\nthat is different from the original point in only\n$O(\\sqrt{n\\ln(1/(\\epsilon\\delta))})$ coordinates. We obtain the tight\ncomputational version of this result, showing how given a random point and\naccess to an $S$-membership oracle, we can find such a close point in\npolynomial time. This resolves an open question of [Mahloujifar and Mahmoody,\nALT 2019]. As corollaries, we obtain polynomial-time poisoning and (in certain\nsettings) evasion attacks against learning algorithms when the original\nvulnerabilities have any cryptographically non-negligible probability.\n  We call our algorithm MUCIO (\"MUltiplicative Conditional Influence\nOptimizer\") since proceeding through the coordinates, it decides to change each\ncoordinate of the given point based on a multiplicative version of the\ninfluence of that coordinate, where influence is computed conditioned on\npreviously updated coordinates.\n  We also define a new notion of algorithmic reduction between computational\nconcentration of measure in different metric probability spaces. As an\napplication, we get computational concentration of measure for high-dimensional\nGaussian distributions under the $\\ell_1$ metric.\n  We prove several extensions to the results above: (1) Our computational\nconcentration result is also true when the Hamming distance is weighted. (2) We\nobtain an algorithmic version of concentration around mean, more specifically,\nMcDiarmid's inequality. (3) Our result generalizes to discrete random\nprocesses, and this leads to new tampering algorithms for collective coin\ntossing protocols. (4) We prove exponential lower bounds on the average running\ntime of non-adaptive query algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:33:03 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Etesami", "Omid", ""], ["Mahloujifar", "Saeed", ""], ["Mahmoody", "Mohammad", ""]]}, {"id": "1907.05515", "submitter": "Chris Jones", "authors": "Chris Jones and Matt McPartlon", "title": "Spherical Discrepancy Minimization and Algorithmic Lower Bounds for\n  Covering the Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the boolean discrepancy problem, we study the following\noptimization problem which we term \\textsc{Spherical Discrepancy}: given $m$\nunit vectors $v_1, \\dots, v_m$, find another unit vector $x$ that minimizes\n$\\max_i \\langle x, v_i\\rangle$. We show that \\textsc{Spherical Discrepancy} is\nAPX-hard and develop a multiplicative weights-based algorithm that achieves\noptimal worst-case error bounds up to lower order terms. We use our algorithm\nto give the first non-trivial lower bounds for the problem of covering a\nhypersphere by hyperspherical caps of uniform volume at least\n$2^{-o(\\sqrt{n})}$. We accomplish this by proving a related covering bound in\nGaussian space and showing that in this \\textit{large cap regime} the bound\ntransfers to spherical space. Up to a log factor, our lower bounds match known\nupper bounds in the large cap regime.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 22:55:05 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 04:21:10 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jones", "Chris", ""], ["McPartlon", "Matt", ""]]}, {"id": "1907.05548", "submitter": "Priyanka Mukhopadhyay Ms", "authors": "Priyanka Mukhopadhyay", "title": "The Projection Games Conjecture and the Hardness of Approximation of\n  SSAT and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Super-SAT or SSAT problem was introduced by Dinur, Kindler, Raz and\nSafra[2002,2003] to prove the NP-hardness of approximation of two popular\nlattice problems - Shortest Vector Problem (SVP) and Closest Vector Problem\n(CVP). They conjectured that SSAT is NP-hard to approximate to within factor\n$n^c$ for some constant $c>0$, where $n$ is the size of the SSAT instance. In\nthis paper we prove this conjecture assuming the Projection Games Conjecture\n(PGC), given by Moshkovitz[2012]. This implies hardness of approximation of SVP\nand CVP within polynomial factors, assuming the Projection Games Conjecture.\n  We also reduce SSAT to the Nearest Codeword Problem (NCP) and Learning\nHalfspace Problem (LHP), as considered by Arora, Babai, Stern and\nSweedyk[1997]. This proves that both these problems are NP-hard to approximate\nwithin factor $N^{c'/\\log\\log n}$ for some constant $c'>0$ where $N$ is the\nsize of the instances of the respective problems. Assuming the Projection Games\nConjecture these problems are proved to be NP-hard to approximate within\npolynomial factors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 02:12:57 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 23:46:07 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mukhopadhyay", "Priyanka", ""]]}, {"id": "1907.05981", "submitter": "Greg Kuperberg", "authors": "Greg Kuperberg (UC Davis), Eric Samperton (UC Santa Barbara)", "title": "Coloring invariants of knots and links are often intractable", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CC math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a nonabelian, simple group with a nontrivial conjugacy class $C\n\\subseteq G$. Let $K$ be a diagram of an oriented knot in $S^3$, thought of as\ncomputational input. We show that for each such $G$ and $C$, the problem of\ncounting homomorphisms $\\pi_1(S^3\\setminus K) \\to G$ that send meridians of $K$\nto $C$ is almost parsimoniously $\\mathsf{\\#P}$-complete. This work is a sequel\nto a previous result by the authors that counting homomorphisms from\nfundamental groups of integer homology 3-spheres to $G$ is almost\nparsimoniously $\\mathsf{\\#P}$-complete. Where we previously used mapping class\ngroups actions on closed, unmarked surfaces, we now use braid group actions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 00:22:43 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kuperberg", "Greg", "", "UC Davis"], ["Samperton", "Eric", "", "UC Santa Barbara"]]}, {"id": "1907.06012", "submitter": "Xinyu Du", "authors": "Xinyu Du, Chao Wang, Tianze Wang, Zeyu Gao", "title": "Efficient methods to determine the reversibility of general 1D linear\n  cellular automata in polynomial complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study reversibility of one-dimensional(1D) linear cellular\nautomata(LCA) under null boundary condition, whose core problems have been\ndivided into two main parts: calculating the period of reversibility and\nverifying the reversibility in a period. With existing methods, the time and\nspace complexity of these two parts are still too expensive to be employed. So\nthe process soon becomes totally incalculable with a slightly big size, which\ngreatly limits its application. In this paper, we set out to solve these two\nproblems using two efficient algorithms, which make it possible to solve\nreversible LCA of very large size. Furthermore, we provide an interesting\nperspective to conversely generate 1D LCA from a given period of reversibility.\nDue to our methods' efficiency, we can calculate the reversible LCA with large\nsize, which has much potential to enhance security in cryptography system.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 05:27:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Du", "Xinyu", ""], ["Wang", "Chao", ""], ["Wang", "Tianze", ""], ["Gao", "Zeyu", ""]]}, {"id": "1907.06257", "submitter": "Zhuoran Yang", "authors": "Xinyang Yi, Zhaoran Wang, Zhuoran Yang, Constantine Caramanis, Han Liu", "title": "More Supervision, Less Computation: Statistical-Computational Tradeoffs\n  in Weakly Supervised Learning", "comments": "This work has been published in NeurIPS 2016. The first three authors\n  contribute equally", "journal-ref": "Advances in Neural Information Processing Systems (2016):\n  4482-4490", "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the weakly supervised binary classification problem where the\nlabels are randomly flipped with probability $1- {\\alpha}$. Although there\nexist numerous algorithms for this problem, it remains theoretically unexplored\nhow the statistical accuracies and computational efficiency of these algorithms\ndepend on the degree of supervision, which is quantified by ${\\alpha}$. In this\npaper, we characterize the effect of ${\\alpha}$ by establishing the\ninformation-theoretic and computational boundaries, namely, the minimax-optimal\nstatistical accuracy that can be achieved by all algorithms, and\npolynomial-time algorithms under an oracle computational model. For small\n${\\alpha}$, our result shows a gap between these two boundaries, which\nrepresents the computational price of achieving the information-theoretic\nboundary due to the lack of supervision. Interestingly, we also show that this\ngap narrows as ${\\alpha}$ increases. In other words, having more supervision,\ni.e., more correct labels, not only improves the optimal statistical accuracy\nas expected, but also enhances the computational efficiency for achieving such\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 18:34:44 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Yi", "Xinyang", ""], ["Wang", "Zhaoran", ""], ["Yang", "Zhuoran", ""], ["Caramanis", "Constantine", ""], ["Liu", "Han", ""]]}, {"id": "1907.06529", "submitter": "Michal Wlodarczyk", "authors": "Micha{\\l} W{\\l}odarczyk", "title": "Parameterized inapproximability for Steiner Orientation by Gap\n  Amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-Steiner Orientation problem, we are given a mixed graph, that is,\nwith both directed and undirected edges, and a set of $k$ terminal pairs. The\ngoal is to find an orientation of the undirected edges that maximizes the\nnumber of terminal pairs for which there is a path from the source to the sink.\nThe problem is known to be W[1]-hard when parameterized by k and hard to\napproximate up to some constant for FPT algorithms assuming Gap-ETH. On the\nother hand, no approximation factor better than $O(k)$ is known.\n  We show that $k$-Steiner Orientation is unlikely to admit an approximation\nalgorithm with any constant factor, even within FPT running time. To obtain\nthis result, we construct a self-reduction via a hashing-based gap\namplification technique, which turns out useful even outside of the FPT\nparadigm. Precisely, we rule out any approximation factor of the form $(\\log\nk)^{o(1)}$ for FPT algorithms (assuming FPT $\\ne$ W[1]) and $(\\log n)^{o(1)}$\nfor~purely polynomial-time algorithms (assuming that the class W[1] does not\nadmit randomized FPT algorithms). Moreover, we prove $k$-Steiner Orientation to\nbelong to W[1], which entails W[1]-completeness of $(\\log\nk)^{o(1)}$-approximation for $k$-Steiner Orientation This provides an example\nof a natural approximation task that is complete in a parameterized complexity\nclass.\n  Finally, we apply our technique to the maximization version of directed\nmulticut - Max $(k,p)$-Directed Multicut - where we are given a directed graph,\n$k$ terminals pairs, and a budget $p$. The goal is to maximize the number of\nseparated terminal pairs by removing $p$ edges. We present a simple proof that\nthe problem admits no FPT approximation with factor $O(k^{\\frac 1 2 -\n\\epsilon})$ (assuming FPT $\\ne$ W[1]) and no polynomial-time approximation with\nratio $O(|E(G)|^{\\frac 1 2 - \\epsilon})$ (assuming NP $\\not\\subseteq$ co-RP).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 14:55:41 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 09:36:22 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 13:30:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "1907.06731", "submitter": "William Kretschmer", "authors": "William Kretschmer", "title": "Lower Bounding the AND-OR Tree via Symmetrization", "comments": "12 pages, 1 figure. V2: fixed typos. V3: improved presentation, added\n  journal reference. V4: added forward reference to [HV20]", "journal-ref": "ACM Transactions on Computation Theory 13:1 (2021)", "doi": "10.1145/3434385", "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a simple, nearly tight lower bound on the approximate degree of the\ntwo-level $\\mathsf{AND}$-$\\mathsf{OR}$ tree using symmetrization arguments.\nSpecifically, we show that $\\widetilde{\\mathrm{deg}}(\\mathsf{AND}_m \\circ\n\\mathsf{OR}_n) = \\widetilde{\\Omega}(\\sqrt{mn})$. We prove this lower bound via\nreduction to the $\\mathsf{OR}$ function through a series of symmetrization\nsteps, in contrast to most other proofs that involve formulating approximate\ndegree as a linear program [BT13, She13, BDBGK18]. Our proof also demonstrates\nthe power of a symmetrization technique involving Laurent polynomials\n(polynomials with negative exponents) that was previously introduced by\nAaronson, Kothari, Kretschmer, and Thaler [AKKT19].\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:20:40 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 19:33:23 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 21:56:41 GMT"}, {"version": "v4", "created": "Fri, 2 Apr 2021 03:25:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kretschmer", "William", ""]]}, {"id": "1907.07020", "submitter": "Daniel Hausmann", "authors": "Daniel Hausmann and Lutz Schr\\\"oder", "title": "Quasipolynomial Computation of Nested Fixpoints", "comments": "extended version of conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the winning region of a parity game with $n$ nodes and\n$k$ priorities can be computed as a $k$-nested fixpoint of a suitable function;\nstraightforward computation of this nested fixpoint requires\n$\\mathcal{O}(n^{\\frac{k}{2}})$ iterations of the function. Calude et al.'s\nrecent quasipolynomial-time parity game solving algorithm essentially shows how\nto compute the same fixpoint in only quasipolynomially many iterations by\nreducing parity games to quasipolynomially sized safety games. Universal graphs\nhave been used to modularize this transformation of parity games to equivalent\nsafety games that are obtained by combining the original game with a universal\ngraph. We show that this approach naturally generalizes to the computation of\nsolutions of systems of \\emph{any} fixpoint equations over finite lattices;\nhence, the solution of fixpoint equation systems can be computed by\nquasipolynomially many iterations of the equations. We present applications to\nmodal fixpoint logics and games beyond relational semantics. For instance, the\nmodel checking problems for the energy $\\mu$-calculus, finite latticed\n$\\mu$-calculi, and the graded and the (two-valued) probabilistic $\\mu$-calculus\n-- with numbers coded in binary -- can be solved via nested fixpoints of\nfunctions that differ substantially from the function for parity games but\nstill can be computed in quasipolynomial time; our result hence implies that\nmodel checking for these $\\mu$-calculi is in QP. Moreover, we improve the\nexponent in known exponential bounds on satisfiability checking.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:12:11 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 00:42:01 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:50:38 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 12:00:50 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 21:33:31 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hausmann", "Daniel", ""], ["Schr\u00f6der", "Lutz", ""]]}, {"id": "1907.07367", "submitter": "Zekun Ye", "authors": "Zekun Ye, Yunqi Huang, Lvzhou Li, Yuyi Wang", "title": "Query complexity of generalized Simon's problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simon's problem plays an important role in the history of quantum algorithms,\nas it inspired Shor to discover the celebrated quantum algorithm solving\ninteger factorization in polynomial time. Besides, the quantum algorithm for\nSimon's problem has been recently applied to break symmetric cryptosystems.\nGeneralized Simon's problem, denoted by $\\mathsf{GSP}(p,n,k)$, is a natural\nextension of Simon's problem. In this paper we consider the query complexity of\n$\\mathsf{GSP}(p,n,k)$. First, it is not difficult to design a quantum algorithm\nsolving the above problem with query complexity of $O(n-k)$. However, so far it\nis not clear what is the classical query complexity of the problem, and\nrevealing this complexity is necessary for clarifying the computational power\ngap between quantum and classical computing on the problem.\n  To tackle this problem, we prove that any classical (deterministic or\nrandomized) algorithm for $\\mathsf{GSP}(p,n,k)$ has to query at least\n$\\Omega\\left(\\max\\{k, \\sqrt{p^{n-k}}\\}\\right)$ values and any classical\nnonadaptive deterministic algorithm for $\\mathsf{GSP}(p,n,k)$ has to query at\nleast $\\Omega\\left(\\max\\{k, \\sqrt{k \\cdot p^{n-k}}\\}\\right)$ values. Hence, we\nclearly show the classical computing model is less powerful than the quantum\ncounterpart, in terms of query complexity for the generalized Simon's problem.\nMoreover, we obtain an upper bound $O\\left(\\max\\{k, \\sqrt{k \\cdot\np^{n-k}}\\}\\right)$ on the classical deterministic query complexity of\n$\\mathsf{GSP}(p,n,k)$, by devising a subtle classical algorithm based on group\ntheory and the divide-and-conquer approach. Therefore, we have an almost full\ncharacterization of the classical deterministic query complexity of the\ngeneralized Simon\\u2019s problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 07:40:27 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 07:17:42 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 12:23:56 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 14:19:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ye", "Zekun", ""], ["Huang", "Yunqi", ""], ["Li", "Lvzhou", ""], ["Wang", "Yuyi", ""]]}, {"id": "1907.07471", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "Interesting Open Problem Related to Complexity of Computing the Fourier\n  Transform and Group Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fourier Transform is one of the most important linear transformations\nused in science and engineering. Cooley and Tukey's Fast Fourier Transform\n(FFT) from 1964 is a method for computing this transformation in time $O(n\\log\nn)$. From a lower bound perspective, relatively little is known. Ailon shows in\n2013 an $\\Omega(n\\log n)$ bound for computing the normalized Fourier Transform\nassuming only unitary operations on pairs of coordinates is allowed. The goal\nof this document is to describe a natural open problem that arises from this\nwork, which is related to group theory, and in particular to representation\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:29:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Ailon", "Nir", ""]]}, {"id": "1907.07833", "submitter": "Fernando Granha Jeronimo", "authors": "Vedat Levi Alev, Fernando Granha Jeronimo, Madhur Tulsiani", "title": "Approximating Constraint Satisfaction Problems on High-Dimensional\n  Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximately solving constraint satisfaction\nproblems with arity $k > 2$ ($k$-CSPs) on instances satisfying certain\nexpansion properties, when viewed as hypergraphs. Random instances of $k$-CSPs,\nwhich are also highly expanding, are well-known to be hard to approximate using\nknown algorithmic techniques (and are widely believed to be hard to approximate\nin polynomial time). However, we show that this is not necessarily the case for\ninstances where the hypergraph is a high-dimensional expander.\n  We consider the spectral definition of high-dimensional expansion used by\nDinur and Kaufman [FOCS 2017] to construct certain primitives related to PCPs.\nThey measure the expansion in terms of a parameter $\\gamma$ which is the\nanalogue of the second singular value for expanding graphs. Extending the\nresults by Barak, Raghavendra and Steurer [FOCS 2011] for 2-CSPs, we show that\nif an instance of MAX k-CSP over alphabet $[q]$ is a high-dimensional expander\nwith parameter $\\gamma$, then it is possible to approximate the maximum\nfraction of satisfiable constraints up to an additive error $\\epsilon$ using\n$q^{O(k)} \\cdot (k/\\epsilon)^{O(1)}$ levels of the sum-of-squares SDP\nhierarchy, provided $\\gamma \\leq \\epsilon^{O(1)} \\cdot (1/(kq))^{O(k)}$.\n  Based on our analysis, we also suggest a notion of threshold-rank for\nhypergraphs, which can be used to extend the results for approximating 2-CSPs\non low threshold-rank graphs. We show that if an instance of MAX k-CSP has\nthreshold rank $r$ for a threshold $\\tau = (\\epsilon/k)^{O(1)} \\cdot\n(1/q)^{O(k)}$, then it is possible to approximately solve the instance up to\nadditive error $\\epsilon$, using $r \\cdot q^{O(k)} \\cdot (k/\\epsilon)^{O(1)}$\nlevels of the sum-of-squares hierarchy. As in the case of graphs,\nhigh-dimensional expanders (with sufficiently small $\\gamma$) have threshold\nrank 1 according to our definition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:31:52 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Alev", "Vedat Levi", ""], ["Jeronimo", "Fernando Granha", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1907.07922", "submitter": "Stanislav Zivny", "authors": "Andrei A. Bulatov and Stanislav Zivny", "title": "Approximate counting CSP seen from the other side", "comments": "Full version of an MFCS'19 paper", "journal-ref": "ACM Transactions on Computation Theory 12(2) Article No. 11 (2020)", "doi": "10.1145/3389390", "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the complexity of counting Constraint Satisfaction\nProblems (CSPs) of the form #CSP($\\mathcal{C}$,-), in which the goal is, given\na relational structure $\\mathbf{A}$ from a class $\\mathcal{C}$ of structures\nand an arbitrary structure $\\mathbf{B}$, to find the number of homomorphisms\nfrom $\\mathbf{A}$ to $\\mathbf{B}$. Flum and Grohe showed that\n#CSP($\\mathcal{C}$,-) is solvable in polynomial time if $\\mathcal{C}$ has\nbounded treewidth [FOCS'02]. Building on the work of Grohe [JACM'07] on\ndecision CSPs, Dalmau and Jonsson then showed that, if $\\mathcal{C}$ is a\nrecursively enumerable class of relational structures of bounded arity, then\nassuming FPT $\\neq$ #W[1], there are no other cases of #CSP($\\mathcal{C}$,-)\nsolvable exactly in polynomial time (or even fixed-parameter time) [TCS'04].\n  We show that, assuming FPT $\\neq$ W[1] (under randomised parametrised\nreductions) and for $\\mathcal{C}$ satisfying certain general conditions,\n#CSP($\\mathcal{C}$,-) is not solvable even approximately for $\\mathcal{C}$ of\nunbounded treewidth; that is, there is no fixed parameter tractable (and thus\nalso not fully polynomial) randomised approximation scheme for\n#CSP($\\mathcal{C}$,-). In particular, our condition generalises the case when\n$\\mathcal{C}$ is closed under taking minors.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 08:38:00 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 18:35:15 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Bulatov", "Andrei A.", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1907.07969", "submitter": "Aditya Potukuchi", "authors": "Aditya Potukuchi", "title": "On the $\\text{AC}^0[\\oplus]$ complexity of Andreev's Problem", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Andreev's Problem states the following: Given an integer $d$ and a subset of\n$S \\subseteq \\mathbb{F}_q \\times \\mathbb{F}_q$, is there a polynomial $y =\np(x)$ of degree at most $d$ such that for every $a \\in \\mathbb{F}_q$, $(a,p(a))\n\\in S$? We show an $\\text{AC}^0[\\oplus]$ lower bound for this problem.\n  This problem appears to be similar to the list recovery problem for degree\n$d$-Reed-Solomon codes over $\\mathbb{F}_q$ which states the following: Given\nsubsets $A_1,\\ldots,A_q$ of $\\mathbb{F}_q$, output all (if any) the\nReed-Solomon codewords contained in $A_1\\times \\cdots \\times A_q$. For our\npurpose, we study this problem when $A_1, \\ldots, A_q$ are random subsets of a\ngiven size, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:28:32 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Potukuchi", "Aditya", ""]]}, {"id": "1907.08019", "submitter": "Axel Dahlberg", "authors": "Axel Dahlberg, Jonas Helsen, Stephanie Wehner", "title": "Transforming graph states to Bell-pairs is NP-Complete", "comments": "21 pages, 8 figures", "journal-ref": "Quantum 4, 348 (2020)", "doi": "10.22331/q-2020-10-22-348", "report-no": null, "categories": "quant-ph cs.CC math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Critical to the construction of large scale quantum networks, i.e. a quantum\ninternet, is the development of fast algorithms for managing entanglement\npresent in the network. One fundamental building block for a quantum internet\nis the distribution of Bell pairs between distant nodes in the network. Here we\nfocus on the problem of transforming multipartite entangled states into the\ntensor product of bipartite Bell pairs between specific nodes using only a\ncertain class of local operations and classical communication. In particular we\nstudy the problem of deciding whether a given graph state, and in general a\nstabilizer state, can be transformed into a set of Bell pairs on specific\nvertices using only single-qubit Clifford operations, single-qubit Pauli\nmeasurements and classical communication. We prove that this problem is\nNP-Complete.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:47:56 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:20:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Dahlberg", "Axel", ""], ["Helsen", "Jonas", ""], ["Wehner", "Stephanie", ""]]}, {"id": "1907.08024", "submitter": "Axel Dahlberg", "authors": "Axel Dahlberg, Jonas Helsen, Stephanie Wehner", "title": "Counting single-qubit Clifford equivalent graph states is #P-Complete", "comments": "10 pages, no figures", "journal-ref": "Journal of Mathematical Physics 61, 022202 (2020)", "doi": "10.1063/1.5120591", "report-no": null, "categories": "quant-ph cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph states, which include for example Bell states, GHZ states and cluster\nstates, form a well-known class of quantum states with applications ranging\nfrom quantum networks to error-correction. Deciding whether two graph states\nare equivalent up to single-qubit Clifford operations is known to be decidable\nin polynomial time and have been studied both in the context of producing\ncertain required states in a quantum network but also in relation to stabilizer\ncodes. The reason for the latter this is that single-qubit Clifford equivalent\ngraph states exactly corresponds to equivalent stabilizer codes. We here\nconsider the computational complexity of, given a graph state |G>, counting the\nnumber of graph states, single-qubit Clifford equivalent to |G>. We show that\nthis problem is #P-Complete. To prove our main result we make use of the notion\nof isotropic systems in graph theory. We review the definition of isotropic\nsystems and point out their strong relation to graph states. We believe that\nthese isotropic systems can be useful beyond the results presented in this\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:58:45 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Dahlberg", "Axel", ""], ["Helsen", "Jonas", ""], ["Wehner", "Stephanie", ""]]}, {"id": "1907.08093", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet and Nidhi Purohit", "title": "Metric Dimension Parameterized by Treewidth", "comments": "23 pages, 4 figures, long version of IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A resolving set $S$ of a graph $G$ is a subset of its vertices such that no\ntwo vertices of $G$ have the same distance vector to $S$. The Metric Dimension\nproblem asks for a resolving set of minimum size, and in its decision form, a\nresolving set of size at most some specified integer. This problem is\nNP-complete, and remains so in very restricted classes of graphs. It is also\nW[2]-complete with respect to the size of the solution. Metric Dimension has\nproven elusive on graphs of bounded treewidth. On the algorithmic side, a\npolytime algorithm is known for trees, and even for outerplanar graphs, but the\ngeneral case of treewidth at most two is open. On the complexity side, no\nparameterized hardness is known. This has led several papers on the topic to\nask for the parameterized complexity of Metric Dimension with respect to\ntreewidth. We provide a first answer to the question.\n  We show that Metric Dimension parameterized by the treewidth of the input\ngraph is W[1]-hard. More refinedly we prove that, unless the Exponential Time\nHypothesis fails, there is no algorithm solving Metric Dimension in time\n$f(\\text{pw})n^{o(\\text{pw})}$ on $n$-vertex graphs of constant degree, with\n$\\text{pw}$ the pathwidth of the input graph, and $f$ any computable function.\nThis is in stark contrast with an FPT algorithm of Belmonte et al. [SIAM J.\nDiscrete Math. '17] with respect to the combined parameter $\\text{tl}+\\Delta$,\nwhere $\\text{tl}$ is the tree-length and $\\Delta$ the maximum-degree of the\ninput graph.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:46:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Purohit", "Nidhi", ""]]}, {"id": "1907.08185", "submitter": "Nikhil Vyas", "authors": "Mitali Bafna, Nikhil Vyas", "title": "Imperfect Gaps in Gap-ETH and PCPs", "comments": "To appear in CCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of perfect completeness in probabilistically checkable\nproof systems (PCPs) and give a new way to transform a PCP with imperfect\ncompleteness to a PCP with perfect completeness when the initial gap is a\nconstant. In particular, we show that $\\text{PCP}_{c,s}[r,q] \\subseteq\n\\text{PCP}_{1,1-\\Omega(1)}[r+O(1),q+O(r)]$, for $c-s=\\Omega(1)$. This implies\nthat one can convert imperfect completeness to perfect in linear-sized PCPs for\n$NTIME[O(n)]$ with a $O(\\log n)$ additive loss in the query complexity $q$. We\nshow our result by constructing a \"robust circuit\" using threshold gates. These\nresults are a gap amplification procedure for PCPs (when completeness is\nimperfect), analogous to questions studied in parallel repetition and\npseudorandomness.\n  We also investigate the time complexity of approximating perfectly\nsatisfiable instances of 3SAT versus those with imperfect completeness. We show\nthat the Gap-ETH conjecture without perfect completeness is equivalent to\nGap-ETH with perfect completeness, i.e. we show that Gap-3SAT, where the gap is\nnot around 1, has a subexponential algorithm, if and only if, Gap-3SAT with\nperfect completeness has subexponential algorithms. We also relate the time\ncomplexities of these two problems in a more fine-grained way, to show that\n$T_2(n) \\leq T_1(n(\\log\\log n)^{O(1)})$, where $T_1(n),T_2(n)$ denote the\nrandomized time-complexity of approximating MAX 3SAT with perfect and imperfect\ncompleteness, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 17:48:19 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bafna", "Mitali", ""], ["Vyas", "Nikhil", ""]]}, {"id": "1907.08865", "submitter": "Marc Hellmuth", "authors": "Marc Hellmuth, Manuela Gei{\\ss} and Peter F. Stadler", "title": "Complexity of Modification Problems for Reciprocal Best Match Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocal best match graphs (RBMGs) are vertex colored graphs whose vertices\nrepresent genes and the colors the species where the genes reside. Edges\nidentify pairs of genes that are most closely related with respect to an\nunderlying evolutionary tree. In practical applications this tree is unknown\nand the edges of the RBMGs are inferred by quantifying sequence similarity. Due\nto noise in the data, these empirically determined graphs in general violate\nthe condition of being a ``biologically feasible'' RBMG. Therefore, it is of\npractical interest in computational biology to correct the initial estimate.\nHere we consider deletion (remove at most $k$ edges) and editing (add or delete\nat most $k$ edges) problems. We show that the decision version of the deletion\nand editing problem to obtain RBMGs from vertex colored graphs is NP-hard.\nUsing known results for the so-called bicluster editing, we show that the RBMG\nediting problem for $2$-colored graphs is fixed-parameter tractable.\n  A restricted class of RBMGs appears in the context of orthology detection.\nThese are cographs with a specific type of vertex coloring known as\nhierarchical coloring. We show that the decision problem of modifying a\nvertex-colored graph (either by edge-deletion or editing) into an RBMG with\ncograph structure or, equivalently, to an hierarchically colored cograph is\nNP-complete.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 20:22:08 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hellmuth", "Marc", ""], ["Gei\u00df", "Manuela", ""], ["Stadler", "Peter F.", ""]]}, {"id": "1907.09108", "submitter": "EPTCS", "authors": "Edith Hemaspaandra, Lane A. Hemaspaandra, J\\\"org Rothe", "title": "The Complexity of Online Bribery in Sequential Elections (Extended\n  Abstract)", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 233-251", "doi": "10.4204/EPTCS.297.16", "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on the complexity of bribery assumes that the bribery happens\nsimultaneously, and that the briber has full knowledge of all voters' votes.\nBut neither of those assumptions always holds. In many real-world settings,\nvotes come in sequentially, and the briber may have a use-it-or-lose-it moment\nto decide whether to bribe/alter a given vote, and at the time of making that\ndecision, the briber may not know what votes remaining voters are planning on\ncasting.\n  In this paper, we introduce a model for, and initiate the study of, bribery\nin such an online, sequential setting. We show that even for election systems\nwhose winner-determination problem is polynomial-time computable, an online,\nsequential setting may vastly increase the complexity of bribery, in fact\njumping the problem up to completeness for high levels of the polynomial\nhierarchy or even PSPACE. On the other hand, we show that for some natural,\nimportant election systems, such a dramatic complexity increase does not occur,\nand we pinpoint the complexity of their bribery problems in the online,\nsequential setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:16:55 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hemaspaandra", "Edith", ""], ["Hemaspaandra", "Lane A.", ""], ["Rothe", "J\u00f6rg", ""]]}, {"id": "1907.09280", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Kunihiko Sadakane, Srinivasa Rao Satti", "title": "Optimal In-place Algorithms for Basic Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present linear time {\\it in-place} algorithms for several basic and\nfundamental graph problems including the well-known graph search methods (like\ndepth-first search, breadth-first search, maximum cardinality search),\nconnectivity problems (like biconnectivity, $2$-edge connectivity),\ndecomposition problem (like chain decomposition) among various others,\nimproving the running time (by polynomial multiplicative factor) of the recent\nresults of Chakraborty et al. [ESA, 2018] who designed $O(n^3 \\lg n)$ time\nin-place algorithms for a strict subset of the above mentioned problems. The\nrunning times of all our algorithms are essentially optimal as they run in\nlinear time. One of the main ideas behind obtaining these algorithms is the\ndetection and careful exploitation of sortedness present in the input\nrepresentation for any graph without loss of generality. This observation alone\nis powerful enough to design some basic linear time in-place algorithms, but\nmore non-trivial graph problems require extra techniques which, we believe, may\nfind other applications while designing in-place algorithms for different graph\nproblems in the future.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:41:59 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1907.09371", "submitter": "Micha{\\l} Przyby{\\l}ek", "authors": "Michal R. Przybylek and Pawel Siedlecki", "title": "A note on the complexity of a phaseless polynomial interpolation", "comments": "Corrected formatting", "journal-ref": "Journal of Complexity 2020", "doi": "10.1016/j.jco.2019.101456", "report-no": null, "categories": "math.NA cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the classical problem of polynomial interpolation,\nwith a slight twist; namely, polynomial evaluations are available up to a group\naction of the unit circle on the complex plane. It turns out that this new\nsetting allows for a phaseless recovery of a polynomial in a polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:38:18 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 19:48:50 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Przybylek", "Michal R.", ""], ["Siedlecki", "Pawel", ""]]}, {"id": "1907.09415", "submitter": "Ronald de Wolf", "authors": "Ronald de Wolf (QuSoft, CWI and University of Amsterdam)", "title": "Quantum Computing: Lecture Notes", "comments": "184 pages. Version 2: added a new chapter about QMA and local\n  Hamiltonian, more exercises in several chapters, and some small\n  corrections/clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a set of lecture notes suitable for a Master's course on quantum\ncomputation and information from the perspective of theoretical computer\nscience. The first version was written in 2011, with many extensions and\nimprovements in subsequent years. The first 10 chapters cover the circuit model\nand the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup\nProblem, Grover, quantum walks, Hamiltonian simulation and HHL). They are\nfollowed by 3 chapters about complexity, 4 chapters about distributed (\"Alice\nand Bob\") settings, and a final chapter about quantum error correction.\nAppendices A and B give a brief introduction to the required linear algebra and\nsome other mathematical and computer science background. All chapters come with\nexercises, with some hints provided in Appendix C.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:56:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 14:51:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["de Wolf", "Ronald", "", "QuSoft, CWI and University of Amsterdam"]]}, {"id": "1907.09531", "submitter": "D\\'aniel Gerbner", "authors": "D\\'aniel Gerbner", "title": "Between the deterministic and non-deterministic query complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems that can be solved by asking certain queries. The\ndeterministic query complexity $D(P)$ of a problem $P$ is the smallest number\nof queries needed to ask in order to find the solution (in the worst case),\nwhile the non-deterministic query complexity $D_0(P)$ is the smallest number of\nqueries needed to ask, in case we know the solution, to prove that it is indeed\nthe solution (in the worst case). Equivalently, $D(P)$ is the largest number of\nqueries needed to find the solution in case an Adversary is answering the\nqueries, while $D_0(P)$ is the largest number of queries needed to find the\nsolution in case an Adversary chooses the input. We define a series of\nquantities between these two values, $D_k(P)$ is the largest number of queries\nneeded to find the solution in case an Adversary chooses the input, and answers\nthe queries, but he can change the input at most $k$ times. We give bounds on\n$D_k(P)$ for various problems $P$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:01:42 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gerbner", "D\u00e1niel", ""]]}, {"id": "1907.09582", "submitter": "Rik Sengupta", "authors": "Neil Immerman and Rik Sengupta", "title": "The $k$-Dimensional Weisfeiler-Leman Algorithm", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide details of the $k$-dimensional Weisfeiler-Leman\nAlgorithm and its analysis from Immerman-Lander (1990). In particular, we\npresent an optimized version of the algorithm that runs in time $O(n^{k+1}\\log\nn)$, where $k$ is fixed (not varying with $n$).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:16:29 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Immerman", "Neil", ""], ["Sengupta", "Rik", ""]]}, {"id": "1907.09820", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Christos Nomikos, Panos Rondogiannis", "title": "The Expressive Power of Higher-Order Datalog", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  24 pages, LaTeX", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 925-940", "doi": "10.1017/S1471068419000279", "report-no": null, "categories": "cs.PL cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result in descriptive complexity theory states that Datalog\nexpresses exactly the class of polynomially computable queries on ordered\ndatabases. In this paper we extend this result to the case of higher-order\nDatalog. In particular, we demonstrate that on ordered databases, for all\n$k\\geq2$, $k$-order Datalog captures $(k-1)$-EXPTIME. This result suggests that\nhigher-order extensions of Datalog possess superior expressive power and they\nare worthwhile of further investigation both in theory and in practice. This\npaper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:21:49 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 14:03:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Nomikos", "Christos", ""], ["Rondogiannis", "Panos", ""]]}, {"id": "1907.10333", "submitter": "Gonzague Yernaux", "authors": "Gonzague Yernaux and Wim Vanhoof", "title": "Anti-unification in Constraint Logic Programming", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 773-789", "doi": "10.1017/S1471068419000188", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anti-unification refers to the process of generalizing two (or more) goals\ninto a single, more general, goal that captures some of the structure that is\ncommon to all initial goals. In general one is typically interested in\ncomputing what is often called a most specific generalization, that is a\ngeneralization that captures a maximal amount of shared structure. In this work\nwe address the problem of anti-unification in CLP, where goals can be seen as\nunordered sets of atoms and/or constraints. We show that while the concept of a\nmost specific generalization can easily be defined in this context, computing\nit becomes an NP-complete problem. We subsequently introduce a generalization\nalgorithm that computes a well-defined abstraction whose computation can be\nbound to a polynomial execution time. Initial experiments show that even a\nnaive implementation of our algorithm produces acceptable generalizations in an\nefficient way. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:55:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yernaux", "Gonzague", ""], ["Vanhoof", "Wim", ""]]}, {"id": "1907.10468", "submitter": "Vittorio Bil\\`o", "authors": "Vittorio Bil\\`o and Marios Mavronicolas", "title": "The Complexity of Computational Problems about Nash Equilibria in\n  Symmetric Win-Lose Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the complexity of deciding, given a {\\it bimatrix game,} whether\nit has a {\\it Nash equilibrium} with certain natural properties; such decision\nproblems were early known to be ${\\mathcal{NP}}$-hard~\\cite{GZ89}. We show that\n${\\mathcal{NP}}$-hardness still holds under two significant restrictions in\nsimultaneity: the game is {\\it win-lose} (that is, all {\\it utilities} are $0$\nor $1$) and {\\it symmetric}. To address the former restriction, we design\nwin-lose {\\it gadgets} and a win-lose reduction; to accomodate the latter\nrestriction, we employ and analyze the classical {\\it\n${\\mathsf{GHR}}$-symmetrization}~\\cite{GHR63} in the win-lose setting. Thus,\n{\\it symmetric win-lose bimatrix games} are as complex as general bimatrix\ngames with respect to such decision problems. As a byproduct of our techniques,\nwe derive hardness results for search, counting and parity problems about Nash\nequilibria in symmetric win-lose bimatrix games.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:42:24 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bil\u00f2", "Vittorio", ""], ["Mavronicolas", "Marios", ""]]}, {"id": "1907.11010", "submitter": "Dominik Velan", "authors": "Tom\\'a\\v{s} Br\\'azdil, Krishnendu Chatterjee, Anton\\'in Ku\\v{c}era,\n  Petr Novotn\\'y, Dominik Velan", "title": "Deciding Fast Termination for Probabilistic VASS with Nondeterminism", "comments": "23 pages, was accepted to ATVA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic vector addition system with states (pVASS) is a finite state\nMarkov process augmented with non-negative integer counters that can be\nincremented or decremented during each state transition, blocking any behaviour\nthat would cause a counter to decrease below zero. The pVASS can be used as\nabstractions of probabilistic programs with many decidable properties. The use\nof pVASS as abstractions requires the presence of nondeterminism in the model.\nIn this paper, we develop techniques for checking fast termination of pVASS\nwith nondeterminism.\n  That is, for every initial configuration of size n, we consider the worst\nexpected number of transitions needed to reach a configuration with some\ncounter negative (the expected termination time). We show that the problem\nwhether the asymptotic expected termination time is linear is decidable in\npolynomial time for a certain natural class of pVASS with nondeterminism.\nFurthermore, we show the following dichotomy: if the asymptotic expected\ntermination time is not linear, then it is at least quadratic, i.e., in\n$\\Omega(n^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:41:23 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Br\u00e1zdil", "Tom\u00e1\u0161", ""], ["Chatterjee", "Krishnendu", ""], ["Ku\u010dera", "Anton\u00edn", ""], ["Novotn\u00fd", "Petr", ""], ["Velan", "Dominik", ""]]}, {"id": "1907.11078", "submitter": "Karol W\\k{e}grzycki", "authors": "Karl Bringmann, Marvin K\\\"unnemann, Karol W\\k{e}grzycki", "title": "Approximating APSP without Scaling: Equivalence of Approximate Min-Plus\n  and Exact Min-Max", "comments": "Presented at STOC'19. Full Version. 35 pages", "journal-ref": null, "doi": "10.1145/3313276.3316373", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zwick's $(1+\\varepsilon)$-approximation algorithm for the All Pairs Shortest\nPath (APSP) problem runs in time $\\widetilde{O}(\\frac{n^\\omega}{\\varepsilon}\n\\log{W})$, where $\\omega \\le 2.373$ is the exponent of matrix multiplication\nand $W$ denotes the largest weight. This can be used to approximate several\ngraph characteristics including the diameter, radius, median, minimum-weight\ntriangle, and minimum-weight cycle in the same time bound.\n  Since Zwick's algorithm uses the scaling technique, it has a factor $\\log W$\nin the running time. In this paper, we study whether APSP and related problems\nadmit approximation schemes avoiding the scaling technique. That is, the number\nof arithmetic operations should be independent of $W$; this is called strongly\npolynomial. Our main results are as follows.\n  - We design approximation schemes in strongly polynomial time\n$O(\\frac{n^\\omega}{\\varepsilon} \\text{polylog}(\\frac{n}{\\varepsilon}))$ for\nAPSP on undirected graphs as well as for the graph characteristics diameter,\nradius, median, minimum-weight triangle, and minimum-weight cycle on directed\nor undirected graphs.\n  - For APSP on directed graphs we design an approximation scheme in strongly\npolynomial time $O(n^{\\frac{\\omega + 3}{2}} \\varepsilon^{-1}\n\\text{polylog}(\\frac{n}{\\varepsilon}))$. This is significantly faster than the\nbest exact algorithm.\n  - We explain why our approximation scheme for APSP on directed graphs has a\nworse exponent than $\\omega$: Any improvement over our exponent $\\frac{\\omega +\n3}{2}$ would improve the best known algorithm for Min-Max Product In fact, we\nprove that approximating directed APSP and exactly computing the Min-Max\nProduct are equivalent.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:14:06 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""], ["W\u0119grzycki", "Karol", ""]]}, {"id": "1907.11338", "submitter": "Margarida Carvalho", "authors": "Margarida Carvalho", "title": "A note on the complexity of integer programming games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief note, we prove that the existence of Nash equilibria on integer\nprogramming games is $\\Sigma^p_2$-complete.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 00:01:46 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 13:21:49 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Carvalho", "Margarida", ""]]}, {"id": "1907.11368", "submitter": "Jeremy Cook", "authors": "Jeremy Cook", "title": "On the relationships between Z-, C-, and H-local unitaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum walk algorithms can speed up search of physical regions of space in\nboth the discrete-time [arXiv:quant-ph/0402107] and continuous-time setting\n[arXiv:quant-ph/0306054], where the physical region of space being searched is\nmodeled as a connected graph. In such a model, Aaronson and Ambainis\n[arXiv:quant-ph/0303041] provide three different criteria for a unitary matrix\nto act locally with respect to a graph, called $Z$-local, $C$-local, and\n$H$-local unitaries, and left the open question of relating these three\nlocality criteria. Using a correspondence between continuous- and discrete-time\nquantum walks by Childs [arXiv:0810.0312], we provide a way to approximate\n$N\\times N$ $H$-local unitaries with error $\\delta$ using\n$O(1/\\sqrt{\\delta},\\sqrt{N})$ $C$-local unitaries, where the comma denotes the\nmaximum of the two terms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:05:26 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 19:49:06 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Cook", "Jeremy", ""]]}, {"id": "1907.11416", "submitter": "Sangram Kishor Jena Mr", "authors": "Sangram K. Jena, Ramesh K. Jallu, Gautam K. Das", "title": "On $d$-distance $m$-tuple ($\\ell, r$)-domination in graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the $d$-distance $m$-tuple ($\\ell, r$)-domination\nproblem. Given a simple undirected graph $G=(V, E)$, and positive integers $d,\nm, \\ell$ and $r$, a subset $V' \\subseteq V$ is said to be a $d$-distance\n$m$-tuple ($\\ell, r$)-dominating set if it satisfies the following conditions:\n(i) each vertex $v \\in V$ is $d$-distance dominated by at least $m$ vertices in\n$V'$, and (ii) each $r$ size subset $U$ of $V$ is $d$-distance dominated by at\nleast $\\ell$ vertices in $V'$. Here, a vertex $v$ is $d$-distance dominated by\nanother vertex $u$ means the shortest path distance between $u$ and $v$ is at\nmost $d$ in $G$. A set $U$ is $d$-distance dominated by a set of $\\ell$\nvertices means size of the union of the $d$-distance neighborhood of all\nvertices of $U$ in $V'$ is at least $\\ell$. The objective of the $d$-distance\n$m$-tuple ($\\ell, r$)-domination problem is to find a minimum size subset $V'\n\\subseteq V$ satisfying the above two conditions.\n  We prove that the problem of deciding whether a graph $G$ has (i) a\n1-distance $m$-tuple ($\\ell, r$)-dominating set for each fixed value of $m,\n\\ell$, and $r$, and (ii) a $d$-distance $m$-tuple ($\\ell, 2$)-dominating set\nfor each fixed value of $d (> 1), m$, and $\\ell$ of cardinality at most $k$\n(here $k$ is a positive integer) are NP-complete. We also prove that for any\n$\\varepsilon>0$, the 1-distance $m$-tuple $(\\ell, r)$-domination problem and\nthe $d$-distance $m$-tuple $(\\ell,2)$-domination problem cannot be approximated\nwithin a factor of $(\\frac{1}{2}- \\varepsilon)\\ln |V|$ and $(\\frac{1}{4}-\n\\varepsilon)\\ln |V|$, respectively, unless $P = NP$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:01:10 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:11:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jena", "Sangram K.", ""], ["Jallu", "Ramesh K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "1907.11632", "submitter": "Michal Parnas", "authors": "Michal Parnas and Adi Shraibman", "title": "On maximal isolation sets in the uniform intersection matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A_{k,t}$ be the matrix that represents the adjacency matrix of the\nintersection bipartite graph of all subsets of size $t$ of $\\{1,2,...,k\\}$. We\ngive constructions of large isolation sets in $A_{k,t}$, where, for a large\nenough $k$, our constructions are the best possible.\n  We first prove that the largest identity submatrix in $A_{k,t}$ is of size\n$k-2t+2$. Then we provide constructions of isolations sets in $A_{k,t}$ for any\n$t\\geq 2$, as follows: \\begin{itemize} \\item If $k = 2t+r$ and $0 \\leq r \\leq\n2t-3$, there exists an isolation set of size $2r+3 = 2k-4t+3$. \\item If $k \\geq\n4t-3$, there exists an isolation set of size $k$. \\end{itemize} The\nconstruction is maximal for $k\\geq 4t-3$, since the Boolean rank of $A_{k,t}$\nis $k$ in this case. As we prove, the construction is maximal also for $k = 2t,\n2t+1$.\n  Finally, we consider the problem of the maximal triangular isolation\nsubmatrix of $A_{k,t}$ that has ones in every entry on the main diagonal and\nbelow it, and zeros elsewhere. We give an optimal construction of such a\nsubmatrix of size $({2t \\choose t}-1) \\times ({2t \\choose t}-1)$, for any $t\n\\geq 1$ and a large enough $k$. This construction is tight, as there is a\nmatching upper bound, which can be derived from a theorem of Frankl about skew\nmatrices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:37:02 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Parnas", "Michal", ""], ["Shraibman", "Adi", ""]]}, {"id": "1907.11635", "submitter": "Alexander Wein", "authors": "Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Subexponential-Time Algorithms for Sparse PCA", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational cost of recovering a unit-norm sparse principal\ncomponent $x \\in \\mathbb{R}^n$ planted in a random matrix, in either the Wigner\nor Wishart spiked model (observing either $W + \\lambda xx^\\top$ with $W$ drawn\nfrom the Gaussian orthogonal ensemble, or $N$ independent samples from\n$\\mathcal{N}(0, I_n + \\beta xx^\\top)$, respectively). Prior work has shown that\nwhen the signal-to-noise ratio ($\\lambda$ or $\\beta\\sqrt{N/n}$, respectively)\nis a small constant and the fraction of nonzero entries in the planted vector\nis $\\|x\\|_0 / n = \\rho$, it is possible to recover $x$ in polynomial time if\n$\\rho \\lesssim 1/\\sqrt{n}$. While it is possible to recover $x$ in exponential\ntime under the weaker condition $\\rho \\ll 1$, it is believed that\npolynomial-time recovery is impossible unless $\\rho \\lesssim 1/\\sqrt{n}$. We\ninvestigate the precise amount of time required for recovery in the \"possible\nbut hard\" regime $1/\\sqrt{n} \\ll \\rho \\ll 1$ by exploring the power of\nsubexponential-time algorithms, i.e., algorithms running in time\n$\\exp(n^\\delta)$ for some constant $\\delta \\in (0,1)$. For any $1/\\sqrt{n} \\ll\n\\rho \\ll 1$, we give a recovery algorithm with runtime roughly $\\exp(\\rho^2\nn)$, demonstrating a smooth tradeoff between sparsity and runtime. Our family\nof algorithms interpolates smoothly between two existing algorithms: the\npolynomial-time diagonal thresholding algorithm and the $\\exp(\\rho n)$-time\nexhaustive search algorithm. Furthermore, by analyzing the low-degree\nlikelihood ratio, we give rigorous evidence suggesting that the tradeoff\nachieved by our algorithms is optimal.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:45:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 02:24:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ding", "Yunzi", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11636", "submitter": "Alexander Wein", "authors": "Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Notes on Computational Hardness of Hypothesis Testing: Predictions using\n  the Low-Degree Likelihood Ratio", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes survey and explore an emerging method, which we call the\nlow-degree method, for predicting and understanding\nstatistical-versus-computational tradeoffs in high-dimensional inference\nproblems. In short, the method posits that a certain quantity -- the second\nmoment of the low-degree likelihood ratio -- gives insight into how much\ncomputational time is required to solve a given hypothesis testing problem,\nwhich can in turn be used to predict the computational hardness of a variety of\nstatistical inference tasks. While this method originated in the study of the\nsum-of-squares (SoS) hierarchy of convex programs, we present a self-contained\nintroduction that does not require knowledge of SoS. In addition to showing how\nto carry out predictions using the method, we include a discussion\ninvestigating both rigorous and conjectural consequences of these predictions.\n  These notes include some new results, simplified proofs, and refined\nconjectures. For instance, we point out a formal connection between spectral\nmethods and the low-degree likelihood ratio, and we give a sharp low-degree\nlower bound against subexponential-time algorithms for tensor PCA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:46:05 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.12061", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Diptapriyo Majumdar, Sebastian Ordyniak, Magnus\n  Wahlstr\\\"om", "title": "Parameterized Pre-coloring Extension and List Coloring Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Golovach, Paulusma and Song (Inf. Comput. 2014) asked to determine the\nparameterized complexity of the following problems parameterized by $k$: (1)\nGiven a graph $G$, a clique modulator $D$ (a clique modulator is a set of\nvertices, whose removal results in a clique) of size $k$ for $G$, and a list\n$L(v)$ of colors for every $v\\in V(G)$, decide whether $G$ has a proper list\ncoloring; (2) Given a graph $G$, a clique modulator $D$ of size $k$ for $G$,\nand a pre-coloring $\\lambda_P: X \\rightarrow Q$ for $X \\subseteq V(G),$ decide\nwhether $\\lambda_P$ can be extended to a proper coloring of $G$ using only\ncolors from $Q.$ For Problem 1 we design an $O^*(2^k)$-time randomized\nalgorithm and for Problem 2 we obtain a kernel with at most $3k$ vertices.\nBanik et al. (IWOCA 2019) proved the the following problem is fixed-parameter\ntractable and asked whether it admits a polynomial kernel: Given a graph $G$,\nan integer $k$, and a list $L(v)$ of exactly $n-k$ colors for every $v \\in\nV(G),$ decide whether there is a proper list coloring for $G.$ We obtain a\nkernel with $O(k^2)$ vertices and colors and a compression to a variation of\nthe problem with $O(k)$ vertices and $O(k^2)$ colors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 10:02:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Gutin", "Gregory", ""], ["Majumdar", "Diptapriyo", ""], ["Ordyniak", "Sebastian", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1907.12287", "submitter": "Christian Engels", "authors": "Markus Blaeser, Christian Engels", "title": "Parameterized Valiant's Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define a theory of parameterized algebraic complexity classes in analogy\nto parameterized Boolean counting classes. We define the classes VFPT and\nVW[t], which mirror the Boolean counting classes #FPT and #W[t], and define\nappropriate reductions and completeness notions. Our main contribution is the\nVW[1]-completeness proof of the parameterized clique family. This proof is far\nmore complicated than in the Boolean world. It requires some new concepts like\ncomposition theorems for bounded exponential sums and Boolean-arithmetic\nformulas. In addition, we also look at two polynomials linked to the permanent\nwith vastly different parameterized complexity.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 09:03:31 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 06:58:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Blaeser", "Markus", ""], ["Engels", "Christian", ""]]}, {"id": "1907.12619", "submitter": "Claude Cr\\'epeau", "authors": "Claude Cr\\'epeau and Nan Yang", "title": "Non-Locality and Zero-Knowledge MIPs", "comments": "33 pages, 14 figures. Submitted to TCC-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The foundation of zero-knowledge is the simulator: a weak machine capable of\npretending to be a weak verifier talking with all-powerful provers. To achieve\nthis, simulators need some kind of advantage such as the knowledge of a\ntrapdoor. In existing zero-knowledge multi-prover protocols, this advantage is\nessentially signalling, something that the provers are explicitly forbidden to\ndo. In most cases, this advantage is stronger than necessary as it is possible\nto define a sense in which simulators need much less to simulate. We define a\nframework in which we can quantify the simulators' non-local advantage and\nexhibit examples of zero-knowledge protocols that are sound against local or\nentangled provers but that are not sound against no-signalling provers\nprecisely because the no-signalling simulation strategy can be adopted by\nmalicious provers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 19:59:46 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Cr\u00e9peau", "Claude", ""], ["Yang", "Nan", ""]]}, {"id": "1907.12713", "submitter": "Cristopher Moore", "authors": "Cristopher Moore", "title": "Lecture Notes on Automata, Languages, and Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These lecture notes are intended as a supplement to Moore and Mertens' The\nNature of Computation or as a standalone resource, and are available to anyone\nwho wants to use them. Comments are welcome, and please let me know if you use\nthese notes in a course. There are 61 exercises.\n  I emphasize that automata are elementary playgrounds where we can explore the\nissues of deterministic and nondeterministic computation. Unlike P vs. NP, we\ncan prove that nondeterminism is equivalent to determinism, or strictly more\npowerful than determinism, in finite-state and push-down automata respectively.\nI also correct several historical and aesthetic injustices: in particular, the\nMyhill-Nerode theorem and the idea of building minimal DFAs from equivalence\nclasses of prefixes is restored to its rightful place above the Pumping Lemma\nfor regular languages. I also discuss the Pumping Lemma for context-free\nlanguages, and briefly discuss counter automata, queue automata, and the\nconnection between unambiguous context-free languages and algebraic generating\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 02:54:30 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Moore", "Cristopher", ""]]}, {"id": "1907.12854", "submitter": "Steffen Schuldenzucker", "authors": "Steffen Schuldenzucker and Sven Seuken", "title": "Monotonic and Non-Monotonic Solution Concepts for Generalized Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized circuits are an important tool in the study of the computational\ncomplexity of equilibrium approximation problems. However, in this paper, we\nreveal that they have a conceptual flaw, namely that the solution concept is\nnot monotonic. By this we mean that if $\\varepsilon < \\varepsilon'$, then an\n$\\varepsilon$-approximate solution for a certain generalized circuit is not\nnecessarily also an $\\varepsilon'$-approximate solution. The reason for this\nnon-monotonicity is the way Boolean operations are modeled. We illustrate that\nnon-monotonicity creates subtle technical issues in prior work that require\nintricate additional arguments to circumvent. To eliminate this problem, we\nshow that the Boolean gates are a redundant feature: one can simulate stronger,\nmonotonic versions of the Boolean gates using the other gate types. Arguing at\nthe level of these stronger Boolean gates eliminates all of the aforementioned\nissues in a natural way. We hope that our results will enable new studies of\nsub-classes of generalized circuits and enabler simpler and more natural\nreductions from generalized circuits to other equilibrium search problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:17:45 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Schuldenzucker", "Steffen", ""], ["Seuken", "Sven", ""]]}, {"id": "1907.13100", "submitter": "Chao Qian", "authors": "Chao Bian, Chao Qian, Yang Yu", "title": "On the Robustness of Median Sampling in Noisy Evolutionary Optimization", "comments": "19 pages. arXiv admin note: text overlap with arXiv:1810.05045,\n  arXiv:1711.00956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world optimization tasks, the objective (i.e., fitness) function\nevaluation is often disturbed by noise due to a wide range of uncertainties.\nEvolutionary algorithms (EAs) have been widely applied to tackle noisy\noptimization, where reducing the negative effect of noise is a crucial issue.\nOne popular strategy to cope with noise is sampling, which evaluates the\nfitness multiple times and uses the sample average to approximate the true\nfitness. In this paper, we introduce median sampling as a noise handling\nstrategy into EAs, which uses the median of the multiple evaluations to\napproximate the true fitness instead of the mean. We theoretically show that\nmedian sampling can reduce the expected running time of EAs from exponential to\npolynomial by considering the (1+1)-EA on OneMax under the commonly used\none-bit noise. We also compare mean sampling with median sampling by\nconsidering two specific noise models, suggesting that when the 2-quantile of\nthe noisy fitness increases with the true fitness, median sampling can be a\nbetter choice. The results provide us with some guidance to employ median\nsampling efficiently in practice.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 11:54:18 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bian", "Chao", ""], ["Qian", "Chao", ""], ["Yu", "Yang", ""]]}]