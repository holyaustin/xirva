[{"id": "1609.00109", "submitter": "Akira Suzuki", "authors": "Hiroki Osawa, Akira Suzuki, Takehiro Ito and Xiao Zhou", "title": "The Complexity of (List) Edge-Coloring Reconfiguration Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph such that each edge has its list of available colors, and\nassume that each list is a subset of the common set consisting of $k$ colors.\nSuppose that we are given two list edge-colorings $f_0$ and $f_r$ of $G$, and\nasked whether there exists a sequence of list edge-colorings of $G$ between\n$f_0$ and $f_r$ such that each list edge-coloring can be obtained from the\nprevious one by changing a color assignment of exactly one edge. This problem\nis known to be PSPACE-complete for every integer $k \\ge 6$ and planar graphs of\nmaximum degree three, but any complexity hardness was unknown for the non-list\nvariant. In this paper, we first improve the known result by proving that, for\nevery integer $k \\ge 4$, the problem remains PSPACE-complete even if an input\ngraph is planar, bounded bandwidth, and of maximum degree three. We then give\nthe first complexity hardness result for the non-list variant: for every\ninteger $k \\ge 5$, we prove that the non-list variant is PSPACE-complete even\nif an input graph is planar, of bandwidth linear in $k$, and of maximum degree\n$k$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 05:05:47 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Osawa", "Hiroki", ""], ["Suzuki", "Akira", ""], ["Ito", "Takehiro", ""], ["Zhou", "Xiao", ""]]}, {"id": "1609.00110", "submitter": "Hector Zenil", "authors": "Hector Zenil, Santiago Hern\\'andez-Orozco, Narsis A. Kiani, Fernando\n  Soler-Toscano and Antonio Rueda-Toicen", "title": "A Decomposition Method for Global Evaluation of Shannon Entropy and\n  Local Estimations of Algorithmic Complexity", "comments": "39 pages, 46 with appendix. 15 figures total and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the properties of a Block Decomposition Method (BDM), which\nextends the power of a Coding Theorem Method (CTM) that approximates local\nestimations of algorithmic complexity based upon Solomonoff-Levin's theory of\nalgorithmic probability providing a closer connection to algorithmic complexity\nthan previous attempts based on statistical regularities e.g. as spotted by\nsome popular lossless compression schemes. The strategy behind BDM is to find\nsmall computer programs that produce the components of a larger, decomposed\nobject. The set of short computer programs can then be artfully arranged in\nsequence so as to produce the original object and to estimate an upper bound on\nthe length of the shortest computer program that produces said original object.\nWe show that the method provides efficient estimations of algorithmic\ncomplexity but that it performs like Shannon entropy when it loses accuracy. We\nestimate errors and study the behaviour of BDM for different boundary\nconditions, all of which are compared and assessed in detail. The measure may\nbe adapted for use with more multi-dimensional objects than strings, objects\nsuch as arrays and tensors. To test the measure we demonstrate the power of CTM\non low algorithmic-randomness objects that are assigned maximal entropy (e.g.\n$\\pi$) but whose numerical approximations are closer to the theoretical low\nalgorithmic-randomness expectation. We also test the measure on larger objects\nincluding dual, isomorphic and cospectral graphs for which we know that\nalgorithmic randomness is low. We also release implementations of the methods\nin most major programming languages---Wolfram Language (Mathematica), Matlab,\nR, Perl, Python, Pascal, C++, and Haskell---and a free online algorithmic\ncomplexity calculator.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 05:13:10 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 22:40:26 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 12:58:02 GMT"}, {"version": "v4", "created": "Sun, 22 Oct 2017 10:17:31 GMT"}, {"version": "v5", "created": "Thu, 1 Feb 2018 22:22:42 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 21:05:09 GMT"}, {"version": "v7", "created": "Mon, 18 Jun 2018 22:00:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Zenil", "Hector", ""], ["Hern\u00e1ndez-Orozco", "Santiago", ""], ["Kiani", "Narsis A.", ""], ["Soler-Toscano", "Fernando", ""], ["Rueda-Toicen", "Antonio", ""]]}, {"id": "1609.00457", "submitter": "Tomoyuki Morimae", "authors": "Tomoyuki Morimae", "title": "Finding resource states of measurement-based quantum computing is harder\n  than quantum computing", "comments": "5 pages, 1 figure", "journal-ref": "Phys. Rev. A 96, 052308 (2017)", "doi": "10.1103/PhysRevA.96.052308", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cond-mat.str-el cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement-based quantum computing enables universal quantum computing with\nonly adaptive single-qubit measurements on certain many-qubit states, such as\nthe graph state, the Affleck-Kennedy-Lieb-Tasaki (AKLT) state, and several\ntensor-network states. Finding new resource states of measurement-based quantum\ncomputing is a hard task, since for a given state there are exponentially many\npossible measurement patterns on the state. In this paper, we consider the\nproblem of deciding, for a given state and a set of unitary operators, whether\nthere exists a way of measurement-based quantum computing on the state that can\nrealize all unitaries in the set, or not. We show that the decision problem is\nQCMA-hard, which means that finding new resource states of measurement-based\nquantum computing is harder than quantum computing itself (unless BQP is equal\nto QCMA). We also derive an upperbound of the decision problem: the problem is\nin a quantum version of the second level of the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 03:08:33 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Morimae", "Tomoyuki", ""]]}, {"id": "1609.01078", "submitter": "J\\'er\\^ome Monnot Mr", "authors": "Laurent Gourv\\`es, J\\'er\\^ome Monnot, Lydia Tlilane", "title": "Subset Sum Problems With Digraph Constraints", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study four optimization problems that generalize the\nwell-known subset sum problem. Given a node-weighted digraph, select a subset\nof vertices whose total weight does not exceed a given budget. Some additional\nconstraints need to be satisfied. The (weak resp.) digraph constraint imposes\nthat if (all incoming nodes of resp.) a node $x$ belongs to the solution, then\nthe latter comprises all its outgoing nodes (node $x$ itself resp.). The\nmaximality constraint ensures that a solution cannot be extended without\nviolating the budget or the (weak) digraph constraint. We study the complexity\nof these problems and we present some approximation results according to the\ntype of digraph given in input, e.g. directed acyclic graphs and oriented\ntrees.\n  Key words. Subset Sum, Maximal problems, digraph constraints, complexity,\ndirected acyclic graphs, oriented trees, PTAS.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 09:52:25 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Gourv\u00e8s", "Laurent", ""], ["Monnot", "J\u00e9r\u00f4me", ""], ["Tlilane", "Lydia", ""]]}, {"id": "1609.01541", "submitter": "Alexandre de Castro", "authors": "Alexandre de Castro", "title": "Quantum one-way permutation over the finite field of two elements", "comments": "16 pages", "journal-ref": "Quantum Information Processing. 16:149 (2017)", "doi": "10.1007/s11128-017-1599-6", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quantum cryptography, a one-way permutation is a bounded unitary operator\n$U:\\mathcal{H} \\to \\mathcal{H}$ on a Hilbert space $\\mathcal{H}$ that is easy\nto compute on every input, but hard to invert given the image of a random\ninput. Levin [Probl. Inf. Transm., vol. 39 (1): 92-103 (2003)] has conjectured\nthat the unitary transformation $g(a,x)=(a,f(x)+ax)$, where $f$ is any\nlength-preserving function and $a,x \\in GF_{{2}^{\\|x\\|}}$, is an\ninformation-theoretically secure operator within a polynomial factor. Here, we\nshow that Levin's one-way permutation is provably secure because its output\nvalues are four maximally entangled two-qubit states, and whose probability of\nfactoring them approaches zero faster than the multiplicative inverse of any\npositive polynomial $poly(x)$ over the Boolean ring of all subsets of $x$. Our\nresults demonstrate through well-known theorems that existence of classical\none-way functions implies existence of a universal quantum one-way permutation\nthat cannot be inverted in subexponential time in the worst ca\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 11:44:18 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 20:02:24 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 22:05:06 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["de Castro", "Alexandre", ""]]}, {"id": "1609.01575", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "On the Existence of Weak One-Way Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is an attempt to unconditionally prove the existence of weak one\nway functions (OWF). Starting from a provably intractable decision problem\n$L_D$ (whose existence is nonconstructively assured from the well-known\ndiscrete time-hierarchy theorem from complexity theory), we construct another\nintractable decision problem $L\\subseteq \\{0,1\\}^*$ that has its words\nscattered across $\\{0,1\\}^\\ell$ at a relative frequency $p(\\ell)$, for which\nupper and lower bounds can be worked out. The value $p(\\ell)$ is computed from\nthe density of the language within $\\{0,1\\}^\\ell$ divided by the total word\ncount $2^\\ell$. It corresponds to the probability of retrieving a yes-instance\nof a decision problem upon a uniformly random draw from $\\{0,1\\}^\\ell$. The\ntrick to find a language with known bounds on $p(\\ell)$ relies on switching\nfrom $L_D$ to $L_0:=L_D\\cap L'$, where $L'$ is an easy-to-decide language with\na known density across $\\{0,1\\}^*$. In defining $L'$ properly (and upon a\nsuitable G\\\"odel numbering), the hardness of deciding $L_D\\cap L'$ is inherited\nfrom $L_D$, while its density is controlled by that of $L'$. The lower and\nupper approximation of $p(\\ell)$ then let us construct an explicit threshold\nfunction (as in random graph theory) that can be used to efficiently and\nintentionally sample yes- or no-instances of the decision problem (language)\n$L_0$ (however, without any auxiliary information that could ease the decision\nlike a polynomial witness). In turn, this allows to construct a weak OWF that\nencodes a bit string $w\\in\\{0,1\\}^*$ by efficiently (in polynomial time)\nemitting a sequence of randomly constructed intractable decision problems,\nwhose answers correspond to the preimage $w$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 14:35:40 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 14:01:38 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 14:15:57 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "1609.01727", "submitter": "Aidan Chatwin-Davies", "authors": "Ning Bao, Aidan Chatwin-Davies", "title": "The Complexity of Identifying Ryu-Takayanagi Surfaces in AdS3/CFT2", "comments": "14 pages, 4 figures, v2: small typo fixes, added Ref. [10]", "journal-ref": null, "doi": "10.1007/JHEP11(2016)034", "report-no": "CALT-TH-2016-023", "categories": "hep-th cs.CC gr-qc quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constructive algorithm for the determination of Ryu-Takayanagi\nsurfaces in AdS3/CFT2 which exploits previously noted connections between\nholographic entanglement entropy and max-flow/min-cut. We then characterize its\ncomplexity as a polynomial time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 20:00:06 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 20:51:16 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Bao", "Ning", ""], ["Chatwin-Davies", "Aidan", ""]]}, {"id": "1609.02103", "submitter": "J. M. Landsberg", "authors": "Klim Efremenko, J.M. Landsberg, Hal Schenck and Jerzy Weyman", "title": "The method of shifted partial derivatives cannot separate the permanent\n  from the determinant", "comments": "This is one half the replacement of arXiv:1504.05171 which has been\n  split into two papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of shifted partial derivatives was used to prove a\nsuper-polynomial lower bound on the size of depth four circuits needed to\ncompute the permanent. We show that this method alone cannot prove that the\npadded permanent $\\ell^{n-m} perm_m$ cannot be realized inside the\n$GL_{n^2}$-orbit closure of the determinant $ det_n$ when $n>2m^2+2m$. Our\nproof relies on several simple degenerations of the determinant polynomial,\nMacaulay's theorem that gives a lower bound on the growth of an ideal, and a\nlower bound estimate from Gupta et. al. regarding the shifted partial\nderivatives of the determinant.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 18:40:00 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Efremenko", "Klim", ""], ["Landsberg", "J. M.", ""], ["Schenck", "Hal", ""], ["Weyman", "Jerzy", ""]]}, {"id": "1609.02562", "submitter": "M. Umut Isik", "authors": "M. Umut Isik", "title": "Complexity Classes and Completeness in Algebraic Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of sequences of projective varieties.\nWe define analogues of the complexity classes P and NP for these and prove the\nNP-completeness of a sequence called the universal circuit resultant. This is\nthe first family of compact spaces shown to be NP-complete in a geometric\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 20:00:04 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Isik", "M. Umut", ""]]}, {"id": "1609.02879", "submitter": "Daniel Perrucci", "authors": "Daniel Perrucci and Marie-Fran\\c{c}oise Roy", "title": "Elementary recursive quantifier elimination based on Thom encoding and\n  sign determination", "comments": "Final version, to appear in Annals of Pure and Applied Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new quantifier elimination algorithm for real closed fields\nbased on Thom encoding and sign determination. The complexity of this algorithm\nis elementary recursive and its proof of correctness is completely algebraic.\nIn particular, the notion of connected components of semialgebraic sets is not\nused.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 18:16:08 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 01:14:53 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Perrucci", "Daniel", ""], ["Roy", "Marie-Fran\u00e7oise", ""]]}, {"id": "1609.02888", "submitter": "Adam Bouland", "authors": "Adam Bouland, Lijie Chen, Dhiraj Holden, Justin Thaler, Prashant\n  Nalini Vasudevan", "title": "On the Power of Statistical Zero Knowledge", "comments": "Changed title and exposition, results unchanged. 71 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the power of statistical zero knowledge proofs (captured by the\ncomplexity class SZK) and their variants. First, we give the strongest known\nrelativized evidence that SZK contains hard problems, by exhibiting an oracle\nrelative to which SZK (indeed, even NISZK) is not contained in the class UPP,\ncontaining those problems solvable by randomized algorithms with unbounded\nerror. This answers an open question of Watrous from 2002 [Aar]. Second, we\n\"lift\" this oracle separation to the setting of communication complexity,\nthereby answering a question of G\\\"o\\\"os et al. (ICALP 2016). Third, we give\nrelativized evidence that perfect zero knowledge proofs (captured by the class\nPZK) are weaker than general zero knowledge proofs. Specifically, we exhibit\noracles relative to which SZK is not contained in PZK, NISZK is not contained\nin NIPZK, and PZK is not equal to coPZK. The first of these results answers a\nquestion raised in 1991 by Aiello and H{\\aa}stad (Information and Computation),\nand the second answers a question of Lovett and Zhang (2016). We also describe\nadditional applications of these results outside of structural complexity.\n  The technical core of our results is a stronger hardness amplification\ntheorem for approximate degree, which roughly says that composing the\ngapped-majority function with any function of high approximate degree yields a\nfunction with high threshold degree.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 18:49:10 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 17:56:59 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 23:21:28 GMT"}, {"version": "v4", "created": "Mon, 8 May 2017 20:37:56 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Bouland", "Adam", ""], ["Chen", "Lijie", ""], ["Holden", "Dhiraj", ""], ["Thaler", "Justin", ""], ["Vasudevan", "Prashant Nalini", ""]]}, {"id": "1609.03278", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "Paraunitary Matrices, Entropy, Algebraic Condition Number and Fourier\n  Computation", "comments": "arXiv admin note: text overlap with arXiv:1404.1741", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fourier Transform is one of the most important linear transformations\nused in science and engineering. Cooley and Tukey's Fast Fourier Transform\n(FFT) from 1964 is a method for computing this transformation in time $O(n\\log\nn)$. From a lower bound perspective, relatively little is known. Ailon shows in\n2013 an $\\Omega(n\\log n)$ bound for computing the normalized Fourier Transform\nassuming only unitary operations on two coordinates are allowed at each step,\nand no extra memory is allowed. In 2014, Ailon then improved the result to show\nthat, in a $\\kappa$-well conditioned computation, Fourier computation can be\nsped up by no more than $O(\\kappa)$. The main conjecture is that Ailon's result\ncan be exponentially improved, in the sense that $\\kappa$-well condition cannot\nadmit $\\omega(\\log \\kappa)$ speedup.\n  The main result here is that `algebraic' $\\kappa$-well condition admits no\nmore than $O(\\sqrt \\kappa)$ speedup. The definition of algebraic condition\nnumber is obtained by formally viewing multiplication by constants, as\nperformed by the algorithm, as multiplication by indeterminates, giving rise to\ncomputation over polynomials. The algebraic condition number is related to the\ndegree of these polynomials. Using the maximum modulus theorem from complex\nanalysis, we show that algebraic condition number upper bounds standard\ncondition number, and equals it in certain cases. Algebraic condition number is\nan interesting measure of numerical computation stability in its own right.\nMoreover, we believe that the approach of algebraic condition number has a good\nchance of establishing an algebraic version of the main conjecture.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 06:43:03 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 20:07:29 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 20:56:49 GMT"}, {"version": "v4", "created": "Fri, 3 Nov 2017 11:06:25 GMT"}, {"version": "v5", "created": "Sat, 9 Dec 2017 09:22:41 GMT"}, {"version": "v6", "created": "Thu, 8 Nov 2018 07:43:12 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Ailon", "Nir", ""]]}, {"id": "1609.03437", "submitter": "Fabio Cozman", "authors": "Fabio Gagliardi Cozman", "title": "First-Order Bayesian Network Specifications Capture the Complexity Class\n  PP", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point of this note is to prove that a language is in the complexity class\nPP if and only if the strings of the language encode valid inferences in a\nBayesian network defined using function-free first-order logic with equality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 15:11:58 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Cozman", "Fabio Gagliardi", ""]]}, {"id": "1609.03650", "submitter": "James P. Crutchfield", "authors": "C. Aghamohammadi, J. R. Mahoney, and J. P. Crutchfield", "title": "Extreme Quantum Advantage when Simulating Strongly Coupled Classical\n  Systems", "comments": "9 pages, 6 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/isingcql.htm; Correct algebraic\n  error", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical stochastic processes can be generated by quantum simulators instead\nof the more standard classical ones, such as hidden Markov models. One reason\nfor using quantum simulators is that they generally require less memory than\ntheir classical counterparts. Here, we examine this quantum advantage for\nstrongly coupled spin systems---the Dyson-like one-dimensional Ising spin chain\nwith variable interaction length. We find that the advantage scales with both\ninteraction range and temperature, growing without bound as interaction\nincreases. Thus, quantum systems can very efficiently simulate strongly coupled\nclassical systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 01:12:09 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 03:57:07 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Aghamohammadi", "C.", ""], ["Mahoney", "J. R.", ""], ["Crutchfield", "J. P.", ""]]}, {"id": "1609.03737", "submitter": "Abbas Bazzi", "authors": "Abbas Bazzi and Samuel Fiorini and Sangxia Huang and Ola Svensson", "title": "Small Extended Formulation for Knapsack Cover Inequalities from Monotone\n  Circuits", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially developed for the min-knapsack problem, the knapsack cover\ninequalities are used in the current best relaxations for numerous\ncombinatorial optimization problems of covering type. In spite of their\nwidespread use, these inequalities yield linear programming (LP) relaxations of\nexponential size, over which it is not known how to optimize exactly in\npolynomial time. In this paper we address this issue and obtain LP relaxations\nof quasi-polynomial size that are at least as strong as that given by the\nknapsack cover inequalities.\n  For the min-knapsack cover problem, our main result can be stated formally as\nfollows: for any $\\varepsilon >0$, there is a $(1/\\varepsilon)^{O(1)}n^{O(\\log\nn)}$-size LP relaxation with an integrality gap of at most $2+\\varepsilon$,\nwhere $n$ is the number of items. Prior to this work, there was no known\nrelaxation of subexponential size with a constant upper bound on the\nintegrality gap.\n  Our construction is inspired by a connection between extended formulations\nand monotone circuit complexity via Karchmer-Wigderson games. In particular,\nour LP is based on $O(\\log^2 n)$-depth monotone circuits with fan-in~$2$ for\nevaluating weighted threshold functions with $n$ inputs, as constructed by\nBeimel and Weinreb. We believe that a further understanding of this connection\nmay lead to more positive results complementing the numerous lower bounds\nrecently proved for extended formulations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 09:20:56 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 22:14:49 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Bazzi", "Abbas", ""], ["Fiorini", "Samuel", ""], ["Huang", "Sangxia", ""], ["Svensson", "Ola", ""]]}, {"id": "1609.03840", "submitter": "Karthik C. S.", "authors": "Karthik C. S.", "title": "Did the Train Reach its Destination: The Complexity of Finding a Witness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Dohrau et al. studied a zero-player game on switch graphs and\nproved that deciding the termination of the game is in NP $\\cap$ coNP. In this\nshort paper, we show that the search version of this game on switch graphs,\ni.e., the task of finding a witness of termination (or of non-termination) is\nin PLS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 14:05:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 17:27:16 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["S.", "Karthik C.", ""]]}, {"id": "1609.04096", "submitter": "EPTCS", "authors": "Pierre Ganty (IMDEA Software Institute, Madrid, Spain), Damir Valput\n  (IMDEA Software Institute, Madrid, Spain)", "title": "Bounded-oscillation Pushdown Automata", "comments": "In Proceedings GandALF 2016, arXiv:1609.03648", "journal-ref": "EPTCS 226, 2016, pp. 178-197", "doi": "10.4204/EPTCS.226.13", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an underapproximation for context-free languages by filtering out\nruns of the underlying pushdown automaton depending on how the stack height\nevolves over time. In particular, we assign to each run a number quantifying\nthe oscillating behavior of the stack along the run. We study languages\naccepted by pushdown automata restricted to k-oscillating runs. We relate\noscillation on pushdown automata with a counterpart restriction on context-free\ngrammars. We also provide a way to filter all but the k-oscillating runs from a\ngiven PDA by annotating stack symbols with information about the oscillation.\nFinally, we study closure properties of the defined class of languages and the\ncomplexity of the k-emptiness problem asking, given a pushdown automaton P and\nk >= 0, whether P has a k-oscillating run. We show that, when k is not part of\nthe input, the k-emptiness problem is NLOGSPACE-complete.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 00:59:26 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Ganty", "Pierre", "", "IMDEA Software Institute, Madrid, Spain"], ["Valput", "Damir", "", "IMDEA Software Institute, Madrid, Spain"]]}, {"id": "1609.04101", "submitter": "EPTCS", "authors": "Yoshiki Nakamura (Tokyo Institute of Technology)", "title": "The Almost Equivalence by Asymptotic Probabilities for Regular Languages\n  and Its Computational Complexities", "comments": "In Proceedings GandALF 2016, arXiv:1609.03648", "journal-ref": "EPTCS 226, 2016, pp. 272-286", "doi": "10.4204/EPTCS.226.19", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce p-equivalence by asymptotic probabilities, which is a weak\nalmost-equivalence based on zero-one laws in finite model theory. In this\npaper, we consider the computational complexities of p-equivalence problems for\nregular languages and provide the following details. First, we give an\nrobustness of p-equivalence and a logical characterization for p-equivalence.\nThe characterization is useful to generate some algorithms for p-equivalence\nproblems by coupling with standard results from descriptive complexity. Second,\nwe give the computational complexities for the p-equivalence problems by the\nlogical characterization. The computational complexities are the same as for\nthe (fully) equivalence problems. Finally, we apply the proofs for\np-equivalence to some generalized equivalences.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:00:25 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Nakamura", "Yoshiki", "", "Tokyo Institute of Technology"]]}, {"id": "1609.04235", "submitter": "Omri Ben-Eliezer", "authors": "Noga Alon and Omri Ben-Eliezer", "title": "Efficient Removal Lemmas for Matrices", "comments": "To appear in RANDOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors and Fischer recently proved that any hereditary property of\ntwo-dimensional matrices (where the row and column order is not ignored) over a\nfinite alphabet is testable with a constant number of queries, by establishing\nthe following (ordered) matrix removal lemma: For any finite alphabet $\\Sigma$,\nany hereditary property $\\mathcal{P}$ of matrices over $\\Sigma$, and any\n$\\epsilon > 0$, there exists $f_{\\mathcal{P}}(\\epsilon)$ such that for any\nmatrix $M$ over $\\Sigma$ that is $\\epsilon$-far from satisfying $\\mathcal{P}$,\nmost of the $f_{\\mathcal{P}}(\\epsilon) \\times f_{\\mathcal{P}}(\\epsilon)$\nsubmatrices of $M$ do not satisfy $\\mathcal{P}$. Here being $\\epsilon$-far from\n$\\mathcal{P}$ means that one needs to modify at least an $\\epsilon$-fraction of\nthe entries of $M$ to make it satisfy $\\mathcal{P}$.\n  However, in the above general removal lemma, $f_{\\mathcal{P}}(\\epsilon)$\ngrows very fast as a function of $\\epsilon^{-1}$, even when $\\mathcal{P}$ is\ncharacterized by a single forbidden submatrix. In this work we establish much\nmore efficient removal lemmas for several special cases of the above problem.\nIn particular, we show the following: For any fixed $s \\times t$ binary matrix\n$A$ and any $\\epsilon > 0$ there exists $\\delta > 0$ polynomial in $\\epsilon$,\nsuch that for any binary matrix $M$ in which less than a $\\delta$-fraction of\nthe $s \\times t$ submatrices are equal to $A$, there exists a set of less than\nan $\\epsilon$-fraction of the entries of $M$ that intersects every $A$-copy in\n$M$.\n  We generalize the work of Alon, Fischer and Newman [SICOMP'07] and make\nprogress towards proving one of their conjectures. The proofs combine their\nefficient conditional regularity lemma for matrices with additional\ncombinatorial and probabilistic ideas.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:22:56 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 15:09:14 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 21:54:50 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""]]}, {"id": "1609.04274", "submitter": "Gustav Nordh", "authors": "Gustav Nordh", "title": "Polymorphisms and Circuit Complexity", "comments": "Updated notation and fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for studying circuit complexity that is inspired by\ntechniques that are used for analyzing the complexity of CSPs. We prove that\nthe circuit complexity of a Boolean function $f$ is characterized by the\npartial polymorphisms of $f$'s truth table. Moreover, the non-deterministic\ncircuit complexity of $f$ is characterized by the polymorphisms of $f$'s truth\ntable.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 13:59:42 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 10:17:30 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Nordh", "Gustav", ""]]}, {"id": "1609.04342", "submitter": "Qian Li", "authors": "Kun He, Qian Li, and Xiaoming Sun", "title": "A Tighter Relation between Sensitivity and Certificate Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sensitivity conjecture which claims that the sensitivity complexity is\npolynomially related to block sensitivity complexity, is one of the most\nimportant and challenging problem in decision tree complexity theory. Despite\nof a lot of efforts, the best known upper bound of block sensitivity, as well\nas the certificate complexity, are still exponential in terms of sensitivity:\n$bs(f)\\leq C(f)\\leq\\max\\{2^{s(f)-1}(s(f)-\\frac{1}{3}),s(f)\\}$. In this paper,\nwe give a better upper bound of $bs(f)\\leq C(f)\\leq(\\frac{8}{9} +\no(1))s(f)2^{s(f) - 1}$. The proof is based on a deep investigation on the\nstructure of the sensitivity graph. We also provide a tighter relationship\nbetween $C_0(f)$ and $s_0(f)$ for functions with $s_1(f)=2$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 16:45:39 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["He", "Kun", ""], ["Li", "Qian", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1609.04449", "submitter": "Samson Zhou", "authors": "Jeremiah Blocki, Samson Zhou", "title": "On the Computational Complexity of Minimal Cumulative Cost Graph\n  Pebbling", "comments": "Full version of Financial Cryptography and Data Security 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the computational complexity of finding a legal black pebbling of\na DAG $G=(V,E)$ with minimum cumulative cost. A black pebbling is a sequence\n$P_0,\\ldots, P_t \\subseteq V$ of sets of nodes which must satisfy the following\nproperties: $P_0 = \\emptyset$ (we start off with no pebbles on $G$),\n$\\mathsf{sinks}(G) \\subseteq \\bigcup_{j \\leq t} P_j$ (every sink node was\npebbled at some point) and $\\mathsf{parents}\\big(P_{i+1}\\backslash P_i\\big)\n\\subseteq P_i$ (we can only place a new pebble on a node $v$ if all of $v$'s\nparents had a pebble during the last round). The cumulative cost of a pebbling\n$P_0,P_1,\\ldots, P_t \\subseteq V$ is $\\mathsf{cc}(P) = | P_1| + \\ldots + |\nP_t|$. The cumulative pebbling cost is an especially important security metric\nfor data-independent memory hard functions, an important primitive for password\nhashing. Thus, an efficient (approximation) algorithm would be an invaluable\ntool for the cryptanalysis of password hash functions as it would provide an\nautomated tool to establish tight bounds on the amortized space-time cost of\ncomputing the function. We show that such a tool is unlikely to exist. In\nparticular, we prove the following results. (1) It is\n$\\texttt{NP}\\mbox{-}\\texttt{Hard}$ to find a pebbling minimizing cumulative\ncost. (2) The natural linear program relaxation for the problem has integrality\ngap $\\tilde{O}(n)$, where $n$ is the number of nodes in $G$. We conjecture that\nthe problem is hard to approximate. (3) We show that a related problem, find\nthe minimum size subset $S\\subseteq V$ such that $\\textsf{depth}(G-S) \\leq d$,\nis also $\\texttt{NP}\\mbox{-}\\texttt{Hard}$. In fact, under the unique games\nconjecture there is no $(2-\\epsilon)$-approximation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 21:22:51 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 20:52:50 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Zhou", "Samson", ""]]}, {"id": "1609.04593", "submitter": "Etienne Birmele", "authors": "Etienne Birmel\\'e (MAP5), Fabien De Montgolfier (IRIF), L\\'eo Planche\n  (MAP5, IRIF)", "title": "Minimum Eccentricity Shortest Path Problem: an Approximation Algorithm\n  and Relation with the k-Laminarity Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": "MAP5 2016-26", "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Eccentricity Shortest Path (MESP) Problem consists in determining\na shortest path (a path whose length is the distance between its extremities)\nof minimum eccentricity in a graph. It was introduced by Dragan and Leitert [9]\nwho described a linear-time algorithm which is an 8-approximation of the\nproblem. In this paper, we study deeper the double-BFS procedure used in that\nalgorithm and extend it to obtain a linear-time 3-approximation algorithm. We\nmoreover study the link between the MESP problem and the notion of laminarity,\nintroduced by V{\\\"o}lkel et al [12], corresponding to its restriction to a\ndiameter (i.e. a shortest path of maximum length), and show tight bounds\nbetween MESP and laminarity parameters.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 12:18:57 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Birmel\u00e9", "Etienne", "", "MAP5"], ["De Montgolfier", "Fabien", "", "IRIF"], ["Planche", "L\u00e9o", "", "MAP5, IRIF"]]}, {"id": "1609.04918", "submitter": "Jimmy Wu", "authors": "Alex Khodaverdian, Benjamin Weitz, Jimmy Wu, Nir Yosef", "title": "Steiner Network Problems on Temporal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a temporal Steiner network problem in which a graph, as well as\nchanges to its edges and/or vertices over a set of discrete times, are given as\ninput; the goal is to find a minimal subgraph satisfying a set of $k$\ntime-sensitive connectivity demands. We show that this problem, $k$-Temporal\nSteiner Network ($k$-TSN), is NP-hard to approximate to a factor of $k -\n\\epsilon$, for every fixed $k \\geq 2$ and $\\epsilon > 0$. This bound is tight,\nas certified by a trivial approximation algorithm. Conceptually this\ndemonstrates, in contrast to known results for traditional Steiner problems,\nthat a time dimension adds considerable complexity even when the problem is\noffline.\n  We also discuss special cases of $k$-TSN in which the graph changes satisfy a\nmonotonicity property. We show approximation-preserving reductions from\nmonotonic $k$-TSN to well-studied problems such as Priority Steiner Tree and\nDirected Steiner Tree, implying improved approximation algorithms.\n  Lastly, $k$-TSN and its variants arise naturally in computational biology; to\nfacilitate such applications, we devise an integer linear program for $k$-TSN\nbased on network flows.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 06:49:22 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 03:30:55 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Khodaverdian", "Alex", ""], ["Weitz", "Benjamin", ""], ["Wu", "Jimmy", ""], ["Yosef", "Nir", ""]]}, {"id": "1609.05136", "submitter": "Aris Filos-Ratsikas", "authors": "Aris Filos-Ratsikas, Soren Kristoffer Stiil Frederiksen, Paul W.\n  Goldberg and Jie Zhang", "title": "Hardness Results for Consensus-Halving", "comments": "Published in MFCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the consensus-halving problem of dividing an object into two\nportions, such that each of $n$ agents has equal valuation for the two\nportions. The $\\epsilon$-approximate consensus-halving problem allows each\nagent to have an $\\epsilon$ discrepancy on the values of the portions. We prove\nthat computing $\\epsilon$-approximate consensus-halving solution using $n$ cuts\nis in PPA, and is PPAD-hard, where $\\epsilon$ is some positive constant; the\nproblem remains PPAD-hard when we allow a constant number of additional cuts.\nIt is NP-hard to decide whether a solution with $n-1$ cuts exists for the\nproblem. As a corollary of our results, we obtain that the approximate\ncomputational version of the Continuous Necklace Splitting Problem is PPAD-hard\nwhen the number of portions $t$ is two.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:51:23 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 15:50:00 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Filos-Ratsikas", "Aris", ""], ["Frederiksen", "Soren Kristoffer Stiil", ""], ["Goldberg", "Paul W.", ""], ["Zhang", "Jie", ""]]}, {"id": "1609.05320", "submitter": "Ilan Karpas", "authors": "Ilan Karpas", "title": "Lower bounds for sensitivity of graph properties", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the sensitivity of any non-trivial graph property on $n$\nvertices is at least $\\lfloor \\frac{1}{2}n \\rfloor$ , provided $n$ is\nsufficiently large.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 11:42:26 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Karpas", "Ilan", ""]]}, {"id": "1609.05472", "submitter": "Weidong Luo", "authors": "Weidong Luo", "title": "Frameworks for Solving Turing Kernel Lower Bound Problem and Finding\n  Natural Candidate Problems in NP-intermediate", "comments": "25 pages, 1 figure. Some corrections in order to making the article\n  more readable (include: corrected typos and syntax errors, added references\n  and some details, replaced some examples etc.), and the results in the paper\n  are unimpaired. Added some new results to the parts of section 4.2 and\n  section 4.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is a significant topic in parameterized complexity. Turing\nkernelization is a general form of kernelization. In the aspect of\nkernelization, an impressive hardness theory has been established [Bodlaender\netc. (ICALP 2008, JCSS2009), Fortnow and Santhanam (STOC 2008, JCSS 2011), Dell\nand van Melkebeek (STOC 2010, J. ACM 2014), Drucker (FOCS 2012, SIAM J. Comput.\n2015)], which can obtain lower bounds of kernel size. Unfortunately, there is\nyet no tool can prove Turing kernel lower bound for any FPT problem modulo any\nreasonable complexity hypothesis. Thus, constructing a framework for Turing\nkernel lower bound was proposed as an open problem in different occasions\n[Fernau etc. (STACS 2009), Misra etc. (Discrete Optimization 2011), Kratsch\n(Bulletin of the EATCS 2014), Cygan etc. (Dagstuhl Seminars on kernels 2014)].\nLadner [J. ACM 1975] proved that if $P \\not = NP$, then there exist infinitely\nmany NP-intermediate problems. However, the NP-intermediate problem constructed\nby Ladner is artificial. Thus, finding natural NP-intermediate problems under\nthe assumption of $P \\not= NP$ is a longstanding open problem in computation\ncomplexity community. This paper builds a new bridge between parameterized\ncomplexity and classic computational complexity. By using this new connection,\nsome frameworks can be constructed. Based on the assumption that the polynomial\nhierarchy and the exponential hierarchy will not collapse, these frameworks\nhave three main applications. Firstly, these frameworks can be used to obtain\nTuring kernel lower bounds of some important FPT problems, thus solving the\nfirst open problem. Secondly, these frameworks can also be used to obtain\nbetter kernel lower bounds for these problems. Thirdly, these frameworks can be\nused to figure out a large number of natural problems in NP-intermediate, thus\nmaking some contributions to the second open problem.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 11:36:08 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 09:00:19 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Luo", "Weidong", ""]]}, {"id": "1609.05537", "submitter": "Fernando Brandao", "authors": "Fernando G.S.L. Brandao and Krysta Svore", "title": "Quantum Speed-ups for Semidefinite Programming", "comments": "24 pages. v2: modification of input model 2 and minor revisions v3:\n  several errors corrected, v4: more corrections and clarifications, v5:\n  published version, Proceedings FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a quantum algorithm for solving semidefinite programs (SDPs). It has\nworst-case running time $n^{\\frac{1}{2}} m^{\\frac{1}{2}} s^2\n\\text{poly}(\\log(n), \\log(m), R, r, 1/\\delta)$, with $n$ and $s$ the dimension\nand row-sparsity of the input matrices, respectively, $m$ the number of\nconstraints, $\\delta$ the accuracy of the solution, and $R, r$ a upper bounds\non the size of the optimal primal and dual solutions. This gives a square-root\nunconditional speed-up over any classical method for solving SDPs both in $n$\nand $m$. We prove the algorithm cannot be substantially improved (in terms of\n$n$ and $m$) giving a $\\Omega(n^{\\frac{1}{2}}+m^{\\frac{1}{2}})$ quantum lower\nbound for solving semidefinite programs with constant $s, R, r$ and $\\delta$.\n  The quantum algorithm is constructed by a combination of quantum Gibbs\nsampling and the multiplicative weight method. In particular it is based on a\nclassical algorithm of Arora and Kale for approximately solving SDPs. We\npresent a modification of their algorithm to eliminate the need for solving an\ninner linear program which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 20:13:50 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 17:01:24 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 17:53:24 GMT"}, {"version": "v4", "created": "Thu, 20 Apr 2017 21:52:51 GMT"}, {"version": "v5", "created": "Sun, 24 Sep 2017 02:03:23 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Brandao", "Fernando G. S. L.", ""], ["Svore", "Krysta", ""]]}, {"id": "1609.05709", "submitter": "Marcel R\\'emon", "authors": "Marcel R\\'emon and Johan Barth\\'elemy", "title": "A 3-CNF-SAT descriptor algebra and the solution of the P=NP conjecture", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "Report naXys-04-2016 (www.naxys.be)", "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between the complexity classes P and NP is an unsolved\nquestion in the field of theoretical computer science. In this paper, we\ninvestigate a descriptor approach based on lattice properties. This paper\nproposes a new way to decide the satisfiability of any 3-CNF-SAT problem. The\nanalysis of this exact [non heuristical] algorithm shows a strictly bounded\nexponential complexity. The complexity of any 3-CNF-SAT solution is bounded by\nO(2^490). This over-estimated bound is reached by an algorithm working on the\nsmallest description (via descriptor functions) of the evolving set of\nsolutions in function of the already considered clauses, without exploring\nthese solutions. Any remark about this paper is warmly welcome.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 20:43:17 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["R\u00e9mon", "Marcel", ""], ["Barth\u00e9lemy", "Johan", ""]]}, {"id": "1609.05876", "submitter": "Roberto Alonso", "authors": "Roberto Alonso, Ra\\'ul Monroy, Eduardo Aguirre", "title": "On the Phase Transition of Finding a Biclique in a larger Bipartite\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the phase transition of finding a complete subgraph, of\nspecified dimensions, in a bipartite graph. Finding a complete subgraph in a\nbipartite graph is a problem that has growing attention in several domains,\nincluding bioinformatics, social network analysis and domain clustering. A key\nstep for a successful phase transition study is identifying a suitable order\nparameter, when none is known. To this purpose, we have applied a decision tree\nclassifier to real-world instances of this problem, in order to understand what\nproblem features separate an instance that is hard to solve from those that is\nnot. We have successfully identified one such order parameter and with it the\nphase transition of finding a complete bipartite subgraph of specified\ndimensions. Our phase transition study shows an\neasy-to-hard-to-easy-to-hard-to-easy pattern. Further, our results indicate\nthat the hardest instances are in a region where it is more likely that the\ncorresponding bipartite graph will have a complete subgraph of specified\ndimensions, a positive answer. By contrast, instances with a negative answer\nare more likely to appear in a region where the computational cost is\nnegligible. This behaviour is remarkably similar for problems of a number of\ndifferent sizes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:22:08 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Alonso", "Roberto", ""], ["Monroy", "Ra\u00fal", ""], ["Aguirre", "Eduardo", ""]]}, {"id": "1609.05942", "submitter": "Christian Ikenmeyer", "authors": "Christian Ikenmeyer and Stefan Mengel", "title": "On the relative power of reduction notions in arithmetic circuit\n  complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the two main reduction notions in arithmetic circuit complexity,\np-projections and c-reductions, differ in power. We do so by showing\nunconditionally that there are polynomials that are VNP-complete under\nc-reductions but not under p-projections. We also show that the question of\nwhich polynomials are VNP-complete under which type of reductions depends on\nthe underlying field.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 21:18:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Ikenmeyer", "Christian", ""], ["Mengel", "Stefan", ""]]}, {"id": "1609.05984", "submitter": "Marius Zimand", "authors": "Marius Zimand", "title": "List approximation for increasing Kolmogorov complexity", "comments": "This version corrects a bug in the previous version, which\n  unfortunately is published in the proceedings of STACS 2017. Theorem 1.1\n  holds with parameters that are slightly weaker than previously claimed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is impossible to effectively modify a string in order to increase its\nKolmogorov complexity. But is it possible to construct a few strings, not\nlonger than the input string, so that most of them have larger complexity? We\nshow that the answer is yes. We present an algorithm that on input a string $x$\nof length $n$ returns a list with $O(n^2)$ many strings, all of length $n$,\nsuch that 99\\% of them are more complex than $x$, provided the complexity of\n$x$ is less than $n - \\log \\log n - O(1)$. We obtain similar results for other\nparameters, including a polynomial-time construction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 01:19:02 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 18:38:57 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 16:39:17 GMT"}, {"version": "v4", "created": "Mon, 6 Feb 2017 15:27:43 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Zimand", "Marius", ""]]}, {"id": "1609.06355", "submitter": "Sivakanth Gopi", "authors": "Jop Bri\\\"et, Zeev Dvir and Sivakanth Gopi", "title": "Outlaw distributions and locally decodable codes", "comments": "A preliminary version of this paper appeared in the proceedings of\n  ITCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally decodable codes (LDCs) are error correcting codes that allow for\ndecoding of a single message bit using a small number of queries to a corrupted\nencoding. Despite decades of study, the optimal trade-off between query\ncomplexity and codeword length is far from understood. In this work, we give a\nnew characterization of LDCs using distributions over Boolean functions whose\nexpectation is hard to approximate (in~$L_\\infty$~norm) with a small number of\nsamples. We coin the term `outlaw distributions' for such distributions since\nthey `defy' the Law of Large Numbers. We show that the existence of outlaw\ndistributions over sufficiently `smooth' functions implies the existence of\nconstant query LDCs and vice versa. We give several candidates for outlaw\ndistributions over smooth functions coming from finite field incidence\ngeometry, additive combinatorics and from hypergraph (non)expanders.\n  We also prove a useful lemma showing that (smooth) LDCs which are only\nrequired to work on average over a random message and a random message index\ncan be turned into true LDCs at the cost of only constant factors in the\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 21:02:02 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 21:42:41 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Bri\u00ebt", "Jop", ""], ["Dvir", "Zeev", ""], ["Gopi", "Sivakanth", ""]]}, {"id": "1609.06508", "submitter": "Mika\\\"el Rabie", "authors": "Rabie Mika\\\"el", "title": "Global Versus Local Computations: Fast Computing with Identifiers", "comments": "Long version of SSS 2016 publication, appendixed version of SIROCCO\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies what can be computed by using probabilistic local\ninteractions with agents with a very restricted power in polylogarithmic\nparallel time. It is known that if agents are only finite state (corresponding\nto the Population Protocol model by Angluin et al.), then only semilinear\npredicates over the global input can be computed. In fact, if the population\nstarts with a unique leader, these predicates can even be computed in a\npolylogarithmic parallel time. If identifiers are added (corresponding to the\nCommunity Protocol model by Guerraoui and Ruppert), then more global predicates\nover the input multiset can be computed. Local predicates over the input sorted\naccording to the identifiers can also be computed, as long as the identifiers\nare ordered. The time of some of those predicates might require exponential\nparallel time. In this paper, we consider what can be computed with Community\nProtocol in a polylogarithmic number of parallel interactions. We introduce the\nclass CPPL corresponding to protocols that use $O(n\\log^k n)$, for some k,\nexpected interactions to compute their predicates, or equivalently a\npolylogarithmic number of parallel expected interactions. We provide some\ncomputable protocols, some boundaries of the class, using the fact that the\npopulation can compute its size. We also prove two impossibility results\nproviding some arguments showing that local computations are no longer easy:\nthe population does not have the time to compare a linear number of consecutive\nidentifiers. The Linearly Local languages, such that the rational language\n$(ab)^*$, are not computable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 11:44:25 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 11:39:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Mika\u00ebl", "Rabie", ""]]}, {"id": "1609.06515", "submitter": "Shunichi Matsubara", "authors": "Shunichi Matsubara", "title": "The Complexity of the Numerical Semigroup Gap Counting Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that the numerical-semigroup-gap counting problem is\n#NP-complete as a main theorem. A numerical semigroup is an additive semigroup\nover the set of all nonnegative integers. A gap of a numerical semigroup is\ndefined as a positive integer that does not belong to the numerical semigroup.\nThe computation of gaps of numerical semigroups has been actively studied from\nthe 19th century. However, little has been known on the computational\ncomplexity. In 2005, Ramirez-Alfonsin proposed a question whether or not the\nnumerical-semigroup-gap counting problem is #P-complete. This work is an answer\nfor his question. For proving the main theorem, we show the #NP-completenesses\nof other two variants of the numerical-semigroup-gap counting problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:08:53 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 03:23:41 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Matsubara", "Shunichi", ""]]}, {"id": "1609.06736", "submitter": "Oded Lachish Dr", "authors": "Eldar Fischer and Oded Lachish and Yadu Vasudev", "title": "Improving and extending the testing of distributions for\n  shape-restricted properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution testing deals with what information can be deduced about an\nunknown distribution over $\\{1,\\ldots,n\\}$, where the algorithm is only allowed\nto obtain a relatively small number of independent samples from the\ndistribution. In the extended conditional sampling model, the algorithm is also\nallowed to obtain samples from the restriction of the original distribution on\nsubsets of $\\{1,\\ldots,n\\}$.\n  In 2015, Canonne, Diakonikolas, Gouleakis and Rubinfeld unified several\nprevious results, and showed that for any property of distributions satisfying\na \"decomposability\" criterion, there exists an algorithm (in the basic model)\nthat can distinguish with high probability distributions satisfying the\nproperty from distributions that are far from it in the variation distance.\n  We present here a more efficient yet simpler algorithm for the basic model,\nas well as very efficient algorithms for the conditional model, which until now\nwas not investigated under the umbrella of decomposable properties.\nAdditionally, we provide an algorithm for the conditional model that handles a\nmuch larger class of properties.\n  Our core mechanism is a way of efficiently producing an interval-partition of\n$\\{1,\\ldots,n\\}$ that satisfies a \"fine-grain\" quality. We show that with such\na partition at hand we can directly move forward with testing individual\nintervals, instead of first searching for the \"correct\" partition of\n$\\{1,\\ldots,n\\}$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 20:15:14 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Fischer", "Eldar", ""], ["Lachish", "Oded", ""], ["Vasudev", "Yadu", ""]]}, {"id": "1609.07048", "submitter": "Arno Pauly", "authors": "St\\'ephane Le Roux and Arno Pauly and Jean-Fran\\c{c}ois Raskin", "title": "Minkowski games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study Minkowski games. These are two player games, where the\nplayers take turns to chose positions in $\\mathbb{R}^d$ based on some rules.\nVariants include boundedness games, where one player wants to keep the\npositions bounded, and the other wants to escape to infinity; as well as safety\ngames, where one player wants to stay within a prescribed set, while the other\nwants to leave it.\n  We provide some general characterizations of which player can win such games,\nand explore the computational complexity of the associated decision problems. A\nnatural representation of boundedness games yields coNP-completeness, whereas\nthe safety games are undecidable.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:10:15 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 17:40:33 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Roux", "St\u00e9phane Le", ""], ["Pauly", "Arno", ""], ["Raskin", "Jean-Fran\u00e7ois", ""]]}, {"id": "1609.07059", "submitter": "Lucas Boczkowski", "authors": "Lucas Boczkowski, Iordanis Kerenidis, Fr\\'ed\\'eric Magniez", "title": "Streaming Communication Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the Streaming Communication model that combines the main aspects of\ncommunication complexity and streaming. We consider two agents that want to\ncompute some function that depends on inputs that are distributed to each\nagent. The inputs arrive as data streams and each agent has a bounded memory.\nAgents are allowed to communicate with each other and also update their memory\nbased on the input bit they read and the previous message they received.\n  We provide tight tradeoffs between the necessary resources, i.e.\ncommunication and memory, for some of the canonical problems from communication\ncomplexity by proving a strong general lower bound technique. Second, we\nanalyze the Approximate Matching problem and show that the complexity of this\nproblem (i.e. the achievable approximation ratio) in the one-way variant of our\nmodel is strictly different both from the streaming complexity and the one-way\ncommunication complexity thereof.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:41:23 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Boczkowski", "Lucas", ""], ["Kerenidis", "Iordanis", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1609.07460", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, Xenophon Chatzigeorgiou, George A. Krimpas,\n  Alexandros A. Voudouris", "title": "Optimizing positional scoring rules for rank aggregation", "comments": "Accepted to Artificial Intelligence Journal. A preliminary version\n  appeared in Proceedings of the 31st AAAI Conference on Artificial\n  Intelligence (AAAI), pages 430-436, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, several crowdsourcing projects exploit social choice methods for\ncomputing an aggregate ranking of alternatives given individual rankings\nprovided by workers. Motivated by such systems, we consider a setting where\neach worker is asked to rank a fixed (small) number of alternatives and, then,\na positional scoring rule is used to compute the aggregate ranking. Among the\napparently infinite such rules, what is the best one to use? To answer this\nquestion, we assume that we have partial access to an underlying true ranking.\nThen, the important optimization problem to be solved is to compute the\npositional scoring rule whose outcome, when applied to the profile of\nindividual rankings, is as close as possible to the part of the underlying true\nranking we know. We study this fundamental problem from a theoretical viewpoint\nand present positive and negative complexity results and, furthermore,\ncomplement our theoretical findings with experiments on real-world and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 17:18:55 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 15:00:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Chatzigeorgiou", "Xenophon", ""], ["Krimpas", "George A.", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1609.07476", "submitter": "Jeroen Zuiddam", "authors": "Matthias Christandl, P\\'eter Vrana, Jeroen Zuiddam", "title": "Asymptotic tensor rank of graph tensors: beyond matrix multiplication", "comments": null, "journal-ref": "J. comput. complex. (2019) 28: 57", "doi": "10.1007/s00037-018-0172-8", "report-no": null, "categories": "math.CO cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an upper bound on the exponent of the asymptotic behaviour of the\ntensor rank of a family of tensors defined by the complete graph on $k$\nvertices. For $k\\geq4$, we show that the exponent per edge is at most 0.77,\noutperforming the best known upper bound on the exponent per edge for matrix\nmultiplication ($k=3$), which is approximately 0.79. We raise the question\nwhether for some $k$ the exponent per edge can be below $2/3$, i.e. can\noutperform matrix multiplication even if the matrix multiplication exponent\nequals 2. In order to obtain our results, we generalise to higher order tensors\na result by Strassen on the asymptotic subrank of tight tensors and a result by\nCoppersmith and Winograd on the asymptotic rank of matrix multiplication. Our\nresults have applications in entanglement theory and communication complexity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 19:55:05 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 13:24:49 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Christandl", "Matthias", ""], ["Vrana", "P\u00e9ter", ""], ["Zuiddam", "Jeroen", ""]]}, {"id": "1609.07554", "submitter": "Enrico Borriello Dr.", "authors": "Enrico Borriello and Sara Imari Walker", "title": "An information-based classification of Elementary Cellular Automata", "comments": "6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel, information-based classification of elementary cellular automata is\nproposed that circumvents the problems associated with isolating whether\ncomplexity is in fact intrinsic to a dynamical rule, or if it arises merely as\na product of a complex initial state. Transfer entropy variations processed by\nthe system split the 256 elementary rules into three information classes, based\non sensitivity to initial conditions. These classes form a hierarchy such that\ncoarse-graining transitions observed among elementary cellular automata rules\npredominately occur within each information- based class, or much more rarely,\ndown the hierarchy.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 02:17:44 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 01:02:18 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Borriello", "Enrico", ""], ["Walker", "Sara Imari", ""]]}, {"id": "1609.07786", "submitter": "Fr\\'ed\\'eric Magniez", "authors": "Titouan Carette and Mathieu Lauri\\`ere and Fr\\'ed\\'eric Magniez", "title": "Extended Learning Graphs for Triangle Finding", "comments": "Fixing few typos in references", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new quantum algorithms for Triangle Finding improving its best\npreviously known quantum query complexities for both dense and spare\ninstances.For dense graphs on $n$ vertices, we get a query complexity of\n$O(n^{5/4})$ without any of the extra logarithmic factors present in the\nprevious algorithm of Le Gall [FOCS'14]. For sparse graphs with $m\\geq n^{5/4}$\nedges, we get a query complexity of $O(n^{11/12}m^{1/6}\\sqrt{\\log n})$, which\nis better than the one obtained by Le Gall and Nakajima [ISAAC'15] when $m \\geq\nn^{3/2}$. We also obtain an algorithm with query complexity ${O}(n^{5/6}(m\\log\nn)^{1/6}+d_2\\sqrt{n})$ where $d_2$ is the variance of the degree distribution.\nOur algorithms are designed and analyzed in a new model of learning graphs that\nwe call extended learning graphs. In addition, we present a framework in order\nto easily combine and analyze them. As a consequence we get much simpler\nalgorithms and analyses than previous algorithms of Le Gall {\\it et al} based\non the MNRS quantum walk framework [SICOMP'11].\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 19:33:40 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 20:01:25 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Carette", "Titouan", ""], ["Lauri\u00e8re", "Mathieu", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1609.07895", "submitter": "Thomas Seiller", "authors": "Thomas Seiller", "title": "Interaction Graphs: Nondeterministic Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exhibits a series of semantic characterisations of sublinear\nnondeterministic complexity classes. These results fall into the general domain\nof logic-based approaches to complexity theory and so-called implicit\ncomputational complexity (ICC), i.e. descriptions of complexity classes without\nreference to specific machine models. In particular, it relates strongly to ICC\nresults based on linear logic since the semantic framework considered stems\nfrom work on the latter. Moreover, the obtained characterisations are of a\ngeometric nature: each class is characterised by a specific action of a group\nby measure-preserving maps.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 09:22:15 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Seiller", "Thomas", ""]]}, {"id": "1609.08059", "submitter": "Amaury Pouly", "authors": "Olivier Bournez, Daniel S. Gra\\c{a}a, Amaury Pouly", "title": "Polynomial Time Corresponds to Solutions of Polynomial Ordinary\n  Differential Equations of Polynomial Length (Journal version)", "comments": "arXiv admin note: text overlap with arXiv:1601.05360 Submitted to\n  JACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outcomes of this paper are twofold. Implicit complexity.\n  We provide an implicit characterization of polynomial time computation in\nterms of ordinary differential equations: we characterize the class PTIME of\nlanguages computable in polynomial time in terms of differential equations with\npolynomial right-hand side. This result gives a purely continuous elegant and\nsimple characterization of PTIME. We believe it is the first time complexity\nclasses are characterized using only ordinary differential equations. Our\ncharacterization extends to functions computable in polynomial time over the\nreals in the sense of computable analysis. Our results may provide a new\nperspective on classical complexity, by giving a way to define complexity\nclasses, like PTIME, in a very simple way, without any reference to a notion of\n(discrete) machine. This may also provide ways to state classical questions\nabout computational complexity via ordinary differential equations.\n  Continuous-Time Models of Computation.\n  Our results can also be interpreted in terms of analog computers or analog\nmodels of computation: As a side effect, we get that the 1941 General Purpose\nAnalog Computer (GPAC) of Claude Shannon is provably equivalent to Turing\nmachines both in terms of computability and complexity, a fact that has never\nbeen established before. This result provides arguments in favour of a\ngeneralised form of the Church-Turing Hypothesis, which states that any\nphysically realistic (macroscopic) computer is equivalent to Turing machines\nboth in terms of computability and complexity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 16:13:28 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 07:26:21 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 03:25:43 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Bournez", "Olivier", ""], ["Gracaa", "Daniel S.", ""], ["Pouly", "Amaury", ""]]}, {"id": "1609.08095", "submitter": "Ignasi Sau", "authors": "Marin Bougeret, Ignasi Sau", "title": "How much does a treedepth modulator help to obtain polynomial kernels\n  beyond sparse graphs?", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, kernelization with structural parameters has been an\nactive area of research within the field of parameterized complexity. As a\nrelevant example, Gajarsk{\\`y} et al. [ESA 2013] proved that every graph\nproblem satisfying a property called finite integer index admits a linear\nkernel on graphs of bounded expansion and an almost linear kernel on nowhere\ndense graphs, parameterized by the size of a $c$-treedepth modulator, which is\na vertex set whose removal results in a graph of treedepth at most $c$, where\n$c \\geq 1$ is a fixed integer. The authors left as further research to\ninvestigate this parameter on general graphs, and in particular to find\nproblems that, while admitting polynomial kernels on sparse graphs, behave\ndifferently on general graphs.\n  In this article we answer this question by finding two very natural such\nproblems: we prove that Vertex Cover admits a polynomial kernel on general\ngraphs for any integer $c \\geq 1$, and that Dominating Set does not for any\ninteger $c \\geq 2$ even on degenerate graphs, unless $\\text{NP} \\subseteq\n\\text{coNP}/\\text{poly}$. For the positive result, we build on the techniques\nof Jansen and Bodlaender [STACS 2011], and for the negative result we use a\npolynomial parameter transformation for $c\\geq 3$ and an OR-cross-composition\nfor $c = 2$. As existing results imply that Dominating Set admits a polynomial\nkernel on degenerate graphs for $c = 1$, our result provides a dichotomy about\nthe existence of polynomial kernels for Dominating Set on degenerate graphs\nwith this parameter.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 17:41:03 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 10:44:14 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Bougeret", "Marin", ""], ["Sau", "Ignasi", ""]]}, {"id": "1609.08253", "submitter": "David J. Rosenbaum", "authors": "Fran\\c{c}ois Le Gall and David J. Rosenbaum", "title": "On the Group and Color Isomorphism Problems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove results on the relationship between the complexity of\nthe group and color isomorphism problems. The difficulty of color isomorphism\nproblems is known to be closely linked to the the composition factors of the\npermutation group involved. Previous works are primarily concerned with\napplying color isomorphism to bou nded degree graph isomorphism, and have\ntherefore focused on the alternating composit ion factors, since those are the\nbottleneck in the case of graph isomorphism.\n  We consider the color isomorphism problem with composition factors restricted\nto those other than the alternating group, show that group isomorphism reduces\nin n^(O(log log n)) time to this problem, and, conversely, that a special case\nof this color isomorphism problem reduces to a slight generalization of group\nisomorphism. We then sharpen our results by identifying the projective special\nlinear group as the main obstacle to faster algorithms for group isomorphism\nand prove that the aforementioned reduc tion from group isomorphism to color\nisomorphism in fact produces only cyclic and projective special linear factors.\nOur results demonstrate that, just as the alternatin g group was a barrier to\nfaster algorithms for graph isomorphism for three decades, the projective\nspecial linear group is an obstacle to faster algorithms for group isomorphism.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 04:10:44 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Rosenbaum", "David J.", ""]]}, {"id": "1609.08403", "submitter": "Jacob Evald", "authors": "S{\\o}ren Dahlgaard, Jacob Evald", "title": "Tight Hardness Results for Distance and Centrality Problems in Constant\n  Degree Graphs", "comments": "14 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding important nodes in a graph and measuring their importance is a\nfundamental problem in the analysis of social networks, transportation\nnetworks, biological systems, etc. Among popular such metrics are graph\ncentrality, betweenness centrality (BC), and reach centrality (RC). These\nmeasures are also very related to classic notions like diameter and radius.\nRoditty and Vassilevska Williams~[STOC'13] showed that no algorithm can compute\na (3/2-\\delta)-approximation of the diameter in sparse and unweighted graphs\nfaster that n^{2-o(1)} time unless the widely believed strong exponential time\nhypothesis (SETH) is false. Abboud et al.~[SODA'15] and [SODA'16] further\nanalyzed these problems under the recent line of research on hardness in P.\nThey showed that in sparse and unweighted graphs (weighted for BC) none of\nthese problems can be solved faster than n^{2-o(1)} unless some popular\nconjecture is false. Furthermore they ruled out a (2-\\delta)-approximation for\nRC, a (3/2-\\delta)-approximation for Radius and a (5/3-\\delta)-approximation\nfor computing all eccentricities of a graph for any \\delta > 0. We extend these\nresults to the case of unweighted graphs with constant maximum degree. Through\nnew graph constructions we are able to obtain the same approximation and time\nbounds as for sparse graphs even in unweighted bounded-degree graphs. We show\nthat no (3/2-\\delta) approximation of Radius or Diameter,\n(2-\\delta)-approximation of RC, (5/3-\\delta)-approximation of all\neccentricities or exact algorithm for BC exists in time n^{2-o(1)} for such\ngraphs and any \\delta > 0. This strengthens the result for BC of Abboud et\nal.~[SODA'16] by showing a hardness result for unweighted graphs, and follows\nin the footsteps of Abboud et al.~[SODA'16] and Abboud and Dahlgaard~[FOCS'16]\nin showing conditional lower bounds for restricted but realistic graph classes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:20:49 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 11:43:23 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Evald", "Jacob", ""]]}, {"id": "1609.08486", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Tsvi Kopelowitz, Seth Pettie, Ruosong Wang, Wei Zhan", "title": "Exponential Separations in the Energy Complexity of Leader Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is often the most constrained resource for battery-powered wireless\ndevices and the lion's share of energy is often spent on transceiver usage\n(sending/receiving packets), not on computation. In this paper we study the\nenergy complexity of LeaderElection and ApproximateCounting in several models\nof wireless radio networks. It turns out that energy complexity is very\nsensitive to whether the devices can generate random bits and their ability to\ndetect collisions. We consider four collision-detection models: Strong-CD (in\nwhich transmitters and listeners detect collisions), Sender-CD and Receiver-CD\n(in which only transmitters or only listeners detect collisions), and No-CD (in\nwhich no one detects collisions.)\n  The take-away message of our results is quite surprising. For randomized\nLeaderElection algorithms, there is an exponential gap between the energy\ncomplexity of Sender-CD and Receiver-CD, and for deterministic LeaderElection\nalgorithms there is another exponential gap, but in the reverse direction.\n  In particular, the randomized energy complexity of LeaderElection is\n$\\Theta(\\log^* n)$ in Sender-CD but $\\Theta(\\log(\\log^* n))$ in Receiver-CD,\nwhere $n$ is the (unknown) number of devices. Its deterministic complexity is\n$\\Theta(\\log N)$ in Receiver-CD but $\\Theta(\\log\\log N)$ in Sender-CD, where\n$N$ is the (known) size of the devices' ID space.\n  There is a tradeoff between time and energy. We give a new upper bound on the\ntime-energy tradeoff curve for randomized LeaderElection and\nApproximateCounting. A critical component of this algorithm is a new\ndeterministic LeaderElection algorithm for dense instances, when $n=\\Theta(N)$,\nwith inverse-Ackermann-type ($O(\\alpha(N))$) energy complexity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 14:58:35 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 14:05:57 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 19:34:48 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Kopelowitz", "Tsvi", ""], ["Pettie", "Seth", ""], ["Wang", "Ruosong", ""], ["Zhan", "Wei", ""]]}, {"id": "1609.08934", "submitter": "Ioannis Avramopoulos", "authors": "Ioannis Avramopoulos", "title": "Multiplicative weights, equalizers, and P=PPAD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, by using multiplicative weights in a game-theoretic thought\nexperiment (and an important convexity result on the composition of\nmultiplicative weights with the relative entropy function), a symmetric\nbimatrix game (that is, a bimatrix matrix wherein the payoff matrix of each\nplayer is the transpose of the payoff matrix of the other) either has an\ninterior symmetric equilibrium or there is a pure strategy that is weakly\ndominated by some mixed strategy. Weakly dominated pure strategies can be\ndetected and eliminated in polynomial time by solving a linear program.\nFurthermore, interior symmetric equilibria are a special case of a more general\nnotion, namely, that of an \"equalizer,\" which can also be computed efficiently\nin polynomial time by solving a linear program. An elegant \"symmetrization\nmethod\" of bimatrix games [Jurg et al., 1992] and the well-known\nPPAD-completeness results on equilibrium computation in bimatrix games\n[Daskalakis et al., 2009, Chen et al., 2009] imply then the compelling P =\nPPAD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 14:43:51 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Avramopoulos", "Ioannis", ""]]}, {"id": "1609.09433", "submitter": "Charis Papadopoulos", "authors": "Athanasios Konstantinidis and Charis Papadopoulos", "title": "Maximizing the Strong Triadic Closure in Split Graphs and Proper\n  Interval Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social networks the {\\sc Strong Triadic Closure} is an assignment of the\nedges with strong or weak labels such that any two vertices that have a common\nneighbor with a strong edge are adjacent. The problem of maximizing the number\nof strong edges that satisfy the strong triadic closure was recently shown to\nbe NP-complete for general graphs. Here we initiate the study of graph classes\nfor which the problem is solvable. We show that the problem admits a\npolynomial-time algorithm for two unrelated classes of graphs: proper interval\ngraphs and trivially-perfect graphs. To complement our result, we show that the\nproblem remains NP-complete on split graphs, and consequently also on chordal\ngraphs. Thus we contribute to define the first border between graph classes on\nwhich the problem is polynomially solvable and on which it remains NP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:25:14 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 11:37:56 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Konstantinidis", "Athanasios", ""], ["Papadopoulos", "Charis", ""]]}, {"id": "1609.09562", "submitter": "Edward Haeusler", "authors": "Lew Gordeev and Edward Hermann Haeusler", "title": "NP vs PSPACE", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof of the conjecture $\\mathcal{NP}$ = $\\mathcal{PSPACE}$ by\nshowing that arbitrary tautologies of Johansson's minimal propositional logic\nadmit \"small\" polynomial-size dag-like natural deductions in Prawitz's system\nfor minimal propositional logic. These \"small\" deductions arise from standard\n\"large\"\\ tree-like inputs by horizontal dag-like compression that is obtained\nby merging distinct nodes labeled with identical formulas occurring in\nhorizontal sections of deductions involved. The underlying \"geometric\" idea: if\nthe height, $h\\left( \\partial \\right) $ , and the total number of distinct\nformulas, $\\phi \\left( \\partial \\right) $ , of a given tree-like deduction\n$\\partial$ of a minimal tautology $\\rho$ are both polynomial in the length of\n$\\rho$, $\\left| \\rho \\right|$, then the size of the horizontal dag-like\ncompression is at most $h\\left( \\partial \\right) \\times \\phi \\left( \\partial\n\\right) $, and hence polynomial in $\\left| \\rho \\right|$. The attached proof is\ndue to the first author, but it was the second author who proposed an initial\nidea to attack a weaker conjecture $\\mathcal{NP}= \\mathcal{\\mathit{co}NP}$ by\nreductions in diverse natural deduction formalisms for propositional logic.\nThat idea included interactive use of minimal, intuitionistic and classical\nformalisms, so its practical implementation was too involved. The attached\nproof of $ \\mathcal{NP}=\\mathcal{PSPACE}$ runs inside the natural deduction\ninterpretation of Hudelmaier's cutfree sequent calculus for minimal logic.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 01:20:56 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Gordeev", "Lew", ""], ["Haeusler", "Edward Hermann", ""]]}, {"id": "1609.09643", "submitter": "Mateus de Oliveira Oliveira", "authors": "Mateus de Oliveira Oliveira", "title": "A Near-Quadratic Lower Bound for the Size of Quantum Circuits of\n  Constant Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any quantum circuit of treewidth $t$, built from $r$-qubit\ngates, requires at least $\\Omega(\\frac{n^{2}}{2^{O(r\\cdot t)}\\cdot \\log^4 n})$\ngates to compute the element distinctness function. Our result generalizes a\nnear-quadratic lower bound for quantum formula size obtained by Roychowdhury\nand Vatan [SIAM J. on Computing, 2001]. The proof of our lower bound follows by\nan extension of Ne\\v{c}iporuk's method to the context of quantum circuits of\nconstant treewidth. This extension is made via a combination of techniques from\nstructural graph theory, tensor-network theory, and the connected-component\ncounting method, which is a classic tool in algebraic geometry.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 09:13:12 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Oliveira", "Mateus de Oliveira", ""]]}]