[{"id": "1910.00305", "submitter": "Fabian Frei", "authors": "Fabian Frei and Edith Hemaspaandra and J\\\"org Rothe", "title": "Complexity of Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph parameters such as the clique number, the chromatic number, and the\nindependence number are central in many areas, ranging from computer networks\nto linguistics to computational neuroscience to social networks. In particular,\nthe chromatic number of a graph (i.e., the smallest number of colors needed to\ncolor all vertices such that no two adjacent vertices are of the same color)\ncan be applied in solving practical tasks as diverse as pattern matching,\nscheduling jobs to machines, allocating registers in compiler optimization, and\neven solving Sudoku puzzles. Typically, however, the underlying graphs are\nsubject to (often minor) changes. To make these applications of graph\nparameters robust, it is important to know which graphs are stable for them in\nthe sense that adding or deleting single edges or vertices does not change\nthem. We initiate the study of stability of graphs for such parameters in terms\nof their computational complexity. We show that, for various central graph\nparameters, the problem of determining whether or not a given graph is stable\nis complete for \\Theta_2^p, a well-known complexity class in the second level\nof the polynomial hierarchy, which is also known as \"parallel access to NP.\"\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:14:59 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 01:44:07 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 19:14:03 GMT"}, {"version": "v4", "created": "Sat, 12 Dec 2020 18:15:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Frei", "Fabian", ""], ["Hemaspaandra", "Edith", ""], ["Rothe", "J\u00f6rg", ""]]}, {"id": "1910.00440", "submitter": "Tim Hartmann", "authors": "Jan Dreier, Janosch Fuchs, Tim A. Hartmann, Philipp Kuinke, Peter\n  Rossmanith, Bjoern Tauer, Hung-Lung Wang", "title": "The Complexity of Packing Edge-Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the complexity of Path Packing. Given a graph $G$ and\na list of paths, the task is to embed the paths edge-disjoint in $G$. This\ngeneralizes the well known Hamiltonian-Path problem.\n  Since Hamiltonian Path is efficiently solvable for graphs of small treewidth,\nwe study how this result translates to the much more general Path Packing. On\nthe positive side, we give an FPT-algorithm on trees for the number of paths as\nparameter. Further, we give an XP-algorithm with the combined parameters\nmaximal degree, number of connected components and number of nodes of degree at\nleast three. Surprisingly the latter is an almost tight result by runtime and\nparameterization. We show an ETH lower bound almost matching our runtime.\nMoreover, if two of the three values are constant and one is unbounded the\nproblem becomes NP-hard.\n  Further, we study restrictions to the given list of paths. On the positive\nside, we present an FPT-algorithm parameterized by the sum of the lengths of\nthe paths. Packing paths of length two is polynomial time solvable, while\npacking paths of length three is NP-hard. Finally, even the spacial case EPC\nwhere the paths have to cover every edge in $G$ exactly once is already NP-hard\nfor two paths on 4-regular graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:26:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dreier", "Jan", ""], ["Fuchs", "Janosch", ""], ["Hartmann", "Tim A.", ""], ["Kuinke", "Philipp", ""], ["Rossmanith", "Peter", ""], ["Tauer", "Bjoern", ""], ["Wang", "Hung-Lung", ""]]}, {"id": "1910.00510", "submitter": "Lou Sala\\\"un", "authors": "Lou Sala\\\"un, Marceau Coupechoux, Chung Shue Chen", "title": "Joint Subcarrier and Power Allocation in NOMA: Optimal and Approximate\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2982786", "report-no": null, "categories": "math.OC cs.CC cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal multiple access (NOMA) is a promising technology to increase\nthe spectral efficiency and enable massive connectivity in 5G and future\nwireless networks. In contrast to orthogonal schemes, such as OFDMA, NOMA\nmultiplexes several users on the same frequency and time resource. Joint\nsubcarrier and power allocation problems (JSPA) in NOMA are NP-hard to solve in\ngeneral. In this family of problems, we consider the weighted sum-rate (WSR)\nobjective function as it can achieve various tradeoffs between sum-rate\nperformance and user fairness. Because of JSPA's intractability, a common\napproach in the literature is to solve separately the power control and\nsubcarrier allocation (also known as user selection) problems, therefore\nachieving sub-optimal result. In this work, we first improve the computational\ncomplexity of existing single-carrier power control and user selection schemes.\nThese improved procedures are then used as basic building blocks to design new\nalgorithms, namely Opt-JSPA, $\\varepsilon$-JSPA and Grad-JSPA. Opt-JSPA\ncomputes an optimal solution with lower complexity than current optimal schemes\nin the literature. It can be used as a benchmark for optimal WSR performance in\nsimulations. However, its pseudo-polynomial time complexity remains impractical\nfor real-world systems with low latency requirements. To further reduce the\ncomplexity, we propose a fully polynomial-time approximation scheme called\n$\\varepsilon$-JSPA. Since, no approximation has been studied in the literature,\n$\\varepsilon$-JSPA stands out by allowing to control a tight trade-off between\nperformance guarantee and complexity. Finally, Grad-JSPA is a heuristic based\non gradient descent. Numerical results show that it achieves near-optimal WSR\nwith much lower complexity than existing optimal methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:59:38 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:22:52 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 09:43:22 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Sala\u00fcn", "Lou", ""], ["Coupechoux", "Marceau", ""], ["Chen", "Chung Shue", ""]]}, {"id": "1910.00581", "submitter": "Sebastian Siebertz", "authors": "Daniel Lokshtanov and Amer E. Mouawad and Fahad Panolan and Sebastian\n  Siebertz", "title": "On the Parameterized Complexity of Reconfiguration of Connected\n  Dominating Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a reconfiguration version of an optimization problem $\\mathcal{Q}$ the\ninput is an instance of $\\mathcal{Q}$ and two feasible solutions $S$ and $T$.\nThe objective is to determine whether there exists a step-by-step\ntransformation between $S$ and $T$ such that all intermediate steps also\nconstitute feasible solutions. In this work, we study the parameterized\ncomplexity of the \\textsc{Connected Dominating Set Reconfiguration} problem\n(\\textsc{CDS-R)}. It was shown in previous work that the \\textsc{Dominating Set\nReconfiguration} problem (\\textsc{DS-R}) parameterized by $k$, the maximum\nallowed size of a dominating set in a reconfiguration sequence, is\nfixed-parameter tractable on all graphs that exclude a biclique $K_{d,d}$ as a\nsubgraph, for some constant $d \\geq 1$. We show that the additional\nconnectivity constraint makes the problem much harder, namely, that\n\\textsc{CDS-R} is \\textsf{W}$[1]$-hard parameterized by $k+\\ell$, the maximum\nallowed size of a dominating set plus the length of the reconfiguration\nsequence, already on $5$-degenerate graphs. On the positive side, we show that\n\\textsc{CDS-R} parameterized by $k$ is fixed-parameter tractable, and in fact\nadmits a polynomial kernel on planar graphs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:38:50 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Mouawad", "Amer E.", ""], ["Panolan", "Fahad", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1910.00724", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Saurav Prakash, Haleh Akrami, Peter A. Beerel, Keith M.\n  Chugg", "title": "A Pre-defined Sparse Kernel Based Convolution for Deep CNNs", "comments": "8 pages, 12 figures, Computer vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high demand for computational and storage resources severely impede the\ndeployment of deep convolutional neural networks (CNNs) in limited-resource\ndevices. Recent CNN architectures have proposed reduced complexity versions\n(e.g. SuffleNet and MobileNet) but at the cost of modest decreases inaccuracy.\nThis paper proposes pSConv, a pre-defined sparse 2D kernel-based convolution,\nwhich promises significant improvements in the trade-off between complexity and\naccuracy for both CNN training and inference. To explore the potential of this\napproach, we have experimented with two widely accepted datasets, CIFAR-10 and\nTiny ImageNet, in sparse variants of both the ResNet18 and VGG16 architectures.\nOur approach shows a parameter count reduction of up to 4.24x with modest\ndegradation in classification accuracy relative to that of standard CNNs. Our\napproach outperforms a popular variant of ShuffleNet using a variant of\nResNet18 with pSConv having 3x3 kernels with only four of nine elements not\nfixed at zero. In particular, the parameter count is reduced by 1.7x for\nCIFAR-10 and 2.29x for Tiny ImageNet with an increased accuracy of ~4%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:38:38 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:31:05 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kundu", "Souvik", ""], ["Prakash", "Saurav", ""], ["Akrami", "Haleh", ""], ["Beerel", "Peter A.", ""], ["Chugg", "Keith M.", ""]]}, {"id": "1910.00777", "submitter": "Michael Fiske S", "authors": "Michael Stephen Fiske", "title": "Prime Clocks", "comments": "13 pages", "journal-ref": "Proceedings of the 10th GI Conference. Autonomous Systems 2017.\n  Reihe 10. Nr. 857. Fortschritt-Berichte VDI, 2017. ISBN 978-3-18-385710-4", "doi": null, "report-no": null, "categories": "cs.CC math.GR math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical implementations of digital computers began in the latter half of the\n1930's and were first constructed from various forms of logic gates. Based on\nthe prime numbers, we introduce prime clocks and prime clock sums, where the\nclocks utilize time and act as computational primitives instead of gates. The\nprime clocks generate an infinite abelian group, where for each n, there is a\nfinite subgroup S such that for each Boolean function f : {0, 1}^n --> {0, 1},\nthere exists a finite prime clock sum in S that can represent and compute f. A\nparallelizable algorithm, implemented with a finite prime clock sum, is\nprovided that computes f. In contrast, the negation, conjunction, and\ndisjunction operations generate a Boolean algebra. In terms of computation,\nBoolean circuits computed with logic gates NOT, AND, OR have a depth. This\nmeans that a completely parallel computation of Boolean functions is not\npossible with these gates. Overall, some new connections between number theory,\nBoolean functions and computation are established.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 04:52:19 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Fiske", "Michael Stephen", ""]]}, {"id": "1910.00901", "submitter": "Elazar Goldenberg", "authors": "Elazar Goldenberg, Robert Krauthgamer and Barna Saha", "title": "Sublinear Algorithms for Gap Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance is a way of quantifying how similar two strings are to one\nanother by counting the minimum number of character insertions, deletions, and\nsubstitutions required to transform one string into the other. A simple dynamic\nprogramming computes the edit distance between two strings of length $n$ in\n$O(n^2)$ time, and a more sophisticated algorithm runs in time $O(n+t^2)$ when\nthe edit distance is $t$ [Landau, Myers and Schmidt, SICOMP 1998]. In pursuit\nof obtaining faster running time, the last couple of decades have seen a flurry\nof research on approximating edit distance, including polylogarithmic\napproximation in near-linear time [Andoni, Krauthgamer and Onak, FOCS 2010],\nand a constant-factor approximation in subquadratic time [Chakrabarty, Das,\nGoldenberg, Kouck\\'y and Saks, FOCS 2018].\n  We study sublinear-time algorithms for small edit distance, which was\ninvestigated extensively because of its numerous applications. Our main result\nis an algorithm for distinguishing whether the edit distance is at most $t$ or\nat least $t^2$ (the quadratic gap problem) in time\n$\\tilde{O}(\\frac{n}{t}+t^3)$. This time bound is sublinear roughly for all $t$\nin $[\\omega(1), o(n^{1/3})]$, which was not known before. The best previous\nalgorithms solve this problem in sublinear time only for $t=\\omega(n^{1/3})$\n[Andoni and Onak, STOC 2009].\n  Our algorithm is based on a new approach that adaptively switches between\nuniform sampling and reading contiguous blocks of the input strings. In\ncontrast, all previous algorithms choose which coordinates to query\nnon-adaptively. Moreover, it can be extended to solve the $t$ vs\n$t^{2-\\epsilon}$ gap problem in time $\\tilde{O}(\\frac{n}{t^{1-\\epsilon}}+t^3)$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 12:18:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Goldenberg", "Elazar", ""], ["Krauthgamer", "Robert", ""], ["Saha", "Barna", ""]]}, {"id": "1910.00941", "submitter": "Madhu Sudan", "authors": "Madhu Sudan and David Xiang", "title": "A Self-contained Analysis of the Lempel-Ziv Compression Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a self-contained analysis of the performance of the\nLempel-Ziv compression algorithm on (hidden) Markovian sources. Specifically we\ninclude a full proof of the assertion that the compression rate approaches the\nentropy rate of the chain being compressed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:29:44 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Sudan", "Madhu", ""], ["Xiang", "David", ""]]}, {"id": "1910.00994", "submitter": "Dhiraj Holden", "authors": "Michel Goemans, Shafi Goldwasser, Dhiraj Holden", "title": "Doubly-Efficient Pseudo-Deterministic Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [20] Goldwasser, Grossman and Holden introduced pseudo-deterministic\ninteractive proofs for search problems where a powerful prover can convince a\nprobabilistic polynomial time verifier that a solution to a search problem is\ncanonical. They studied search problems for which polynomial time algorithms\nare not known and for which many solutions are possible. They showed that\nwhereas there exists a constant round pseudo deterministic proof for graph\nisomorphism where the canonical solution is the lexicographically smallest\nisomorphism, the existence of pseudo-deterministic interactive proofs for\nNP-hard problems would imply the collapse of the polynomial time hierarchy.\n  In this paper, we turn our attention to studying doubly-efficient\npseudo-deterministic proofs for polynomial time search problems:\npseudo-deterministic proofs with the extra requirement that the prover runtime\nis polynomial and the verifier runtime to verify that a solution is canonical\nis significantly lower than the complexity of finding any solution, canonical\nor otherwise. Naturally this question is particularly interesting for search\nproblems for which a lower bound on its worst case complexity is known or has\nbeen widely conjectured.\n  We show doubly-efficient pseudo-deterministic algorithms for a host of\nnatural problems whose complexity has long been conjectured. In particular, we\nshow a doubly efficient pseudo-deterministic NP proof for linear programming,\n3-SUM and problems reducible to 3-SUM, the hitting set problem, and the Zero\nWeight Triangle problem and show a doubly-efficient pseudo-deterministic MA\nproof for the Orthogonal Vectors problem and the $k$-Clique problem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:53:33 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 21:03:20 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Goemans", "Michel", ""], ["Goldwasser", "Shafi", ""], ["Holden", "Dhiraj", ""]]}, {"id": "1910.01047", "submitter": "Markus Hecher", "authors": "Johannes Klaus Fichte, Markus Hecher, Andreas Pfandler", "title": "Lower Bounds for QBFs of Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deciding the validity (QSAT) of quantified Boolean formulas\n(QBF) is a vivid research area in both theory and practice. In the field of\nparameterized algorithmics, the well-studied graph measure treewidth turned out\nto be a successful parameter. A well-known result by Chen in parameterized\ncomplexity is that QSAT when parameterized by the treewidth of the primal graph\nof the input formula together with the quantifier depth of the formula is\nfixed-parameter tractable. More precisely, the runtime of such an algorithm is\npolynomial in the formula size and exponential in the treewidth, where the\nexponential function in the treewidth is a tower, whose height is the\nquantifier depth. A natural question is whether one can significantly improve\nthese results and decrease the tower while assuming the Exponential Time\nHypothesis (ETH). In the last years, there has been a growing interest in the\nquest of establishing lower bounds under ETH, showing mostly problem-specific\nlower bounds up to the third level of the polynomial hierarchy. Still, an\nimportant question is to settle this as general as possible and to cover the\nwhole polynomial hierarchy. In this work, we show lower bounds based on the ETH\nfor arbitrary QBFs parameterized by treewidth (and quantifier depth). More\nformally, we establish lower bounds for QSAT and treewidth, namely, that under\nETH there cannot be an algorithm that solves QSAT of quantifier depth i in\nruntime significantly better than i-fold exponential in the treewidth and\npolynomial in the input size. In doing so, we provide a versatile reduction\ntechnique to compress treewidth that encodes the essence of dynamic programming\non arbitrary tree decompositions. Further, we describe a general methodology\nfor a more fine-grained analysis of problems parameterized by treewidth that\nare at higher levels of the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:09:22 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:05:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fichte", "Johannes Klaus", ""], ["Hecher", "Markus", ""], ["Pfandler", "Andreas", ""]]}, {"id": "1910.01082", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Jana Novotn\\'a, Karolina Okrasa, Micha{\\l} Pilipczuk, Pawe{\\l}\n  Rz\\k{a}\\.zewski, Erik Jan van Leeuwen, Bartosz Walczak", "title": "Subexponential-time algorithms for finding large induced sparse\n  subgraphs", "comments": "Appeared on IPEC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{C}$ and $\\mathcal{D}$ be hereditary graph classes. Consider the\nfollowing problem: given a graph $G\\in\\mathcal{D}$, find a largest, in terms of\nthe number of vertices, induced subgraph of $G$ that belongs to $\\mathcal{C}$.\nWe prove that it can be solved in $2^{o(n)}$ time, where $n$ is the number of\nvertices of $G$, if the following conditions are satisfied:\n  * the graphs in $\\mathcal{C}$ are sparse, i.e., they have linearly many edges\nin terms of the number of vertices;\n  * the graphs in $\\mathcal{D}$ admit balanced separators of size governed by\ntheir density, e.g., $\\mathcal{O}(\\Delta)$ or $\\mathcal{O}(\\sqrt{m})$, where\n$\\Delta$ and $m$ denote the maximum degree and the number of edges,\nrespectively; and\n  * the considered problem admits a single-exponential fixed-parameter\nalgorithm when parameterized by the treewidth of the input graph.\n  This leads, for example, to the following corollaries for specific classes\n$\\mathcal{C}$ and $\\mathcal{D}$:\n  * a largest induced forest in a $P_t$-free graph can be found in\n$2^{\\tilde{\\mathcal{O}}(n^{2/3})}$ time, for every fixed $t$; and\n  * a largest induced planar graph in a string graph can be found in\n$2^{\\tilde{\\mathcal{O}}(n^{3/4})}$ time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:56:36 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Novotn\u00e1", "Jana", ""], ["Okrasa", "Karolina", ""], ["Pilipczuk", "Micha\u0142", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["van Leeuwen", "Erik Jan", ""], ["Walczak", "Bartosz", ""]]}, {"id": "1910.01251", "submitter": "Rafael Oliveira", "authors": "Ankit Garg, Christian Ikenmeyer, Visu Makam, Rafael Oliveira, Michael\n  Walter, Avi Wigderson", "title": "Search problems in algebraic complexity, GCT, and hardness of generator\n  for invariant rings", "comments": "17 pages", "journal-ref": "35th Computational Complexity Conference (CCC 2020), Leibniz\n  International Proceedings in Informatics, vol. 169, 2020, pp. 12:1 - 12:17", "doi": "10.4230/LIPIcs.CCC.2020.12", "report-no": null, "categories": "cs.CC math.RA math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing succinct encodings of lists of\ngenerators for invariant rings for group actions. Mulmuley conjectured that\nthere are always polynomial sized such encodings for invariant rings of\n$\\SL_n(\\C)$-representations. We provide simple examples that disprove this\nconjecture (under standard complexity assumptions).\n  We develop a general framework, denoted \\emph{algebraic circuit search\nproblems}, that captures many important problems in algebraic complexity and\ncomputational invariant theory. This framework encompasses various proof\nsystems in proof complexity and some of the central problems in invariant\ntheory as exposed by the Geometric Complexity Theory (GCT) program, including\nthe aforementioned problem of computing succinct encodings for generators for\ninvariant rings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:28:03 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 13:00:50 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Garg", "Ankit", ""], ["Ikenmeyer", "Christian", ""], ["Makam", "Visu", ""], ["Oliveira", "Rafael", ""], ["Walter", "Michael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1910.01293", "submitter": "Frank Stephan", "authors": "Gordon Hoi, Sanjay Jain and Frank Stephan", "title": "A Fast Exponential Time Algorithm for Max Hamming Distance X3SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X3SAT is the problem of whether one can satisfy a given set of clauses with\nup to three literals such that in every clause, exactly one literal is true and\nthe others are false. A related question is to determine the maximal Hamming\ndistance between two solutions of the instance. Dahll\\\"of provided an algorithm\nfor Maximum Hamming Distance XSAT, which is more complicated than the same\nproblem for X3SAT, with a runtime of $O(1.8348^n)$; Fu, Zhou and Yin considered\nMaximum Hamming Distance for X3SAT and found for this problem an algorithm with\nruntime $O(1.6760^n)$. In this paper, we propose an algorithm in $O(1.3298^n)$\ntime to solve the Max Hamming Distance X3SAT problem; the algorithm actually\ncounts for each $k$ the number of pairs of solutions which have Hamming\nDistance $k$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 03:45:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Hoi", "Gordon", ""], ["Jain", "Sanjay", ""], ["Stephan", "Frank", ""]]}, {"id": "1910.01331", "submitter": "Lou Salaun", "authors": "Lou Salaun (LINCS, LTCI), Chung Shue Chen (LINCS), Marceau Coupechoux\n  (LTCI)", "title": "Optimal Joint Subcarrier and Power Allocation in NOMA is Strongly\n  NP-Hard", "comments": null, "journal-ref": "IEEE International Conference on Communications (ICC), May 2018,\n  Kansas City, United States. pp.1-7", "doi": "10.1109/ICC.2018.8422362", "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal multiple access (NOMA) is a promising radio access technology\nfor 5G. It allows several users to transmit on the same frequency and time\nresource by performing power-domain multiplexing. At the receiver side,\nsuccessive interference cancellation (SIC) is applied to mitigate interference\namong the multiplexed signals. In this way, NOMA can outperform orthogonal\nmultiple access schemes used in conventional cellular networks in terms of\nspectral efficiency and allows more simultaneous users. This paper investigates\nthe computational complexity of joint subcarrier and power allocation problems\nin multi-carrier NOMA systems. We prove that these problems are strongly\nNP-hard for a large class of objective functions, namely the weighted\ngeneralized means of the individual data rates. This class covers the popular\nweighted sum-rate, proportional fairness, harmonic mean and max-min fairness\nutilities. Our results show that the optimal power and subcarrier allocation\ncannot be computed in polynomial time in the general case, unless P = NP.\nNevertheless, we present some tractable special cases and we show that they can\nbe solved efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:35:29 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Salaun", "Lou", "", "LINCS, LTCI"], ["Chen", "Chung Shue", "", "LINCS"], ["Coupechoux", "Marceau", "", "LTCI"]]}, {"id": "1910.01357", "submitter": "Xiangyu Gao", "authors": "Xiangyu Gao, Jianzhong Li, Dongjing Miao and Xianmin Liu", "title": "Recognizing the Tractability in Big Data Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limitation on computational power of existing computers, the\npolynomial time does not works for identifying the tractable problems in big\ndata computing. This paper adopts the sublinear time as the new tractable\nstandard to recognize the tractability in big data computing, and the\nrandom-access Turing machine is used as the computational model to characterize\nthe problems that are tractable on big data. First, two pure-tractable classes\nare first proposed. One is the class $\\mathrm{PL}$ consisting of the problems\nthat can be solved in polylogarithmic time by a RATM. The another one is the\nclass $\\mathrm{ST}$ including all the problems that can be solved in sublinear\ntime by a RATM. The structure of the two pure-tractable classes is deeply\ninvestigated and they are proved $\\mathrm{PL^i} \\subsetneq \\mathrm{PL^{i+1}}$\nand $\\mathrm{PL} \\subsetneq \\mathrm{ST}$. Then, two pseudo-tractable classes,\n$\\mathrm{PTR}$ and $\\mathrm{PTE}$, are proposed. $\\mathrm{PTR}$ consists of all\nthe problems that can solved by a RATM in sublinear time after a PTIME\npreprocessing by reducing the size of input dataset. $\\mathrm{PTE}$ includes\nall the problems that can solved by a RATM in sublinear time after a PTIME\npreprocessing by extending the size of input dataset. The relations among the\ntwo pseudo-tractable classes and other complexity classes are investigated and\nthey are proved that $\\mathrm{PT} \\subseteq \\mathrm{P}$, $\\sqcap'\\mathrm{T^0_Q}\n\\subsetneq \\mathrm{PTR^0_Q}$ and $\\mathrm{PT_P} = \\mathrm{P}$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:54:30 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 05:39:39 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Gao", "Xiangyu", ""], ["Li", "Jianzhong", ""], ["Miao", "Dongjing", ""], ["Liu", "Xianmin", ""]]}, {"id": "1910.01565", "submitter": "Bhaskar DasGupta", "authors": "Tanima Chatterjee and Bhaskar DasGupta", "title": "On partisan bias in redistricting: computational complexity meets the\n  science of gerrymandering", "comments": "Disclaimer: The authors were not supported, financially or otherwise,\n  by any political party. The research results reported in this paper are\n  purely scientific and reported as they are without any regard to which\n  political party they may be of help (if at all)", "journal-ref": "Journal of Combinatorial Optimization, 40(2), 512-546, 2020", "doi": "10.1007/s10878-020-00589-x", "report-no": null, "categories": "cs.CC cs.CG cs.DM math.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of this paper is \"gerrymandering\", namely the curse of deliberate\ncreations of district maps with highly asymmetric electoral outcomes to\ndisenfranchise voters, and it has a long legal history. Measuring and\neliminating gerrymandering has enormous implications to sustain the backbone of\ndemocratic principles of a society. Although there is no dearth of legal briefs\ninvolving gerrymandering over many years, it is only more recently that\nmathematicians and applied computational researchers have started to\ninvestigate this topic. However, it has received relatively little attention so\nfar from the computational complexity researchers dealing with theoretical\nanalysis of computational complexity issues, such as computational hardness,\napproximability issues, etc. There could be many reasons for this, such as\ndescriptions of these problem non-CS non-math (often legal or political)\njournals that theoretical CS (TCS) people usually do not follow, or the lack of\ncoverage of these topics in TCS publication venues. One of our modest goals in\nwriting this article is to improve upon this situation by stimulating further\ninteractions between the gerrymandering and TCS researchers. To this effect,\nour main contributions are twofold: (1) we provide formalization of several\nmodels, related concepts, and corresponding problem statements using TCS\nframeworks from the descriptions of these problems as available in existing\nnon-TCS (perhaps legal) venues, and (2) we also provide computational\ncomplexity analysis of some versions of these problems, leaving other versions\nfor future research.\n  The goal of writing this article is not to have the final word on\ngerrymandering, but to introduce a series of concepts, models and problems to\nthe TCS community and to show that science of gerrymandering involves an\nintriguing set of partitioning problems involving geometric and combinatorial\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:11:18 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chatterjee", "Tanima", ""], ["DasGupta", "Bhaskar", ""]]}, {"id": "1910.01935", "submitter": "Petra Wolf", "authors": "Petra Wolf", "title": "Synchronization under Dynamic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine an assembly line where a box with a lid and liquid in it enters in\nsome unknown orientation. The box should leave the line with the open lid\nfacing upwards with the liquid still in it. To save costs there are no complex\nsensors or image recognition software available on the assembly line, so a\nreset sequence needs to be computed. But how can the dependencies of the\ndeforming impact of a transformation of the box, such as 'do not tilt the box\nover when the lid is open' or 'open the lid again each time it gets closed' be\nmodeled? We present three attempts to model constraints of these kinds on the\norder in which the states of an automaton are transitioned by a synchronizing\nword. The first two concepts relate the last visits of states and form\nconstraints on which states still need to be reached, whereas the third concept\nconcerns the first visits of states and forms constraints on which states might\nstill be reached. We examine the computational complexity of different variants\nof the problem, whether an automaton can be synchronized with a word that\nrespects the constraints defined in the respective concept, and obtain nearly a\nfull classification. While most of the problems are PSPACE-complete we also\nobserve NP-complete variants and variants solvable in polynomial time. We will\nalso observe a drop of the complexity if we track the orders of states on\nseveral paths simultaneously instead of tracking the set of active states.\nFurther, we give upper bounds on the length of a synchronizing word depending\non the size of the input relation and show that the Cerny conjecture holds for\npartial weakly acyclic automata.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 13:07:54 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:10:39 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 20:08:52 GMT"}, {"version": "v4", "created": "Sun, 3 May 2020 17:16:01 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 13:20:32 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wolf", "Petra", ""]]}, {"id": "1910.02048", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, \\.Ismail \\.Ilkan Ceylan", "title": "The Dichotomy of Evaluating Homomorphism-Closed Queries on Probabilistic\n  Graphs", "comments": "30 pages. Journal version of the ICDT'20 paper\n  https://drops.dagstuhl.de/opus/volltexte/2020/11939/. Submitted to LMCS. The\n  previous version (version 2) was the same as the ICDT'20 paper with some\n  minor formatting tweaks and 7 extra pages of technical appendix", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2020.5", "report-no": null, "categories": "cs.DB cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of probabilistic query evaluation on probabilistic\ngraphs, namely, tuple-independent probabilistic databases on signatures of\narity two. Our focus is the class of queries that is closed under\nhomomorphisms, or equivalently, the infinite unions of conjunctive queries. Our\nmain result states that all unbounded queries from this class are #P-hard for\nprobabilistic query evaluation. As bounded queries from this class are\nequivalent to a union of conjunctive queries, they are already classified by\nthe dichotomy of Dalvi and Suciu (2012). Hence, our result and theirs imply a\ncomplete data complexity dichotomy, between polynomial time and #P-hardness,\nfor evaluating infinite unions of conjunctive queries over probabilistic\ngraphs. This dichotomy covers in particular all fragments of infinite unions of\nconjunctive queries such as negation-free (disjunctive) Datalog, regular path\nqueries, and a large class of ontology-mediated queries on arity-two\nsignatures. Our result is shown by reducing from counting the valuations of\npositive partitioned 2-DNF formulae for some queries, or from the\nsource-to-target reliability problem in an undirected graph for other queries,\ndepending on properties of minimal models. The presented dichotomy result\napplies to even a special case of probabilistic query evaluation called\ngeneralized model counting, where fact probabilities must be 0, 0.5, or 1.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:19:35 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:37:45 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 10:32:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Amarilli", "Antoine", ""], ["Ceylan", "\u0130smail \u0130lkan", ""]]}, {"id": "1910.02145", "submitter": "Alexis Ghyselen", "authors": "Patrick Baillot and Alexis Ghyselen", "title": "Types for Parallel Complexity in the Pi-calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type systems as a way to control or analyze programs have been largely\nstudied in the context of functional programming languages. Some of those work\nallow to extract from a typing derivation for a program a complexity bound on\nthis program. We present how to adapt this result for parallel complexity in\nthe pi-calculus, as a model of concurrency and parallel communication. We study\ntwo notions of time complexity: the total computation time without parallelism\n(the work) and the computation time under maximal parallelism (the span). We\ndefine reduction relations in the pi-calculus to capture those two notions, and\nwe present two type systems from which one can extract a complexity bound on a\nprocess. The type systems are inspired by input/output types and size types,\nwith temporal information about communications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:43:43 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Baillot", "Patrick", ""], ["Ghyselen", "Alexis", ""]]}, {"id": "1910.02351", "submitter": "Aditya Kumar", "authors": "Evandro Menezes, Sebastian Pop, Aditya Kumar", "title": "Clustering case statements for indirect branch predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an O(nlogn) algorithm to compile a switch statement into jump\ntables. To generate jump tables that can be efficiently predicted by current\nhardware branch predictors, we added an upper bound on the number of entries\nfor each table. This modification of the previously best known algorithm\nreduces the complexity from O(n^2) to O(nlogn).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 01:10:57 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 16:05:42 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 14:22:44 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Menezes", "Evandro", ""], ["Pop", "Sebastian", ""], ["Kumar", "Aditya", ""]]}, {"id": "1910.02465", "submitter": "S Venkitesh", "authors": "Srikanth Srinivasan, Utkarsh Tripathi, S. Venkitesh", "title": "On the Probabilistic Degrees of Symmetric Boolean functions", "comments": "A preliminary version of this paper will appear in the conference\n  FSTTCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic degree of a Boolean function $f:\\{0,1\\}^n\\rightarrow\n\\{0,1\\}$ is defined to be the smallest $d$ such that there is a random\npolynomial $\\mathbf{P}$ of degree at most $d$ that agrees with $f$ at each\npoint with high probability. Introduced by Razborov (1987), upper and lower\nbounds on probabilistic degrees of Boolean functions --- specifically symmetric\nBoolean functions --- have been used to prove explicit lower bounds, design\npseudorandom generators, and devise algorithms for combinatorial problems.\n  In this paper, we characterize the probabilistic degrees of all symmetric\nBoolean functions up to polylogarithmic factors over all fields of fixed\ncharacteristic (positive or zero).\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 15:43:27 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Srinivasan", "Srikanth", ""], ["Tripathi", "Utkarsh", ""], ["Venkitesh", "S.", ""]]}, {"id": "1910.02590", "submitter": "Dhiraj Holden", "authors": "Dhiraj Holden, Yael Kalai", "title": "Non-Signaling Proofs with $O(\\sqrt{\\log n})$ Provers are in PSPACE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-signaling proofs, motivated by quantum computation, have found\napplications in cryptography and hardness of approximation. An important open\nproblem is characterizing the power of no-signaling proofs. It is known that\n2-prover no-signaling proofs are characterized by PSPACE, and that no-signaling\nproofs with $poly(n)$-provers are characterized by EXP. However, the power of\n$k$-prover no-signaling proofs, for $2<k<poly(n)$ remained an open problem.\n  We show that $k$-prover no-signaling proofs (with negligible soundness) for\n$k=O(\\sqrt{\\log n})$ are contained in PSPACE. We prove this via two different\nroutes that are of independent interest. In both routes we consider a\nrelaxation of no-signaling called sub-no-signaling. Our main technical\ncontribution (which is used in both our proofs) is a reduction showing how to\nconvert any sub-no-signaling strategy with value at least $1-2^{-\\Omega(k^2)}$\ninto a no-signaling one with value at least $2^{-O(k^2)}$.\n  In the first route, we show that the classical prover reduction method for\nconverting $k$-prover games into $2$-prover games carries over to the\nno-signaling setting with the following loss in soundness: if a $k$-player game\nhas value less than $2^{-ck^2}$ (for some constant~$c>0$), then the\ncorresponding 2-prover game has value at most $1 - 2^{dk^2}$ (for some\nconstant~$d>0$). In the second route we show that the value of a\nsub-no-signaling game can be approximated in space that is polynomial in the\ncommunication complexity and exponential in the number of provers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:20:41 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 12:26:33 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 14:27:49 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Holden", "Dhiraj", ""], ["Kalai", "Yael", ""]]}, {"id": "1910.02849", "submitter": "Iggy van Hoof", "authors": "Iggy van Hoof", "title": "Space-efficient quantum multiplication of polynomials for binary finite\n  fields with sub-quadratic Toffoli gate count", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is an essential step in a lot of calculations. In this paper\nwe look at multiplication of 2 binary polynomials of degree at most $n-1$,\nmodulo an irreducible polynomial of degree $n$ with $2n$ input and $n$ output\nqubits, without ancillary qubits, assuming no errors. With straightforward\nschoolbook methods this would result in a quadratic number of Toffoli gates and\na linear number of CNOT gates. This paper introduces a new algorithm that uses\nthe same space, but by utilizing space-efficient variants of Karatsuba\nmultiplication methods it requires only $O(n^{\\log_2(3)})$ Toffoli gates at the\ncost of a higher CNOT gate count: theoretically up to $O(n^2)$ but in examples\nthe CNOT gate count looks a lot better.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:18:38 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 22:53:56 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["van Hoof", "Iggy", ""]]}, {"id": "1910.04162", "submitter": "Yizhen Chen", "authors": "Yizhen Chen", "title": "Mobile Sensor Networks: Bounds on Capacity and Complexity of\n  Realizability", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the mathematical theory of a model, constructed by C. Gu, I.\nDownes, O. Gnawali, and L. Guibas, of networks that diffuse continuously\nacquired information from mobile sensor nodes. We prove new results on the\nmaximum, minimum, and expected capacity of their model of combinatorial and\ngeometric mobile sensor networks, as well as modified versions of these models.\nWe also give complexity results for the problem of deciding when a\ncombinatorial mobile sensor network is generated from a geometric mobile sensor\nnetwork.\n  A more detailed description of the concerned concepts is the following. In a\nrestricted combinatorial mobile sensor network (RCMSN), there are n sensors\nthat continuously receive and store information from outside. Every two sensors\ncommunicate exactly once, and at an event when two sensors communicate, they\nreceive and store additionally all information the other has stored. In a\ngeometric mobile sensor network (GMSN), a type of RCMSN, the sensors move with\nconstant speed on a straight line and communicate whenever a pair meet. For the\npurpose of analysis, all information received by two sensors before a\ncommunication event and after the previous communication events for each of\nthem is collected into one information packet, and the capacity is defined as\nthe ratio of the average number of sensors reached by the packets and the total\nnumber of sensors.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:48:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 23:20:58 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 00:04:54 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Yizhen", ""]]}, {"id": "1910.04228", "submitter": "\\'Eric Colin de Verdi\\`ere", "authors": "Sergio Cabello and \\'Eric Colin de Verdi\\`ere", "title": "Hardness of Minimum Barrier Shrinkage and Minimum Installation Path", "comments": null, "journal-ref": "Theoretical Computer Science 835 (2020) 120-133", "doi": "10.1016/j.tcs.2020.06.016", "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Minimum Installation Path problem, we are given a graph $G$ with edge\nweights $w(.)$ and two vertices $s,t$ of $G$. We want to assign a non-negative\npower $p(v)$ to each vertex $v$ of $G$ so that the edges $uv$ such that\n$p(u)+p(v)$ is at least $w(uv)$ contain some $s$-$t$-path, and minimize the sum\nof assigned powers. In the Minimum Barrier Shrinkage problem, we are given a\nfamily of disks in the plane and two points $x$ and $y$ lying outside the\ndisks. The task is to shrink the disks, each one possibly by a different\namount, so that we can draw an $x$-$y$ curve that is disjoint from the interior\nof the shrunken disks, and the sum of the decreases in the radii is minimized.\n  We show that the Minimum Installation Path and the Minimum Barrier Shrinkage\nproblems (or, more precisely, the natural decision problems associated with\nthem) are weakly NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 20:01:17 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 10:25:14 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 08:06:53 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Cabello", "Sergio", ""], ["de Verdi\u00e8re", "\u00c9ric Colin", ""]]}, {"id": "1910.04555", "submitter": "Paul Burchard", "authors": "Paul Burchard", "title": "Lower Bounds for Parallel Quantum Counting", "comments": "added discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a generalization of the parallel adversary method to multi-valued\nfunctions, and apply it to prove that there is no parallel quantum advantage\nfor approximate counting.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:11:17 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 13:42:13 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Burchard", "Paul", ""]]}, {"id": "1910.04600", "submitter": "Martin Helfrich", "authors": "Michael Blondin, Javier Esparza, Blaise Genest, Martin Helfrich and\n  Stefan Jaax", "title": "Succinct Population Protocols for Presburger Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angluin et al. proved that population protocols compute exactly the\npredicates definable in Presburger arithmetic (PA), the first-order theory of\naddition. As part of this result, they presented a procedure that translates\nany formula $\\varphi$ of quantifier-free PA with remainder predicates (which\nhas the same expressive power as full PA) into a population protocol with\n$2^{O(\\text{poly}(|\\varphi|))}$ states that computes $\\varphi$. More precisely,\nthe number of states of the protocol is exponential in both the bit length of\nthe largest coefficient in the formula, and the number of nodes of its syntax\ntree.\n  In this paper, we prove that every formula $\\varphi$ of quantifier-free PA\nwith remainder predicates is computable by a leaderless population protocol\nwith $O(\\text{poly}(|\\varphi|))$ states. Our proof is based on several new\nconstructions, which may be of independent interest. Given a formula $\\varphi$\nof quantifier-free PA with remainder predicates, a first construction produces\na succinct protocol (with $O(|\\varphi|^3)$ leaders) that computes $\\varphi$;\nthis completes the work initiated in [STACS'18], where we constructed such\nprotocols for a fragment of PA. For large enough inputs, we can get rid of\nthese leaders. If the input is not large enough, then it is small, and we\ndesign another construction producing a succinct protocol with one leader that\ncomputes $\\varphi$. Our last construction gets rid of this leader for small\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:28:13 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:07:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Genest", "Blaise", ""], ["Helfrich", "Martin", ""], ["Jaax", "Stefan", ""]]}, {"id": "1910.04848", "submitter": "Xiao-Yue Gong", "authors": "James B. Orlin, Xiao-Yue Gong", "title": "A Fast Max Flow Algorithm", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2013, Orlin proved that the max flow problem could be solved in $O(nm)$\ntime. His algorithm ran in $O(nm + m^{1.94})$ time, which was the fastest for\ngraphs with fewer than $n^{1.06}$ arcs. If the graph was not sufficiently\nsparse, the fastest running time was an algorithm due to King, Rao, and Tarjan.\nWe describe a new variant of the excess scaling algorithm for the max flow\nproblem whose running time strictly dominates the running time of the algorithm\nby King et al. Moreover, for graphs in which $m = O(n \\log n)$, the running\ntime of our algorithm dominates that of King et al. by a factor of $O(\\log\\log\nn)$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 20:58:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Orlin", "James B.", ""], ["Gong", "Xiao-Yue", ""]]}, {"id": "1910.05492", "submitter": "Chao Qian", "authors": "Chao Qian", "title": "Multi-objective Evolutionary Algorithms are Still Good: Maximizing\n  Monotone Approximately Submodular Minus Modular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As evolutionary algorithms (EAs) are general-purpose optimization algorithms,\nrecent theoretical studies have tried to analyze their performance for solving\ngeneral problem classes, with the goal of providing a general theoretical\nexplanation of the behavior of EAs. Particularly, a simple multi-objective EA,\ni.e., GSEMO, has been shown to be able to achieve good polynomial-time\napproximation guarantees for submodular optimization, where the objective\nfunction is only required to satisfy some properties but without explicit\nformulation. Submodular optimization has wide applications in diverse areas,\nand previous studies have considered the cases where the objective functions\nare monotone submodular, monotone non-submodular, or non-monotone submodular.\nTo complement this line of research, this paper studies the problem class of\nmaximizing monotone approximately submodular minus modular functions (i.e.,\n$f=g-c$) with a size constraint, where $g$ is a non-negative monotone\napproximately submodular function and $c$ is a non-negative modular function,\nresulting in the objective function $f$ being non-monotone non-submodular. We\nprove that the GSEMO can achieve the best-known polynomial-time approximation\nguarantee. Empirical studies on the applications of Bayesian experimental\ndesign and directed vertex cover show the excellent performance of the GSEMO.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:56:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Qian", "Chao", ""]]}, {"id": "1910.05589", "submitter": "Ioannis Katsikarelis", "authors": "Ioannis Katsikarelis, Michael Lampis, Vangelis Th. Paschos", "title": "Improved (In-)Approximability Bounds for d-Scattered Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $d$-Scattered Set problem we are asked to select at least $k$ vertices\nof a given graph, so that the distance between any pair is at least $d$. We\nstudy the problem's (in-)approximability and offer improvements and extensions\nof known results for Independent Set, of which the problem is a generalization.\n  Specifically, we show:\n  - A lower bound of $\\Delta^{\\lfloor d/2\\rfloor-\\epsilon}$ on the\napproximation ratio of any polynomial-time algorithm for graphs of maximum\ndegree $\\Delta$ and an improved upper bound of $O(\\Delta^{\\lfloor d/2\\rfloor})$\non the approximation ratio of any greedy scheme for this problem.\n  - A polynomial-time $2\\sqrt{n}$-approximation for bipartite graphs and even\nvalues of $d$, that matches the known lower bound by considering the only\nremaining case.\n  - A lower bound on the complexity of any $\\rho$-approximation algorithm of\n(roughly) $2^{\\frac{n^{1-\\epsilon}}{\\rho d}}$ for even $d$ and\n$2^{\\frac{n^{1-\\epsilon}}{\\rho(d+\\rho)}}$ for odd $d$ (under the randomized\nETH), complemented by $\\rho$-approximation algorithms of running times that\n(almost) match these bounds.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 16:13:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Katsikarelis", "Ioannis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1910.06088", "submitter": "Michael Benzaquen", "authors": "Samy Lakhal, Alexandre Darmon, Jean-Philippe Bouchaud and Michael\n  Benzaquen", "title": "Beauty and structural complexity", "comments": "5 pages, 3 figures, 1 table", "journal-ref": "Phys. Rev. Research 2, 022058 (2020)", "doi": "10.1103/PhysRevResearch.2.022058", "report-no": null, "categories": "cond-mat.stat-mech cs.CC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the long-standing question of the relation between image\nappreciation and its statistical properties. We generate two different sets of\nrandom images well distributed along three measures of entropic complexity. We\nrun a large-scale survey in which people are asked to sort the images by\npreference, which reveals maximum appreciation at intermediate entropic\ncomplexity. We show that the algorithmic complexity of the coarse-grained\nimages, expected to capture structural complexity while abstracting from high\nfrequency noise, is a good predictor of preferences. Our analysis suggests that\nthere might exist some universal quantitative criteria for aesthetic judgement.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:24:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Lakhal", "Samy", ""], ["Darmon", "Alexandre", ""], ["Bouchaud", "Jean-Philippe", ""], ["Benzaquen", "Michael", ""]]}, {"id": "1910.06281", "submitter": "Jonas Schmidt", "authors": "Jonas Schmidt, Thomas Schwentick, Nils Vortmeier, Thomas Zeume,\n  Ioannis Kokkinis", "title": "Dynamic Complexity Meets Parameterised Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Complexity studies the maintainability of queries with logical\nformulas in a setting where the underlying structure or database changes over\ntime. Most often, these formulas are from first-order logic, giving rise to the\ndynamic complexity class DynFO. This paper investigates extensions of DynFO in\nthe spirit of parameterised algorithms. In this setting structures come with a\nparameter $k$ and the extensions allow additional \"space\" of size $f(k)$ (in\nthe form of an additional structure of this size) or additional time $f(k)$ (in\nthe form of iterations of formulas) or both. The resulting classes are compared\nwith their non-dynamic counterparts and other classes. The main part of the\npaper explores the applicability of methods for parameterised algorithms to\nthis setting through case studies for various well-known parameterised\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:05:14 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 12:56:05 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Schmidt", "Jonas", ""], ["Schwentick", "Thomas", ""], ["Vortmeier", "Nils", ""], ["Zeume", "Thomas", ""], ["Kokkinis", "Ioannis", ""]]}, {"id": "1910.06435", "submitter": "Nate Veldt", "authors": "Junhao Gan and David F. Gleich and Nate Veldt and Anthony Wirth and\n  Xin Zhang", "title": "Graph Clustering in All Parameter Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution parameters in graph clustering represent a size and quality\ntrade-off. We address the task of efficiently solving a parameterized graph\nclustering objective for all values of a resolution parameter. Specifically, we\nconsider an objective we call LambdaPrime, involving a parameter $\\lambda \\in\n(0,1)$. This objective is related to other parameterized clustering problems,\nsuch as parametric generalizations of modularity, and captures a number of\nspecific clustering problems as special cases, including sparsest cut and\ncluster deletion. While previous work provides approximation results for a\nsingle resolution parameter, we seek a set of approximately optimal clusterings\nfor all values of $\\lambda$ in polynomial time. In particular, we ask the\nquestion, how small a family of clusterings suffices to optimize -- or to\napproximately optimize -- the LambdaPrime objective over the full possible\nspectrum of $\\lambda$?\n  We obtain a family of logarithmically many clusterings by solving the\nparametric linear programming relaxation of LambdaPrime at a logarithmic number\nof parameter values, and round their solutions using existing approximation\nalgorithms. We prove that this number is tight up to a constant factor.\nSpecifically, for a certain class of ring graphs, a logarithmic number of\nfeasible solutions is required to provide a constant-factor approximation for\nthe LambdaPrime LP relaxation in all parameter regimes. We additionally show\nthat for any graph with $n$ nodes and $m$ edges, there exists a set of $m$ or\nfewer clusterings such that for every $\\lambda \\in (0,1)$, the family contains\nan exact solution to the LambdaPrime objective. There also exists a set of\n$O(\\log n)$ clusterings that provide a $(1+\\varepsilon)$-approximate solution\nin all parameter regimes; we demonstrate simple graph classes for which these\nbounds are tight.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 21:40:41 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Gan", "Junhao", ""], ["Gleich", "David F.", ""], ["Veldt", "Nate", ""], ["Wirth", "Anthony", ""], ["Zhang", "Xin", ""]]}, {"id": "1910.06846", "submitter": "Dan Vilenchik", "authors": "Guy Holtzman, Adam Soffer and Dan Vilenchik", "title": "A greedy anytime algorithm for sparse PCA", "comments": "improving results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The taxing computational effort that is involved in solving some\nhigh-dimensional statistical problems, in particular problems involving\nnon-convex optimization, has popularized the development and analysis of\nalgorithms that run efficiently (polynomial-time) but with no general guarantee\non statistical consistency. In light of the ever-increasing compute power and\ndecreasing costs, a more useful characterization of algorithms is by their\nability to calibrate the invested computational effort with various\ncharacteristics of the input at hand and with the available computational\nresources. For example, design an algorithm that always guarantees statistical\nconsistency of its output by increasing the running time as the SNR weakens. We\npropose a new greedy algorithm for the $\\ell_0$-sparse PCA problem which\nsupports the calibration principle. We provide both a rigorous analysis of our\nalgorithm in the spiked covariance model, as well as simulation results and\ncomparison with other existing methods. Our findings show that our algorithm\nrecovers the spike in SNR regimes where all polynomial-time algorithms fail\nwhile running in a reasonable parallel-time on a cluster.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:09:13 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 19:47:25 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 16:07:42 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 13:44:58 GMT"}, {"version": "v5", "created": "Wed, 12 Feb 2020 12:32:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Holtzman", "Guy", ""], ["Soffer", "Adam", ""], ["Vilenchik", "Dan", ""]]}, {"id": "1910.08503", "submitter": "Anupam Das", "authors": "Sam Buss, Anupam Das and Alexander Knop", "title": "Proof complexity of systems of (non-deterministic) decision trees and\n  branching programs", "comments": "36 pages, 1 figure, full version of CSL 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies propositional proof systems in which lines are sequents of\ndecision trees or branching programs - deterministic and nondeterministic. The\nsystems LDT and LNDT are propositional proof systems in which lines represent\ndeterministic or non-deterministic decision trees. Branching programs are\nmodeled as decision dags. Adding extension to LDT and LNDT gives systems eLDT\nand eLNDT in which lines represent deterministic and non-deterministic\nbranching programs, respectively.\n  Deterministic and non-deterministic branching programs correspond to\nlog-space (L) and nondeterministic log-space (NL). Thus the systems eLDT and\neLNDT are propositional proof systems that reason with (nonuniform) L and NL\nproperties.\n  The main results of the paper are simulation and non-simulation results for\ntree-like and dag-like proofs in the systems LDT, LNDT, eLDT, and eLNDT. These\nsystems are also compared with Frege systems, constantdepth Frege systems and\nextended Frege systems\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:54:33 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Buss", "Sam", ""], ["Das", "Anupam", ""], ["Knop", "Alexander", ""]]}, {"id": "1910.08517", "submitter": "Manuel Sorge", "authors": "Shaohua Li, Marcin Pilipczuk, Manuel Sorge", "title": "Cluster Editing parameterized above the size of a modification-disjoint\n  $P_3$ packing is para-NP-hard", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ and an integer $k$, the Cluster Editing problem asks\nwhether we can transform $G$ into a union of vertex-disjoint cliques by at most\n$k$ modifications (edge deletions or insertions). In this paper, we study the\nfollowing variant of Cluster Editing. We are given a graph $G=(V,E)$, a packing\n$\\mathcal{H}$ of modification-disjoint induced $P_3$s (no pair of $P_3$s in\n$\\cal H$ share an edge or non-edge) and an integer $\\ell$. The task is to\ndecide whether $G$ can be transformed into a union of vertex-disjoint cliques\nby at most $\\ell+|\\cal H|$ modifications (edge deletions or insertions). We\nshow that this problem is NP-hard even when $\\ell=0$ (in which case the problem\nasks to turn $G$ into a disjoint union of cliques by performing exactly one\nedge deletion or insertion per element of $\\cal H$). This answers negatively a\nquestion of van Bevern, Froese, and Komusiewicz (CSR 2016, ToCS 2018), repeated\nby Komusiewicz at Shonan meeting no. 144 in March 2019.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:28:09 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Li", "Shaohua", ""], ["Pilipczuk", "Marcin", ""], ["Sorge", "Manuel", ""]]}, {"id": "1910.08571", "submitter": "Titus Dose", "authors": "Titus Dose", "title": "$\\mathrm{P}$-Optimal Proof Systems for Each $\\mathrm{coNP}$-Complete Set\n  and no Complete Problems in $\\mathrm{NP}\\cap\\mathrm{coNP}$ Relative to an\n  Oracle", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.06175,\n  arXiv:1909.02839", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on a working program initiated by Pudl\\'ak [Pud17] and construct an\noracle relative to which each $\\mathrm{coNP}$-complete set has\n$\\mathrm{P}$-optimal proof systems and $\\mathrm{NP}\\cap\\mathrm{coNP}$ does not\nhave complete problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 18:06:19 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:30:25 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Dose", "Titus", ""]]}, {"id": "1910.08915", "submitter": "Johann Makowsky", "authors": "Tomer Kotek and Johann A. Makowsky", "title": "The exact complexity of the Tutte polynomial", "comments": "26 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a survey on the exact complexity of computing the Tutte polynomial.\nIt is the longer 2017 version of Chapter 25 of the CRC Handbook on the Tutte\npolynomial and related topics, edited by J. Ellis-Monaghan and I. Moffatt,\nwhich is due to appear in the first quarter of 2020. In the version to be\npublished in the Handbook the Sections 5 and 6 are shortened and made into a\nsingle section.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 07:06:55 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kotek", "Tomer", ""], ["Makowsky", "Johann A.", ""]]}, {"id": "1910.09190", "submitter": "Mikhail Volkov", "authors": "N. V. Kitov, M. V. Volkov", "title": "Identities of the Kauffman Monoid $\\mathcal{K}_4$ and of the Jones\n  monoid $\\mathcal{J}_4$", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kauffman monoids $\\mathcal{K}_n$ and Jones monoids $\\mathcal{J}_n$,\n$n=2,3,\\dots$, are two families of monoids relevant in knot theory. We prove a\nsomewhat counterintuitive result that the Kauffman monoids $\\mathcal{K}_3$ and\n$\\mathcal{K}_4$ satisfy exactly the same identities. This leads to a polynomial\ntime algorithm to check whether a given identity holds in $\\mathcal{K}_4$. As a\nbyproduct, we also find a polynomial time algorithm for checking identities in\nthe Jones monoid $\\mathcal{J}_4$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:48:23 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kitov", "N. V.", ""], ["Volkov", "M. V.", ""]]}, {"id": "1910.09228", "submitter": "Ulrich Bauer", "authors": "Ulrich Bauer, Abhishek Rathod, Jonathan Spreer", "title": "Parametrized Complexity of Expansion Height", "comments": "15 pages, 2 figures", "journal-ref": "27th Annual European Symposium on Algorithms (ESA 2019), Leibniz\n  International Proceedings in Informatics (LIPIcs) vol. 144, 2019, p. 13:1-15", "doi": "10.4230/LIPIcs.ESA.2019.13", "report-no": null, "categories": "math.AT cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding whether two simplicial complexes are homotopy equivalent is a\nfundamental problem in topology, which is famously undecidable. There exists a\ncombinatorial refinement of this concept, called simple-homotopy equivalence:\ntwo simplicial complexes are of the same simple-homotopy type if they can be\ntransformed into each other by a sequence of two basic homotopy equivalences,\nan elementary collapse and its inverse, an elementary expansion. In this\narticle we consider the following related problem: given a 2-dimensional\nsimplicial complex, is there a simple-homotopy equivalence to a 1-dimensional\nsimplicial complex using at most p expansions? We show that the problem, which\nwe call the erasability expansion height, is W[P]-complete in the natural\nparameter p.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:32:43 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bauer", "Ulrich", ""], ["Rathod", "Abhishek", ""], ["Spreer", "Jonathan", ""]]}, {"id": "1910.09625", "submitter": "Cristobal Rojas", "authors": "Cristobal Rojas and Michael Yampolsky", "title": "How to lose at Monte Carlo: a simple dynamical system whose typical\n  statistical behavior is non computable", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the simplest non-linear discrete dynamical systems, given by the\nlogistic maps $f_{a}(x)=ax(1-x)$ of the interval $[0,1]$. We show that there\nexist real parameters $a\\in (0,4)$ for which almost every orbit of $f_a$ has\nthe same statistical distribution in $[0,1]$, but this limiting distribution is\nnot Turing computable. In particular, the Monte Carlo method cannot be applied\nto study these dynamical systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:38:22 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:01:25 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Rojas", "Cristobal", ""], ["Yampolsky", "Michael", ""]]}, {"id": "1910.09791", "submitter": "Ei Ando", "authors": "Ei Ando", "title": "The Distribution Function of the Longest Path Length in Constant\n  Treewidth DAGs with Random Edge Length", "comments": "40 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the length $X_{\\rm MAX}$ of the longest path in directed\nacyclic graph (DAG) $G=(V,E)$ that have random edge lengths, where $|V|=n$ and\n$|E|=m$. Especially, when the edge lengths are mutually independent and\nuniformly distributed, the problem of computing the distribution function\n$\\Pr[X_{\\rm MAX}\\le x]$ is known to be $#$P-hard even in case $G$ is a directed\npath. This is because $\\Pr[X_{\\rm MAX}\\le x]$ is equal to the volume of the\nknapsack polytope, an $m$-dimensional unit hypercube truncated by a halfspace.\nIn this paper, we show that there is a {\\em deterministic} fully polynomial\ntime approximation scheme (FPTAS) for computing $\\Pr[X_{\\rm MAX}\\le x]$ in case\nthe treewidth of $G$ is bounded by a constant $k$, where there may be\nexponentially many $s-t$ paths in $G$. The running time of our algorithm is\n$O((3k+2)^2 n(\\frac{(6k+6)mn}{\\epsilon})^{9k^2+15k+6})$ to achieve a\nmultiplicative approximation ratio $1+\\epsilon$. On the way to show our FPTAS,\nwe show a fundamental formula that represents $\\Pr[X_{\\rm MAX}\\le x]$ by at\nmost $n-1$ repetition of definite integrals. This also leads us to more\nresults. In case the edge lengths obey the mutually independent standard\nexponential distribution, we show that there exists a $((6k+4)mn)^{O(k)}$ time\nexact algorithm. We also show, for random edge lengths satisfying certain\nconditions, that computing $\\Pr[X_{\\rm MAX}\\le x]$ is fixed parameter tractable\nif we choose treewidth $k$, the additive error $\\epsilon'$ and $x$ as the\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:56:53 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 03:19:37 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 15:50:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ando", "Ei", ""]]}, {"id": "1910.10357", "submitter": "Ryoma Senda", "authors": "Ryoma Senda, Yoshiaki Takata and Hiroyuki Seki", "title": "Complexity Results on Register Pushdown Automata", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Register pushdown automata (RPDA) is an extension of classical pushdown\nautomata to handle data values in a restricted way. RPDA attracts attention as\na model of a query language for structured documents with data values. The\nmembership and emptiness problems for RPDA are known to be EXPTIME-complete.\nThis paper shows the membership problem becomes PSPACE-complete and NP-complete\nfor nondecreasing and growing RPDA, respectively, while the emptiness problem\nremains EXPTIME-complete for these subclasses.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:19:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Senda", "Ryoma", ""], ["Takata", "Yoshiaki", ""], ["Seki", "Hiroyuki", ""]]}, {"id": "1910.10708", "submitter": "Stepan Margaryan", "authors": "Stepan G. Margaryan", "title": "Necessary and sufficient conditions for Boolean satisfiability", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study in this article seeks to find conditions that are necessary and\nsufficient for the satisfiability of a Boolean function. We will use the\nconcept of special covering of a set introduced in [9] and study the relation\nof this concept with the satisfiability of Boolean functions. We show that the\nproblem of existence of a special covering of a set is equivalent to the\nBoolean satisfiability problem. Thus, an important result is the proof of the\nexistence of necessary and sufficient conditions for the existence of special\ncovering of the set. This result allows us to formulate the necessary and\nsufficient conditions for Boolean satisfiability, considering the function in\nconjunctive normal form as a set of clauses. To formulate the same result in\nterm of Boolean function we introduce the concept of proportional conjunctive\nnormal form of a function, which is a conjunctive normal form of a function\nwith the condition that each clause contains a negative literal or each clause\ncontains a positive literal. Thus, we obtain that the satisfiability of a\nBoolean function represented in conjunctive normal form is equivalent to the\npossibility of converting it into a function in proportional conjunctive normal\nform by literal inversion.To prove these results, some algorithmic procedures\nare used. As a result of these procedures, in parallel, we obtain the Boolean\nvalues for the variables that provide the satisfiability of the\nfunction.Estimates of the complexity of these algorithmic procedures will be\npresented in the next article. Generally accepted terminology on set theory,\nBoolean functions, and graph theory is consistent with the terminology found in\nthe relevant works included in the bibliography [1],[2],[3]. The newly\nintroduced terms are not found in use by other authors and do not contradict to\nother terms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 06:43:34 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:46:48 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Margaryan", "Stepan G.", ""]]}, {"id": "1910.10882", "submitter": "Diego Maldonado", "authors": "Eric Goles, Diego Maldonado, Pedro Montealegre, Mart\\'in R\\'ios-Wilson", "title": "On the Complexity of Asynchronous Freezing Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the family of freezing cellular automata (FCA) in the\ncontext of asynchronous updating schemes. A cellular automaton is called\nfreezing if there exists an order of its states, and the transitions are only\nallowed to go from a lower to a higher state. A cellular automaton is\nasynchronous if at each time-step only one cell is updated. Given\nconfiguration, we say that a cell is unstable if there exists a sequential\nupdating scheme that changes its state. In this context, we define the problem\nAsyncUnstability, which consists in deciding if a cell is unstable or not. In\ngeneral AsyncUnstability is in NP, and we study in which cases we can solve the\nproblem by a more efficient algorithm.\n  We begin showing that AsyncUnstability is in NL for any one-dimensional FCA.\nThen we focus on the family of life-like freezing CA (LFCA), which is a family\nof two-dimensional two-state FCA that generalize the freezing version of the\ngame of life, known as life without death. We study the complexity of\nAsyncUnstability for all LFCA in the triangular and square grids, showing that\nalmost all of them can be solved in NC, except for one rule for which the\nproblem is NP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:16:43 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Goles", "Eric", ""], ["Maldonado", "Diego", ""], ["Montealegre", "Pedro", ""], ["R\u00edos-Wilson", "Mart\u00edn", ""]]}, {"id": "1910.11325", "submitter": "Oleg Verbitsky", "authors": "V. Arvind, Frank Fuhlbr\\\"uck, Johannes K\\\"obler, Oleg Verbitsky", "title": "On the Weisfeiler-Leman Dimension of Fractional Packing", "comments": "26 pages, 1 figure, major revision of the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-dimensional Weisfeiler-Leman procedure ($k$-WL), which colors\n$k$-tuples of vertices in rounds based on the neighborhood structure in the\ngraph, has proven to be immensely fruitful in the algorithmic study of Graph\nIsomorphism. More generally, it is of fundamental importance in understanding\nand exploiting symmetries in graphs in various settings. Two graphs are\n$k$-WL-equivalent if the $k$-dimensional Weisfeiler-Leman procedure produces\nthe same final coloring on both graphs. 1-WL-equivalence is known as fractional\nisomorphism of graphs, and the $k$-WL-equivalence relation becomes finer as $k$\nincreases.\n  We investigate to what extent standard graph parameters are preserved by\n$k$-WL-equivalence, focusing on fractional graph packing numbers. The integral\npacking numbers are typically NP-hard to compute, and we discuss applicability\nof $k$-WL-invariance for estimating the integrality gap of the LP relaxation\nprovided by their fractional counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:58:24 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 11:37:13 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 13:39:30 GMT"}, {"version": "v4", "created": "Fri, 5 Jun 2020 16:07:46 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Arvind", "V.", ""], ["Fuhlbr\u00fcck", "Frank", ""], ["K\u00f6bler", "Johannes", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "1910.11749", "submitter": "Jonathan Blanchette", "authors": "Jonathan Blanchette, Robert Lagani\\`ere", "title": "A Curious Link Between Prime Numbers, the Maundy Cake Problem and\n  Parallel Sorting", "comments": "10 pages, 2 figures, 3 tables, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new theoretical algorithms that sums the n-ary comparators output\nin order to get the permutation indices in order to sort a sequence. By\nanalysing the parallel ranking algorithm, we found that the special comparators\nnumber of elements it processes divide the number of elements to be sorted.\nUsing the divide and conquer method, we can express the sorting problem into\nsumming output of comparators taking a prime number of elements, given that\nthis prime number divides the initial disordered sequence length. The number of\nsums is directly related to the Maundy cake problem. Furthermore, we provide a\nnew sequence that counts the number of comparators used in the algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:17:30 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 16:58:01 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 15:27:33 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 14:44:21 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Blanchette", "Jonathan", ""], ["Lagani\u00e8re", "Robert", ""]]}, {"id": "1910.11850", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "Tight Running Time Lower Bounds for Strong Inapproximability of Maximum\n  $k$-Coverage, Unique Set Cover and Related Problems (via $t$-Wise Agreement\n  Testing Theorem)", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, assuming the (randomized) Gap Exponential Time Hypothesis (Gap-ETH),\nthat the following tasks cannot be done in $T(k) \\cdot N^{o(k)}$-time for any\nfunction $T$ where $N$ denote the input size:\n  - $\\left(1 - \\frac{1}{e} + \\epsilon\\right)$-approximation for Max\n$k$-Coverage for any $\\epsilon > 0$,\n  - $\\left(1 + \\frac{2}{e} - \\epsilon\\right)$-approximation for $k$-Median (in\ngeneral metrics) for any constant $\\epsilon > 0$.\n  - $\\left(1 + \\frac{8}{e} - \\epsilon\\right)$-approximation for $k$-Mean (in\ngeneral metrics) for any constant $\\epsilon > 0$.\n  - Any constant factor approximation for $k$-Unique Set Cover, $k$-Nearest\nCodeword Problem and $k$-Closest Vector Problem.\n  - $(1 + \\delta)$-approximation for $k$-Minimum Distance Problem and\n$k$-Shortest Vector Problem for some $\\delta > 0$.\n  Since these problems can be trivially solved in $N^{O(k)}$ time, our running\ntime lower bounds are essentially tight. In terms of approximation ratios, Max\n$k$-Coverage is well-known to admit polynomial-time $\\left(1 -\n\\frac{1}{e}\\right)$-approximation algorithms, and, recently, it was shown that\n$k$-Median and $k$-Mean are approximable to within factors of $\\left(1 +\n\\frac{2}{e}\\right)$ and $\\left(1 + \\frac{8}{e}\\right)$ respectively in FPT time\n[Cohen-Addad et al. 2019]; hence, our inapproximability ratios are also tight\nfor these three problems. For the remaining problems, no non-trivial FPT\napproximation algorithms are known.\n  The starting point of all our hardness results mentioned above is the Label\nCover problem (with projection constraints). We show that Label Cover cannot be\napproximated to within any constant factor in $T(k) \\cdot N^{o(k)}$ time, where\n$N$ and $k$ denote the size of the input and the number of nodes on the side\nwith the larger alphabet respectively. With this hardness, the above results\nfollow immediately from known reductions...\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:10:48 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "1910.11921", "submitter": "Cyrus Rashtchian", "authors": "Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian", "title": "Equivalence of Systematic Linear Data Structures and Matrix Rigidity", "comments": "23 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong\nlower bounds for linear data structures would imply new bounds for rigid\nmatrices. However, their result utilizes an algorithm that requires an $NP$\noracle, and hence, the rigid matrices are not explicit. In this work, we derive\nan equivalence between rigidity and the systematic linear model of data\nstructures. For the $n$-dimensional inner product problem with $m$ queries, we\nprove that lower bounds on the query time imply rigidity lower bounds for the\nquery set itself. In particular, an explicit lower bound of\n$\\omega\\left(\\frac{n}{r}\\log m\\right)$ for $r$ redundant storage bits would\nyield better rigidity parameters than the best bounds due to Alon, Panigrahy,\nand Yekhanin. We also prove a converse result, showing that rigid matrices\ndirectly correspond to hard query sets for the systematic linear model. As an\napplication, we prove that the set of vectors obtained from rank one binary\nmatrices is rigid with parameters matching the known results for explicit sets.\nThis implies that the vector-matrix-vector problem requires query time\n$\\Omega(n^{3/2}/r)$ for redundancy $r \\geq \\sqrt{n}$ in the systematic linear\nmodel, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove\na cell probe lower bound for the vector-matrix-vector problem in the high error\nregime, improving a result of Chattopadhyay, Kouck\\'{y}, Loff, and\nMukhopadhyay.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:14:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ramamoorthy", "Sivaramakrishnan Natarajan", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1910.12026", "submitter": "Duncan Adamson", "authors": "Duncan Adamson, Argyrios Deligkas, Vladimir Gusev, and Igor Potapov", "title": "On the Hardness of Energy Minimisation for Crystal Structure Prediction", "comments": "Short version to be published in SOFSEM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crystal Structure Prediction (csp) is one of the central and most challenging\nproblems in materials science and computational chemistry. In csp, the goal is\nto find a configuration of ions in 3D space that yields the lowest potential\nenergy. Finding an efficient procedure to solve this complex optimisation\nquestion is a well known open problem in computational chemistry. Due to the\nexponentially large search space, the problem has been referred in several\nmaterials-science papers as \"NP-Hard and very challenging\" without any formal\nproof though. This paper fills a gap in the literature providing the first set\nof formally proven NP-Hardness results for a variant of csp with various\nrealistic constraints. In particular, we focus on the problem of removal: the\ngoal is to find a substructure with minimal potential energy, by removing a\nsubset of the ions from a given initial structure. Our main contributions are\nNP-Hardness results for the csp removal problem, new embeddings of\ncombinatorial graph problems into geometrical settings, and a more systematic\nexploration of the energy function to reveal the complexity of csp. In a wider\ncontext, our results contribute to the analysis of computational problems for\nweighted graphs embedded into the three-dimensional Euclidean space.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 09:05:11 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 17:37:47 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 14:22:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Adamson", "Duncan", ""], ["Deligkas", "Argyrios", ""], ["Gusev", "Vladimir", ""], ["Potapov", "Igor", ""]]}, {"id": "1910.12085", "submitter": "Samuel Gunn", "authors": "Scott Aaronson and Sam Gunn", "title": "On the Classical Hardness of Spoofing Linear Cross-Entropy Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Google announced the first demonstration of quantum computational\nsupremacy with a programmable superconducting processor. Their demonstration is\nbased on collecting samples from the output distribution of a noisy random\nquantum circuit, then applying a statistical test to those samples called\nLinear Cross-Entropy Benchmarking (Linear XEB). This raises a theoretical\nquestion: how hard is it for a classical computer to spoof the results of the\nLinear XEB test? In this short note, we adapt an analysis of Aaronson and Chen\n[2017] to prove a conditional hardness result for Linear XEB spoofing.\nSpecifically, we show that the problem is classically hard, assuming that there\nis no efficient classical algorithm that, given a random n-qubit quantum\ncircuit C, estimates the probability of C outputting a specific output string,\nsay 0^n, with variance even slightly better than that of the trivial estimator\nthat always estimates 1/2^n. Our result automatically encompasses the case of\nnoisy circuits.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:15:51 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 04:45:49 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:01:30 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 02:49:46 GMT"}, {"version": "v5", "created": "Thu, 6 Feb 2020 03:54:30 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Aaronson", "Scott", ""], ["Gunn", "Sam", ""]]}, {"id": "1910.12353", "submitter": "Ramin Javadi", "authors": "Ramin Javadi and Amir Nikabadi", "title": "On the Parameterized Complexity of Sparsest Cut and Small-set Expansion\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the NP-hard \\textsc{$k$-Sparsest Cut} problem ($k$SC) in which,\ngiven an undirected graph $G = (V, E)$ and a parameter $k$, the objective is to\npartition vertex set into $k$ subsets whose maximum edge expansion is\nminimized. Herein, the edge expansion of a subset $S \\subseteq V$ is defined as\nthe sum of the weights of edges exiting $S$ divided by the number of vertices\nin $S$. Another problem that has been investigated is \\textsc{$k$-Small-Set\nExpansion} problem ($k$SSE), which aims to find a subset with minimum edge\nexpansion with a restriction on the size of the subset. We extend previous\nstudies on $k$SC and $k$SSE by inspecting their parameterized complexity. On\nthe positive side, we present two FPT algorithms for both $k$SSE and 2SC\nproblems where in the first algorithm we consider the parameter treewidth of\nthe input graph and uses exponential space, and in the second we consider the\nparameter vertex cover number of the input graph and uses polynomial space.\nMoreover, we consider the unweighted version of the $k$SC problem where $k \\geq\n2$ is fixed and proposed two FPT algorithms with parameters treewidth and\nvertex cover number of the input graph. We also propose a randomized FPT\nalgorithm for $k$SSE when parameterized by $k$ and the maximum degree of the\ninput graph combined. Its derandomization is done efficiently.\n  \\noindent On the negative side, first we prove that for every fixed integer\n$k,\\tau\\geq 3$, the problem $k$SC is NP-hard for graphs with vertex cover\nnumber at most $\\tau$. We also show that $k$SC is W[1]-hard when parameterized\nby the treewidth of the input graph and the number~$k$ of components combined\nusing a reduction from \\textsc{Unary Bin Packing}. Furthermore, we prove that\n$k$SC remains NP-hard for graphs with maximum degree three and also graphs with\ndegeneracy two. Finally, we prove that the unweighted $k$SSE is W[1]-hard for\nthe parameter $k$.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 21:08:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Javadi", "Ramin", ""], ["Nikabadi", "Amir", ""]]}, {"id": "1910.12458", "submitter": "Jiaheng Wang", "authors": "Xiaoming Sun, Yuan Sun, Jiaheng Wang, Kewen Wu, Zhiyu Xia, Yufan Zheng", "title": "On the Degree of Boolean Functions as Polynomials over $\\mathbb{Z}_m$", "comments": "To appear in ICALP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial representations of Boolean functions over various rings such as\n$\\mathbb{Z}$ and $\\mathbb{Z}_m$ have been studied since Minsky and Papert\n(1969). From then on, they have been employed in a large variety of fields\nincluding communication complexity, circuit complexity, learning theory, coding\ntheory and so on. For any integer $m\\ge2$, each Boolean function has a unique\nmultilinear polynomial representation over ring $\\mathbb Z_m$. The degree of\nsuch polynomial is called modulo-$m$ degree, denoted as\n$\\mathrm{deg}_m(\\cdot)$.\n  In this paper, we investigate the lower bound of modulo-$m$ degree of Boolean\nfunctions. When $m=p^k$ ($k\\ge 1$) for some prime $p$, we give a tight lower\nbound that $\\mathrm{deg}_m(f)\\geq k(p-1)$ for any non-degenerated function\n$f:\\{0,1\\}^n\\to\\{0,1\\}$, provided that $n$ is sufficient large. When $m$\ncontains two different prime factors $p$ and $q$, we give a nearly optimal\nlower bound for any symmetric function $f:\\{0,1\\}^n\\to\\{0,1\\}$ that\n$\\mathrm{deg}_m(f) \\geq \\frac{n}{2+\\frac{1}{p-1}+\\frac{1}{q-1}}$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:06:15 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 04:37:34 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 10:10:03 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sun", "Xiaoming", ""], ["Sun", "Yuan", ""], ["Wang", "Jiaheng", ""], ["Wu", "Kewen", ""], ["Xia", "Zhiyu", ""], ["Zheng", "Yufan", ""]]}, {"id": "1910.12504", "submitter": "Marc Goerigk", "authors": "Trivikram Dokka and Marc Goerigk", "title": "The Multi-level Bottleneck Assignment Problem: Complexity and Solution\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-level bottleneck assignment problem (MBA), which has\nimportant applications in scheduling and quantitative finance. Given a weight\nmatrix, the task is to rearrange entries in each column such that the maximum\nsum of values in each row is as small as possible. We analyze the complexity of\nthis problem in a generalized setting, where there are restrictions in how\nvalues in columns can be permuted. We present a lower bound on its\napproximability by giving a non-trivial gap reduction from three-dimensional\nmatching to MBA.\n  To solve MBA, a greedy method has been used in the literature. We present new\nsolution methods based on an extension of the greedy method, an integer\nprogramming formulation, and a column generation heuristic. In computational\nexperiments we show that it is possible to outperform the standard greedy\napproach by around 10% on random instances.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 08:43:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Dokka", "Trivikram", ""], ["Goerigk", "Marc", ""]]}, {"id": "1910.12937", "submitter": "Fan Chen", "authors": "Fan Chen, Yini Zhang, and Karl Rohe", "title": "Targeted sampling from massive block model graphs with personalized\n  PageRank", "comments": "61 pages, 5 figures", "journal-ref": "J. R. Stat. Soc. B (2020), 82: 99-126", "doi": "10.1111/rssb.12349", "report-no": null, "categories": "cs.SI cs.CC cs.DL stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper provides statistical theory and intuition for personalized PageRank\n(called \"PPR\"): a popular technique that samples a small community from a\nmassive network. We study a setting where the entire network is expensive to\nobtain thoroughly or to maintain, but we can start from a seed node of interest\nand \"crawl\" the network to find other nodes through their connections. By\ncrawling the graph in a designed way, the PPR vector can be approximated\nwithout querying the entire massive graph, making it an alternative to snowball\nsampling. Using the degree-corrected stochastic block model, we study whether\nthe PPR vector can select nodes that belong to the same block as the seed node.\nWe provide a simple and interpretable form for the PPR vector, highlighting its\nbiases towards high degree nodes outside the target block. We examine a simple\nadjustment based on node degrees and establish consistency results for PPR\nclustering that allows for directed graphs. These results are enabled by recent\ntechnical advances showing the elementwise convergence of eigenvectors. We\nillustrate the method with the massive Twitter friendship graph, which we crawl\nby using the Twitter application programming interface. We find that the\nadjusted and unadjusted PPR techniques are complementary approaches, where the\nadjustment makes the results particularly localized around the seed node, and\nthat the bias adjustment greatly benefits from degree regularization.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:48:57 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 17:07:19 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Zhang", "Yini", ""], ["Rohe", "Karl", ""]]}, {"id": "1910.13530", "submitter": "Fabrizio Riguzzi PhD", "authors": "Fabrizio Riguzzi", "title": "Quantum Weighted Model Counting", "comments": null, "journal-ref": "European Conference on Artificial Intelligence (ECAI 2020)", "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Weighted Model Counting (WMC) we assign weights to Boolean literals and we\nwant to compute the sum of the weights of the models of a Boolean function\nwhere the weight of a model is the product of the weights of its literals. WMC\nwas shown to be particularly effective for performing inference in graphical\nmodels, with a complexity of $O(n2^w)$ where $n$ is the number of variables and\n$w$ is the treewidth. In this paper, we propose a quantum algorithm for\nperforming WMC, Quantum WMC (QWMC), that modifies the quantum model counting\nalgorithm to take into account the weights. In turn, the model counting\nalgorithm uses the algorithms of quantum search, phase estimation and Fourier\ntransform. In the black box model of computation, where we can only query an\noracle for evaluating the Boolean function given an assignment, QWMC solves the\nproblem approximately with a complexity of $\\Theta(2^{\\frac{n}{2}})$ oracle\ncalls while classically the best complexity is $\\Theta(2^n)$, thus achieving a\nquadratic speedup.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 13:01:07 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 09:59:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riguzzi", "Fabrizio", ""]]}, {"id": "1910.13543", "submitter": "Young Kun Ko", "authors": "Young Kun Ko, Omri Weinstein", "title": "An Adaptive Step Toward the Multiphase Conjecture", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2010, P\\v{a}tra\\c{s}cu proposed the following three-phase dynamic problem,\nas a candidate for proving polynomial lower bounds on the operational time of\ndynamic data structures:\n  I: Preprocess a collection of sets $\\vec{S} = S_1, \\ldots , S_k \\subseteq\n[n]$, where $k=\\operatorname{poly}(n)$.\n  II: A set $T\\subseteq [n]$ is revealed, and the data structure updates its\nmemory.\n  III: An index $i \\in [k]$ is revealed, and the data structure must determine\nif $S_i\\cap T=^? \\emptyset$.\n  P\\v{a}tra\\c{s}cu conjectured that any data structure for the Multiphase\nproblem must make $n^\\epsilon$ cell-probes in either Phase II or III, and\nshowed that this would imply similar unconditional lower bounds on many\nimportant dynamic data structure problems. Alas, there has been almost no\nprogress on this conjecture in the past decade since its introduction. We show\nan $\\tilde{\\Omega}(\\sqrt{n})$ cell-probe lower bound on the Multiphase problem\nfor data structures with general (adaptive) updates, and queries with unbounded\nbut \"layered\" adaptivity. This result captures all known set-intersection data\nstructures and significantly strengthens previous Multiphase lower bounds,\nwhich only captured non-adaptive data structures.\n  Our main technical result is a communication lower bound on a 4-party variant\nof P\\v{a}tra\\c{s}cu's Number-On-Forehead Multiphase game, using information\ncomplexity techniques. We also show that a lower bound on P\\v{a}tra\\c{s}cu's\noriginal NOF game would imply a polynomial ($n^{1+\\epsilon}$) lower bound on\nthe number of wires of any constant-depth circuit with arbitrary gates\ncomputing a random $\\tilde{O}(n)\\times n$ linear operator $x \\mapsto Ax$, a\nlong-standing open problem in circuit complexity. This suggests that the NOF\nconjecture is much stronger than its data structure counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:30:14 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Ko", "Young Kun", ""], ["Weinstein", "Omri", ""]]}, {"id": "1910.13615", "submitter": "Xiang Huang", "authors": "Xiang Huang, Jack H. Lutz, Elvira Mayordomo, and Donald M. Stull", "title": "Asymptotic Divergences and Strong Dichotomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.FL cs.GT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schnorr-Stimm dichotomy theorem concerns finite-state gamblers that bet\non infinite sequences of symbols taken from a finite alphabet $\\Sigma$.\n  In this paper we use the Kullback-Leibler divergence to formulate the\n$\\textit{lower asymptotic divergence}$ $\\text{div}(S||\\alpha)$ of a probability\nmeasure $\\alpha$ on $\\Sigma$ from a sequence $S$ over $\\Sigma$ and the\n$\\textit{upper asymptotic divergence}$ $\\text{Div}(S||\\alpha)$ of $\\alpha$ from\n$S$ in such a way that a sequence $S$ is $\\alpha$-normal (meaning that every\nstring $w$ has asymptotic frequency $\\alpha(w)$ in $S$) if and only if\n$\\text{Div}(S||\\alpha)=0$. We also use the Kullback-Leibler divergence to\nquantify the $\\textit{total risk }$ $\\text{Risk}_G(w)$ that a finite-state\ngambler $G$ takes when betting along a prefix $w$ of $S$.\n  Our main theorem is a $\\textit{strong dichotomy theorem}$ that uses the above\nnotions to $\\textit{quantify}$ the exponential rates of winning and losing on\nthe two sides of the Schnorr-Stimm dichotomy theorem (with the latter routinely\nextended from normality to $\\alpha$-normality). Modulo asymptotic caveats in\nthe paper, our strong dichotomy theorem says that the following two things hold\nfor prefixes $w$ of $S$.\n  (1) The infinitely-often exponential rate of winning is\n$2^{\\text{Div}(S||\\alpha)|w|}$.\n  (2) The exponential rate of loss is $2^{-\\text{Risk}_G(w)}$.\n  We also use (1) to show that $1-\\text{Div}(S||\\alpha)/c$, where $c= \\log(1/\n\\min_{a\\in\\Sigma}\\alpha(a))$, is an upper bound on the finite-state\n$\\alpha$-dimension of $S$ and prove the dual fact that\n$1-\\text{div}(S||\\alpha)/c$ is an upper bound on the finite-state strong\n$\\alpha$-dimension of $S$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 01:30:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Huang", "Xiang", ""], ["Lutz", "Jack H.", ""], ["Mayordomo", "Elvira", ""], ["Stull", "Donald M.", ""]]}, {"id": "1910.14646", "submitter": "Adam Bouland", "authors": "Adam Bouland, Bill Fefferman, Umesh Vazirani", "title": "Computational pseudorandomness, the wormhole growth paradox, and\n  constraints on the AdS/CFT duality", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC gr-qc hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental issue in the AdS/CFT correspondence is the wormhole growth\nparadox. Susskind's conjectured resolution of the paradox was to equate the\nvolume of the wormhole with the circuit complexity of its dual quantum state in\nthe CFT. We study the ramifications of this conjecture from a\ncomplexity-theoretic perspective. Specifically we give evidence for the\nexistence of computationally pseudorandom states in the CFT, and argue that\nwormhole volume is measureable in a non-physical but computational sense, by\namalgamating the experiences of multiple observers in the wormhole. In other\nwords the conjecture equates a quantity which is difficult to compute with one\nwhich is easy to compute. The pseudorandomness argument further implies that\nthis is a necessary feature of any resolution of the wormhole growth paradox,\nnot just of Susskind's Complexity=Volume conjecture. As a corollary we conclude\nthat either the AdS/CFT dictionary map must be exponentially complex, or the\nquantum Extended Church-Turing thesis must be false in quantum gravity.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:35:36 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Bouland", "Adam", ""], ["Fefferman", "Bill", ""], ["Vazirani", "Umesh", ""]]}]