[{"id": "2107.00072", "submitter": "David Schaller", "authors": "David Schaller, Marc Hellmuth, Peter F. Stadler", "title": "A Linear-Time Algorithm for the Common Refinement of Rooted Phylogenetic\n  Trees on a Common Leaf Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a common refinement of a set of rooted trees with\ncommon leaf set $L$ appears naturally in mathematical phylogenetics whenever\npoorly resolved information on the same taxa from different sources is to be\nreconciled. This constitutes a special case of the well-studied supertree\nproblem, where the leaf sets of the input trees may differ. Algorithms that\nsolve the rooted tree compatibility problem are of course applicable to this\nspecial case. However, they require sophisticated auxiliary data structures and\nhave a running time of at least $O(k|L|\\log^2(k|L|))$ for $k$ input trees.\nHere, we show that the problem can be solved in $O(k|L|)$ time using a simple\nbottom-up algorithm called LinCR. An implementation of LinCR in Python is\nfreely available at https://github.com/david-schaller/tralda.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:45:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Schaller", "David", ""], ["Hellmuth", "Marc", ""], ["Stadler", "Peter F.", ""]]}, {"id": "2107.00086", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Thomas Rubiano (LIPN), Neea Rusch, Thomas Seiller\n  (LIPN, CNRS)", "title": "An extended and more practical mwp flow analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve and refine a method for certifying that the values' sizes computed\nby an imperative program will be bounded by polynomials in the program's\ninputs' sizes. Our work ''tames'' the non-determinism of the original analysis,\nand offers an innovative way of completing the analysis when a non-polynomial\ngrowth is found. We furthermore enrich the analyzed language by adding function\ndefinitions and calls, allowing to compose the analysis of different libraries\nand offering generally more modularity. The implementation of our improved\nmethod, discussed in a tool paper\n(https://hal.archives-ouvertes.fr/hal-03269121), also required to reason about\nthe efficiency of some of the needed operations on the matrices produced by the\nanalysis. It is our hope that this work will enable and facilitate static\nanalysis of source code to guarantee its correctness with respect to resource\nusages.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:18:00 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:45:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LIPN"], ["Rubiano", "Thomas", "", "LIPN"], ["Rusch", "Neea", "", "LIPN, CNRS"], ["Seiller", "Thomas", "", "LIPN, CNRS"]]}, {"id": "2107.00097", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Thomas Rubiano (LIPN), Neea Rusch, Thomas Seiller\n  (LIPN, CNRS)", "title": "An implementation of flow calculus for complexity analysis (tool paper)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract. We present a tool to automatically perform the data-size analysis\nof imperative programs written in C. This tool, called pymwp, is inspired by a\nclassical work on complexity analysis [10], and allows to certify that the size\nof the values computed by a program will be bounded by a polynomial in the\nprogram's inputs. Strategies to provide meaningful feedback on non-polynomial\nprograms and to ``tame'' the non-determinism of the original analysis were\nimplemented following recent progresses [3], but required particular care to\naccommodate the growing complexity of the analysis. The Python source code is\nintensively documented, and our numerous example files encompass the original\nexamples as well as multiple test cases. A pip package should make it easy to\ninstall pymwp on any plat-form, but an on-line demo is also available for\nconvenience.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:19:32 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:44:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LIPN"], ["Rubiano", "Thomas", "", "LIPN"], ["Rusch", "Neea", "", "LIPN, CNRS"], ["Seiller", "Thomas", "", "LIPN, CNRS"]]}, {"id": "2107.00216", "submitter": "Chris Jones", "authors": "Chris Jones, Aaron Potechin", "title": "Almost-Orthogonal Bases for Inner Product Polynomials", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider low-degree polynomials of inner products between a\ncollection of random vectors. We give an almost orthogonal basis for this\nvector space of polynomials when the random vectors are Gaussian, spherical, or\nBoolean. In all three cases, our basis admits an interesting combinatorial\ndescription based on the topology of the underlying graph of inner products.\n  We also analyze the expected value of the product of two polynomials in our\nbasis. In all three cases, we show that this expected value can be expressed in\nterms of collections of matchings on the underlying graph of inner products. In\nthe Gaussian and Boolean cases, we show that this expected value is always\nnon-negative. In the spherical case, we show that this expected value can be\nnegative but we conjecture that if the underlying graph of inner products is\nplanar then this expected value will always be non-negative.\n  We hope that these polynomials will be a useful analytical tool in settings\nwhere one has a symmetric function of a collection of random or pseudorandom\nvectors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 04:55:58 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Jones", "Chris", ""], ["Potechin", "Aaron", ""]]}, {"id": "2107.00223", "submitter": "Kei Uchizawa Dr.", "authors": "Kei Uchizawa and Haruki Abe", "title": "Circuit Complexity of Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n  We first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:37:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Uchizawa", "Kei", ""], ["Abe", "Haruki", ""]]}, {"id": "2107.00314", "submitter": "Daan Van Den Berg", "authors": "Joeri Sleegers and Daan van den Berg", "title": "Backtracking (the) Algorithms on the Hamiltonian Cycle Problem", "comments": "Not yet peer-reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Even though the Hamiltonian cycle problem is NP-complete, many of its problem\ninstances aren't. In fact, almost all the hard instances reside in one area:\nnear the Koml\\'os-Szemer\\'edi bound, of $\\frac{1}{2}\\ v\\cdot ln(v) +\n\\frac{1}{2}\\ v\\cdot ln( ln(v))$ edges, where randomly generated graphs have an\napproximate 50\\% chance of being Hamiltonian. If the number of edges is either\nmuch higher or much lower, the problem is not hard -- most backtracking\nalgorithms decide such instances in (near) polynomial time. Recently however,\ntargeted search efforts have identified very hard Hamiltonian cycle problem\ninstances very far away from the Koml\\'os-Szemer\\'edi bound. In that study, the\nused backtracking algorithm was Vandegriend-Culberson's, which was supposedly\nthe most efficient of all Hamiltonian backtracking algorithms.\n  In this paper, we make a unified large scale quantitative comparison for the\nbest known backtracking algorithms described between 1877 and 2016. We confirm\nthe suspicion that the Koml\\'os-Szemer\\'edi bound is a hard area for all\nbacktracking algorithms, but also that Vandegriend-Culberson is indeed the most\nefficient algorithm, when expressed in consumed computing time. When measured\nin recursive effectiveness however, the algorithm by Frank Rubin, almost half a\ncentury old, performs best. In a more general algorithmic assessment, we\nconjecture that edge pruning and non-Hamiltonicity checks might be largely\nresponsible for these recursive savings. When expressed in system time however,\ndenser problem instances require much more time per recursion. This is most\nlikely due to the costliness of the extra search pruning procedures, which are\nrelatively elaborate. We supply large amounts of experimental data, and a\nunified single-program implementation for all six algorithms. All data and\nalgorithmic source code is made public for further use by our colleagues.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:07:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sleegers", "Joeri", ""], ["Berg", "Daan van den", ""]]}, {"id": "2107.00341", "submitter": "Gonzague Yernaux", "authors": "Gonzague Yernaux and Wim Vanhoof", "title": "Technical Report: Anti-unification of Unordered Goals", "comments": "Submitted to CSL 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anti-unification in logic programming refers to the process of capturing\ncommon syntactic structure among given goals, computing as such a single new\ngoal that is more general and hence called a generalization of the given goals.\nFinding an arbitrary common generalization for two goals is trivial, but\nlooking for those common generalizations that are either as large as possible\n(called largest common generalizations) or as specific as possible (called most\nspecific generalizations) is a non-trivial optimization problem, in particular\nwhen goals are considered to be unordered sets of atoms. In this work we\nprovide an in-depth study of the problem by defining two different\ngeneralization relations. We formulate a characterization of what constitutes a\nmost specific generalization in both settings. While these generalizations can\nbe computed in polynomial time, we show that when the number of variables in\nthe generalization needs to be minimized, the problem becomes NP-hard. We\nsubsequently revisit an abstraction of the largest common generalization when\nanti-unification is based on injective variable renamings, and prove that it\ncan be computed in polynomially bounded time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:11:07 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yernaux", "Gonzague", ""], ["Vanhoof", "Wim", ""]]}, {"id": "2107.00629", "submitter": "Radu Curticapean", "authors": "Radu Curticapean, Holger Dell, Thore Husfeldt", "title": "Modular counting of subgraphs: Matchings, matching-splittable graphs,\n  and paths", "comments": "23 pages, to appear at ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We systematically investigate the complexity of counting subgraph patterns\nmodulo fixed integers. For example, it is known that the parity of the number\nof $k$-matchings can be determined in polynomial time by a simple reduction to\nthe determinant. We generalize this to an $n^{f(t,s)}$-time algorithm to\ncompute modulo $2^t$ the number of subgraph occurrences of patterns that are\n$s$ vertices away from being matchings. This shows that the known\npolynomial-time cases of subgraph detection (Jansen and Marx, SODA 2015) carry\nover into the setting of counting modulo $2^t$.\n  Complementing our algorithm, we also give a simple and self-contained proof\nthat counting $k$-matchings modulo odd integers $q$ is Mod_q-W[1]-complete and\nprove that counting $k$-paths modulo $2$ is Parity-W[1]-complete, answering an\nopen question by Bj\\\"orklund, Dell, and Husfeldt (ICALP 2015).\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:38:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Curticapean", "Radu", ""], ["Dell", "Holger", ""], ["Husfeldt", "Thore", ""]]}, {"id": "2107.01133", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam, Norma Makarem and Maryam Shehab", "title": "An Improved Fixed-Parameter Algorithm for 2-Club Cluster Edge Deletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 2-club is a graph of diameter at most two. In the decision version of the\nparametrized {\\sc 2-Club Cluster Edge Deletion} problem, an undirected graph\n$G$ is given along with an integer $k\\geq 0$ as parameter, and the question is\nwhether $G$ can be transformed into a disjoint union of 2-clubs by deleting at\nmost $k$ edges. A simple fixed-parameter algorithm solves the problem in\n$\\mathcal{O}^*(3^k)$, and a decade-old algorithm was claimed to have an\nimproved running time of $\\mathcal{O}^*(2.74^k)$ via a sophisticated case\nanalysis. Unfortunately, this latter algorithm suffers from a flawed branching\nscenario. In this paper, an improved fixed-parameter algorithm is presented\nwith a running time in $\\mathcal{O}^*(2.695^k)$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:22:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Makarem", "Norma", ""], ["Shehab", "Maryam", ""]]}, {"id": "2107.01235", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "Linear Discrepancy is $\\Pi_2$-Hard to Approximate", "comments": "9 pages; to appear in Information Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we prove that the problem of computing the linear discrepancy\nof a given matrix is $\\Pi_2$-hard, even to approximate within $9/8 - \\epsilon$\nfactor for any $\\epsilon > 0$. This strengthens the NP-hardness result of Li\nand Nikolov [ESA 2020] for the exact version of the problem, and answers a\nquestion posed by them. Furthermore, since Li and Nikolov showed that the\nproblem is contained in $\\Pi_2$, our result makes linear discrepancy another\nnatural problem that is $\\Pi_2$-complete (to approximate).\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 18:40:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "2107.01335", "submitter": "Cyrus Rashtchian", "authors": "Cyrus Rashtchian, David P. Woodruff, Peng Ye, Hanlin Zhu", "title": "Average-Case Communication Complexity of Statistical Problems", "comments": "28 pages. Conference on Learning Theory (COLT), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical problems, such as planted clique, its variants, and\nsparse principal component analysis in the context of average-case\ncommunication complexity. Our motivation is to understand the\nstatistical-computational trade-offs in streaming, sketching, and query-based\nmodels. Communication complexity is the main tool for proving lower bounds in\nthese models, yet many prior results do not hold in an average-case setting. We\nprovide a general reduction method that preserves the input distribution for\nproblems involving a random graph or matrix with planted structure. Then, we\nderive two-party and multi-party communication lower bounds for detecting or\nfinding planted cliques, bipartite cliques, and related problems. As a\nconsequence, we obtain new bounds on the query complexity in the edge-probe,\nvector-matrix-vector, matrix-vector, linear sketching, and\n$\\mathbb{F}_2$-sketching models. Many of these results are nearly tight, and we\nuse our techniques to provide simple proofs of some known lower bounds for the\nedge-probe model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 03:31:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rashtchian", "Cyrus", ""], ["Woodruff", "David P.", ""], ["Ye", "Peng", ""], ["Zhu", "Hanlin", ""]]}, {"id": "2107.01428", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Peter Jonsson and Sebastian Ordyniak and\n  George Osipov", "title": "Solving Infinite-Domain CSPs Using the Patchwork Property", "comments": "34 pages, 2 figures. Parts of this article appeared in the\n  proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) has important applications in\ncomputer science and AI. In particular, infinite-domain CSPs have been\nintensively used in subareas of AI such as spatio-temporal reasoning. Since\nconstraint satisfaction is a computationally hard problem, much work has been\ndevoted to identifying restricted problems that are efficiently solvable. One\nway of doing this is to restrict the interactions of variables and constraints,\nand a highly successful approach is to bound the treewidth of the underlying\nprimal graph. Bodirsky & Dalmau [J. Comput. System. Sci. 79(1), 2013] and Huang\net al. [Artif. Intell. 195, 2013] proved that CSP$(\\Gamma)$ can be solved in\n$n^{f(w)}$ time (where $n$ is the size of the instance, $w$ is the treewidth of\nthe primal graph and $f$ is a computable function) for certain classes of\nconstraint languages $\\Gamma$. We improve this bound to $f(w) \\cdot n^{O(1)}$,\nwhere the function $f$ only depends on the language $\\Gamma$, for CSPs whose\nbasic relations have the patchwork property. Hence, such problems are\nfixed-parameter tractable and our algorithm is asymptotically faster than the\nprevious ones. Additionally, our approach is not restricted to binary\nconstraints, so it is applicable to a strictly larger class of problems than\nthat of Huang et al. However, there exist natural problems that are covered by\nBodirsky & Dalmau's algorithm but not by ours, and we begin investigating ways\nof generalising our results to larger families of languages. We also analyse\nour algorithm with respect to its running time and show that it is optimal\n(under the Exponential Time Hypothesis) for certain languages such as Allen's\nInterval Algebra.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 13:04:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Jonsson", "Peter", ""], ["Ordyniak", "Sebastian", ""], ["Osipov", "George", ""]]}, {"id": "2107.01520", "submitter": "Hengzhao Ma", "authors": "Hengzhao Ma, Jianzhong Li", "title": "The PCP-like Theorem for Sub-linear Time Inapproximability", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.02320", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the PCP-like theorem for sub-linear time\ninapproximability. Abboud et al. have devised the distributed PCP framework for\nproving sub-quadratic time inapproximability. Here we try to go further in this\ndirection. Staring from SETH, we first find a problem denoted as Ext-$k$-SAT,\nwhich can not be computed in linear time, then devise an efficient MA-like\nprotocol for this problem. To use this protocol to prove the sub-linear time\ninapproximability of other problems, we devise a new kind of reduction denoted\nas Ext-reduction, and it is different from existing reduction techniques. We\nalso define two new hardness class, the problems in which can be computed in\nlinear-time, but can not be efficiently approximated in sub-linear time. Some\nproblems are shown to be in the newly defined hardness class.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 01:39:23 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 13:21:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Hengzhao", ""], ["Li", "Jianzhong", ""]]}, {"id": "2107.01609", "submitter": "Hendrik Molter", "authors": "Hendrik Molter", "title": "The Complexity of Finding Temporal Separators under Waiting Time\n  Constraints", "comments": "This work is based on a previously unpublished chapter of the\n  author's PhD-thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the computational complexity of Restless\nTemporal $(s,z)$-Separation, where we are asked whether it is possible to\ndestroy all restless temporal paths between two distinct vertices $s$ and $z$\nby deleting at most $k$ vertices from a temporal graph. A temporal graph has a\nfixed vertex but the edges have (discrete) time stamps. A restless temporal\npath uses edges with non-decreasing time stamps and the time spent at each\nvertex must not exceed a given duration $\\Delta$.\n  Restless Temporal $(s,z)$-Separation naturally generalizes the NP-hard\nTemporal $(s,z)$-Separation problem. We show that Restless Temporal\n$(s,z)$-Separation is complete for $\\Sigma_2^\\text{P}$, a complexity class\nlocated in the second level of the polynomial time hierarchy. We further\nprovide some insights in the parameterized complexity of Restless Temporal\n$(s,z)$-Separation parameterized by the separator size $k$.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 12:52:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Molter", "Hendrik", ""]]}, {"id": "2107.01721", "submitter": "Alejandro Cassis", "authors": "Karl Bringmann, Alejandro Cassis, Nick Fischer, Marvin K\\\"unnemann", "title": "Fine-Grained Completeness for Optimization in P", "comments": "Full version of APPROX'21 paper, abstract shortened to fit ArXiv\n  requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of fine-grained completeness theorems for exact and\napproximate optimization in the polynomial-time regime. Inspired by the first\ncompleteness results for decision problems in P (Gao, Impagliazzo, Kolokolova,\nWilliams, TALG 2019) as well as the classic class MaxSNP and\nMaxSNP-completeness for NP optimization problems (Papadimitriou, Yannakakis,\nJCSS 1991), we define polynomial-time analogues MaxSP and MinSP, which contain\na number of natural optimization problems in P, including Maximum Inner\nProduct, general forms of nearest neighbor search and optimization variants of\nthe $k$-XOR problem. Specifically, we define MaxSP as the class of problems\ndefinable as $\\max_{x_1,\\dots,x_k} \\#\\{ (y_1,\\dots,y_\\ell) :\n\\phi(x_1,\\dots,x_k, y_1,\\dots,y_\\ell) \\}$, where $\\phi$ is a quantifier-free\nfirst-order property over a given relational structure (with MinSP defined\nanalogously). On $m$-sized structures, we can solve each such problem in time\n$O(m^{k+\\ell-1})$. Our results are:\n  - We determine (a sparse variant of) the Maximum/Minimum Inner Product\nproblem as complete under *deterministic* fine-grained reductions: A strongly\nsubquadratic algorithm for Maximum/Minimum Inner Product would beat the\nbaseline running time of $O(m^{k+\\ell-1})$ for *all* problems in MaxSP/MinSP by\na polynomial factor.\n  - This completeness transfers to approximation: Maximum/Minimum Inner Product\nis also complete in the sense that a strongly subquadratic $c$-approximation\nwould give a $(c+\\varepsilon)$-approximation for all MaxSP/MinSP problems in\ntime $O(m^{k+\\ell-1-\\delta})$, where $\\varepsilon > 0$ can be chosen\narbitrarily small. Combining our completeness with~(Chen, Williams, SODA 2019),\nwe obtain the perhaps surprising consequence that refuting the OV Hypothesis is\n*equivalent* to giving a $O(1)$-approximation for all MinSP problems in\nfaster-than-$O(m^{k+\\ell-1})$ time.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 20:08:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bringmann", "Karl", ""], ["Cassis", "Alejandro", ""], ["Fischer", "Nick", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "2107.01778", "submitter": "Yuni Iwamasa", "authors": "Soichiro Fujii and Yuni Iwamasa and Kei Kimura", "title": "Quantaloidal approach to constraint satisfaction", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) is a computational problem that\nincludes a range of important problems in computer science. We point out that\nfundamental concepts of the CSP, such as the solution set of an instance and\npolymorphisms, can be formulated abstractly inside the 2-category\n$\\mathcal{P}\\mathbf{FinSet}$ of finite sets and sets of functions between them.\nThe 2-category $\\mathcal{P}\\mathbf{FinSet}$ is a quantaloid, and the\nformulation relies mainly on structure available in any quantaloid. This\nobservation suggests a formal development of generalisations of the CSP and\nconcomitant notions of polymorphism in a large class of quantaloids. We extract\na class of optimisation problems as a special case, and show that their\ncomputational complexity can be classified by the associated notion of\npolymorphism.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 04:00:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Fujii", "Soichiro", ""], ["Iwamasa", "Yuni", ""], ["Kimura", "Kei", ""]]}, {"id": "2107.02027", "submitter": "Mario Michael Krell", "authors": "Matej Kosec and Sheng Fu and Mario Michael Krell", "title": "Packing: Towards 2x NLP BERT Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We find that at sequence length 512 padding tokens represent in excess of 50%\nof the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder\nRepresentations from Transformers). Therefore by removing all padding we\nachieve a 2x speed-up in terms of sequences/sec. To exploit this characteristic\nof the dataset, we develop and contrast two deterministic packing algorithms.\nBoth algorithms rely on the assumption that sequences are interchangeable and\ntherefore packing can be performed on the histogram of sequence lengths, rather\nthan per sample. This transformation of the problem leads to algorithms which\nare fast and have linear complexity in dataset size. The shortest-pack-first\nhistogram-packing (SPFHP) algorithm determines the packing order for the\nWikipedia dataset of over 16M sequences in 0.02 seconds. The non-negative\nleast-squares histogram-packing (NNLSHP) algorithm converges in 28.4 seconds\nbut produces solutions which are more depth efficient, managing to get near\noptimal packing by combining a maximum of 3 sequences in one sample. Using the\ndataset with multiple sequences per sample requires additional masking in the\nattention layer and a modification of the MLM loss function. We demonstrate\nthat both of these changes are straightforward to implement and have relatively\nlittle impact on the achievable performance gain on modern hardware. Finally,\nwe pretrain BERT-Large using the packed dataset, demonstrating no loss of\nconvergence and the desired 2x speed-up.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:37:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kosec", "Matej", ""], ["Fu", "Sheng", ""], ["Krell", "Mario Michael", ""]]}, {"id": "2107.02060", "submitter": "Eike Neumann", "authors": "Julian D'Costa, Engel Lefaucheux, Eike Neumann, Jo\\\"el Ouaknine, James\n  Worrell", "title": "On the Complexity of the Escape Problem for Linear Dynamical Systems\n  over Compact Semialgebraic Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of the Escape Problem for discrete-time\nlinear dynamical systems over compact semialgebraic sets, or equivalently the\nTermination Problem for affine loops with compact semialgebraic guard sets.\nConsider the fragment of the theory of the reals consisting of negation-free\n$\\exists \\forall$-sentences without strict inequalities. We derive several\nequivalent characterisations of the associated complexity class which\ndemonstrate its robustness and illustrate its expressive power. We show that\nthe Compact Escape Problem is complete for this class.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:45:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["D'Costa", "Julian", ""], ["Lefaucheux", "Engel", ""], ["Neumann", "Eike", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""]]}, {"id": "2107.02139", "submitter": "Lin Chen", "authors": "Lin Chen, Hossein Esfandiari, Gang Fu, Vahab S. Mirrokni, Qian Yu", "title": "Feature Cross Search via Submodular Optimization", "comments": "Accepted to ESA 2021. Authors are ordered alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study feature cross search as a fundamental primitive in\nfeature engineering. The importance of feature cross search especially for the\nlinear model has been known for a while, with well-known textbook examples. In\nthis problem, the goal is to select a small subset of features, combine them to\nform a new feature (called the crossed feature) by considering their Cartesian\nproduct, and find feature crosses to learn an \\emph{accurate} model. In\nparticular, we study the problem of maximizing a normalized Area Under the\nCurve (AUC) of the linear model trained on the crossed feature column.\n  First, we show that it is not possible to provide an $n^{1/\\log\\log\nn}$-approximation algorithm for this problem unless the exponential time\nhypothesis fails. This result also rules out the possibility of solving this\nproblem in polynomial time unless $\\mathsf{P}=\\mathsf{NP}$. On the positive\nside, by assuming the \\naive\\ assumption, we show that there exists a simple\ngreedy $(1-1/e)$-approximation algorithm for this problem. This result is\nestablished by relating the AUC to the total variation of the commutator of two\nprobability measures and showing that the total variation of the commutator is\nmonotone and submodular. To show this, we relate the submodularity of this\nfunction to the positive semi-definiteness of a corresponding kernel matrix.\nThen, we use Bochner's theorem to prove the positive semi-definiteness by\nshowing that its inverse Fourier transform is non-negative everywhere. Our\ntechniques and structural results might be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:58:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chen", "Lin", ""], ["Esfandiari", "Hossein", ""], ["Fu", "Gang", ""], ["Mirrokni", "Vahab S.", ""], ["Yu", "Qian", ""]]}, {"id": "2107.02239", "submitter": "Pranav Jeevan P", "authors": "Pranav Jeevan, Amit Sethi (Indian Institute of Technology Bombay)", "title": "Vision Xformers: Efficient Attention for Image Classification", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear attention mechanisms provide hope for overcoming the bottleneck of\nquadratic complexity which restricts application of transformer models in\nvision tasks. We modify the ViT architecture to work on longer sequence data by\nreplacing the quadratic attention with efficient transformers like Performer,\nLinformer and Nystr\\\"omformer of linear complexity creating Vision X-formers\n(ViX). We show that ViX performs better than ViT in image classification\nconsuming lesser computing resources. We further show that replacing the\nembedding linear layer by convolutional layers in ViX further increases their\nperformance. Our test on recent visions transformer models like LeViT and\nCompact Convolutional Transformer (CCT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nperformance. Incorporating these changes can democratize transformers by making\nthem accessible to those with limited data and computing resources.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 19:24:23 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jeevan", "Pranav", "", "Indian Institute of Technology Bombay"], ["Sethi", "Amit", "", "Indian Institute of Technology Bombay"]]}, {"id": "2107.02320", "submitter": "Sumegha Garg", "authors": "Sumegha Garg, Pravesh K. Kothari, Pengda Liu and Ran Raz", "title": "Memory-Sample Lower Bounds for Learning Parity with Noise", "comments": "19 pages. To appear in RANDOM 2021. arXiv admin note: substantial\n  text overlap with arXiv:1708.02639", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show, for the well-studied problem of learning parity under\nnoise, where a learner tries to learn $x=(x_1,\\ldots,x_n) \\in \\{0,1\\}^n$ from a\nstream of random linear equations over $\\mathrm{F}_2$ that are correct with\nprobability $\\frac{1}{2}+\\varepsilon$ and flipped with probability\n$\\frac{1}{2}-\\varepsilon$, that any learning algorithm requires either a memory\nof size $\\Omega(n^2/\\varepsilon)$ or an exponential number of samples.\n  In fact, we study memory-sample lower bounds for a large class of learning\nproblems, as characterized by [GRT'18], when the samples are noisy. A matrix\n$M: A \\times X \\rightarrow \\{-1,1\\}$ corresponds to the following learning\nproblem with error parameter $\\varepsilon$: an unknown element $x \\in X$ is\nchosen uniformly at random. A learner tries to learn $x$ from a stream of\nsamples, $(a_1, b_1), (a_2, b_2) \\ldots$, where for every $i$, $a_i \\in A$ is\nchosen uniformly at random and $b_i = M(a_i,x)$ with probability\n$1/2+\\varepsilon$ and $b_i = -M(a_i,x)$ with probability $1/2-\\varepsilon$\n($0<\\varepsilon< \\frac{1}{2}$). Assume that $k,\\ell, r$ are such that any\nsubmatrix of $M$ of at least $2^{-k} \\cdot |A|$ rows and at least $2^{-\\ell}\n\\cdot |X|$ columns, has a bias of at most $2^{-r}$. We show that any learning\nalgorithm for the learning problem corresponding to $M$, with error, requires\neither a memory of size at least $\\Omega\\left(\\frac{k \\cdot \\ell}{\\varepsilon}\n\\right)$, or at least $2^{\\Omega(r)}$ samples. In particular, this shows that\nfor a large class of learning problems, same as those in [GRT'18], any learning\nalgorithm requires either a memory of size at least $\\Omega\\left(\\frac{(\\log\n|X|) \\cdot (\\log |A|)}{\\varepsilon}\\right)$ or an exponential number of noisy\nsamples.\n  Our proof is based on adapting the arguments in [Raz'17,GRT'18] to the noisy\ncase.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 23:34:39 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Garg", "Sumegha", ""], ["Kothari", "Pravesh K.", ""], ["Liu", "Pengda", ""], ["Raz", "Ran", ""]]}, {"id": "2107.02554", "submitter": "Micha{\\l} W{\\l}odarczyk", "authors": "Bart M. P. Jansen, Shivesh K. Roy, Micha{\\l} W{\\l}odarczyk", "title": "On the Hardness of Compressing Weights", "comments": "To appear at MFCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate computational problems involving large weights through the\nlens of kernelization, which is a framework of polynomial-time preprocessing\naimed at compressing the instance size. Our main focus is the weighted Clique\nproblem, where we are given an edge-weighted graph and the goal is to detect a\nclique of total weight equal to a prescribed value. We show that the weighted\nvariant, parameterized by the number of vertices $n$, is significantly harder\nthan the unweighted problem by presenting an $O(n^{3 - \\varepsilon})$ lower\nbound on the size of the kernel, under the assumption that NP $\\not \\subseteq$\ncoNP/poly. This lower bound is essentially tight: we show that we can reduce\nthe problem to the case with weights bounded by $2^{O(n)}$, which yields a\nrandomized kernel of $O(n^3)$ bits.\n  We generalize these results to the weighted $d$-Uniform Hyperclique problem,\nSubset Sum, and weighted variants of Boolean Constraint Satisfaction Problems\n(CSPs). We also study weighted minimization problems and show that weight\ncompression is easier when we only want to preserve the collection of optimal\nsolutions. Namely, we show that for node-weighted Vertex Cover on bipartite\ngraphs it is possible to maintain the set of optimal solutions using integer\nweights from the range $[1, n]$, but if we want to maintain the ordering of the\nweights of all inclusion-minimal solutions, then weights as large as\n$2^{\\Omega(n)}$ are necessary.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:52:51 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Roy", "Shivesh K.", ""], ["W\u0142odarczyk", "Micha\u0142", ""]]}, {"id": "2107.02617", "submitter": "Pavel Hub\\'a\\v{c}ek", "authors": "Pavel Hub\\'a\\v{c}ek and Jan V\\'aclavek", "title": "On Search Complexity of Discrete Logarithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the discrete logarithm problem in the context of TFNP\n- the complexity class of search problems with a syntactically guaranteed\nexistence of a solution for all instances. Our main results establish that\nsuitable variants of the discrete logarithm problem are complete for the\ncomplexity class PPP, respectively PWPP, i.e., the subclasses of TFNP capturing\ntotal search problems with a solution guaranteed by the pigeonhole principle,\nrespectively the weak pigeonhole principle. Besides answering an open problem\nfrom the recent work of Sotiraki, Zampetakis, and Zirdelis (FOCS'18), our\ncompleteness results for PPP and PWPP have implications for the recent line of\nwork proving conditional lower bounds for problems in TFNP under cryptographic\nassumptions. In particular, they highlight that any attempt at basing\naverage-case hardness in subclasses of TFNP (other than PWPP and PPP) on the\naverage-case hardness of the discrete logarithm problem must exploit its\nstructural properties beyond what is necessary for constructions of\ncollision-resistant hash functions.\n  Additionally, our reductions provide new structural insights into the class\nPWPP by establishing two new PWPP-complete problems. First, the problem DOVE, a\nrelaxation of the PPP-complete problem PIGEON. DOVE is the first PWPP-complete\nproblem not defined in terms of an explicitly shrinking function. Second, the\nproblem CLAW, a total search problem capturing the computational complexity of\nbreaking claw-free permutations. In the context of TFNP, the PWPP-completeness\nof CLAW matches the known intrinsic relationship between collision-resistant\nhash functions and claw-free permutations established in the cryptographic\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:49:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hub\u00e1\u010dek", "Pavel", ""], ["V\u00e1clavek", "Jan", ""]]}, {"id": "2107.02748", "submitter": "Shyan Akmal", "authors": "Shyan Akmal and Ryan Williams", "title": "MAJORITY-3SAT (and Related Problems) in Polynomial Time", "comments": "Abstract shortened to fit arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Majority-SAT is the problem of determining whether an input $n$-variable\nformula in conjunctive normal form (CNF) has at least $2^{n-1}$ satisfying\nassignments. Majority-SAT and related problems have been studied extensively in\nvarious AI communities interested in the complexity of probabilistic planning\nand inference. Although Majority-SAT has been known to be PP-complete for over\n40 years, the complexity of a natural variant has remained open:\nMajority-$k$SAT, where the input CNF formula is restricted to have clause width\nat most $k$.\n  We prove that for every $k$, Majority-$k$SAT is in P. In fact, for any\npositive integer $k$ and rational $\\rho \\in (0,1)$ with bounded denominator, we\ngive an algorithm that can determine whether a given $k$-CNF has at least $\\rho\n\\cdot 2^n$ satisfying assignments, in deterministic linear time (whereas the\nprevious best-known algorithm ran in exponential time). Our algorithms have\ninteresting positive implications for counting complexity and the complexity of\ninference, significantly reducing the known complexities of related problems\nsuch as E-MAJ-$k$SAT and MAJ-MAJ-$k$SAT. At the heart of our approach is an\nefficient method for solving threshold counting problems by extracting\nsunflowers found in the corresponding set system of a $k$-CNF.\n  We also show that the tractability of Majority-$k$SAT is somewhat fragile.\nFor the closely related GtMajority-SAT problem (where we ask whether a given\nformula has greater than $2^{n-1}$ satisfying assignments) which is known to be\nPP-complete, we show that GtMajority-$k$SAT is in P for $k\\le 3$, but becomes\nNP-complete for $k\\geq 4$. These results are counterintuitive, because the\n``natural'' classifications of these problems would have been PP-completeness,\nand because there is a stark difference in the complexity of GtMajority-$k$SAT\nand Majority-$k$SAT for all $k\\ge 4$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:24:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Akmal", "Shyan", ""], ["Williams", "Ryan", ""]]}, {"id": "2107.02882", "submitter": "\\'Edouard Bonnet", "authors": "\\'Edouard Bonnet, Eun Jung Kim, Amadeus Reinald, St\\'ephan Thomass\\'e,\n  R\\'emi Watrigant", "title": "Twin-width and polynomial kernels", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence of polynomial kernels, for parameterized problems\nwithout a polynomial kernel on general graphs, when restricted to graphs of\nbounded twin-width. Our main result is that a polynomial kernel for\n$k$-Dominating Set on graphs of twin-width at most 4 would contradict a\nstandard complexity-theoretic assumption. The reduction is quite involved,\nespecially to get the twin-width upper bound down to 4, and can be tweaked to\nwork for Connected $k$-Dominating Set and Total $k$-Dominating Set (albeit with\na worse upper bound on the twin-width). The $k$-Independent Set problem admits\nthe same lower bound by a much simpler argument, previously observed [ICALP\n'21], which extends to $k$-Independent Dominating Set, $k$-Path, $k$-Induced\nPath, $k$-Induced Matching, etc. On the positive side, we obtain a simple\nquadratic vertex kernel for Connected $k$-Vertex Cover and Capacitated\n$k$-Vertex Cover on graphs of bounded twin-width. Interestingly the kernel\napplies to graphs of Vapnik-Chervonenkis density 1, and does not require a\nwitness sequence. We also present a more intricate $O(k^{1.5})$ vertex kernel\nfor Connected $k$-Vertex Cover. Finally we show that deciding if a graph has\ntwin-width at most 1 can be done in polynomial time, and observe that most\noptimization/decision graph problems can be solved in polynomial time on graphs\nof twin-width at most 1.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:46:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bonnet", "\u00c9douard", ""], ["Kim", "Eun Jung", ""], ["Reinald", "Amadeus", ""], ["Thomass\u00e9", "St\u00e9phan", ""], ["Watrigant", "R\u00e9mi", ""]]}, {"id": "2107.02922", "submitter": "Pooya Nikbakht", "authors": "Shahin Kamali, Pooya Nikbakht", "title": "On the Fault-Tolerant Online Bin Packing Problem", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fault-tolerant variant of the online bin packing problem.\nSimilar to the classic bin packing problem, an online sequence of items of\nvarious sizes should be packed into a minimum number of bins of uniform\ncapacity. For applications such as server consolidation, where bins represent\nservers and items represent jobs of different loads, it is required to maintain\nfault-tolerant solutions. In a fault-tolerant packing, any job is replicated\ninto f+1 servers, for some integer f > 1, so that the failure of up to f\nservers does not interrupt service. We build over a practical model introduced\nby Li and Tang [SPAA 2017] in which each job of load $x$ has a primary replica\nof load $x$ and $f$ standby replicas, each of load $x/\\eta$, where $\\eta >1$ is\na parameter of the problem. Upon failure of up to $f$ servers, any primary\nreplica in a failed bin should be replaced by one of its standby replicas so\nthat the extra load of the new primary replica does not cause an overflow in\nits bin. We study a general setting in which bins might fail while the input is\nstill being revealed. Our main contribution is an algorithm, named\nHarmonic-Stretch, which maintains fault-tolerant packings under this general\nsetting. We prove that Harmonic-Stretch has an asymptotic competitive ratio of\nat most 1.75. This is an improvement over the best existing asymptotic\ncompetitive ratio 2 of an algorithm by Li and Tang [TPDS 2020], which works\nunder a model in which bins fail only after all items are packed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 22:09:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kamali", "Shahin", ""], ["Nikbakht", "Pooya", ""]]}, {"id": "2107.02987", "submitter": "Zekun Ye", "authors": "Zekun Ye, Lvzhou Li", "title": "Sample complexity of hidden subgroup problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hidden subgroup problem ($\\mathsf{HSP}$) has been attracting much\nattention in quantum computing, since several well-known quantum algorithms\nincluding Shor algorithm can be described in a uniform framework as quantum\nmethods to address different instances of it. One of the central issues about\n$\\mathsf{HSP}$ is to characterize its quantum/classical complexity. For\nexample, from the viewpoint of learning theory, sample complexity is a crucial\nconcept. However, while the quantum sample complexity of the problem has been\nstudied, a full characterization of the classical sample complexity of\n$\\mathsf{HSP}$ seems to be absent, which will thus be the topic in this paper.\n$\\mathsf{HSP}$ over a finite group is defined as follows: For a finite group\n$G$ and a finite set $V$, given a function $f:G \\to V$ and the promise that for\nany $x, y \\in G, f(x) = f(xy)$ iff $y \\in H$ for a subgroup $H \\in\n\\mathcal{H}$, where $\\mathcal{H}$ is a set of candidate subgroups of $G$, the\ngoal is to identify $H$.\n  Our contributions are as follows: For $\\mathsf{HSP}$, we give the upper and\nlower bounds on the sample complexity of $\\mathsf{HSP}$. Furthermore, we have\napplied the result to obtain the sample complexity of some concrete instances\nof hidden subgroup problem. Particularly, we discuss generalized Simon's\nproblem ($\\mathsf{GSP}$), a special case of $\\mathsf{HSP}$, and show that the\nsample complexity of $\\mathsf{GSP}$ is $\\Theta\\left(\\max\\left\\{k,\\sqrt{k\\cdot\np^{n-k}}\\right\\}\\right)$. Thus we obtain a complete characterization of the\nsample complexity of $\\mathsf{GSP}$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 02:54:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ye", "Zekun", ""], ["Li", "Lvzhou", ""]]}, {"id": "2107.03171", "submitter": "S Venkitesh", "authors": "Srikanth Srinivasan, S. Venkitesh", "title": "On the Probabilistic Degree of an $n$-variate Boolean Function", "comments": "To appear in the conference RANDOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nisan and Szegedy (CC 1994) showed that any Boolean function\n$f:\\{0,1\\}^n\\rightarrow \\{0,1\\}$ that depends on all its input variables, when\nrepresented as a real-valued multivariate polynomial $P(x_1,\\ldots,x_n)$, has\ndegree at least $\\log n - O(\\log \\log n)$. This was improved to a tight $(\\log\nn - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar\nstatements are also known for other Boolean function complexity measures such\nas Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate\ndegree (Ambainis and de Wolf (CC 2014)).\n  In this paper, we address this question for \\emph{Probabilistic degree}. The\nfunction $f$ has probabilistic degree at most $d$ if there is a random\nreal-valued polynomial of degree at most $d$ that agrees with $f$ at each input\nwith high probability. Our understanding of this complexity measure is\nsignificantly weaker than those above: for instance, we do not even know the\nprobabilistic degree of the OR function, the best-known bounds put it between\n$(\\log n)^{1/2-o(1)}$ and $O(\\log n)$ (Beigel, Reingold, Spielman (STOC 1991);\nTarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).\n  Here we can give a near-optimal understanding of the probabilistic degree of\n$n$-variate functions $f$, \\emph{modulo} our lack of understanding of the\nprobabilistic degree of OR. We show that if the probabilistic degree of OR is\n$(\\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is\nat least $(\\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\\log\nn)^{o(1)}$ factors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:03:02 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Srinivasan", "Srikanth", ""], ["Venkitesh", "S.", ""]]}, {"id": "2107.03569", "submitter": "Umang Mathur", "authors": "Rucha Kulkarni and Umang Mathur and Andreas Pavlogiannis", "title": "Dynamic Data-Race Detection through the Fine-Grained Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data races are among the most common bugs in concurrency. The standard\napproach to data-race detection is via dynamic analyses, which work over\nexecutions of concurrent programs, instead of the program source code. The rich\nliterature on the topic has created various notions of dynamic data races,\nwhich are known to be detected efficiently when certain parameters (e.g.,\nnumber of threads) are small. However, the \\emph{fine-grained} complexity of\nall these notions of races has remained elusive, making it impossible to\ncharacterize their trade-offs between precision and efficiency.\n  In this work we establish several fine-grained separations between many\npopular notions of dynamic data races. The input is an execution trace with $N$\nevents, $T$ threads and $L$ locks. Our main results are as follows. First, we\nshow that happens-before (HB) races can be detected in $O(N\\cdot \\min(T, L))$\ntime, improving over the standard $O(N\\cdot T)$ bound when $L=o(T)$. Moreover,\nwe show that even reporting an HB race that involves a read access is hard for\n2-orthogonal vectors (2-OV). This is the first rigorous proof of the\nconjectured quadratic lower-bound in detecting HB races. Second, we show that\nthe recently introduced synchronization-preserving races are hard to detect for\nOV-3 and thus have a cubic lower bound, when $T=\\Omega(N)$. This establishes a\ncomplexity separation from HB races which are known to be less expressive.\nThird, we show that lock-cover races are hard for 2-OV, and thus have a\nquadratic lower-bound, even when $T=2$ and $L = \\omega(\\log N)$. The similar\nnotion of lock-set races is known to be detectable in $O(N\\cdot L)$ time, and\nthus we achieve a complexity separation between the two. Moreover, we show that\nlock-set races become hitting-set (HS)-hard when $L=\\Theta(N)$, and thus also\nhave a quadratic lower bound, when the input is sufficiently complex.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 02:21:31 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Kulkarni", "Rucha", ""], ["Mathur", "Umang", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "2107.03778", "submitter": "Benedikt Pago", "authors": "Benedikt Pago", "title": "Choiceless Polynomial Time, Symmetric Circuits and Cai-F\\\"urer-Immerman\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Choiceless Polynomial Time (CPT) is currently the only candidate logic for\ncapturing PTIME (that is, it is contained in PTIME and has not been separated\nfrom it). A prominent example of a decision problem in PTIME that is not known\nto be CPT-definable is the isomorphism problem on unordered\nCai-F\\\"urer-Immerman graphs (the CFI-query). We study the expressive power of\nCPT with respect to this problem and develop a partial characterisation of\nsolvable instances in terms of properties of symmetric XOR-circuits over the\nCFI-graphs: The CFI-query is CPT-definable on a given class of graphs only if:\nFor each graph $G$, there exists an XOR-circuit $C$, whose input gates are\nlabelled with edges of $G$, such that $C$ is sufficiently symmetric with\nrespect to the automorphisms of $G$ and satisfies certain other circuit\nproperties. We also give a sufficient condition for CFI being solvable in CPT\nand develop a new CPT-algorithm for the CFI-query. It takes as input structures\nwhich contain, along with the CFI-graph, an XOR-circuit with suitable\nproperties. The strongest known CPT-algorithm for this problem can solve\ninstances equipped with a preorder with colour classes of logarithmic size. Our\nresult implicitly extends this to preorders with colour classes of\npolylogarithmic size (plus some unordered additional structure). Finally, our\nwork provides new insights regarding a much more general problem: The existence\nof a solution to an unordered linear equation system $A \\cdot x = b$ over a\nfinite field is CPT-definable if the matrix $A$ has at most logarithmic rank\n(with respect to the size of the structure that encodes the equation system).\nThis is another example that separates CPT from fixed-point logic with\ncounting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:41:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pago", "Benedikt", ""]]}, {"id": "2107.03885", "submitter": "Boaz Menuhin", "authors": "Boaz Menuhin and Moni Naor", "title": "Keep That Card in Mind: Card Guessing with Limited Memory", "comments": "53 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A card guessing game is played between two players, Guesser and Dealer. At\nthe beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1,\n..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser\nguesses which card was drawn, and then the card is discarded from the deck. The\nGuesser receives a point for each correctly guessed card. With perfect memory,\na Guesser can keep track of all cards that were played so far and pick at\nrandom a card that has not appeared so far, yielding in expectation $\\ln n$\ncorrect guesses. With no memory, the best a Guesser can do will result in a\nsingle guess in expectation. We consider the case of a memory bounded Guesser\nthat has $m < n$ memory bits. We show that the performance of such a memory\nbounded Guesser depends much on the behavior of the Dealer. In more detail, we\nshow that there is a gap between the static case, where the Dealer draws cards\nfrom a properly shuffled deck or a prearranged one, and the adaptive case,\nwhere the Dealer draws cards thoughtfully, in an adversarial manner.\nSpecifically:\n  1. We show a Guesser with $O(\\log^2 n)$ memory bits that scores a near\noptimal result against any static Dealer.\n  2. We show that no Guesser with $m$ bits of memory can score better than\n$O(\\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\\min\n\\{\\sqrt{m}, \\ln n\\}$, i.e., the above Guesser is optimal.\n  3. We show an efficient adaptive Dealer against which no Guesser with $m$\nmemory bits can make more than $\\ln m + 2 \\ln \\log n + O(1)$ correct guesses in\nexpectation.\n  These results are (almost) tight, and we prove them using compression\narguments that harness the guessing strategy for encoding.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:09:20 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Menuhin", "Boaz", ""], ["Naor", "Moni", ""]]}, {"id": "2107.04100", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Aaron Potechin, Elias Samuel Wirth", "title": "SoS certification for symmetric quadratic functions and its connection\n  to constrained Boolean hypercube optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the rank of the Sum of Squares (SoS) hierarchy over the Boolean\nhypercube for Symmetric Quadratic Functions (SQFs) in $n$ variables with roots\nplaced in points $k-1$ and $k$. Functions of this type have played a central\nrole in deepening the understanding of the performance of the SoS method for\nvarious unconstrained Boolean hypercube optimization problems, including the\nMax Cut problem. Recently, Lee, Prakash, de Wolf, and Yuen proved a lower bound\non the SoS rank for SQFs of $\\Omega(\\sqrt{k(n-k)})$ and conjectured the lower\nbound of $\\Omega(n)$ by similarity to a polynomial representation of the\n$n$-bit OR function.\n  Using Chebyshev polynomials, we refute the Lee -- Prakash -- de~Wolf -- Yuen\nconjecture and prove that the SoS rank for SQFs is at most\n$O(\\sqrt{nk}\\log(n))$.\n  We connect this result to two constrained Boolean hypercube optimization\nproblems. First, we provide a degree $O( \\sqrt{n})$ SoS certificate that\nmatches the known SoS rank lower bound for an instance of Min Knapsack, a\nproblem that was intensively studied in the literature. Second, we study an\ninstance of the Set Cover problem for which Bienstock and Zuckerberg\nconjectured an SoS rank lower bound of $n/4$. We refute the Bienstock --\nZuckerberg conjecture and provide a degree $O(\\sqrt{n}\\log(n))$ SoS certificate\nfor this problem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 20:38:25 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kurpisz", "Adam", ""], ["Potechin", "Aaron", ""], ["Wirth", "Elias Samuel", ""]]}, {"id": "2107.04300", "submitter": "Kristoffer Arnsfelt Hansen", "authors": "Kristoffer Arnsfelt Hansen and Troels Bjerre Lund", "title": "Computational Complexity of Computing a Quasi-Proper Equilibrium", "comments": "Full version of paper to appear at the 23rd International Symposium\n  on Fundamentals of Computation Theory (FCT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of computing or approximating a\nquasi-proper equilibrium for a given finite extensive form game of perfect\nrecall. We show that the task of computing a symbolic quasi-proper equilibrium\nis $\\mathrm{PPAD}$-complete for two-player games. For the case of zero-sum\ngames we obtain a polynomial time algorithm based on Linear Programming. For\ngeneral $n$-player games we show that computing an approximation of a\nquasi-proper equilibrium is $\\mathrm{FIXP}_a$-complete.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:22:06 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hansen", "Kristoffer Arnsfelt", ""], ["Lund", "Troels Bjerre", ""]]}, {"id": "2107.04321", "submitter": "Pascal Kunz", "authors": "Pascal Kunz, Till Fluschnik, Rolf Niedermeier, Malte Renken", "title": "Most Classic Problems Remain NP-hard on Relative Neighborhood Graphs and\n  their Relatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximity graphs have been studied for several decades, motivated by\napplications in computational geometry, geography, data mining, and many other\nfields. However, the computational complexity of classic graph problems on\nproximity graphs mostly remained open. We now study 3-Colorability, Dominating\nSet, Feedback Vertex Set, Hamiltonian Cycle, and Independent Set on the\nproximity graph classes relative neighborhood graphs, Gabriel graphs, and\nrelatively closest graphs. We prove that all of the problems remain NP-hard on\nthese graphs, except for 3-Colorability and Hamiltonian Cycle on relatively\nclosest graphs, where the former is trivial and the latter is left open.\nMoreover, for every NP-hard case we additionally show that no\n$2^{o(n^{1/4})}$-time algorithm exists unless the ETH fails, where n denotes\nthe number of vertices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:20:30 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kunz", "Pascal", ""], ["Fluschnik", "Till", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""]]}, {"id": "2107.04547", "submitter": "Sravanthi Chede", "authors": "Sravanthi Chede, Anil Shukla", "title": "Does QRAT simulate IR-calc? QRAT simulation algorithm for\n  $\\forall$Exp+Res cannot be lifted to IR-calc", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the QRAT simulation algorithm of $\\forall$Exp+Res from [B. Kiesl\nand M. Seidl, 2019] cannot be lifted to IR-calc.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:03:13 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chede", "Sravanthi", ""], ["Shukla", "Anil", ""]]}, {"id": "2107.04706", "submitter": "Ryan Williams", "authors": "Brynmor Chapman and Ryan Williams", "title": "Smaller ACC0 Circuits for Symmetric Functions", "comments": "15 pages; abstract edited to fit arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  What is the power of constant-depth circuits with $MOD_m$ gates, that can\ncount modulo $m$? Can they efficiently compute MAJORITY and other symmetric\nfunctions? When $m$ is a constant prime power, the answer is well understood:\nRazborov and Smolensky proved in the 1980s that MAJORITY and $MOD_m$ require\nsuper-polynomial-size $MOD_q$ circuits, where $q$ is any prime power not\ndividing $m$. However, relatively little is known about the power of $MOD_m$\ncircuits for non-prime-power $m$. For example, it is still open whether every\nproblem in $EXP$ can be computed by depth-$3$ circuits of polynomial size and\nonly $MOD_6$ gates.\n  We shed some light on the difficulty of proving lower bounds for $MOD_m$\ncircuits, by giving new upper bounds. We construct $MOD_m$ circuits computing\nsymmetric functions with non-prime power $m$, with size-depth tradeoffs that\nbeat the longstanding lower bounds for $AC^0[m]$ circuits for prime power $m$.\nOur size-depth tradeoff circuits have essentially optimal dependence on $m$ and\n$d$ in the exponent, under a natural circuit complexity hypothesis.\n  For example, we show for every $\\varepsilon > 0$ that every symmetric\nfunction can be computed with depth-3 $MOD_m$ circuits of\n$\\exp(O(n^{\\varepsilon}))$ size, for a constant $m$ depending only on\n$\\varepsilon > 0$. That is, depth-$3$ $CC^0$ circuits can compute any symmetric\nfunction in \\emph{subexponential} size. This demonstrates a significant\ndifference in the power of depth-$3$ $CC^0$ circuits, compared to other models:\nfor certain symmetric functions, depth-$3$ $AC^0$ circuits require\n$2^{\\Omega(\\sqrt{n})}$ size [H{\\aa}stad 1986], and depth-$3$ $AC^0[p^k]$\ncircuits (for fixed prime power $p^k$) require $2^{\\Omega(n^{1/6})}$ size\n[Smolensky 1987]. Even for depth-two $MOD_p \\circ MOD_m$ circuits,\n$2^{\\Omega(n)}$ lower bounds were known [Barrington Straubing Th\\'erien 1990].\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:41:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chapman", "Brynmor", ""], ["Williams", "Ryan", ""]]}, {"id": "2107.05018", "submitter": "Stanislav \\v{Z}ivn\\'y", "authors": "Lorenzo Ciardo and Stanislav \\v{Z}ivn\\'y", "title": "CLAP: A New Algorithm for Promise CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for Promise Constraint Satisfaction Problems\n(PCSPs). It is a combination of the Constraint Basic LP relaxation and the\nAffine IP relaxation (CLAP). We give a characterisation of the power of CLAP in\nterms of a minion homomorphism. Using this characterisation, we identify a\ncertain weak notion of symmetry which, if satisfied by infinitely many\npolymorphisms of PCSPs, guarantees tractability.\n  We demonstrate that there are PCSPs solved by CLAP that are not solved by any\nof the existing algorithms for PCSPs; in particular, not by the BLP+AIP\nalgorithm of Brakensiek and Guruswami [SODA'20] and not by a reduction to\ntractable finite-domain CSPs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:51:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ciardo", "Lorenzo", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "2107.05128", "submitter": "Balagopal Komarath", "authors": "Christian Ikenmeyer, Balagopal Komarath, Nitin Saurabh", "title": "Karchmer-Wigderson Games for Hazard-free Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Karchmer-Wigderson game to study the complexity of hazard-free\nformulas. This new game is both a generalization of the monotone\nKarchmer-Wigderson game and an analog of the classical Boolean\nKarchmer-Wigderson game. Therefore, it acts as a bridge between the existing\nmonotone and general games.\n  Using this game, we prove hazard-free formula size and depth lower bounds\nthat are provably stronger than those possible by the standard technique of\ntransferring results from monotone complexity in a black-box fashion. For the\nmultiplexer function we give (1) a hazard-free formula of optimal size and (2)\nan improved low-depth hazard-free formula of almost optimal size and (3) a\nhazard-free formula with alternation depth $2$ that has optimal depth. We then\nuse our optimal constructions to obtain an improved universal worst-case\nhazard-free formula size upper bound. We see our results as a significant step\ntowards establishing hazard-free computation as an independent missing link\nbetween Boolean complexity and monotone complexity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 20:24:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ikenmeyer", "Christian", ""], ["Komarath", "Balagopal", ""], ["Saurabh", "Nitin", ""]]}, {"id": "2107.05161", "submitter": "Yihan Zhang", "authors": "Yihan Zhang, Shashank Vatedka", "title": "Bounds for Multiple Packing and List-Decoding Error Exponents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CC cs.IT math.CO math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the problem of high-dimensional multiple packing in Euclidean\nspace. Multiple packing is a natural generalization of sphere packing and is\ndefined as follows. Let $ N>0 $ and $ L\\in\\mathbb{Z}_{\\ge2} $. A multiple\npacking is a set $\\mathcal{C}$ of points in $ \\mathbb{R}^n $ such that any\npoint in $ \\mathbb{R}^n $ lies in the intersection of at most $ L-1 $ balls of\nradius $ \\sqrt{nN} $ around points in $ \\mathcal{C} $. We study the multiple\npacking problem for both bounded point sets whose points have norm at most\n$\\sqrt{nP}$ for some constant $P>0$ and unbounded point sets whose points are\nallowed to be anywhere in $ \\mathbb{R}^n $. Given a well-known connection with\ncoding theory, multiple packings can be viewed as the Euclidean analog of\nlist-decodable codes, which are well-studied for finite fields. In this paper,\nwe derive various bounds on the largest possible density of a multiple packing\nin both bounded and unbounded settings. A related notion called average-radius\nmultiple packing is also studied. Some of our lower bounds exactly pin down the\nasymptotics of certain ensembles of average-radius list-decodable codes, e.g.,\n(expurgated) Gaussian codes and (expurgated) Poisson Point Processes. To this\nend, we apply tools from high-dimensional geometry and large deviation theory.\nSome of our lower bounds on the optimal multiple packing density are the best\nknown lower bounds. These bounds are obtained via a proxy known as error\nexponent. The latter quantity is the best exponent of the probability of\nlist-decoding error when the code is corrupted by a Gaussian noise. We\nestablish a curious inequality which relates the error exponent, a quantity of\naverage-case nature, to the list-decoding radius, a quantity of worst-case\nnature. We derive various bounds on the error exponent in both bounded and\nunbounded settings which are of independent interest beyond multiple packing.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 01:49:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Yihan", ""], ["Vatedka", "Shashank", ""]]}, {"id": "2107.05198", "submitter": "Debajyoti Mondal", "authors": "J. Mark Keil, Debajyoti Mondal, Ehsan Moradi, Yakov Nekrich", "title": "Finding a Maximum Clique in a Grounded 1-Bend String Graph", "comments": "A preliminary version of the paper was presented at the 32nd Canadian\n  Conference on Computational Geometry (CCCG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grounded 1-bend string graph is an intersection graph of a set of polygonal\nlines, each with one bend, such that the lines lie above a common horizontal\nline $\\ell$ and have exactly one endpoint on $\\ell$. We show that the problem\nof finding a maximum clique in a grounded 1-bend string graph is APX-hard, even\nfor strictly $y$-monotone strings. For general 1-bend strings, the problem\nremains APX-hard even if we restrict the position of the bends and end-points\nto lie on at most three parallel horizontal lines. We give fast algorithms to\ncompute a maximum clique for different subclasses of grounded segment graphs,\nwhich are formed by restricting the strings to various forms of $L$-shapes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:17:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Keil", "J. Mark", ""], ["Mondal", "Debajyoti", ""], ["Moradi", "Ehsan", ""], ["Nekrich", "Yakov", ""]]}, {"id": "2107.05296", "submitter": "Anuj Dawar", "authors": "Anuj Dawar and Felipe Ferreira Santos", "title": "Separating LREC from LFP", "comments": "21 pages. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LREC= is an extension of first-order logic with a logarithmic recursion\noperator. It was introduced by Grohe et al. and shown to capture the complexity\nclass L over trees and interval graphs. It does not capture L in general as it\nis contained in FPC - fixed-point logic with counting. We show that this\ncontainment is strict. In particular, we show that the path systems problem, a\nclassic P-complete problem which is definable in LFP - fixed-point logic - is\nnot definable in LREC= This shows that the logarithmic recursion mechanism is\nprovably weaker than general least fixed points. The proof is based on a novel\nSpoiler-Duplicator game tailored for this logic.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 10:14:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dawar", "Anuj", ""], ["Santos", "Felipe Ferreira", ""]]}, {"id": "2107.05355", "submitter": "Uwe Naumann", "authors": "Uwe Naumann", "title": "On the Computational Complexity of the Chain Rule of Differential\n  Calculus", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many modern numerical methods in computational science and engineering rely\non derivatives of mathematical models for the phenomena under investigation.\nThe computation of these derivatives often represents the bottleneck in terms\nof overall runtime performance. First and higher derivative tensors need to be\nevaluated efficiently.\n  The chain rule of differentiation is the fundamental prerequisite for\ncomputing accurate derivatives of composite functions which perform a\npotentially very large number of elemental function evaluations. Data flow\ndependences amongst the elemental functions give rise to a combinatorial\noptimization problem. We formulate {\\sc Chain Rule Differentiation} and we\nprove it to be NP-complete. Pointers to research on its approximate solution\nare given.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:22:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Naumann", "Uwe", ""]]}, {"id": "2107.05407", "submitter": "Andrea Banino", "authors": "Andrea Banino, Jan Balaguer, Charles Blundell", "title": "PonderNet: Learning to Ponder", "comments": "16 pages, 2 figures, 2 tables, 8th ICML Workshop on Automated Machine\n  Learning (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard neural networks the amount of computation used grows with the\nsize of the inputs, but not with the complexity of the problem being learnt. To\novercome this limitation we introduce PonderNet, a new algorithm that learns to\nadapt the amount of computation based on the complexity of the problem at hand.\nPonderNet learns end-to-end the number of computational steps to achieve an\neffective compromise between training prediction accuracy, computational cost\nand generalization. On a complex synthetic problem, PonderNet dramatically\nimproves performance over previous adaptive computation methods and\nadditionally succeeds at extrapolation tests where traditional neural networks\nfail. Also, our method matched the current state of the art results on a real\nworld question and answering dataset, but using less compute. Finally,\nPonderNet reached state of the art results on a complex task designed to test\nthe reasoning capabilities of neural networks.1\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:24:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Banino", "Andrea", ""], ["Balaguer", "Jan", ""], ["Blundell", "Charles", ""]]}, {"id": "2107.05486", "submitter": "Heng Guo", "authors": "Andreas Galanis, Heng Guo, Jiaheng Wang", "title": "Inapproximability of counting hypergraph colourings", "comments": "abstract shortened for arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in approximate counting have made startling progress in\ndeveloping fast algorithmic methods for approximating the number of solutions\nto constraint satisfaction problems (CSPs) with large arities, using\nconnections to the Lovasz Local Lemma. Nevertheless, the boundaries of these\nmethods for CSPs with non-Boolean domain are not well-understood. Our goal in\nthis paper is to fill in this gap and obtain strong inapproximability results\nby studying the prototypical problem in this class of CSPs, hypergraph\ncolourings.\n  More precisely, we focus on the problem of approximately counting\n$q$-colourings on $K$-uniform hypergraphs with bounded degree $\\Delta$. An\nefficient algorithm exists if $\\Delta\\lesssim \\frac{q^{K/3-1}}{4^KK^2}$ (Jain,\nPham, and Voung, 2021; He, Sun, and Wu, 2021). Somewhat surprisingly however, a\nhardness bound is not known even for the easier problem of finding colourings.\nFor the counting problem, the situation is even less clear and there is no\nevidence of the right constant controlling the growth of the exponent in terms\nof $K$.\n  To this end, we first establish that for general $q$ computational hardness\nfor finding a colouring on simple/linear hypergraphs occurs at $\\Delta\\gtrsim\nKq^K$, almost matching the algorithm from the Lovasz Local Lemma. Our second\nand main contribution is to obtain a far more refined bound for the counting\nproblem that goes well beyond the hardness of finding a colouring and which we\nconjecture that is asymptotically tight (up to constant factors). We show in\nparticular that for all even $q\\geq 4$ it is NP-hard to approximate the number\nof colourings when $\\Delta\\gtrsim q^{K/2}$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:02:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Galanis", "Andreas", ""], ["Guo", "Heng", ""], ["Wang", "Jiaheng", ""]]}, {"id": "2107.05524", "submitter": "Guangsheng Ma", "authors": "Guangsheng Ma, Hongbo Li, Jiman Zhao", "title": "Quantum Radon Transform and Its Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper extends the Radon transform, a classical image processing tool for\nfast tomography and denoising, to the quantum computing platform. A new kind of\nperiodic discrete Radon transform (PDRT), called quantum Radon transform (QRT),\nis proposed. The QRT has a quantum implementation that is exponentially faster\nthan the classical Radon transform. Based on the QRT, we design an efficient\nquantum image denoising algorithm. The simulation results show that QRT\npreserves the good denoising capability as in the classical PDRT. Also, a\nquantum algorithm for interpolation-based discrete Radon transform (IDRT) is\nproposed, which can be used for fast line detection. Both the quantum extension\nof IDRT and the line detection algorithm can provide polynomial speedups over\nthe classical counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:51:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Guangsheng", ""], ["Li", "Hongbo", ""], ["Zhao", "Jiman", ""]]}, {"id": "2107.05810", "submitter": "Amit Chakrabarti", "authors": "Amit Chakrabarti and Manuel Stoeckl", "title": "The Element Extraction Problem and the Cost of Determinism and Limited\n  Adaptivity in Linear Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Two widely-used computational paradigms for sublinear algorithms are using\nlinear measurements to perform computations on a high dimensional input and\nusing structured queries to access a massive input. Typically, algorithms in\nthe former paradigm are non-adaptive whereas those in the latter are highly\nadaptive. This work studies the fundamental search problem of\n\\textsc{element-extraction} in a query model that combines both: linear\nmeasurements with bounded adaptivity.\n  In the \\textsc{element-extraction} problem, one is given a nonzero vector\n$\\mathbf{z} = (z_1,\\ldots,z_n) \\in \\{0,1\\}^n$ and must report an index $i$\nwhere $z_i = 1$. The input can be accessed using arbitrary linear functions of\nit with coefficients in some ring. This problem admits an efficient nonadaptive\nrandomized solution (through the well known technique of $\\ell_0$-sampling) and\nan efficient fully adaptive deterministic solution (through binary search). We\nprove that when confined to only $k$ rounds of adaptivity, a deterministic\n\\textsc{element-extraction} algorithm must spend $\\Omega(k (n^{1/k} -1))$\nqueries, when working in the ring of integers modulo some fixed $q$. This\nmatches the corresponding upper bound. For queries using integer arithmetic, we\nprove a $2$-round $\\widetilde{\\Omega}(\\sqrt{n})$ lower bound, also tight up to\npolylogarithmic factors. Our proofs reduce to classic problems in\ncombinatorics, and take advantage of established results on the {\\em zero-sum\nproblem} as well as recent improvements to the {\\em sunflower lemma}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 02:07:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Stoeckl", "Manuel", ""]]}, {"id": "2107.05886", "submitter": "Albert Atserias", "authors": "Albert Atserias and V\\'ictor Dalmau", "title": "Promise Constraint Satisfaction and Width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the power of the bounded-width consistency algorithm in the context\nof the fixed-template Promise Constraint Satisfaction Problem (PCSP). Our main\ntechnical finding is that the template of every PCSP that is solvable in\nbounded width satisfies a certain structural condition implying that its\nalgebraic closure-properties include weak near unanimity polymorphisms of all\nlarge arities. While this parallels the standard (non-promise) CSP theory, the\nmethod of proof is quite different and applies even to the regime of sublinear\nwidth. We also show that, in contrast with the CSP world, the presence of weak\nnear unanimity polymorphisms of all large arities does not guarantee\nsolvability in bounded width. The separating example is even solvable in the\nsecond level of the Sherali-Adams (SA) hierarchy of linear programming\nrelaxations. This shows that, unlike for CSPs, linear programming can be\nstronger than bounded width. A direct application of these methods also show\nthat the problem of $q$-coloring $p$-colorable graphs is not solvable in\nbounded or even sublinear width, for any two constants $p$ and $q$ such that $3\n\\leq p \\leq q$. Turning to algorithms, we note that Wigderson's algorithm for\n$O(\\sqrt{n})$-coloring $3$-colorable graphs with $n$ vertices is implementable\nin width $4$. Indeed, by generalizing the method we see that, for any $\\epsilon\n> 0$ smaller than $1/2$, the optimal width for solving the problem of\n$O(n^\\epsilon)$-coloring $3$-colorable graphs with $n$ vertices lies between\n$n^{1-3\\epsilon}$ and $n^{1-2\\epsilon}$. The upper bound gives a simple\n$2^{\\Theta(n^{1-2\\epsilon}\\log(n))}$-time algorithm that, asymptotically, beats\nthe straightforward $2^{\\Theta(n^{1-\\epsilon})}$ bound that follows from\npartitioning the graph into $O(n^\\epsilon)$ many independent parts each of size\n$O(n^{1-\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:36:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Atserias", "Albert", ""], ["Dalmau", "V\u00edctor", ""]]}, {"id": "2107.06111", "submitter": "Falko Hegerfeld", "authors": "Falko Hegerfeld, Stefan Kratsch", "title": "Towards exact structural thresholds for parameterized complexity", "comments": "49 pages, 12 figures, shortened abstract due to character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity seeks to use input structure to obtain faster\nalgorithms for NP-hard problems. This has been most successful for graphs of\nlow treewidth: Many problems admit fast algorithms relative to treewidth and\nmany of them are optimal under SETH. Fewer such results are known for more\ngeneral structure such as low clique-width and more restrictive structure such\nas low deletion distance to a sparse graph class.\n  Despite these successes, such results remain \"islands'' within the realm of\npossible structure. Rather than adding more islands, we seek to determine the\ntransitions between them, that is, we aim for structural thresholds where the\ncomplexity increases as input structure becomes more general. Going from\ndeletion distance to treewidth, is a single deletion set to a graph with simple\ncomponents enough to yield the same lower bound as for treewidth or does it\ntake many disjoint separators? Going from treewidth to clique-width, how much\nmore density entails the same complexity as clique-width? Conversely, what is\nthe most restrictive structure that yields the same lower bound?\n  For treewidth, we obtain both refined and new lower bounds that apply already\nto graphs with a single separator $X$ such that $G-X$ has treewidth $r=O(1)$,\nwhile $G$ has treewidth $|X|+O(1)$. We rule out algorithms running in time\n$O^*((r+1-\\epsilon)^{k})$ for Deletion to $r$-Colorable parameterized by\n$k=|X|$. For clique-width, we rule out time $O^*((2^r-\\epsilon)^k)$ for\nDeletion to $r$-Colorable, where $X$ is now allowed to consist of $k$\ntwinclasses. There are further results on Vertex Cover, Dominating Set and\nMaximum Cut. All lower bounds are matched by existing and newly designed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:06:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hegerfeld", "Falko", ""], ["Kratsch", "Stefan", ""]]}, {"id": "2107.06121", "submitter": "Nils Vortmeier", "authors": "Nils Vortmeier, Ioannis Kokkinis", "title": "The Dynamic Complexity of Acyclic Hypergraph Homomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a homomorphism from some hypergraph $\\mathcal{Q}$ (or some relational\nstructure) to another hypergraph $\\mathcal{D}$ is a fundamental problem in\ncomputer science. We show that an answer to this problem can be maintained\nunder single-edge changes of $\\mathcal{Q}$, as long as it stays acyclic, in the\nDynFO framework of Patnaik and Immerman that uses updates expressed in\nfirst-order logic. If additionally also changes of $\\mathcal{D}$ are allowed,\nwe show that it is unlikely that existence of homomorphisms can be maintained\nin DynFO.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:21:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Vortmeier", "Nils", ""], ["Kokkinis", "Ioannis", ""]]}, {"id": "2107.06156", "submitter": "Uma Girish", "authors": "Uma Girish, Justin Holmgren, Kunal Mittal, Ran Raz and Wei Zhan", "title": "Parallel Repetition for the GHZ Game: A Simpler Proof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new proof of the fact that the parallel repetition of the\n(3-player) GHZ game reduces the value of the game to zero polynomially quickly.\nThat is, we show that the value of the $n$-fold GHZ game is at most\n$n^{-\\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We\npresent a new proof of this theorem that we believe to be simpler and more\ndirect. Unlike most previous works on parallel repetition, our proof makes no\nuse of information theory, and relies on the use of Fourier analysis.\n  The GHZ game [GHZ89] has played a foundational role in the understanding of\nquantum information theory, due in part to the fact that quantum strategies can\nwin the GHZ game with probability 1. It is possible that improved parallel\nrepetition bounds may find applications in this setting.\n  Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game\nas a simple three-player game, which is in some sense maximally far from the\nclass of multi-player games whose behavior under parallel repetition is well\nunderstood. Dinur et al. conjectured that parallel repetition decreases the\nvalue of the GHZ game exponentially quickly, and speculated that progress on\nproving this would shed light on parallel repetition for general multi-player\n(multi-prover) games.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:12:02 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Girish", "Uma", ""], ["Holmgren", "Justin", ""], ["Mittal", "Kunal", ""], ["Raz", "Ran", ""], ["Zhan", "Wei", ""]]}, {"id": "2107.06261", "submitter": "Liangde Tao", "authors": "Lin Chen, Liangde Tao, Jos\\'e Verschae", "title": "Tight running times for minimum $\\ell_q$-norm load balancing: beyond\n  exponential dependencies on $1/\\epsilon$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a classical scheduling problem on $m$ identical machines. For an\narbitrary constant $q>1$, the aim is to assign jobs to machines such that\n$\\sum_{i=1}^m C_i^q$ is minimized, where $C_i$ is the total processing time of\njobs assigned to machine $i$. It is well known that this problem is strongly\nNP-hard.\n  Under mild assumptions, the running time of an $(1+\\epsilon)$-approximation\nalgorithm for a strongly NP-hard problem cannot be polynomial on $1/\\epsilon$,\nunless $\\text{P}=\\text{NP}$. For most problems in the literature, this\ntranslates into algorithms with running time at least as large as\n$2^{\\Omega(1/\\varepsilon)}+n^{O(1)}$. For the natural scheduling problem above,\nwe establish the existence of an algorithm which violates this threshold. More\nprecisely, we design a PTAS that runs in\n$2^{\\tilde{O}(\\sqrt{1/\\epsilon})}+n^{O(1)}$ time. This result is in sharp\ncontrast to the closely related minimum makespan variant, where an exponential\nlower bound is known under the exponential time hypothesis (ETH). We complement\nour result with an essentially matching lower bound on the running time,\nshowing that our algorithm is best-possible under ETH. The lower bound proof\nexploits new number-theoretical constructions for variants of progression-free\nsets, which might be of independent interest.\n  Furthermore, we provide a fine-grained characterization on the running time\nof a PTAS for this problem depending on the relation between $\\epsilon$ and the\nnumber of machines $m$. More precisely, our lower bound only holds when\n$m=\\Theta(\\sqrt{1/\\epsilon})$. Better algorithms, that go beyond the lower\nbound, exist for other values of $m$. In particular, there even exists an\nalgorithm with running time polynomial in $1/\\epsilon$ if we restrict ourselves\nto instances with $m=\\Omega(1/\\epsilon\\log^21/\\epsilon)$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:41:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chen", "Lin", ""], ["Tao", "Liangde", ""], ["Verschae", "Jos\u00e9", ""]]}, {"id": "2107.06309", "submitter": "Anup Rao", "authors": "Siddharth Iyer, Anup Rao, Victor Reis, Thomas Rothvoss, Amir\n  Yehudayoff", "title": "Tight bounds on the Fourier growth of bounded functions on the hypercube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.FA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give tight bounds on the degree $\\ell$ homogenous parts $f_\\ell$ of a\nbounded function $f$ on the cube. We show that if $f: \\{\\pm 1\\}^n \\rightarrow\n[-1,1]$ has degree $d$, then $\\| f_\\ell \\|_\\infty$ is bounded by\n$d^\\ell/\\ell!$, and $\\| \\hat{f}_\\ell \\|_1$ is bounded by $d^\\ell\ne^{\\binom{\\ell+1}{2}} n^{\\frac{\\ell-1}{2}}$. We describe applications to\npseudorandomness and learning theory. We use similar methods to generalize the\nclassical Pisier's inequality from convex analysis. Our analysis involves\nproperties of real-rooted polynomials that may be useful elsewhere.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:11:12 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 17:12:44 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Iyer", "Siddharth", ""], ["Rao", "Anup", ""], ["Reis", "Victor", ""], ["Rothvoss", "Thomas", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "2107.06785", "submitter": "Novanto Yudistira", "authors": "Kuncahyo Setyo Nugroho, Anantha Yullian Sukmadewa, Novanto Yudistira", "title": "Large-Scale News Classification using BERT Language Model: Spark NLP\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of big data analytics on top of NLP increases the computational\nburden for text processing at scale. The problems faced in NLP are very high\ndimensional text, so it takes a high computation resource. The MapReduce allows\nparallelization of large computations and can improve the efficiency of text\nprocessing. This research aims to study the effect of big data processing on\nNLP tasks based on a deep learning approach. We classify a big text of news\ntopics with fine-tuning BERT used pre-trained models. Five pre-trained models\nwith a different number of parameters were used in this study. To measure the\nefficiency of this method, we compared the performance of the BERT with the\npipelines from Spark NLP. The result shows that BERT without Spark NLP gives\nhigher accuracy compared to BERT with Spark NLP. The accuracy average and\ntraining time of all models using BERT is 0.9187 and 35 minutes while using\nBERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will\ntake more computation resources and need a longer time to complete the tasks.\nHowever, the accuracy of BERT with Spark NLP only decreased by an average of\n5.7%, while the training time was reduced significantly by 62.9% compared to\nBERT without Spark NLP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:42:15 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:04:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Nugroho", "Kuncahyo Setyo", ""], ["Sukmadewa", "Anantha Yullian", ""], ["Yudistira", "Novanto", ""]]}, {"id": "2107.06889", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Jacob Focke and D\\'aniel Marx and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Counting list homomorphisms from graphs of bounded treewidth: tight\n  complexity bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to give precise bounds on the counting complexity of\na family of generalized coloring problems (list homomorphisms) on\nbounded-treewidth graphs. Given graphs $G$, $H$, and lists $L(v)\\subseteq V(H)$\nfor every $v\\in V(G)$, a {\\em list homomorphism} is a function $f:V(G)\\to V(H)$\nthat preserves the edges (i.e., $uv\\in E(G)$ implies $f(u)f(v)\\in E(H)$) and\nrespects the lists (i.e., $f(v)\\in L(v))$. Standard techniques show that if $G$\nis given with a tree decomposition of width $t$, then the number of list\nhomomorphisms can be counted in time $|V(H)|^t\\cdot n^{\\mathcal{O}(1)}$. Our\nmain result is determining, for every fixed graph $H$, how much the base\n$|V(H)|$ in the running time can be improved. For a connected graph $H$ we\ndefine $\\operatorname{irr}(H)$ the following way: if $H$ has a loop or is\nnonbipartite, then $\\operatorname{irr}(H)$ is the maximum size of a set\n$S\\subseteq V(H)$ where any two vertices have different neighborhoods; if $H$\nis bipartite, then $\\operatorname{irr}(H)$ is the maximum size of such a set\nthat is fully in one of the bipartition classes. For disconnected $H$, we\ndefine $\\operatorname{irr}(H)$ as the maximum of $\\operatorname{irr}(C)$ over\nevery connected component $C$ of $H$. We show that, for every fixed graph $H$,\nthe number of list homomorphisms from $(G,L)$ to $H$\n  * can be counted in time $\\operatorname{irr}(H)^t\\cdot n^{\\mathcal{O}(1)}$ if\na tree decomposition of $G$ having width at most $t$ is given in the input, and\n  * cannot be counted in time $(\\operatorname{irr}(H)-\\epsilon)^t\\cdot\nn^{\\mathcal{O}(1)}$ for any $\\epsilon>0$, even if a tree decomposition of $G$\nhaving width at most $t$ is given in the input, unless the #SETH fails.\n  Thereby we give a precise and complete complexity classification featuring\nmatching upper and lower bounds for all target graphs with or without loops.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:19:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Focke", "Jacob", ""], ["Marx", "D\u00e1niel", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "2107.07038", "submitter": "Manuel Garcia-Piqueras", "authors": "Manuel Garcia-Piqueras and Jos\\'e Hern\\'andez-Orallo", "title": "Conditional Teaching Size", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent research in machine teaching has explored the instruction of any\nconcept expressed in a universal language. In this compositional context, new\nexperimental results have shown that there exist data teaching sets\nsurprisingly shorter than the concept description itself. However, there exists\na bound for those remarkable experimental findings through teaching size and\nconcept complexity that we further explore here. As concepts are rarely taught\nin isolation we investigate the best configuration of concepts to teach a given\nset of concepts, where those that have been acquired first can be reused for\nthe description of new ones. This new notion of conditional teaching size\nuncovers new insights, such as the interposition phenomenon: certain prior\nknowledge generates simpler compatible concepts that increase the teaching size\nof the concept that we want to teach. This does not happen for conditional\nKolmogorov complexity. Furthermore, we provide an algorithm that constructs\noptimal curricula based on interposition avoidance. This paper presents a\nseries of theoretical results, including their proofs, and some directions for\nfuture work. New research possibilities in curriculum teaching in compositional\nscenarios are now wide open to exploration.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 23:08:58 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Garcia-Piqueras", "Manuel", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""]]}, {"id": "2107.07359", "submitter": "Maxime Chaveroche", "authors": "Maxime Chaveroche, Franck Davoine, V\\'eronique Cherfaoui", "title": "Efficient M\\\"obius Transformations and their applications to\n  Dempster-Shafer Theory: Clarification and implementation", "comments": "Extension of an article published in the proceedings of the\n  international conference on Scalable Uncertainty Management (SUM) in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dempster-Shafer Theory (DST) generalizes Bayesian probability theory,\noffering useful additional information, but suffers from a high computational\nburden. A lot of work has been done to reduce the complexity of computations\nused in information fusion with Dempster's rule. The main approaches exploit\neither the structure of Boolean lattices or the information contained in belief\nsources. Each has its merits depending on the situation. In this paper, we\npropose sequences of graphs for the computation of the zeta and M\\\"obius\ntransformations that optimally exploit both the structure of distributive\nsemilattices and the information contained in belief sources. We call them the\nEfficient M\\\"obius Transformations (EMT). We show that the complexity of the\nEMT is always inferior to the complexity of algorithms that consider the whole\nlattice, such as the Fast M\\\"obius Transform (FMT) for all DST transformations.\nWe then explain how to use them to fuse two belief sources. More generally, our\nEMTs apply to any function in any finite distributive lattice, focusing on a\nmeet-closed or join-closed subset. This article extends our work published at\nthe international conference on Scalable Uncertainty Management (SUM). It\nclarifies it, brings some minor corrections and provides implementation details\nsuch as data structures and algorithms applied to DST.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:35:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chaveroche", "Maxime", ""], ["Davoine", "Franck", ""], ["Cherfaoui", "V\u00e9ronique", ""]]}, {"id": "2107.07386", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "Scheme-theoretic Approach to Computational Complexity I. The Separation\n  of P and NP", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We lay the foundations of a new theory for algorithms and computational\ncomplexity by parameterizing the instances of a computational problem as a\nmoduli scheme. Considering the geometry of the scheme associated to 3-SAT, we\nseparate P and NP.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:00:08 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 10:41:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "2107.07387", "submitter": "Ali \\c{C}ivril", "authors": "Ali \\c{C}ivril", "title": "Scheme-theoretic Approach to Computational Complexity II. The Separation\n  of P and NP over $\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{Z}$", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We show that the problem of determining the feasibility of quadratic systems\nover $\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{Z}$ requires exponential time.\nThis separates P and NP over these fields/rings in the BCSS model of\ncomputation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:00:58 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 10:44:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["\u00c7ivril", "Ali", ""]]}, {"id": "2107.07792", "submitter": "Andr\\'e Nusser", "authors": "Karl Bringmann, Anne Driemel, Andr\\'e Nusser, Ioannis Psarros", "title": "Tight Bounds for Approximate Near Neighbor Searching for Time Series\n  under the Fr\\'echet Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the $c$-approximate near neighbor problem under the continuous\nFr\\'echet distance: Given a set of $n$ polygonal curves with $m$ vertices, a\nradius $\\delta > 0$, and a parameter $k \\leq m$, we want to preprocess the\ncurves into a data structure that, given a query curve $q$ with $k$ vertices,\neither returns an input curve with Fr\\'echet distance at most $c\\cdot \\delta$\nto $q$, or returns that there exists no input curve with Fr\\'echet distance at\nmost $\\delta$ to $q$. We focus on the case where the input and the queries are\none-dimensional polygonal curves -- also called time series -- and we give a\ncomprehensive analysis for this case. We obtain new upper bounds that provide\ndifferent tradeoffs between approximation factor, preprocessing time, and query\ntime.\n  Our data structures improve upon the state of the art in several ways. We\nshow that for any $0 < \\varepsilon \\leq 1$ an approximation factor of\n$(1+\\varepsilon)$ can be achieved within the same asymptotic time bounds as the\npreviously best result for $(2+\\varepsilon)$. Moreover, we show that an\napproximation factor of $(2+\\varepsilon)$ can be obtained by using\npreprocessing time and space $O(nm)$, which is linear in the input size, and\nquery time in $O(\\frac{1}{\\varepsilon})^{k+2}$, where the previously best\nresult used preprocessing time in $n \\cdot O(\\frac{m}{\\varepsilon k})^k$ and\nquery time in $O(1)^k$. We complement our upper bounds with matching\nconditional lower bounds based on the Orthogonal Vectors Hypothesis.\nInterestingly, some of our lower bounds already hold for any super-constant\nvalue of $k$. This is achieved by proving hardness of a one-sided sparse\nversion of the Orthogonal Vectors problem as an intermediate problem, which we\nbelieve to be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:35:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bringmann", "Karl", ""], ["Driemel", "Anne", ""], ["Nusser", "Andr\u00e9", ""], ["Psarros", "Ioannis", ""]]}, {"id": "2107.08102", "submitter": "Wieslaw Kubiak", "authors": "Wieslaw Kubiak", "title": "On the complexity of open shop scheduling with time lags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The minimization of makespan in open shop with time lags has been shown\nNP-hard in the strong sense even for the case of two machines and unit-time\noperations. The minimization of total completion time however has remained open\nfor that case though it has been shown NP-hard in the strong sense for weighted\ntotal completion time or for jobs with release dates. This note shows that the\nminimization of total completion time for two machines and unit-time operations\nis NP-hard in the strong sense which answers the long standing open question.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 20:09:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kubiak", "Wieslaw", ""]]}, {"id": "2107.08444", "submitter": "Shay Moran", "authors": "Noga Alon and Steve Hanneke and Ron Holzman and Shay Moran", "title": "A Theory of PAC Learnability of Partial Concept Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We extend the theory of PAC learning in a way which allows to model a rich\nvariety of learning tasks where the data satisfy special properties that ease\nthe learning process. For example, tasks where the distance of the data from\nthe decision boundary is bounded away from zero. The basic and simple idea is\nto consider partial concepts: these are functions that can be undefined on\ncertain parts of the space. When learning a partial concept, we assume that the\nsource distribution is supported only on points where the partial concept is\ndefined.\n  This way, one can naturally express assumptions on the data such as lying on\na lower dimensional surface or margin conditions. In contrast, it is not at all\nclear that such assumptions can be expressed by the traditional PAC theory. In\nfact we exhibit easy-to-learn partial concept classes which provably cannot be\ncaptured by the traditional PAC theory. This also resolves a question posed by\nAttias, Kontorovich, and Mansour 2019.\n  We characterize PAC learnability of partial concept classes and reveal an\nalgorithmic landscape which is fundamentally different than the classical one.\nFor example, in the classical PAC model, learning boils down to Empirical Risk\nMinimization (ERM). In stark contrast, we show that the ERM principle fails in\nexplaining learnability of partial concept classes. In fact, we demonstrate\nclasses that are incredibly easy to learn, but such that any algorithm that\nlearns them must use an hypothesis space with unbounded VC dimension. We also\nfind that the sample compression conjecture fails in this setting.\n  Thus, this theory features problems that cannot be represented nor solved in\nthe traditional way. We view this as evidence that it might provide insights on\nthe nature of learnability in realistic scenarios which the classical theory\nfails to explain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:29:26 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:25:35 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Alon", "Noga", ""], ["Hanneke", "Steve", ""], ["Holzman", "Ron", ""], ["Moran", "Shay", ""]]}, {"id": "2107.08676", "submitter": "Aniruddha Biswas", "authors": "Aniruddha Biswas and Palash Sarkar", "title": "Influence of a Set of Variables on a Boolean Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.CR math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The influence of a set of variables on a Boolean function has three separate\ndefinitions in the literature, the first due to Ben-Or and Linial (1989), the\nsecond due to Fischer et al. (2002) and Blais (2009) and the third due to Tal\n(2017). The goal of the present work is to carry out a comprehensive study of\nthe notion of influence of a set of variables on a Boolean function. To this\nend, we introduce a definition of this notion using the auto-correlation\nfunction. A modification of the definition leads to the notion of\npseudo-influence. Somewhat surprisingly, it turns out that the auto-correlation\nbased definition of influence is equivalent to the definition introduced by\nFischer et al. (2002) and Blais (2009) and the notion of pseudo-influence is\nequivalent to the definition of influence considered by Tal (2017). Extensive\nanalysis of influence and pseduo-influence as well as the Ben-Or and Linial\nnotion of influence is carried out and the relations between these notions are\nestablished.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:25:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Biswas", "Aniruddha", ""], ["Sarkar", "Palash", ""]]}, {"id": "2107.09173", "submitter": "J. Maurice Rojas", "authors": "J. Maurice Rojas and Yuyu Zhu", "title": "Root Repulsion and Faster Solving for Very Sparse Polynomials Over\n  $p$-adic Fields", "comments": "36 pages, 3 figures, submitted to a journal for publication. A much\n  shorter preliminary version appeared as an extended abstract at ISSAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For any fixed field $K\\!\\in\\!\\{\\mathbb{Q}_2,\\mathbb{Q}_3,\\mathbb{Q}_5,\n\\ldots\\}$, we prove that all polynomials $f\\!\\in\\!\\mathbb{Z}[x]$ with exactly\n$3$ (resp. $2$) monomial terms, degree $d$, and all coefficients having\nabsolute value at most $H$, can be solved over $K$ within deterministic time\n$\\log^{7+o(1)}(dH)$ (resp. $\\log^{2+o(1)}(dH)$) in the classical Turing model:\nOur underlying algorithm correctly counts the number of roots of $f$ in $K$,\nand for each such root generates an approximation in $\\mathbb{Q}$ with\nlogarithmic height $O(\\log^3(dH))$ that converges at a rate of\n$O\\!\\left((1/p)^{2^i}\\right)$ after $i$ steps of Newton iteration. We also\nprove significant speed-ups in certain settings, a minimal spacing bound of\n$p^{-O(p\\log^2_p(dH)\\log d)}$ for distinct roots in $\\mathbb{C}_p$, and even\nstronger repulsion when there are nonzero degenerate roots in $\\mathbb{C}_p$:\n$p$-adic distance $p^{-O(\\log_p(dH))}$. On the other hand, we prove that there\nis an explicit family of tetranomials with distinct nonzero roots in\n$\\mathbb{Z}_p$ indistinguishable in their first $\\Omega(d\\log_p H)$ most\nsignificant base-$p$ digits.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 21:57:10 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Rojas", "J. Maurice", ""], ["Zhu", "Yuyu", ""]]}, {"id": "2107.09320", "submitter": "Sravanthi Chede", "authors": "Sravanthi Chede and Anil Shukla", "title": "QRAT Polynomially Simulates Merge Resolution", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merge Resolution (MRes [Beyersdorff et al. J. Autom. Reason.'2021] ) is a\nrefutational proof system for quantified Boolean formulas (QBF). Each line of\nMRes consists of clauses with only existential literals, together with\ninformation of countermodels stored as merge maps. As a result, MRes has\nstrategy extraction by design. The QRAT [Heule et al. J. Autom. Reason.'2017]\nproof system was designed to capture QBF preprocessing. QRAT can simulate both\nthe expansion-based proof system $\\forall$Exp+Res and CDCL-based QBF proof\nsystem LD-Q-Res.\n  A family of false QBFs called SquaredEquality formulas were introduced in\n[Beyersdorff et al. J. Autom. Reason.'2021] and shown to be easy for MRes but\nneed exponential size proofs in Q-Res, QU-Res, CP+$\\forall$red,\n$\\forall$Exp+Res, IR-calc and reductionless LD-Q-Res. As a result none of these\nsystems can simulate MRes. In this paper, we show a short QRAT refutation of\nthe SquaredEquality formulas. We further show that QRAT strictly p-simulates\nMRes. Besides highlighting the power of QRAT system, this work also presents\nthe first simulation result for MRes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:19:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chede", "Sravanthi", ""], ["Shukla", "Anil", ""]]}, {"id": "2107.09423", "submitter": "Marcin Kozik", "authors": "Libor Barto and Marcin Kozik", "title": "Combinatorial Gap Theorem and Reductions between Promise CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A value of a CSP instance is typically defined as a fraction of constraints\nthat can be simultaneously met. We propose an alternative definition of a value\nof an instance and show that, for purely combinatorial reasons, a value of an\nunsolvable instance is bounded away from one; we call this fact a gap theorem.\n  We show that the gap theorem implies NP-hardness of a gap version of the\nLayered Label Cover Problem. The same result can be derived from the PCP\nTheorem, but a full, self-contained proof of our reduction is quite short and\nthe result can still provide PCP-free NP-hardness proofs for numerous problems.\nThe simplicity of our reasoning also suggests that weaker versions of\nUnique-Games-type conjectures, e.g., the d-to-1 conjecture, might be accessible\nand serve as an intermediate step for proving these conjectures in their full\nstrength.\n  As the second, main application we provide a sufficient condition under which\na fixed template Promise Constraint Satisfaction Problem (PCSP) reduces to\nanother PCSP. The correctness of the reduction hinges on the gap theorem, but\nthe reduction itself is very simple. As a consequence, we obtain that every CSP\ncan be canonically reduced to most of the known NP-hard PCSPs, such as the\napproximate hypergraph coloring problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:36:17 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Barto", "Libor", ""], ["Kozik", "Marcin", ""]]}, {"id": "2107.09471", "submitter": "Eva Miranda", "authors": "Robert Cardona, Eva Miranda and Daniel Peralta-Salas", "title": "Looking at Euler flows through a contact mirror: Universality and\n  undecidability", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CC math.DS math.SG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of an inviscid and incompressible fluid flow on a Riemannian\nmanifold is governed by the Euler equations. In recent papers [5, 6, 7, 8]\nseveral unknown facets of the Euler flows have been discovered, including\nuniversality properties of the stationary solutions to the Euler equations. The\nstudy of these universality features was suggested by Tao as a novel way to\naddress the problem of global existence for Euler and Navier-Stokes [28].\nUniversality of the Euler equations was proved in [7] for stationary solutions\nusing a contact mirror which reflects a Beltrami flow as a Reeb vector field.\nThis contact mirror permits the use of advanced geometric techniques in fluid\ndynamics. On the other hand, motivated by Tao's approach relating Turing\nmachines to Navier-Stokes equations, a Turing complete stationary Euler\nsolution on a Riemannian $3$-dimensional sphere was constructed in [8]. Since\nthe Turing completeness of a vector field can be characterized in terms of the\nhalting problem, which is known to be undecidable [30], a striking consequence\nof this fact is that a Turing complete Euler flow exhibits undecidable particle\npaths [8]. In this article, we give a panoramic overview of this fascinating\nsubject, and go one step further in investigating the undecidability of\ndifferent dynamical properties of Turing complete flows. In particular, we show\nthat variations of [8] allow us to construct a stationary Euler flow of\nBeltrami type (and, via the contact mirror, a Reeb vector field) for which it\nis undecidable to determine whether its orbits through an explicit set of\npoints are periodic.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:24:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Cardona", "Robert", ""], ["Miranda", "Eva", ""], ["Peralta-Salas", "Daniel", ""]]}, {"id": "2107.09703", "submitter": "Suryajith Chillara", "authors": "Suryajith Chillara", "title": "Functional lower bounds for restricted arithmetic circuits of depth four", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Forbes, Kumar and Saptharishi [CCC, 2016] proved that there exists\nan explicit $d^{O(1)}$-variate and degree $d$ polynomial $P_{d}\\in VNP$ such\nthat if any depth four circuit $C$ of bounded formal degree $d$ which computes\na polynomial of bounded individual degree $O(1)$, that is functionally\nequivalent to $P_d$, then $C$ must have size $2^{\\Omega(\\sqrt{d}\\log{d})}$.\n  The motivation for their work comes from Boolean Circuit Complexity. Based on\na characterization for $ACC^0$ circuits by Yao [FOCS, 1985] and Beigel and\nTarui [CC, 1994], Forbes, Kumar and Saptharishi [CCC, 2016] observed that\nfunctions in $ACC^0$ can also be computed by algebraic\n$\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits (i.e., circuits of the form -- sums\nof powers of polynomials) of $2^{\\log^{O(1)}n}$ size. Thus they argued that a\n$2^{\\omega(\\log^{O(1)}{n})}$ \"functional\" lower bound for an explicit\npolynomial $Q$ against $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits would imply a\nlower bound for the \"corresponding Boolean function\" of $Q$ against non-uniform\n$ACC^0$. In their work, they ask if their lower bound be extended to\n$\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuits.\n  In this paper, for large integers $n$ and $d$ such that $\\omega(\\log^2n)\\leq\nd\\leq n^{0.01}$, we show that any $\\Sigma\\mathord{\\wedge}\\Sigma\\Pi$ circuit of\nbounded individual degree at most $O\\left(\\frac{d}{k^2}\\right)$ that\nfunctionally computes Iterated Matrix Multiplication polynomial $IMM_{n,d}$\n($\\in VP$) over $\\{0,1\\}^{n^2d}$ must have size $n^{\\Omega(k)}$. Since Iterated\nMatrix Multiplication $IMM_{n,d}$ over $\\{0,1\\}^{n^2d}$ is functionally in\n$GapL$, improvement of the afore mentioned lower bound to hold for\nquasipolynomially large values of individual degree would imply a fine-grained\nseparation of $ACC^0$ from $GapL$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:13:34 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chillara", "Suryajith", ""]]}, {"id": "2107.09735", "submitter": "Itzik Mizrahi", "authors": "Itzik Mizrahi, Shai Avidan", "title": "kNet: A Deep kNN Network To Handle Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks require large amounts of labeled data for their\ntraining. Collecting this data at scale inevitably causes label noise.Hence,the\nneed to develop learning algorithms that are robust to label noise. In recent\nyears, k Nearest Neighbors (kNN) emerged as a viable solution to this problem.\nDespite its success, kNN is not without its problems. Mainly, it requires a\nhuge memory footprint to store all the training samples and it needs an\nadvanced data structure to allow for fast retrieval of the relevant examples,\ngiven a query sample. We propose a neural network, termed kNet, that learns to\nperform kNN. Once trained, we no longer need to store the training data, and\nprocessing a query sample is a simple matter of inference. To use kNet, we\nfirst train a preliminary network on the data set, and then train kNet on the\npenultimate layer of the preliminary network.We find that kNet gives a smooth\napproximation of kNN,and cannot handle the sharp label changes between samples\nthat kNN can exhibit. This indicates that currently kNet is best suited to\napproximate kNN with a fairly large k. Experiments on two data sets show that\nthis is the regime in which kNN works best,and can therefore be replaced by\nkNet.In practice, kNet consistently improve the results of all preliminary\nnetworks, in all label noise regimes, by up to 3%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 19:12:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Mizrahi", "Itzik", ""], ["Avidan", "Shai", ""]]}, {"id": "2107.10797", "submitter": "Chin Ho Lee", "authors": "Jaros{\\l}aw B{\\l}asiok, Peter Ivanov, Yaonan Jin, Chin Ho Lee, Rocco\n  A. Servedio, Emanuele Viola", "title": "Fourier growth of structured $\\mathbb{F}_2$-polynomials and applications", "comments": "Full version of the RANDOM 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$\n(denoted $L_{1,k}$), of various well-studied classes of \"structured\"\n$\\mathbb{F}_2$-polynomials. This study is motivated by applications in\npseudorandomness, in particular recent results and conjectures due to\n[CHHL19,CHLT19,CGLSS20] which show that upper bounds on Fourier growth (even at\nlevel $k=2$) give unconditional pseudorandom generators.\n  Our main structural results on Fourier growth are as follows:\n  - We show that any symmetric degree-$d$ $\\mathbb{F}_2$-polynomial $p$ has\n$L_{1,k}(p) \\le \\Pr[p=1] \\cdot O(d)^k$, and this is tight for any constant $k$.\nThis quadratically strengthens an earlier bound that was implicit in [RSV13].\n  - We show that any read-$\\Delta$ degree-$d$ $\\mathbb{F}_2$-polynomial $p$ has\n$L_{1,k}(p) \\le \\Pr[p=1] \\cdot (k \\Delta d)^{O(k)}$.\n  - We establish a composition theorem which gives $L_{1,k}$ bounds on disjoint\ncompositions of functions that are closed under restrictions and admit\n$L_{1,k}$ bounds.\n  Finally, we apply the above structural results to obtain new unconditional\npseudorandom generators and new correlation bounds for various classes of\n$\\mathbb{F}_2$-polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:52:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Ivanov", "Peter", ""], ["Jin", "Yaonan", ""], ["Lee", "Chin Ho", ""], ["Servedio", "Rocco A.", ""], ["Viola", "Emanuele", ""]]}, {"id": "2107.10822", "submitter": "Joshua Brakensiek", "authors": "Joshua Brakensiek, Sivakanth Gopi, Visu Makam", "title": "Lower Bounds for Maximally Recoverable Tensor Code and Higher Order MDS\n  Codes", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(m,n,a,b)$-tensor code consists of $m\\times n$ matrices whose columns\nsatisfy `$a$' parity checks and rows satisfy `$b$' parity checks (i.e., a\ntensor code is the tensor product of a column code and row code). Tensor codes\nare useful in distributed storage because a single erasure can be corrected\nquickly either by reading its row or column. Maximally Recoverable (MR) Tensor\nCodes, introduced by Gopalan et al., are tensor codes which can correct every\nerasure pattern that is information theoretically possible to correct. The main\nquestions about MR Tensor Codes are characterizing which erasure patterns are\ncorrectable and obtaining explicit constructions over small fields.\n  In this paper, we study the important special case when $a=1$, i.e., the\ncolumns satisfy a single parity check equation. We introduce the notion of\nhigher order MDS codes (MDS$(\\ell)$ codes) which is an interesting\ngeneralization of the well-known MDS codes, where $\\ell$ captures the order of\ngenericity of points in a low-dimensional space. We then prove that a tensor\ncode with $a=1$ is MR iff the row code is an MDS$(m)$ code. We then show that\nMDS$(m)$ codes satisfy some weak duality. Using this characterization and\nduality, we prove that $(m,n,a=1,b)$-MR tensor codes require fields of size\n$q=\\Omega_{m,b}(n^{\\min\\{b,m\\}-1})$. Our lower bound also extends to the\nsetting of $a>1$. We also give a deterministic polynomial time algorithm to\ncheck if a given erasure pattern is correctable by the MR tensor code (when\n$a=1$).\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:22:22 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Brakensiek", "Joshua", ""], ["Gopi", "Sivakanth", ""], ["Makam", "Visu", ""]]}, {"id": "2107.10986", "submitter": "Gregory Wilsenach", "authors": "Anuj Dawar and Gregory Wilsenach", "title": "Lower Bounds for Symmetric Circuits for the Determinant", "comments": "29 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dawar and Wilsenach (ICALP 2020) introduce the model of symmetric arithmetic\ncircuits and show an exponential separation between the sizes of symmetric\ncircuits for computing the determinant and the permanent. The symmetry\nrestriction is that the circuits which take a matrix input are unchanged by a\npermutation applied simultaneously to the rows and columns of the matrix. Under\nsuch restrictions we have polynomial-size circuits for computing the\ndeterminant but no subexponential size circuits for the permanent. Here, we\nconsider a more stringent symmetry requirement, namely that the circuits are\nunchanged by arbitrary even permutations applied separately to rows and\ncolumns, and prove an exponential lower bound even for circuits computing the\ndeterminant. The result requires substantial new machinery. We develop a\ngeneral framework for proving lower bounds for symmetric circuits with\nrestricted symmetries, based on a new support theorem and new two-player\nrestricted bijection games. These are applied to the determinant problem with a\nnovel construction of matrices that are bi-adjacency matrices of graphs based\non the CFI construction. Our general framework opens the way to exploring a\nvariety of symmetry restrictions and studying trade-offs between symmetry and\nother resources used by arithmetic circuits.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 01:35:49 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 21:25:17 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Dawar", "Anuj", ""], ["Wilsenach", "Gregory", ""]]}, {"id": "2107.11205", "submitter": "Subhamoy Maitra", "authors": "Subhamoy Maitra, Chandra Sekhar Mukherjee, Pantelimon Stanica and Deng\n  Tang", "title": "On Boolean Functions with Low Polynomial Degree and Higher Order\n  Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Boolean functions are important primitives in different domains of\ncryptology, complexity and coding theory. In this paper, we connect the tools\nfrom cryptology and complexity theory in the domain of Boolean functions with\nlow polynomial degree and high sensitivity. It is well known that the\npolynomial degree of of a Boolean function and its resiliency are directly\nconnected. Using this connection we analyze the polynomial degree-sensitivity\nvalues through the lens of resiliency, demonstrating existence and\nnon-existence results of functions with low polynomial degree and high\nsensitivity on small number of variables (upto 10). In this process, borrowing\nan idea from complexity theory, we show that one can implement resilient\nBoolean functions on a large number of variables with linear size and\nlogarithmic depth. Finally, we extend the notion of sensitivity to higher order\nand note that the existing construction idea of Nisan and Szegedy (1994) can\nprovide only constant higher order sensitivity when aiming for polynomial\ndegree of $n-\\omega(1)$. In this direction, we present a construction with low\n($n-\\omega(1)$) polynomial degree and super-constant $\\omega(1)$ order\nsensitivity exploiting Maiorana-McFarland constructions, that we borrow from\nconstruction of resilient functions. The questions we raise identify novel\ncombinatorial problems in the domain of Boolean functions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 13:02:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Maitra", "Subhamoy", ""], ["Mukherjee", "Chandra Sekhar", ""], ["Stanica", "Pantelimon", ""], ["Tang", "Deng", ""]]}, {"id": "2107.11623", "submitter": "Naresh Goud Boddu", "authors": "Naresh Goud Boddu, Rahul Jain and Han-Hsuan Lin", "title": "On relating one-way classical and quantum communication complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $f: X \\times Y \\rightarrow \\{0,1,\\bot \\}$ be a partial function and $\\mu$\nbe a distribution with support contained in $f^{-1}(0) \\cup f^{-1}(1)$. Let\n$\\mathsf{D}^{1,\\mu}_\\epsilon(f)$ be the classical one-way communication\ncomplexity of $f$ with average error under $\\mu$ at most $\\epsilon$,\n$\\mathsf{Q}^{1,\\mu}_\\epsilon(f)$ be the quantum one-way communication\ncomplexity of $f$ with average error under $\\mu$ at most $\\epsilon$ and\n$\\mathsf{Q}^{1,\\mu, *}_\\epsilon(f)$ be the entanglement assisted one-way\ncommunication complexity of $f$ with average error under $\\mu$ at most\n$\\epsilon$. We show:\n  1. If $\\mu$ is a product distribution, then $\\forall \\epsilon, \\eta > 0$,\n$$\\mathsf{D}^{1,\\mu}_{2\\epsilon + \\eta}(f) \\leq \\mathsf{Q}^{1,\\mu,\n*}_{\\epsilon}(f) /\\eta+O\\bigl(\\log(\\mathsf{Q}^{1,\\mu,\n*}_{\\epsilon}(f))/\\eta\\bigr).$$\n  2. If $\\mu$ is a non-product distribution, then $\\forall \\epsilon, \\eta > 0$\nsuch that $\\epsilon/\\eta + \\eta < 0.5$, $$\\mathsf{D}^{1,\\mu}_{3\\eta}(f) =\nO(\\mathsf{Q}^{1,\\mu}_{{\\epsilon}}(f) \\cdot \\mathsf{CS}(f)/\\eta^4)\\enspace,$$\nwhere \\[\\mathsf{CS}(f) = \\max_{y} \\min_{z\\in\\{0,1\\}} \\{ \\vert \\{x~|~f(x,y)=z\\}\n\\vert\\} \\enspace.\\]\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 14:35:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Boddu", "Naresh Goud", ""], ["Jain", "Rahul", ""], ["Lin", "Han-Hsuan", ""]]}, {"id": "2107.11663", "submitter": "Marcus Schaefer", "authors": "Marcus Schaefer", "title": "RAC-drawability is $\\exists\\mathbb{R}$-complete", "comments": "Appears in the Proceedings of the 29th International Symposium on\n  Graph Drawing and Network Visualization (GD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A RAC-drawing of a graph is a straight-line drawing in which every crossing\noccurs at a right-angle. We show that deciding whether a graph has a\nRAC-drawing is as hard as the existential theory of the reals, even if we know\nthat every edge is involved in at most ten crossings and even if the drawing is\nspecified up to isomorphism.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 17:54:32 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 17:34:54 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Schaefer", "Marcus", ""]]}, {"id": "2107.11784", "submitter": "Tapani Toivonen Dr.", "authors": "Tapani Toivonen", "title": "Power of human-algorithm collaboration in solving combinatorial\n  optimization problems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many combinatorial optimization problems are often considered intractable to\nsolve exactly or by approximation. An example of such problem is maximum clique\nwhich -- under standard assumptions in complexity theory -- cannot be solved in\nsub-exponential time or be approximated within polynomial factor efficiently.\nWe show that if a polynomial time algorithm can query informative Gaussian\npriors from an expert $poly(n)$ times, then a class of combinatorial\noptimization problems can be solved efficiently in expectation up to a\nmultiplicative factor $\\epsilon$ where $\\epsilon$ is arbitrary constant. While\nour proposed methods are merely theoretical, they cast new light on how to\napproach solving these problems that have been usually considered intractable.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:21:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Toivonen", "Tapani", ""]]}, {"id": "2107.11806", "submitter": "Nikhil Mande", "authors": "Nikhil S. Mande, Ronald de Wolf", "title": "Tight Bounds for the Randomized and Quantum Communication Complexities\n  of Equality with Small Error", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the randomized and quantum communication complexities of the\nwell-studied Equality function with small error probability $\\epsilon$, getting\nthe optimal constant factors in the leading terms in a number of different\nmodels.\n  In the randomized model,\n  1) we give a general technique to convert public-coin protocols to\nprivate-coin protocols by incurring a small multiplicative error, at a small\nadditive cost. This is an improvement over Newman's theorem [Inf. Proc.\nLet.'91] in the dependence on the error parameter.\n  2) Using this we obtain a $(\\log(n/\\epsilon^2)+4)$-cost private-coin\ncommunication protocol that computes the $n$-bit Equality function, to error\n$\\epsilon$. This improves upon the $\\log(n/\\epsilon^3)+O(1)$ upper bound\nimplied by Newman's theorem, and matches the best known lower bound, which\nfollows from Alon [Comb. Prob. Comput.'09], up to an additive\n$\\log\\log(1/\\epsilon)+O(1)$.\n  In the quantum model,\n  1) we exhibit a one-way protocol of cost $\\log(n/\\epsilon)+4$, that uses only\npure states and computes the $n$-bit Equality function to error $\\epsilon$.\nThis bound was implicitly already shown by Nayak [PhD thesis'99].\n  2) We show that any $\\epsilon$-error one-way protocol for $n$-bit Equality\nthat uses only pure states communicates at least\n$\\log(n/\\epsilon)-\\log\\log(1/\\epsilon)-O(1)$ qubits.\n  3) We exhibit a one-way protocol of cost $\\log(\\sqrt{n}/\\epsilon)+3$, that\nuses mixed states and computes the $n$-bit Equality function to error\n$\\epsilon$. This is also tight up to an additive $\\log\\log(1/\\epsilon)+O(1)$,\nwhich follows from Alon's result.\n  Our upper bounds also yield upper bounds on the approximate rank and related\nmeasures of the Identity matrix. This also implies improved upper bounds on\nthese measures for the distributed SINK function, which was recently used to\nrefute the randomized and quantum versions of the log-rank conjecture.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 13:52:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mande", "Nikhil S.", ""], ["de Wolf", "Ronald", ""]]}, {"id": "2107.11886", "submitter": "Jay Mardia", "authors": "Jay Mardia", "title": "Logspace Reducibility From Secret Leakage Planted Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The planted clique problem is well-studied in the context of observing,\nexplaining, and predicting interesting computational phenomena associated with\nstatistical problems. When equating computational efficiency with the existence\nof polynomial time algorithms, the computational hardness of (some variant of)\nthe planted clique problem can be used to infer the computational hardness of a\nhost of other statistical problems.\n  Is this ability to transfer computational hardness from (some variant of) the\nplanted clique problem to other statistical problems robust to changing our\nnotion of computational efficiency to space efficiency?\n  We answer this question affirmatively for three different statistical\nproblems, namely Sparse PCA, submatrix detection, and testing almost k-wise\nindependence. The key challenge is that space efficient randomized reductions\nneed to repeatedly access the randomness they use. Known reductions to these\nproblems are all randomized and need polynomially many random bits to\nimplement. Since we can not store polynomially many random bits in memory, it\nis unclear how to implement these existing reductions space efficiently. There\nare two ideas involved in circumventing this issue and implementing known\nreductions to these problems space efficiently.\n  1. When solving statistical problems, we can use parts of the input itself as\nrandomness.\n  2. Secret leakage variants of the planted clique problem with appropriate\nsecret leakage can be more useful than the standard planted clique problem when\nwe want to use parts of the input as randomness.\n  (abstract shortened due to arxiv constraints)\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:33:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mardia", "Jay", ""]]}, {"id": "2107.12267", "submitter": "Amer Mouawad", "authors": "Alexandre Cooper, Stephanie Maaz, Amer E.Mouawad, Naomi Nishimura", "title": "Parameterized complexity of reconfiguration of atoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work is motivated by the challenges presented in preparing arrays of\natoms for use in quantum simulation. The recently-developed process of loading\natoms into traps results in approximately half of the traps being filled. To\nconsolidate the atoms so that they form a dense and regular arrangement, such\nas all locations in a grid, atoms are rearranged using moving optical tweezers.\nTime is of the essence, as the longer that the process takes and the more that\natoms are moved, the higher the chance that atoms will be lost in the process.\n  Viewed as a problem on graphs, we wish to solve the problem of reconfiguring\none arrangement of tokens (representing atoms) to another using as few moves as\npossible. Because the problem is NP-complete on general graphs as well as on\ngrids, we focus on the parameterized complexity for various parameters,\nconsidering both undirected and directed graphs, and tokens with and without\nlabels. For unlabelled tokens, the problem is in FPT when parameterizing by the\nnumber of tokens, the number of moves, or the number of moves plus the number\nof vertices without tokens in either the source or target configuration, but\nintractable when parameterizing by the difference between the number of moves\nand the number of differences in the placement of tokens in the source and\ntarget configurations. When labels are added to tokens, however, most of the\ntractability results are replaced by hardness results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:16:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cooper", "Alexandre", ""], ["Maaz", "Stephanie", ""], ["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""]]}, {"id": "2107.12973", "submitter": "Kshitij Gajjar", "authors": "Henning Fernau and Kshitij Gajjar", "title": "The Space Complexity of Sum Labelling", "comments": "24 pages, to be presented at FCT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A graph is called a sum graph if its vertices can be labelled by distinct\npositive integers such that there is an edge between two vertices if and only\nif the sum of their labels is the label of another vertex of the graph. Most\npapers on sum graphs consider combinatorial questions like the minimum number\nof isolated vertices that need to be added to a given graph to make it a sum\ngraph. In this paper, we initiate the study of sum graphs from the viewpoint of\ncomputational complexity. Notice that every $n$-vertex sum graph can be\nrepresented by a sorted list of $n$ positive integers where edge queries can be\nanswered in $O(\\log n)$ time. Therefore, limiting the size of the vertex labels\nalso upper-bounds the space complexity of storing the graph in the database.\n  We show that every $n$-vertex, $m$-edge, $d$-degenerate graph can be made a\nsum graph by adding at most $m$ isolated vertices to it, such that the size of\neach vertex label is at most $O(n^2d)$. This enables us to store the graph\nusing $O(m\\log n)$ bits of memory. For sparse graphs (graphs with $O(n)$\nedges), this matches the trivial lower bound of $\\Omega(n\\log n)$. Since planar\ngraphs and forests have constant degeneracy, our result implies an upper bound\nof $O(n^2)$ on their label size. The previously best known upper bound on the\nlabel size of general graphs with the minimum number of isolated vertices was\n$O(4^n)$, due to Kratochv\\'il, Miller & Nguyen. Furthermore, their proof was\nexistential, whereas our labelling can be constructed in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:34:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Fernau", "Henning", ""], ["Gajjar", "Kshitij", ""]]}, {"id": "2107.13282", "submitter": "Pierre Cazals", "authors": "Bazgan Cristina, Casel Katrin, Cazals Pierre", "title": "Dense Graph Partitioning on sparse and dense graphs", "comments": "18 pages, 6 figures, submission to ISAAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of partitioning a graph into a non-fixed number of\nnon-overlapping subgraphs of maximum density. The density of a partition is the\nsum of the densities of the subgraphs, where the density of the subgraph is the\nratio of its number of edges and its number of vertices. This problem, called\nDense Graph Partition, is known to be NP-hard on general graphs and\npolynomial-time solvable on trees, and polynomial-time 2-approximable.\n  In this paper we study the restriction of Dense Graph Partition to particular\nsparse and dense graph classes. In particular, we prove that it is NP-hard on\ndense bipartite graphs as well as on cubic graphs. On dense bipartite graphs,\nwe further show that it is W[2]-hard parameterized by the number of sets in the\noptimum solution. On dense graphs on $n$ vertices, it is polynomial-time\nsolvable on graphs with minimum degree $n-3$ and NP-hard on $(n-4)$-regular\ngraphs. We prove that it is polynomial-time $4/3$-approximable on cubic graphs\nand admits an efficient polynomial-time approximation scheme on $(n-4)$-regular\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:22:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Cristina", "Bazgan", ""], ["Katrin", "Casel", ""], ["Pierre", "Cazals", ""]]}, {"id": "2107.13809", "submitter": "Alexey Barsukov", "authors": "Alexey Barsukov and Mamadou Moustapha Kant\\'e", "title": "Generalisations of Matrix Partitions : Complexity and Obstructions", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trigraph is a graph where each pair of vertices is labelled either 0 (a\nnon-edge), 1 (an edge) or $\\star$ (both an edge and a non-edge). In a series of\npapers, Hell et al. proposed to study the complexity of homomorphisms from\ngraphs to trigraphs, called Matrix Partition Problems, where edges and\nnon-edges can be both mapped to $\\star$-edges, while a non-edge cannot be\nmapped to an edge, and vice-versa. Even though, Matrix Partition Problems are\ngeneralisations of CSPs, they share with them the property of being\n\"intrinsically\" combinatorial. So, the question of a possible dichotomy, i.e.\nP-time vs NP-complete, is a very natural one and raised in Hell et al.'s\npapers. We propose in this paper to study Matrix Partition Problems on\nrelational structures, wrt a dichotomy question, and, in particular,\nhomomorphisms between trigraphs. We first show that trigraph homomorphisms and\nMatrix Partition Problems are P-time equivalent, and then prove that one can\nalso restrict (wrt dichotomy) to relational structures with one relation.\nFailing in proving that Matrix Partition Problems on directed graphs are not\nP-time equivalent to Matrix Partitions on relational structures, we give some\nevidence that it is unlikely by showing that reductions used in the case of\nCSPs cannot work. We turn then our attention to Matrix Partitions with finite\nsets of obstructions. We show that, for a fixed trigraph, the set of\ninclusion-wise minimal obstructions is finite for directed graphs if and only\nif it is finite for trigraphs. We also prove similar results for relational\nstructures. We conclude by showing that on trees (seen as trigraphs) it is\nNP-complete to decide whether a given tree has a trigraph homomorphism to\nanother input trigraph. The latter shows a notable difference on tractability\nbetween CSP and Matrix Partition as it is well-known that CSP is tractable on\nthe class of trees.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:13:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Barsukov", "Alexey", ""], ["Kant\u00e9", "Mamadou Moustapha", ""]]}, {"id": "2107.14126", "submitter": "Michail Theofilatos", "authors": "George B. Mertzios, Othon Michail, George Skretas, Paul G. Spirakis,\n  Michail Theofilatos", "title": "The Complexity of Growing a Graph", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by biological processes, we introduce here the model of growing\ngraphs, a new model of highly dynamic networks. Such networks have as nodes\nentities that can self-replicate and thus can expand the size of the network.\nThis gives rise to the problem of creating a target network $G$ starting from a\nsingle entity (node). To properly model this, we assume that every node $u$ can\ngenerate at most one node $v$ at every round (or time slot), and every\ngenerated node $v$ can activate edges with other nodes only at the time of its\nbirth, provided that these nodes are up to a small distance $d$ away from $v$.\nWe show that the most interesting case is when the distance is $d=2$. Edge\ndeletions are allowed at any time slot. This creates a natural balance between\nhow fast (time) and how efficiently (number of deleted edges) a target network\ncan be generated. A central question here is, given a target network $G$ of $n$\nnodes, can $G$ be constructed in the model of growing graphs in at most $k$\ntime slots and with at most $\\ell$ excess edges (i.e., auxiliary edges $\\notin\nE(G)$ that are activated and later deleted)? We consider here both centralized\nand distributed algorithms for such questions (and also their computational\ncomplexity). Our results include lower bounds based on properties of the target\nnetwork and algorithms for general graph classes that try to balance speed and\nefficiency. We then show that the optimal number of time slots to construct an\ninput target graph with zero-waste (i.e., no edge deletions allowed), is hard\neven to approximate within $n^{1-\\varepsilon}$, for any $\\varepsilon>0$, unless\nP=NP. On the contrary, the question of the feasibility of constructing a given\ntarget graph in $\\log n$ time slots and zero-waste, can be answered in\npolynomial time. Finally, we initiate a discussion on possible extensions for\nthis model for a distributed setting.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:59:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Mertzios", "George B.", ""], ["Michail", "Othon", ""], ["Skretas", "George", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}]