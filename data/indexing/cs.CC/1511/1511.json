[{"id": "1511.00075", "submitter": "Bingkai Lin", "authors": "Yijia Chen and Bingkai Lin", "title": "The Constant Inapproximability of the Parameterized Dominating Set\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that there is no fpt-algorithm that can approximate the dominating\nset problem with any constant ratio, unless FPT= W[1]. Our hardness reduction\nis built on the second author's recent W[1]-hardness proof of the biclique\nproblem. This yields, among other things, a proof without the PCP machinery\nthat the classical dominating set problem has no polynomial time constant\napproximation under the exponential time hypothesis.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 05:06:17 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 08:08:21 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Chen", "Yijia", ""], ["Lin", "Bingkai", ""]]}, {"id": "1511.00151", "submitter": "Eugene Luks", "authors": "Eugene M. Luks", "title": "Group Isomorphism with Fixed Subnormal Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, Rosenbaum and Wagner showed that isomorphism of explicitly\nlisted $p$-groups of order $n$ could be tested in $n^{\\frac{1}{2}\\log_p n +\nO(p)}$ time, roughly a square root of the classical bound. The $O(p)$ term is\nentirely due to an $n^{O(p)}$ cost of testing for isomorphisms that match fixed\ncomposition series in the two groups. We focus here on the\nfixed-composition-series subproblem and exhibit a polynomial-time algorithm\nthat is valid for general groups. A subsequent paper will construct canonical\nforms within the same time bound.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 17:06:21 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Luks", "Eugene M.", ""]]}, {"id": "1511.00382", "submitter": "Steven Heilman", "authors": "Steven Heilman", "title": "Low Correlation Noise Stability of Symmetric Sets", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gaussian noise stability of subsets A of Euclidean space\nsatisfying A=-A. It is shown that an interval centered at the origin, or its\ncomplement, maximizes noise stability for small correlation, among symmetric\nsubsets of the real line of fixed Gaussian measure. On the other hand, in\ndimension two and higher, the ball or its complement does not always maximize\nnoise stability among symmetric sets of fixed Gaussian measure. In summary, we\nprovide the first known positive and negative results for the Symmetric\nGaussian Problem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 05:29:16 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 05:27:03 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 21:55:33 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2016 04:03:39 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Heilman", "Steven", ""]]}, {"id": "1511.00442", "submitter": "Neil Lutz", "authors": "Jack H. Lutz and Neil Lutz", "title": "Algorithmic information, plane Kakeya sets, and conditional dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the conditional Kolmogorov complexity of x given y at precision\nr, where x and y are points in Euclidean spaces and r is a natural number. We\ndemonstrate the utility of this notion in two ways.\n  1. We prove a point-to-set principle that enables one to use the\n(relativized, constructive) dimension of a single point in a set E in a\nEuclidean space to establish a lower bound on the (classical) Hausdorff\ndimension of E. We then use this principle, together with conditional\nKolmogorov complexity in Euclidean spaces, to give a new proof of the known,\ntwo-dimensional case of the Kakeya conjecture. This theorem of geometric\nmeasure theory, proved by Davies in 1971, says that every plane set containing\na unit line segment in every direction has Hausdorff dimension 2.\n  2. We use conditional Kolmogorov complexity in Euclidean spaces to develop\nthe lower and upper conditional dimensions dim(x|y) and Dim(x|y) of x given y,\nwhere x and y are points in Euclidean spaces. Intuitively these are the lower\nand upper asymptotic algorithmic information densities of x conditioned on the\ninformation in y. We prove that these conditional dimensions are robust and\nthat they have the correct information-theoretic relationships with the\nwell-studied dimensions dim(x) and Dim(x) and the mutual dimensions mdim(x:y)\nand Mdim(x:y).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 11:04:35 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 02:47:01 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 05:29:38 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 19:16:55 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Lutz", "Jack H.", ""], ["Lutz", "Neil", ""]]}, {"id": "1511.00529", "submitter": "Carlos Gershenson", "authors": "Guillermo Santamar\\'ia-Bonfil, Nelson Fern\\'andez, Carlos Gershenson", "title": "Measuring the Complexity of Continuous Distributions", "comments": "21 pages, 5 Tables, 4 Figures", "journal-ref": "Entropy, 18(3):72. 2016", "doi": "10.3390/e18030072", "report-no": null, "categories": "nlin.AO cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend previously proposed measures of complexity, emergence, and\nself-organization to continuous distributions using differential entropy. This\nallows us to calculate the complexity of phenomena for which distributions are\nknown. We find that a broad range of common parameters found in Gaussian and\nscale-free distributions present high complexity values. We also explore the\nrelationship between our measure of complexity and information adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:47:29 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Santamar\u00eda-Bonfil", "Guillermo", ""], ["Fern\u00e1ndez", "Nelson", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1511.00778", "submitter": "Sitan Chen", "authors": "Sitan Chen", "title": "Basis Collapse for Holographic Algorithms Over All Domain Sizes", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of holographic algorithms introduced by Valiant represents a novel\napproach to achieving polynomial-time algorithms for seemingly intractable\ncounting problems via a reduction to counting planar perfect matchings and a\nlinear change of basis. Two fundamental parameters in holographic algorithms\nare the \\emph{domain size} and the \\emph{basis size}. Roughly, the domain size\nis the range of colors involved in the counting problem at hand (e.g. counting\ngraph $k$-colorings is a problem over domain size $k$), while the basis size\n$\\ell$ captures the dimensionality of the representation of those colors. A\nmajor open problem has been: for a given $k$, what is the smallest $\\ell$ for\nwhich any holographic algorithm for a problem over domain size $k$ \"collapses\nto\" (can be simulated by) a holographic algorithm with basis size $\\ell$? Cai\nand Lu showed in 2008 that over domain size 2, basis size 1 suffices, opening\nthe door to an extensive line of work on the structural theory of holographic\nalgorithms over the Boolean domain. Cai and Fu later showed for signatures of\nfull rank that over domain sizes 3 and 4, basis sizes 1 and 2, respectively,\nsuffice, and they conjectured that over domain size $k$ there is a collapse to\nbasis size $\\lfloor\\log_2 k\\rfloor$. In this work, we resolve this conjecture\nin the affirmative for signatures of full rank for all $k$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 04:51:03 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Chen", "Sitan", ""]]}, {"id": "1511.00785", "submitter": "Bo  Tang", "authors": "Xi Chen, Yu Cheng, Bo Tang", "title": "Well-Supported versus Approximate Nash Equilibria: Query Complexity of\n  Large Games", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the randomized query complexity of approximate Nash equilibria (ANE)\nin large games. We prove that, for some constant $\\epsilon>0$, any randomized\noracle algorithm that computes an $\\epsilon$-ANE in a binary-action, $n$-player\ngame must make $2^{\\Omega(n/\\log n)}$ payoff queries. For the stronger solution\nconcept of well-supported Nash equilibria (WSNE), Babichenko previously gave an\nexponential $2^{\\Omega(n)}$ lower bound for the randomized query complexity of\n$\\epsilon$-WSNE, for some constant $\\epsilon>0$; the same lower bound was shown\nto hold for $\\epsilon$-ANE, but only when $\\epsilon=O(1/n)$.\n  Our result answers an open problem posed by Hart and Nisan and Babichenko and\nis very close to the trivial upper bound of $2^n$. Our proof relies on a\ngeneric reduction from the problem of finding an $\\epsilon$-WSNE to the problem\nof finding an $\\epsilon/(4\\alpha)$-ANE, in large games with $\\alpha$ actions,\nwhich might be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 05:37:37 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Chen", "Xi", ""], ["Cheng", "Yu", ""], ["Tang", "Bo", ""]]}, {"id": "1511.00813", "submitter": "Olivier Bailleux", "authors": "Olivier Bailleux", "title": "SAT as a game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a funny representation of SAT. While the primary interest is to\npresent propositional satisfiability in a playful way for pedagogical purposes,\nit could also inspire new search heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 08:53:01 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Bailleux", "Olivier", ""]]}, {"id": "1511.00900", "submitter": "Joel Rybicki", "authors": "Sebastian Brandt, Orr Fischer, Juho Hirvonen, Barbara Keller, Tuomo\n  Lempi\\\"ainen, Joel Rybicki, Jukka Suomela, Jara Uitto", "title": "A Lower Bound for the Distributed Lov\\'asz Local Lemma", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any randomised Monte Carlo distributed algorithm for the\nLov\\'asz local lemma requires $\\Omega(\\log \\log n)$ communication rounds,\nassuming that it finds a correct assignment with high probability. Our result\nholds even in the special case of $d = O(1)$, where $d$ is the maximum degree\nof the dependency graph. By prior work, there are distributed algorithms for\nthe Lov\\'asz local lemma with a running time of $O(\\log n)$ rounds in\nbounded-degree graphs, and the best lower bound before our work was\n$\\Omega(\\log^* n)$ rounds [Chung et al. 2014].\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 13:24:41 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Brandt", "Sebastian", ""], ["Fischer", "Orr", ""], ["Hirvonen", "Juho", ""], ["Keller", "Barbara", ""], ["Lempi\u00e4inen", "Tuomo", ""], ["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""], ["Uitto", "Jara", ""]]}, {"id": "1511.00984", "submitter": "Arefin Huq", "authors": "Arefin Huq", "title": "Undirected Cat-and-Mouse is P-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cat-and-mouse is a two-player game on a finite graph. Chandra and Stockmeyer\nshowed cat-and-mouse is P-complete on directed graphs. We show cat-and-mouse is\nP-complete on undirected graphs. To our knowledge, no proof of the directed\ncase was ever published. To fill this gap we give a proof for directed graphs\nand extend it to undirected graphs. The proof is a reduction from a variant of\nthe circuit value problem.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 16:59:06 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Huq", "Arefin", ""]]}, {"id": "1511.01211", "submitter": "Dmitry Gavinsky", "authors": "Ralph C. Bottesch, Dmitry Gavinsky, Hartmut Klauck", "title": "Equality, Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new lower bound method for analysing the complexity of the\nEquality function (EQ) in the Simultaneous Message Passing (SMP) model of\ncommunication complexity. The new technique gives tight lower bounds of\n$\\Omega(\\sqrt n)$ for both EQ and its negation NE in the non-deterministic\nversion of quantum-classical SMP, where Merlin is also quantum $-$ this is the\nstrongest known version of SMP where the complexity of both EQ and NE remain\nhigh (previously known techniques seem to be insufficient for this).\n  Besides, our analysis provides to a unified view of the communication\ncomplexity of EQ and NE, allowing to obtain tight characterisation in all\npreviously studied and a few newly introduced versions of SMP, including all\npossible combination of either quantum or randomised Alice, Bob and Merlin in\nthe non-deterministic case.\n  Some of our results highlight that NE is easier than EQ in the presence of\nclassical proofs, whereas the problems have (roughly) the same complexity when\na quantum proof is present.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 05:38:04 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Bottesch", "Ralph C.", ""], ["Gavinsky", "Dmitry", ""], ["Klauck", "Hartmut", ""]]}, {"id": "1511.01230", "submitter": "Mingji Xia", "authors": "Mingji Xia", "title": "Base collapse of holographic algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISCAS-SKLCS-14-20", "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A holographic algorithm solves a problem in domain of size $n$, by reducing\nit to counting perfect matchings in planar graphs. It may simulate a $n$-value\nvariable by a bunch of $t$ matchgate bits, which has $2^t$ values. The\ntransformation in the simulation can be expressed as a $n \\times 2^t$ matrix\n$M$, called the base of the holographic algorithm. We wonder whether more\nmatchgate bits bring us more powerful holographic algorithms. In another word,\nwhether we can solve the same original problem, with a collapsed base of size\n$n \\times 2^{r}$, where $r<t$.\n  Base collapse was discovered for small domain $n=2,3,4$. For $n=3, 4$, the\nbase collapse was proved under the condition that there is a full rank\ngenerator. We prove for any $n$, the base collapse to a $r\\leq \\lfloor \\log n\n\\rfloor$, with some similar conditions. One of them is that the original\nproblem is defined by one symmetric function. In the proof, we utilize\nelementary matchgate transformations instead of matchgate identities.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 07:47:21 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Xia", "Mingji", ""]]}, {"id": "1511.01379", "submitter": "Micha{\\l} Pilipczuk", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Micha{\\l} Pilipczuk, Saket Saurabh,\n  Marcin Wrochna", "title": "Fully polynomial-time parameterized computations for graphs and matrices\n  of low treewidth", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of several fundamental polynomial-time solvable\nproblems on graphs and on matrices, when the given instance has low treewidth;\nin the case of matrices, we consider the treewidth of the graph formed by\nnon-zero entries. In each of the considered cases, the best known algorithms\nworking on general graphs run in polynomial time, however the exponent of the\npolynomial is large. Therefore, our main goal is to construct algorithms with\nrunning time of the form $\\textrm{poly}(k)\\cdot n$ or $\\textrm{poly}(k)\\cdot\nn\\log n$, where $k$ is the width of the tree decomposition given on the input.\nSuch procedures would outperform the best known algorithms for the considered\nproblems already for moderate values of the treewidth, like $O(n^{1/c})$ for\nsome small constant $c$.\n  Our results include:\n  -- an algorithm for computing the determinant and the rank of an $n\\times n$\nmatrix using $O(k^3\\cdot n)$ time and arithmetic operations;\n  -- an algorithm for solving a system of linear equations using $O(k^3\\cdot\nn)$ time and arithmetic operations;\n  -- an $O(k^3\\cdot n\\log n)$-time randomized algorithm for finding the\ncardinality of a maximum matching in a graph;\n  -- an $O(k^4\\cdot n\\log^2 n)$-time randomized algorithm for constructing a\nmaximum matching in a graph;\n  -- an $O(k^2\\cdot n\\log n)$-time algorithm for finding a maximum vertex flow\nin a directed graph.\n  Moreover, we give an approximation algorithm for treewidth with time\ncomplexity suited to the running times as above. Namely, the algorithm, when\ngiven a graph $G$ and integer $k$, runs in time $O(k^7\\cdot n\\log n)$ and\neither correctly reports that the treewidth of $G$ is larger than $k$, or\nconstructs a tree decomposition of $G$ of width $O(k^2)$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 16:06:09 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Pilipczuk", "Micha\u0142", ""], ["Saurabh", "Saket", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1511.01411", "submitter": "Vasilis Syrgkanis", "authors": "Constantinos Daskalakis, Vasilis Syrgkanis", "title": "Learning in Auctions: Regret is Hard, Envy is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of recent work provides welfare guarantees of simple combinatorial\nauction formats, such as selling m items via simultaneous second price auctions\n(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et\nal. 2013). These guarantees hold even when the auctions are repeatedly executed\nand players use no-regret learning algorithms. Unfortunately, off-the-shelf\nno-regret algorithms for these auctions are computationally inefficient as the\nnumber of actions is exponential. We show that this obstacle is insurmountable:\nthere are no polynomial-time no-regret algorithms for SiSPAs, unless\nRP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises\nthe question of how good outcomes polynomially-bounded bidders may discover in\nsuch auctions.\n  To answer this question, we propose a novel concept of learning in auctions,\ntermed \"no-envy learning.\" This notion is founded upon Walrasian equilibrium,\nand we show that it is both efficiently implementable and results in\napproximately optimal welfare, even when the bidders have fractionally\nsubadditive (XOS) valuations (assuming demand oracles) or coverage valuations\n(without demand oracles). No-envy learning outcomes are a relaxation of\nno-regret outcomes, which maintain their approximate welfare optimality while\nendowing them with computational tractability. Our results extend to other\nauction formats that have been studied in the literature via the smoothness\nparadigm.\n  Our results for XOS valuations are enabled by a novel\nFollow-The-Perturbed-Leader algorithm for settings where the number of experts\nis infinite, and the payoff function of the learner is non-linear. This\nalgorithm has applications outside of auction settings, such as in security\ngames. Our result for coverage valuations is based on a novel use of convex\nrounding schemes and a reduction to online convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 17:39:30 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 16:48:48 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2015 19:49:30 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2015 15:29:10 GMT"}, {"version": "v5", "created": "Sat, 19 Dec 2015 17:41:20 GMT"}, {"version": "v6", "created": "Wed, 6 Apr 2016 17:55:25 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1511.01699", "submitter": "Chen Dan", "authors": "Chen Dan, Kristoffer Arnsfelt Hansen, He Jiang, Liwei Wang, Yuchen\n  Zhou", "title": "Low Rank Approximation of Binary Matrices: Column Subset Selection and\n  Generalizations", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank matrix approximation is an important tool in machine learning. Given\na data matrix, low rank approximation helps to find factors, patterns and\nprovides concise representations for the data. Research on low rank\napproximation usually focus on real matrices. However, in many applications\ndata are binary (categorical) rather than continuous. This leads to the problem\nof low rank approximation of binary matrix. Here we are given a $d \\times n$\nbinary matrix $A$ and a small integer $k$. The goal is to find two binary\nmatrices $U$ and $V$ of sizes $d \\times k$ and $k \\times n$ respectively, so\nthat the Frobenius norm of $A - U V$ is minimized. There are two models of this\nproblem, depending on the definition of the dot product of binary vectors: The\n$\\mathrm{GF}(2)$ model and the Boolean semiring model. Unlike low rank\napproximation of real matrix which can be efficiently solved by Singular Value\nDecomposition, approximation of binary matrix is $NP$-hard even for $k=1$.\n  In this paper, we consider the problem of Column Subset Selection (CSS), in\nwhich one low rank matrix must be formed by $k$ columns of the data matrix. We\ncharacterize the approximation ratio of CSS for binary matrices. For $GF(2)$\nmodel, we show the approximation ratio of CSS is bounded by\n$\\frac{k}{2}+1+\\frac{k}{2(2^k-1)}$ and this bound is asymptotically tight. For\nBoolean model, it turns out that CSS is no longer sufficient to obtain a bound.\nWe then develop a Generalized CSS (GCSS) procedure in which the columns of one\nlow rank matrix are generated from Boolean formulas operating bitwise on\ncolumns of the data matrix. We show the approximation ratio of GCSS is bounded\nby $2^{k-1}+1$, and the exponential dependency on $k$ is inherent.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 11:26:52 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 14:47:54 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dan", "Chen", ""], ["Hansen", "Kristoffer Arnsfelt", ""], ["Jiang", "He", ""], ["Wang", "Liwei", ""], ["Zhou", "Yuchen", ""]]}, {"id": "1511.01937", "submitter": "Robin Kothari", "authors": "Scott Aaronson, Shalev Ben-David, Robin Kothari", "title": "Separations in query complexity using cheat sheets", "comments": "v1: 31 pages; subsumes arXiv:1506.08106. v2: Typos and minor bug\n  fixes", "journal-ref": "Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of\n  Computing (STOC 2016), pp. 863-876 (2016)", "doi": "10.1145/2897518.2897644", "report-no": "MIT-CTP #4730", "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a power 2.5 separation between bounded-error randomized and quantum\nquery complexity for a total Boolean function, refuting the widely believed\nconjecture that the best such separation could only be quadratic (from Grover's\nalgorithm). We also present a total function with a power 4 separation between\nquantum query complexity and approximate polynomial degree, showing severe\nlimitations on the power of the polynomial method. Finally, we exhibit a total\nfunction with a quadratic gap between quantum query complexity and certificate\ncomplexity, which is optimal (up to log factors). These separations are shown\nusing a new, general technique that we call the cheat sheet technique. The\ntechnique is based on a generic transformation that converts any (possibly\npartial) function into a new total function with desirable properties for\nshowing separations. The framework also allows many known separations,\nincluding some recent breakthrough results of Ambainis et al., to be shown in a\nunified manner.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 22:31:22 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 17:53:01 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Aaronson", "Scott", ""], ["Ben-David", "Shalev", ""], ["Kothari", "Robin", ""]]}, {"id": "1511.02235", "submitter": "Shelby Kimmel", "authors": "Stacey Jeffery and Shelby Kimmel", "title": "NAND-Trees, Average Choice Complexity, and Effective Resistance", "comments": "This article is superseded by arXiv:1704.00765", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the quantum query complexity of evaluating NAND-tree instances\nwith average choice complexity at most $W$ is $O(W)$, where average choice\ncomplexity is a measure of the difficulty of winning the associated two-player\ngame. This generalizes a superpolynomial speedup over classical query\ncomplexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further\nshow that the player with a winning strategy for the two-player game associated\nwith the NAND-tree can win the game with an expected\n$\\widetilde{O}(N^{1/4}\\sqrt{{\\cal C}(x)})$ quantum queries against a random\nopponent, where ${\\cal C }(x)$ is the average choice complexity of the\ninstance. This gives an improvement over the query complexity of the naive\nstrategy, which costs $\\widetilde{O}(\\sqrt{N})$ queries.\n  The results rely on a connection between NAND-tree evaluation and\n$st$-connectivity problems on certain graphs, and span programs for\n$st$-connectivity problems. Our results follow from relating average choice\ncomplexity to the effective resistance of these graphs, which itself\ncorresponds to the span program witness size.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 21:01:07 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 13:55:19 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Jeffery", "Stacey", ""], ["Kimmel", "Shelby", ""]]}, {"id": "1511.02308", "submitter": "S Raja", "authors": "V. Arvind, S. Raja", "title": "Some Lower Bound Results for Set-Multilinear Arithmetic Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the structure of set-multilinear arithmetic circuits\nand set-multilinear branching programs with the aim of showing lower bound\nresults. We define some natural restrictions of these models for which we are\nable to show lower bound results. Some of our results extend existing lower\nbounds, while others are new and raise open questions. More specifically, our\nmain results are the following:\n  (1) We observe that set-multilinear arithmetic circuits can be transformed\ninto shallow set-multilinear circuits efficiently, similar to depth reduction\nresults of [VSBR83,RY08] for more general commutative circuits. As a\nconsequence, we note that polynomial size set-multilinear circuits have\nquasi-polynomial size set-multilinear branching programs. We show that\n\\emph{narrow} set-multilinear ABPs (with a restricted number of set types)\ncomputing the Permanent polynomial $\\mathrm{PER}_n$ require $2^{n^{\\Omega(1)}}$\nsize. A similar result for general set-multilinear ABPs appears difficult as it\nwould imply that the Permanent requires superpolynomial size set-multilinear\ncircuits. It would also imply that the noncommutative Permanent requires\nsuperpolynomial size noncommutative arithmetic circuits.\n  (2) Indeed, we also show that set-multilinear branching programs are\nexponentially more powerful than \\emph{interval} multilinear circuits (where\nthe index sets for each gate is restricted to be an interval w.r.t.\\ some\nordering), assuming the sum-of-squares conjecture. This further underlines the\npower of set-multilinear branching programs.\n  (3) Finally, we consider set-multilinear circuits with restrictions on the\nnumber of proof trees of monomials computed by it, and prove exponential lower\nbounds results. This raises some new lower bound questions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 05:56:57 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Arvind", "V.", ""], ["Raja", "S.", ""]]}, {"id": "1511.02321", "submitter": "Radu Curticapean", "authors": "Radu Curticapean, Mingji Xia", "title": "Parameterizing the Permanent: Genus, Apices, Minors, Evaluation mod 2^k", "comments": "35 pages, appears in FOCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify and study relevant structural parameters for the problem\nPerfMatch of counting perfect matchings in a given input graph $G$. These\ngeneralize the well-known tractable planar case, and they include the genus of\n$G$, its apex number (the minimum number of vertices whose removal renders $G$\nplanar), and its Hadwiger number (the size of a largest clique minor).\n  To study these parameters, we first introduce the notion of combined\nmatchgates, a general technique that bridges parameterized counting problems\nand the theory of so-called Holants and matchgates: Using combined matchgates,\nwe can simulate certain non-existing gadgets $F$ as linear combinations of\n$t=O(1)$ existing gadgets. If a graph $G$ features $k$ occurrences of $F$, we\ncan then reduce $G$ to $t^k$ graphs that feature only existing gadgets, thus\nenabling parameterized reductions.\n  As applications of this technique, we simplify known $4^g n^{O(1)}$ time\nalgorithms for PerfMatch on graphs of genus $g$. Orthogonally to this, we show\n#W[1]-hardness of the permanent on $k$-apex graphs, implying its #W[1]-hardness\nunder the Hadwiger number. Additionally, we rule out $n^{o(k/\\log k)}$ time\nalgorithms under the counting exponential-time hypothesis #ETH.\n  Finally, we use combined matchgates to prove parity-W[1]-hardness of\nevaluating the permanent modulo $2^k$, complementing an $O(n^{4k-3})$ time\nalgorithm by Valiant and answering an open question of Bj\\\"orklund. We also\nobtain a lower bound of $n^{\\Omega(k/\\log k)}$ under the parity version of the\nexponential-time hypothesis.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 08:28:25 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Curticapean", "Radu", ""], ["Xia", "Mingji", ""]]}, {"id": "1511.02454", "submitter": "Liron Yedidsion", "authors": "Tamar Cohen, Liron Yedidsion", "title": "The Periodic Joint Replenishment Problem is Strongly NP-Hard", "comments": "29 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the long-standing open question regarding the\ncomputational complexity of one of the core problems in supply chains\nmanagement, the periodic joint replenishment problem. This problem has received\na lot of attention over the years and many heuristic and approximation\nalgorithms were suggested. However, in spite of the vast effort, the complexity\nof the problem remained unresolved. In this paper, we provide a proof that the\nproblem is indeed strongly NP-hard.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 08:00:51 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Cohen", "Tamar", ""], ["Yedidsion", "Liron", ""]]}, {"id": "1511.02910", "submitter": "Radu Curticapean", "authors": "Radu Curticapean", "title": "Block Interpolation: A Framework for Tight Exponential-Time Counting\n  Complexity", "comments": "20 pages, added explanations and references to subsequent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a framework for proving tight lower bounds under the counting\nexponential-time hypothesis #ETH introduced by Dell et al. (ACM Transactions on\nAlgorithms, 2014). Our framework allows us to convert classical #P-hardness\nresults for counting problems into tight lower bounds under #ETH, thus ruling\nout algorithms with running time $2^{o(n)}$ on graphs with $n$ vertices and\n$O(n)$ edges. As exemplary applications of this framework, we obtain tight\nlower bounds under #ETH for the evaluation of the zero-one permanent, the\nmatching polynomial, and the Tutte polynomial on all non-easy points except for\none line. This remaining line was settled very recently by Brand et al. (IPEC\n2016).\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:12:58 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:18:26 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Curticapean", "Radu", ""]]}, {"id": "1511.02927", "submitter": "Christian Ikenmeyer", "authors": "Peter B\\\"urgisser and Christian Ikenmeyer", "title": "Fundamental invariants of orbit closures", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several objects of interest in geometric complexity theory, namely for\nthe determinant, the permanent, the product of variables, the power sum, the\nunit tensor, and the matrix multiplication tensor, we introduce and study a\nfundamental SL-invariant function that relates the coordinate ring of the orbit\nwith the coordinate ring of its closure. For the power sums we can write down\nthis fundamental invariant explicitly in most cases. Our constructions\ngeneralize the two Aronhold invariants on ternary cubics. For the other objects\nwe identify the invariant function conditional on intriguing combinatorial\nproblems much like the well-known Alon-Tarsi conjecture on Latin squares. We\nprovide computer calculations in small dimensions for these cases. As a main\ntool for our analysis, we determine the stabilizers, and we establish the\npolystability of all the mentioned forms and tensors (including the generic\nones).\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 23:30:56 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 22:47:08 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Ikenmeyer", "Christian", ""]]}, {"id": "1511.03403", "submitter": "Shmuel Onn", "authors": "Shmuel Onn", "title": "Huge tables and multicommodity flows are fixed parameter tractable via\n  unimodular integer Caratheodory", "comments": null, "journal-ref": "Journal of Computer and System Sciences, 83: 207-214 (2017)", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-way table problem is to decide if there exists an l x m x n table\nsatisfying given line sums, and find a table if there is one. It is NP-complete\nalready for l=3 and every bounded integer program can be isomorphically\nrepresented in polynomial time for some m and n as some 3 x m x n table\nproblem. Recently, the problem was shown to be fixed-parameter tractable with\nparameters l,m. Here we extend this and show that the huge version of the\nproblem, where the variable side n is a huge number encoded in binary, is also\nfixed-parameter tractable with parameters l,m. We also conclude that the huge\nmulticommodity flow problem with m suppliers and a huge number n of consumers\nis fixed-parameter tractable parameterized by the numbers of commodities and\nconsumer types.\n  One of our tools is a theorem about unimodular monoids which is of interest\non its own right. The monoid problem is to decide if a given integer vector is\na finite nonnegative integer combination of a given set of integer vectors, and\nfind such a decomposition if one exists. We consider sets given implicitly by\nan inequality system. For such sets, it was recently shown that in fixed\ndimension the problem is solvable in polynomial time with degree which is\nexponential in the dimension. Here we show that when the inequality system\nwhich defines the set is defined by a totally unimodular matrix, the monoid\nproblem can be solved in polynomial time even in variable dimension.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:10:10 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 10:45:51 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 13:53:30 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Onn", "Shmuel", ""]]}, {"id": "1511.03552", "submitter": "Laszlo Kish", "authors": "Bruce Zhang, Laszlo B. Kish, Claes-Goran Granqvist", "title": "Drawing from hats by noise-based logic", "comments": "Accepted for Publication in the International Journal of Parallel,\n  Emergent and Distributed Systems. December 17, 2015", "journal-ref": null, "doi": "10.1080/17445760.2016.1140168", "report-no": null, "categories": "cs.ET cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We utilize the asymmetric random telegraph wave-based instantaneous\nnoise-base logic scheme to represent the problem of drawing numbers from a hat,\nand we consider two identical hats with the first 2^N integer numbers. In the\nfirst problem, Alice secretly draws an arbitrary number from one of the hats,\nand Bob must find out which hat is missing a number. In the second problem,\nAlice removes a known number from one of the hats and another known number from\nthe other hat, and Bob must identify these hats. We show that, when the\npreparation of the hats with the numbers is accounted for, the noise-based\nlogic scheme always provides an exponential speed-up and/or it requires\nexponentially smaller computational complexity than deterministic alternatives.\nBoth the stochasticity and the ability to superpose numbers are essential\ncomponents of the exponential improvement.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 16:12:07 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 15:35:37 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 15:04:24 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Zhang", "Bruce", ""], ["Kish", "Laszlo B.", ""], ["Granqvist", "Claes-Goran", ""]]}, {"id": "1511.03675", "submitter": "Michael Walter", "authors": "Peter B\\\"urgisser and Matthias Christandl and Ketan D. Mulmuley and\n  Michael Walter", "title": "Membership in moment polytopes is in NP and coNP", "comments": "20 pages", "journal-ref": "SIAM J. Comput., 46 (3), 972-991 (2017)", "doi": "10.1137/15M1048859", "report-no": null, "categories": "cs.CC math-ph math.MP math.RT math.SG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of deciding membership in the moment polytope\nassociated with a finite-dimensional unitary representation of a compact,\nconnected Lie group is in NP and coNP. This is the first non-trivial result on\nthe computational complexity of this problem, which naively amounts to a\nquadratically-constrained program. Our result applies in particular to the\nKronecker polytopes, and therefore to the problem of deciding positivity of the\nstretched Kronecker coefficients. In contrast, it has recently been shown that\ndeciding positivity of a single Kronecker coefficient is NP-hard in general\n[Ikenmeyer, Mulmuley and Walter, arXiv:1507.02955]. We discuss the consequences\nof our work in the context of complexity theory and the quantum marginal\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:00:56 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 20:03:41 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Christandl", "Matthias", ""], ["Mulmuley", "Ketan D.", ""], ["Walter", "Michael", ""]]}, {"id": "1511.03730", "submitter": "Ankit Garg", "authors": "Ankit Garg and Leonid Gurvits and Rafael Oliveira and Avi Wigderson", "title": "Operator scaling: theory and applications", "comments": "Fixed minor typos and bugs. Final version to be published in FoCM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AC math.AG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deterministic polynomial time algorithm for\ntesting if a symbolic matrix in non-commuting variables over $\\mathbb{Q}$ is\ninvertible or not. The analogous question for commuting variables is the\ncelebrated polynomial identity testing (PIT) for symbolic determinants. In\ncontrast to the commutative case, which has an efficient probabilistic\nalgorithm, the best previous algorithm for the non-commutative setting required\nexponential time (whether or not randomization is allowed). The algorithm\nefficiently solves the \"word problem\" for the free skew field, and the identity\ntesting problem for arithmetic formulae with division over non-commuting\nvariables, two problems which had only exponential-time algorithms prior to\nthis work.\n  The main contribution of this paper is a complexity analysis of an existing\nalgorithm due to Gurvits, who proved it was polynomial time for certain classes\nof inputs. We prove it always runs in polynomial time. The main component of\nour analysis is a simple (given the necessary known tools) lower bound on\ncentral notion of capacity of operators (introduced by Gurvits). We extend the\nalgorithm to actually approximate capacity to any accuracy in polynomial time,\nand use this analysis to give quantitative bounds on the continuity of capacity\n(the latter is used in a subsequent paper on Brascamp-Lieb inequalities).\n  Symbolic matrices in non-commuting variables, and the related structural and\nalgorithmic questions, have a remarkable number of diverse origins and\nmotivations. They arise independently in (commutative) invariant theory and\nrepresentation theory, linear algebra, optimization, linear system theory,\nquantum information theory, approximation of the permanent and naturally in\nnon-commutative algebra. We provide a detailed account of some of these sources\nand their interconnections.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 23:30:07 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 21:20:20 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 21:45:36 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 02:34:16 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Garg", "Ankit", ""], ["Gurvits", "Leonid", ""], ["Oliveira", "Rafael", ""], ["Wigderson", "Avi", ""]]}, {"id": "1511.04385", "submitter": "Fr\\'ed\\'eric Grosshans", "authors": "Fr\\'ed\\'eric Grosshans, Thomas Lawson, Fran\\c{c}ois Morain, Benjamin\n  Smith", "title": "Factoring Safe Semiprimes with a Single Quantum Query", "comments": "v2 : Typo correction and rewriting for improved clarity v3 : Slight\n  expansion, for improved clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shor's factoring algorithm (SFA), by its ability to efficiently factor large\nnumbers, has the potential to undermine contemporary encryption. At its heart\nis a process called order finding, which quantum mechanics lets us perform\nefficiently. SFA thus consists of a \\emph{quantum order finding algorithm}\n(QOFA), bookended by classical routines which, given the order, return the\nfactors. But, with probability up to $1/2$, these classical routines fail, and\nQOFA must be rerun. We modify these routines using elementary results in number\ntheory, improving the likelihood that they return the factors.\n  The resulting quantum factoring algorithm is better than SFA at factoring\nsafe semiprimes, an important class of numbers used in cryptography. With just\none call to QOFA, our algorithm almost always factors safe semiprimes. As well\nas a speed-up, improving efficiency gives our algorithm other, practical\nadvantages: unlike SFA, it does not need a randomly picked input, making it\nsimpler to construct in the lab; and in the (unlikely) case of failure, the\nsame circuit can be rerun, without modification.\n  We consider generalizing this result to other cases, although we do not find\na simple extension, and conclude that SFA is still the best algorithm for\ngeneral numbers (non safe semiprimes, in other words). Even so, we present some\nsimple number theoretic tricks for improving SFA in this case.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:14:37 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 12:04:53 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 17:45:06 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Grosshans", "Fr\u00e9d\u00e9ric", ""], ["Lawson", "Thomas", ""], ["Morain", "Fran\u00e7ois", ""], ["Smith", "Benjamin", ""]]}, {"id": "1511.04731", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang", "title": "Hardness of RNA Folding Problem with Four Symbols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An RNA sequence is a string composed of four types of nucleotides, $A, C, G$,\nand $U$. The goal of the RNA folding problem is to find a maximum cardinality\nset of crossing-free pairs of the form $\\{A,U\\}$ or $\\{C,G\\}$ in a given RNA\nsequence. The problem is central in bioinformatics and has received much\nattention over the years. Abboud, Backurs, and Williams (FOCS 2015)\ndemonstrated a conditional lower bound for a generalized version of the RNA\nfolding problem based on a conjectured hardness of the $k$-clique problem.\nTheir lower bound requires the RNA sequence to have at least 36 types of\nsymbols, making the result not applicable to the RNA folding problem in real\nlife (i.e., alphabet size 4). In this paper, we present an improved lower bound\nthat works for the alphabet size 4 case.\n  We also investigate the Dyck edit distance problem, which is a string problem\nclosely related to RNA folding. We demonstrate a reduction from RNA folding to\nDyck edit distance with alphabet size 10. This leads to a much simpler proof of\nthe conditional lower bound for Dyck edit distance problem given by Abboud,\nBackurs, and Williams (FOCS 2015), and lowers the alphabet size requirement.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 17:09:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 23:28:29 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 14:47:42 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Chang", "Yi-Jun", ""]]}, {"id": "1511.05006", "submitter": "Samuel Epstein", "authors": "Samuel Epstein", "title": "An Extended Coding Theorem with Application to Quantum Complexities", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new inequality in algorithmic information theory that\ncan be seen as an extended coding theorem. This inequality has applications in\nnew bounds between quantum complexity measures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:56:21 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 13:41:47 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 16:52:46 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Epstein", "Samuel", ""]]}, {"id": "1511.05027", "submitter": "Philippe Moser", "authors": "Philippe Moser, Frank Stephan", "title": "Depth, Highness and DNR degrees", "comments": "journal version, dmtcs", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  4, FCT '15, special issue FCT'15 (October 26, 2017) dmtcs:4012", "doi": "10.23638/DMTCS-19-4-2", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bennett deep sequences in the context of recursion theory; in\nparticular we investigate the notions of O(1)-deepK, O(1)-deepC , order-deep K\nand order-deep C sequences. Our main results are that Martin-Loef random sets\nare not order-deepC , that every many-one degree contains a set which is not\nO(1)-deepC , that O(1)-deepC sets and order-deepK sets have high or DNR Turing\ndegree and that no K-trival set is O(1)-deepK.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 16:40:28 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 17:08:43 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Moser", "Philippe", ""], ["Stephan", "Frank", ""]]}, {"id": "1511.05053", "submitter": "Aleksandrs Belovs", "authors": "Aleksandrs Belovs, Eric Blais", "title": "A Polynomial Lower Bound for Testing Monotonicity", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every algorithm for testing $n$-variate Boolean functions for\nmonotonicity must have query complexity $\\tilde{\\Omega}(n^{1/4})$. All previous\nlower bounds for this problem were designed for non-adaptive algorithms and, as\na result, the best previous lower bound for general (possibly adaptive)\nmonotonicity testers was only $\\Omega(\\log n)$. Combined with the query\ncomplexity of the non-adaptive monotonicity tester of Khot, Minzer, and Safra\n(FOCS 2015), our lower bound shows that adaptivity can result in at most a\nquadratic reduction in the query complexity for testing monotonicity.\n  By contrast, we show that there is an exponential gap between the query\ncomplexity of adaptive and non-adaptive algorithms for testing regular linear\nthreshold functions (LTFs) for monotonicity. Chen, De, Servedio, and Tan (STOC\n2015) recently showed that non-adaptive algorithms require almost\n$\\Omega(n^{1/2})$ queries for this task. We introduce a new adaptive\nmonotonicity testing algorithm which has query complexity $O(\\log n)$ when the\ninput is a regular LTF.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:38:59 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Belovs", "Aleksandrs", ""], ["Blais", "Eric", ""]]}, {"id": "1511.05178", "submitter": "Shlomo Jozeph", "authors": "Shlomo Jozeph", "title": "A Note on Almost Perfect Probabilistically Checkable Proofs of Proximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistically checkable proofs of proximity (PCPP) are proof systems\nwhere the verifier is given a 3SAT formula, but has only oracle access to an\nassignment and a proof. The verifier accepts a satisfying assignment with a\nvalid proof, and rejects (with high enough probability) an assignment that is\nfar from all satisfying assignments (for any given proof).\n  In this work, we focus on the type of computation the verifier is allowed to\nmake. Assuming P $\\neq$ NP, there can be no PCPP when the verifier is only\nallowed to answer according to constraints from a set that forms a CSP that is\nsolvable in P. Therefore, the notion of PCPP is relaxed to almost perfect\nprobabilistically checkable proofs of proximity (APPCPP), where the verifier is\nallowed to reject a satisfying assignment with a valid proof, with arbitrary\nsmall probability.\n  We show, unconditionally, a dichotomy of sets of allowable computations: sets\nthat have APPCPPs (which actually follows because they have PCPPs) and sets\nthat do not. This dichotomy turns out to be the same as that of the Dichotomy\nTheorem, which can be thought of as dividing sets of allowable verifier\ncomputations into sets that give rise to NP-hard CSPs, and sets that give rise\nto CSPs that are solvable in P.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:09:43 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Jozeph", "Shlomo", ""]]}, {"id": "1511.05210", "submitter": "Holger Petersen", "authors": "Holger Petersen", "title": "Counting Ones Without Broadword Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lower time bound $\\Omega(\\min(\\nu(x), n-\\nu(x))$ for counting the number of\nones in a binary input word $x$ of length $n$ is presented, where $\\nu(x)$ is\nthe number of ones. The operations available are increment, decrement, bit-wise\nlogical operations, and assignment. The only constant available is zero. An\nalmost matching upper bound is also obtained.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:46:32 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 21:23:32 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Petersen", "Holger", ""]]}, {"id": "1511.05449", "submitter": "Christian Komusiewicz", "authors": "Christian Komusiewicz", "title": "Tight Running Time Lower Bounds for Vertex Deletion Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph class $\\Pi$, the $\\Pi$-Vertex Deletion problem has as input an\nundirected graph $G=(V,E)$ and an integer $k$ and asks whether there is a set\nof at most $k$ vertices that can be deleted from $G$ such that the resulting\ngraph is a member of $\\Pi$. By a classic result of Lewis and Yannakakis [J.\nComput. Syst. Sci. '80], $\\Pi$-Vertex Deletion is NP-hard for all hereditary\nproperties $\\Pi$. We adapt the original NP-hardness construction to show that\nunder the Exponential Time Hypothesis (ETH) tight complexity results can be\nobtained. We show that $\\Pi$-Vertex Deletion does not admit a $2^{o(n)}$-time\nalgorithm where $n$ is the number of vertices in $G$. We also obtain a\ndichotomy for running time bounds that include the number $m$ of edges in the\ninput graph: On the one hand, if $\\Pi$ contains all independent sets, then\nthere is no $2^{o(n+m)}$-time algorithm for $\\Pi$-Vertex Deletion. On the other\nhand, if there is a fixed independent set that is not contained in $\\Pi$ and\ncontainment in $\\Pi$ can determined in $2^{O(n)}$ time or $2^{o(m)}$ time, then\n$\\Pi$-Vertex Deletion can be solved in $2^{O(\\sqrt{m})}+O(n)$ or\n$2^{o({m})}+O(n)$ time, respectively. We also consider restrictions on the\ndomain of the input graph $G$. For example, we obtain that $\\Pi$-Vertex\nDeletion cannot be solved in $2^{o(\\sqrt{n})}$ time if $G$ is planar and $\\Pi$\nis hereditary and contains and excludes infinitely many planar graphs. Finally,\nwe provide similar results for the problem variant where the deleted vertex set\nhas to induce a connected graph.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:59:57 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 09:05:23 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Komusiewicz", "Christian", ""]]}, {"id": "1511.05546", "submitter": "Holger Dell", "authors": "Holger Dell, Eun Jung Kim, Michael Lampis, Valia Mitsou, Tobias\n  M\\\"omke", "title": "Complexity and Approximability of Parameterized MAX-CSPs", "comments": "Appeared in IPEC 2015", "journal-ref": "Algorithmica 79(1): 230-250 (2017)", "doi": "10.1007/s00453-017-0310-8 10.4230/LIPIcs.IPEC.2015.294", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the optimization version of constraint satisfaction problems\n(Max-CSPs) in the framework of parameterized complexity; the goal is to compute\nthe maximum fraction of constraints that can be satisfied simultaneously. In\nstandard CSPs, we want to decide whether this fraction equals one. The\nparameters we investigate are structural measures, such as the treewidth or the\nclique-width of the variable-constraint incidence graph of the CSP instance.\n  We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY,\nand with various parameters k, and we attempt to fully classify them into the\nfollowing three cases: 1. The exact optimum can be computed in FPT time. 2. It\nis W[1]-hard to compute the exact optimum, but there is a randomized FPT\napproximation scheme (FPTAS), which computes a $(1-\\epsilon)$-approximation in\ntime $f(k,\\epsilon)\\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].\n  For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 20:51:47 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 10:42:11 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dell", "Holger", ""], ["Kim", "Eun Jung", ""], ["Lampis", "Michael", ""], ["Mitsou", "Valia", ""], ["M\u00f6mke", "Tobias", ""]]}, {"id": "1511.05750", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert (LACL), Ioana Cristescu (PPS)", "title": "Contextual equivalences in configuration structures and reversibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual equivalence equate terms that have the same observable behaviour\nin any context. A standard contextual equivalence for CCS is the strong barbed\ncongruence. Configuration structures are a denotational semantics for processes\nin which one define equivalences that are more discriminating, i.e. that\ndistinguish the denotation of terms equated by barbed congruence. Hereditary\nhistory preserving bisimulation (HHPB) is such a relation. We define a strong\nback-and-forth barbed congruence on RCCS, a reversible variant of CCS. We show\nthat the relation induced by the back-and-forth congruence on configuration\nstructures is equivalent to HHPB, thus providing a contextual characterization\nof HHPB.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:10:02 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LACL"], ["Cristescu", "Ioana", "", "PPS"]]}, {"id": "1511.05886", "submitter": "Jesper W. Mikkelsen", "authors": "Jesper W. Mikkelsen", "title": "Randomization can be as helpful as a glimpse of the future in online\n  computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple but surprisingly useful direct product theorems for proving\nlower bounds on online algorithms with a limited amount of advice about the\nfuture. As a consequence, we are able to translate decades of research on\nrandomized online algorithms to the advice complexity model. Doing so improves\nsignificantly on the previous best advice complexity lower bounds for many\nonline problems, or provides the first known lower bounds. For example, if $n$\nis the number of requests, we show that:\n  (1) A paging algorithm needs $\\Omega(n)$ bits of advice to achieve a\ncompetitive ratio better than $H_k=\\Omega(\\log k)$, where $k$ is the cache\nsize. Previously, it was only known that $\\Omega(n)$ bits of advice were\nnecessary to achieve a constant competitive ratio smaller than $5/4$.\n  (2) Every $O(n^{1-\\varepsilon})$-competitive vertex coloring algorithm must\nuse $\\Omega(n\\log n)$ bits of advice. Previously, it was only known that\n$\\Omega(n\\log n)$ bits of advice were necessary to be optimal.\n  For certain online problems, including the MTS, $k$-server, paging, list\nupdate, and dynamic binary search tree problem, our results imply that\nrandomization and sublinear advice are equally powerful (if the underlying\nmetric space or node set is finite). This means that several long-standing open\nquestions regarding randomized online algorithms can be equivalently stated as\nquestions regarding online algorithms with sublinear advice. For example, we\nshow that there exists a deterministic $O(\\log k)$-competitive $k$-server\nalgorithm with advice complexity $o(n)$ if and only if there exists a\nrandomized $O(\\log k)$-competitive $k$-server algorithm without advice.\n  Technically, our main direct product theorem is obtained by extending an\ninformation theoretical lower bound technique due to Emek, Fraigniaud, Korman,\nand Ros\\'en [ICALP'09].\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 17:20:34 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 10:38:05 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Mikkelsen", "Jesper W.", ""]]}, {"id": "1511.06022", "submitter": "Amir Abboud", "authors": "Amir Abboud and Thomas Dueholm Hansen and Virginia Vassilevska\n  Williams and Ryan Williams", "title": "Simulating Branching Programs with Edit Distance and Friends or: A\n  Polylog Shaved is a Lower Bound Made", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent and active line of work achieves tight lower bounds for fundamental\nproblems under the Strong Exponential Time Hypothesis (SETH). A celebrated\nresult of Backurs and Indyk (STOC'15) proves that the Edit Distance of two\nsequences of length n cannot be computed in strongly subquadratic time under\nSETH. The result was extended by follow-up works to simpler looking problems\nlike finding the Longest Common Subsequence (LCS).\n  SETH is a very strong assumption, asserting that even linear size CNF\nformulas cannot be analyzed for satisfiability with an exponential speedup over\nexhaustive search. We consider much safer assumptions, e.g. that such a speedup\nis impossible for SAT on much more expressive representations, like NC\ncircuits. Intuitively, this seems much more plausible: NC circuits can\nimplement complex cryptographic primitives, while CNFs cannot even\napproximately compute an XOR of bits.\n  Our main result is a surprising reduction from SAT on Branching Programs to\nfundamental problems in P like Edit Distance, LCS, and many others. Truly\nsubquadratic algorithms for these problems therefore have consequences that we\nconsider to be far more remarkable than merely faster CNF SAT algorithms. For\nexample, SAT on arbitrary o(n)-depth bounded fan-in circuits (and therefore\nalso NC-Circuit-SAT) can be solved in (2-eps)^n time.\n  A very interesting feature of our work is that we can prove major\nconsequences even from mildly subquadratic algorithms for Edit Distance or LCS.\nFor example, we show that if we can shave an arbitrarily large polylog factor\nfrom n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. A\nmore fine-grained examination shows that even shaving a $\\log^c{n}$ factor, for\na specific constant $c \\approx 10^3$, already implies new circuit lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 23:28:37 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Abboud", "Amir", ""], ["Hansen", "Thomas Dueholm", ""], ["Williams", "Virginia Vassilevska", ""], ["Williams", "Ryan", ""]]}, {"id": "1511.06384", "submitter": "Pradeep Dubey Dr.", "authors": "Pradeep Dubey", "title": "Decentralization of a Machine: Some Definitions", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define some notions of the decentralization of a deterministic\ninput-output machine. This opens the possibility for introducing game-theoretic\nelements -- such as strategic players -- inside the machine, as part of its\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 22:52:37 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Dubey", "Pradeep", ""]]}, {"id": "1511.06558", "submitter": "Preetum Nakkiran", "authors": "Pasin Manurangsi, Preetum Nakkiran, Luca Trevisan", "title": "Near-Optimal UGC-hardness of Approximating Max k-CSP_R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove an almost-optimal hardness for Max $k$-CSP$_R$ based\non Khot's Unique Games Conjecture (UGC). In Max $k$-CSP$_R$, we are given a set\nof predicates each of which depends on exactly $k$ variables. Each variable can\ntake any value from $1, 2, \\dots, R$. The goal is to find an assignment to\nvariables that maximizes the number of satisfied predicates.\n  Assuming the Unique Games Conjecture, we show that it is NP-hard to\napproximate Max $k$-CSP$_R$ to within factor $2^{O(k \\log k)}(\\log\nR)^{k/2}/R^{k - 1}$ for any $k, R$. To the best of our knowledge, this result\nimproves on all the known hardness of approximation results when $3 \\leq k =\no(\\log R/\\log \\log R)$. In this case, the previous best hardness result was\nNP-hardness of approximating within a factor $O(k/R^{k-2})$ by Chan. When $k =\n2$, our result matches the best known UGC-hardness result of Khot, Kindler,\nMossel and O'Donnell.\n  In addition, by extending an algorithm for Max 2-CSP$_R$ by Kindler, Kolla\nand Trevisan, we provide an $\\Omega(\\log R/R^{k - 1})$-approximation algorithm\nfor Max $k$-CSP$_R$. This algorithm implies that our inapproximability result\nis tight up to a factor of $2^{O(k \\log k)}(\\log R)^{k/2 - 1}$. In comparison,\nwhen $3 \\leq k$ is a constant, the previously known gap was $O(R)$, which is\nsignificantly larger than our gap of $O(\\text{polylog } R)$.\n  Finally, we show that we can replace the Unique Games Conjecture assumption\nwith Khot's $d$-to-1 Conjecture and still get asymptotically the same hardness\nof approximation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 11:13:10 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Manurangsi", "Pasin", ""], ["Nakkiran", "Preetum", ""], ["Trevisan", "Luca", ""]]}, {"id": "1511.07070", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk", "title": "Which Regular Expression Patterns are Hard to Match?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions constitute a fundamental notion in formal language theory\nand are frequently used in computer science to define search patterns. A\nclassic algorithm for these problems constructs and simulates a\nnon-deterministic finite automaton corresponding to the expression, resulting\nin an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is\nthe length of the text). This running time can be improved slightly (by a\npolylogarithmic factor), but no significantly faster solutions are known. At\nthe same time, much faster algorithms exist for various special cases of\nregular expressions, including dictionary matching, wildcard matching, subset\nmatching, word break problem etc.\n  In this paper, we show that the complexity of regular expression matching can\nbe characterized based on its {\\em depth} (when interpreted as a formula). Our\nresults hold for expressions involving concatenation, OR, Kleene star and\nKleene plus. For regular expressions of depth two (involving any combination of\nthe above operators), we show the following dichotomy: matching and membership\ntesting can be solved in near-linear time, except for \"concatenations of\nstars\", which cannot be solved in strongly sub-quadratic time assuming the\nStrong Exponential Time Hypothesis (SETH). For regular expressions of depth\nthree the picture is more complex. Nevertheless, we show that all problems can\neither be solved in strongly sub-quadratic time, or cannot be solved in\nstrongly sub-quadratic time assuming SETH.\n  An intriguing special case of membership testing involves regular expressions\nof the form \"a star of an OR of concatenations\", e.g., $[a|ab|bc]^*$. This\ncorresponds to the so-called {\\em word break} problem, for which a dynamic\nprogramming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We\nshow that the latter bound is not tight and improve the runtime to\n$O(nm^{0.44\\ldots})$.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 21:10:49 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 22:37:30 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""]]}, {"id": "1511.07100", "submitter": "Boris Brimkov", "authors": "Boris Brimkov", "title": "A reduction of the logspace shortest path problem to biconnected graphs", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reduce the logspace shortest path problem to biconnected\ngraphs; in particular, we present a logspace shortest path algorithm for\ngeneral graphs which uses a logspace shortest path oracle for biconnected\ngraphs. We also present a linear time logspace shortest path algorithm for\ngraphs with bounded vertex degree and biconnected component size, which does\nnot rely on an oracle. The asymptotic time-space product of this algorithm is\nthe best possible among all shortest path algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 02:55:46 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Brimkov", "Boris", ""]]}, {"id": "1511.07136", "submitter": "Ben Lee Volk", "authors": "Matthew Anderson, Michael A. Forbes, Ramprasad Saptharishi, Amir\n  Shpilka, Ben Lee Volk", "title": "Identity Testing and Lower Bounds for Read-$k$ Oblivious Algebraic\n  Branching Programs", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Read-$k$ oblivious algebraic branching programs are a natural generalization\nof the well-studied model of read-once oblivious algebraic branching program\n(ROABPs). In this work, we give an exponential lower bound of\n$\\exp(n/k^{O(k)})$ on the width of any read-$k$ oblivious ABP computing some\nexplicit multilinear polynomial $f$ that is computed by a polynomial size\ndepth-$3$ circuit. We also study the polynomial identity testing (PIT) problem\nfor this model and obtain a white-box subexponential-time PIT algorithm. The\nalgorithm runs in time $2^{\\tilde{O}(n^{1-1/2^{k-1}})}$ and needs white box\naccess only to know the order in which the variables appear in the ABP.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 08:37:39 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Anderson", "Matthew", ""], ["Forbes", "Michael A.", ""], ["Saptharishi", "Ramprasad", ""], ["Shpilka", "Amir", ""], ["Volk", "Ben Lee", ""]]}, {"id": "1511.07480", "submitter": "Radu Curticapean", "authors": "Radu Curticapean", "title": "Parity Separation: A Scientifically Proven Method for Permanent Weight\n  Loss", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an edge-weighted graph G, let PerfMatch(G) denote the weighted sum over\nall perfect matchings M in G, weighting each matching M by the product of\nweights of edges in M. If G is unweighted, this plainly counts the perfect\nmatchings of G.\n  In this paper, we introduce parity separation, a new method for reducing\nPerfMatch to unweighted instances: For graphs G with edge-weights -1 and 1, we\nconstruct two unweighted graphs G1 and G2 such that PerfMatch(G) =\nPerfMatch(G1) - PerfMatch(G2). This yields a novel weight removal technique for\ncounting perfect matchings, in addition to those known from classical\n#P-hardness proofs. We derive the following applications:\n  1. An alternative #P-completeness proof for counting unweighted perfect\nmatchings.\n  2. C=P-completeness for deciding whether two given unweighted graphs have the\nsame number of perfect matchings. To the best of our knowledge, this is the\nfirst C=P-completeness result for the \"equality-testing version\" of any natural\ncounting problem that is not already #P-hard under parsimonious reductions.\n  3. An alternative tight lower bound for counting unweighted perfect matchings\nunder the counting exponential-time hypothesis #ETH.\n  Our technique is based upon matchgates and the Holant framework. To make our\n#P-hardness proof self-contained, we also apply matchgates for an alternative\n#P-hardness proof of PerfMatch on graphs with edge-weights -1 and 1.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 21:59:00 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Curticapean", "Radu", ""]]}, {"id": "1511.07488", "submitter": "John Kim", "authors": "John Kim, Swastik Kopparty", "title": "Decoding Reed-Muller codes over product sets", "comments": "25 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time algorithm to decode multivariate polynomial codes\nof degree $d$ up to half their minimum distance, when the evaluation points are\nan arbitrary product set $S^m$, for every $d < |S|$. Previously known\nalgorithms can achieve this only if the set $S$ has some very special algebraic\nstructure, or if the degree $d$ is significantly smaller than $|S|$. We also\ngive a near-linear time randomized algorithm, which is based on tools from\nlist-decoding, to decode these codes from nearly half their minimum distance,\nprovided $d < (1-\\epsilon)|S|$ for constant $\\epsilon > 0$.\n  Our result gives an $m$-dimensional generalization of the well known decoding\nalgorithms for Reed-Solomon codes, and can be viewed as giving an algorithmic\nversion of the Schwartz-Zippel lemma.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 22:11:19 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Kim", "John", ""], ["Kopparty", "Swastik", ""]]}, {"id": "1511.07558", "submitter": "Sivakanth Gopi", "authors": "Arnab Bhattacharyya, Sivakanth Gopi", "title": "Lower bounds for constant query affine-invariant LCCs and LTCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affine-invariant codes are codes whose coordinates form a vector space over a\nfinite field and which are invariant under affine transformations of the\ncoordinate space. They form a natural, well-studied class of codes; they\ninclude popular codes such as Reed-Muller and Reed-Solomon. A particularly\nappealing feature of affine-invariant codes is that they seem well-suited to\nadmit local correctors and testers.\n  In this work, we give lower bounds on the length of locally correctable and\nlocally testable affine-invariant codes with constant query complexity. We show\nthat if a code $\\mathcal{C} \\subset \\Sigma^{\\mathbb{K}^n}$ is an $r$-query\nlocally correctable code (LCC), where $\\mathbb{K}$ is a finite field and\n$\\Sigma$ is a finite alphabet, then the number of codewords in $\\mathcal{C}$ is\nat most $\\exp(O_{\\mathbb{K}, r, |\\Sigma|}(n^{r-1}))$. Also, we show that if\n$\\mathcal{C} \\subset \\Sigma^{\\mathbb{K}^n}$ is an $r$-query locally testable\ncode (LTC), then the number of codewords in $\\mathcal{C}$ is at most\n$\\exp(O_{\\mathbb{K}, r, |\\Sigma|}(n^{r-2}))$. The dependence on $n$ in these\nbounds is tight for constant-query LCCs/LTCs, since Guo, Kopparty and Sudan\n(ITCS `13) construct affine-invariant codes via lifting that have the same\nasymptotic tradeoffs. Note that our result holds for non-linear codes, whereas\npreviously, Ben-Sasson and Sudan (RANDOM `11) assumed linearity to derive\nsimilar results.\n  Our analysis uses higher-order Fourier analysis. In particular, we show that\nthe codewords corresponding to an affine-invariant LCC/LTC must be far from\neach other with respect to Gowers norm of an appropriate order. This then\nallows us to bound the number of codewords, using known decomposition theorems\nwhich approximate any bounded function in terms of a finite number of\nlow-degree non-classical polynomials, upto a small error in the Gowers norm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 03:58:22 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gopi", "Sivakanth", ""]]}, {"id": "1511.07605", "submitter": "Nisheeth Vishnoi", "authors": "Christos H. Papadimitriou and Nisheeth K. Vishnoi", "title": "On the Computational Complexity of Limit Cycles in Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Poincare-Bendixson theorem for two-dimensional continuous\ndynamical systems in compact domains from the point of view of computation,\nseeking algorithms for finding the limit cycle promised by this classical\nresult. We start by considering a discrete analogue of this theorem and show\nthat both finding a point on a limit cycle, and determining if a given point is\non one, are PSPACE-complete.\n  For the continuous version, we show that both problems are uncomputable in\nthe real complexity sense; i.e., their complexity is arbitrarily high.\nSubsequently, we introduce a notion of an \"approximate cycle\" and prove an\n\"approximate\" Poincar\\'e-Bendixson theorem guaranteeing that some orbits come\nvery close to forming a cycle in the absence of approximate fixpoints;\nsurprisingly, it holds for all dimensions. The corresponding computational\nproblem defined in terms of arithmetic circuits is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:31:03 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1511.07729", "submitter": "Michael Saks", "authors": "Justin Gilmer, Michal Kouck\\'y, and Michael Saks", "title": "A communication game related to the sensitivity conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major outstanding foundational problems about boolean functions is\nthe sensitivity conjecture, which (in one of its many forms) asserts that the\ndegree of a boolean function (i.e. the minimum degree of a real polynomial that\ninterpolates the function) is bounded above by some fixed power of its\nsensitivity (which is the maximum vertex degree of the graph defined on the\ninputs where two inputs are adjacent if they differ in exactly one coordinate\nand their function values are different). We propose an attack on the\nsensitivity conjecture in terms of a novel two-player communication game. A\nlower bound of the form $n^{\\Omega(1)}$ on the cost of this game would imply\nthe sensitivity conjecture.\n  To investigate the problem of bounding the cost of the game, three natural\n(stronger) variants of the question are considered. For two of these variants,\nprotocols are presented that show that the hoped for lower bound does not hold.\nThese protocols satisfy a certain monotonicity property, and (in contrast to\nthe situation for the two variants) we show that the cost of any monotone\nprotocol satisfies a strong lower bound.\n  There is an easy upper bound of $\\sqrt{n}$ on the cost of the game. We also\nimprove slightly on this upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:35:18 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Gilmer", "Justin", ""], ["Kouck\u00fd", "Michal", ""], ["Saks", "Michael", ""]]}, {"id": "1511.07860", "submitter": "Ryan Williams", "authors": "Daniel M. Kane and Ryan Williams", "title": "Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two\n  and Depth-Three Threshold Circuits", "comments": null, "journal-ref": "ACM Symposium on Theory of Computing (STOC), 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to formally understand the power of neural computing, we first need\nto crack the frontier of threshold circuits with two and three layers, a regime\nthat has been surprisingly intractable to analyze. We prove the first\nsuper-linear gate lower bounds and the first super-quadratic wire lower bounds\nfor depth-two linear threshold circuits with arbitrary weights, and depth-three\nmajority circuits computing an explicit function.\n  $\\bullet$ We prove that for all $\\epsilon\\gg \\sqrt{\\log(n)/n}$, the\nlinear-time computable Andreev's function cannot be computed on a\n$(1/2+\\epsilon)$-fraction of $n$-bit inputs by depth-two linear threshold\ncircuits of $o(\\epsilon^3 n^{3/2}/\\log^3 n)$ gates, nor can it be computed with\n$o(\\epsilon^{3} n^{5/2}/\\log^{7/2} n)$ wires. This establishes an average-case\n``size hierarchy'' for threshold circuits, as Andreev's function is computable\nby uniform depth-two circuits of $o(n^3)$ linear threshold gates, and by\nuniform depth-three circuits of $O(n)$ majority gates.\n  $\\bullet$ We present a new function in $P$ based on small-biased sets, which\nwe prove cannot be computed by a majority vote of depth-two linear threshold\ncircuits with $o(n^{3/2}/\\log^3 n)$ gates, nor with $o(n^{5/2}/\\log^{7/2}n)$\nwires.\n  $\\bullet$ We give tight average-case (gate and wire) complexity results for\ncomputing PARITY with depth-two threshold circuits; the answer turns out to be\nthe same as for depth-two majority circuits.\n  The key is a new random restriction lemma for linear threshold functions. Our\nmain analytical tool is the Littlewood-Offord Lemma from additive\ncombinatorics.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 20:45:51 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Kane", "Daniel M.", ""], ["Williams", "Ryan", ""]]}, {"id": "1511.07949", "submitter": "Manoj Prabhakaran", "authors": "Manoj M. Prabhakaran and Vinod M. Prabhakaran", "title": "R\\'enyi Information Complexity and an Information Theoretic\n  Characterization of the Partition Bound", "comments": "Full version of paper appearing at ICALP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new information-theoretic complexity measure $IC_\\infty$ for\n2-party functions which is a lower-bound on communication complexity, and has\nthe two leading lower-bounds on communication complexity as its natural\nrelaxations: (external) information complexity ($IC$) and logarithm of\npartition complexity ($\\text{prt}$), which have so far appeared conceptually\nquite different from each other. $IC_\\infty$ is an external information\ncomplexity measure based on R\\'enyi mutual information of order infinity. In\nthe definition of $IC_\\infty$, relaxing the order of R\\'enyi mutual information\nfrom infinity to 1 yields $IC$, while $\\log \\text{prt}$ is obtained by\nreplacing protocol transcripts with what we term \"pseudotranscripts,\" which\nomits the interactive nature of a protocol, but only requires that the\nprobability of any transcript given the inputs $x$ and $y$ to the two parties,\nfactorizes into two terms which depend on $x$ and $y$ separately. Further\nunderstanding $IC_\\infty$ might have consequences for important direct-sum\nproblems in communication complexity, as it lies between communication\ncomplexity and information complexity.\n  We also show that applying both the above relaxations simultaneously to\n$IC_\\infty$ gives a complexity measure that is lower-bounded by the (log of)\nrelaxed partition complexity, a complexity measure introduced by Kerenidis et\nal. (FOCS 2012). We obtain a sharper connection between (external) information\ncomplexity and relaxed partition complexity than Kerenidis et al., using an\narguably more direct proof.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 05:00:54 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 13:55:56 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Prabhakaran", "Manoj M.", ""], ["Prabhakaran", "Vinod M.", ""]]}, {"id": "1511.08113", "submitter": "Peter B\\\"urgisser", "authors": "Peter B\\\"urgisser", "title": "Permanent versus determinant, obstructions, and Kronecker coefficients", "comments": "Survey, 17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an introduction to some of the recent ideas that go under the name\n\"geometric complexity theory\". We first sketch the proof of the known upper and\nlower bounds for the determinantal complexity of the permanent. We then\nintroduce the concept of a representation theoretic obstruction, which has\nclose links to algebraic combinatorics, and we explain some of the insights\ngained so far. In particular, we address very recent insights on the complexity\nof testing the positivity of Kronecker coefficients. We also briefly discuss\nthe related asymptotic version of this question.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 16:49:53 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 08:05:07 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["B\u00fcrgisser", "Peter", ""]]}, {"id": "1511.08189", "submitter": "Eric Allender", "authors": "Eric Allender and Joshua A. Grochow and Dieter van Melkebeek and\n  Cristopher Moore and Andrew Morgan", "title": "Graph Isomorphism and Circuit Size", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known [KST93] that the complexity of the Graph Automorphism\nproblem is characterized by a special case of Graph Isomorphism, where the\ninput graphs satisfy the \"promise\" of being rigid (that is, having no\nnontrivial automorphisms). In this brief note, we observe that the reduction of\nGraph Automorphism to the Rigid Graph Ismorphism problem can be accomplished\neven using Grollman and Selman's notion of a \"smart reduction\".\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 20:20:48 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 10:54:51 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Allender", "Eric", ""], ["Grochow", "Joshua A.", ""], ["van Melkebeek", "Dieter", ""], ["Moore", "Cristopher", ""], ["Morgan", "Andrew", ""]]}, {"id": "1511.08245", "submitter": "Cyrus Rashtchian", "authors": "Shay Moran, Cyrus Rashtchian", "title": "Shattered Sets and the Hilbert Function", "comments": "19 pages, 2 figures. Fixed typo in Theorem 2.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study complexity measures on subsets of the boolean hypercube and exhibit\nconnections between algebra (the Hilbert function) and combinatorics (VC\ntheory). These connections yield results in both directions. Our main\ncomplexity-theoretic result proves that most linear program feasibility\nproblems cannot be computed by polynomial-sized constant-depth circuits.\nMoreover, our result applies to a stronger regime in which the hyperplanes are\nfixed and only the directions of the inequalities are given as input to the\ncircuit. We derive this result by proving that a rich class of extremal\nfunctions in VC theory cannot be approximated by low-degree polynomials. We\nalso present applications of algebra to combinatorics. We provide a new\nalgebraic proof of the Sandwich Theorem, which is a generalization of the\nwell-known Sauer-Perles-Shelah Lemma. Finally, we prove a structural result\nabout downward-closed sets, related to the Chv\\'{a}tal conjecture in extremal\ncombinatorics.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 22:31:27 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 22:14:27 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 19:10:31 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Moran", "Shay", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1511.08270", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Ameet Gadekar and Suprovat Ghoshal and Rishi\n  Saket", "title": "On the hardness of learning sparse parities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the hardness of computing sparse solutions to systems\nof linear equations over F_2. Consider the k-EvenSet problem: given a\nhomogeneous system of linear equations over F_2 on n variables, decide if there\nexists a nonzero solution of Hamming weight at most k (i.e. a k-sparse\nsolution). While there is a simple O(n^{k/2})-time algorithm for it,\nestablishing fixed parameter intractability for k-EvenSet has been a notorious\nopen problem. Towards this goal, we show that unless k-Clique can be solved in\nn^{o(k)} time, k-EvenSet has no poly(n)2^{o(sqrt{k})} time algorithm and no\npolynomial time algorithm when k = (log n)^{2+eta} for any eta > 0.\n  Our work also shows that the non-homogeneous generalization of the problem --\nwhich we call k-VectorSum -- is W[1]-hard on instances where the number of\nequations is O(k log n), improving on previous reductions which produced\nOmega(n) equations. We also show that for any constant eps > 0, given a system\nof O(exp(O(k))log n) linear equations, it is W[1]-hard to decide if there is a\nk-sparse linear form satisfying all the equations or if every function on at\nmost k-variables (k-junta) satisfies at most (1/2 + eps)-fraction of the\nequations. In the setting of computational learning, this shows hardness of\napproximate non-proper learning of k-parities. In a similar vein, we use the\nhardness of k-EvenSet to show that that for any constant d, unless k-Clique can\nbe solved in n^{o(k)} time there is no poly(m, n)2^{o(sqrt{k}) time algorithm\nto decide whether a given set of m points in F_2^n satisfies: (i) there exists\na non-trivial k-sparse homogeneous linear form evaluating to 0 on all the\npoints, or (ii) any non-trivial degree d polynomial P supported on at most k\nvariables evaluates to zero on approx. Pr_{F_2^n}[P(z) = 0] fraction of the\npoints i.e., P is fooled by the set of points.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:08:36 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gadekar", "Ameet", ""], ["Ghoshal", "Suprovat", ""], ["Saket", "Rishi", ""]]}, {"id": "1511.08682", "submitter": "Martins Kokainis", "authors": "Scott Aaronson, Andris Ambainis, J\\=anis Iraids, Martins Kokainis,\n  Juris Smotrovs", "title": "Polynomials, Quantum Query Complexity, and Grothendieck's Inequality", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an equivalence between 1-query quantum algorithms and representations\nby degree-2 polynomials. Namely, a partial Boolean function $f$ is computable\nby a 1-query quantum algorithm with error bounded by $\\epsilon<1/2$ iff $f$ can\nbe approximated by a degree-2 polynomial with error bounded by $\\epsilon'<1/2$.\nThis result holds for two different notions of approximation by a polynomial:\nthe standard definition of Nisan and Szegedy and the approximation by\nblock-multilinear polynomials recently introduced by Aaronson and Ambainis\n(STOC'2015, arxiv:1411.5729).\n  We also show two results for polynomials of higher degree. First, there is a\ntotal Boolean function which requires $\\tilde{\\Omega}(n)$ quantum queries but\ncan be represented by a block-multilinear polynomial of degree\n$\\tilde{O}(\\sqrt{n})$. Thus, in the general case (for an arbitrary number of\nqueries), block-multilinear polynomials are not equivalent to quantum\nalgorithms.\n  Second, for any constant degree $k$, the two notions of approximation by a\npolynomial (the standard and the block-multilinear) are equivalent. As a\nconsequence, we solve an open problem of Aaronson and Ambainis, showing that\none can estimate the value of any bounded degree-$k$ polynomial $p:\\{0, 1\\}^n\n\\rightarrow [-1, 1]$ with $O(n^{1-\\frac{1}{2k}})$ queries.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 14:17:38 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 12:14:24 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 08:45:01 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Aaronson", "Scott", ""], ["Ambainis", "Andris", ""], ["Iraids", "J\u0101nis", ""], ["Kokainis", "Martins", ""], ["Smotrovs", "Juris", ""]]}, {"id": "1511.08841", "submitter": "Hans Raj Tiwary", "authors": "Jakub Gajarsk\\'y and Petr Hlin\\v{e}n\\'y and Hans Raj Tiwary", "title": "Parameterized Extension Complexity of Independent Set and Related\n  Problems", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a graph on $n$ vertices and $\\mathrm{STAB}_k(G)$ be the convex\nhull of characteristic vectors of its independent sets of size at most $k$. We\nstudy extension complexity of $\\mathrm{STAB}_k(G)$ with respect to a fixed\nparameter $k$ (analogously to, e.g., parameterized computational complexity of\nproblems). We show that for graphs $G$ from a class of bounded expansion it\nholds that $\\mathrm{xc}(\\mathrm{STAB}_k(G))\\leqslant \\mathcal{O}(f(k)\\cdot n)$\nwhere the function $f$ depends only on the class. This result can be extended\nin a simple way to a wide range of similarly defined graph polytopes. In case\nof general graphs we show that there is {\\em no function $f$} such that, for\nall values of the parameter $k$ and for all graphs on $n$ vertices, the\nextension complexity of $\\mathrm{STAB}_k(G)$ is at most $f(k)\\cdot\nn^{\\mathcal{O}(1)}.$ While such results are not surprising since it is known\nthat optimizing over $\\mathrm{STAB}_k(G)$ is $FPT$ for graphs of bounded\nexpansion and $W[1]$-hard in general, they are also not trivial and in both\ncases stronger than the corresponding computational complexity results.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 22:31:26 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 10:35:03 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 12:41:47 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Gajarsk\u00fd", "Jakub", ""], ["Hlin\u011bn\u00fd", "Petr", ""], ["Tiwary", "Hans Raj", ""]]}, {"id": "1511.08854", "submitter": "Alexander Kozachinskiy", "authors": "Alexander Kozachinskiy", "title": "Some Bounds on Communication Complexity of Gap Hamming Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we obtain some bounds on communication complexity of Gap\nHamming Distance problem ($\\mathsf{GHD}^n_{L, U}$): Alice and Bob are given\nbinary string of length $n$ and they are guaranteed that Hamming distance\nbetween their inputs is either $\\le L$ or $\\ge U$ for some $L < U$. They have\nto output 0, if the first inequality holds, and 1, if the second inequality\nholds.\n  In this paper we study the communication complexity of $\\mathsf{GHD}^n_{L,\nU}$ for probabilistic protocols with one-sided error and for deterministic\nprotocols. Our first result is a protocol which communicates\n$O\\left(\\left(\\frac{s}{U}\\right)^\\frac{1}{3} \\cdot n\\log n\\right)$ bits and has\none-sided error probability $e^{-s}$ provided $s \\ge \\frac{(L +\n\\frac{10}{n})^3}{U^2}$.\n  Our second result is about deterministic communication complexity of\n$\\mathsf{GHD}^n_{0,\\, t}$. Surprisingly, it can be computed with logarithmic\nprecision: $$\\mathrm{D}(\\mathsf{GHD}^n_{0,\\, t}) = n - \\log_2 V_2\\left(n,\n\\left\\lfloor\\frac{t}{2}\\right\\rfloor\\right) + O(\\log n),$$ where $V_2(n, r)$\ndenotes the size of Hamming ball of radius $r$.\n  As an application of this result for every $c < 2$ we prove a\n$\\Omega\\left(\\frac{n(2 - c)^2}{p}\\right)$ lower bound on the space complexity\nof any $c$-approximate deterministic $p$-pass streaming algorithm for computing\nthe number of distinct elements in a data stream of length $n$ with tokens\ndrawn from the universe $U = \\{1, 2, \\ldots, n\\}$. Previously that lower bound\nwas known for $c < \\frac{3}{2}$ and for $c < 2$ but with larger $|U|$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 23:44:55 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 08:30:06 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Kozachinskiy", "Alexander", ""]]}, {"id": "1511.09229", "submitter": "Djamal Belazzougui", "authors": "Djamal Belazzougui", "title": "Efficient Deterministic Single Round Document Exchange for Edit Distance", "comments": "12 pages, under submission. This version has some minor corrections,\n  clarifications and a simplification of the message size bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we have two parties that possess each a binary string. Suppose\nthat the length of the first string (document) is $n$ and that the two strings\n(documents) have edit distance (minimal number of deletes, inserts and\nsubstitutions needed to transform one string into the other) at most $k$. The\nproblem we want to solve is to devise an efficient protocol in which the first\nparty sends a single message that allows the second party to guess the first\nparty's string. In this paper we show an efficient deterministic protocol for\nthis problem. The protocol runs in time $O(n\\cdot \\mathtt{polylog}(n))$ and has\nmessage size $O(k^2+k\\log^2n)$ bits. To the best of our knowledge, ours is the\nfirst efficient deterministic protocol for this problem, if efficiency is\nmeasured in both the message size and the running time. As an immediate\napplication of our new protocol, we show a new error correcting code that is\nefficient even for large numbers of (adversarial) edit errors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 10:26:49 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 13:51:04 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Belazzougui", "Djamal", ""]]}, {"id": "1511.09360", "submitter": "Faisal Abu-Khzam", "authors": "Faisal N. Abu-Khzam", "title": "On the Complexity of Multi-Parameterized Cluster Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cluster Editing problem seeks a transformation of a given undirected\ngraph into a disjoint union of cliques via a minimum number of edge additions\nor deletions. A multi-parameterized version of the problem is studied,\nfeaturing a number of input parameters that bound the amount of both\nedge-additions and deletions per single vertex, as well as the size of a\nclique-cluster. We show that the problem remains NP-hard even when only one\nedge can be deleted and at most two edges can be added per vertex. However, the\nnew formulation allows us to solve Cluster Editing (exactly) in polynomial time\nwhen the number of edge-edit operations per vertex is smaller than half the\nminimum cluster size. In other words, Correlation Clustering can be solved\nefficiently when the number of false positives/negatives per single data\nelement is expected to be small compared to the minimum cluster size. As a\nbyproduct, we obtain a kernelization algorithm that delivers linear-size\nkernels when the two edge-edit bounds are small constants.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 15:56:47 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""]]}]