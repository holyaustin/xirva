[{"id": "1911.00178", "submitter": "Anindya De", "authors": "Anindya De and Rocco A. Servedio", "title": "Kruskal-Katona for convex sets, with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known Kruskal-Katona theorem in combinatorics says that (under mild\nconditions) every monotone Boolean function $f: \\{0,1\\}^n \\to \\{0,1\\}$ has a\nnontrivial \"density increment.\" This means that the fraction of inputs of\nHamming weight $k+1$ for which $f=1$ is significantly larger than the fraction\nof inputs of Hamming weight $k$ for which $f=1.$\n  We prove an analogous statement for convex sets. Informally, our main result\nsays that (under mild conditions) every convex set $K \\subset \\mathbb{R}^n$ has\na nontrivial density increment. This means that the fraction of the radius-$r$\nsphere that lies within $K$ is significantly larger than the fraction of the\nradius-$r'$ sphere that lies within $K$, for $r'$ suitably larger than $r$. For\ncentrally symmetric convex sets we show that our density increment result is\nessentially optimal.\n  As a consequence of our Kruskal-Katona type theorem, we obtain the first\nefficient weak learning algorithm for convex sets under the Gaussian\ndistribution. We show that any convex set can be weak learned to advantage\n$\\Omega(1/n)$ in $\\mathsf{poly}(n)$ time under any Gaussian distribution and\nthat any centrally symmetric convex set can be weak learned to advantage\n$\\Omega(1/\\sqrt{n})$ in $\\mathsf{poly}(n)$ time. We also give an\ninformation-theoretic lower bound showing that the latter advantage is\nessentially optimal for $\\mathsf{poly}(n)$ time weak learning algorithms. As\nanother consequence of our Kruskal-Katona theorem, we give the first nontrivial\nGaussian noise stability bounds for convex sets at high noise rates. Our\nresults extend the known correspondence between monotone Boolean functions over\n$ \\{0,1\\}^n$ and convex bodies in Gaussian space.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 01:39:40 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["De", "Anindya", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1911.00236", "submitter": "Ilya Chernykh", "authors": "Ilya Chernykh and Ekaterina Lgotina", "title": "Two-machine routing open shop on a tree: instance reduction and\n  efficiently solvable subclass", "comments": "Submitted to Optimization Methods and Software", "journal-ref": null, "doi": "10.1080/10556788.2020.1734802", "report-no": null, "categories": "cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-machine routing open shop problem on a tree. In this problem\na transportation network with a tree-like structure is given, and each node\ncontains some jobs to be processed by two mobile machines. Machines are\ninitially located in the predefined node called the depot, have to traverse the\nnetwork to perform their operations on each job (and each job has to be\nprocessed by both machines in arbitrary order), and machines have to return to\nthe depot after performing all the operations. The goal is to construct a\nfeasible schedule for machines to process all the jobs and to return to the\ndepot in shortest time possible. This problem is known to be NP-hard even in\nthe case when the transportation network consists of just two nodes.\n  We propose an instance reduction procedure which allows to transform any\ninstance of the problem to a simplified instance on a chain with limited number\nof jobs. The reduction considered preserves the standard lower bound on the\noptimum. We describe four possible outcomes of this procedure and prove that in\nthree of them the initial instance can be solved to the optimum in linear time,\nthus introducing a wide polynomially solvable subclass of the problem\nconsidered. Our research can be used as a foundation to construct efficient\napproximation algorithms for the two-machine routing open shop on a tree.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:44:12 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 10:36:05 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chernykh", "Ilya", ""], ["Lgotina", "Ekaterina", ""]]}, {"id": "1911.00403", "submitter": "Barnaby Martin", "authors": "Stefan Dantchev, Abdul Ghani, Barnaby Martin", "title": "Sherali-Adams and the binary encoding of combinatorial principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Sherali-Adams (SA) refutation system together with the\nunusual binary encoding of certain combinatorial principles. For the unary\nencoding of the Pigeonhole Principle and the Least Number Principle, it is\nknown that linear rank is required for refutations in SA, although both admit\nrefutations of polynomial size. We prove that the binary encoding of the\nPigeonhole Principle requires exponentially-sized SA refutations, whereas the\nbinary encoding of the Least Number Principle admits logarithmic rank,\npolynomially-sized SA refutations. We continue by considering a refutation\nsystem between SA and Lasserre (Sum-of-Squares). In this system, the Least\nNumber Principle requires linear rank while the Pigeonhole Principle becomes\nconstant rank.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 14:30:32 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Dantchev", "Stefan", ""], ["Ghani", "Abdul", ""], ["Martin", "Barnaby", ""]]}, {"id": "1911.00722", "submitter": "Boyu Sima", "authors": "Boyu Sima", "title": "A Solution of the P versus NP Problem based on specific property of\n  clique function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circuit lower bounds are important since it is believed that a\nsuper-polynomial circuit lower bound for a problem in NP implies that P!=NP.\nRazborov has proved superpolynomial lower bounds for monotone circuits by using\nmethod of approximation. By extending this approach, researchers have proved\nexponential lower bounds for the monotone network complexity of several\ndifferent functions. But until now, no one could prove a non-linear lower bound\nfor the non-monotone complexity of any Boolean function in NP. While we show\nthat in this paper by replacement of each Not gates into constant 1\nequivalently in standard circuit for clique problem, it can be proved that\nnon-monotone network has the same or higher lower bound compared to the\nmonotone one for computing the clique function. This indicates that the\nnon-monotone network complexity of the clique function is super-polynomial\nwhich implies that P!=NP.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 14:35:41 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 12:40:06 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Sima", "Boyu", ""]]}, {"id": "1911.00911", "submitter": "Anindya De", "authors": "Xue Chen, Anindya De and Rocco A. Servedio", "title": "Testing noisy linear functions for sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following basic inference problem: there is an unknown\nhigh-dimensional vector $w \\in \\mathbb{R}^n$, and an algorithm is given access\nto labeled pairs $(x,y)$ where $x \\in \\mathbb{R}^n$ is a measurement and $y = w\n\\cdot x + \\mathrm{noise}$. What is the complexity of deciding whether the\ntarget vector $w$ is (approximately) $k$-sparse? The recovery analogue of this\nproblem --- given the promise that $w$ is sparse, find or approximate the\nvector $w$ --- is the famous sparse recovery problem, with a rich body of work\nin signal processing, statistics, and computer science.\n  We study the decision version of this problem (i.e.~deciding whether the\nunknown $w$ is $k$-sparse) from the vantage point of property testing. Our\nfocus is on answering the following high-level question: when is it possible to\nefficiently test whether the unknown target vector $w$ is sparse versus\nfar-from-sparse using a number of samples which is completely independent of\nthe dimension $n$? We consider the natural setting in which $x$ is drawn from a\ni.i.d.~product distribution $\\mathcal{D}$ over $\\mathbb{R}^n$ and the\n$\\mathrm{noise}$ process is independent of the input $x$. As our main result,\nwe give a general algorithm which solves the above-described testing problem\nusing a number of samples which is completely independent of the ambient\ndimension $n$, as long as $\\mathcal{D}$ is not a Gaussian. In fact, our\nalgorithm is fully noise tolerant, in the sense that for an arbitrary $w$, it\napproximately computes the distance of $w$ to the closest $k$-sparse vector. To\ncomplement this algorithmic result, we show that weakening any of our condition\nmakes it information-theoretically impossible for any algorithm to solve the\ntesting problem with fewer than essentially $\\log n$ samples.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:40:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chen", "Xue", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1911.00944", "submitter": "G\\'abor Simonyi", "authors": "G\\'abor Simonyi", "title": "Shannon capacity and the categorical product", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shannon OR-capacity $C_{\\rm OR}(G)$ of a graph $G$, that is the traditionally\nmore often used Shannon AND-capacity of the complementary graph, is a\nhomomorphism monotone graph parameter satisfying $C_{\\rm OR}(F\\times\nG)\\le\\min\\{C_{\\rm OR}(F),C_{\\rm OR}(G)\\}$ for every pair of graphs, where\n$F\\times G$ is the categorical product of graphs $F$ and $G$. Here we initiate\nthe study of the question when could we expect equality in this inequality.\nUsing a strong recent result of Zuiddam, we show that if this \"Hedetniemi-type\"\nequality is not satisfied for some pair of graphs then the analogous equality\nis also not satisfied for this graph pair by some other graph invariant that\nhas a much \"nicer\" behavior concerning some different graph operations. In\nparticular, unlike Shannon capacity or the chromatic number, this other\ninvariant is both multiplicative under the OR-product and additive under the\njoin operation, while it is also nondecreasing along graph homomorphisms. We\nalso present a natural lower bound on $C_{\\rm OR}(F\\times G)$ and elaborate on\nthe question of how to find graph pairs for which it is known to be strictly\nless, than the upper bound $\\min\\{C_{\\rm OR}(F),C_{\\rm OR}(G)\\}$. We present\nsuch graph pairs using the properties of Paley graphs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 18:47:53 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Simonyi", "G\u00e1bor", ""]]}, {"id": "1911.01381", "submitter": "Dmitry Gavinsky", "authors": "Dmitry Gavinsky", "title": "Bare quantum simultaneity versus classical interactivity in\n  communication complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relational bipartite communication problem is presented that has an\nefficient quantum simultaneous-messages protocol, but no efficient classical\ntwo-way protocol.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:09:31 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 13:55:47 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Gavinsky", "Dmitry", ""]]}, {"id": "1911.01411", "submitter": "Sidhanth Mohanty", "authors": "Sidhanth Mohanty, Prasad Raghavendra, Jeff Xu", "title": "Lifting Sum-of-Squares Lower Bounds: Degree-$2$ to Degree-$4$", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree-$4$ Sum-of-Squares (SoS) SDP relaxation is a powerful algorithm\nthat captures the best known polynomial time algorithms for a broad range of\nproblems including MaxCut, Sparsest Cut, all MaxCSPs and tensor PCA. Despite\nbeing an explicit algorithm with relatively low computational complexity, the\nlimits of degree-$4$ SoS SDP are not well understood. For example, existing\nintegrality gaps do not rule out a $(2-\\varepsilon)$-algorithm for Vertex Cover\nor a $(0.878+\\varepsilon)$-algorithm for MaxCut via degree-$4$ SoS SDPs, each\nof which would refute the notorious Unique Games Conjecture.\n  We exhibit an explicit mapping from solutions for degree-$2$ Sum-of-Squares\nSDP (Goemans-Williamson SDP) to solutions for the degree-$4$ Sum-of-Squares SDP\nrelaxation on boolean variables. By virtue of this mapping, one can lift lower\nbounds for degree-$2$ SoS SDP relaxation to corresponding lower bounds for\ndegree-$4$ SoS SDPs. We use this approach to obtain degree-$4$ SoS SDP lower\nbounds for MaxCut on random $d$-regular graphs, Sherington-Kirkpatrick model\nfrom statistical physics and PSD Grothendieck problem.\n  Our constructions use the idea of pseudocalibration towards candidate SDP\nvectors, while it was previously only used to produce the candidate matrix\nwhich one would show is PSD using much technical work. In addition, we develop\na different technique to bound the spectral norms of _graphical matrices_ that\narise in the context of SoS SDPs. The technique is much simpler and yields\nbetter bounds in many cases than the _trace method_ -- which was the sole\ntechnique for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:53:35 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mohanty", "Sidhanth", ""], ["Raghavendra", "Prasad", ""], ["Xu", "Jeff", ""]]}, {"id": "1911.01479", "submitter": "Michael Kompatscher", "authors": "Michael Kompatscher", "title": "CC-circuits and the expressive power of nilpotent algebras", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that CC-circuits of bounded depth have the same expressive power as\npolynomials over finite nilpotent algebras from congruence modular varieties.\nWe use this result to phrase and discuss an algebraic version of Barrington,\nStraubing and Th\\'erien's conjecture, which states that CC-circuits of bounded\ndepth need exponential size to compute AND.\n  Furthermore we investigate the complexity of deciding identities and solving\nequations in a fixed nilpotent algebra. Under the assumption that the\nconjecture is true, we obtain quasipolynomial algorithms for both problems. On\nthe other hand, if AND is computable by uniform CC-circuits of bounded depth\nand polynomial size, we can construct a nilpotent algebra with coNP-complete,\nrespectively NP-complete problem.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 20:41:58 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 12:25:39 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 17:17:29 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 10:25:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kompatscher", "Michael", ""]]}, {"id": "1911.01504", "submitter": "Ewan Davies", "authors": "Matthew Coulson, Ewan Davies, Alexandra Kolla, Viresh Patel, and Guus\n  Regts", "title": "Statistical physics approaches to Unique Games", "comments": "26 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.CCC.2020.13", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how two techniques from statistical physics can be adapted to solve a\nvariant of the notorious Unique Games problem, potentially opening new avenues\ntowards the Unique Games Conjecture. The variant, which we call Count Unique\nGames, is a promise problem in which the \"yes\" case guarantees a certain number\nof highly satisfiable assignments to the Unique Games instance. In the standard\nUnique Games problem, the \"yes\" case only guarantees at least one such\nassignment. We exhibit efficient algorithms for Count Unique Games based on\napproximating a suitable partition function for the Unique Games instance via\n(i) a zero-free region and polynomial interpolation, and (ii) the cluster\nexpansion. We also show that a modest improvement to the parameters for which\nwe give results would refute the Unique Games Conjecture.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 21:58:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Coulson", "Matthew", ""], ["Davies", "Ewan", ""], ["Kolla", "Alexandra", ""], ["Patel", "Viresh", ""], ["Regts", "Guus", ""]]}, {"id": "1911.01522", "submitter": "Cesar A. Uribe", "authors": "Pavel Dvurechensky, Mathias Staudigl, C\\'esar A. Uribe", "title": "Generalized Self-concordant Hessian-barrier algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in statistical learning, imaging, and computer vision involve\nthe optimization of a non-convex objective function with singularities at the\nboundary of the feasible set. For such challenging instances, we develop a new\ninterior-point technique building on the Hessian-barrier algorithm recently\nintroduced in Bomze, Mertikopoulos, Schachinger and Staudigl, [SIAM J. Opt.\n2019 29(3), pp. 2100-2127], where the Riemannian metric is induced by a\ngeneralized self-concordant function. This class of functions is sufficiently\ngeneral to include most of the commonly used barrier functions in the\nliterature of interior point methods. We prove global convergence to an\napproximate stationary point of the method, and in cases where the feasible set\nadmits an easily computable self-concordant barrier, we verify worst-case\noptimal iteration complexity of the method. Applications in non-convex\nstatistical estimation and $L^{p}$-minimization are discussed to given the\nefficiency of the method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:58:01 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 05:22:03 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Staudigl", "Mathias", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1911.01662", "submitter": "G\\'abor Ivanyos", "authors": "Gabor Ivanyos and Antoine Joux and Miklos Santha", "title": "Discrete logarithm and Diffie-Hellman problems in identity black-box\n  groups", "comments": "13 pages. Revision with minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of the discrete logarithm, the\ncomputational Diffie-Hellman and the decisional Diffie-Hellman problems in some\nidentity black-box groups G_{p,t}, where p is a prime number and t is a\npositive integer. These are defined as quotient groups of vector space\nZ_p^{t+1} by a hyperplane H given through an identity oracle. While in general\nblack-box groups with unique encoding these computational problems are\nclassically all hard and quantumly all easy, we find that in the groups G_{p,t}\nthe situation is more contrasted. We prove that while there is a polynomial\ntime probabilistic algorithm to solve the decisional Diffie-Hellman problem in\n$G_{p,1}$, the probabilistic query complexity of all the other problems is\nOmega(p), and their quantum query complexity is Omega(sqrt(p)). Our results\ntherefore provide a new example of a group where the computational and the\ndecisional Diffie-Hellman problems have widely different complexity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 08:12:24 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 17:20:32 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ivanyos", "Gabor", ""], ["Joux", "Antoine", ""], ["Santha", "Miklos", ""]]}, {"id": "1911.01663", "submitter": "Yang Li", "authors": "Yang Li, Junbin Gao, Mingyuan Bai, Chengjun Li and Gang Liu", "title": "A Heuristic Algorithm Based on Tour Rebuilding Operator for the\n  Traveling Salesman Problem", "comments": "The value of Euclidean distance is not rounded, so there is some\n  error in the result. This paper is currently under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TSP (Traveling Salesman Problem), a classic NP-complete problem in\ncombinatorial optimization, is of great significance in multiple fields. Exact\nalgorithms for TSP are not practical due to their exponential time cost. Thus,\napproximate algorithms become the research focus and can be further divided\ninto two types, tour construction algorithms and tour improvement algorithms.\nResearches show that the latter type of algorithms can obtain better solutions\nthan the former one. However, traditional tour improvement algorithms have\nshortcomings. They converge very slowly and tend to be trapped in local optima.\nIn practice, tour construction algorithms are often used in initialization of\ntour improvement algorithms to speed up convergence. Nevertheless, such a\ncombination leads to no improvement on quality of solutions. In this paper, a\nheuristic algorithm based on the new tour rebuilding operator is proposed. The\nalgorithm features rapid convergence and powerful global search. In the\nexperiments based on 40 instances in TSPLIB, the best known solutions of 22\ninstances are refreshed by the proposed method. Meanwhile, the best known\nsolutions of the other 18 instances are obtained.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 08:15:58 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 07:00:07 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Li", "Yang", ""], ["Gao", "Junbin", ""], ["Bai", "Mingyuan", ""], ["Li", "Chengjun", ""], ["Liu", "Gang", ""]]}, {"id": "1911.01973", "submitter": "Chunhao Wang", "authors": "Scott Aaronson, Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang, Ruizhe\n  Zhang", "title": "On the Quantum Complexity of Closest Pair and Related Problems", "comments": "46 pages, 3 figures, presentation improved", "journal-ref": "Proceedings of the 35th Computational Complexity Conference (CCC),\n  pages 16:1--16:43, 2020", "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The closest pair problem is a fundamental problem of computational geometry:\ngiven a set of $n$ points in a $d$-dimensional space, find a pair with the\nsmallest distance. A classical algorithm taught in introductory courses solves\nthis problem in $O(n\\log n)$ time in constant dimensions (i.e., when $d=O(1)$).\nThis paper asks and answers the question of the problem's quantum time\ncomplexity. Specifically, we give an $\\tilde{O}(n^{2/3})$ algorithm in constant\ndimensions, which is optimal up to a polylogarithmic factor by the lower bound\non the quantum query complexity of element distinctness. The key to our\nalgorithm is an efficient history-independent data structure that supports\nquantum interference.\n  In $\\mathrm{polylog}(n)$ dimensions, no known quantum algorithms perform\nbetter than brute force search, with a quadratic speedup provided by Grover's\nalgorithm. To give evidence that the quadratic speedup is nearly optimal, we\ninitiate the study of quantum fine-grained complexity and introduce the Quantum\nStrong Exponential Time Hypothesis (QSETH), which is based on the assumption\nthat Grover's algorithm is optimal for CNF-SAT when the clause width is large.\nWe show that the na\\\"{i}ve Grover approach to closest pair in higher dimensions\nis optimal up to an $n^{o(1)}$ factor unless QSETH is false. We also study the\nbichromatic closest pair problem and the orthogonal vectors problem, with\nbroadly similar results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 18:01:23 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 15:00:28 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Aaronson", "Scott", ""], ["Chia", "Nai-Hui", ""], ["Lin", "Han-Hsuan", ""], ["Wang", "Chunhao", ""], ["Zhang", "Ruizhe", ""]]}, {"id": "1911.02040", "submitter": "Alexander Wolff", "authors": "William Evans and Ellen Gethner and Jack Spalding-Jamieson and\n  Alexander Wolff", "title": "Angle Covers: Algorithms and Complexity", "comments": null, "journal-ref": "Proc. 14th Int. Workshop Algorithms Comput. (WALCOM 2020), pages\n  94-106, LNCS 12049, Springer", "doi": "10.1007/978-3-030-39881-1_9", "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph with a rotation system, namely, for every vertex, a circular\nordering of the incident edges. Given such a graph, an angle cover maps every\nvertex to a pair of consecutive edges in the ordering -- an angle -- such that\neach edge participates in at least one such pair. We show that any graph of\nmaximum degree 4 admits an angle cover, give a poly-time algorithm for deciding\nif a graph with no degree-3 vertices has an angle-cover, and prove that, given\na graph of maximum degree 5, it is NP-hard to decide whether it admits an angle\ncover. We also consider extensions of the angle cover problem where every\nvertex selects a fixed number $a>1$ of angles or where an angle consists of\nmore than two consecutive edges. We show an application of angle covers to the\nproblem of deciding if the 2-blowup of a planar graph has isomorphic thickness\n2.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:05:04 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 17:51:21 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Evans", "William", ""], ["Gethner", "Ellen", ""], ["Spalding-Jamieson", "Jack", ""], ["Wolff", "Alexander", ""]]}, {"id": "1911.02218", "submitter": "Uma Girish", "authors": "Uma Girish, Ran Raz, Avishay Tal", "title": "Quantum versus Randomized Communication Complexity, with Efficient\n  Players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new type of separation between quantum and classical communication\ncomplexity which is obtained using quantum protocols where all parties are\nefficient, in the sense that they can be implemented by small quantum circuits\nwith oracle access to their inputs. More precisely, we give an explicit partial\nBoolean function that can be computed in the\nquantum-simultaneous-with-entanglement model of communication, however, every\ninteractive randomized protocol is of exponentially larger cost. Furthermore,\nall the parties in the quantum protocol can be implemented by quantum circuits\nof small size with blackbox access to the inputs. Our result qualitatively\nmatches the strongest known separation between quantum and classical\ncommunication complexity and is obtained using a quantum protocol where all\nparties are efficient.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 06:33:32 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Girish", "Uma", ""], ["Raz", "Ran", ""], ["Tal", "Avishay", ""]]}, {"id": "1911.02440", "submitter": "Alexander Golovnev", "authors": "Divesh Aggarwal, Huck Bennett, Alexander Golovnev, Noah\n  Stephens-Davidowitz", "title": "Fine-grained hardness of CVP(P) -- Everything that we can prove (and\n  nothing else)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a number of fine-grained hardness results for the Closest Vector\nProblem in the $\\ell_p$ norm ($\\mathrm{CVP}_p$), and its approximate and\nnon-uniform variants. First, we show that $\\mathrm{CVP}_p$ cannot be solved in\n$2^{(1-\\varepsilon)n}$ time for all $p \\notin 2\\mathbb{Z}$ and $\\varepsilon >\n0$, assuming the Strong Exponential Time Hypothesis (SETH). Second, we extend\nthis by showing that there is no $2^{(1-\\varepsilon)n}$-time algorithm for\napproximating $\\mathrm{CVP}_p$ to within a constant factor $\\gamma$ for such\n$p$ assuming a \"gap\" version of SETH, with an explicit relationship between\n$\\gamma$, $p$, and the arity $k = k(\\varepsilon)$ of the underlying hard CSP.\nThird, we show the same hardness result for (exact) $\\mathrm{CVP}_p$ with\npreprocessing (assuming non-uniform SETH). For exact \"plain\" $\\mathrm{CVP}_p$,\nthe same hardness result was shown in [Bennett, Golovnev, and\nStephens-Davidowitz FOCS 2017] for all but finitely many $p \\notin\n2\\mathbb{Z}$, where the set of exceptions depended on $\\varepsilon$ and was not\nexplicit. For the approximate and preprocessing problems, only very weak bounds\nwere known prior to this work. We also show that the restriction to $p \\notin\n2\\mathbb{Z}$ is in some sense inherent. In particular, we show that no\n\"natural\" reduction can rule out even a $2^{3n/4}$-time algorithm for\n$\\mathrm{CVP}_2$ under SETH. For this, we prove that the possible sets of\nclosest lattice vectors to a target in the $\\ell_2$ norm have quite rigid\nstructure, which essentially prevents them from being as expressive as\n$3$-CNFs. We prove these results using techniques from many different fields,\nincluding complex analysis, functional analysis, additive combinatorics, and\ndiscrete Fourier analysis. E.g., along the way, we give a new (and tighter)\nproof of Szemer\\'{e}di's cube lemma for the boolean cube.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 15:34:28 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:28:27 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Bennett", "Huck", ""], ["Golovnev", "Alexander", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1911.02534", "submitter": "Anurag Pandey", "authors": "Markus Bl\\\"aser, Christian Ikenmeyer, Vladimir Lysikov, Anurag Pandey,\n  Frank-Olaf Schreyer", "title": "Variety Membership Testing, Algebraic Natural Proofs, and Geometric\n  Complexity Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variety membership testing problem in the case when the variety\nis given as an orbit closure and the ambient space is the set of all 3-tensors.\nThe first variety that we consider is the slice rank variety, which consists of\nall 3-tensors of slice rank at most $r$. We show that the membership testing\nproblem for the slice rank variety is $\\NP$-hard. While the slice rank variety\nis a union of orbit closures, we define another variety, the minrank variety,\nexpressible as a single orbit closure. Our next result is the $\\NP$-hardness of\nmembership testing in the minrank variety, hence we establish the\n$\\NP$-hardness of the orbit closure containment problem for 3-tensors.\n  Algebraic natural proofs were recently introduced by Forbes, Shpilka and Volk\nand independently by Grochow, Kumar, Saks and Saraf. Bl\\\"aser et al. gave a\nversion of an algebraic natural proof barrier for the matrix completion problem\nwhich relies on $\\coNP \\subseteq \\exists \\BPP$. It implied that constructing\nequations for the corresponding variety should be hard. We generalize their\napproach to work with any family of varieties for which the membership problem\nis $\\NP$-hard and for which we can efficiently generate a dense subset.\nTherefore, a similar barrier holds for the slice rank and the minrank\nvarieties, too. This allows us to set up the slice rank and the minrank\nvarieties as a test-bed for geometric complexity theory (GCT). We determine the\nstabilizers of the tensors that generate the orbit closures of the two\nvarieties and prove that these tensors are almost characterized by their\nsymmetries. We prove several nontrivial equations for both the varieties using\ndifferent GCT methods. Many equations also work in the regime where membership\ntesting in the slice rank or minrank varieties is $\\NP$-hard. We view this as a\npromising sign that the GCT approach might indeed be successful.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:14:02 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Bl\u00e4ser", "Markus", ""], ["Ikenmeyer", "Christian", ""], ["Lysikov", "Vladimir", ""], ["Pandey", "Anurag", ""], ["Schreyer", "Frank-Olaf", ""]]}, {"id": "1911.02540", "submitter": "Anurag Pandey", "authors": "Gorav Jindal, Anurag Pandey, Himanshu Shukla, Charilaos Zisopoulos", "title": "How many zeros of a random sparse polynomial are real?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the number of real zeros of a univariate $k$-sparse polynomial\n$f$ over the reals, when the coefficients of $f$ come from independent standard\nnormal distributions. Recently B\\\"urgisser, Erg\\\"ur and Tonelli-Cueto showed\nthat the expected number of real zeros of $f$ in such cases is bounded by\n$O(\\sqrt{k} \\log k)$. In this work, we improve the bound to $O(\\sqrt{k})$ and\nalso show that this bound is tight by constructing a family of sparse support\nwhose expected number of real zeros is lower bounded by $\\Omega(\\sqrt{k})$. Our\nmain technique is an alternative formulation of the Kac integral by\nEdelman-Kostlan which allows us to bound the expected number of zeros of $f$ in\nterms of the expected number of zeros of polynomials of lower sparsity. Using\nour technique, we also recover the $O(\\log n)$ bound on the expected number of\nreal zeros of a dense polynomial of degree $n$ with coefficients coming from\nindependent standard normal distributions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:24:47 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jindal", "Gorav", ""], ["Pandey", "Anurag", ""], ["Shukla", "Himanshu", ""], ["Zisopoulos", "Charilaos", ""]]}, {"id": "1911.02555", "submitter": "Daniel Grier", "authors": "Daniel Grier, Luke Schaeffer", "title": "Interactive shallow Clifford circuits: quantum advantage against NC$^1$\n  and beyond", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work of Bravyi et al. and follow-up work by Bene Watts et al.\ndemonstrates a quantum advantage for shallow circuits: constant-depth quantum\ncircuits can perform a task which constant-depth classical (i.e., AC$^0$)\ncircuits cannot. Their results have the advantage that the quantum circuit is\nfairly practical, and their proofs are free of hardness assumptions (e.g.,\nfactoring is classically hard, etc.). Unfortunately, constant-depth classical\ncircuits are too weak to yield a convincing real-world demonstration of quantum\nadvantage. We attempt to hold on to the advantages of the above results, while\nincreasing the power of the classical model.\n  Our main result is a two-round interactive task which is solved by a\nconstant-depth quantum circuit (using only Clifford gates, between neighboring\nqubits of a 2D grid, with Pauli measurements), but such that any classical\nsolution would necessarily solve $\\oplus$L-hard problems. This implies a more\npowerful class of constant-depth classical circuits (e.g., AC$^0[p]$ for any\nprime $p$) unconditionally cannot perform the task. Furthermore, under standard\ncomplexity-theoretic conjectures, log-depth circuits and log-space Turing\nmachines cannot perform the task either.\n  Using the same techniques, we prove hardness results for weaker complexity\nclasses under more restrictive circuit topologies. Specifically, we give\nQNC$^0$ interactive tasks on $2 \\times n$ and $1 \\times n$ grids which require\nclassical simulations of power NC$^1$ and AC$^{0}[6]$, respectively. Moreover,\nthese hardness results are robust to a small constant fraction of error in the\nclassical simulation.\n  We use ideas and techniques from the theory of branching programs, quantum\ncontextuality, measurement-based quantum computation, and Kilian randomization.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:51:31 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Grier", "Daniel", ""], ["Schaeffer", "Luke", ""]]}, {"id": "1911.02675", "submitter": "Jonathan Lacotte", "authors": "Jonathan Lacotte and Mert Pilanci", "title": "Faster Least Squares Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate iterative methods with randomized preconditioners for solving\noverdetermined least-squares problems, where the preconditioners are based on a\nrandom embedding of the data matrix. We consider two distinct approaches: the\nsketch is either computed once (fixed preconditioner), or, the random\nprojection is refreshed at each iteration, i.e., sampled independently of\nprevious ones (varying preconditioners). Although fixed sketching-based\npreconditioners have received considerable attention in the recent literature,\nlittle is known about the performance of refreshed sketches. For a fixed\nsketch, we characterize the optimal iterative method, that is, the\npreconditioned conjugate gradient as well as its rate of convergence in terms\nof the subspace embedding properties of the random embedding. For refreshed\nsketches, we provide a closed-form formula for the expected error of the\niterative Hessian sketch (IHS), a.k.a. preconditioned steepest descent. In\ncontrast to the guarantees and analysis for fixed preconditioners based on\nsubspace embedding properties, our formula is exact and it involves the\nexpected inverse moments of the random projection. Our main technical\ncontribution is to show that this convergence rate is, surprisingly,\nunimprovable with heavy-ball momentum. Additionally, we construct the locally\noptimal first-order method whose convergence rate is bounded by that of the\nIHS. Based on these theoretical and numerical investigations, we do not observe\nthat the additional randomness of refreshed sketches provides a clear advantage\nover a fixed preconditioner. Therefore, we prescribe PCG as the method of\nchoice along with an optimized sketch size according to our analysis. Our\nprescribed sketch size yields state-of-the-art computational complexity for\nsolving highly overdetermined linear systems. Lastly, we illustrate the\nnumerical benefits of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 23:29:54 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 09:15:51 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 20:57:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lacotte", "Jonathan", ""], ["Pilanci", "Mert", ""]]}, {"id": "1911.02822", "submitter": "Seiichiro Tani", "authors": "Akinori Hosoyamada, Yu Sasaki, Seiichiro Tani, Keita Xagawa", "title": "Quantum Algorithm for the Multicollision Problem", "comments": "23 pages, 2 figures and 2 tables; a significantly revised version of\n  two conference papers (Asiacrypt 2017 [Cryptology ePrint Archive Report\n  2017/864] and PQCrypto 2019 [arXiv:1811.08097]) with additional time and\n  space complexity analyses and discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper presents a new quantum algorithm for finding\nmulticollisions, often denoted by $\\ell$-collisions, where an $\\ell$-collision\nfor a function is a set of $\\ell$ distinct inputs that are mapped by the\nfunction to the same value. The tight bound of quantum query complexity for\nfinding a $2$-collisions of a random function has been revealed to be\n$\\Theta(N^{1/3})$, where $N$ is the size of the range of the function, but\nneither the lower nor upper bounds are known for general $\\ell$-collisions. The\npaper first integrates the results from existing research to derive several new\nobservations, e.g.,~$\\ell$-collisions can be generated only with $O(N^{1/2})$\nquantum queries for any integer constant $\\ell$. It then provides a quantum\nalgorithm that finds an $\\ell$-collision for a random function with the average\nquantum query complexity of $O(N^{(2^{\\ell-1}-1) / (2^{\\ell}-1)})$, which\nmatches the tight bound of $\\Theta(N^{1/3})$ for $\\ell=2$ and improves upon the\nknown bounds, including the above simple bound of $O(N^{1/2})$. More generally,\nthe algorithm achieves the average quantum query complexity of $O\\big(c_N \\cdot\nN^{({2^{\\ell-1}-1})/({ 2^{\\ell}-1})}\\big)$ and runs over $\\tilde{O}\\big(c_N\n\\cdot N^{({2^{\\ell-1}-1})/({ 2^{\\ell}-1})}\\big)$ qubits in $\\tilde{O}\\big(c_N\n\\cdot N^{({2^{\\ell-1}-1})/({ 2^{\\ell}-1})}\\big)$ expected time for a random\nfunction $F\\colon X\\to Y$ such that $|X| \\geq \\ell \\cdot |Y| / c_N$ for any\n$1\\le c_N \\in o(N^{{1}/({2^\\ell - 1})})$. With the same complexities, it is\nactually able to find a multiclaw for random functions, which is harder to find\nthan a multicollision.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 09:56:00 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hosoyamada", "Akinori", ""], ["Sasaki", "Yu", ""], ["Tani", "Seiichiro", ""], ["Xagawa", "Keita", ""]]}, {"id": "1911.02911", "submitter": "Jonah Brown-Cohen", "authors": "Jonah Brown-Cohen and Prasad Raghavendra", "title": "Extended Formulation Lower Bounds for Refuting Random CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random constraint satisfaction problems (CSPs) such as random $3$-SAT are\nconjectured to be computationally intractable. The average case hardness of\nrandom $3$-SAT and other CSPs has broad and far-reaching implications on\nproblems in approximation, learning theory and cryptography.\n  In this work, we show subexponential lower bounds on the size of linear\nprogramming relaxations for refuting random instances of constraint\nsatisfaction problems. Formally, suppose $P : \\{0,1\\}^k \\to \\{0,1\\}$ is a\npredicate that supports a $t-1$-wise uniform distribution on its satisfying\nassignments. Consider the distribution of random instances of CSP $P$ with $m =\n\\Delta n$ constraints. We show that any linear programming extended formulation\nthat can refute instances from this distribution with constant probability must\nhave size at least\n$\\Omega\\left(\\exp\\left(\\left(\\frac{n^{t-2}}{\\Delta^2}\\right)^{\\frac{1-\\nu}{k}}\\right)\\right)$\nfor all $\\nu > 0$. For example, this yields a lower bound of size\n$\\exp(n^{1/3})$ for random $3$-SAT with a linear number of clauses.\n  We use the technique of pseudocalibration to directly obtain extended\nformulation lower bounds from the planted distribution. This approach bypasses\nthe need to construct Sherali-Adams integrality gaps in proving general LP\nlower bounds. As a corollary, one obtains a self-contained proof of\nsubexponential Sherali-Adams LP lower bounds for these problems. We believe the\nresult sheds light on the technique of pseudocalibration, a promising but\nconjectural approach to LP/SDP lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 14:02:19 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Brown-Cohen", "Jonah", ""], ["Raghavendra", "Prasad", ""]]}, {"id": "1911.03026", "submitter": "Tsuyoshi Yagita", "authors": "Duc A. Hoang, Akira Suzuki, Tsuyoshi Yagita", "title": "Reconfiguring k-path vertex covers", "comments": "29 pages, 4 figures, to appear in WALCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex subset $I$ of a graph $G$ is called a $k$-path vertex cover if every\npath on $k$ vertices in $G$ contains at least one vertex from $I$. The\n\\textsc{$k$-Path Vertex Cover Reconfiguration ($k$-PVCR)} problem asks if one\ncan transform one $k$-path vertex cover into another via a sequence of $k$-path\nvertex covers where each intermediate member is obtained from its predecessor\nby applying a given reconfiguration rule exactly once. We investigate the\ncomputational complexity of \\textsc{$k$-PVCR} from the viewpoint of graph\nclasses under the well-known reconfiguration rules: $\\mathsf{TS}$,\n$\\mathsf{TJ}$, and $\\mathsf{TAR}$. The problem for $k=2$, known as the\n\\textsc{Vertex Cover Reconfiguration (VCR)} problem, has been well-studied in\nthe literature. We show that certain known hardness results for \\textsc{VCR} on\ndifferent graph classes including planar graphs, bounded bandwidth graphs,\nchordal graphs, and bipartite graphs, can be extended for \\textsc{$k$-PVCR}. In\nparticular, we prove a complexity dichotomy for \\textsc{$k$-PVCR} on general\ngraphs: on those whose maximum degree is $3$ (and even planar), the problem is\n$\\mathtt{PSPACE}$-complete, while on those whose maximum degree is $2$ (i.e.,\npaths and cycles), the problem can be solved in polynomial time. Additionally,\nwe also design polynomial-time algorithms for \\textsc{$k$-PVCR} on trees under\neach of $\\mathsf{TJ}$ and $\\mathsf{TAR}$. Moreover, on paths, cycles, and\ntrees, we describe how one can construct a reconfiguration sequence between two\ngiven $k$-path vertex covers in a yes-instance. In particular, on paths, our\nconstructed reconfiguration sequence is shortest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:49:14 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hoang", "Duc A.", ""], ["Suzuki", "Akira", ""], ["Yagita", "Tsuyoshi", ""]]}, {"id": "1911.03272", "submitter": "David Purser", "authors": "Marco Gaboardi, Kobbi Nissim and David Purser", "title": "The Complexity of Verifying Loop-Free Programs as Differentially Private", "comments": null, "journal-ref": "47th International Colloquium on Automata, Languages, and\n  Programming (ICALP 2020). Volume 168 of LIPIcs, pages 129:1-129:17. Schloss\n  Dagstuhl - Leibniz-Zentrum fur Informatik, 2020", "doi": "10.4230/LIPIcs.ICALP.2020.129", "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of verifying differential privacy for loop-free programs\nwith probabilistic choice. Programs in this class can be seen as randomized\nBoolean circuits, which we will use as a formal model to answer two different\nquestions: first, deciding whether a program satisfies a prescribed level of\nprivacy; second, approximating the privacy parameters a program realizes. We\nshow that the problem of deciding whether a program satisfies\n$\\varepsilon$-differential privacy is $coNP^{\\#P}$-complete. In fact, this is\nthe case when either the input domain or the output range of the program is\nlarge. Further, we show that deciding whether a program is\n$(\\varepsilon,\\delta)$-differentially private is $coNP^{\\#P}$-hard, and in\n$coNP^{\\#P}$ for small output domains, but always in $coNP^{\\#P^{\\#P}}$.\nFinally, we show that the problem of approximating the level of differential\nprivacy is both $NP$-hard and $coNP$-hard. These results complement previous\nresults by Murtagh and Vadhan showing that deciding the optimal composition of\ndifferentially private components is $\\#P$-complete, and that approximating the\noptimal composition of differentially private components is in $P$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:07:30 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 11:14:18 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 10:20:51 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Gaboardi", "Marco", ""], ["Nissim", "Kobbi", ""], ["Purser", "David", ""]]}, {"id": "1911.03427", "submitter": "Jonathan Tidor", "authors": "Jacob Fox, Jonathan Tidor, and Yufei Zhao", "title": "Induced arithmetic removal: complexity 1 patterns over finite fields", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an arithmetic analog of the induced graph removal lemma for\ncomplexity 1 patterns over finite fields. Informally speaking, we show that\ngiven a fixed collection of $r$-colored complexity 1 arithmetic patterns over\n$\\mathbb F_q$, every coloring $\\phi \\colon \\mathbb F_q^n \\setminus\\{0\\} \\to\n[r]$ with $o(1)$ density of every such pattern can be recolored on an\n$o(1)$-fraction of the space so that no such pattern remains.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 18:22:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fox", "Jacob", ""], ["Tidor", "Jonathan", ""], ["Zhao", "Yufei", ""]]}, {"id": "1911.03748", "submitter": "Nathan Keller", "authors": "Nathan Keller and Ohad Klein", "title": "Quantum speedups need structure", "comments": "Unfortunately, our proof contains a serious flaw. Specifically, Lemma\n  5.3 does not prove the assertion it claims to prove and this collapses the\n  entire argument. We thank Paata Ivanishvili for pointing out the flaw, and\n  apologize to the community for posting an eventually incorrect proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the following conjecture, raised by Aaronson and Ambainis in 2008:\nLet $f:\\{-1,1\\}^n \\rightarrow [-1,1]$ be a multilinear polynomial of degree\n$d$. Then there exists a variable $x_i$ whose influence on $f$ is at least\n$\\mathrm{poly}(\\mathrm{Var}(f)/d)$.\n  As was shown by Aaronson and Ambainis, this result implies the following\nwell-known conjecture on the power of quantum computing, dating back to 1999:\nLet $Q$ be a quantum algorithm that makes $T$ queries to a Boolean input and\nlet $\\epsilon,\\delta > 0$. Then there exists a deterministic classical\nalgorithm that makes $\\mathrm{poly}(T,1/\\epsilon,1/\\delta)$ queries to the\ninput and that approximates $Q$'s acceptance probability to within an additive\nerror $\\epsilon$ on a $1-\\delta$ fraction of inputs. In other words, any\nquantum algorithm can be simulated on most inputs by a classical algorithm\nwhich is only polynomially slower, in terms of query complexity.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 18:24:31 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 00:14:36 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Keller", "Nathan", ""], ["Klein", "Ohad", ""]]}, {"id": "1911.03757", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms", "title": "Universal Communication, Universal Graphs, and Graph Labeling", "comments": "26 pages, 1 figure. To appear in ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a communication model called universal SMP, in which Alice and\nBob receive a function $f$ belonging to a family $\\mathcal{F}$, and inputs $x$\nand $y$. Alice and Bob use shared randomness to send a message to a third party\nwho cannot see $f, x, y$, or the shared randomness, and must decide $f(x,y)$.\nOur main application of universal SMP is to relate communication complexity to\ngraph labeling, where the goal is to give a short label to each vertex in a\ngraph, so that adjacency or other functions of two vertices $x$ and $y$ can be\ndetermined from the labels $\\ell(x),\\ell(y)$. We give a universal SMP protocol\nusing $O(k^2)$ bits of communication for deciding whether two vertices have\ndistance at most $k$ on distributive lattices (generalizing the $k$-Hamming\nDistance problem in communication complexity), and explain how this implies an\n$O(k^2\\log n)$ labeling scheme for determining $\\mathrm{dist}(x,y) \\leq k$ on\ndistributive lattices with size $n$; in contrast, we show that a universal SMP\nprotocol for determining $\\mathrm{dist}(x,y) \\leq 2$ in modular lattices (a\nsuperset of distributive lattices) has super-constant $\\Omega(n^{1/4})$\ncommunication cost. On the other hand, we demonstrate that many graph families\nknown to have efficient adjacency labeling schemes, such as trees,\nlow-arboricity graphs, and planar graphs, admit constant-cost communication\nprotocols for adjacency. Trees also have an $O(k)$ protocol for deciding\n$\\mathrm{dist}(x,y) \\leq k$ and planar graphs have an $O(1)$ protocol for\n$\\mathrm{dist}(x,y) \\leq 2$, which implies a new $O(\\log n)$ labeling scheme\nfor the same problem on planar graphs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 19:11:08 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Harms", "Nathaniel", ""]]}, {"id": "1911.03858", "submitter": "Andrii Riazanov", "authors": "Venkatesan Guruswami, Andrii Riazanov, Min Ye", "title": "Ar{\\i}kan meets Shannon: Polar codes with near-optimal convergence to\n  channel capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $W$ be a binary-input memoryless symmetric (BMS) channel with Shannon\ncapacity $I(W)$ and fix any $\\alpha > 0$. We construct, for any sufficiently\nsmall $\\delta > 0$, binary linear codes of block length\n$O(1/\\delta^{2+\\alpha})$ and rate $I(W)-\\delta$ that enable reliable\ncommunication on $W$ with quasi-linear time encoding and decoding. Shannon's\nnoisy coding theorem established the \\emph{existence} of such codes (without\nefficient constructions or decoding) with block length $O(1/\\delta^2)$. This\nquadratic dependence on the gap $\\delta$ to capacity is known to be best\npossible. Our result thus yields a constructive version of Shannon's theorem\nwith near-optimal convergence to capacity as a function of the block length.\nThis resolves a central theoretical challenge associated with the attainment of\nShannon capacity. Previously such a result was only known for the erasure\nchannel.\n  Our codes are a variant of Ar{\\i}kan's polar codes based on multiple\ncarefully constructed local kernels, one for each intermediate channel that\narises in the decoding. A crucial ingredient in the analysis is a strong\nconverse of the noisy coding theorem when communicating using random linear\ncodes on arbitrary BMS channels. Our converse theorem shows extreme\nunpredictability of even a single message bit for random coding at rates\nslightly above capacity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 05:45:33 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 20:51:11 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Riazanov", "Andrii", ""], ["Ye", "Min", ""]]}, {"id": "1911.03989", "submitter": "Bahman Kalantari", "authors": "Bahman Kalantari", "title": "On the Equivalence of SDP Feasibility and a Convex Hull Relaxation for\n  System of Quadratic Equations", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show {\\it semidefinite programming} (SDP) feasibility problem is\nequivalent to solving a {\\it convex hull relaxation} (CHR) for a finite system\nof quadratic equations. On the one hand, this offers a simple description of\nSDP. On the other hand, this equivalence makes it possible to describe a\nversion of the {\\it Triangle Algorithm} for SDP feasibility based on solving\nCHR. Specifically, the Triangle Algorithm either computes an approximation to\nthe least-norm feasible solution of SDP, or using its {\\it distance duality},\nprovides a separation when no solution within a prescribed norm exists. The\nworst-case complexity of each iteration is computing the largest eigenvalue of\na symmetric matrix arising in that iteration. Alternate complexity bounds on\nthe total number of iterations can be derived. The Triangle Algorithm thus\nprovides an alternative to the existing interior-point algorithms for SDP\nfeasibility and SDP optimization. In particular, based on a preliminary\ncomputational result, we can efficiently solve SDP relaxation of {\\it binary\nquadratic} feasibility via the Triangle Algorithm. This finds application in\nsolving SDP relaxation of MAX-CUT. We also show in the case of testing the\nfeasibility of a system of convex quadratic inequalities, the problem is\nreducible to a corresponding CHR, where the worst-case complexity of each\niteration via the Triangle Algorithm is solving a {\\it trust region\nsubproblem}. Gaining from these results, we discuss potential extension of CHR\nand the Triangle Algorithm to solving general system of polynomial equations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 20:02:56 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 17:19:30 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kalantari", "Bahman", ""]]}, {"id": "1911.03990", "submitter": "Christian Ikenmeyer", "authors": "Christian Ikenmeyer and Umangathan Kandasamy", "title": "Implementing geometric complexity theory: On the separation of orbit\n  closures via symmetries", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the difference between group orbits and their closures is a key\ndifficulty in geometric complexity theory (GCT): While the GCT program is set\nup to separate certain orbit closures, many beautiful mathematical properties\nare only known for the group orbits, in particular close relations with\nsymmetry groups and invariant spaces, while the orbit closures seem much more\ndifficult to understand. However, in order to prove lower bounds in algebraic\ncomplexity theory, considering group orbits is not enough.\n  In this paper we tighten the relationship between the orbit of the power sum\npolynomial and its closure, so that we can separate this orbit closure from the\norbit closure of the product of variables by just considering the symmetry\ngroups of both polynomials and their representation theoretic decomposition\ncoefficients. In a natural way our construction yields a multiplicity\nobstruction that is neither an occurrence obstruction, nor a so-called\nvanishing ideal occurrence obstruction. All multiplicity obstructions so far\nhave been of one of these two types.\n  Our paper is the first implementation of the ambitious approach that was\noriginally suggested in the first papers on geometric complexity theory by\nMulmuley and Sohoni (SIAM J Comput 2001, 2008): Before our paper, all existence\nproofs of obstructions only took into account the symmetry group of one of the\ntwo polynomials (or tensors) that were to be separated. In our paper the\nmultiplicity obstruction is obtained by comparing the representation theoretic\ndecomposition coefficients of both symmetry groups.\n  Our proof uses a semi-explicit description of the coordinate ring of the\norbit closure of the power sum polynomial in terms of Young tableaux, which\nenables its comparison to the coordinate ring of the orbit.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 20:08:44 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ikenmeyer", "Christian", ""], ["Kandasamy", "Umangathan", ""]]}, {"id": "1911.04026", "submitter": "Daniel Leivant", "authors": "Daniel Leivant", "title": "A generic imperative language for polynomial time", "comments": "18 pages, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ramification method in Implicit Computational Complexity has been\nassociated with functional programming, but adapting it to generic imperative\nprogramming is highly desirable, given the wider algorithmic applicability of\nimperative programming. We introduce a new approach to ramification which,\namong other benefits, adapts readily to fully general imperative programming.\nThe novelty is in ramifying finite second-order objects, namely finite\nstructures, rather than ramifying elements of free algebras. In so doing we\nbridge between Implicit Complexity's type theoretic characterizations of\nfeasibility, and the data-flow approach of Static Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 01:15:07 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 02:10:18 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Leivant", "Daniel", ""]]}, {"id": "1911.04112", "submitter": "Jie-Hong Jiang", "authors": "Nian-Ze Lee and Jie-Hong R. Jiang", "title": "Dependency Stochastic Boolean Satisfiability: A Logical Formalism for\n  NEXPTIME Decision Problems with Uncertainty", "comments": "10 pages, 5 figures. A condensed version of this work is published in\n  the AAAI Conference on Artificial Intelligence (AAAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Boolean Satisfiability (SSAT) is a logical formalism to model\ndecision problems with uncertainty, such as Partially Observable Markov\nDecision Process (POMDP) for verification of probabilistic systems. SSAT,\nhowever, is limited by its descriptive power within the PSPACE complexity\nclass. More complex problems, such as the NEXPTIME-complete Decentralized POMDP\n(Dec-POMDP), cannot be succinctly encoded with SSAT. To provide a logical\nformalism of such problems, we extend the Dependency Quantified Boolean Formula\n(DQBF), a representative problem in the NEXPTIME-complete class, to its\nstochastic variant, named Dependency SSAT (DSSAT), and show that DSSAT is also\nNEXPTIME-complete. We demonstrate the potential applications of DSSAT to\ncircuit synthesis of probabilistic and approximate design. Furthermore, to\nstudy the descriptive power of DSSAT, we establish a polynomial-time reduction\nfrom Dec-POMDP to DSSAT. With the theoretical foundations paved in this work,\nwe hope to encourage the development of DSSAT solvers for potential broad\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 06:58:25 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 03:16:23 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 08:46:47 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lee", "Nian-Ze", ""], ["Jiang", "Jie-Hong R.", ""]]}, {"id": "1911.04122", "submitter": "Shi-Xin Zhang", "authors": "Shi-Xin Zhang", "title": "Classification on the Computational Complexity of Spin Models", "comments": "4.6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.str-el cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide a unifying framework to investigate the\ncomputational complexity of classical spin models and give the full\nclassification on spin models in terms of system dimensions, randomness,\nexternal magnetic fields and types of spin coupling. We further discuss about\nthe implications of NP-complete Hamiltonian models in physics and the\nfundamental limitations of all numerical methods imposed by such models. We\nconclude by a brief discussion on the picture when quantum computation and\nquantum complexity theory are included.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 07:57:21 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhang", "Shi-Xin", ""]]}, {"id": "1911.04734", "submitter": "Yuki Takeuchi", "authors": "Yuki Takeuchi, Tomoyuki Morimae, Seiichiro Tani", "title": "Sumcheck-based delegation of quantum computing to rational server", "comments": "28 pages, 1 figure, Because of the character limitation, the abstract\n  was shortened compared with the PDF file", "journal-ref": "Proceedings of the 16th International Conference on Theory and\n  Applications of Models of Computation (TAMC 2020), pp. 69-81, 2020", "doi": "10.1007/978-3-030-59267-7_7", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delegated quantum computing enables a client with a weak computational power\nto delegate quantum computing to a remote quantum server in such a way that the\nintegrity of the server is efficiently verified by the client. Recently, a new\nmodel of delegated quantum computing has been proposed, namely, rational\ndelegated quantum computing. In this model, after the client interacts with the\nserver, the client pays a reward to the server. The rational server sends\nmessages that maximize the expected value of the reward. It is known that the\nclassical client can delegate universal quantum computing to the rational\nquantum server in one round. In this paper, we propose novel one-round rational\ndelegated quantum computing protocols by generalizing the classical rational\nsumcheck protocol. The construction of the previous rational protocols depends\non gate sets, while our sumcheck technique can be easily realized with any\nlocal gate set. Furthermore, as with the previous protocols, our reward\nfunction satisfies natural requirements. We also discuss the reward gap. Simply\nspeaking, the reward gap is a minimum loss on the expected value of the\nserver's reward incurred by the server's behavior that makes the client accept\nan incorrect answer. Although our sumcheck-based protocols have only\nexponentially small reward gaps as with the previous protocols, we show that a\nconstant reward gap can be achieved if two non-communicating but entangled\nrational servers are allowed. We also discuss that a single rational server is\nsufficient under the (widely-believed) assumption that the learning-with-errors\nproblem is hard for polynomial-time quantum computing. Apart from these\nresults, we show, under a certain condition, the equivalence between $rational$\nand $ordinary$ delegated quantum computing protocols. Based on this\nequivalence, we give a reward-gap amplification method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 08:32:33 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Takeuchi", "Yuki", ""], ["Morimae", "Tomoyuki", ""], ["Tani", "Seiichiro", ""]]}, {"id": "1911.04871", "submitter": "Bernhard Nebel", "authors": "Bernhard Nebel", "title": "On the Computational Complexity of Multi-Agent Pathfinding on Directed\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of the computational complexity of multi-agent pathfinding\non directed graphs has been an open problem for many years. For undirected\ngraphs, solvability can be decided in polynomial time, as has been shown\nalready in the eighties. Further, recently it has been shown that a special\ncase on directed graphs is solvable in polynomial time. In this paper, we show\nthat the problem is NP-hard in the general case. In addition, some upper bounds\nare proven.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 10:55:19 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Nebel", "Bernhard", ""]]}, {"id": "1911.05416", "submitter": "Warut Suksompong", "authors": "Paul W. Goldberg, Alexandros Hollender, Warut Suksompong", "title": "Contiguous Cake Cutting: Hardness Results and Approximation Algorithms", "comments": "Appears in the 34th AAAI Conference on Artificial Intelligence\n  (AAAI), 2020", "journal-ref": "Journal of Artificial Intelligence Research, 69:109-141 (2020)", "doi": "10.1613/jair.1.12222", "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fair allocation of a cake, which serves as a metaphor for a\ndivisible resource, under the requirement that each agent should receive a\ncontiguous piece of the cake. While it is known that no finite envy-free\nalgorithm exists in this setting, we exhibit efficient algorithms that produce\nallocations with low envy among the agents. We then establish NP-hardness\nresults for various decision problems on the existence of envy-free\nallocations, such as when we fix the ordering of the agents or constrain the\npositions of certain cuts. In addition, we consider a discretized setting where\nindivisible items lie on a line and show a number of hardness results extending\nand strengthening those from prior work. Finally, we investigate connections\nbetween approximate and exact envy-freeness, as well as between continuous and\ndiscrete cake cutting.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 12:06:48 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 21:36:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Goldberg", "Paul W.", ""], ["Hollender", "Alexandros", ""], ["Suksompong", "Warut", ""]]}, {"id": "1911.05686", "submitter": "Florian Speelman", "authors": "Harry Buhrman, Subhasree Patro, Florian Speelman", "title": "The Quantum Strong Exponential-Time Hypothesis", "comments": "changes: updated grant information", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong exponential-time hypothesis (SETH) is a commonly used conjecture\nin the field of complexity theory. It states that CNF formulas cannot be\nanalyzed for satisfiability with a speedup over exhaustive search. This\nhypothesis and its variants gave rise to a fruitful field of research,\nfine-grained complexity, obtaining (mostly tight) lower bounds for many\nproblems in P whose unconditional lower bounds are hard to find. In this work,\nwe introduce a framework of Quantum Strong Exponential-Time Hypotheses, as\nquantum analogues to SETH.\n  Using the QSETH framework, we are able to translate quantum query lower\nbounds on black-box problems to conditional quantum time lower bounds for many\nproblems in BQP. As an example, we illustrate the use of the QSETH by providing\na conditional quantum time lower bound of $\\Omega(n^{1.5})$ for the Edit\nDistance problem. We also show that the $n^2$ SETH-based lower bound for a\nrecent scheme for Proofs of Useful Work, based on the Orthogonal Vectors\nproblem holds for quantum computation assuming QSETH, maintaining a quadratic\ngap between verifier and prover.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 18:09:20 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 15:27:39 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Buhrman", "Harry", ""], ["Patro", "Subhasree", ""], ["Speelman", "Florian", ""]]}, {"id": "1911.05834", "submitter": "Ronny Tredup", "authors": "Ronny Tredup", "title": "The Complexity of Synthesizing nop-Equipped Boolean Nets from g-Bounded\n  Inputs (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean Petri nets equipped with nop allow places and transitions to be\nindependent by being related by nop. We characterize for any fixed natural\nnumber g the computational complexity of synthesizing nop-equipped Boolean\nPetri nets from labeled directed graphs whose states have at most g incoming\nand at most g outgoing arcs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 05:16:02 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Tredup", "Ronny", ""]]}, {"id": "1911.05911", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Recent Advances in Algorithmic High-Dimensional Robust Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in the presence of outliers is a fundamental problem in statistics.\nUntil recently, all known efficient unsupervised learning algorithms were very\nsensitive to outliers in high dimensions. In particular, even for the task of\nrobust mean estimation under natural distributional assumptions, no efficient\nalgorithm was known. Recent work in theoretical computer science gave the first\nefficient robust estimators for a number of fundamental statistical tasks,\nincluding mean and covariance estimation. Since then, there has been a flurry\nof research activity on algorithmic high-dimensional robust estimation in a\nrange of settings. In this survey article, we introduce the core ideas and\nalgorithmic techniques in the emerging area of algorithmic high-dimensional\nrobust statistics with a focus on robust mean estimation. We also provide an\noverview of the approaches that have led to computationally efficient robust\nestimators for a range of broader statistical tasks and discuss new directions\nand opportunities for future work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:56:56 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "1911.06358", "submitter": "Suprovat Ghoshal", "authors": "Suprovat Ghoshal and Rishi Saket", "title": "Hardness of Learning DNFs using Halfspaces", "comments": "27 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning $t$-term DNF formulas (for $t = O(1)$) has been\nstudied extensively in the PAC model since its introduction by Valiant (STOC\n1984). A $t$-term DNF can be efficiently learnt using a $t$-term DNF only if $t\n= 1$ i.e., when it is an AND, while even weakly learning a $2$-term DNF using a\nconstant term DNF was shown to be NP-hard by Khot and Saket (FOCS 2008). On the\nother hand, Feldman et al. (FOCS 2009) showed the hardness of weakly learning a\nnoisy AND using a halfspace -- the latter being a generalization of an AND,\nwhile Khot and Saket (STOC 2008) showed that an intersection of two halfspaces\nis hard to weakly learn using any function of constantly many halfspaces. The\nquestion of whether a $2$-term DNF is efficiently learnable using $2$ or\nconstantly many halfspaces remained open.\n  In this work we answer this question in the negative by showing the hardness\nof weakly learning a $2$-term DNF as well as a noisy AND using any function of\na constant number of halfspaces. In particular we prove the following. For any\nconstants $\\nu, \\zeta > 0$ and $\\ell \\in \\mathbb{N}$, given a distribution over\npoint-value pairs $\\{0,1\\}^n \\times \\{0,1\\}$, it is NP-hard to decide whether,\n  YES Case: There is a $2$-term DNF that classifies all the points of the\ndistribution, and an AND that classifies at least $1-\\zeta$ fraction of the\npoints correctly.\n  NO Case: Any boolean function depending on at most $\\ell$ halfspaces\nclassifies at most $1/2 + \\nu$ fraction of the points of the distribution\ncorrectly.\n  Our result generalizes and strengthens the previous best results mentioned\nabove on the hardness of learning a $2$-term DNF, learning an intersection of\ntwo halfspaces, and learning a noisy AND.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 19:53:02 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ghoshal", "Suprovat", ""], ["Saket", "Rishi", ""]]}, {"id": "1911.06664", "submitter": "Julien Langou", "authors": "Auguste Olivry, Julien Langou, Louis-No\\\"el Pouchet, P. Sadayappan,\n  and Fabrice Rastello", "title": "Automated Derivation of Parametric Data Movement Lower Bounds for Affine\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most relevant computation, the energy and time needed for data movement\ndominates that for performing arithmetic operations on all computing systems\ntoday. Hence it is of critical importance to understand the minimal total data\nmovement achievable during the execution of an algorithm. The achieved total\ndata movement for different schedules of an algorithm can vary widely depending\non how efficiently the cache is used, e.g., untiled versus effectively tiled\nmatrix-matrix multiplication. A significant current challenge is that no\nexisting tool is able to meaningfully quantify the potential reduction to the\ndata movement of a computation that can be achieved by more effective use of\nthe cache through operation rescheduling. Asymptotic parametric expressions of\ndata movement lower bounds have previously been manually derived for a limited\nnumber of algorithms, often without scaling constants. In this paper, we\npresent the first compile-time approach for deriving non-asymptotic parametric\nexpressions of data movement lower bounds for arbitrary affine computations.\nThe approach has been implemented in a fully automatic tool (IOLB) that can\ngenerate these lower bounds for input affine programs.\n  IOLB's use is demonstrated by exercising it on all the benchmarks of the\nPolyBench suite. The advantages of IOLB are many: (1) IOLB enables us to derive\nbounds for few dozens of algorithms for which these lower bounds have never\nbeen derived. This reflects an increase of productivity by automation. (2)\nAnyone is able to obtain these lower bounds through IOLB, no expertise is\nrequired. (3) For some of the most well-studied algorithms, the lower bounds\nobtained by \\tool are higher than any previously reported manually derived\nlower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 14:31:28 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Olivry", "Auguste", ""], ["Langou", "Julien", ""], ["Pouchet", "Louis-No\u00ebl", ""], ["Sadayappan", "P.", ""], ["Rastello", "Fabrice", ""]]}, {"id": "1911.06738", "submitter": "Iddo Tzameret", "authors": "Yaroslav Alekseev and Dima Grigoriev and Edward A. Hirsch and Iddo\n  Tzameret", "title": "Semi-Algebraic Proofs, IPS Lower Bounds and the $\\tau$-Conjecture: Can a\n  Natural Number be Negative?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the binary value principle which is a simple subset-sum instance\nexpressing that a natural number written in binary cannot be negative, relating\nit to central problems in proof and algebraic complexity. We prove conditional\nsuperpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of\nthis instance, based on a well-known hypothesis by Shub and Smale about the\nhardness of computing factorials, where IPS is the strong algebraic proof\nsystem introduced by Grochow and Pitassi (2018). Conversely, we show that short\nIPS refutations of this instance bridge the gap between sufficiently strong\nalgebraic and semi-algebraic proof systems. Our results extend to full-fledged\nIPS the paradigm introduced in Forbes et al. (2016), whereby lower bounds\nagainst subsystems of IPS were obtained using restricted algebraic circuit\nlower bounds, and demonstrate that the binary value principle captures the\nadvantage of semi-algebraic over algebraic reasoning, for sufficiently strong\nsystems. Specifically, we show the following: (abstract continues in document.)\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:43:29 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Alekseev", "Yaroslav", ""], ["Grigoriev", "Dima", ""], ["Hirsch", "Edward A.", ""], ["Tzameret", "Iddo", ""]]}, {"id": "1911.06793", "submitter": "Jonathan Tidor", "authors": "Jonathan Tidor and Yufei Zhao", "title": "Testing linear-invariant properties", "comments": "40 pages; updated with significantly improved main result", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fix a prime $p$ and a positive integer $R$. We study the property testing of\nfunctions $\\mathbb F_p^n\\to[R]$. We say that a property is testable if there\nexists an oblivious tester for this property with one-sided error and constant\nquery complexity. Furthermore, a property is proximity oblivious-testable\n(PO-testable) if the test is also independent of the proximity parameter\n$\\epsilon$. It is known that a number of natural properties such as linearity\nand being a low degree polynomial are PO-testable. These properties are\nexamples of linear-invariant properties, meaning that they are preserved under\nlinear automorphisms of the domain. Following work of Kaufman and Sudan, the\nstudy of linear-invariant properties has been an important problem in\narithmetic property testing.\n  A central conjecture in this field, proposed by Bhattacharyya, Grigorescu,\nand Shapira, is that a linear-invariant property is testable if and only if it\nis semi subspace-hereditary. We prove two results, the first resolves this\nconjecture and the second classifies PO-testable properties.\n  (1) A linear-invariant property is testable if and only if it is semi\nsubspace-hereditary.\n  (2) A linear-invariant property is PO-testable if and only if it is locally\ncharacterized.\n  Our innovations are two-fold. We give a more powerful version of the\ncompactness argument first introduced by Alon and Shapira. This relies on a new\nstrong arithmetic regularity lemma in which one mixes different levels of\nGowers uniformity. This allows us to extend the work of Bhattacharyya, Fischer,\nHatami, Hatami, and Lovett by removing the bounded complexity restriction in\ntheir work. Our second innovation is a novel recoloring technique called\npatching. This Ramsey-theoretic technique is critical for working in the\nlinear-invariant setting and allows us to remove the translation-invariant\nrestriction present in previous work.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:28:23 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 17:46:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Tidor", "Jonathan", ""], ["Zhao", "Yufei", ""]]}, {"id": "1911.06924", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova and Erik Waingarten", "title": "Approximating the Distance to Monotonicity of Boolean Functions", "comments": "To be published in Random Structures & Algorithms", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2021.80", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design a nonadaptive algorithm that, given oracle access to a function $f:\n\\{0,1\\}^n \\to \\{0,1\\}$ which is $\\alpha$-far from monotone, makes poly$(n,\n1/\\alpha)$ queries and returns an estimate that, with high probability, is an\n$\\widetilde{O}(\\sqrt{n})$-approximation to the distance of $f$ to monotonicity.\nThe analysis of our algorithm relies on an improvement to the directed\nisoperimetric inequality of Khot, Minzer, and Safra (SIAM J. Comput., 2018).\nFurthermore, we rule out a poly$(n, 1/\\alpha)$-query nonadaptive algorithm that\napproximates the distance to monotonicity significantly better by showing that,\nfor all constant $\\kappa > 0,$ every nonadaptive $n^{1/2 -\n\\kappa}$-approximation algorithm for this problem requires $2^{n^\\kappa}$\nqueries. This answers a question of Seshadhri (Property Testing Review, 2014)\nfor the case of nonadaptive algorithms. We obtain our lower bound by proving an\nanalogous bound for erasure-resilient (and tolerant) testers. Our method also\nyields the same lower bounds for unateness and being a $k$-junta.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:43:43 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 08:35:43 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1911.07124", "submitter": "Matthew Groff S.", "authors": "Matt Groff", "title": "Faster Integer Multiplication Using Preprocessing", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.NT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A New Number Theoretic Transform(NTT), which is a form of FFT, is introduced,\nthat is faster than FFTs. Also, a multiplication algorithm is introduced that\nuses this to perform integer multiplication faster than O(n log n). It uses\npreprocessing to achieve an upper bounds of (n log n/(log log n/ log log log\nn).\n  Also, we explore the possibility of O(n) time multiplication via NTTs that\nrequire only O(n) operations, using preprocessing.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 01:18:03 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 00:15:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Groff", "Matt", ""]]}, {"id": "1911.07306", "submitter": "Simon Apers", "authors": "Simon Apers and Ronald de Wolf", "title": "Quantum Speedup for Graph Sparsification, Cut Approximation and\n  Laplacian Solving", "comments": "v2: several small improvements to the text. An extended abstract will\n  appear in FOCS'20; v3: corrected a minor mistake in Appendix A", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sparsification underlies a large number of algorithms, ranging from\napproximation algorithms for cut problems to solvers for linear systems in the\ngraph Laplacian. In its strongest form, \"spectral sparsification\" reduces the\nnumber of edges to near-linear in the number of nodes, while approximately\npreserving the cut and spectral structure of the graph. In this work we\ndemonstrate a polynomial quantum speedup for spectral sparsification and many\nof its applications. In particular, we give a quantum algorithm that, given a\nweighted graph with $n$ nodes and $m$ edges, outputs a classical description of\nan $\\epsilon$-spectral sparsifier in sublinear time\n$\\tilde{O}(\\sqrt{mn}/\\epsilon)$. This contrasts with the optimal classical\ncomplexity $\\tilde{O}(m)$. We also prove that our quantum algorithm is optimal\nup to polylog-factors. The algorithm builds on a string of existing results on\nsparsification, graph spanners, quantum algorithms for shortest paths, and\nefficient constructions for $k$-wise independent random strings. Our algorithm\nimplies a quantum speedup for solving Laplacian systems and for approximating a\nrange of cut problems such as min cut and sparsest cut.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 17:29:40 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 17:36:12 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 12:29:02 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Apers", "Simon", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1911.07375", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Top-down induction of decision trees: rigorous guarantees and inherent\n  limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following heuristic for building a decision tree for a function\n$f : \\{0,1\\}^n \\to \\{\\pm 1\\}$. Place the most influential variable $x_i$ of $f$\nat the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the\nleft and right subtrees respectively; terminate once the tree is an\n$\\varepsilon$-approximation of $f$. We analyze the quality of this heuristic,\nobtaining near-matching upper and lower bounds:\n  $\\circ$ Upper bound: For every $f$ with decision tree size $s$ and every\n$\\varepsilon \\in (0,\\frac1{2})$, this heuristic builds a decision tree of size\nat most $s^{O(\\log(s/\\varepsilon)\\log(1/\\varepsilon))}$.\n  $\\circ$ Lower bound: For every $\\varepsilon \\in (0,\\frac1{2})$ and $s \\le\n2^{\\tilde{O}(\\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that\nthis heuristic builds a decision tree of size $s^{\\tilde{\\Omega}(\\log s)}$.\n  We also obtain upper and lower bounds for monotone functions:\n$s^{O(\\sqrt{\\log s}/\\varepsilon)}$ and $s^{\\tilde{\\Omega}(\\sqrt[4]{\\log s } )}$\nrespectively. The lower bound disproves conjectures of Fiat and Pechyony (2004)\nand Lee (2009).\n  Our upper bounds yield new algorithms for properly learning decision trees\nunder the uniform distribution. We show that these algorithms---which are\nmotivated by widely employed and empirically successful top-down decision tree\nlearning heuristics such as ID3, C4.5, and CART---achieve provable guarantees\nthat compare favorably with those of the current fastest algorithm (Ehrenfeucht\nand Haussler, 1989). Our lower bounds shed new light on the limitations of\nthese heuristics.\n  Finally, we revisit the classic work of Ehrenfeucht and Haussler. We extend\nit to give the first uniform-distribution proper learning algorithm that\nachieves polynomial sample and memory complexity, while matching its\nstate-of-the-art quasipolynomial runtime.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:25:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1911.07378", "submitter": "Roie Levin", "authors": "Parikshit Gopalan, Roie Levin and Udi Wieder", "title": "Finding Skewed Subcubes Under a Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Say that we are given samples from a distribution $\\psi$ over an\n$n$-dimensional space. We expect or desire $\\psi$ to behave like a product\ndistribution (or a $k$-wise independent distribution over its marginals for\nsmall $k$). We propose the problem of enumerating/list-decoding all large\nsubcubes where the distribution $\\psi$ deviates markedly from what we expect;\nwe refer to such subcubes as skewed subcubes. Skewed subcubes are certificates\nof dependencies between small subsets of variables in $\\psi$. We motivate this\nproblem by showing that it arises naturally in the context of algorithmic\nfairness and anomaly detection.\n  In this work we focus on the special but important case where the space is\nthe Boolean hypercube, and the expected marginals are uniform. We show that the\nobvious definition of skewed subcubes can lead to intractable list sizes, and\npropose a better definition of a minimal skewed subcube, which are subcubes\nwhose skew cannot be attributed to a larger subcube that contains it. Our main\ntechnical contribution is a list-size bound for this definition and an\nalgorithm to efficiently find all such subcubes. Both the bound and the\nalgorithm rely on Fourier-analytic techniques, especially the powerful\nhypercontractive inequality.\n  On the lower bounds side, we show that finding skewed subcubes is as hard as\nthe sparse noisy parity problem, and hence our algorithms cannot be improved on\nsubstantially without a breakthrough on this problem which is believed to be\nintractable. Motivated by this, we study alternate models allowing query access\nto $\\psi$ where finding skewed subcubes might be easier.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:32:58 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 02:13:20 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Gopalan", "Parikshit", ""], ["Levin", "Roie", ""], ["Wieder", "Udi", ""]]}, {"id": "1911.07536", "submitter": "Torstein J. F. Str{\\o}mme", "authors": "Fedor V. Fomin and Torstein J. F. Str{\\o}mme", "title": "Time-inconsistent Planning: Simple Motivation Is Hard to Find", "comments": "An extended abstract of this paper is accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of the graph-theoretic time-inconsistent planning model\ndue to Kleinberg and Oren, it has been possible to investigate the\ncomputational complexity of how a task designer best can support a\npresent-biased agent in completing the task. In this paper, we study the\ncomplexity of finding a choice reduction for the agent; that is, how to remove\nedges and vertices from the task graph such that a present-biased agent will\nremain motivated to reach his target even for a limited reward. While this\nproblem is NP-complete in general, this is not necessarily true for instances\nwhich occur in practice, or for solutions which are of interest to task\ndesigners. For instance, a task designer may desire to find the best task graph\nwhich is not too complicated.\n  We therefore investigate the problem of finding simple motivating subgraphs.\nThese are structures where the agent will modify his plan at most $k$ times\nalong the way. We quantify this simplicity in the time-inconsistency model as a\nstructural parameter: The number of branching vertices (vertices with\nout-degree at least $2$) in a minimal motivating subgraph.\n  Our results are as follows: We give a linear algorithm for finding an optimal\nmotivating path, i.e. when $k=0$. On the negative side, we show that finding a\nsimple motivating subgraph is NP-complete even if we allow only a single\nbranching vertex --- revealing that simple motivating subgraphs are indeed hard\nto find. However, we give a pseudo-polynomial algorithm for the case when $k$\nis fixed and edge weights are rationals, which might be a reasonable assumption\nin practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:57:43 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Str\u00f8mme", "Torstein J. F.", ""]]}, {"id": "1911.07782", "submitter": "Alex B. Grilo", "authors": "Anne Broadbent and Alex B. Grilo", "title": "QMA-hardness of Consistency of Local Density Matrices with Applications\n  to Quantum Zero-Knowledge", "comments": "Title changed to highlight the QMA-hardness proof of CLDM.\n  Improvement on the presentation of the paper (including self-contained proofs\n  of results needed from Grilo, Slofstra, and Yuen'19). The extended abstract\n  of this paper appears in the proceedings of FOCS'2020", "journal-ref": null, "doi": "10.1109/FOCS46700.2020.00027", "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide several advances to the understanding of the class of Quantum\nMerlin-Arthur proof systems (QMA), the quantum analogue of NP. Our central\ncontribution is proving a longstanding conjecture that the Consistency of Local\nDensity Matrices (CLDM) problem is QMA-hard under Karp reductions. The input of\nCLDM consists of local reduced density matrices on sets of at most k qubits,\nand the problem asks if there is an n-qubit global quantum state that is\nconsistent with all of the k-qubit local density matrices. The containment of\nthis problem in QMA and the QMA-hardness under Turing reductions were proved by\nLiu [APPROX-RANDOM 2006]. Liu also conjectured that CLDM is QMA-hard under Karp\nreductions, which is desirable for applications, and we finally prove this\nconjecture. We establish this result using the techniques of simulatable codes\nof Grilo, Slofstra, and Yuen [FOCS 2019], simplifying their proofs and\ntailoring them to the context of QMA.\n  In order to develop applications of CLDM, we propose a framework that we call\nlocally simulatable proofs for QMA: this provides QMA proofs that can be\nefficiently verified by probing only k qubits and, furthermore, the reduced\ndensity matrix of any k-qubit subsystem of an accepting witness can be computed\nin polynomial time, independently of the witness. Within this framework, we\nshow advances in quantum zero-knowledge. We show the first commit-and-open\ncomputational zero-knowledge proof system for all of QMA, as a quantum analogue\nof a \"sigma\" protocol. We then define a Proof of Quantum Knowledge, which\nguarantees that a prover is effectively in possession of a quantum witness in\nan interactive proof, and show that our zero-knowledge proof system satisfies\nthis definition. Finally, we show that our proof system can be used to\nestablish that QMA has a quantum non-interactive zero-knowledge proof system in\nthe secret parameter setting.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:30:11 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 15:43:10 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Broadbent", "Anne", ""], ["Grilo", "Alex B.", ""]]}, {"id": "1911.07793", "submitter": "Oliver Korten", "authors": "Oliver Korten", "title": "On the Complexity of 2-Player Packing Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational complexity of two 2-player games involving\npacking objects into a box. In the first game, players alternate drawing\npolycubes from a shared pile and placing them into an initially empty box in\nany available location; the first player who can't place another piece loses.\nIn the second game, there is a fixed sequence of polycubes, and on a player's\nturn they drop the next piece in through the top of the box, after which it\nfalls until it hits a previously placed piece (as in Tetris); the first player\nwho can't place the next piece loses. We prove that in both games, deciding the\noutcome under perfect play is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:48:47 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Korten", "Oliver", ""]]}, {"id": "1911.07981", "submitter": "J. M. Landsberg", "authors": "Austin Conner, Alicia Harper and J.M. Landsberg", "title": "New lower bounds for matrix multiplication and the 3x3 determinant", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M_{\\langle u,v,w\\rangle}\\in C^{uv}\\otimes C^{vw}\\otimes C^{wu}$ denote\nthe matrix multiplication tensor (and write $M_n=M_{\\langle n,n,n\\rangle}$) and\nlet $det_3\\in ( C^9)^{\\otimes 3}$ denote the determinant polynomial considered\nas a tensor. For a tensor $T$, let $\\underline R(T)$ denote its border rank. We\n(i) give the first hand-checkable algebraic proof that $\\underline\nR(M_2)=7$,(ii) prove $\\underline R(M_{\\langle 223\\rangle})=10$, and $\\underline\nR(M_{\\langle 233\\rangle})=14$, where previously the only nontrivial matrix\nmultiplication tensor whose border rank had been determined was $M_2$,(iii)\nprove $\\underline R( M_3)\\geq 17$, (iv) prove $\\underline R( det_3)=17$,\nimproving the previous lower bound of $12$, (v) prove $\\underline R(M_{\\langle\n2nn\\rangle})\\geq n^2+1.32n$ for all $n\\geq 25$ (previously only $\\underline\nR(M_{\\langle 2nn\\rangle})\\geq n^2+1$ was known) as well as lower bounds for\n$4\\leq n\\leq 25$, and (vi) prove $\\underline R(M_{\\langle 3nn\\rangle})\\geq\nn^2+2 n+1$ for all $ n\\geq 21$, where previously only $\\underline R(M_{\\langle\n3nn\\rangle})\\geq n^2+2$ was known, as well as lower boundsfor $4\\leq n\\leq 21$.\n  Our results utilize a new technique initiated by Buczy\\'{n}ska and\nBuczy\\'{n}ski, called border apolarity. The two key ingredients are: (i) the\nuse of a multi-graded ideal associated to a border rank $r$ decomposition of\nany tensor, and (ii) the exploitation of the large symmetry group of $T$ to\nrestrict to $B_T$-invariant ideals, where $B_T$ is a maximal solvable subgroup\nof the symmetry group of $T$.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:18:34 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Conner", "Austin", ""], ["Harper", "Alicia", ""], ["Landsberg", "J. M.", ""]]}, {"id": "1911.08043", "submitter": "Bas Lodewijks", "authors": "Bas Lodewijks", "title": "Mapping NP-hard and NP-complete optimisation problems to Quadratic\n  Unconstrained Binary Optimisation problems", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several mappings from well-known NP-hard problems to Quadratic\nUnconstrained Binary Optimisation problems which are treated incorrectly by\nLucas. We provide counterexamples and correct the mappings. We also extend the\nbody of QUBO formulations of NP-complete and NP-hard optimisation problems by\ndiscussing additional problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:14:44 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 22:56:51 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 15:35:20 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 08:22:43 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lodewijks", "Bas", ""]]}, {"id": "1911.08101", "submitter": "Shih-Han Hung", "authors": "Gorjan Alagic, Andrew M. Childs, Alex B. Grilo, Shih-Han Hung", "title": "Non-interactive classical verification of quantum computation", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough, Mahadev constructed an interactive protocol that\nenables a purely classical party to delegate any quantum computation to an\nuntrusted quantum prover. In this work, we show that this same task can in fact\nbe performed non-interactively and in zero-knowledge.\n  Our protocols result from a sequence of significant improvements to the\noriginal four-message protocol of Mahadev. We begin by making the first message\ninstance-independent and moving it to an offline setup phase. We then establish\na parallel repetition theorem for the resulting three-message protocol, with an\nasymptotically optimal rate. This, in turn, enables an application of the\nFiat-Shamir heuristic, eliminating the second message and giving a\nnon-interactive protocol. Finally, we employ classical non-interactive\nzero-knowledge (NIZK) arguments and classical fully homomorphic encryption\n(FHE) to give a zero-knowledge variant of this construction. This yields the\nfirst purely classical NIZK argument system for QMA, a quantum analogue of NP.\n  We establish the security of our protocols under standard assumptions in\nquantum-secure cryptography. Specifically, our protocols are secure in the\nQuantum Random Oracle Model, under the assumption that Learning with Errors is\nquantumly hard. The NIZK construction also requires circuit-private FHE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:13:25 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:03:45 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Alagic", "Gorjan", ""], ["Childs", "Andrew M.", ""], ["Grilo", "Alex B.", ""], ["Hung", "Shih-Han", ""]]}, {"id": "1911.08297", "submitter": "Igor Carboni Oliveira", "authors": "Lijie Chen, Shuichi Hirahara, Igor C. Oliveira, Jan Pich, Ninad\n  Rajgopal, Rahul Santhanam", "title": "Beyond Natural Proofs: Hardness Magnification and Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardness magnification reduces major complexity separations (such as\n$\\mathsf{\\mathsf{EXP}} \\nsubseteq \\mathsf{NC}^1$) to proving lower bounds for\nsome natural problem $Q$ against weak circuit models. Several recent works\n[OS18, MMW19, CT19, OPS19, CMMW19, Oli19, CJW19a] have established results of\nthis form. In the most intriguing cases, the required lower bound is known for\nproblems that appear to be significantly easier than $Q$, while $Q$ itself is\nsusceptible to lower bounds but these are not yet sufficient for magnification.\n  In this work, we provide more examples of this phenomenon, and investigate\nthe prospects of proving new lower bounds using this approach. In particular,\nwe consider the following essential questions associated with the hardness\nmagnification program:\n  Does hardness magnification avoid the natural proofs barrier of Razborov and\nRudich [RR97]?\n  Can we adapt known lower bound techniques to establish the desired lower\nbound for $Q$?\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:35:26 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Chen", "Lijie", ""], ["Hirahara", "Shuichi", ""], ["Oliveira", "Igor C.", ""], ["Pich", "Jan", ""], ["Rajgopal", "Ninad", ""], ["Santhanam", "Rahul", ""]]}, {"id": "1911.08558", "submitter": "Fatemeh Keshavarz-Kohjerdi", "authors": "Ruo-Wei Hung and Fatemeh Keshavarz-Kohjerdi", "title": "The Longest $(s, t)$-paths of $O$-shaped Supergrid Graphs", "comments": "21 pages, 27 figures. arXiv admin note: substantial text overlap with\n  arXiv:1908.07447, arXiv:1904.02581", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we continue the study of the Hamiltonian and longest $(s,\nt)$-paths of supergrid graphs. The Hamiltonian $(s, t)$-path of a graph is a\nHamiltonian path between any two given vertices $s$ and $t$ in the graph, and\nthe longest $(s, t)$-path is a simple path with the maximum number of vertices\nfrom $s$ to $t$ in the graph. A graph holds Hamiltonian connected property if\nit contains a Hamiltonian $(s, t)$-path. These two problems are well-known\nNP-complete for general supergrid graphs. An $O$-shaped supergrid graph is a\nspecial kind of a rectangular grid graph with a rectangular hole. In this\npaper, we first prove the Hamiltonian connectivity of $O$-shaped supergrid\ngraphs except few conditions. We then show that the longest $(s, t)$-path of an\n$O$-shaped supergrid graph can be computed in linear time. The Hamiltonian and\nlongest $(s, t)$-paths of $O$-shaped supergrid graphs can be applied to compute\nthe minimum trace of computerized embroidery machine and 3D printer when a\nhollow object is printed.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 16:04:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hung", "Ruo-Wei", ""], ["Keshavarz-Kohjerdi", "Fatemeh", ""]]}, {"id": "1911.08704", "submitter": "Nikos Mouzakis", "authors": "Dimitris Fotakis, Vardis Kandiros, Thanasis Lianeas, Nikos Mouzakis,\n  Panagiotis Patsilinakos, Stratis Skoulakis", "title": "Node Max-Cut and Computing Equilibria in Linear Weighted Congestion\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we seek a more refined understanding of the complexity of local\noptimum computation for Max-Cut and pure Nash equilibrium (PNE) computation for\ncongestion games with weighted players and linear latency functions. We show\nthat computing a PNE of linear weighted congestion games is PLS-complete either\nfor very restricted strategy spaces, namely when player strategies are paths on\na series-parallel network with a single origin and destination, or for very\nrestricted latency functions, namely when the latency on each resource is equal\nto the congestion. Our results reveal a remarkable gap regarding the complexity\nof PNE in congestion games with weighted and unweighted players, since in case\nof unweighted players, a PNE can be easily computed by either a simple greedy\nalgorithm (for series-parallel networks) or any better response dynamics (when\nthe latency is equal to the congestion). For the latter of the results above,\nwe need to show first that computing a local optimum of a natural restriction\nof Max-Cut, which we call \\emph{Node-Max-Cut}, is PLS-complete. In\nNode-Max-Cut, the input graph is vertex-weighted and the weight of each edge is\nequal to the product of the weights of its endpoints. Due to the very\nrestricted nature of Node-Max-Cut, the reduction requires a careful combination\nof new gadgets with ideas and techniques from previous work. We also show how\nto compute efficiently a $(1+\\eps)$-approximate equilibrium for Node-Max-Cut,\nif the number of different vertex weights is constant.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 04:43:24 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 02:21:37 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kandiros", "Vardis", ""], ["Lianeas", "Thanasis", ""], ["Mouzakis", "Nikos", ""], ["Patsilinakos", "Panagiotis", ""], ["Skoulakis", "Stratis", ""]]}, {"id": "1911.08964", "submitter": "Louis Dublois", "authors": "Louis Dublois, Michael Lampis, and Vangelis Th. Paschos", "title": "New Algorithms for Mixed Dominating Set", "comments": "This paper has been accepted to IPEC 2020", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 23 no.\n  1, Discrete Algorithms (April 30, 2021) dmtcs:7407", "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed dominating set is a collection of vertices and edges that dominates\nall vertices and edges of a graph. We study the complexity of exact and\nparameterized algorithms for \\textsc{Mixed Dominating Set}, resolving some open\nquestions. In particular, we settle the problem's complexity parameterized by\ntreewidth and pathwidth by giving an algorithm running in time $O^*(5^{tw})$\n(improving the current best $O^*(6^{tw})$), as well as a lower bound showing\nthat our algorithm cannot be improved under the Strong Exponential Time\nHypothesis (SETH), even if parameterized by pathwidth (improving a lower bound\nof $O^*((2 - \\varepsilon)^{pw})$). Furthermore, by using a simple but so far\noverlooked observation on the structure of minimal solutions, we obtain\nbranching algorithms which improve both the best known FPT algorithm for this\nproblem, from $O^*(4.172^k)$ to $O^*(3.510^k)$, and the best known\nexponential-time exact algorithm, from $O^*(2^n)$ and exponential space, to\n$O^*(1.912^n)$ and polynomial space.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:24:47 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 13:38:43 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 10:50:58 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 14:18:12 GMT"}, {"version": "v5", "created": "Mon, 26 Apr 2021 14:57:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dublois", "Louis", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1911.09065", "submitter": "Marcin Wrochna", "authors": "Alex Brandts, Marcin Wrochna, Stanislav \\v{Z}ivn\\'y", "title": "The complexity of promise SAT on non-Boolean domains", "comments": "Full version of an ICALP 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While 3-SAT is NP-hard, 2-SAT is solvable in polynomial time. Austrin,\nGuruswami, and H\\r{a}stad roved a result known as \"$(2+\\varepsilon)$-SAT is\nNP-hard\" [FOCS'14/SICOMP'17]. They showed that the problem of distinguishing\nk-CNF formulas that are g-satisfiable (i.e. some assignment satisfies at least\ng literals in every clause) from those that are not even 1-satisfiable is\nNP-hard if $\\frac{g}{k} < \\frac{1}{2}$ and is in P otherwise. We study a\ngeneralisation of SAT on arbitrary finite domains, with clauses that are\ndisjunctions of unary constraints, and establish analogous behaviour. Thus we\ngive a dichotomy for a natural fragment of promise constraint satisfaction\nproblems (PCSPs) on arbitrary finite domains.\n  The hardness side is proved using the algebraic approach, via a new general\nNP-hardness criterion on polymorphisms of the problem, based on a gap version\nof the Layered Label Cover problem. We show that previously used criteria are\ninsufficient -- the problem hence gives an interesting benchmark of algebraic\ntechniques for proving hardness of approximation problems such as PCSPs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:57:38 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 11:36:59 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 18:40:00 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Brandts", "Alex", ""], ["Wrochna", "Marcin", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1911.09133", "submitter": "Harro Wimmel", "authors": "Harro Wimmel", "title": "Synthesis of Reduced Asymmetric Choice Petri Nets", "comments": "27 pages, 10 figures, V2 due to font problem with ulsy.sty (one font\n  symbol had been erroneously replaced with a greek Psi by LiveTeX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Petri net is choice-free if any place has at most one transition in its\npostset (consuming its tokens) and it is (extended) free-choice (EFC) if the\npostsets of any two places are either equal or disjoint. Asymmetric choice (AC)\nextends EFC such that two places may also have postsets where one is contained\nin the other. In reduced AC nets this containment is limited: If the postsets\nare neither disjoint nor equal, one is a singleton and the other has exactly\ntwo transitions. The aim of Petri net synthesis is to find an unlabelled Petri\nnet in some target class with a reachability graph isomorphic to a given finite\nlabelled transition system (lts). Choice-free nets have strong properties,\nallowing to often easily detect when synthesis will fail or at least to quicken\nthe synthesis. With EFC as the target class, only few properties can be checked\nahead and there seem to be no short cuts lowering the complexity of the\nsynthesis (compared to arbitrary Petri nets). For AC nets no synthesis\nprocedure is known at all. We show here how synthesis to a superclass of\nreduced AC nets (not containing the full AC net class) can be done.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:07:10 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:28:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Wimmel", "Harro", ""]]}, {"id": "1911.09176", "submitter": "Luowen Qian", "authors": "Kai-Min Chung, Tai-Ning Liao, Luowen Qian", "title": "Lower Bounds for Function Inversion with Quantum Advice", "comments": "ITC full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Function inversion is the problem that given a random function $f: [M] \\to\n[N]$, we want to find pre-image of any image $f^{-1}(y)$ in time $T$. In this\nwork, we revisit this problem under the preprocessing model where we can\ncompute some auxiliary information or advice of size $S$ that only depends on\n$f$ but not on $y$. It is a well-studied problem in the classical settings,\nhowever, it is not clear how quantum algorithms can solve this task any better\nbesides invoking Grover's algorithm, which does not leverage the power of\npreprocessing.\n  Nayebi et al. proved a lower bound $ST^2 \\ge \\tilde\\Omega(N)$ for quantum\nalgorithms inverting permutations, however, they only consider algorithms with\nclassical advice. Hhan et al. subsequently extended this lower bound to fully\nquantum algorithms for inverting permutations. In this work, we give the same\nasymptotic lower bound to fully quantum algorithms for inverting functions for\nfully quantum algorithms under the regime where $M = O(N)$.\n  In order to prove these bounds, we generalize the notion of quantum random\naccess code, originally introduced by Ambainis et al., to the setting where we\nare given a list of (not necessarily independent) random variables, and we wish\nto compress them into a variable-length encoding such that we can retrieve a\nrandom element just using the encoding with high probability. As our main\ntechnical contribution, we give a nearly tight lower bound (for a wide\nparameter range) for this generalized notion of quantum random access codes,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:13:26 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 05:23:57 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chung", "Kai-Min", ""], ["Liao", "Tai-Ning", ""], ["Qian", "Luowen", ""]]}, {"id": "1911.09221", "submitter": "Hugo Rosado", "authors": "Lehilton Lelis Chaves Pedrosa, Hugo Kooki Kasuya Rosado", "title": "A 2-approximation for the $k$-prize-collecting Steiner tree problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-prize-collecting Steiner tree problem. An instance is\ncomposed of an integer $k$ and a graph $G$ with costs on edges and penalties on\nvertices. The objective is to find a tree spanning at least $k$ vertices which\nminimizes the cost of the edges in the tree plus the penalties of vertices not\nin the tree. This is one of the most fundamental network design problems and is\na common generalization of the prize-collecting Steiner tree and the\n$k$-minimum spanning tree problems. Our main result is a 2-approximation\nalgorithm, which improves on the currently best known approximation factor of\n3.96 and has a faster running time. The algorithm builds on a modification of\nthe primal-dual framework of Goemans and Williamson, and reveals interesting\nproperties that can be applied to other similar problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:52:03 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Pedrosa", "Lehilton Lelis Chaves", ""], ["Rosado", "Hugo Kooki Kasuya", ""]]}, {"id": "1911.09379", "submitter": "Klaus Heeger", "authors": "Robert Bredereck, Klaus Heeger, Du\\v{s}an Knop, Rolf Niedermeier", "title": "Parameterized Complexity of Stable Roommates with Ties and Incomplete\n  Lists Through the Lens of Graph Parameters", "comments": "An extended abstract of this paper appears at ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue and extend previous work on the parameterized complexity analysis\nof the NP-hard Stable Roommates with Ties and Incomplete Lists problem, thereby\nstrengthening earlier results both on the side of parameterized hardness as\nwell as on the side of fixed-parameter tractability. Other than for its famous\nsister problem Stable Marriage which focuses on a bipartite scenario, Stable\nRoommates with Incomplete Lists allows for arbitrary acceptability graphs whose\nedges specify the possible matchings of each two agents (agents are represented\nby graph vertices). Herein, incomplete lists and ties reflect the fact that in\nrealistic application scenarios the agents cannot bring all other agents into a\nlinear order. Among our main contributions is to show that it is W[1]-hard to\ncompute a maximum-cardinality stable matching for acceptability graphs of\nbounded treedepth, bounded tree-cut width, and bounded disjoint paths modulator\nnumber (these are each time the respective parameters). However, if we `only'\nask for perfect stable matchings or the mere existence of a stable matching,\nthen we obtain fixed-parameter tractability with respect to tree-cut width but\nnot with respect to treedepth. On the positive side, we also provide\nfixed-parameter tractability results for the parameter feedback edge set\nnumber.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:12:36 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 22:12:25 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bredereck", "Robert", ""], ["Heeger", "Klaus", ""], ["Knop", "Du\u0161an", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1911.09944", "submitter": "Andreas Lenz", "authors": "Andreas Lenz, Cyrus Rashtchian, Paul H. Siegel, Eitan Yaakobi", "title": "Covering Codes using Insertions or Deletions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A covering code is a set of codewords with the property that the union of\nballs, suitably defined, around these codewords covers an entire space.\nGenerally, the goal is to find the covering code with the minimum size\ncodebook. While most prior work on covering codes has focused on the Hamming\nmetric, we consider the problem of designing covering codes defined in terms of\neither insertions or deletions. First, we provide new sphere-covering lower\nbounds on the minimum possible size of such codes. Then, we provide new\nexistential upper bounds on the size of optimal covering codes for a single\ninsertion or a single deletion that are tight up to a constant factor. Finally,\nwe derive improved upper bounds for covering codes using $R\\geq 2$ insertions\nor deletions. We prove that codes exist with density that is only a factor $O(R\n\\log R)$ larger than the lower bounds for all fixed~$R$. In particular, our\nupper bounds have an optimal dependence on the word length, and we achieve\nasymptotic density matching the best known bounds for Hamming distance covering\ncodes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:44:38 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 11:00:41 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 08:33:44 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 10:00:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lenz", "Andreas", ""], ["Rashtchian", "Cyrus", ""], ["Siegel", "Paul H.", ""], ["Yaakobi", "Eitan", ""]]}, {"id": "1911.10337", "submitter": "Andrei Khrennikov Yu", "authors": "Andrei Khrennikov", "title": "Echoing the recent Google success: Foundational Roots of Quantum\n  Supremacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Google's claim on breakthrough in quantum computing is a gong\nsignal for further analysis of foundational roots of (possible) superiority of\nsome quantum algorithms over the corresponding classical algorithms. This note\nis a step in this direction. We start with critical analysis of rather common\nreference to entanglement and quantum nonlocality as the basic sources of\nquantum superiority. We elevate the role of the Bohr's {\\it principle of\ncomplementarity}\\footnote{} (PCOM) by interpreting the Bell-experiments as\nstatistical tests of this principle. (Our analysis also includes comparison of\nclassical vs genuine quantum entanglements.) After a brief presentation of PCOM\nand endowing it with the information interpretation, we analyze its\ncomputational counterpart. The main implication of PCOM is that by using the\nquantum representation of probability, one need not compute the joint\nprobability distribution (jpd) for observables involved in the process of\ncomputation. Jpd's calculation is exponentially time consuming. Consequently,\nclassical probabilistic algorithms involving calculation of jpd for $n$ random\nvariables can be over-performed by quantum algorithms (for big values of $n).$\nQuantum algorithms are based on quantum probability calculus. It is crucial\nthat the latter modifies the classical formula of total probability (FTP).\nProbability inference based on the quantum version of FTP leads to constructive\ninterference of probabilities increasing probabilities of some events. We also\nstress the role the basic feature of the genuine quantum superposition\ncomparing with the classical wave superposition: generation of discrete events\nin measurements on superposition states. Finally, the problem of superiority of\nquantum computations is coupled with the quantum measurement problem and\nlinearity of dynamics of the quantum state update.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:31:53 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Khrennikov", "Andrei", ""]]}, {"id": "1911.10363", "submitter": "Rudini Sampaio", "authors": "Thiago Marcilon, Nicolas Martins, Rudini Sampaio", "title": "Hardness of some variants of the graph coloring game", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, a long-standing open question proposed by Bodlaender in 1991\nwas answered: the graph coloring game is PSPACE-complete. In 2019, Andres and\nLock proposed five variants of the graph coloring game and left open the\nquestion of PSPACE-hardness related to them. In this paper, we prove that these\nvariants are PSPACE-complete for the graph coloring game and also for the\ngreedy coloring game, even if the number of colors is the chromatic number.\nFinally, we also prove that a connected version of the graph coloring game,\nproposed by Charpentier et al. in 2019, is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 13:16:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Marcilon", "Thiago", ""], ["Martins", "Nicolas", ""], ["Sampaio", "Rudini", ""]]}, {"id": "1911.10381", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Xi Chen, Chenghao Guo, Emmanouil-Vasileios Vlatakis-Gkaragkounis,\n  Mihalis Yannakakis and Xinzhi Zhang", "title": "Smoothed complexity of local Max-Cut and binary Max-CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the smoothed complexity of the FLIP algorithm for local Max-Cut\nis at most $\\smash{\\phi n^{O(\\sqrt{\\log n})}}$, where $n$ is the number of\nnodes in the graph and $\\phi$ is a parameter that measures the magnitude of\nperturbations applied on its edge weights. This improves the previously best\nupper bound of $\\phi n^{O(\\log n)}$ by Etscheid and R\\\"{o}glin. Our result is\nbased on an analysis of long sequences of flips, which shows~that~it is very\nunlikely for every flip in a long sequence to incur a positive but small\nimprovement in the cut weight. We also extend the same upper bound on the\nsmoothed complexity of FLIP to all binary Maximum Constraint Satisfaction\nProblems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 16:15:11 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Xi", ""], ["Guo", "Chenghao", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Yannakakis", "Mihalis", ""], ["Zhang", "Xinzhi", ""]]}, {"id": "1911.10698", "submitter": "Suprovat Ghoshal", "authors": "Arnab Bhattacharyya, L. Sunil Chandran and Suprovat Ghoshal", "title": "Combinatorial lower bounds for 3-query LDCs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A code is called a $q$-query locally decodable code (LDC) if there is a\nrandomized decoding algorithm that, given an index $i$ and a received word $w$\nclose to an encoding of a message $x$, outputs $x_i$ by querying only at most\n$q$ coordinates of $w$. Understanding the tradeoffs between the dimension,\nlength and query complexity of LDCs is a fascinating and unresolved research\nchallenge. In particular, for $3$-query binary LDCs of dimension $k$ and length\n$n$, the best known bounds are: $2^{k^{o(1)}} \\geq n \\geq \\tilde{\\Omega}(k^2)$.\n  In this work, we take a second look at binary $3$-query LDCs. We investigate\na class of 3-uniform hypergraphs that are equivalent to strong binary 3-query\nLDCs. We prove an upper bound on the number of edges in these hypergraphs,\nreproducing the known lower bound of $\\tilde{\\Omega}(k^2)$ for the length of\nstrong $3$-query LDCs. In contrast to previous work, our techniques are purely\ncombinatorial and do not rely on a direct reduction to $2$-query LDCs, opening\nup a potentially different approach to analyzing 3-query LDCs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 04:32:12 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Chandran", "L. Sunil", ""], ["Ghoshal", "Suprovat", ""]]}, {"id": "1911.10833", "submitter": "Xiaojin Zhang", "authors": "Xiaojin Zhang", "title": "Near-Optimal Algorithm for Distribution-Free Junta Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive algorithm with one-sided error for the problem of\njunta testing for Boolean function under the challenging distribution-free\nsetting, the query complexity of which is $\\tilde O(k)/\\epsilon$. This improves\nthe upper bound of $\\tilde O(k^2)/\\epsilon$ by \\cite{liu2019distribution}. From\nthe $\\Omega(k\\log k)$ lower bound for junta testing under the uniform\ndistribution by \\cite{sauglam2018near}, our algorithm is nearly optimal. In the\nstandard uniform distribution, the optimal junta testing algorithm is mainly\ndesigned by bridging between relevant variables and relevant blocks. At the\nheart of the analysis is the Efron-Stein orthogonal decomposition. However, it\nis not clear how to generalize this tool to the general setting. Surprisingly,\nwe find that junta could be tested in a very simple and efficient way even in\nthe distribution-free setting. It is interesting that the analysis does not\nrely on Fourier tools directly which are commonly used in junta testing.\nFurther, we present a simpler algorithm with the same query complexity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 11:19:01 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:48:33 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 01:24:14 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 13:38:52 GMT"}, {"version": "v5", "created": "Tue, 13 Jul 2021 12:05:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Xiaojin", ""]]}, {"id": "1911.11257", "submitter": "Daniel Wiebking", "authors": "Daniel Wiebking", "title": "Graph isomorphism in quasipolynomial time parameterized by treewidth", "comments": "52 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Babai's quasipolynomial-time graph isomorphism test (STOC 2016) and\ndevelop a quasipolynomial-time algorithm for the multiple-coset isomorphism\nproblem. The algorithm for the multiple-coset isomorphism problem allows to\nexploit graph decompositions of the given input graphs within Babai's\ngroup-theoretic framework.\n  We use it to develop a graph isomorphism test that runs in time\n$n^{\\operatorname{polylog}(k)}$ where $n$ is the number of vertices and $k$ is\nthe minimum treewidth of the given graphs and $\\operatorname{polylog}(k)$ is\nsome polynomial in $\\operatorname{log}(k)$. Our result generalizes Babai's\nquasipolynomial-time graph isomorphism test.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 22:15:37 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 13:57:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wiebking", "Daniel", ""]]}, {"id": "1911.11368", "submitter": "Sidhanth Mohanty", "authors": "Shafi Goldwasser, Ofer Grossman, Sidhanth Mohanty, David P. Woodruff", "title": "Pseudo-deterministic Streaming", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudo-deterministic algorithm is a (randomized) algorithm which, when run\nmultiple times on the same input, with high probability outputs the same result\non all executions. Classic streaming algorithms, such as those for finding\nheavy hitters, approximate counting, $\\ell_2$ approximation, finding a nonzero\nentry in a vector (for turnstile algorithms) are not pseudo-deterministic. For\nexample, in the instance of finding a nonzero entry in a vector, for any known\nlow-space algorithm $A$, there exists a stream $x$ so that running $A$ twice on\n$x$ (using different randomness) would with high probability result in two\ndifferent entries as the output.\n  In this work, we study whether it is inherent that these algorithms output\ndifferent values on different executions. That is, we ask whether these\nproblems have low-memory pseudo-deterministic algorithms. For instance, we show\nthat there is no low-memory pseudo-deterministic algorithm for finding a\nnonzero entry in a vector (given in a turnstile fashion), and also that there\nis no low-dimensional pseudo-deterministic sketching algorithm for $\\ell_2$\nnorm estimation. We also exhibit problems which do have low memory\npseudo-deterministic algorithms but no low memory deterministic algorithm, such\nas outputting a nonzero row of a matrix, or outputting a basis for the row-span\nof a matrix.\n  We also investigate multi-pseudo-deterministic algorithms: algorithms which\nwith high probability output one of a few options. We show the first lower\nbounds for such algorithms. This implies that there are streaming problems such\nthat every low space algorithm for the problem must have inputs where there are\nmany valid outputs, all with a significant probability of being outputted.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:50:28 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Grossman", "Ofer", ""], ["Mohanty", "Sidhanth", ""], ["Woodruff", "David P.", ""]]}, {"id": "1911.11405", "submitter": "Zhujun Zhang", "authors": "Zhujun Zhang", "title": "A Note on Computational Complexity of Kill-all Go", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kill-all Go is a variant of Go in which Black tries to capture all white\nstones, while White tries to survive. We consider computational complexity of\nKill-all Go with two rulesets, Chinese rules and Japanese rules. We prove that:\n(i) Kill-all Go with Chinese rules is PSPACE-hard, and (ii) Kill-all Go with\nJapanese rules is EXPTIME-complete.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:49:54 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Zhang", "Zhujun", ""]]}, {"id": "1911.11793", "submitter": "Prerona Chatterjee", "authors": "Prerona Chatterjee, Mrinal Kumar, Adrian She, Ben Lee Volk", "title": "A Quadratic Lower Bound for Algebraic Branching Programs and Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any Algebraic Branching Program (ABP) computing the polynomial\n$\\sum_{i = 1}^n x_i^n$ has at least $\\Omega(n^2)$ vertices. This improves upon\nthe lower bound of $\\Omega(n\\log n)$, which follows from the classical result\nof Baur and Strassen [Str73, BS83], and extends the results in [K19], which\nshowed a quadratic lower bound for \\emph{homogeneous} ABPs computing the same\npolynomial.\n  Our proof relies on a notion of depth reduction which is reminiscent of\nsimilar statements in the context of matrix rigidity, and shows that any small\nenough ABP computing the polynomial $\\sum_{i=1}^n x_i^n$ can be depth reduced\nto essentially a homogeneous ABP of the same size which computes the polynomial\n$\\sum_{i = 1}^n x_i^n + \\epsilon(x_1, \\ldots, x_n)$, for a structured \"error\npolynomial\" $\\epsilon(x_1, \\ldots, x_n)$. To complete the proof, we then\nobserve that the lower bound in [K19] is robust enough and continues to hold\nfor all polynomials $\\sum_{i = 1}^n x_i^n + \\epsilon(x_1, \\ldots, x_n)$, where\n$\\epsilon(x_1, \\ldots, x_n)$ has the appropriate structure.\n  We also use our ideas to show an $\\Omega(n^2)$ lower bound of the size of\nalgebraic formulas computing the elementary symmetric polynomial of degree\n$0.1n$ on $n$ variables. This is a slight improvement upon the prior best known\nformula lower bound (proved for a different polynomial) of $\\Omega(n^2/\\log n)$\n[Nec66, K85, SY10]. Interestingly, this lower bound is asymptotically better\nthan $n^2/\\log n$, the strongest lower bound that can be proved using previous\nmethods. This lower bound also matches the upper bound, due to Ben-Or, who\nshowed that elementary symmetric polynomials can be computed by algebraic\nformula (in fact depth-$3$ formula) of size $O(n^2)$. Prior to this work,\nBen-Or's construction was known to be optimal only for algebraic formulas of\ndepth-$3$ [SW01].\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:06:47 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 16:17:52 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chatterjee", "Prerona", ""], ["Kumar", "Mrinal", ""], ["She", "Adrian", ""], ["Volk", "Ben Lee", ""]]}, {"id": "1911.11852", "submitter": "Chenye Wu", "authors": "Mingkuan Xu, Yang Yu and Chenye Wu", "title": "Rule Designs for Optimal Online Game Matchmaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online games are the most popular form of entertainment among youngsters as\nwell as elders. Recognized as e-Sports, they may become an official part of the\nOlympic Games by 2020. However, a long waiting time for matchmaking will\nlargely affect players' experiences. We examine different matchmaking\nmechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic\nframework, we decompose the rule design process into a sequence of decision\nmaking problems, and derive the optimal mechanism with minimum expected waiting\ntime. We further the result by exploring additional static as well as dynamic\nrule designs' impacts. In the static setting, we consider the game allows\nplayers to choose sides before the battle. In the dynamic setting, we consider\nthe game offers multiple zones for players of different skill levels. In both\nsettings, we examine the value of choice-free players. Closed form expressions\nfor the expected waiting time in different settings illuminate the guidelines\nfor online game rule designs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 13:17:12 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Xu", "Mingkuan", ""], ["Yu", "Yang", ""], ["Wu", "Chenye", ""]]}, {"id": "1911.12242", "submitter": "Roman Schutski", "authors": "Roman Schutski and Danil Lykov and Ivan Oseledets", "title": "An adaptive algorithm for quantum circuit simulation", "comments": "10 pages, 11 figures", "journal-ref": "Phys. Rev. A 101, 042335 (2020)", "doi": "10.1103/PhysRevA.101.042335", "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient simulation of quantum computers is essential for the development\nand validation of near-term quantum devices and the research on quantum\nalgorithms. Up to date, two main approaches to simulation were in use, based on\neither full state or single amplitude evaluation. We propose an algorithm that\nefficiently interpolates between these two possibilities. Our approach\nelucidates the connection between quantum circuit simulation and partial\nevaluation of expressions in tensor algebra.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 16:00:23 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 12:37:56 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Schutski", "Roman", ""], ["Lykov", "Danil", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1911.12520", "submitter": "Mrinal Kumar", "authors": "Prasad Chaugule, Mrinal Kumar, Nutan Limaye, Chandra Kanta Mohapatra,\n  Adrian She, Srikanth Srinivasan", "title": "Schur Polynomials do not have small formulas if the Determinant doesn't!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schur Polynomials are families of symmetric polynomials that have been\nclassically studied in Combinatorics and Algebra alike. They play a central\nrole in the study of Symmetric functions, in Representation theory [Sta99], in\nSchubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,\nSta99]. In recent years, they have also shown up in various incarnations in\nComputer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric\ncomplexity theory [IP17].\n  However, unlike some other families of symmetric polynomials like the\nElementary Symmetric polynomials, the Power Symmetric polynomials and the\nComplete Homogeneous Symmetric polynomials, the computational complexity of\nsyntactically computing Schur polynomials has not been studied much. In\nparticular, it is not known whether Schur polynomials can be computed\nefficiently by algebraic formulas. In this work, we address this question, and\nshow that unless \\emph{every} polynomial with a small algebraic branching\nprogram (ABP) has a small algebraic formula, there are Schur polynomials that\ncannot be computed by algebraic formula of polynomial size. In other words,\nunless the algebraic complexity class $\\mathrm{VBP}$ is equal to the complexity\nclass $\\mathrm{VF}$, there exist Schur polynomials which do not have polynomial\nsize algebraic formulas.\n  As a consequence of our proof, we also show that computing the determinant of\ncertain \\emph{generalized} Vandermonde matrices is essentially as hard as\ncomputing the general symbolic determinant. To the best of our knowledge, these\nare one of the first hardness results of this kind for families of polynomials\nwhich are not \\emph{multilinear}. A key ingredient of our proof is the study of\ncomposition of \\emph{well behaved} algebraically independent polynomials with a\nhomogeneous polynomial, and might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 04:26:29 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chaugule", "Prasad", ""], ["Kumar", "Mrinal", ""], ["Limaye", "Nutan", ""], ["Mohapatra", "Chandra Kanta", ""], ["She", "Adrian", ""], ["Srinivasan", "Srikanth", ""]]}, {"id": "1911.12638", "submitter": "J\\=anis Iraids", "authors": "Andris Ambainis, Kaspars Balodis, J\\=anis Iraids, Kri\\v{s}j\\=anis\n  Pr\\=usis, Juris Smotrovs", "title": "Quantum Lower Bounds for 2D-Grid and Dyck Language", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show quantum lower bounds for two problems. First, we consider the problem\nof determining if a sequence of parentheses is a properly balanced one (a Dyck\nword), with a depth of at most $k$. It has been known that, for any $k$,\n$\\tilde{O}(\\sqrt{n})$ queries suffice, with a $\\tilde{O}$ term depending on\n$k$. We prove a lower bound of $\\Omega(c^k \\sqrt{n})$, showing that the\ncomplexity of this problem increases exponentially in $k$. This is interesting\nas a representative example of star-free languages for which a surprising\n$\\tilde{O}(\\sqrt{n})$ query quantum algorithm was recently constructed by\nAaronson et al.\n  Second, we consider connectivity problems on directed/undirected grid in 2\ndimensions, if some of the edges of the grid may be missing. By embedding the\n\"balanced parentheses\" problem into the grid, we show a lower bound of\n$\\Omega(n^{1.5-\\epsilon})$ for the directed 2D grid and\n$\\Omega(n^{2-\\epsilon})$ for the undirected 2D grid. The directed problem is\ninteresting as a black-box model for a class of classical dynamic programming\nstrategies including the one that is usually used for the well-known edit\ndistance problem. We also show a generalization of this result to more than 2\ndimensions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 10:54:32 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ambainis", "Andris", ""], ["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""], ["Pr\u016bsis", "Kri\u0161j\u0101nis", ""], ["Smotrovs", "Juris", ""]]}, {"id": "1911.12884", "submitter": "Graham Campbell", "authors": "Graham Campbell and Detlef Plump", "title": "Efficient Recognition of Graph Languages", "comments": "Project Report, Department of Computer Science, University of York,\n  83 pages, 2019. arXiv admin note: substantial text overlap with\n  arXiv:1906.05170", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph transformation is the rule-based modification of graphs, and is a\ndiscipline dating back to the 1970s. In general, to match the left-hand graph\nof a fixed rule within a host graph requires polynomial time, but to improve\nmatching performance, D\\\"orr proposed to equip rules and host graphs with\ndistinguished root nodes. This model was implemented by Plump and Bak, but\nunfortunately, such rules are not invertible. We address this problem by\ndefining rootedness using a partial function into a two-point set rather than\npointing graphs with root nodes, meaning derivations are natural double\npushouts. Moreover, we give a sufficient condition on rules to give constant\ntime rule application on graphs of bounded degree, and that, the graph class of\ntrees can be recognised in linear time, given an input graph of bounded degree.\nFinally, we define a new notion of confluence up to garbage and non-garbage\ncritical pairs, showing it is sufficient to require strong joinability of only\nthe non-garbage critical pairs to establish confluence up to garbage. Finally,\nthis new result, presented for conventional graph transformation systems, can\nbe lifted to our rooted setting by encoding node labels and rootedness as\nlooped edges.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:32:41 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 20:42:37 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 12:34:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Campbell", "Graham", ""], ["Plump", "Detlef", ""]]}, {"id": "1911.13091", "submitter": "Yufan Zheng", "authors": "Xiaoming Sun, Yufan Zheng", "title": "Hybrid Decision Trees: Longer Quantum Time is Strictly More Powerful", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the hybrid query complexity, denoted as\n$\\mathrm{Q}(f;q)$, which is the minimal query number needed to compute $f$,\nwhen a classical decision tree is allowed to call $q'$-query quantum\nsubroutines for any $q'\\leq q$. We present the following results:\n  $\\bullet$ There exists a total Boolean function $f$ such that\n$\\mathrm{Q}(f;1) = \\widetilde{\\mathcal{O}}(\\mathrm{R}(f)^{4/5})$.\n  $\\bullet$ $\\mathrm{Q}(f;q) = \\Omega(\\mathrm{bs}(f)/q +\n\\sqrt{\\mathrm{bs}(f)})$ for any Boolean function $f$; the lower bound is tight\nwhen $f$ is the ${\\rm O{\\small R}}$ function.\n  $\\bullet$ $\\mathrm{Q}(g \\circ {\\rm X{\\small OR}}_{C \\log n};1) =\n\\widetilde{\\Omega}(\\sqrt{n})$ for some sufficiently large constant $C$, where\n$g := {\\rm B{\\small OOL}S{\\small IMON}}_n$ is a variant of Simon's problem.\nNote that $\\mathrm{Q}(g\\circ {\\rm X{\\small OR}}_{C \\log n}) =\n\\mathcal{O}(\\mathrm{polylog}\\; n)$. Therefore an exponential separation is\nestablished. Furthermore, this open the road to prove the conjecture $\\forall\nk,\\,\\mathrm{Q}(g \\circ {\\rm X{\\small OR}}_{C \\log^{k+1} n};\\log^{k} n) =\n\\widetilde{\\Omega}(\\sqrt{n})$, which would imply the oracle separation\n$\\mathsf{HP}(\\mathsf{QSIZE}(n^\\alpha))^\\mathfrak{O} \\subsetneq\n\\mathsf{BQP}^\\mathfrak{O}$ for any $\\alpha$, where\n$\\mathsf{HP}(\\mathsf{QSIZE}(n^\\alpha))$ is a complexity class that contains\n$\\mathsf{BQTIME}(n^\\alpha)^{\\mathsf{BPP}}$ and\n$\\mathsf{BPP}^{\\mathsf{BQTIME}(n^\\alpha)}$ in any relativized world.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:05:16 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sun", "Xiaoming", ""], ["Zheng", "Yufan", ""]]}, {"id": "1911.13097", "submitter": "Johan Kwisthout", "authors": "Abdullahi Ali and Johan Kwisthout", "title": "A spiking neural algorithm for the Network Flow problem", "comments": "Supported by Intel Corporation in the Intel Neuromorphic Research\n  Community", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is currently not clear what the potential is of neuromorphic hardware\nbeyond machine learning and neuroscience. In this project, a problem is\ninvestigated that is inherently difficult to fully implement in neuromorphic\nhardware by introducing a new machine model in which a conventional Turing\nmachine and neuromorphic oracle work together to solve such types of problems.\nWe show that the P-complete Max Network Flow problem is intractable in models\nwhere the oracle may be consulted only once (`create-and-run' model) but\nbecomes tractable using an interactive (`neuromorphic co-processor') model of\ncomputation. More in specific we show that a logspace-constrained Turing\nmachine with access to an interactive neuromorphic oracle with linear space,\ntime, and energy constraints can solve Max Network Flow. A modified variant of\nthis algorithm is implemented on the Intel Loihi chip; a neuromorphic manycore\nprocessor developed by Intel Labs. We show that by off-loading the search for\naugmenting paths to the neuromorphic processor we can get energy efficiency\ngains, while not sacrificing runtime resources. This result demonstrates how\nP-complete problems can be mapped on neuromorphic architectures in a\ntheoretically and potentially practically efficient manner.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:19:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ali", "Abdullahi", ""], ["Kwisthout", "Johan", ""]]}, {"id": "1911.13104", "submitter": "Flavio Ferrarotti", "authors": "Flavio Ferrarotti, Sen\\'en Gonz\\'alez, Klaus-Dieter Schewe, Jos\\'e\n  Mar\\'ia Turull-Torres", "title": "Proper Hierarchies in Polylogarithmic Time and Absence of Complete\n  Problems", "comments": "Paper submitted to FoIKS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polylogarithmic time hierarchy structures sub-linear time complexity. In\nrecent work it was shown that all classes $\\tilde{\\Sigma}_{m}^{\\mathit{plog}}$\nor $\\tilde{\\Pi}_{m}^{\\mathit{plog}}$ ($m \\in \\mathbb{N}$) in this hierarchy can\nbe captured by semantically restricted fragments of second-order logic. In this\npaper the descriptive complexity theory of polylogarithmic time is taken\nfurther showing that there are strict hierarchies inside each of the classes of\nthe hierarchy. A straightforward consequence of this result is that there are\nno complete problems for these complexity classes, not even under polynomial\ntime reductions. As another consequence we show that the polylogarithmic time\nhierarchy itself is strict.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:30:16 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ferrarotti", "Flavio", ""], ["Gonz\u00e1lez", "Sen\u00e9n", ""], ["Schewe", "Klaus-Dieter", ""], ["Turull-Torres", "Jos\u00e9 Mar\u00eda", ""]]}, {"id": "1911.13161", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, Andreas Emil Feldmann, MohammadTaghi Hajiaghayi,\n  D\\'aniel Marx", "title": "Tight Bounds for Planar Strongly Connected Steiner Subgraph with Fixed\n  Number of Terminals (and Extensions)", "comments": "To appear in SICOMP. Extended abstract in SODA 2014. This version has\n  a new author (Andreas Emil Feldmann), and the algorithm is faster and\n  considerably simplified as compared to conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (see paper for full abstract)\n  Given a vertex-weighted directed graph $G=(V,E)$ and a set $T=\\{t_1, t_2,\n\\ldots t_k\\}$ of $k$ terminals, the objective of the SCSS problem is to find a\nvertex set $H\\subseteq V$ of minimum weight such that $G[H]$ contains a\n$t_{i}\\rightarrow t_j$ path for each $i\\neq j$. The problem is NP-hard, but\nFeldman and Ruhl [FOCS '99; SICOMP '06] gave a novel $n^{O(k)}$ algorithm for\nthe SCSS problem, where $n$ is the number of vertices in the graph and $k$ is\nthe number of terminals. We explore how much easier the problem becomes on\nplanar directed graphs:\n  - Our main algorithmic result is a $2^{O(k)}\\cdot n^{O(\\sqrt{k})}$ algorithm\nfor planar SCSS, which is an improvement of a factor of $O(\\sqrt{k})$ in the\nexponent over the algorithm of Feldman and Ruhl.\n  - Our main hardness result is a matching lower bound for our algorithm: we\nshow that planar SCSS does not have an $f(k)\\cdot n^{o(\\sqrt{k})}$ algorithm\nfor any computable function $f$, unless the Exponential Time Hypothesis (ETH)\nfails.\n  The following additional results put our upper and lower bounds in context:\n  - In general graphs, we cannot hope for such a dramatic improvement over the\n$n^{O(k)}$ algorithm of Feldman and Ruhl: assuming ETH, SCSS in general graphs\ndoes not have an $f(k)\\cdot n^{o(k/\\log k)}$ algorithm for any computable\nfunction $f$.\n  - Feldman and Ruhl generalized their $n^{O(k)}$ algorithm to the more general\nDirected Steiner Network (DSN) problem; here the task is to find a subgraph of\nminimum weight such that for every source $s_i$ there is a path to the\ncorresponding terminal $t_i$. We show that, assuming ETH, there is no\n$f(k)\\cdot n^{o(k)}$ time algorithm for DSN on acyclic planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:49:44 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Feldmann", "Andreas Emil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1911.13294", "submitter": "Jukka Suomela", "authors": "Alkida Balliu, Sebastian Brandt, Yuval Efron, Juho Hirvonen, Yannic\n  Maus, Dennis Olivetti, Jukka Suomela", "title": "Classification of distributed binary labeling problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete classification of the deterministic distributed time\ncomplexity for a family of graph problems: binary labeling problems in trees.\nThese are locally checkable problems that can be encoded with an alphabet of\nsize two in the edge labeling formalism. Examples of binary labeling problems\ninclude sinkless orientation, sinkless and sourceless orientation, 2-vertex\ncoloring, perfect matching, and the task of coloring edges red and blue such\nthat all nodes are incident to at least one red and at least one blue edge.\nMore generally, we can encode e.g. any cardinality constraints on indegrees and\noutdegrees.\n  We study the deterministic time complexity of solving a given binary labeling\nproblem in trees, in the usual LOCAL model of distributed computing. We show\nthat the complexity of any such problem is in one of the following classes:\n$O(1)$, $\\Theta(\\log n)$, $\\Theta(n)$, or unsolvable. In particular, a problem\nthat can be represented in the binary labeling formalism cannot have time\ncomplexity $\\Theta(\\log^* n)$, and hence we know that e.g. any encoding of\nmaximal matchings has to use at least three labels (which is tight).\n  Furthermore, given the description of any binary labeling problem, we can\neasily determine in which of the four classes it is and what is an\nasymptotically optimal algorithm for solving it. Hence the distributed time\ncomplexity of binary labeling problems is decidable, not only in principle, but\nalso in practice: there is a simple and efficient algorithm that takes the\ndescription of a binary labeling problem and outputs its distributed time\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:51:35 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:11:17 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 12:32:21 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Efron", "Yuval", ""], ["Hirvonen", "Juho", ""], ["Maus", "Yannic", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}]