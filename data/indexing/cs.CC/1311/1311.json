[{"id": "1311.0293", "submitter": "David Liu", "authors": "David Liu", "title": "Pebbling Arguments for Tree Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree Evaluation Problem was introduced by Cook et al. in 2010 as a\ncandidate for separating P from L and NL. The most general space lower bounds\nknown for the Tree Evaluation Problem require a semantic restriction on the\nbranching programs and use a connection to well-known pebble games to generate\na bottleneck argument. These bounds are met by corresponding upper bounds\ngenerated by natural implementations of optimal pebbling algorithms. In this\npaper we extend these ideas to a variety of restricted families of both\ndeterministic and non-deterministic branching programs, proving tight lower\nbounds under these restricted models. We also survey and unify known lower\nbounds in our \"pebbling argument\" framework.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 20:13:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Liu", "David", ""]]}, {"id": "1311.0366", "submitter": "Ishay Haviv", "authors": "Ishay Haviv and Oded Regev", "title": "On the Lattice Isomorphism Problem", "comments": "23 pages, SODA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Lattice Isomorphism Problem (LIP), in which given two lattices\nL_1 and L_2 the goal is to decide whether there exists an orthogonal linear\ntransformation mapping L_1 to L_2. Our main result is an algorithm for this\nproblem running in time n^{O(n)} times a polynomial in the input size, where n\nis the rank of the input lattices. A crucial component is a new generalized\nisolation lemma, which can isolate n linearly independent vectors in a given\nsubset of Z^n and might be useful elsewhere. We also prove that LIP lies in the\ncomplexity class SZK.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 10:38:44 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Haviv", "Ishay", ""], ["Regev", "Oded", ""]]}, {"id": "1311.0849", "submitter": "Abuzer Yakaryilmaz", "authors": "Marzio De Biasi and Abuzer Yakaryilmaz", "title": "Unary languages recognized by two-way one-counter automata", "comments": "14 pages. An improved version accepted to CIAA2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-way deterministic finite state automaton with one counter (2D1CA) is a\nfundamental computational model that has been examined in many different\naspects since sixties, but we know little about its power in the case of unary\nlanguages. Up to our knowledge, the only known unary nonregular languages\nrecognized by 2D1CAs are those formed by strings having exponential length,\nwhere the exponents form some trivial unary regular language. In this paper, we\npresent some non-trivial subsets of these languages. By using the input head as\na second counter, we present simulations of two-way deterministic finite\nautomata with linearly bounded counters and linear--space Turing machines. We\nalso show how a fixed-size quantum register can help to simplify some of these\nlanguages. Finally, we compare unary 2D1CAs with two--counter machines and\nprovide some insights about the limits of their computational power.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:53:30 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 19:45:20 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["De Biasi", "Marzio", ""], ["Yakaryilmaz", "Abuzer", ""]]}, {"id": "1311.1029", "submitter": "Pascal Michel", "authors": "Pascal Michel", "title": "Problems in number theory from busy beaver competition", "comments": "35 pages", "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  14, 2015) lmcs:1611", "doi": "10.2168/LMCS-11(4:10)2015", "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By introducing the busy beaver competition of Turing machines, in 1962, Rado\ndefined noncomputable functions on positive integers. The study of these\nfunctions and variants leads to many mathematical challenges. This article\ntakes up the following one: How can a small Turing machine manage to produce\nvery big numbers? It provides the following answer: mostly by simulating\nCollatz-like functions, that are generalizations of the famous 3x+1 function.\nThese functions, like the 3x+1 function, lead to new unsolved problems in\nnumber theory.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 12:22:32 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 07:00:18 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 17:58:24 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Michel", "Pascal", ""]]}, {"id": "1311.1098", "submitter": "Niao He", "authors": "Niao He, Anatoli Juditsky, Arkadi Nemirovski", "title": "Mirror Prox Algorithm for Multi-Term Composite Minimization and\n  Semi-Separable Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we develop a composite version of Mirror Prox algorithm for\nsolving convex-concave saddle point problems and monotone variational\ninequalities of special structure, allowing to cover saddle point/variational\nanalogies of what is usually called \"composite minimization\" (minimizing a sum\nof an easy-to-handle nonsmooth and a general-type smooth convex functions \"as\nif\" there were no nonsmooth component at all). We demonstrate that the\ncomposite Mirror Prox inherits the favourable (and unimprovable already in the\nlarge-scale bilinear saddle point case) $O(1/\\epsilon)$ efficiency estimate of\nits prototype. We demonstrate that the proposed approach can be naturally\napplied to Lasso-type problems with several penalizing terms (e.g. acting\ntogether $\\ell_1$ and nuclear norm regularization) and to problems of the\nstructure considered in the alternating directions methods, implying in both\ncases methods with the $O(\\epsilon^{-1})$ complexity bounds.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 15:54:35 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 18:40:56 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 21:18:57 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["He", "Niao", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1311.1616", "submitter": "Mark Bun", "authors": "Mark Bun and Justin Thaler", "title": "Hardness Amplification and the Approximate Degree of Constant-Depth\n  Circuits", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a generic form of hardness amplification for the approximability\nof constant-depth Boolean circuits by polynomials. Specifically, we show that\nif a Boolean circuit cannot be pointwise approximated by low-degree polynomials\nto within constant error in a certain one-sided sense, then an OR of disjoint\ncopies of that circuit cannot be pointwise approximated even with very high\nerror. As our main application, we show that for every sequence of degrees\n$d(n)$, there is an explicit depth-three circuit $F: \\{-1,1\\}^n \\to \\{-1,1\\}$\nof polynomial-size such that any degree-$d$ polynomial cannot pointwise\napproximate $F$ to error better than\n$1-\\exp\\left(-\\tilde{\\Omega}(nd^{-3/2})\\right)$. As a consequence of our main\nresult, we obtain an $\\exp\\left(-\\tilde{\\Omega}(n^{2/5})\\right)$ upper bound on\nthe the discrepancy of a function in AC$^0$, and an\n$\\exp\\left(\\tilde{\\Omega}(n^{2/5})\\right)$ lower bound on the threshold weight\nof AC$^0$, improving over the previous best results of\n$\\exp\\left(-\\Omega(n^{1/3})\\right)$ and $\\exp\\left(\\Omega(n^{1/3})\\right)$\nrespectively.\n  Our techniques also yield a new lower bound of\n$\\Omega\\left(n^{1/2}/\\log^{(d-2)/2}(n)\\right)$ on the approximate degree of the\nAND-OR tree of depth $d$, which is tight up to polylogarithmic factors for any\nconstant $d$, as well as new bounds for read-once DNF formulas. In turn, these\nresults imply new lower bounds on the communication and circuit complexity of\nthese classes, and demonstrate strong limitations on existing PAC learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 09:26:48 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 15:57:30 GMT"}, {"version": "v3", "created": "Wed, 25 Dec 2013 06:19:54 GMT"}, {"version": "v4", "created": "Mon, 28 Apr 2014 18:55:34 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Bun", "Mark", ""], ["Thaler", "Justin", ""]]}, {"id": "1311.1666", "submitter": "Alexander Yu. Vlasov", "authors": "Alexander Yu. Vlasov", "title": "Quantum Circuits and Spin(3n) Groups", "comments": "v1. REVTeX 4-1, 2 columns, 10 pages, no figures, v3. extended,\n  LaTeX2e, 1 col., 23+2 pages, v4. typos, accepted for publication", "journal-ref": "Q. Inf. Comp., vol. 15, no.3/4, pp. 235-259, 2015", "doi": null, "report-no": null, "categories": "quant-ph cs.CC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All quantum gates with one and two qubits may be described by elements of\n$Spin$ groups due to isomorphisms $Spin(3) \\simeq SU(2)$ and $Spin(6) \\simeq\nSU(4)$. However, the group of $n$-qubit gates $SU(2^n)$ for $n > 2$ has bigger\ndimension than $Spin(3n)$. A quantum circuit with one- and two-qubit gates may\nbe used for construction of arbitrary unitary transformation $SU(2^n)$.\nAnalogously, the `$Spin(3n)$ circuits' are introduced in this work as products\nof elements associated with one- and two-qubit gates with respect to the\nabove-mentioned isomorphisms.\n  The matrix tensor product implementation of the $Spin(3n)$ group together\nwith relevant models by usual quantum circuits with $2n$ qubits are\ninvestigated in such a framework. A certain resemblance with well-known sets of\nnon-universal quantum gates e.g., matchgates, noninteracting-fermion quantum\ncircuits) related with $Spin(2n)$ may be found in presented approach. Finally,\na possibility of the classical simulation of such circuits in polynomial time\nis discussed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 13:14:47 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:51:41 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2014 12:05:16 GMT"}, {"version": "v4", "created": "Fri, 19 Sep 2014 11:18:24 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Vlasov", "Alexander Yu.", ""]]}, {"id": "1311.2092", "submitter": "Bruno Bauwens", "authors": "Bruno Bauwens", "title": "Relating and contrasting plain and prefix Kolmogorov complexity", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [3] a short proof is given that some strings have maximal plain Kolmogorov\ncomplexity but not maximal prefix-free complexity. The proof uses Levin's\nsymmetry of information, Levin's formula relating plain and prefix complexity\nand Gacs' theorem that complexity of complexity given the string can be high.\nWe argue that the proof technique and results mentioned above are useful to\nsimplify existing proofs and to solve open questions.\n  We present a short proof of Solovay's result [21] relating plain and prefix\ncomplexity: $K (x) = C (x) + CC (x) + O(CCC (x))$ and $C (x) = K (x) - KK (x) +\nO(KKK (x))$, (here $CC(x)$ denotes $C(C(x))$, etc.).\n  We show that there exist $\\omega$ such that $\\liminf C(\\omega_1\\dots\n\\omega_n) - C(n)$ is infinite and $\\liminf K(\\omega_1\\dots \\omega_n) - K(n)$ is\nfinite, i.e. the infinitely often C-trivial reals are not the same as the\ninfinitely often K-trivial reals (i.e. [1,Question 1]).\n  Solovay showed that for infinitely many $x$ we have $|x| - C (x) \\le O(1)$\nand $|x| + K (|x|) - K (x) \\ge \\log^{(2)} |x| - O(\\log^{(3)} |x|)$, (here $|x|$\ndenotes the length of $x$ and $\\log^{(2)} = \\log\\log$, etc.). We show that this\nresult holds for prefixes of some 2-random sequences.\n  Finally, we generalize our proof technique and show that no monotone relation\nexists between expectation and probability bounded randomness deficiency (i.e.\n[6, Question 1]).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 21:55:06 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 14:18:04 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Bauwens", "Bruno", ""]]}, {"id": "1311.2128", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii and Tomoyuki Morimae", "title": "Quantum Commuting Circuits and Complexity of Ising Partition Functions", "comments": "36 pages, 5 figures", "journal-ref": "New J. Phys. 19 033003 (2017)", "doi": "10.1088/1367-2630/aa5fdb", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instantaneous quantum polynomial-time (IQP) computation is a class of quantum\ncomputation consisting only of commuting two-qubit gates and is not universal\nin the sense of standard quantum computation. Nevertheless, it has been shown\nthat if there is a classical algorithm that can simulate IQP efficiently, the\npolynomial hierarchy (PH) collapses at the third level, which is highly\nimplausible. However, the origin of the classical intractability is still less\nunderstood. Here we establish a relationship between IQP and computational\ncomplexity of the partition functions of Ising models. We apply the established\nrelationship in two opposite directions. One direction is to find subclasses of\nIQP that are classically efficiently simulatable in the strong sense, by using\nexact solvability of certain types of Ising models. Another direction is\napplying quantum computational complexity of IQP to investigate (im)possibility\nof efficient classical approximations of Ising models with imaginary coupling\nconstants. Specifically, we show that there is no fully polynomial randomized\napproximation scheme (FPRAS) for Ising models with almost all imaginary\ncoupling constants even on a planar graph of a bounded degree, unless the PH\ncollapses at the third level. Furthermore, we also show a multiplicative\napproximation of such a class of Ising partition functions is at least as hard\nas a multiplicative approximation for the output distribution of an arbitrary\nquantum circuit.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 02:30:29 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 15:55:27 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Fujii", "Keisuke", ""], ["Morimae", "Tomoyuki", ""]]}, {"id": "1311.2138", "submitter": "Dimitris Paparas", "authors": "Xi Chen, Ilias Diakonikolas, Dimitris Paparas, Xiaorui Sun, Mihalis\n  Yannakakis", "title": "The Complexity of Optimal Multidimensional Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve the complexity of revenue-optimal deterministic auctions in the\nunit-demand single-buyer Bayesian setting, i.e., the optimal item pricing\nproblem, when the buyer's values for the items are independent. We show that\nthe problem of computing a revenue-optimal pricing can be solved in polynomial\ntime for distributions of support size 2, and its decision version is\nNP-complete for distributions of support size 3. We also show that the problem\nremains NP-complete for the case of identical distributions.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 06:21:33 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Chen", "Xi", ""], ["Diakonikolas", "Ilias", ""], ["Paparas", "Dimitris", ""], ["Sun", "Xiaorui", ""], ["Yannakakis", "Mihalis", ""]]}, {"id": "1311.2272", "submitter": "Amit Daniely", "authors": "Amit Daniely, Nati Linial, Shai Shalev-Shwartz", "title": "From average case complexity to improper learning complexity", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic problem in the PAC model of computational learning theory is to\ndetermine which hypothesis classes are efficiently learnable. There is\npresently a dearth of results showing hardness of learning problems. Moreover,\nthe existing lower bounds fall short of the best known algorithms.\n  The biggest challenge in proving complexity results is to establish hardness\nof {\\em improper learning} (a.k.a. representation independent learning).The\ndifficulty in proving lower bounds for improper learning is that the standard\nreductions from $\\mathbf{NP}$-hard problems do not seem to apply in this\ncontext. There is essentially only one known approach to proving lower bounds\non improper learning. It was initiated in (Kearns and Valiant 89) and relies on\ncryptographic assumptions.\n  We introduce a new technique for proving hardness of improper learning, based\non reductions from problems that are hard on average. We put forward a (fairly\nstrong) generalization of Feige's assumption (Feige 02) about the complexity of\nrefuting random constraint satisfaction problems. Combining this assumption\nwith our new technique yields far reaching implications. In particular,\n  1. Learning $\\mathrm{DNF}$'s is hard.\n  2. Agnostically learning halfspaces with a constant approximation ratio is\nhard.\n  3. Learning an intersection of $\\omega(1)$ halfspaces is hard.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 13:35:50 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 19:11:40 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Daniely", "Amit", ""], ["Linial", "Nati", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1311.2355", "submitter": "Mika G\\\"o\\\"os", "authors": "Mika G\\\"o\\\"os and Toniann Pitassi", "title": "Communication Lower Bounds via Critical Block Sensitivity", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use critical block sensitivity, a new complexity measure introduced by\nHuynh and Nordstr\\\"om (STOC 2012), to study the communication complexity of\nsearch problems. To begin, we give a simple new proof of the following central\nresult of Huynh and Nordstr\\\"om: if $S$ is a search problem with critical block\nsensitivity $b$, then every randomised two-party protocol solving a certain\ntwo-party lift of $S$ requires $\\Omega(b)$ bits of communication. Besides\nsimplicity, our proof has the advantage of generalising to the multi-party\nsetting. We combine these results with new critical block sensitivity lower\nbounds for Tseitin and Pebbling search problems to obtain the following\napplications:\n  (1) Monotone Circuit Depth: We exhibit a monotone $n$-variable function in NP\nwhose monotone circuits require depth $\\Omega(n/\\log n)$; previously, a bound\nof $\\Omega(\\sqrt{n})$ was known (Raz and Wigderson, JACM 1992). Moreover, we\nprove a $\\Theta(\\sqrt{n})$ monotone depth bound for a function in monotone P.\n  (2) Proof Complexity: We prove new rank lower bounds as well as obtain the\nfirst length--space lower bounds for semi-algebraic proof systems, including\nLov\\'asz--Schrijver and Lasserre (SOS) systems. In particular, these results\nextend and simplify the works of Beame et al. (SICOMP 2007) and Huynh and\nNordstr\\\"om.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 05:19:30 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 10:39:16 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["G\u00f6\u00f6s", "Mika", ""], ["Pitassi", "Toniann", ""]]}, {"id": "1311.2358", "submitter": "Bin Fu", "authors": "Bin Fu", "title": "Derandomizing Polynomial Identity over Finite Fields Implies\n  Super-Polynomial Circuit Lower Bounds for NEXP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that derandomizing polynomial identity testing over an arbitrary\nfinite field implies that NEXP does not have polynomial size boolean circuits.\nIn other words, for any finite field F(q) of size q, $PIT_q\\in\nNSUBEXP\\Rightarrow NEXP\\not\\subseteq P/poly$, where $PIT_q$ is the polynomial\nidentity testing problem over F(q), and NSUBEXP is the nondeterministic\nsubexpoential time class of languages. Our result is in contract to Kabanets\nand Impagliazzo's existing theorem that derandomizing the polynomial identity\ntesting in the integer ring Z implies that NEXP does have polynomial size\nboolean circuits or permanent over Z does not have polynomial size arithmetic\ncircuits.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 05:38:30 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Fu", "Bin", ""]]}, {"id": "1311.2369", "submitter": "Thomas Rothvoss", "authors": "Thomas Rothvoss", "title": "The matching polytope has exponential extension complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular method in combinatorial optimization is to express polytopes P,\nwhich may potentially have exponentially many facets, as solutions of linear\nprograms that use few extra variables to reduce the number of constraints down\nto a polynomial. After two decades of standstill, recent years have brought\namazing progress in showing lower bounds for the so called extension\ncomplexity, which for a polytope P denotes the smallest number of inequalities\nnecessary to describe a higher dimensional polytope Q that can be linearly\nprojected on P.\n  However, the central question in this field remained wide open: can the\nperfect matching polytope be written as an LP with polynomially many\nconstraints?\n  We answer this question negatively. In fact, the extension complexity of the\nperfect matching polytope in a complete n-node graph is 2^Omega(n). By a known\nreduction this also improves the lower bound on the extension complexity for\nthe TSP polytope from 2^Omega(n^1/2) to 2^Omega(n).\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 08:02:06 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 00:07:43 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 01:19:13 GMT"}, {"version": "v4", "created": "Fri, 17 Mar 2017 22:57:37 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Rothvoss", "Thomas", ""]]}, {"id": "1311.2466", "submitter": "Michael Lampis", "authors": "Michael Lampis", "title": "Parameterized Approximation Schemes using Graph Widths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining the techniques of approximation algorithms and parameterized\ncomplexity has long been considered a promising research area, but relatively\nfew results are currently known. In this paper we study the parameterized\napproximability of a number of problems which are known to be hard to solve\nexactly when parameterized by treewidth or clique-width. Our main contribution\nis to present a natural randomized rounding technique that extends well-known\nideas and can be used for both of these widths. Applying this very generic\ntechnique we obtain approximation schemes for a number of problems, evading\nboth polynomial-time inapproximability and parameterized intractability bounds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 15:38:31 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 10:27:18 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Lampis", "Michael", ""]]}, {"id": "1311.2501", "submitter": "Jan Krajicek", "authors": "Jan Krajicek", "title": "A reduction of proof complexity to computational complexity for\n  $AC^0[p]$ Frege systems", "comments": "the final version accepted to the Proc.AMS", "journal-ref": "Proceedings of the AMS, 143(11), (2015), pp.4951-4965", "doi": "10.1090/S0002-9939-2015-12610-X#sthash.pHkjNBMN.dpuf", "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a general reduction of lengths-of-proofs lower bounds for constant\ndepth Frege systems in DeMorgan language augmented by a connective counting\nmodulo a prime $p$ (the so called $AC^0[p]$ Frege systems) to computational\ncomplexity lower bounds for search tasks involving search trees branching upon\nvalues of maps on the vector space of low degree polynomials over the finite\nfiled with $p$ elements.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 16:55:50 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 17:11:45 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2013 08:05:42 GMT"}, {"version": "v4", "created": "Fri, 22 Aug 2014 07:29:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Krajicek", "Jan", ""]]}, {"id": "1311.2513", "submitter": "Timon Hertli", "authors": "Timon Hertli", "title": "Breaking the PPSZ Barrier for Unique 3-SAT", "comments": "13 pages; major revision with simplified algorithm but slightly worse\n  constants", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PPSZ algorithm by Paturi, Pudl\\'ak, Saks, and Zane (FOCS 1998) is the\nfastest known algorithm for (Promise) Unique k-SAT. We give an improved\nalgorithm with exponentially faster bounds for Unique 3-SAT.\n  For uniquely satisfiable 3-CNF formulas, we do the following case\ndistinction: We call a clause critical if exactly one literal is satisfied by\nthe unique satisfying assignment. If a formula has many critical clauses, we\nobserve that PPSZ by itself is already faster. If there are only few clauses\nallover, we use an algorithm by Wahlstr\\\"om (ESA 2005) that is faster than PPSZ\nin this case. Otherwise we have a formula with few critical and many\nnon-critical clauses. Non-critical clauses have at least two literals\nsatisfied; we show how to exploit this to improve PPSZ.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 17:48:45 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 13:41:04 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Hertli", "Timon", ""]]}, {"id": "1311.2571", "submitter": "Hamza Fawzi", "authors": "Hamza Fawzi, Pablo A. Parrilo", "title": "Exponential lower bounds on fixed-size psd rank and semidefinite\n  extension complexity", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of interest recently in proving lower bounds on the size\nof linear programs needed to represent a given polytope P. In a breakthrough\npaper Fiorini et al. [Proceedings of 44th ACM Symposium on Theory of Computing\n2012, pages 95-106] showed that any linear programming formulation of\nmaximum-cut must have exponential size. A natural question to ask is whether\none can prove such strong lower bounds for semidefinite programming\nformulations. In this paper we take a step towards this goal and we prove\nstrong lower bounds for a certain class of SDP formulations, namely SDPs over\nthe Cartesian product of fixed-size positive semidefinite cones. In practice\nthis corresponds to semidefinite programs with a block-diagonal structure and\nwhere blocks have constant size d. We show that any such extended formulation\nof the cut polytope must have exponential size (when d is fixed). The result of\nFiorini et al. for LP formulations is obtained as a special case when d=1. For\nblocks of size d=2 the result rules out any small formulations using\nsecond-order cone programming. Our study of SDP lifts over Cartesian product of\nfixed-size positive semidefinite cones is motivated mainly from practical\nconsiderations where it is well known that such SDPs can be solved more\nefficiently than general SDPs. The proof of our lower bound relies on new\nresults about the sparsity pattern of certain matrices with small psd rank,\ncombined with an induction argument inspired from the recent paper by Kaibel\nand Weltge [arXiv:1307.3543] on the LP extension complexity of the correlation\npolytope.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:37:32 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Fawzi", "Hamza", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "1311.2839", "submitter": "He Sun", "authors": "Zeyu Guo and He Sun", "title": "Gossip vs. Markov Chains, and Randomness-Efficient Rumor Spreading", "comments": "41 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1304.1359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study gossip algorithms for the rumor spreading problem which asks one\nnode to deliver a rumor to all nodes in an unknown network. We present the\nfirst protocol for any expander graph $G$ with $n$ nodes such that, the\nprotocol informs every node in $O(\\log n)$ rounds with high probability, and\nuses $\\tilde{O}(\\log n)$ random bits in total. The runtime of our protocol is\ntight, and the randomness requirement of $\\tilde{O}(\\log n)$ random bits almost\nmatches the lower bound of $\\Omega(\\log n)$ random bits for dense graphs. We\nfurther show that, for many graph families, polylogarithmic number of random\nbits in total suffice to spread the rumor in $O(\\mathrm{poly}\\log n)$ rounds.\nThese results together give us an almost complete understanding of the\nrandomness requirement of this fundamental gossip process.\n  Our analysis relies on unexpectedly tight connections among gossip processes,\nMarkov chains, and branching programs. First, we establish a connection between\nrumor spreading processes and Markov chains, which is used to approximate the\nrumor spreading time by the mixing time of Markov chains. Second, we show a\nreduction from rumor spreading processes to branching programs, and this\nreduction provides a general framework to derandomize gossip processes. In\naddition to designing rumor spreading protocols, these novel techniques may\nhave applications in studying parallel and multiple random walks, and\nrandomness complexity of distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 17:09:25 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Guo", "Zeyu", ""], ["Sun", "He", ""]]}, {"id": "1311.2972", "submitter": "Sewoong Oh", "authors": "Prateek Jain and Sewoong Oh", "title": "Learning Mixtures of Discrete Product Distributions using Spectral\n  Decompositions", "comments": "30 pages no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a distribution from samples, when the\nunderlying distribution is a mixture of product distributions over discrete\ndomains. This problem is motivated by several practical applications such as\ncrowd-sourcing, recommendation systems, and learning Boolean functions. The\nexisting solutions either heavily rely on the fact that the number of\ncomponents in the mixtures is finite or have sample/time complexity that is\nexponential in the number of components. In this paper, we introduce a\npolynomial time/sample complexity method for learning a mixture of $r$ discrete\nproduct distributions over $\\{1, 2, \\dots, \\ell\\}^n$, for general $\\ell$ and\n$r$. We show that our approach is statistically consistent and further provide\nfinite sample guarantees.\n  We use techniques from the recent work on tensor decompositions for\nhigher-order moment matching. A crucial step in these moment matching methods\nis to construct a certain matrix and a certain tensor with low-rank spectral\ndecompositions. These tensors are typically estimated directly from the\nsamples. The main challenge in learning mixtures of discrete product\ndistributions is that these low-rank tensors cannot be obtained directly from\nthe sample moments. Instead, we reduce the tensor estimation problem to: $a$)\nestimating a low-rank matrix using only off-diagonal block elements; and $b$)\nestimating a tensor using a small number of linear measurements. Leveraging on\nrecent developments in matrix completion, we give an alternating minimization\nbased method to estimate the low-rank matrix, and formulate the tensor\ncompletion problem as a least-squares problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 22:15:35 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 19:38:34 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Jain", "Prateek", ""], ["Oh", "Sewoong", ""]]}, {"id": "1311.3054", "submitter": "Amir Abboud", "authors": "Amir Abboud, Kevin Lewi, Ryan Williams", "title": "Losing Weight by Gaining Edges", "comments": "Title of an earlier version of this paper: On the Parameterized\n  Complexity of k-SUM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way to encode weighted sums into unweighted pairwise\nconstraints, obtaining the following results.\n  - Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there\nk which sum to zero? (It is well known that the same problem over arbitrary\nintegers is equivalent to the above definition, by linear-time randomized\nreductions.) We prove that this definition of k-SUM remains W[1]-hard, and is\nin fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of\nk-Clique.\n  - The maximum node-weighted k-Clique and node-weighted k-dominating set\nproblems can be reduced to n^o(1) instances of the unweighted k-Clique and\nk-dominating set problems, respectively. This implies a strong equivalence\nbetween the time complexities of the node weighted problems and the unweighted\nproblems: any polynomial improvement on one would imply an improvement for the\nother.\n  - A triangle of weight 0 in a node weighted graph with m edges can be\ndeterministically found in m^1.41 time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 09:26:57 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 02:58:29 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Abboud", "Amir", ""], ["Lewi", "Kevin", ""], ["Williams", "Ryan", ""]]}, {"id": "1311.3171", "submitter": "Hamid Jahanjou", "authors": "Hamid Jahanjou, Eric Miles, Emanuele Viola", "title": "Local reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce non-deterministic time $T \\ge 2^n$ to a 3SAT instance $\\phi$ of\nquasilinear size $|\\phi| = T \\cdot \\log^{O(1)} T$ such that there is an\nexplicit circuit $C$ that on input an index $i$ of $\\log |\\phi|$ bits outputs\nthe $i$th clause, and each output bit of $C$ depends on $O(1)$ input bits. The\nprevious best result was $C$ in NC$^1$. Even in the simpler setting of\npolynomial size $|\\phi| = \\poly(T)$ the previous best result was $C$ in AC$^0$.\n  More generally, for any time $T \\ge n$ and parameter $r \\leq n$ we obtain\n$\\log_2 |\\phi| = \\max(\\log T, n/r) + O(\\log n) + O(\\log\\log T)$ and each output\nbit of $C$ is a decision tree of depth $O(\\log r)$.\n  As an application, we tighten Williams' connection between satisfiability\nalgorithms and circuit lower bounds (STOC 2010; SIAM J. Comput. 2013).\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 15:28:37 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 14:24:46 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 23:11:42 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Jahanjou", "Hamid", ""], ["Miles", "Eric", ""], ["Viola", "Emanuele", ""]]}, {"id": "1311.3297", "submitter": "David Gosset", "authors": "Andrew M. Childs, David Gosset, Zak Webb", "title": "The Bose-Hubbard model is QMA-complete", "comments": null, "journal-ref": "Proceedings of the 41st International Colloquium on Automata,\n  Languages, and Programming (ICALP 2014), pp. 308-319 (2014)", "doi": "10.1007/978-3-662-43948-7_26", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bose-Hubbard model is a system of interacting bosons that live on the\nvertices of a graph. The particles can move between adjacent vertices and\nexperience a repulsive on-site interaction. The Hamiltonian is determined by a\nchoice of graph that specifies the geometry in which the particles move and\ninteract. We prove that approximating the ground energy of the Bose-Hubbard\nmodel on a graph at fixed particle number is QMA-complete. In our QMA-hardness\nproof, we encode the history of an n-qubit computation in the subspace with at\nmost one particle per site (i.e., hard-core bosons). This feature, along with\nthe well-known mapping between hard-core bosons and spin systems, lets us prove\na related result for a class of 2-local Hamiltonians defined by graphs that\ngeneralizes the XY model. By avoiding the use of perturbation theory in our\nanalysis, we circumvent the need to multiply terms in the Hamiltonian by large\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 21:00:01 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Childs", "Andrew M.", ""], ["Gosset", "David", ""], ["Webb", "Zak", ""]]}, {"id": "1311.3607", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini and Giordano Da Lozzo and Daniel Neuwirth", "title": "Advancements on SEFE and Partitioned Book Embedding Problems", "comments": "29 pages, 10 figures, extended version of 'On Some NP-complete SEFE\n  Problems' (Eighth International Workshop on Algorithms and Computation, 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the complexity of some problems related to the\n{\\em Simultaneous Embedding with Fixed Edges} (SEFE) of $k$ planar graphs and\nthe PARTITIONED $k$-PAGE BOOK EMBEDDING (PBE-$k$) problems, which are known to\nbe equivalent under certain conditions.\n  While the computational complexity of SEFE for $k=2$ is still a central open\nquestion in Graph Drawing, the problem is NP-complete for $k \\geq 3$ [Gassner\n{\\em et al.}, WG '06], even if the intersection graph is the same for each pair\nof graphs ({\\em sunflower intersection}) [Schaefer, JGAA (2013)].\n  We improve on these results by proving that SEFE with $k \\geq 3$ and\nsunflower intersection is NP-complete even when the intersection graph is a\ntree and all the input graphs are biconnected. Also, we prove NP-completeness\nfor $k \\geq 3$ of problem PBE-$k$ and of problem PARTITIONED T-COHERENT\n$k$-PAGE BOOK EMBEDDING (PTBE-$k$) - that is the generalization of PBE-$k$ in\nwhich the ordering of the vertices on the spine is constrained by a tree $T$ -\neven when two input graphs are biconnected. Further, we provide a linear-time\nalgorithm for PTBE-$k$ when $k-1$ pages are assigned a connected graph.\nFinally, we prove that the problem of maximizing the number of edges that are\ndrawn the same in a SEFE of two graphs is NP-complete in several restricted\nsettings ({\\em optimization version of SEFE}, Open Problem $9$, Chapter $11$ of\nthe Handbook of Graph Drawing and Visualization).\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 19:00:59 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 13:38:21 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Angelini", "Patrizio", ""], ["Da Lozzo", "Giordano", ""], ["Neuwirth", "Daniel", ""]]}, {"id": "1311.3785", "submitter": "Vijay  Menon", "authors": "Vijay Menon", "title": "Deterministic Primality Testing - understanding the AKS algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prime numbers play a very vital role in modern cryptography and especially\nthe difficulties involved in factoring numbers composed of product of two large\nprime numbers have been put to use in many modern cryptographic designs. Thus,\nthe problem of distinguishing prime numbers from the rest is vital and\ntherefore there is a need to have efficient primality testing algorithms.\nAlthough there had been many probabilistic algorithms for primality testing,\nthere wasn't a deterministic polynomial time algorithm until 2002 when Agrawal,\nKayal and Saxena came with an algorithm, popularly known as the AKS algorithm,\nwhich could test whether a given number is prime or composite in polynomial\ntime. This project is an attempt at understanding the ingenious idea behind\nthis algorithm and the underlying principles of mathematics that is required to\nstudy it. In fact, through out this project, one of the major objectives has\nbeen to make it as much self contained as possible. Finally, the project\nprovides an implementation of the algorithm using Software for Algebra and\nGeometry Experimentation (SAGE) and arrives at conclusions on how practical or\notherwise it is.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 09:41:54 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Menon", "Vijay", ""]]}, {"id": "1311.3826", "submitter": "Umang Mathur", "authors": "Shankara Narayanan Krishna, Umang Mathur, Ashutosh Trivedi", "title": "Weak Singular Hybrid Automata", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-10512-3_12", "report-no": null, "categories": "cs.FL cs.CC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of Hybrid automata, introduced by Alur, Courcourbetis,\nHenzinger, and Ho, provides a formal modeling and analysis environment to\nanalyze the interaction between the discrete and the continuous parts of\ncyber-physical systems. Hybrid automata can be considered as generalizations of\nfinite state automata augmented with a finite set of real-valued variables\nwhose dynamics in each state is governed by a system of ordinary differential\nequations. Moreover, the discrete transitions of hybrid automata are guarded by\nconstraints over the values of these real-valued variables, and enable\ndiscontinuous jumps in the evolution of these variables. Singular hybrid\nautomata are a subclass of hybrid automata where dynamics is specified by\nstate-dependent constant vectors. Henzinger, Kopke, Puri, and Varaiya showed\nthat for even very restricted subclasses of singular hybrid automata, the\nfundamental verification questions, like reachability and schedulability, are\nundecidable. In this paper we present \\emph{weak singular hybrid automata}\n(WSHA), a previously unexplored subclass of singular hybrid automata, and show\nthe decidability (and the exact complexity) of various verification questions\nfor this class including reachability (NP-Complete) and LTL model-checking\n(PSPACE-Complete). We further show that extending WSHA with a single\nunrestricted clock or extending WSHA with unrestricted variable updates lead to\nundecidability of reachability problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 12:31:14 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 05:34:33 GMT"}, {"version": "v3", "created": "Thu, 19 Jun 2014 19:22:40 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Krishna", "Shankara Narayanan", ""], ["Mathur", "Umang", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "1311.4001", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun, Samuel Fiorini, Sebastian Pokutta", "title": "Average case polyhedral complexity of the maximum stable set problem", "comments": null, "journal-ref": "Math. Program. 160(1), 2016, 407-431", "doi": "10.1007/s10107-016-0989-3", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum number of constraints needed to formulate random\ninstances of the maximum stable set problem via linear programs (LPs), in two\ndistinct models. In the uniform model, the constraints of the LP are not\nallowed to depend on the input graph, which should be encoded solely in the\nobjective function. There we prove a $2^{\\Omega(n/ \\log n)}$ lower bound with\nprobability at least $1 - 2^{-2^n}$ for every LP that is exact for a randomly\nselected set of instances; each graph on at most n vertices being selected\nindependently with probability $p \\geq 2^{-\\binom{n/4}{2}+n}$. In the\nnon-uniform model, the constraints of the LP may depend on the input graph, but\nwe allow weights on the vertices. The input graph is sampled according to the\nG(n, p) model. There we obtain upper and lower bounds holding with high\nprobability for various ranges of p. We obtain a super-polynomial lower bound\nall the way from $p = \\Omega(\\log^{6+\\varepsilon} / n)$ to $p = o (1 / \\log\nn)$. Our upper bound is close to this as there is only an essentially quadratic\ngap in the exponent, which currently also exists in the worst-case model.\nFinally, we state a conjecture that would close this gap, both in the\naverage-case and worst-case models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 23:28:58 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 20:31:43 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 18:56:44 GMT"}, {"version": "v4", "created": "Wed, 2 Mar 2016 19:25:59 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Fiorini", "Samuel", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1311.4066", "submitter": "Susan Margulies", "authors": "S. Margulies and J. Morton", "title": "Polynomial-time Solvable #CSP Problems via Algebraic Models and Pfaffian\n  Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Pfaffian circuit is a tensor contraction network where the edges are\nlabeled with changes of bases in such a way that a very specific set of\ncombinatorial properties are satisfied. By modeling the permissible changes of\nbases as systems of polynomial equations, and then solving via computation, we\nare able to identify classes of 0/1 planar #CSP problems solvable in\npolynomial-time via the Pfaffian circuit evaluation theorem (a variant of L.\nValiant's Holant Theorem). We present two different models of 0/1 variables,\none that is possible under a homogeneous change of basis, and one that is\npossible under a heterogeneous change of basis only. We enumerate a series of\n1,2,3, and 4-arity gates/cogates that represent constraints, and define a class\nof constraints that is possible under the assumption of a ``bridge\" between two\nparticular changes of bases. We discuss the issue of planarity of Pfaffian\ncircuits, and demonstrate possible directions in algebraic computation for\ndesigning a Pfaffian tensor contraction network fragment that can simulate a\nswap gate/cogate. We conclude by developing the notion of a decomposable\ngate/cogate, and discuss the computational benefits of this definition.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 14:20:00 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Margulies", "S.", ""], ["Morton", "J.", ""]]}, {"id": "1311.4219", "submitter": "Stanislav Zivny", "authors": "Vladimir Kolmogorov, Johan Thapper, Stanislav Zivny", "title": "The power of linear programming for general-valued CSPs", "comments": "A full version of a FOCS'12 paper by the last two authors\n  (arXiv:1204.1079) and an ICALP'13 paper by the first author (arXiv:1207.7213)\n  to appear in SIAM Journal on Computing (SICOMP)", "journal-ref": "SIAM Journal on Computing 44(1) (2015) 1-36", "doi": "10.1137/130945648", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $D$, called the domain, be a fixed finite set and let $\\Gamma$, called\nthe valued constraint language, be a fixed set of functions of the form\n$f:D^m\\to\\mathbb{Q}\\cup\\{\\infty\\}$, where different functions might have\ndifferent arity $m$. We study the valued constraint satisfaction problem\nparametrised by $\\Gamma$, denoted by VCSP$(\\Gamma)$. These are minimisation\nproblems given by $n$ variables and the objective function given by a sum of\nfunctions from $\\Gamma$, each depending on a subset of the $n$ variables.\nFinite-valued constraint languages contain functions that take on only rational\nvalues and not infinite values.\n  Our main result is a precise algebraic characterisation of valued constraint\nlanguages whose instances can be solved exactly by the basic linear programming\nrelaxation (BLP). For a valued constraint language $\\Gamma$, BLP is a decision\nprocedure for $\\Gamma$ if and only if $\\Gamma$ admits a symmetric fractional\npolymorphism of every arity. For a finite-valued constraint language $\\Gamma$,\nBLP is a decision procedure if and only if $\\Gamma$ admits a symmetric\nfractional polymorphism of some arity, or equivalently, if $\\Gamma$ admits a\nsymmetric fractional polymorphism of arity 2.\n  Using these results, we obtain tractability of several novel classes of\nproblems, including problems over valued constraint languages that are: (1)\nsubmodular on arbitrary lattices; (2) $k$-submodular on arbitrary finite\ndomains; (3) weakly (and hence strongly) tree-submodular on arbitrary trees.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 21:37:51 GMT"}, {"version": "v2", "created": "Wed, 9 Jul 2014 21:06:36 GMT"}, {"version": "v3", "created": "Tue, 25 Nov 2014 15:33:12 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Kolmogorov", "Vladimir", ""], ["Thapper", "Johan", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1311.4451", "submitter": "Andreas Galanis", "authors": "Jin-Yi Cai and Andreas Galanis and Leslie Ann Goldberg and Heng Guo\n  and Mark Jerrum and Daniel Stefankovic and Eric Vigoda", "title": "#BIS-Hardness for 2-Spin Systems on Bipartite Bounded Degree Graphs in\n  the Tree Nonuniqueness Region", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcss.2015.11.009", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting independent sets on bipartite graphs (#BIS) is considered a\ncanonical counting problem of intermediate approximation complexity. It is\nconjectured that #BIS neither has an FPRAS nor is as hard as #SAT to\napproximate. We study #BIS in the general framework of two-state spin systems\non bipartite graphs. We define two notions, nearly-independent phase-correlated\nspins and unary symmetry breaking. We prove that it is #BIS-hard to approximate\nthe partition function of any 2-spin system on bipartite graphs supporting\nthese two notions. As a consequence, we classify the complexity of\napproximating the partition function of antiferromagnetic 2-spin systems on\nbounded-degree bipartite graphs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:03:51 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 20:07:28 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 12:42:36 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2015 14:25:30 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Cai", "Jin-Yi", ""], ["Galanis", "Andreas", ""], ["Goldberg", "Leslie Ann", ""], ["Guo", "Heng", ""], ["Jerrum", "Mark", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""]]}, {"id": "1311.4821", "submitter": "Will Perkins", "authors": "Vitaly Feldman, Will Perkins, Santosh Vempala", "title": "On the Complexity of Random Satisfiability Problems with Planted\n  Solutions", "comments": "Extended abstract appeared in STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying a planted assignment given a random $k$-SAT\nformula consistent with the assignment exhibits a large algorithmic gap: while\nthe planted solution becomes unique and can be identified given a formula with\n$O(n\\log n)$ clauses, there are distributions over clauses for which the best\nknown efficient algorithms require $n^{k/2}$ clauses. We propose and study a\nunified model for planted $k$-SAT, which captures well-known special cases. An\ninstance is described by a planted assignment $\\sigma$ and a distribution on\nclauses with $k$ literals. We define its distribution complexity as the largest\n$r$ for which the distribution is not $r$-wise independent ($1 \\le r \\le k$ for\nany distribution with a planted assignment).\n  Our main result is an unconditional lower bound, tight up to logarithmic\nfactors, for statistical (query) algorithms [Kearns 1998, Feldman et. al 2012],\nmatching known upper bounds, which, as we show, can be implemented using a\nstatistical algorithm. Since known approaches for problems over distributions\nhave statistical analogues (spectral, MCMC, gradient-based, convex optimization\netc.), this lower bound provides a rigorous explanation of the observed\nalgorithmic gap. The proof introduces a new general technique for the analysis\nof statistical query algorithms. It also points to a geometric paring\nphenomenon in the space of all planted assignments.\n  We describe consequences of our lower bounds to Feige's refutation hypothesis\n[Feige 2002] and to lower bounds on general convex programs that solve planted\n$k$-SAT. Our bounds also extend to other planted $k$-CSP models, and, in\nparticular, provide concrete evidence for the security of Goldreich's one-way\nfunction and the associated pseudorandom generator when used with a\nsufficiently hard predicate [Goldreich 2000].\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 18:13:32 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 17:17:43 GMT"}, {"version": "v3", "created": "Thu, 17 Jul 2014 13:20:31 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 07:29:18 GMT"}, {"version": "v5", "created": "Thu, 14 May 2015 04:57:56 GMT"}, {"version": "v6", "created": "Fri, 15 Jan 2016 18:58:50 GMT"}, {"version": "v7", "created": "Sun, 15 Oct 2017 16:37:42 GMT"}, {"version": "v8", "created": "Tue, 6 Mar 2018 18:24:11 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Feldman", "Vitaly", ""], ["Perkins", "Will", ""], ["Vempala", "Santosh", ""]]}, {"id": "1311.4839", "submitter": "Andreas Galanis", "authors": "Andreas Galanis, Daniel Stefankovic, Eric Vigoda, and Linji Yang", "title": "Ferromagnetic Potts Model: Refined #BIS-hardness and Related Results", "comments": "To appear in SIAM J. Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math-ph math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results establish for 2-spin antiferromagnetic systems that the\ncomputational complexity of approximating the partition function on graphs of\nmaximum degree D undergoes a phase transition that coincides with the\nuniqueness phase transition on the infinite D-regular tree. For the\nferromagnetic Potts model we investigate whether analogous hardness results\nhold. Goldberg and Jerrum showed that approximating the partition function of\nthe ferromagnetic Potts model is at least as hard as approximating the number\nof independent sets in bipartite graphs (#BIS-hardness). We improve this\nhardness result by establishing it for bipartite graphs of maximum degree D. We\nfirst present a detailed picture for the phase diagram for the infinite\nD-regular tree, giving a refined picture of its first-order phase transition\nand establishing the critical temperature for the coexistence of the disordered\nand ordered phases. We then prove for all temperatures below this critical\ntemperature that it is #BIS-hard to approximate the partition function on\nbipartite graphs of maximum degree D. As a corollary, it is #BIS-hard to\napproximate the number of k-colorings on bipartite graphs of maximum degree D\nwhen k <= D/(2 ln D).\n  The #BIS-hardness result for the ferromagnetic Potts model uses random\nbipartite regular graphs as a gadget in the reduction. The analysis of these\nrandom graphs relies on recent connections between the maxima of the\nexpectation of their partition function, attractive fixpoints of the associated\ntree recursions, and induced matrix norms. We extend these connections to\nrandom regular graphs for all ferromagnetic models and establish the Bethe\nprediction for every ferromagnetic spin system on random regular graphs. We\nalso prove for the ferromagnetic Potts model that the Swendsen-Wang algorithm\nis torpidly mixing on random D-regular graphs at the critical temperature for\nlarge q.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 18:54:14 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 20:36:44 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 20:36:24 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Galanis", "Andreas", ""], ["Stefankovic", "Daniel", ""], ["Vigoda", "Eric", ""], ["Yang", "Linji", ""]]}, {"id": "1311.5090", "submitter": "Madhur Tulsiani", "authors": "Arnab Bhattacharyya and Pooya Hatami and Madhur Tulsiani", "title": "Algorithmic regularity for polynomials and applications", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analogy with the regularity lemma of Szemer\\'edi, regularity lemmas for\npolynomials shown by Green and Tao (Contrib. Discrete Math. 2009) and by\nKaufman and Lovett (FOCS 2008) modify a given collection of polynomials \\calF =\n{P_1,...,P_m} to a new collection \\calF' so that the polynomials in \\calF' are\n\"pseudorandom\". These lemmas have various applications, such as (special cases)\nof Reed-Muller testing and worst-case to average-case reductions for\npolynomials. However, the transformation from \\calF to \\calF' is not\nalgorithmic for either regularity lemma. We define new notions of regularity\nfor polynomials, which are analogous to the above, but which allow for an\nefficient algorithm to compute the pseudorandom collection \\calF'. In\nparticular, when the field is of high characteristic, in polynomial time, we\ncan refine \\calF into \\calF' where every nonzero linear combination of\npolynomials in \\calF' has desirably small Gowers norm.\n  Using the algorithmic regularity lemmas, we show that if a polynomial P of\ndegree d is within (normalized) Hamming distance 1-1/|F| -\\eps of some unknown\npolynomial of degree k over a prime field F (for k < d < |F|), then there is an\nefficient algorithm for finding a degree-k polynomial Q, which is within\ndistance 1-1/|F| -\\eta of P, for some \\eta depending on \\eps. This can be\nthought of as decoding the Reed-Muller code of order k beyond the list decoding\nradius (finding one close codeword), when the received word P itself is a\npolynomial of degree d (with k < d < |F|).\n  We also obtain an algorithmic version of the worst-case to average-case\nreductions by Kaufman and Lovett. They show that if a polynomial of degree d\ncan be weakly approximated by a polynomial of lower degree, then it can be\ncomputed exactly using a collection of polynomials of degree at most d-1. We\ngive an efficient (randomized) algorithm to find this collection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 05:07:16 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Hatami", "Pooya", ""], ["Tulsiani", "Madhur", ""]]}, {"id": "1311.5102", "submitter": "Zeev Dvir", "authors": "Zeev Dvir, Shubhangi Saraf, Avi Wigderson", "title": "Breaking the quadratic barrier for 3-LCCs over the Reals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that 3-query linear locally correctable codes over the Reals of\ndimension $d$ require block length $n>d^{2+\\lambda}$ for some fixed, positive\n$\\lambda >0$. Geometrically, this means that if $n$ vectors in $R^d$ are such\nthat each vector is spanned by a linear number of disjoint triples of others,\nthen it must be that $n > d^{2+\\lambda}$. This improves the known quadratic\nlower bounds (e.g. {KdW04, Wood07}). While a modest improvement, we expect that\nthe new techniques introduced in this work will be useful for further progress\non lower bounds of locally correctable and decodable codes with more than 2\nqueries, possibly over other fields as well.\n  Our proof introduces several new ideas to existing lower bound techniques,\nseveral of which work over every field. At a high level, our proof has two\nparts, {\\it clustering} and {\\it random restriction}.\n  The clustering step uses a powerful theorem of Barthe from convex geometry.\nIt can be used (after preprocessing our LCC to be {\\it balanced}), to apply a\nbasis change (and rescaling) of the vectors, so that the resulting unit vectors\nbecome {\\it nearly isotropic}. This together with the fact that any LCC must\nhave many `correlated' pairs of points, lets us deduce that the vectors must\nhave a surprisingly strong geometric clustering, and hence also combinatorial\nclustering with respect to the spanning triples.\n  In the restriction step, we devise a new variant of the dimension reduction\ntechnique used in previous lower bounds, which is able to take advantage of the\ncombinatorial clustering structure above. The analysis of our random projection\nmethod reduces to a simple (weakly) random graph process, and works over any\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 15:39:38 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Dvir", "Zeev", ""], ["Saraf", "Shubhangi", ""], ["Wigderson", "Avi", ""]]}, {"id": "1311.5186", "submitter": "Mohammad Bavarian", "authors": "Mohammad Bavarian and Peter W. Shor", "title": "Information Causality, Szemer\\'{e}di-Trotter and Algebraic Variants of\n  CHSH", "comments": "Fixed some typos and added minor errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the following family of two prover one-round games.\nIn the CHSH_q game, two parties are given x,y in F_q uniformly at random, and\neach must produce an output a,b in F_q without communicating with the other.\nThe players' objective is to maximize the probability that their outputs\nsatisfy a+b=xy in F_q. This game was introduced by Buhrman and Massar (PRA\n2005) as a large alphabet generalization of the celebrated CHSH game---which is\none of the most well-studied two-prover games in quantum information theory,\nand which has a large number of applications to quantum cryptography and\nquantum complexity.\n  Our main contributions in this paper are the first asymptotic and explicit\nbounds on the entangled and classical values of CHSH_q, and the realization of\na rather surprising connection between CHSH_q and geometric incidence theory.\nOn the way to these results, we also resolve a problem of Pawlowski and Winter\nabout pairwise independent Information Causality, which, beside being\ninteresting on its own, gives as an application a short proof of our upper\nbound for the entangled value of CHSH_q.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 19:38:16 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 22:51:11 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Bavarian", "Mohammad", ""], ["Shor", "Peter W.", ""]]}, {"id": "1311.5414", "submitter": "Akitoshi Kawamura", "authors": "Akitoshi Kawamura (University of Tokyo), Hiroyuki Ota (University of\n  Tokyo), Carsten R\\\"osnick (Technische Universit\\\"at Darmstadt), Martin\n  Ziegler (Technische Universit\\\"at Darmstadt)", "title": "Computational Complexity of Smooth Differential Equations", "comments": "15 pages, 3 figures", "journal-ref": "Logical Methods in Computer Science, Volume 10, Issue 1 (February\n  11, 2014) lmcs:960", "doi": "10.2168/LMCS-10(1:6)2014", "report-no": null, "categories": "cs.CC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of the solutions $h$ to the ordinary\ndifferential equation $h(0)=0$, $h'(t) = g(t, h(t))$ under various assumptions\non the function $g$ has been investigated. Kawamura showed in 2010 that the\nsolution $h$ can be PSPACE-hard even if $g$ is assumed to be Lipschitz\ncontinuous and polynomial-time computable. We place further requirements on the\nsmoothness of $g$ and obtain the following results: the solution $h$ can still\nbe PSPACE-hard if $g$ is assumed to be of class $C^1$; for each $k\\ge2$, the\nsolution $h$ can be hard for the counting hierarchy even if $g$ is of class\n$C^k$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 14:23:05 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2014 17:46:29 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kawamura", "Akitoshi", "", "University of Tokyo"], ["Ota", "Hiroyuki", "", "University of\n  Tokyo"], ["R\u00f6snick", "Carsten", "", "Technische Universit\u00e4t Darmstadt"], ["Ziegler", "Martin", "", "Technische Universit\u00e4t Darmstadt"]]}, {"id": "1311.5467", "submitter": "Emanuele Viola", "authors": "Emanuele Viola", "title": "Challenges in computational lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We draw two incomplete, biased maps of challenges in computational complexity\nlower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 16:28:48 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Viola", "Emanuele", ""]]}, {"id": "1311.5622", "submitter": "Ariel Gabizon", "authors": "Ariel Gabizon", "title": "Improved Extractors for Affine Lines", "comments": "The paper has been withdrawn as it is being merged into a joint paper\n  with additional authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $F$ be the field of $q$ elements.\n  We investigate the following Ramsey coloring problem for vector spaces: Given\na vector space $\\F^n$, give a coloring of the points of $F^n$ with two colors\nsuch that no affine line (i.e., affine subspace of dimension $1$) is\nmonochromatic. Our main result is as follows:\n  For any $q\\geq 25\\cdot n$ and $n>4$, we give an explicit coloring $D:F^n\\ar\n\\set{0,1}$ such that for every affine line $l\\subseteq F^n$, $D(l)=\\set{0,1}$.\nPreviously this was known only for $q\\geq c\\cdot n^2$ for some constant $c$\n\\cite{GR05}. We note that this beats the random coloring for which the expected\nnumber of monochromatic lines will be 0 only when $q\\geq c\\cdot n\\log n$ for\nsome constant $c$. Furthermore, our coloring will be `almost balanced' on every\naffine line. Let us state this formally in the lanuage of \\emph{extractors}. We\nsay that a function $D:F^n\\mapsto \\set{0,1}$ is a \\afsext{1}{\\eps} if for every\naffine line $l\\subseteq \\F^n$, $D(X)$ is $\\eps$-close to uniform when $X$ is\nuniformly distributed over $l$. We construct a \\afsext{1}{\\eps} with $\\eps =\n\\Omega(\\sqrt{n/q})$ whenever $q\\geq c\\cdot n$ for some constant $c$.\n  The previous result of \\cite{GR05} gave a \\afsext{1}{\\eps} only for\n$q=\\Omega(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 00:01:53 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 21:18:55 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Gabizon", "Ariel", ""]]}, {"id": "1311.5690", "submitter": "Bojin Zheng", "authors": "Bojin Zheng, Yangqian Su, Hongrun Wu, Li Kuang", "title": "The Ergodicity of the Collatz Process in Positive Integer Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $3x+1$ problem, also called the Collatz conjecture, is a very interesting\nunsolved mathematical problem related to computer science. This paper\ngeneralized this problem by relaxing the constraints, i.e., generalizing this\ndeterministic process to non-deterministic process, and set up three models.\nThis paper analyzed the ergodicity of these models and proved that the\nergodicity of the Collatz process in positive integer field holds, i.e., all\nthe positive integers can be transformed to 1 by the iterations of the Collatz\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 09:59:26 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Zheng", "Bojin", ""], ["Su", "Yangqian", ""], ["Wu", "Hongrun", ""], ["Kuang", "Li", ""]]}, {"id": "1311.5694", "submitter": "Bruno Grenet", "authors": "Arkadev Chattopadhyay, Bruno Grenet, Pascal Koiran, Natacha Portier\n  and Yann Strozecki", "title": "Computing the multilinear factors of lacunary polynomials without\n  heights", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic algorithm which computes the multilinear factors\nof multivariate lacunary polynomials over number fields. Its complexity is\npolynomial in $\\ell^n$ where $\\ell$ is the lacunary size of the input\npolynomial and $n$ its number of variables, that is in particular polynomial in\nthe logarithm of its degree. We also provide a randomized algorithm for the\nsame problem of complexity polynomial in $\\ell$ and $n$.\n  Over other fields of characteristic zero and finite fields of large\ncharacteristic, our algorithms compute the multilinear factors having at least\nthree monomials of multivariate polynomials. Lower bounds are provided to\nexplain the limitations of our algorithm. As a by-product, we also design\npolynomial-time deterministic polynomial identity tests for families of\npolynomials which were not known to admit any.\n  Our results are based on so-called Gap Theorem which reduce high-degree\nfactorization to repeated low-degree factorizations. While previous algorithms\nused Gap Theorems expressed in terms of the heights of the coefficients, our\nGap Theorems only depend on the exponents of the polynomials. This makes our\nalgorithms more elementary and general, and faster in most cases.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 10:15:20 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 14:51:29 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chattopadhyay", "Arkadev", ""], ["Grenet", "Bruno", ""], ["Koiran", "Pascal", ""], ["Portier", "Natacha", ""], ["Strozecki", "Yann", ""]]}, {"id": "1311.5935", "submitter": "Yann Disser", "authors": "Yann Disser, Martin Skutella", "title": "The Simplex Algorithm is NP-mighty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to classify the power of algorithms by the complexity of the\nproblems that they can be used to solve. Instead of restricting to the problem\na particular algorithm was designed to solve explicitly, however, we include\nproblems that, with polynomial overhead, can be solved 'implicitly' during the\nalgorithm's execution. For example, we allow to solve a decision problem by\nsuitably transforming the input, executing the algorithm, and observing whether\na specific bit in its internal configuration ever switches during the\nexecution. We show that the Simplex Method, the Network Simplex Method (both\nwith Dantzig's original pivot rule), and the Successive Shortest Path Algorithm\nare NP-mighty, that is, each of these algorithms can be used to solve any\nproblem in NP. This result casts a more favorable light on these algorithms'\nexponential worst-case running times. Furthermore, as a consequence of our\napproach, we obtain several novel hardness results. For example, for a given\ninput to the Simplex Algorithm, deciding whether a given variable ever enters\nthe basis during the algorithm's execution and determining the number of\niterations needed are both NP-hard problems. Finally, we close a long-standing\nopen problem in the area of network flows over time by showing that earliest\narrival flows are NP-hard to obtain.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 00:09:17 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 17:44:27 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Disser", "Yann", ""], ["Skutella", "Martin", ""]]}, {"id": "1311.6192", "submitter": "Kazuyuki Amano", "authors": "Manami Shigeta and Kazuyuki Amano", "title": "Ordered Biclique Partitions and Communication Complexity Problems", "comments": "8 pages; the version submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ordered biclique partition of the complete graph $K_n$ on $n$ vertices is\na collection of bicliques (i.e., complete bipartite graphs) such that (i) every\nedge of $K_n$ is covered by at least one and at most two bicliques in the\ncollection, and (ii) if an edge $e$ is covered by two bicliques then each\nendpoint of $e$ is in the first class in one of these bicliques and in the\nsecond class in other one. In this note, we give an explicit construction of\nsuch a collection of size $n^{1/2+o(1)}$, which improves the $O(n^{2/3})$ bound\nshown in the previous work [Disc. Appl. Math., 2014].\n  As the immediate consequences of this result, we show (i) a construction of\n$n \\times n$ 0/1 matrices of rank $n^{1/2+o(1)}$ which have a fooling set of\nsize $n$, i.e., the gap between rank and fooling set size can be at least\nalmost quadratic, and (ii) an improved lower bound $(2-o(1)) \\log N$ on the\nnondeterministic communication complexity of the clique vs. independent set\nproblem, which matches the best known lower bound on the deterministic version\nof the problem shown by Kushilevitz, Linial and Ostrovsky [Combinatorica,\n1999].\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 01:23:02 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 04:09:47 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Shigeta", "Manami", ""], ["Amano", "Kazuyuki", ""]]}, {"id": "1311.6309", "submitter": "Penghui Yao", "authors": "Rahul Jain, Attila Pereszl\\'enyi, Penghui Yao", "title": "A parallel repetition theorem for entangled two-player one-round games\n  under product distributions", "comments": "14 pages. Accepted by CCC 2014, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a parallel repetition theorem for the entangled value $\\omega^*(G)$\nof any two-player one-round game $G$ where the questions $(x,y) \\in\n\\mathcal{X}\\times\\mathcal{Y}$ to Alice and Bob are drawn from a product\ndistribution on $\\mathcal{X}\\times\\mathcal{Y}$. We show that for the $k$-fold\nproduct $G^k$ of the game $G$ (which represents the game $G$ played in parallel\n$k$ times independently),\n  $ \\omega^*(G^k)\n=\\left(1-(1-\\omega^*(G))^3\\right)^{\\Omega\\left(\\frac{k}{\\log(|\\mathcal{A}|\n\\cdot |\\mathcal{B}|)}\\right)} $, where $\\mathcal{A}$ and $\\mathcal{B}$\nrepresent the sets from which the answers of Alice and Bob are drawn.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 14:07:52 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 05:28:52 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Jain", "Rahul", ""], ["Pereszl\u00e9nyi", "Attila", ""], ["Yao", "Penghui", ""]]}, {"id": "1311.6474", "submitter": "Martin Schwarz", "authors": "Martin Schwarz, Toby S. Cubitt, Frank Verstraete", "title": "An Information-Theoretic Proof of the Constructive Commutative Quantum\n  Lov\\'asz Local Lemma", "comments": "7 pages, 9 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quantum Lov\\'asz Local Lemma (QLLL) [AKS12] establishes\nnon-constructively that any quantum system constrained by a local Hamiltonian\nhas a zero-energy ground state, if the local Hamiltonian terms overlap only in\na certain restricted way. In this paper, we present an efficient quantum\nalgorithm to prepare this ground state for the special case of commuting\nprojector terms. The related classical problem has been open for more than 34\nyears. Our algorithm follows the breakthrough ideas of Moser's [Moser09]\nclassical algorithm and lifts his information theoretic argument to the quantum\nsetting. A similar result has been independently published by Arad and Sattath\n[AS13] recently.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 21:00:09 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Schwarz", "Martin", ""], ["Cubitt", "Toby S.", ""], ["Verstraete", "Frank", ""]]}, {"id": "1311.6671", "submitter": "Daniel Dadush", "authors": "Daniel Dadush", "title": "A Deterministic Polynomial Space Construction for eps-nets under any\n  Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic polynomial space construction for nearly optimal\neps-nets with respect to any input n-dimensional convex body K and norm |.|.\nMore precisely, our algorithm can build and iterate over an eps-net of K with\nrespect to |.| in time 2^O(n) x (size of the optimal net) using only\npoly(n)-space. This improves on previous constructions of Alon et al [STOC\n2013] which achieve either a 2^O(n) approximation or an n^O(n) approximation of\nthe optimal net size using 2^n space and poly(n)-space respectively. As in\ntheir work, our algorithm relies on the mathematically classical approach of\nbuilding thin lattice coverings of space, which reduces the task of\nconstructing eps-nets to the problem of enumerating lattice points. Our main\ntechnical contribution is a deterministic 2^O(n)-time and poly(n)-space\nconstruction of thin lattice coverings of space with respect to any convex\nbody, where enumeration in these lattices can be efficiently performed using\npoly(n)-space. This also yields the first existential construction of\npoly(n)-space enumerable thin covering lattices for general convex bodies,\nwhich we believe is of independent interest. Our construction combines the use\nof the M-ellipsoid from convex geometry with lattice sparsification and\ndensification techniques.\n  As an application, we give a 2^O(n)(1+1/eps)^n time and poly(n)-space\ndeterministic algorithm for computing a (1+eps)^n approximation to the volume\nof a general convex body, which nearly matches the lower bounds for volume\nestimation in the oracle model (the dependence on eps is larger by a factor 2\nin the exponent). This improves on the previous results of Dadush and Vempala\n[PNAS 2013], which gave the above result only for symmetric bodies and achieved\na dependence on eps of (1+log^{5/2}(1/eps)/eps^2)^n.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 14:00:52 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 20:18:42 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 14:35:27 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Dadush", "Daniel", ""]]}, {"id": "1311.6716", "submitter": "Mrinal Kumar", "authors": "Mrinal Kumar, Shubhangi Saraf", "title": "The Limits of Depth Reduction for Arithmetic Formulas: It's all about\n  the top fan-in", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a very exciting and promising method for proving lower\nbounds for arithmetic circuits has been proposed. This method combines the\nmethod of {\\it depth reduction} developed in the works of Agrawal-Vinay [AV08],\nKoiran [Koi12] and Tavenas [Tav13], and the use of the shifted partial\nderivative complexity measure developed in the works of Kayal [Kay12] and Gupta\net al [GKKS13a]. These results inspired a flurry of other beautiful results and\nstrong lower bounds for various classes of arithmetic circuits, in particular a\nrecent work of Kayal et al [KSS13] showing superpolynomial lower bounds for\n{\\it regular} arithmetic formulas via an {\\it improved depth reduction} for\nthese formulas. It was left as an intriguing question if these methods could\nprove superpolynomial lower bounds for general (homogeneous) arithmetic\nformulas, and if so this would indeed be a breakthrough in arithmetic circuit\ncomplexity.\n  In this paper we study the power and limitations of depth reduction and\nshifted partial derivatives for arithmetic formulas. We do it via studying the\nclass of depth 4 homogeneous arithmetic circuits. We show: (1) the first {\\it\nsuperpolynomial lower bounds} for the class of homogeneous depth 4 circuits\nwith top fan-in $o(\\log n)$. The core of our result is to show {\\it improved\ndepth reduction} for these circuits. (2) We show that improved depth reduction\n{\\it is not possible} when the top fan-in is $\\Omega(\\log n)$. In particular\nthis shows that the depth reduction procedure of Koiran and Tavenas [Koi12,\nTav13] cannot be improved even for homogeneous formulas, thus strengthening the\nresults of Fournier et al [FLMS13] who showed that depth reduction is tight for\ncircuits, and answering some of the main open questions of [KSS13, FLMS13].\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 15:58:05 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Kumar", "Mrinal", ""], ["Saraf", "Shubhangi", ""]]}, {"id": "1311.7084", "submitter": "Venkatesan Guruswami", "authors": "Venkatesan Guruswami, Carol Wang", "title": "Explicit rank-metric codes list-decodable with optimal redundancy", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an explicit family of linear rank-metric codes over any field\n${\\mathbb F}_h$ that enables efficient list decoding up to a fraction $\\rho$ of\nerrors in the rank metric with a rate of $1-\\rho-\\epsilon$, for any desired\n$\\rho \\in (0,1)$ and $\\epsilon > 0$. Previously, a Monte Carlo construction of\nsuch codes was known, but this is in fact the first explicit construction of\npositive rate rank-metric codes for list decoding beyond the unique decoding\nradius.\n  Our codes are subcodes of the well-known Gabidulin codes, which encode\nlinearized polynomials of low degree via their values at a collection of\nlinearly independent points. The subcode is picked by restricting the message\npolynomials to an ${\\mathbb F}_h$-subspace that evades the structured subspaces\nover an extension field ${\\mathbb F}_{h^t}$ that arise in the linear-algebraic\nlist decoder for Gabidulin codes due to Guruswami and Xing (STOC'13). This\nsubspace is obtained by combining subspace designs contructed by Guruswami and\nKopparty (FOCS'13) with subspace evasive varieties due to Dvir and Lovett\n(STOC'12).\n  We establish a similar result for subspace codes, which are a collection of\nsubspaces, every pair of which have low-dimensional intersection, and which\nhave received much attention recently in the context of network coding. We also\ngive explicit subcodes of folded Reed-Solomon (RS) codes with small folding\norder that are list-decodable (in the Hamming metric) with optimal redundancy,\nmotivated by the fact that list decoding RS codes reduces to list decoding such\nfolded RS codes. However, as we only list decode a subcode of these codes, the\nJohnson radius continues to be the best known error fraction for list decoding\nRS codes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 19:35:46 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 18:47:08 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Wang", "Carol", ""]]}, {"id": "1311.7105", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "Deterministic Approximate Counting for Degree-$2$ Polynomial Threshold\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a {\\em deterministic} algorithm for approximately computing the\nfraction of Boolean assignments that satisfy a degree-$2$ polynomial threshold\nfunction. Given a degree-2 input polynomial $p(x_1,\\dots,x_n)$ and a parameter\n$\\eps > 0$, the algorithm approximates \\[ \\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq 0]\n\\] to within an additive $\\pm \\eps$ in time $\\poly(n,2^{\\poly(1/\\eps)})$. Note\nthat it is NP-hard to determine whether the above probability is nonzero, so\nany sort of multiplicative approximation is almost certainly impossible even\nfor efficient randomized algorithms. This is the first deterministic algorithm\nfor this counting problem in which the running time is polynomial in $n$ for\n$\\eps= o(1)$. For \"regular\" polynomials $p$ (those in which no individual\nvariable's influence is large compared to the sum of all $n$ variable\ninfluences) our algorithm runs in $\\poly(n,1/\\eps)$ time. The algorithm also\nruns in $\\poly(n,1/\\eps)$ time to approximate $\\Pr_{x \\sim N(0,1)^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\eps$, for any degree-2 polynomial $p$.\n  As an application of our counting result, we give a deterministic FPT\nmultiplicative $(1 \\pm \\eps)$-approximation algorithm to approximate the $k$-th\nabsolute moment $\\E_{x \\sim \\{-1,1\\}^n}[|p(x)^k|]$ of a degree-2 polynomial.\nThe algorithm's running time is of the form $\\poly(n) \\cdot f(k,1/\\eps)$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 20:13:18 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1311.7115", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "Deterministic Approximate Counting for Juntas of Degree-$2$ Polynomial\n  Threshold Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $g: \\{-1,1\\}^k \\to \\{-1,1\\}$ be any Boolean function and $q_1,\\dots,q_k$\nbe any degree-2 polynomials over $\\{-1,1\\}^n.$ We give a \\emph{deterministic}\nalgorithm which, given as input explicit descriptions of $g,q_1,\\dots,q_k$ and\nan accuracy parameter $\\eps>0$, approximates \\[\\Pr_{x \\sim\n\\{-1,1\\}^n}[g(\\sign(q_1(x)),\\dots,\\sign(q_k(x)))=1]\\] to within an additive\n$\\pm \\eps$. For any constant $\\eps > 0$ and $k \\geq 1$ the running time of our\nalgorithm is a fixed polynomial in $n$. This is the first fixed polynomial-time\nalgorithm that can deterministically approximately count satisfying assignments\nof a natural class of depth-3 Boolean circuits.\n  Our algorithm extends a recent result \\cite{DDS13:deg2count} which gave a\ndeterministic approximate counting algorithm for a single degree-2 polynomial\nthreshold function $\\sign(q(x)),$ corresponding to the $k=1$ case of our\nresult.\n  Our algorithm and analysis requires several novel technical ingredients that\ngo significantly beyond the tools required to handle the $k=1$ case in\n\\cite{DDS13:deg2count}. One of these is a new multidimensional central limit\ntheorem for degree-2 polynomials in Gaussian random variables which builds on\nrecent Malliavin-calculus-based results from probability theory. We use this\nCLT as the basis of a new decomposition technique for $k$-tuples of degree-2\nGaussian polynomials and thus obtain an efficient deterministic approximate\ncounting algorithm for the Gaussian distribution. Finally, a third new\ningredient is a \"regularity lemma\" for \\emph{$k$-tuples} of degree-$d$\npolynomial threshold functions. This generalizes both the regularity lemmas of\n\\cite{DSTW:10,HKM:09} and the regularity lemma of Gopalan et al \\cite{GOWZ10}.\nOur new regularity lemma lets us extend our deterministic approximate counting\nresults from the Gaussian to the Boolean domain.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 20:33:33 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1311.7178", "submitter": "Anindya De", "authors": "Anindya De, Rocco Servedio", "title": "Efficient deterministic approximate counting for low-degree polynomial\n  threshold functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic algorithm for approximately counting satisfying\nassignments of a degree-$d$ polynomial threshold function (PTF). Given a\ndegree-$d$ input polynomial $p(x_1,\\dots,x_n)$ over $R^n$ and a parameter\n$\\epsilon> 0$, our algorithm approximates $\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\epsilon$ in time $O_{d,\\epsilon}(1)\\cdot\n\\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is\nimpossible even for randomized algorithms assuming $NP\\not=RP$.) Note that the\nrunning time of our algorithm (as a function of $n^d$, the number of\ncoefficients of a degree-$d$ PTF) is a \\emph{fixed} polynomial. The fastest\nprevious algorithm for this problem (due to Kane), based on constructions of\nunconditional pseudorandom generators for degree-$d$ PTFs, runs in time\n$n^{O_{d,c}(1) \\cdot \\epsilon^{-c}}$ for all $c > 0$.\n  The key novel contributions of this work are: A new multivariate central\nlimit theorem, proved using tools from Malliavin calculus and Stein's Method.\nThis new CLT shows that any collection of Gaussian polynomials with small\neigenvalues must have a joint distribution which is very close to a\nmultidimensional Gaussian distribution. A new decomposition of low-degree\nmultilinear polynomials over Gaussian inputs. Roughly speaking we show that (up\nto some small error) any such polynomial can be decomposed into a bounded\nnumber of multilinear polynomials all of which have extremely small\neigenvalues. We use these new ingredients to give a deterministic algorithm for\na Gaussian-space version of the approximate counting problem, and then employ\nstandard techniques for working with low-degree PTFs (invariance principles and\nregularity lemmas) to reduce the original approximate counting problem over the\nBoolean hypercube to the Gaussian version.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 00:00:59 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["De", "Anindya", ""], ["Servedio", "Rocco", ""]]}, {"id": "1311.7229", "submitter": "Friedhelm Meyer auf der Heide", "authors": "Friedhelm Meyer auf der Heide, Kamil Swierkot", "title": "Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity theory for the local distributed setting introduced\nby Korman, Peleg and Fraigniaud. They have defined three complexity classes LD\n(Local Decision), NLD (Nondeterministic Local Decision) and NLD^#n. The class\nLD consists of all languages which can be decided with a constant number of\ncommunication rounds. The class NLD consists of all languages which can be\nverified by a nondeterministic algorithm with a constant number of\ncommunication rounds. In order to define the nondeterministic classes, they\nhave transferred the notation of nondeterminism into the distributed setting by\nthe use of certificates and verifiers. The class NLD^#n consists of all\nlanguages which can be verified by a nondeterministic algorithm where each node\nhas access to an oracle for the number of nodes. They have shown the hierarchy\nLD subset NLD subset NLD^#n.\n  Our main contributions are strict hierarchies within the classes defined by\nKorman, Peleg and Fraigniaud. We define additional complexity classes: the\nclass LD(t) consists of all languages which can be decided with at most t\ncommunication rounds. The class NLD-O(f) consists of all languages which can be\nverified by a local verifier such that the size of the certificates that are\nneeded to verify the language are bounded by a function from O(f). Our main\nresults are refined strict hierarchies within these nondeterministic classes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 07:29:30 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["der Heide", "Friedhelm Meyer auf", ""], ["Swierkot", "Kamil", ""]]}, {"id": "1311.7278", "submitter": "Bruno Bauwens", "authors": "Bruno Bauwens, Marius Zimand", "title": "Linear list-approximation for short programs (or the power of a few\n  random bits)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $c$-short program for a string $x$ is a description of $x$ of length at\nmost $C(x) + c$, where $C(x)$ is the Kolmogorov complexity of $x$. We show that\nthere exists a randomized algorithm that constructs a list of $n$ elements that\ncontains a $O(\\log n)$-short program for $x$. We also show a polynomial-time\nrandomized construction that achieves the same list size for $O(\\log^2\nn)$-short programs. These results beat the lower bounds shown by Bauwens et al.\n\\cite{bmvz:c:shortlist} for deterministic constructions of such lists. We also\nprove tight lower bounds for the main parameters of our result. The\nconstructions use only $O(\\log n)$ ($O(\\log^2 n)$ for the polynomial-time\nresult) random bits . Thus using only few random bits it is possible to do\ntasks that cannot be done by any deterministic algorithm regardless of its\nrunning time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 11:28:31 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 19:35:35 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Bauwens", "Bruno", ""], ["Zimand", "Marius", ""]]}, {"id": "1311.7369", "submitter": "Christian Lavault", "authors": "Christian Lavault (LIPN), Sidi Mohamed Sedjelmaci (LIPN)", "title": "Worst-Case Analysis of Weber's Algorithm", "comments": "11 pages", "journal-ref": "Information Processing Letters 72, 3-4 (1999) 125-130", "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Ken Weber introduced an algorithm for finding the $(a,b)$-pairs\nsatisfying $au+bv\\equiv 0\\pmod{k}$, with $0<|a|,|b|<\\sqrt{k}$, where $(u,k)$\nand $(v,k)$ are coprime. It is based on Sorenson's and Jebelean's \"$k$-ary\nreduction\" algorithms. We provide a formula for $N(k)$, the maximal number of\niterations in the loop of Weber's GCD algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 16:52:43 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Lavault", "Christian", "", "LIPN"], ["Sedjelmaci", "Sidi Mohamed", "", "LIPN"]]}, {"id": "1311.7407", "submitter": "Prahladh Harsha", "authors": "Venkatesan Guruswami, Johan Hastad, Prahladh Harsha, Srikanth\n  Srinivasan, Girish Varma", "title": "Super-polylogarithmic hypergraph coloring hardness via low-degree long\n  codes", "comments": "25 pages", "journal-ref": "SIAM Journal of Computing, 46(1):132-159, 2017", "doi": "10.1137/140995520", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove improved inapproximability results for hypergraph coloring using the\nlow-degree polynomial code (aka, the 'short code' of Barak et. al. [FOCS 2012])\nand the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate\nthis code for inapproximability results. In particular, we prove\nquasi-NP-hardness of the following problems on $n$-vertex hyper-graphs:\n  * Coloring a 2-colorable 8-uniform hypergraph with\n$2^{2^{\\Omega(\\sqrt{\\log\\log n})}}$ colors.\n  * Coloring a 4-colorable 4-uniform hypergraph with\n$2^{2^{\\Omega(\\sqrt{\\log\\log n})}}$ colors.\n  * Coloring a 3-colorable 3-uniform hypergraph with $(\\log\nn)^{\\Omega(1/\\log\\log\\log n)}$ colors.\n  In each of these cases, the hardness results obtained are (at least)\nexponentially stronger than what was previously known for the respective cases.\nIn fact, prior to this result, polylog n colors was the strongest quantitative\nbound on the number of colors ruled out by inapproximability results for\nO(1)-colorable hypergraphs.\n  The fundamental bottleneck in obtaining coloring inapproximability results\nusing the low- degree long code was a multipartite structural restriction in\nthe PCP construction of Dinur-Guruswami. We are able to get around this\nrestriction by simulating the multipartite structure implicitly by querying\njust one partition (albeit requiring 8 queries), which yields our result for\n2-colorable 8-uniform hypergraphs. The result for 4-colorable 4-uniform\nhypergraphs is obtained via a 'query doubling' method. For 3-colorable\n3-uniform hypergraphs, we exploit the ternary domain to design a test with an\nadditive (as opposed to multiplicative) noise function, and analyze its\nefficacy in killing high weight Fourier coefficients via the pseudorandom\nproperties of an associated quadratic form.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 19:44:53 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Hastad", "Johan", ""], ["Harsha", "Prahladh", ""], ["Srinivasan", "Srikanth", ""], ["Varma", "Girish", ""]]}, {"id": "1311.7523", "submitter": "Mark Kambites", "authors": "Mark Kambites and Alexandr Kazda", "title": "The word problem for free adequate semigroups", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.CC math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of computation in finitely generated free left, right\nand two-sided adequate semigroups and monoids. We present polynomial time\n(quadratic in the RAM model of computation) algorithms to solve the word\nproblem and compute normal forms in each of these, and hence also to test\nwhether any given identity holds in the classes of left, right and/or two-sided\nadequate semigroups.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 11:23:06 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Kambites", "Mark", ""], ["Kazda", "Alexandr", ""]]}, {"id": "1311.7685", "submitter": "Robin Kothari", "authors": "Robin Kothari", "title": "An optimal quantum algorithm for the oracle identification problem", "comments": "16 pages; v2: minor changes", "journal-ref": "Proceedings of the 31st International Symposium on Theoretical\n  Aspects of Computer Science (STACS 2014), Leibniz International Proceedings\n  in Informatics 25, pp. 482-493 (2014)", "doi": "10.4230/LIPIcs.STACS.2014.482", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the oracle identification problem, we are given oracle access to an\nunknown N-bit string x promised to belong to a known set C of size M and our\ntask is to identify x. We present a quantum algorithm for the problem that is\noptimal in its dependence on N and M. Our algorithm considerably simplifies and\nimproves the previous best algorithm due to Ambainis et al. Our algorithm also\nhas applications in quantum learning theory, where it improves the complexity\nof exact learning with membership queries, resolving a conjecture of Hunziker\net al.\n  The algorithm is based on ideas from classical learning theory and a new\ncomposition theorem for solutions of the filtered $\\gamma_2$-norm semidefinite\nprogram, which characterizes quantum query complexity. Our composition theorem\nis quite general and allows us to compose quantum algorithms with\ninput-dependent query complexities without incurring a logarithmic overhead for\nerror reduction. As an application of the composition theorem, we remove all\nlog factors from the best known quantum algorithm for Boolean matrix\nmultiplication.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 20:25:28 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 18:38:40 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Kothari", "Robin", ""]]}]