[{"id": "1203.0113", "submitter": "Tianrong Lin", "authors": "Tianrong Lin", "title": "On equivalence, languages equivalence and minimization of multi-letter\n  and multi-letter measure-many quantum automata", "comments": "30 pages, conclusion section corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We first show that given a $k_1$-letter quantum finite automata\n$\\mathcal{A}_1$ and a $k_2$-letter quantum finite automata $\\mathcal{A}_2$ over\nthe same input alphabet $\\Sigma$, they are equivalent if and only if they are\n$(n_1^2+n_2^2-1)|\\Sigma|^{k-1}+k$-equivalent where $n_1$, $i=1,2$, are the\nnumbers of state in $\\mathcal{A}_i$ respectively, and $k=\\max\\{k_1,k_2\\}$. By\napplying a method, due to the author, used to deal with the equivalence problem\nof {\\it measure many one-way quantum finite automata}, we also show that a\n$k_1$-letter measure many quantum finite automaton $\\mathcal{A}_1$ and a\n$k_2$-letter measure many quantum finite automaton $\\mathcal{A}_2$ are\nequivalent if and only if they are $(n_1^2+n_2^2-1)|\\Sigma|^{k-1}+k$-equivalent\nwhere $n_i$, $i=1,2$, are the numbers of state in $\\mathcal{A}_i$ respectively,\nand $k=\\max\\{k_1,k_2\\}$.\n  Next, we study the language equivalence problem of those two kinds of quantum\nfinite automata. We show that for $k$-letter quantum finite automata, the\nnon-strict cut-point language equivalence problem is undecidable, i.e., it is\nundecidable whether\n$L_{\\geq\\lambda}(\\mathcal{A}_1)=L_{\\geq\\lambda}(\\mathcal{A}_2)$ where\n$0<\\lambda\\leq 1$ and $\\mathcal{A}_i$ are $k_i$-letter quantum finite automata.\nFurther, we show that both strict and non-strict cut-point language equivalence\nproblem for $k$-letter measure many quantum finite automata are undecidable.\nThe direct consequences of the above outcomes are summarized in the paper.\n  Finally, we comment on existing proofs about the minimization problem of one\nway quantum finite automata not only because we have been showing great\ninterest in this kind of problem, which is very important in classical automata\ntheory, but also due to that the problem itself, personally, is a challenge.\nThis problem actually remains open.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 08:14:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 13:54:28 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 04:31:42 GMT"}, {"version": "v4", "created": "Fri, 13 Jun 2014 19:18:55 GMT"}, {"version": "v5", "created": "Mon, 16 Jun 2014 08:57:10 GMT"}, {"version": "v6", "created": "Sun, 17 Aug 2014 12:33:00 GMT"}, {"version": "v7", "created": "Sat, 14 Mar 2015 17:35:35 GMT"}, {"version": "v8", "created": "Wed, 10 Jun 2020 13:49:44 GMT"}, {"version": "v9", "created": "Fri, 26 Jun 2020 15:48:16 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Lin", "Tianrong", ""]]}, {"id": "1203.0224", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Guy Kortsarz and Ran Raz", "title": "Label Cover instances with large girth and the hardness of approximating\n  basic k-spanner", "comments": "16 pages, revised to add a reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the well-known Label Cover problem under the additional requirement\nthat problem instances have large girth. We show that if the girth is some $k$,\nthe problem is roughly $2^{\\log^{1-\\epsilon} n/k}$ hard to approximate for all\nconstant $\\epsilon > 0$. A similar theorem was claimed by Elkin and Peleg\n[ICALP 2000], but their proof was later found to have a fundamental error. We\nuse the new proof to show inapproximability for the basic $k$-spanner problem,\nwhich is both the simplest problem in graph spanners and one of the few for\nwhich super-logarithmic hardness was not known. Assuming $NP \\not\\subseteq\nBPTIME(2^{polylog(n)})$, we show that for every $k \\geq 3$ and every constant\n$\\epsilon > 0$ it is hard to approximate the basic $k$-spanner problem within a\nfactor better than $2^{(\\log^{1-\\epsilon} n) / k}$ (for large enough $n$). A\nsimilar hardness for basic $k$-spanner was claimed by Elkin and Peleg [ICALP\n2000], but the error in their analysis of Label Cover made this proof fail as\nwell. Thus for the problem of Label Cover with large girth we give the first\nnon-trivial lower bound. For the basic $k$-spanner problem we improve the\nprevious best lower bound of $\\Omega(\\log n)/k$ by Kortsarz [Algorithmica\n1998]. Our main technique is subsampling the edges of 2-query PCPs, which\nallows us to reduce the degree of a PCP to be essentially equal to the\nsoundness desired. This turns out to be enough to essentially guarantee large\ngirth.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 15:58:01 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 13:24:26 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Dinitz", "Michael", ""], ["Kortsarz", "Guy", ""], ["Raz", "Ran", ""]]}, {"id": "1203.0411", "submitter": "Joerg Rothe", "authors": "Edith Hemaspaandra, Lane A. Hemaspaandra, and Joerg Rothe", "title": "The Complexity of Online Voter Control in Sequential Elections", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on voter control, which refers to situations where a chair\nseeks to change the outcome of an election by deleting, adding, or partitioning\nvoters, takes for granted that the chair knows all the voters' preferences and\nthat all votes are cast simultaneously. However, elections are often held\nsequentially and the chair thus knows only the previously cast votes and not\nthe future ones, yet needs to decide instantaneously which control action to\ntake. We introduce a framework that models online voter control in sequential\nelections. We show that the related problems can be much harder than in the\nstandard (non-online) case: For certain election systems, even with efficient\nwinner problems, online control by deleting, adding, or partitioning voters is\nPSPACE-complete, even if there are only two candidates. In addition, we obtain\n(by a new characterization of coNP in terms of weight-bounded alternating\nTuring machines) completeness for coNP in the deleting/adding cases with a\nbounded deletion/addition limit, and we obtain completeness for NP in the\npartition cases with an additional restriction. We also show that for\nplurality, online control by deleting or adding voters is in P, and for\npartitioning voters is coNP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 10:33:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 23:35:33 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hemaspaandra", "Edith", ""], ["Hemaspaandra", "Lane A.", ""], ["Rothe", "Joerg", ""]]}, {"id": "1203.0494", "submitter": "Minseong Kim", "authors": "Minseong Kim", "title": "Inconsistency of the Zermelo-Fraenkel set theory with the axiom of\n  choice and its effects on the computational complexity", "comments": "I thought the paper was withdrawn, but apparently it was not. So it\n  is withdrawn. This paper of course does not make any sense", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exposes a contradiction in the Zermelo-Fraenkel set theory with\nthe axiom of choice (ZFC). While Godel's incompleteness theorems state that a\nconsistent system cannot prove its consistency, they do not eliminate proofs\nusing a stronger system or methods that are outside the scope of the system.\nThe paper shows that the cardinalities of infinite sets are uncontrollable and\ncontradictory. The paper then states that Peano arithmetic, or first-order\narithmetic, is inconsistent if all of the axioms and axiom schema assumed in\nthe ZFC system are taken as being true, showing that ZFC is inconsistent. The\npaper then exposes some consequences that are in the scope of the computational\ncomplexity theory.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 15:28:11 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 21:05:57 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Kim", "Minseong", ""]]}, {"id": "1203.0586", "submitter": "Frank Ruskey", "authors": "Marcel Celaya and Frank Ruskey", "title": "An Undecidable Nested Recurrence Relation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roughly speaking, a recurrence relation is nested if it contains a\nsubexpression of the form ... A(...A(...)...). Many nested recurrence relations\noccur in the literature, and determining their behavior seems to be quite\ndifficult and highly dependent on their initial conditions. A nested recurrence\nrelation A(n) is said to be undecidable if the following problem is\nundecidable: given a finite set of initial conditions for A(n), is the\nrecurrence relation calculable? Here calculable means that for every n >= 0,\neither A(n) is an initial condition or the calculation of A(n) involves only\ninvocations of A on arguments in {0,1,...,n-1}. We show that the recurrence\nrelation A(n) = A(n-4-A(A(n-4)))+4A(A(n-4)) +A(2A(n-4-A(n-2))+A(n-2)). is\nundecidable by showing how it can be used, together with carefully chosen\ninitial conditions, to simulate Post 2-tag systems, a known Turing complete\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 23:15:42 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Celaya", "Marcel", ""], ["Ruskey", "Frank", ""]]}, {"id": "1203.0594", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Learning DNF Expressions from Fourier Spectrum", "comments": "Appears in Conference on Learning Theory (COLT) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction by Valiant in 1984, PAC learning of DNF expressions\nremains one of the central problems in learning theory. We consider this\nproblem in the setting where the underlying distribution is uniform, or more\ngenerally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed\nthat in this setting a DNF expression can be efficiently approximated from its\n\"heavy\" low-degree Fourier coefficients alone. This is in contrast to previous\napproaches where boosting was used and thus Fourier coefficients of the target\nfunction modified by various distributions were needed. This property is\ncrucial for learning of DNF expressions over smoothed product distributions, a\nlearning model introduced by Kalai et al. (2009) and inspired by the seminal\nsmoothed analysis model of Spielman and Teng (2001).\n  We introduce a new approach to learning (or approximating) a polynomial\nthreshold functions which is based on creating a function with range [-1,1]\nthat approximately agrees with the unknown function on low-degree Fourier\ncoefficients. We then describe conditions under which this is sufficient for\nlearning polynomial threshold functions. Our approach yields a new, simple\nalgorithm for approximating any polynomial-size DNF expression from its \"heavy\"\nlow-degree Fourier coefficients alone. Our algorithm greatly simplifies the\nproof of learnability of DNF expressions over smoothed product distributions.\nWe also describe an application of our algorithm to learning monotone DNF\nexpressions over product distributions. Building on the work of Servedio\n(2001), we give an algorithm that runs in time $\\poly((s \\cdot\n\\log{(s/\\eps)})^{\\log{(s/\\eps)}}, n)$, where $s$ is the size of the target DNF\nexpression and $\\eps$ is the accuracy. This improves on $\\poly((s \\cdot\n\\log{(ns/\\eps)})^{\\log{(s/\\eps)} \\cdot \\log{(1/\\eps)}}, n)$ bound of Servedio\n(2001).\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 00:43:08 GMT"}, {"version": "v2", "created": "Fri, 4 May 2012 03:47:56 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 05:14:46 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1203.0631", "submitter": "Dmitry Chistikov", "authors": "Dmitry V. Chistikov", "title": "Checking Tests for Read-Once Functions over Arbitrary Bases", "comments": "Accepted to the 7th International Computer Science Symposium in\n  Russia (CSR 2012), revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function is called read-once over a basis B if it can be expressed\nby a formula over B where no variable appears more than once. A checking test\nfor a read-once function f over B depending on all its variables is a set of\ninput vectors distinguishing f from all other read-once functions of the same\nvariables. We show that every read-once function f over B has a checking test\ncontaining O(n^l) vectors, where n is the number of relevant variables of f and\nl is the largest arity of functions in B. For some functions, this bound cannot\nbe improved by more than a constant factor. The employed technique involves\nreconstructing f from its l-variable projections and provides a stronger form\nof Kuznetsov's classic theorem on read-once representations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 09:02:40 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2012 18:36:01 GMT"}, {"version": "v3", "created": "Mon, 28 May 2012 18:16:35 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Chistikov", "Dmitry V.", ""]]}, {"id": "1203.0833", "submitter": "Daniel Lokshtanov", "authors": "Daniel Lokshtanov and N. S. Narayanaswamy and Venkatesh Raman and M.\n  S. Ramanujan and Saket Saurabh", "title": "Faster Parameterized Algorithms using Linear Programming", "comments": "A preliminary version of this paper appears in the proceedings of\n  STACS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the parameterized complexity of Vertex Cover parameterized by\nthe difference between the size of the optimal solution and the value of the\nlinear programming (LP) relaxation of the problem. By carefully analyzing the\nchange in the LP value in the branching steps, we argue that combining\npreviously known preprocessing rules with the most straightforward branching\nalgorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the\nexcess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$\nfor a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows\nexponentially with $k$. We proceed to show that a more sophisticated branching\nalgorithm achieves a runtime of $O^*(2.3146^k)$.\n  Following this, using known and new reductions, we give $O^*(2.3146^k)$\nalgorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd\nCycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an\n$O^*(1.5214^k)$ algorithm for Ko\\\"nig Vertex Deletion, Vertex Cover Param by\nOCT and Vertex Cover Param by KVD. These algorithms significantly improve the\nbest known bounds for these problems. The most notable improvement is the new\nbound for Odd Cycle Transversal - this is the first algorithm which beats the\ndependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.\nFinally, using our algorithm, we obtain a kernel for the standard\nparameterization of Vertex Cover with at most $2k - c \\log k$ vertices. Our\nkernel is simpler than previously known kernels achieving the same size bound.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 09:00:23 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 20:45:02 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Narayanaswamy", "N. S.", ""], ["Raman", "Venkatesh", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""]]}, {"id": "1203.0841", "submitter": "Farzin Delavar", "authors": "Farzad Didehvar, Mohsen Mansouri, Zahra Taheri", "title": "How much could we cover a set by c.e sets?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How much c.e. sets could cover a given set?\" in this paper we are going to\nanswer this question. Also, in this approach some old concepts come into a new\narrangement. The major goal of this article is to introduce an appropriate\ndefinition for this purpose. Introduction In Computability Theory (Recursion\nTheory) in the first step we wish to recognize the sets which could be\nenumerated by Turing machines (equivalently, algorithms) and in the next step\nwe will compare these sets by some reasonable order (Like Turing degree). Also\nsometimes with some extra information (Oracles) a class of non c.e. sets show\nthe same behavior as c.e. sets (Post hierarchy and related theorems). Here we\ntry another approach: \"Let A be an arbitrary set and we wish to recognize how\nmuch this set might be covered by a c.e. set?\" Although in some sense this\napproach could be seen in some definitions of Recursion Theory, but at the best\nof our knowledge it didn't considered as an approach yet, even though it is\nable to shed a light on some subjects of Computability of sets. Defining this\napproach is not quite straightforward and there are some obstacles to define\nthem. To overcome these difficulties we modify the definitions. We have an\nalternative problem here when we consider recursive sets and not c.e. sets. In\nthis case, the problem would be: \"Let A be an arbitrary set and we wish to know\nthat how much this set might be covered by a recursive Set?\" Here, we try the\nfirst definition and the first problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 09:43:05 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Didehvar", "Farzad", ""], ["Mansouri", "Mohsen", ""], ["Taheri", "Zahra", ""]]}, {"id": "1203.1042", "submitter": "Sandeep Gupta", "authors": "Sandeep Gupta, Chinya Ravishankar", "title": "Lower bounds for Arrangement-based Range-Free Localization in Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colander are location aware entities that collaborate to determine\napproximate location of mobile or static objects when beacons from an object\nare received by all colanders that are within its distance $R$. This model,\nreferred to as arrangement-based localization, does not require distance\nestimation between entities, which has been shown to be highly erroneous in\npractice. Colander are applicable in localization in sensor networks and\ntracking of mobile objects.\n  A set $S \\subset {\\mathbb R}^2$ is an $(R,\\epsilon)$-colander if by placing\nreceivers at the points of $S$, a wireless device with transmission radius $R$\ncan be localized to within a circle of radius $\\epsilon$. We present tight\nupper and lower bounds on the size of $(R,\\epsilon)$-colanders. We measure the\nexpected size of colanders that will form $(R, \\epsilon)$-colanders if they\ndistributed uniformly over the plane.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 21:01:02 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Gupta", "Sandeep", ""], ["Ravishankar", "Chinya", ""]]}, {"id": "1203.1080", "submitter": "Wei Yu", "authors": "Shiteng Chen, Elad Verbin, Wei Yu", "title": "Data Structure Lower Bounds on Random Access to Grammar-Compressed\n  Strings", "comments": "submitted to ICALP 2012, with strengthened results included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of building a static data structure\nthat represents a string s using space close to its compressed size, and allows\nfast access to individual characters of s. This type of structures was\ninvestigated by the recent paper of Bille et al. Let n be the size of a\ncontext-free grammar that derives a unique string s of length L. (Note that L\nmight be exponential in n.) Bille et al. showed a data structure that uses\nspace O(n) and allows to query for the i-th character of s using running time\nO(log L). Their data structure works on a word RAM with a word size of logL\nbits. Here we prove that for such data structures, if the space is poly(n),\nthen the query time must be at least (log L)^{1-\\epsilon}/log S where S is the\nspace used, for any constant eps>0. As a function of n, our lower bound is\n\\Omega(n^{1/2-\\epsilon}). Our proof holds in the cell-probe model with a word\nsize of log L bits, so in particular it holds in the word RAM model. We show\nthat no lower bound significantly better than n^{1/2-\\epsilon} can be achieved\nin the cell-probe model, since there is a data structure in the cell-probe\nmodel that uses O(n) space and achieves O(\\sqrt{n log n}) query time. The \"bad\"\nsetting of parameters occurs roughly when L=2^{\\sqrt{n}}. We also prove a lower\nbound for the case of not-as-compressible strings, where, say,\nL=n^{1+\\epsilon}. For this case, we prove that if the space is n polylog(n),\nthen the query time must be at least \\Omega(log n/loglog n).\n  The proof works by reduction to communication complexity, namely to the LSD\nproblem, recently employed by Patrascu and others. We prove lower bounds also\nfor the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of\nour lower bounds hold even when the strings are over an alphabet of size 2 and\nhold even for randomized data structures with 2-sided error.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 00:52:14 GMT"}, {"version": "v2", "created": "Thu, 3 May 2012 15:45:08 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Chen", "Shiteng", ""], ["Verbin", "Elad", ""], ["Yu", "Wei", ""]]}, {"id": "1203.1153", "submitter": "Zhaohui Wei", "authors": "Rahul Jain, Yaoyun Shi, Zhaohui Wei, Shengyu Zhang", "title": "Correlation/Communication complexity of generating bipartite states", "comments": "12 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the correlation complexity (or equivalently, the communication\ncomplexity) of generating a bipartite quantum state $\\rho$. When $\\rho$ is a\npure state, we completely characterize the complexity for approximately\ngenerating $\\rho$ by a corresponding approximate rank, closing a gap left in\nAmbainis, Schulman, Ta-Shma, Vazirani and Wigderson (SIAM Journal on Computing,\n32(6):1570-1585, 2003). When $\\rho$ is a classical distribution $P(x,y)$, we\ntightly characterize the complexity of generating $P$ by the psd-rank, a\nmeasure recently proposed by Fiorini, Massar, Pokutta, Tiwary and de Wolf (STOC\n2012). We also present a characterization of the complexity of generating a\ngeneral quantum state $\\rho$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 10:09:37 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Jain", "Rahul", ""], ["Shi", "Yaoyun", ""], ["Wei", "Zhaohui", ""], ["Zhang", "Shengyu", ""]]}, {"id": "1203.1335", "submitter": "Ilir \\c{C}apuni", "authors": "Ilir Capuni and Peter Gacs", "title": "A Turing Machine Resisting Isolated Bursts Of Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider computations of a Turing machine under noise that causes\nconsecutive violations of the machine's transition function. Given a constant\nupper bound B on the size of bursts of faults, we construct a Turing machine\nM(B) subject to faults that can simulate any fault-free machine under the\ncondition that bursts are not closer to each other than V for an appropriate V\n= O(B^2).\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 21:22:00 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Capuni", "Ilir", ""], ["Gacs", "Peter", ""]]}, {"id": "1203.1633", "submitter": "Nathaniel Johnston", "authors": "Nathaniel Johnston", "title": "The Complexity of the Puzzles of Final Fantasy XIII-2", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational complexity of solving the three \"temporal rift\"\npuzzles in the recent popular video game Final Fantasy XIII-2. We show that the\nTile Trial puzzle is NP-hard and we provide an efficient algorithm for solving\nthe Crystal Bonds puzzle. We also show that slight generalizations of the\nCrystal Bonds and Hands of Time puzzles are NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 21:05:24 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Johnston", "Nathaniel", ""]]}, {"id": "1203.1754", "submitter": "Marek Cygan", "authors": "Marek Cygan and Marcin Pilipczuk and Micha{\\l} Pilipczuk", "title": "Known algorithms for EDGE CLIQUE COVER are probably optimal", "comments": "To appear in SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the EDGE CLIQUE COVER (ECC) problem, given a graph G and an integer k, we\nask whether the edges of G can be covered with k complete subgraphs of G or,\nequivalently, whether G admits an intersection model on k-element universe.\nGramm et al. [JEA 2008] have shown a set of simple rules that reduce the number\nof vertices of G to 2^k, and no algorithm is known with significantly better\nrunning time bound than a brute-force search on this reduced instance. In this\npaper we show that the approach of Gramm et al. is essentially optimal: we\npresent a polynomial time algorithm that reduces an arbitrary 3-CNF-SAT formula\nwith n variables and m clauses to an equivalent ECC instance (G,k) with k =\nO(log n) and |V(G)| = O(n + m). Consequently, there is no 2^{2^{o(k)}}poly(n)\ntime algorithm for the ECC problem, unless the Exponential Time Hypothesis\nfails. To the best of our knowledge, these are the first results for a natural,\nfixed-parameter tractable problem, and proving that a doubly-exponential\ndependency on the parameter is essentially necessary.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 11:19:09 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 08:51:46 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1203.1792", "submitter": "Xue Wu", "authors": "Xue Wu", "title": "Calculation of the minimum computational complexity based on information\n  entropy", "comments": "10pages, 2 figures, journal", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vo2, No.1, February 2012, pp. 73-82", "doi": "10.5121/ijcsa.2012.2107", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to find out the limiting speed of solving a specific problem using\ncomputer, this essay provides a method based on information entropy. The\nrelationship between the minimum computational complexity and information\nentropy change is illustrated. A few examples are served as evidence of such\nconnection. Meanwhile some basic rules of modeling problems are established.\nFinally, the nature of solving problems with computer programs is disclosed to\nsupport this theory and a redefinition of information entropy in this filed is\nproposed. This will develop a new field of science.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 13:26:14 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Wu", "Xue", ""]]}, {"id": "1203.1876", "submitter": "Michael Pinsker", "authors": "Manuel Bodirsky, Michael Pinsker", "title": "Topological Birkhoff", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental mathematical contributions of Garrett Birkhoff is\nthe HSP theorem, which implies that a finite algebra B satisfies all equations\nthat hold in a finite algebra A of the same signature if and only if B is a\nhomomorphic image of a subalgebra of a finite power of A. On the other hand, if\nA is infinite, then in general one needs to take an infinite power in order to\nobtain a representation of B in terms of A, even if B is finite.\n  We show that by considering the natural topology on the functions of A and B\nin addition to the equations that hold between them, one can do with finite\npowers even for many interesting infinite algebras A. More precisely, we prove\nthat if A and B are at most countable algebras which are oligomorphic, then the\nmapping which sends each function from A to the corresponding function in B\npreserves equations and is continuous if and only if B is a homomorphic image\nof a subalgebra of a finite power of A.\n  Our result has the following consequences in model theory and in theoretical\ncomputer science: two \\omega-categorical structures are primitive positive\nbi-interpretable if and only if their topological polymorphism clones are\nisomorphic. In particular, the complexity of the constraint satisfaction\nproblem of an \\omega-categorical structure only depends on its topological\npolymorphism clone.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 18:26:42 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2012 22:46:17 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Pinsker", "Michael", ""]]}, {"id": "1203.1895", "submitter": "Alan Guo", "authors": "Greg Aloupis, Erik D. Demaine, Alan Guo, Giovanni Viglietta", "title": "Classic Nintendo Games are (Computationally) Hard", "comments": "36 pages, 36 figures. Fixed some typos. Added NP-hardness results\n  (with proofs and figures) for American SMB2 and Zelda 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove NP-hardness results for five of Nintendo's largest video game\nfranchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokemon. Our\nresults apply to generalized versions of Super Mario Bros. 1-3, The Lost\nLevels, and Super Mario World; Donkey Kong Country 1-3; all Legend of Zelda\ngames; all Metroid games; and all Pokemon role-playing games. In addition, we\nprove PSPACE-completeness of the Donkey Kong Country games and several Legend\nof Zelda games.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 19:37:20 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 18:24:15 GMT"}, {"version": "v3", "created": "Sun, 8 Feb 2015 19:45:26 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Aloupis", "Greg", ""], ["Demaine", "Erik D.", ""], ["Guo", "Alan", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1203.2168", "submitter": "Stephen Cook", "authors": "Stephen Cook", "title": "Relativized Propositional Calculus", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof systems for the Relativized Propositional Calculus are defined and\ncompared.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 20:20:06 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Cook", "Stephen", ""]]}, {"id": "1203.2602", "submitter": "Nike Sun", "authors": "Allan Sly and Nike Sun", "title": "The computational hardness of counting in two-spin models on d-regular\n  graphs", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of two-spin systems contains several important models, including\nrandom independent sets and the Ising model of statistical physics. We show\nthat for both the hard-core (independent set) model and the anti-ferromagnetic\nIsing model with arbitrary external field, it is NP-hard to approximate the\npartition function or approximately sample from the model on d-regular graphs\nwhen the model has non-uniqueness on the d-regular tree. Together with results\nof Jerrum--Sinclair, Weitz, and Sinclair--Srivastava--Thurley giving FPRAS's\nfor all other two-spin systems except at the uniqueness threshold, this gives\nan almost complete classification of the computational complexity of two-spin\nsystems on bounded-degree graphs.\n  Our proof establishes that the normalized log-partition function of any\ntwo-spin system on bipartite locally tree-like graphs converges to a limiting\n\"free energy density\" which coincides with the (non-rigorous) Bethe prediction\nof statistical physics. We use this result to characterize the local structure\nof two-spin systems on locally tree-like bipartite expander graphs, which then\nbecome the basic gadgets in a randomized reduction to approximate MAX-CUT. Our\napproach is novel in that it makes no use of the second moment method employed\nin previous works on these questions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 19:53:53 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Sly", "Allan", ""], ["Sun", "Nike", ""]]}, {"id": "1203.2801", "submitter": "Eun Jung Kim", "authors": "Eun Jung Kim and Daniel Goncalves", "title": "On Exact Algorithms for Permutation CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are\ngiven a set of variables $V$ and a set of constraints C, in which constraints\nare tuples of elements of V. The goal is to find a total ordering of the\nvariables, $\\pi\\ : V \\rightarrow [1,...,|V|]$, which satisfies as many\nconstraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an\nordering $\\pi$ when $\\pi(v_1)<\\pi(v_2)<...<\\pi(v_k)$. An instance has arity $k$\nif all the constraints involve at most $k$ elements.\n  This problem expresses a variety of permutation problems including {\\sc\nFeedback Arc Set} and {\\sc Betweenness} problems. A naive algorithm, listing\nall the $n!$ permutations, requires $2^{O(n\\log{n})}$ time. Interestingly, {\\sc\nPermutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in\ntime $O^*(2^n)$, but no algorithm is known for arity at least 4 with running\ntime significantly better than $2^{O(n\\log{n})}$. In this paper we resolve the\ngap by showing that {\\sc Arity 4 Permutation CSP} cannot be solved in time\n$2^{o(n\\log{n})}$ unless ETH fails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 13:38:45 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Kim", "Eun Jung", ""], ["Goncalves", "Daniel", ""]]}, {"id": "1203.2888", "submitter": "Joshua Grochow", "authors": "Joshua A. Grochow and Korben Rusek", "title": "Report on \"Mathematical Aspects of P vs. NP and its Variants.\"", "comments": "Modified Open Problem 10 and a few small edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG math.NT math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a report on a workshop held August 1 to August 5, 2011 at the\nInstitute for Computational and Experimental Research in Mathematics (ICERM) at\nBrown University, Providence, Rhode Island, organized by Saugata Basu, Joseph\nM. Landsberg, and J. Maurice Rojas. We provide overviews of the more recent\nresults presented at the workshop, including some works-in-progress as well as\ntentative and intriguing ideas for new directions. The main themes we discuss\nare representation theory and geometry in the Mulmuley-Sohoni Geometric\nComplexity Theory Program, and number theory and other ideas in the\nBlum-Shub-Smale model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 18:42:15 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2012 18:30:18 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Rusek", "Korben", ""]]}, {"id": "1203.3249", "submitter": "U\\'everton dos Santos Souza", "authors": "Maise Dantas da Silva and F\\'abio Protti and U\\'everton dos Santos\n  Souza", "title": "Revisiting the Complexity of And/Or Graph Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on two data structures that have been used to\nmodel several problems in computer science: and/or graphs and x-y graphs. An\nand/or graph is an acyclic digraph containing a source, such that every vertex\nv has a label f(v) \\in {and,or} and edges represent dependency relations\nbetween vertices: a vertex labeled and depends on all of its out-neighbors,\nwhile a vertex labeled or depends on only one of its out-neighbors. X-y graphs\nare defined as a natural generalization of and/or graphs: every vertex vi of an\nx-y graph has a label xi-yi to mean that vi depends on xi of its yi\nout-neighbors. We analyze the complexity of the optimization problems\nMin-and/or and Min-x-y, which consist of finding solution subgraphs of optimal\nweight for and/or and x-y graphs, respectively. Motivated by the large\napplicability as well as the hardness of Min-and/or and Min-x-y, we study new\ncomplexity aspects of such problems, both from a classical and a parameterized\npoint of view. We prove that Min-and/or remains NP-hard even for a very\nrestricted family of and/or graphs where edges have weight one and or-vertices\nhave out-degree at most two (apart from other property related to some\nin-degrees), and that deciding whether there is a solution subtree with weight\nexactly k of a given x-y tree is also NP-hard. We also show that: (i) the\nparameterized problem Min-and/or(k, r), which asks whether there is a solution\nsubgraph of weight at most k where every or-vertex has at most r out-edges with\nthe same weight, is FPT; (ii) the parameterized problem Min-and/or0(k), whose\ndomain includes and/or graphs allowing zero-weight edges, is W[2]-hard; (iii)\nthe parameterized problem Min-x-y(k) is W[1]-hard.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 01:16:32 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["da Silva", "Maise Dantas", ""], ["Protti", "F\u00e1bio", ""], ["Souza", "U\u00e9verton dos Santos", ""]]}, {"id": "1203.3298", "submitter": "Yaroslav Sergeyev", "authors": "Yaroslav D. Sergeyev, Alfredo Garro", "title": "Observability of Turing Machines: a Refinement of the Theory of\n  Computation", "comments": "31 pages, 1 figure", "journal-ref": "Informatica, 2010, 21(3), 425-454", "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turing machine is one of the simple abstract computational devices that\ncan be used to investigate the limits of computability. In this paper, they are\nconsidered from several points of view that emphasize the importance and the\nrelativity of mathematical languages used to describe the Turing machines. A\ndeep investigation is performed on the interrelations between mechanical\ncomputations and their mathematical descriptions emerging when a human (the\nresearcher) starts to describe a Turing machine (the object of the study) by\ndifferent mathematical languages (the instruments of investigation). Together\nwith traditional mathematical languages using such concepts as 'enumerable\nsets' and 'continuum' a new computational methodology allowing one to measure\nthe number of elements of different infinite sets is used in this paper. It is\nshown how mathematical languages used to describe the machines limit our\npossibilities to observe them. In particular, notions of observable\ndeterministic and non-deterministic Turing machines are introduced and\nconditions ensuring that the latter can be simulated by the former are\nestablished.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 09:55:28 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Sergeyev", "Yaroslav D.", ""], ["Garro", "Alfredo", ""]]}, {"id": "1203.3368", "submitter": "Dvir Falik", "authors": "Dvir Falik and Ehud Friedgut", "title": "Between Arrow and Gibbard-Satterthwaite; A representation theoretic\n  approach", "comments": "First appeared in FOCS'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.GT math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central theme in social choice theory is that of impossibility theorems,\nsuch as Arrow's theorem and the Gibbard-Satterthwaite theorem, which state that\nunder certain natural constraints, social choice mechanisms are impossible to\nconstruct. In recent years, beginning in Kalai`01, much work has been done in\nfinding \\textit{robust} versions of these theorems, showing \"approximate\"\nimpossibility remains even when most, but not all, of the constraints are\nsatisfied. We study a spectrum of settings between the case where society\nchooses a single outcome (\\'a-la-Gibbard-Satterthwaite) and the choice of a\ncomplete order (as in Arrow's theorem). We use algebraic techniques,\nspecifically representation theory of the symmetric group, and also prove\nrobust versions of the theorems that we state. Our relaxations of the\nconstraints involve relaxing of a version of \"independence of irrelevant\nalternatives\", rather than relaxing the demand of a transitive outcome, as is\ndone in most other robustness results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 11:11:36 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Falik", "Dvir", ""], ["Friedgut", "Ehud", ""]]}, {"id": "1203.3674", "submitter": "Daniil Musatov", "authors": "Daniil Musatov", "title": "Space-Bounded Kolmogorov Extractors", "comments": "12 pages, accepted to CSR2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An extractor is a function that receives some randomness and either\n\"improves\" it or produces \"new\" randomness. There are statistical and\nalgorithmical specifications of this notion. We study an algorithmical one\ncalled Kolmogorov extractors and modify it to resource-bounded version of\nKolmogorov complexity. Following Zimand we prove the existence of such objects\nwith certain parameters. The utilized technique is \"naive\" derandomization: we\nreplace random constructions employed by Zimand by pseudo-random ones obtained\nby Nisan-Wigderson generator.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 11:41:35 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Musatov", "Daniil", ""]]}, {"id": "1203.3967", "submitter": "Lena Schend", "authors": "Joerg Rothe and Lena Schend", "title": "Control Complexity in Bucklin, Fallback, and Plurality Voting: An\n  Experimental Approach", "comments": "Complete version with all results for plurality voting, 370 pages,\n  numerous figures, a short version appeared in the proceedings of the 11th\n  International Symposium on Experimental Algorithms (SEA-2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walsh [Wal10, Wal09], Davies et al. [DKNW10, DKNW11], and Narodytska et al.\n[NWX11] studied various voting systems empirically and showed that they can\noften be manipulated effectively, despite their manipulation problems being\nNP-hard. Such an experimental approach is sorely missing for NP-hard control\nproblems, where control refers to attempts to tamper with the outcome of\nelections by adding/deleting/partitioning either voters or candidates. We\nexperimentally tackle NP-hard control problems for Bucklin and fallback voting.\nAmong natural voting systems with efficient winner determination, fallback\nvoting is currently known to display the broadest resistance to control in\nterms of NP-hardness, and Bucklin voting has been shown to behave almost as\nwell in terms of control resistance [ER10, EPR11, EFPR11]. We also investigate\ncontrol resistance experimentally for plurality voting, one of the first voting\nsystems analyzed with respect to electoral control [BTT92, HHR07]. Our findings\nindicate that NP-hard control problems can often be solved effectively in\npractice. Moreover, our experiments allow a more fine-grained analysis and\ncomparison-across various control scenarios, vote distribution models, and\nvoting systems-than merely stating NP-hardness for all these control problems.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2012 16:10:02 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 13:00:30 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rothe", "Joerg", ""], ["Schend", "Lena", ""]]}, {"id": "1203.4063", "submitter": "Jesper Nederlof", "authors": "Petteri Kaski, Mikko Koivisto, Jesper Nederlof", "title": "Homomorphic Hashing for Sparse Coefficient Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classes of Dynamic Programming (DP) algorithms which, due to their\nalgebraic definitions, are closely related to coefficient extraction methods.\nDP algorithms can easily be modified to exploit sparseness in the DP table\nthrough memorization. Coefficient extraction techniques on the other hand are\nboth space-efficient and parallelisable, but no tools have been available to\nexploit sparseness.\n  We investigate the systematic use of homomorphic hash functions to combine\nthe best of these methods and obtain improved space-efficient algorithms for\nproblems including LINEAR SAT, SET PARTITION, and SUBSET SUM. Our algorithms\nrun in time proportional to the number of nonzero entries of the last segment\nof the DP table, which presents a strict improvement over sparse DP. The last\nproperty also gives an improved algorithm for CNF SAT with sparse projections.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 09:30:28 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Kaski", "Petteri", ""], ["Koivisto", "Mikko", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1203.4155", "submitter": "J\\'er\\'emie Roland", "authors": "S. Laplante, V. Lerays and J. Roland", "title": "Classical and quantum partition bound and detector inefficiency", "comments": "21 pages, extended version", "journal-ref": "Lecture Notes in Computer Science, Volume 7391, pages 617-628,\n  2012", "doi": "10.1007/978-3-642-31594-7_52", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized and quantum efficiency lower bounds in communication\ncomplexity. These arise from the study of zero-communication protocols in which\nplayers are allowed to abort. Our scenario is inspired by the physics setup of\nBell experiments, where two players share a predefined entangled state but are\nnot allowed to communicate. Each is given a measurement as input, which they\nperform on their share of the system. The outcomes of the measurements should\nfollow a distribution predicted by quantum mechanics; however, in practice, the\ndetectors may fail to produce an output in some of the runs. The efficiency of\nthe experiment is the probability that the experiment succeeds (neither of the\ndetectors fails).\n  When the players share a quantum state, this gives rise to a new bound on\nquantum communication complexity (eff*) that subsumes the factorization norm.\nWhen players share randomness instead of a quantum state, the efficiency bound\n(eff), coincides with the partition bound of Jain and Klauck. This is one of\nthe strongest lower bounds known for randomized communication complexity, which\nsubsumes all the known combinatorial and algebraic methods including the\nrectangle (corruption) bound, the factorization norm, and discrepancy.\n  The lower bound is formulated as a convex optimization problem. In practice,\nthe dual form is more feasible to use, and we show that it amounts to\nconstructing an explicit Bell inequality (for eff) or Tsirelson inequality (for\neff*). We give an example of a quantum distribution where the violation can be\nexponentially bigger than the previously studied class of normalized Bell\ninequalities.\n  For one-way communication, we show that the quantum one-way partition bound\nis tight for classical communication with shared entanglement up to arbitrarily\nsmall error.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 16:35:45 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 13:24:11 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Laplante", "S.", ""], ["Lerays", "V.", ""], ["Roland", "J.", ""]]}, {"id": "1203.4532", "submitter": "Zeev Dvir", "authors": "Zeev Dvir, J\\'anos Koll\\'ar and Shachar Lovett", "title": "Variety Evasive Sets", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.AG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an explicit construction of a large subset of F^n, where F is a\nfinite field, that has small intersection with any affine variety of fixed\ndimension and bounded degree. Our construction generalizes a recent result of\nDvir and Lovett (STOC 2012) who considered varieties of degree one (affine\nsubspaces).\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 18:09:31 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Dvir", "Zeev", ""], ["Koll\u00e1r", "J\u00e1nos", ""], ["Lovett", "Shachar", ""]]}, {"id": "1203.4667", "submitter": "Amaury Pouly", "authors": "Amaury Pouly, Olivier Bournez, Daniel S. Gra\\c{c}a", "title": "Turing machines can be efficiently simulated by the General Purpose\n  Analog Computer", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-38236-9_16", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Church-Turing thesis states that any sufficiently powerful computational\nmodel which captures the notion of algorithm is computationally equivalent to\nthe Turing machine. This equivalence usually holds both at a computability\nlevel and at a computational complexity level modulo polynomial reductions.\nHowever, the situation is less clear in what concerns models of computation\nusing real numbers, and no analog of the Church-Turing thesis exists for this\ncase. Recently it was shown that some models of computation with real numbers\nwere equivalent from a computability perspective. In particular it was shown\nthat Shannon's General Purpose Analog Computer (GPAC) is equivalent to\nComputable Analysis. However, little is known about what happens at a\ncomputational complexity level. In this paper we shed some light on the\nconnections between this two models, from a computational complexity level, by\nshowing that, modulo polynomial reductions, computations of Turing machines can\nbe simulated by GPACs, without the need of using more (space) resources than\nthose used in the original Turing computation, as long as we are talking about\nbounded computations. In other words, computations done by the GPAC are as\nspace-efficient as computations done in the context of Computable Analysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 07:48:23 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 17:49:53 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Pouly", "Amaury", ""], ["Bournez", "Olivier", ""], ["Gra\u00e7a", "Daniel S.", ""]]}, {"id": "1203.4740", "submitter": "Scott Aaronson", "authors": "Scott Aaronson and Paul Christiano", "title": "Quantum Money from Hidden Subspaces", "comments": "48 pages, minor corrections and improvements, journal version to\n  appear in Theory of Computing; Proceedings of ACM STOC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forty years ago, Wiesner pointed out that quantum mechanics raises the\nstriking possibility of money that cannot be counterfeited according to the\nlaws of physics. We propose the first quantum money scheme that is (1)\npublic-key, meaning that anyone can verify a banknote as genuine, not only the\nbank that printed it, and (2) cryptographically secure, under a \"classical\"\nhardness assumption that has nothing to do with quantum money. Our scheme is\nbased on hidden subspaces, encoded as the zero-sets of random multivariate\npolynomials. A main technical advance is to show that the \"black-box\" version\nof our scheme, where the polynomials are replaced by classical oracles, is\nunconditionally secure. Previously, such a result had only been known relative\nto a quantum oracle (and even there, the proof was never published). Even in\nWiesner's original setting -- quantum money that can only be verified by the\nbank -- we are able to use our techniques to patch a major security hole in\nWiesner's scheme. We give the first private-key quantum money scheme that\nallows unlimited verifications and that remains unconditionally secure, even if\nthe counterfeiter can interact adaptively with the bank. Our money scheme is\nsimpler than previous public-key quantum money schemes, including a knot-based\nscheme of Farhi et al. The verifier needs to perform only two tests, one in the\nstandard basis and one in the Hadamard basis -- matching the original intuition\nfor quantum money, based on the existence of complementary observables. Our\nsecurity proofs use a new variant of Ambainis's quantum adversary method, and\nseveral other tools that might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 14:10:46 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2012 00:35:23 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2012 19:12:17 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Aaronson", "Scott", ""], ["Christiano", "Paul", ""]]}, {"id": "1203.4885", "submitter": "Abel Molina", "authors": "Abel Molina", "title": "Parallel Repetition of Prover-Verifier Quantum Interactions", "comments": "Chapter 5 includes results from arXiv 1104.1140", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we answer several questions about the behaviour of\nprover-verifier interactions under parallel repetition when quantum information\nis allowed, and the verifier acts independently in them.\n  We first consider the case in which a value is associated with each of the\npossible outcomes of an interaction. We prove that it is not possible for the\nprover to improve on the optimum average value per repetition by repeating the\nprotocol multiple times in parallel.\n  We look then at games in which the outcomes are classified into two types,\nwinning outcomes and losing outcomes. We ask what is the optimal probability\nfor the prover of winning at least k times out of n parallel repetitions, given\nthat the optimal probability of winning when only one repetition is considered\nis $p$. A reasonable conjecture for the answer would be \\sum_{m \\geq k} {n\n\\choose m} p^m (1-p)^{n-m}, as that is the answer when it is optimal for the\nprover to act independently. This is known to be the correct answer when k=n,\nand also in the classical case. It is also correct in some generalizations of\nthe classical case that we will discuss later. We will show how this cannot be\nextended to all cases, presenting an example of an interaction with k=1,n=2 in\nwhich p\\approx 0.85, but it is possible to always win at least once. We will\nthen give some upper bounds on the optimal probability for the prover of\nwinning k times out of n parallel repetitions. These bounds are expressed as a\nfunction of p.\n  Finally, we will connect our results to the study of error reduction for\nquantum interactive proofs using parallel repetition.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 04:53:59 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Molina", "Abel", ""]]}, {"id": "1203.5323", "submitter": "Barnaby Martin", "authors": "Barnaby Martin", "title": "Parameterized Proof Complexity and W[1]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a program of parameterized proof complexity that aims to provide\nevidence that FPT is different from W[1]. A similar program already exists for\nthe classes W[2] and W[SAT]. We contrast these programs and prove upper and\nlower bounds for W[1]-parameterized Resolution.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 19:10:20 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Martin", "Barnaby", ""]]}, {"id": "1203.5414", "submitter": "Stasys Jukna", "authors": "S. Jukna", "title": "Clique problem, cutting plane proofs and communication complexity", "comments": "10 pages. Theorem 1 in the previous version holds only for bipartite\n  graphs, the non-bipartite case remains open. I now separate the bipartite and\n  non-bipartite cases (by switching from independent sets to cliques, hence a\n  new title). Some new open problems as well as references are added", "journal-ref": "Information Processing Letters 112(20) (2012) 772-777", "doi": "10.1016/j.ipl.2012.07.003", "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by its relation to the length of cutting plane proofs for the\nMaximum Biclique problem, we consider the following communication game on a\ngiven graph G, known to both players. Let K be the maximal number of vertices\nin a complete bipartite subgraph of G, which is not necessarily an induced\nsubgraph if G is not bipartite. Alice gets a set A of vertices, and Bob gets a\ndisjoint set B of vertices such that |A|+|B|>K. The goal is to find a nonedge\nof G between A and B. We show that O(\\log n) bits of communication are enough\nfor every n-vertex graph.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 13:51:15 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2012 15:20:40 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Jukna", "S.", ""]]}, {"id": "1203.5423", "submitter": "EPTCS", "authors": "Simona Ronchi della Rocca (UNITO), Elaine Pimentel (UFMG)", "title": "Proceedings 6th Workshop on Logical and Semantic Frameworks with\n  Applications", "comments": null, "journal-ref": "EPTCS 81, 2012", "doi": "10.4204/EPTCS.81", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Sixth Workshop on Logical and\nSemantic Frameworks with Applications (LSFA 2011). The workshop will be hold in\nBelo Horizonte, on August 27th 2011.\n  Logical and semantic frameworks are formal languages used to represent\nlogics, languages and systems. These frameworks provide foundations for formal\nspecification of systems and programming languages, supporting tool development\nand reasoning.\n  The objective of this one-day workshop is to put together theoreticians and\npractitioners to promote new techniques and results, from the theoretical side,\nand feedback on the implementation and the use of such techniques and results,\nfrom the practical side.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 15:27:21 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["della Rocca", "Simona Ronchi", "", "UNITO"], ["Pimentel", "Elaine", "", "UFMG"]]}, {"id": "1203.5706", "submitter": "Peter Scheiblechner", "authors": "Peter Scheiblechner", "title": "Effective de Rham Cohomology - The General Case", "comments": "36 pages", "journal-ref": null, "doi": "10.1142/S0219199718500670", "report-no": null, "categories": "math.AG cs.CC math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grothendieck has proved that each class in the de Rham cohomology of a smooth\ncomplex affine variety can be represented by a differential form with\npolynomial coefficients. After having proved a single exponential bound for the\ndegrees of these forms in the case of a hypersurface, here we generalize this\nresult to arbitrary codimension. More precisely, we show that the p-th de Rham\ncohomology of a smooth affine variety of dimension m and degree D can be\nrepresented by differential forms of degree (pD)^{O(pm)}. This result is\nrelevant for the algorithmic computation of the cohomology, but is also\nmotivated by questions in the theory of ordinary differential equations related\nto the infinitesimal Hilbert 16th problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 15:54:20 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 08:12:38 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Scheiblechner", "Peter", ""]]}, {"id": "1203.5715", "submitter": "Carme \\`Alvarez", "authors": "Carme \\`Alvarez and Aleix Fern\\`andez", "title": "Network Formation: Heterogeneous Traffic, Bilateral Contracting and\n  Myopic Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a network formation game where nodes wish to send traffic to other\nnodes. Nodes can contract bilaterally other nodes to form bidirectional links\nas well as nodes can break unilaterally contracts to eliminate the\ncorresponding links. Our model is an extension of the model considered in\nArcaute et al. The novelty is that we do no require the traffic to be uniform\nall-to-all. Each node specifies the amount of traffic that it wants to send to\nany other node. We characterize stable topologies under a static point of view\nand we also study the game under a myopic dynamics. We show its convergence to\nstable networks under some natural assumptions on the contracting functions.\nFinally we consider the efficiency of pairwise Nash topologies from a social\npoint of view and we show that the problem of deciding the existence stable\ntopologies of a given price is $\\NP$-complete.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 16:23:00 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["\u00c0lvarez", "Carme", ""], ["Fern\u00e0ndez", "Aleix", ""]]}, {"id": "1203.5944", "submitter": "Sergio Cabello", "authors": "Sergio Cabello, Bojan Mohar", "title": "Adding one edge to planar graphs makes crossing number and 1-planarity\n  hard", "comments": "27 pages, 10 figures. Part of the results appeared in Proceedings of\n  the 26th Annual Symposium on Computational Geometry (SoCG), 68-76, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph is near-planar if it can be obtained from a planar graph by adding an\nedge. We show the surprising fact that it is NP-hard to compute the crossing\nnumber of near-planar graphs. A graph is 1-planar if it has a drawing where\nevery edge is crossed by at most one other edge. We show that it is NP-hard to\ndecide whether a given near-planar graph is 1-planar. The main idea in both\nreductions is to consider the problem of simultaneously drawing two planar\ngraphs inside a disk, with some of its vertices fixed at the boundary of the\ndisk. This leads to the concept of anchored embedding, which is of independent\ninterest. As an interesting consequence we obtain a new, geometric proof of\nNP-completeness of the crossing number problem, even when restricted to cubic\ngraphs. This resolves a question of Hlin\\v{e}n\\'y.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 12:03:12 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Cabello", "Sergio", ""], ["Mohar", "Bojan", ""]]}, {"id": "1203.6020", "submitter": "Algirdas Maknickas", "authors": "Algirdas Antano Maknickas", "title": "How to solve kSAT in polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  With using of multi-nary logic analytic formulas proposition that \"kSAT is in\nP and could be solved in $O(n^{3.5})$\" was proved\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 15:37:22 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2012 20:37:11 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Maknickas", "Algirdas Antano", ""]]}, {"id": "1203.6161", "submitter": "EPTCS", "authors": "Anderson de Ara\\'ujo (University of S\\~ao Paulo), Marcelo Finger\n  (University of S\\~ao Paulo)", "title": "Classical and quantum satisfiability", "comments": "In Proceedings LSFA 2011, arXiv:1203.5423", "journal-ref": "EPTCS 81, 2012, pp. 79-84", "doi": "10.4204/EPTCS.81.6", "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the linear algebraic definition of QSAT and propose a direct\nlogical characterization of such a definition. We then prove that this logical\nversion of QSAT is not an extension of classical satisfiability problem (SAT).\nThis shows that QSAT does not allow a direct comparison between the complexity\nclasses NP and QMA, for which SAT and QSAT are respectively complete.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 05:06:45 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["de Ara\u00fajo", "Anderson", "", "University of S\u00e3o Paulo"], ["Finger", "Marcelo", "", "University of S\u00e3o Paulo"]]}, {"id": "1203.6559", "submitter": "Michiel de Bondt", "authors": "Michiel de Bondt", "title": "Solving Mahjong Solitaire boards with peeking", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first prove that solving Mahjong Solitaire boards with peeking is\nNP-complete, even if one only allows isolated stacks of the forms /aab/ and\n/abb/. We subsequently show that layouts of isolated stacks of heights one and\ntwo can always be solved with peeking, and that doing so is in P, as well as\nfinding an optimal algorithm for such layouts without peeking.\n  Next, we describe a practical algorithm for solving Mahjong Solitaire boards\nwith peeking, which is simple and fast. The algorithm uses an effective pruning\ncriterion and a heuristic to find and prioritize critical groups. The ideas of\nthe algorithm can also be applied to solving Shisen-Sho with peeking.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 15:34:39 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["de Bondt", "Michiel", ""]]}, {"id": "1203.6878", "submitter": "Romain Pechoux", "authors": "Jean-Yves Marion (INRIA Lorraine - LORIA), Romain P\\'echoux (INRIA\n  Lorraine - LORIA)", "title": "Complexity Information Flow in a Multi-threaded Imperative Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a type system to analyze the time consumed by multi-threaded\nimperative programs with a shared global memory, which delineates a class of\nsafe multi-threaded programs. We demonstrate that a safe multi-threaded program\nruns in polynomial time if (i) it is strongly terminating wrt a\nnon-deterministic scheduling policy or (ii) it terminates wrt a deterministic\nand quiet scheduling policy. As a consequence, we also characterize the set of\npolynomial time functions. The type system presented is based on the\nfundamental notion of data tiering, which is central in implicit computational\ncomplexity. It regulates the information flow in a computation. This aspect is\ninteresting in that the type system bears a resemblance to typed based\ninformation flow analysis and notions of non-interference. As far as we know,\nthis is the first characterization by a type system of polynomial time\nmulti-threaded programs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 18:13:25 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Marion", "Jean-Yves", "", "INRIA Lorraine - LORIA"], ["P\u00e9choux", "Romain", "", "INRIA\n  Lorraine - LORIA"]]}]