[{"id": "1002.0063", "submitter": "Ali Akbar Safilian", "authors": "Ali Akbar Safilian, Farzad Didehvar", "title": "Enumeration Order Reducibility", "comments": "This paper is the second version of our work on Enumeration Orders", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we define a new reducibility based on the enumeration orders\nof r.e. sets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2010 16:46:48 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2010 11:25:51 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Safilian", "Ali Akbar", ""], ["Didehvar", "Farzad", ""]]}, {"id": "1002.0145", "submitter": "Nitin Saxena", "authors": "Nitin Saxena and C. Seshadhri", "title": "From Sylvester-Gallai Configurations to Rank Bounds: Improved Black-box\n  Identity Test for Depth-3 Circuits", "comments": "33 pages, 1 table, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identity testing for depth-3 circuits of top fanin k\nand degree d. We give a new structure theorem for such identities. A direct\napplication of our theorem improves the known deterministic d^{k^k}-time\nblack-box identity test over rationals (Kayal-Saraf, FOCS 2009) to one that\ntakes d^{k^2}-time. Our structure theorem essentially says that the number of\nindependent variables in a real depth-3 identity is very small. This theorem\nsettles affirmatively the stronger rank conjectures posed by Dvir-Shpilka (STOC\n2005) and Kayal-Saraf (FOCS 2009). Our techniques provide a unified framework\nthat actually beats all known rank bounds and hence gives the best running time\n(for every field) for black-box identity tests.\n  Our main theorem (almost optimally) pins down the relation between higher\ndimensional Sylvester-Gallai theorems and the rank of depth-3 identities in a\nvery transparent manner. The existence of this was hinted at by Dvir-Shpilka\n(STOC 2005), but first proven, for reals, by Kayal-Saraf (FOCS 2009). We\nintroduce the concept of Sylvester-Gallai rank bounds for any field, and show\nthe intimate connection between this and depth-3 identity rank bounds. We also\nprove the first ever theorem about high dimensional Sylvester-Gallai\nconfigurations over any field. Our proofs and techniques are very different\nfrom previous results and devise a very interesting ensemble of combinatorics\nand algebra. The latter concepts are ideal theoretic and involve a new Chinese\nremainder theorem. Our proof methods explain the structure of any depth-3\nidentity C: there is a nucleus of C that forms a low rank identity, while the\nremainder is a high dimensional Sylvester-Gallai configuration.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 18:27:41 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2010 00:35:59 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Saxena", "Nitin", ""], ["Seshadhri", "C.", ""]]}, {"id": "1002.0217", "submitter": "Gergely Palla", "authors": "K. A. Zweig, G. Palla and T. Vicsek", "title": "What makes a phase transition? Analysis of the random satisfiability\n  problem", "comments": null, "journal-ref": "Physica A 389, 1501-1511 (2010)", "doi": "10.1016/j.physa.2009.12.051", "report-no": null, "categories": "physics.data-an cs.CC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last 30 years it was found that many combinatorial systems undergo\nphase transitions. One of the most important examples of these can be found\namong the random k-satisfiability problems (often referred to as k-SAT), asking\nwhether there exists an assignment of Boolean values satisfying a Boolean\nformula composed of clauses with k random variables each. The random 3-SAT\nproblem is reported to show various phase transitions at different critical\nvalues of the ratio of the number of clauses to the number of variables. The\nmost famous of these occurs when the probability of finding a satisfiable\ninstance suddenly drops from 1 to 0. This transition is associated with a rise\nin the hardness of the problem, but until now the correlation between any of\nthe proposed phase transitions and the hardness is not totally clear. In this\npaper we will first show numerically that the number of solutions universally\nfollows a lognormal distribution, thereby explaining the puzzling question of\nwhy the number of solutions is still exponential at the critical point.\nMoreover we provide evidence that the hardness of the closely related problem\nof counting the total number of solutions does not show any phase\ntransition-like behavior. This raises the question of whether the probability\nof finding a satisfiable instance is really an order parameter of a phase\ntransition or whether it is more likely to just show a simple sharp threshold\nphenomenon. More generally, this paper aims at starting a discussion where a\nsimple sharp threshold phenomenon turns into a genuine phase transition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 10:21:37 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Zweig", "K. A.", ""], ["Palla", "G.", ""], ["Vicsek", "T.", ""]]}, {"id": "1002.0562", "submitter": "Philipp Zumstein", "authors": "Michael Hoffmann, Ji\\v{r}\\'i Matou\\v{s}ek, Yoshio Okamoto, Philipp\n  Zumstein", "title": "Minimum and maximum against k lies", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-642-13731-0_14", "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neat 1972 result of Pohl asserts that [3n/2]-2 comparisons are sufficient,\nand also necessary in the worst case, for finding both the minimum and the\nmaximum of an n-element totally ordered set. The set is accessed via an oracle\nfor pairwise comparisons. More recently, the problem has been studied in the\ncontext of the Renyi-Ulam liar games, where the oracle may give up to k false\nanswers. For large k, an upper bound due to Aigner shows that (k+O(\\sqrt{k}))n\ncomparisons suffice. We improve on this by providing an algorithm with at most\n(k+1+C)n+O(k^3) comparisons for some constant C. The known lower bounds are of\nthe form (k+1+c_k)n-D, for some constant D, where c_0=0.5, c_1=23/32=0.71875,\nand c_k=\\Omega(2^{-5k/4}) as k goes to infinity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 18:54:35 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Hoffmann", "Michael", ""], ["Matou\u0161ek", "Ji\u0159\u00ed", ""], ["Okamoto", "Yoshio", ""], ["Zumstein", "Philipp", ""]]}, {"id": "1002.0739", "submitter": "Andrew Novocin", "authors": "Mark Van Hoeij (FSU), Andrew Novocin (LIP)", "title": "Gradual sub-lattice reduction and a new complexity for factoring\n  polynomials", "comments": null, "journal-ref": "in Gradual sub-lattice reduction and a new complexity for\n  factoring polynomials - LATIN 2010, Oaxaca : Mexico (2010)", "doi": null, "report-no": null, "categories": "cs.SC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a lattice algorithm specifically designed for some classical\napplications of lattice reduction. The applications are for lattice bases with\na generalized knapsack-type structure, where the target vectors are boundably\nshort. For such applications, the complexity of the algorithm improves\ntraditional lattice reduction by replacing some dependence on the bit-length of\nthe input vectors by some dependence on the bound for the output vectors. If\nthe bit-length of the target vectors is unrelated to the bit-length of the\ninput, then our algorithm is only linear in the bit-length of the input\nentries, which is an improvement over the quadratic complexity floating-point\nLLL algorithms. To illustrate the usefulness of this algorithm we show that a\ndirect application to factoring univariate polynomials over the integers leads\nto the first complexity bound improvement since 1984. A second application is\nalgebraic number reconstruction, where a new complexity bound is obtained as\nwell.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 13:34:49 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Van Hoeij", "Mark", "", "FSU"], ["Novocin", "Andrew", "", "LIP"]]}, {"id": "1002.0986", "submitter": "Leslie Ann Goldberg", "authors": "Leslie Ann Goldberg and Mark Jerrum", "title": "Approximating the partition function of the ferromagnetic Potts model", "comments": "Minor corrections", "journal-ref": "JACM 59(5) Article 25 October 2012", "doi": "10.1145/2371656.2371660", "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide evidence that it is computationally difficult to approximate the\npartition function of the ferromagnetic q-state Potts model when q>2.\nSpecifically we show that the partition function is hard for the complexity\nclass #RHPi_1 under approximation-preserving reducibility. Thus, it is as hard\nto approximate the partition function as it is to find approximate solutions to\na wide range of counting problems, including that of determining the number of\nindependent sets in a bipartite graph. Our proof exploits the first order phase\ntransition of the \"random cluster\" model, which is a probability distribution\non graphs that is closely related to the q-state Potts model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 13:12:34 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 07:49:01 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2012 10:39:47 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Goldberg", "Leslie Ann", ""], ["Jerrum", "Mark", ""]]}, {"id": "1002.1290", "submitter": "Vishwambhar Rathi", "authors": "Vishwambhar Rathi, Erik Aurell, Lars Rasmussen, and Mikael Skoglund", "title": "Bounds on Threshold of Regular Random $k$-SAT", "comments": "Accepted to SAT 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regular model of formula generation in conjunctive normal\nform (CNF) introduced by Boufkhad et. al. We derive an upper bound on the\nsatisfiability threshold and NAE-satisfiability threshold for regular random\n$k$-SAT for any $k \\geq 3$. We show that these bounds matches with the\ncorresponding bound for the uniform model of formula generation.\n  We derive lower bound on the threshold by applying the second moment method\nto the number of satisfying assignments. For large $k$, we note that the\nobtained lower bounds on the threshold of a regular random formula converges to\nthe lower bound obtained for the uniform model. Thus, we answer the question\nposed in \\cite{AcM06} regarding the performance of the second moment method for\nregular random formulas.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 17:57:36 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2010 15:56:55 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2010 14:43:46 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Rathi", "Vishwambhar", ""], ["Aurell", "Erik", ""], ["Rasmussen", "Lars", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1002.1363", "submitter": "Albert Xin Jiang", "authors": "Albert Xin Jiang and MohammadAli Safari", "title": "Pure Nash Equilibria: Complete Characterization of Hard and Easy\n  Graphical Games", "comments": "8 pages. To appear in AAMAS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the computational complexity of pure Nash equilibria in graphical\ngames. It is known that the problem is NP-complete in general, but tractable\n(i.e., in P) for special classes of graphs such as those with bounded\ntreewidth. It is then natural to ask: is it possible to characterize all\ntractable classes of graphs for this problem? In this work, we provide such a\ncharacterization for the case of bounded in-degree graphs, thereby resolving\nthe gap between existing hardness and tractability results. In particular, we\nanalyze the complexity of PUREGG(C, -), the problem of deciding the existence\nof pure Nash equilibria in graphical games whose underlying graphs are\nrestricted to class C. We prove that, under reasonable complexity theoretic\nassumptions, for every recursively enumerable class C of directed graphs with\nbounded in-degree, PUREGG(C, -) is in polynomial time if and only if the\nreduced graphs (the graphs resulting from iterated removal of sinks) of C have\nbounded treewidth. We also give a characterization for PURECHG(C,-), the\nproblem of deciding the existence of pure Nash equilibria in colored\nhypergraphical games, a game representation that can express the additional\nstructure that some of the players have identical local utility functions. We\nshow that the tractable classes of bounded-arity colored hypergraphical games\nare precisely those whose reduced graphs have bounded treewidth modulo\nhomomorphic equivalence. Our proofs make novel use of Grohe's characterization\nof the complexity of homomorphism problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2010 09:36:31 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Jiang", "Albert Xin", ""], ["Safari", "MohammadAli", ""]]}, {"id": "1002.1496", "submitter": "Maurice  Jansen", "authors": "Maurice Jansen and Youming Qiao and Jayalal Sarma", "title": "Deterministic Black-Box Identity Testing $\\pi$-Ordered Algebraic\n  Branching Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study algebraic branching programs (ABPs) with restrictions\non the order and the number of reads of variables in the program. Given a\npermutation $\\pi$ of $n$ variables, for a $\\pi$-ordered ABP ($\\pi$-OABP), for\nany directed path $p$ from source to sink, a variable can appear at most once\non $p$, and the order in which variables appear on $p$ must respect $\\pi$. An\nABP $A$ is said to be of read $r$, if any variable appears at most $r$ times in\n$A$. Our main result pertains to the identity testing problem. Over any field\n$F$ and in the black-box model, i.e. given only query access to the polynomial,\nwe have the following result: read $r$ $\\pi$-OABP computable polynomials can be\ntested in $\\DTIME[2^{O(r\\log r \\cdot \\log^2 n \\log\\log n)}]$.\n  Our next set of results investigates the computational limitations of OABPs.\nIt is shown that any OABP computing the determinant or permanent requires size\n$\\Omega(2^n/n)$ and read $\\Omega(2^n/n^2)$. We give a multilinear polynomial\n$p$ in $2n+1$ variables over some specifically selected field $G$, such that\nany OABP computing $p$ must read some variable at least $2^n$ times. We show\nthat the elementary symmetric polynomial of degree $r$ in $n$ variables can be\ncomputed by a size $O(rn)$ read $r$ OABP, but not by a read $(r-1)$ OABP, for\nany $0 < 2r-1 \\leq n$. Finally, we give an example of a polynomial $p$ and two\nvariables orders $\\pi \\neq \\pi'$, such that $p$ can be computed by a read-once\n$\\pi$-OABP, but where any $\\pi'$-OABP computing $p$ must read some variable at\nleast $2^n$\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2010 22:40:21 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Jansen", "Maurice", ""], ["Qiao", "Youming", ""], ["Sarma", "Jayalal", ""]]}, {"id": "1002.1606", "submitter": "Irit Dinur", "authors": "Irit Dinur, Or Meir", "title": "Derandomized Parallel Repetition via Structured PCPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A PCP is a proof system for NP in which the proof can be checked by a\nprobabilistic verifier. The verifier is only allowed to read a very small\nportion of the proof, and in return is allowed to err with some bounded\nprobability. The probability that the verifier accepts a false proof is called\nthe soundness error, and is an important parameter of a PCP system that one\nseeks to minimize. Constructing PCPs with sub-constant soundness error and, at\nthe same time, a minimal number of queries into the proof (namely two) is\nespecially important due to applications for inapproximability.\n  In this work we construct such PCP verifiers, i.e., PCPs that make only two\nqueries and have sub-constant soundness error. Our construction can be viewed\nas a combinatorial alternative to the \"manifold vs. point\" construction, which\nis the only construction in the literature for this parameter range. The\n\"manifold vs. point\" PCP is based on a low degree test, while our construction\nis based on a direct product test. We also extend our construction to yield a\ndecodable PCP (dPCP) with the same parameters. By plugging in this dPCP into\nthe scheme of Dinur and Harsha (FOCS 2009) one gets an alternative construction\nof the result of Moshkovitz and Raz (FOCS 2008), namely: a construction of\ntwo-query PCPs with small soundness error and small alphabet size.\n  Our construction of a PCP is based on extending the derandomized direct\nproduct test of Impagliazzo, Kabanets and Wigderson (STOC 09) to a derandomized\nparallel repetition theorem. More accurately, our PCP construction is obtained\nin two steps. We first prove a derandomized parallel repetition theorem for\nspecially structured PCPs. Then, we show that any PCP can be transformed into\none that has the required structure, by embedding it on a de-Bruijn graph.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2010 13:59:28 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2010 10:17:20 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2010 06:59:56 GMT"}, {"version": "v4", "created": "Tue, 18 Mar 2014 16:09:40 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Dinur", "Irit", ""], ["Meir", "Or", ""]]}, {"id": "1002.1880", "submitter": "Florian Sikora", "authors": "Sylvain Guillemot, Florian Sikora", "title": "Finding and counting vertex-colored subtrees", "comments": "Conference version in International Symposium on Mathematical\n  Foundations of Computer Science (MFCS), Brno : Czech Republic (2010) Journal\n  Version in Algorithmica", "journal-ref": null, "doi": "10.1007/s00453-011-9600-8", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems studied in this article originate from the Graph Motif problem\nintroduced by Lacroix et al. in the context of biological networks. The problem\nis to decide if a vertex-colored graph has a connected subgraph whose colors\nequal a given multiset of colors $M$. It is a graph pattern-matching problem\nvariant, where the structure of the occurrence of the pattern is not of\ninterest but the only requirement is the connectedness. Using an algebraic\nframework recently introduced by Koutis et al., we obtain new FPT algorithms\nfor Graph Motif and variants, with improved running times. We also obtain\nresults on the counting versions of this problem, proving that the counting\nproblem is FPT if M is a set, but becomes W[1]-hard if M is a multiset with two\ncolors. Finally, we present an experimental evaluation of this approach on real\ndatasets, showing that its performance compares favorably with existing\nsoftware.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 10 May 2010 12:18:20 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2010 07:42:54 GMT"}, {"version": "v4", "created": "Fri, 24 Feb 2012 15:35:28 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Guillemot", "Sylvain", ""], ["Sikora", "Florian", ""]]}, {"id": "1002.2284", "submitter": "Philip Maymin", "authors": "Philip Maymin", "title": "Markets are efficient if and only if P = NP", "comments": "33 pages; extended literature review and some additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I prove that if markets are weak-form efficient, meaning current prices fully\nreflect all information available in past prices, then P = NP, meaning every\ncomputational problem whose solution can be verified in polynomial time can\nalso be solved in polynomial time. I also prove the converse by showing how we\ncan \"program\" the market to solve NP-complete problems. Since P probably does\nnot equal NP, markets are probably not efficient. Specifically, markets become\nincreasingly inefficient as the time series lengthens or becomes more frequent.\nAn illustration by way of partitioning the excess returns to momentum\nstrategies based on data availability confirms this prediction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 05:56:16 GMT"}, {"version": "v2", "created": "Thu, 13 May 2010 07:26:53 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Maymin", "Philip", ""]]}, {"id": "1002.2440", "submitter": "Bernhard von Stengel", "authors": "Christoph Ambuehl, Bernd Gaertner, Bernhard von Stengel", "title": "Optimal Lower Bounds for Projective List Update Algorithms", "comments": "Version 3 same as version 2, but date in LaTeX \\today macro replaced\n  by March 8, 2012", "journal-ref": "ACM Transactions on Algorithms (TALG) Volume 9, Issue 4, September\n  2013, Article 31, 18 pages", "doi": "10.1145/2500120", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The list update problem is a classical online problem, with an optimal\ncompetitive ratio that is still open, known to be somewhere between 1.5 and\n1.6. An algorithm with competitive ratio 1.6, the smallest known to date, is\nCOMB, a randomized combination of BIT and the TIMESTAMP algorithm TS. This and\nalmost all other list update algorithms, like MTF, are projective in the sense\nthat they can be defined by looking only at any pair of list items at a time.\nProjectivity (also known as \"list factoring\") simplifies both the description\nof the algorithm and its analysis, and so far seems to be the only way to\ndefine a good online algorithm for lists of arbitrary length. In this paper we\ncharacterize all projective list update algorithms and show that their\ncompetitive ratio is never smaller than 1.6 in the partial cost model.\nTherefore, COMB is a best possible projective algorithm in this model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 21:48:07 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2012 08:21:54 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2012 19:00:15 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Ambuehl", "Christoph", ""], ["Gaertner", "Bernd", ""], ["von Stengel", "Bernhard", ""]]}, {"id": "1002.2746", "submitter": "David Doty", "authors": "David Doty, Lila Kari, Benoit Masson", "title": "Negative Interactions in Irreversible Self-Assembly", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-18305-8_4", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of negative (i.e., repulsive) interaction the\nabstract Tile Assembly Model defined by Winfree. Winfree postulated negative\ninteractions to be physically plausible in his Ph.D. thesis, and Reif, Sahu,\nand Yin explored their power in the context of reversible attachment\noperations. We explore the power of negative interactions with irreversible\nattachments, and we achieve two main results. Our first result is an\nimpossibility theorem: after t steps of assembly, Omega(t) tiles will be\nforever bound to an assembly, unable to detach. Thus negative glue strengths do\nnot afford unlimited power to reuse tiles. Our second result is a positive one:\nwe construct a set of tiles that can simulate a Turing machine with space bound\ns and time bound t, while ensuring that no intermediate assembly grows larger\nthan O(s), rather than O(s * t) as required by the standard Turing machine\nsimulation with tiles.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2010 02:56:05 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Doty", "David", ""], ["Kari", "Lila", ""], ["Masson", "Benoit", ""]]}, {"id": "1002.2954", "submitter": "Phuong Nguyen", "authors": "Phuong Nguyen, Stephen Cook", "title": "The Complexity of Proving the Discrete Jordan Curve Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jordan Curve Theorem (JCT) states that a simple closed curve divides the\nplane into exactly two connected regions. We formalize and prove the theorem in\nthe context of grid graphs, under different input settings, in theories of\nbounded arithmetic that correspond to small complexity classes. The theory\n$V^0(2)$ (corresponding to $AC^0(2)$) proves that any set of edges that form\ndisjoint cycles divides the grid into at least two regions. The theory $V^0$\n(corresponding to $AC^0$) proves that any sequence of edges that form a simple\nclosed curve divides the grid into exactly two regions. As a consequence, the\nHex tautologies and the st-connectivity tautologies have polynomial size\n$AC^0(2)$-Frege-proofs, which improves results of Buss which only apply to the\nstronger proof system $TC^0$-Frege.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 21:32:42 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Nguyen", "Phuong", ""], ["Cook", "Stephen", ""]]}, {"id": "1002.2970", "submitter": "Wim van Dam", "authors": "Wim van Dam and Qingqing Yuan", "title": "Quantum Online Memory Checking", "comments": "12 pages; Theory of Quantum Computation, Communication, and\n  Cryptography: Fourth Workshop, TQC 2009", "journal-ref": "Lecture Notes in Computer Science 5906 (2009), 10-19", "doi": "10.1007/978-3-642-10698-9", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of memory checking considers storing files on an unreliable\npublic server whose memory can be modified by a malicious party. The main task\nis to design an online memory checker with the capability to verify that the\ninformation on the server has not been corrupted. To store n bits of public\ninformation, the memory checker has s private reliable bits for verification\npurpose; while to retrieve each bit of public information the checker\ncommunicates t bits with the public memory. Earlier work showed that, for\nclassical memory checkers, the lower bound s*t \\in Omega(n) holds. In this\narticle we study quantum memory checkers that have s private qubits and that\nare allowed to quantum query the public memory using t qubits. We prove an\nexponential improvement over the classical setting by showing the existence of\na quantum checker that, using quantum fingerprints, requires only s \\in O(log\nn) qubits of local memory and t \\in O(polylog n) qubits of communication with\nthe public memory.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 22:22:45 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["van Dam", "Wim", ""], ["Yuan", "Qingqing", ""]]}, {"id": "1002.3183", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "A Complete Characterization of Statistical Query Learning with\n  Applications to Evolvability", "comments": "Simplified Lemma 3.8 and it's applications", "journal-ref": "Proceedings of the 44th IEEE Symposium on Foundations of Computer\n  Science, pp 375-384, 2009", "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical query (SQ) learning model of Kearns (1993) is a natural\nrestriction of the PAC learning model in which a learning algorithm is allowed\nto obtain estimates of statistical properties of the examples but cannot see\nthe examples themselves. We describe a new and simple characterization of the\nquery complexity of learning in the SQ learning model. Unlike the previously\nknown bounds on SQ learning our characterization preserves the accuracy and the\nefficiency of learning. The preservation of accuracy implies that that our\ncharacterization gives the first characterization of SQ learning in the\nagnostic learning framework. The preservation of efficiency is achieved using a\nnew boosting technique and allows us to derive a new approach to the design of\nevolutionary algorithms in Valiant's (2006) model of evolvability. We use this\napproach to demonstrate the existence of a large class of monotone evolutionary\nlearning algorithms based on square loss performance estimation. These results\ndiffer significantly from the few known evolutionary algorithms and give\nevidence that evolvability in Valiant's model is a more versatile phenomenon\nthan there had been previous reason to suspect.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 22:35:39 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 00:43:31 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2013 04:57:18 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1002.3453", "submitter": "Luca Vercelli", "authors": "Luca Vercelli", "title": "On the complexity of stratified logics", "comments": "PhD thesis. about 180 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our primary motivation is the comparison of two different traditions used in\nICC to characterize the class FPTIME of the polynomial time computable\nfunctions. On one side, FPTIME can be captured by Intuitionistic Light Affine\nLogic (ILAL), a logic derived from Linear Logic, characterized by the\nstructural invariant Stratification. On the other side, FPTIME can be captured\nby Safe Recursion on Notation (SRN), an algebra of functions based on\nPredicative Recursion, a restriction of the standard recursion schema used to\ndefiine primitive recursive functions. Stratifiication and Predicative\nRecursion seem to share common underlying principles, whose study is the main\nsubject of this work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 08:41:01 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Vercelli", "Luca", ""]]}, {"id": "1002.3534", "submitter": "Holenstein Thomas", "authors": "Thomas Holenstein and Grant Schoenebeck", "title": "General Hardness Amplification of Predicates and Puzzles", "comments": "Revision 2: Added references, minor changes, slight improvements in\n  presentation of some proof sketches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new proofs for the hardness amplification of efficiently samplable\npredicates and of weakly verifiable puzzles which generalize to new settings.\nMore concretely, in the first part of the paper, we give a new proof of Yao's\nXOR-Lemma that additionally applies to related theorems in the cryptographic\nsetting. Our proof seems simpler than previous ones, yet immediately\ngeneralizes to statements similar in spirit such as the extraction lemma used\nto obtain pseudo-random generators from one-way functions [Hastad, Impagliazzo,\nLevin, Luby, SIAM J. on Comp. 1999].\n  In the second part of the paper, we give a new proof of hardness\namplification for weakly verifiable puzzles, which is more general than\nprevious ones in that it gives the right bound even for an arbitrary monotone\nfunction applied to the checking circuit of the underlying puzzle.\n  Both our proofs are applicable in many settings of interactive cryptographic\nprotocols because they satisfy a property that we call \"non-rewinding\". In\nparticular, we show that any weak cryptographic protocol whose security is\ngiven by the unpredictability of single bits can be strengthened with a natural\ninformation theoretic protocol. As an example, we show how these theorems solve\nthe main open question from [Halevi and Rabin, TCC2008] concerning bit\ncommitment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 14:50:53 GMT"}, {"version": "v2", "created": "Wed, 29 Dec 2010 13:49:01 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Holenstein", "Thomas", ""], ["Schoenebeck", "Grant", ""]]}, {"id": "1002.3664", "submitter": "Andrew Drucker", "authors": "Andrew Drucker", "title": "A PCP Characterization of AM", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a 2-round stochastic constraint-satisfaction problem, and show\nthat its approximation version is complete for (the promise version of) the\ncomplexity class AM. This gives a `PCP characterization' of AM analogous to the\nPCP Theorem for NP. Similar characterizations have been given for higher levels\nof the Polynomial Hierarchy, and for PSPACE; however, we suggest that the\nresult for AM might be of particular significance for attempts to derandomize\nthis class.\n  To test this notion, we pose some `Randomized Optimization Hypotheses'\nrelated to our stochastic CSPs that (in light of our result) would imply\ncollapse results for AM. Unfortunately, the hypotheses appear over-strong, and\nwe present evidence against them. In the process we show that, if some language\nin NP is hard-on-average against circuits of size 2^{Omega(n)}, then there\nexist hard-on-average optimization problems of a particularly elegant form.\n  All our proofs use a powerful form of PCPs known as Probabilistically\nCheckable Proofs of Proximity, and demonstrate their versatility. We also use\nknown results on randomness-efficient soundness- and hardness-amplification. In\nparticular, we make essential use of the Impagliazzo-Wigderson generator; our\nanalysis relies on a recent Chernoff-type theorem for expander walks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 03:34:59 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Drucker", "Andrew", ""]]}, {"id": "1002.3769", "submitter": "Benoit Masson", "authors": "Lila Kari, Beno\\^it Masson", "title": "Polyominoes Simulating Arbitrary-Neighborhood Zippers and Tilings", "comments": "Submitted to Theoretical Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a bridge between the classical tiling theory and the\ncomplex neighborhood self-assembling situations that exist in practice. The\nneighborhood of a position in the plane is the set of coordinates which are\nconsidered adjacent to it. This includes classical neighborhoods of size four,\nas well as arbitrarily complex neighborhoods. A generalized tile system\nconsists of a set of tiles, a neighborhood, and a relation which dictates which\nare the \"admissible\" neighboring tiles of a given tile. Thus, in correctly\nformed assemblies, tiles are assigned positions of the plane in accordance to\nthis relation. We prove that any validly tiled path defined in a given but\narbitrary neighborhood (a zipper) can be simulated by a simple \"ribbon\" of\nmicrotiles. A ribbon is a special kind of polyomino, consisting of a\nnon-self-crossing sequence of tiles on the plane, in which successive tiles\nstick along their adjacent edge. Finally, we extend this construction to the\ncase of traditional tilings, proving that we can simulate\narbitrary-neighborhood tilings by simple-neighborhood tilings, while preserving\nsome of their essential properties.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 16:19:53 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2011 07:34:36 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Kari", "Lila", ""], ["Masson", "Beno\u00eet", ""]]}, {"id": "1002.3864", "submitter": "Prahladh Harsha", "authors": "Prahladh Harsha, Moses Charikar, Matthew Andrews, Sanjeev Arora,\n  Subhash Khot, Dana Moshkovitz, Lisa Zhang, Ashkan Aazami, Dev Desai, Igor\n  Gorodezky, Geetha Jagannathan, Alexander S. Kulikov, Darakhshan J. Mir,\n  Alantha Newman, Aleksandar Nikolov, David Pritchard and Gwen Spencer", "title": "Limits of Approximation Algorithms: PCPs and Unique Games (DIMACS\n  Tutorial Lecture Notes)", "comments": "74 pages, lecture notes", "journal-ref": null, "doi": null, "report-no": "DIMACS Technical Report 2010-02", "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the lecture notes for the DIMACS Tutorial \"Limits of Approximation\nAlgorithms: PCPs and Unique Games\" held at the DIMACS Center, CoRE Building,\nRutgers University on 20-21 July, 2009. This tutorial was jointly sponsored by\nthe DIMACS Special Focus on Hardness of Approximation, the DIMACS Special Focus\non Algorithmic Foundations of the Internet, and the Center for Computational\nIntractability with support from the National Security Agency and the National\nScience Foundation.\n  The speakers at the tutorial were Matthew Andrews, Sanjeev Arora, Moses\nCharikar, Prahladh Harsha, Subhash Khot, Dana Moshkovitz and Lisa Zhang. The\nsribes were Ashkan Aazami, Dev Desai, Igor Gorodezky, Geetha Jagannathan,\nAlexander S. Kulikov, Darakhshan J. Mir, Alantha Newman, Aleksandar Nikolov,\nDavid Pritchard and Gwen Spencer.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2010 10:26:50 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Harsha", "Prahladh", ""], ["Charikar", "Moses", ""], ["Andrews", "Matthew", ""], ["Arora", "Sanjeev", ""], ["Khot", "Subhash", ""], ["Moshkovitz", "Dana", ""], ["Zhang", "Lisa", ""], ["Aazami", "Ashkan", ""], ["Desai", "Dev", ""], ["Gorodezky", "Igor", ""], ["Jagannathan", "Geetha", ""], ["Kulikov", "Alexander S.", ""], ["Mir", "Darakhshan J.", ""], ["Newman", "Alantha", ""], ["Nikolov", "Aleksandar", ""], ["Pritchard", "David", ""], ["Spencer", "Gwen", ""]]}, {"id": "1002.3937", "submitter": "D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi", "authors": "Domotor Palvolgyi", "title": "Partitionability to two trees is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that P2T - the problem of deciding whether the edge set of a simple\ngraph can be partitioned into two trees or not - is NP-complete.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2010 20:37:34 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Palvolgyi", "Domotor", ""]]}, {"id": "1002.4084", "submitter": "Benoit Masson", "authors": "Lila Kari (UWO), Beno\\^it Masson (UWO), Shinnosuke Seki (UWO)", "title": "Properties of Pseudo-Primitive Words and their Applications", "comments": "Submitted to International Journal of Foundations of Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudo-primitive word with respect to an antimorphic involution \\theta is a\nword which cannot be written as a catenation of occurrences of a strictly\nshorter word t and \\theta(t). Properties of pseudo-primitive words are\ninvestigated in this paper. These properties link pseudo-primitive words with\nessential notions in combinatorics on words such as primitive words,\n(pseudo)-palindromes, and (pseudo)-commutativity. Their applications include an\nimproved solution to the extended Lyndon-Sch\\\"utzenberger equation u_1 u_2 ...\nu_l = v_1 ... v_n w_1 ... w_m, where u_1, ..., u_l \\in {u, \\theta(u)}, v_1,\n..., v_n \\in {v, \\theta(v)}, and w_1, ..., w_m \\in {w, \\theata(w)} for some\nwords u, v, w, integers l, n, m \\ge 2, and an antimorphic involution \\theta. We\nprove that for l \\ge 4, n,m \\ge 3, this equation implies that u, v, w can be\nexpressed in terms of a common word t and its image \\theta(t). Moreover,\nseveral cases of this equation where l = 3 are examined.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 09:40:13 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Kari", "Lila", "", "UWO"], ["Masson", "Beno\u00eet", "", "UWO"], ["Seki", "Shinnosuke", "", "UWO"]]}, {"id": "1002.4464", "submitter": "Frank Dehne", "authors": "Frank Dehne and Hamidreza Zaboli", "title": "Deterministic Sample Sort For GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort\nalgorithm for many-core GPUs. Our method is considerably faster than Thrust\nMerge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting\nalgorithm for GPUs, and it is as fast as the new randomized sample sort for\nGPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic\nsample sort has the advantage that bucket sizes are guaranteed and therefore\nits running time does not have the input data dependent fluctuations that can\noccur for randomized sample sort.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 04:27:14 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Dehne", "Frank", ""], ["Zaboli", "Hamidreza", ""]]}, {"id": "1002.4482", "submitter": "Frank Dehne", "authors": "Frank Dehne and Kumanan Yogaratnam", "title": "Exploring the Limits of GPUs With Parallel Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the limits of graphics processors (GPUs) for\ngeneral purpose parallel computing by studying problems that require highly\nirregular data access patterns: parallel graph algorithms for list ranking and\nconnected components. Such graph problems represent a worst case scenario for\ncoalescing parallel memory accesses on GPUs which is critical for good GPU\nperformance. Our experimental study indicates that PRAM algorithms are a good\nstarting point for developing efficient parallel GPU methods but require\nnon-trivial modifications to ensure good GPU performance. We present a set of\nguidelines that help algorithm designers adapt PRAM graph algorithms for\nparallel GPU computation. We point out that the study of parallel graph\nalgorithms for GPUs is of wider interest for discrete and combinatorial\nproblems in general because many of these problems require similar irregular\ndata access patterns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 06:37:29 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Dehne", "Frank", ""], ["Yogaratnam", "Kumanan", ""]]}, {"id": "1002.4577", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "Bounded Rationality, Strategy Simplification, and Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is frequently suggested that predictions made by game theory could be\nimproved by considering computational restrictions when modeling agents. Under\nthe supposition that players in a game may desire to balance maximization of\npayoff with minimization of strategy complexity, Rubinstein and co-authors\nstudied forms of Nash equilibrium where strategies are maximally simplified in\nthat no strategy can be further simplified without sacrificing payoff. Inspired\nby this line of work, we introduce a notion of equilibrium whereby strategies\nare also maximally simplified, but with respect to a simplification procedure\nthat is more careful in that a player will not simplify if the simplification\nincents other players to deviate. We study such equilibria in two-player\nmachine games in which players choose finite automata that succinctly represent\nstrategies for repeated games; in this context, we present techniques for\nestablishing that an outcome is at equilibrium and present results on the\nstructure of equilibria.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 16:29:23 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2011 09:14:02 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2011 23:18:34 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "1002.4676", "submitter": "Dustin Wehr", "authors": "Dustin Wehr", "title": "Pebbling and Branching Programs Solving the Tree Evaluation Problem", "comments": "Written as one of the requirements for my MSc. 29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study restricted computation models related to the Tree Evaluation\nProblem}. The TEP was introduced in earlier work as a simple candidate for the\n(*very*) long term goal of separating L and LogDCFL. The input to the problem\nis a rooted, balanced binary tree of height h, whose internal nodes are labeled\nwith binary functions on [k] = {1,...,k} (each given simply as a list of k^2\nelements of [k]), and whose leaves are labeled with elements of [k]. Each node\nobtains a value in [k] equal to its binary function applied to the values of\nits children, and the output is the value of the root. The first restricted\ncomputation model, called Fractional Pebbling, is a generalization of the\nblack/white pebbling game on graphs, and arises in a natural way from the\nsearch for good upper bounds on the size of nondeterministic branching programs\n(BPs) solving the TEP - for any fixed h, if the binary tree of height h has\nfractional pebbling cost at most p, then there are nondeterministic BPs of size\nO(k^p) solving the height h TEP. We prove a lower bound on the fractional\npebbling cost of d-ary trees that is tight to within an additive constant for\neach fixed d. The second restricted computation model we study is a semantic\nrestriction on (non)deterministic BPs solving the TEP - Thrifty BPs.\nDeterministic (resp. nondeterministic) thrifty BPs suffice to implement the\nbest known algorithms for the TEP, based on black (resp. fractional) pebbling.\nIn earlier work, for each fixed h a lower bound on the size of deterministic\nthrifty BPs was proved that is tight for sufficiently large k. We give an\nalternative proof that achieves the same bound for all k. We show the same\nbound still holds in a less-restricted model, and also that gradually weaker\nlower bounds can be obtained for gradually weaker restrictions on the model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 01:41:38 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Wehr", "Dustin", ""]]}]