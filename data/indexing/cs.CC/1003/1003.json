[{"id": "1003.0425", "submitter": "Giorgi Japaridze", "authors": "Giorgi Japaridze", "title": "A logical basis for constructive systems", "comments": null, "journal-ref": "Journal of Logic and Computation 22 (2012), pp. 605-642", "doi": "10.1093/logcom/exr009", "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work is devoted to Computability Logic (CoL) -- the\nphilosophical/mathematical platform and long-term project for redeveloping\nclassical logic after replacing truth} by computability in its underlying\nsemantics (see http://www.cis.upenn.edu/~giorgi/cl.html). This article\nelaborates some basic complexity theory for the CoL framework. Then it proves\nsoundness and completeness for the deductive system CL12 with respect to the\nsemantics of CoL, including the version of the latter based on polynomial time\ncomputability instead of computability-in-principle. CL12 is a sequent calculus\nsystem, where the meaning of a sequent intuitively can be characterized as \"the\nsuccedent is algorithmically reducible to the antecedent\", and where formulas\nare built from predicate letters, function letters, variables, constants,\nidentity, negation, parallel and choice connectives, and blind and choice\nquantifiers. A case is made that CL12 is an adequate logical basis for\nconstructive applied theories, including complexity-oriented ones.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 19:00:10 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2010 19:15:47 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2011 19:10:31 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Japaridze", "Giorgi", ""]]}, {"id": "1003.0480", "submitter": "Nicolas Brener", "authors": "Nicolas Brener", "title": "A definable number which cannot be approximated algorithmically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turing machine (TM) and the Church thesis have formalized the concept of\ncomputable number, this allowed to display non-computable numbers. This paper\ndefines the concept of number \"approachable\" by a TM and shows that some (if\nnot all) known non-computable numbers are approachable by TMs. Then an example\nof a number not approachable by a TM is given.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 23:11:22 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Brener", "Nicolas", ""]]}, {"id": "1003.0514", "submitter": "Pulkit Grover", "authors": "Pulkit Grover, Se Yong Park and Anant Sahai", "title": "The finite-dimensional Witsenhausen counterexample", "comments": "32 pages, 7 figures, 1 table. Presented at ConCom 2009, Seoul, Korea.\n  Submitted to IEEE Transactions on Automatic Control", "journal-ref": null, "doi": "10.1109/WIOPT.2009.5291559", "report-no": null, "categories": "cs.IT cs.CC math.IT math.OC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recently, a vector version of Witsenhausen's counterexample was considered\nand it was shown that in that limit of infinite vector length, certain\nquantization-based control strategies are provably within a constant factor of\nthe optimal cost for all possible problem parameters. In this paper, finite\nvector lengths are considered with the dimension being viewed as an additional\nproblem parameter. By applying a large-deviation \"sphere-packing\" philosophy, a\nlower bound to the optimal cost for the finite dimensional case is derived that\nuses appropriate shadows of the infinite-length bound. Using the new lower\nbound, we show that good lattice-based control strategies achieve within a\nconstant factor of the optimal cost uniformly over all possible problem\nparameters, including the vector length. For Witsenhausen's original problem --\nthe scalar case -- the gap between regular lattice-based strategies and the\nlower bound is numerically never more than a factor of 8.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 07:44:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Grover", "Pulkit", ""], ["Park", "Se Yong", ""], ["Sahai", "Anant", ""]]}, {"id": "1003.0802", "submitter": "Barnaby Martin", "authors": "Florent Madelaine and Barnaby Martin", "title": "The complexity of positive first-order logic without equality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of evaluating positive equality-free sentences of\nfirst-order (FO) logic over a fixed, finite structure B. This may be seen as a\nnatural generalisation of the non-uniform quantified constraint satisfaction\nproblem QCSP(B). We introduce surjective hyper-endomorphisms and use them in\nproving a Galois connection that characterises definability in positive\nequality-free FO. Through an algebraic method, we derive a complete complexity\nclassification for our problems as B ranges over structures of size at most\nthree. Specifically, each problem is either in Logspace, is NP-complete, is\nco-NP-complete or is Pspace-complete.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 13:35:58 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Madelaine", "Florent", ""], ["Martin", "Barnaby", ""]]}, {"id": "1003.1057", "submitter": "Joerg Endrullis", "authors": "Joerg Endrullis", "title": "Levels of Undecidability in Infinitary Rewriting: Normalization and\n  Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [EGZ09] it has been shown that infinitary strong normalization (SNi) is\nPi-1-1-complete. Suprisingly, it turns out that infinitary weak normalization\n(WNi) is a harder problem, being Pi-1-2-complete, and thereby strictly higher\nin the analytical hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 14:45:42 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Endrullis", "Joerg", ""]]}, {"id": "1003.1164", "submitter": "Deepak Ponvel Chermakani Mr", "authors": "Deepak Ponvel Chermakani", "title": "Repeating Patterns in Linear Programs that express NP-Complete Problems", "comments": "4 Pages, 1 Generalization of earlier Theorem, and 2 Conjectures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of my recent papers transforms an NP-Complete problem into the question\nof whether or not a feasible real solution exists to some Linear Program. The\nunique feature of this Linear Program is that though there is no explicit bound\non the minimum required number of linear inequalities, which is most probably\nexponential to the size of the NP-Complete problem, the Linear Program can\nstill be described efficiently. The reason for this efficient description is\nthat coefficients keep repeating in some pattern, even as the number of\ninequalities is conveniently assumed to tend to Infinity. I discuss why this\nconvenient assumption does not change the feasibility result of the Linear\nProgram. I conclude with two Conjectures, which might help to make an efficient\ndecision on the feasibility of this Linear Program.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 13:02:18 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Chermakani", "Deepak Ponvel", ""]]}, {"id": "1003.1354", "submitter": "Ankan Saha", "authors": "Xinhua Zhang (1), Ankan Saha (2), S.V.N. Vishwanathan (1)((1) Purdue\n  University, (2) University of Chicago)", "title": "Faster Rates for training Max-Margin Markov Networks", "comments": "14 pages Submitted to COLT 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured output prediction is an important machine learning problem both in\ntheory and practice, and the max-margin Markov network (\\mcn) is an effective\napproach. All state-of-the-art algorithms for optimizing \\mcn\\ objectives take\nat least $O(1/\\epsilon)$ number of iterations to find an $\\epsilon$ accurate\nsolution. Recent results in structured optimization suggest that faster rates\nare possible by exploiting the structure of the objective function. Towards\nthis end \\citet{Nesterov05} proposed an excessive gap reduction technique based\non Euclidean projections which converges in $O(1/\\sqrt{\\epsilon})$ iterations\non strongly convex functions. Unfortunately when applied to \\mcn s, this\napproach does not admit graphical model factorization which, as in many\nexisting algorithms, is crucial for keeping the cost per iteration tractable.\nIn this paper, we present a new excessive gap reduction technique based on\nBregman projections which admits graphical model factorization naturally, and\nconverges in $O(1/\\sqrt{\\epsilon})$ iterations. Compared with existing\nalgorithms, the convergence rate of our method has better dependence on\n$\\epsilon$ and other parameters of the problem, and can be easily kernelized.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 05:49:19 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Zhang", "Xinhua", ""], ["Saha", "Ankan", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1003.1443", "submitter": "Shengyu Zhang", "authors": "Troy Lee and Shengyu Zhang", "title": "Composition theorems in communication complexity", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A well-studied class of functions in communication complexity are composed\nfunctions of the form $(f \\comp g^n)(x,y)=f(g(x^1, y^1),..., g(x^n,y^n))$. This\nis a rich family of functions which encompasses many of the important examples\nin the literature. It is thus of great interest to understand what properties\nof $f$ and $g$ affect the communication complexity of $(f \\comp g^n)$, and in\nwhat way.\n  Recently, Sherstov \\cite{She09b} and independently Shi-Zhu \\cite{SZ09b}\ndeveloped conditions on the inner function $g$ which imply that the quantum\ncommunication complexity of $f \\comp g^n$ is at least the approximate\npolynomial degree of $f$. We generalize both of these frameworks. We show that\nthe pattern matrix framework of Sherstov works whenever the inner function $g$\nis {\\em strongly balanced}---we say that $g: X \\times Y \\to \\{-1,+1\\}$ is\nstrongly balanced if all rows and columns in the matrix $M_g=[g(x,y)]_{x,y}$\nsum to zero. This result strictly generalizes the pattern matrix framework of\nSherstov \\cite{She09b}, which has been a very useful idea in a variety of\nsettings \\cite{She08b,RS08,Cha07,LS09,CA08,BHN09}.\n  Shi-Zhu require that the inner function $g$ has small {\\em spectral\ndiscrepancy}, a somewhat awkward condition to verify. We relax this to the\nusual notion of discrepancy. We also enhance the framework of composed\nfunctions studied so far by considering functions $F(x,y) = f(g(x,y))$, where\nthe range of $g$ is a group $G$. When $G$ is Abelian, the analogue of the\nstrongly balanced condition becomes a simple group invariance property of $g$.\nWe are able to formulate a general lower bound on $F$ whenever $g$ satisfies\nthis property.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 08:48:25 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Lee", "Troy", ""], ["Zhang", "Shengyu", ""]]}, {"id": "1003.1991", "submitter": "Minghui Jiang", "authors": "Minghui Jiang", "title": "The zero exemplar distance problem", "comments": "Strengthened and reorganized", "journal-ref": null, "doi": "10.1007/978-3-642-16181-0_7", "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two genomes with duplicate genes, \\textsc{Zero Exemplar Distance} is\nthe problem of deciding whether the two genomes can be reduced to the same\ngenome without duplicate genes by deleting all but one copy of each gene in\neach genome. Blin, Fertin, Sikora, and Vialette recently proved that\n\\textsc{Zero Exemplar Distance} for monochromosomal genomes is NP-hard even if\neach gene appears at most two times in each genome, thereby settling an\nimportant open question on genome rearrangement in the exemplar model. In this\npaper, we give a very simple alternative proof of this result. We also study\nthe problem \\textsc{Zero Exemplar Distance} for multichromosomal genomes\nwithout gene order, and prove the analogous result that it is also NP-hard even\nif each gene appears at most two times in each genome. For the positive\ndirection, we show that both variants of \\textsc{Zero Exemplar Distance} admit\npolynomial-time algorithms if each gene appears exactly once in one genome and\nat least once in the other genome. In addition, we present a polynomial-time\nalgorithm for the related problem \\textsc{Exemplar Longest Common Subsequence}\nin the special case that each mandatory symbol appears exactly once in one\ninput sequence and at least once in the other input sequence. This answers an\nopen question of Bonizzoni et al. We also show that \\textsc{Zero Exemplar\nDistance} for multichromosomal genomes without gene order is fixed-parameter\ntractable if the parameter is the maximum number of chromosomes in each genome.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 22:25:13 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2010 22:23:04 GMT"}, {"version": "v3", "created": "Mon, 25 Oct 2010 17:16:34 GMT"}, {"version": "v4", "created": "Fri, 5 Nov 2010 21:40:52 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Jiang", "Minghui", ""]]}, {"id": "1003.2839", "submitter": "Vamsi Kundeti", "authors": "Vamsi Kundeti, Sanguthevar Rajasekaran, Hieu Dinh", "title": "On the Border Length Minimization Problem (BLMP) on a Square Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein/Peptide microarrays are rapidly gaining momentum in the diagnosis of\ncancer. High-density and highthroughput peptide arrays are being extensively\nused to detect tumor biomarkers, examine kinase activity, identify antibodies\nhaving low serum titers and locate antibody signatures. Improving the yield of\nmicroarray fabrication involves solving a hard combinatorial optimization\nproblem called the Border Length Minimization Problem (BLMP). An important\nquestion that remained open for the past seven years is if the BLMP is\ntractable or not. We settle this open problem by proving that the BLMP is\nNP-hard. We also present a hierarchical refinement algorithm which can refine\nany heuristic solution for the BLMP problem. We also prove that the\nTSP+1-threading heuristic is an O(N)- approximation. The hierarchical\nrefinement solver is available as an opensource code at\nhttp://launchpad.net/blm-solve.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2010 02:33:43 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2010 00:37:20 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kundeti", "Vamsi", ""], ["Rajasekaran", "Sanguthevar", ""], ["Dinh", "Hieu", ""]]}, {"id": "1003.2851", "submitter": "Yushi Uno", "authors": "Erik D. Demaine, Martin L. Demaine, Nicholas J. A. Harvey, Ryuhei\n  Uehara, Takeaki Uno and Yushi Uno", "title": "The complexity of UNO", "comments": "13 body pages, 2 appendix pages, 1 table, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the popular card game UNO from the viewpoint of\nalgorithmic combinatorial game theory. We define simple and concise\nmathematical models for the game, including both cooperative and uncooperative\nversions, and analyze their computational complexity. In particular, we prove\nthat even a single-player version of UNO is NP-complete, although some\nrestricted cases are in P. Surprisingly, we show that the uncooperative\ntwo-player version is also in P.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2010 06:21:50 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 03:33:11 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2013 08:03:28 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Harvey", "Nicholas J. A.", ""], ["Uehara", "Ryuhei", ""], ["Uno", "Takeaki", ""], ["Uno", "Yushi", ""]]}, {"id": "1003.3047", "submitter": "Jakob Nordstr\\\"om", "authors": "Jakob Nordstr\\\"om", "title": "On the Relative Strength of Pebbling and Resolution", "comments": "Full-length version of paper to appear in Proceedings of the 25th\n  Annual IEEE Conference on Computational Complexity (CCC '10), June 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a revival of interest in pebble games in the context\nof proof complexity. Pebbling has proven a useful tool for studying\nresolution-based proof systems when comparing the strength of different\nsubsystems, showing bounds on proof space, and establishing size-space\ntrade-offs. The typical approach has been to encode the pebble game played on a\ngraph as a CNF formula and then argue that proofs of this formula must inherit\n(various aspects of) the pebbling properties of the underlying graph.\nUnfortunately, the reductions used here are not tight. To simulate resolution\nproofs by pebblings, the full strength of nondeterministic black-white pebbling\nis needed, whereas resolution is only known to be able to simulate\ndeterministic black pebbling. To obtain strong results, one therefore needs to\nfind specific graph families which either have essentially the same properties\nfor black and black-white pebbling (not at all true in general) or which admit\nsimulations of black-white pebblings in resolution. This paper contributes to\nboth these approaches. First, we design a restricted form of black-white\npebbling that can be simulated in resolution and show that there are graph\nfamilies for which such restricted pebblings can be asymptotically better than\nblack pebblings. This proves that, perhaps somewhat unexpectedly, resolution\ncan strictly beat black-only pebbling, and in particular that the space lower\nbounds on pebbling formulas in [Ben-Sasson and Nordstrom 2008] are tight.\nSecond, we present a versatile parametrized graph family with essentially the\nsame properties for black and black-white pebbling, which gives sharp\nsimultaneous trade-offs for black and black-white pebbling for various\nparameter settings. Both of our contributions have been instrumental in\nobtaining the time-space trade-off results for resolution-based proof systems\nin [Ben-Sasson and Nordstrom 2009].\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2010 23:16:02 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2010 02:11:16 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Nordstr\u00f6m", "Jakob", ""]]}, {"id": "1003.3508", "submitter": "Enrique Augusto Tobis", "authors": "Alicia Dickenstein and Enrique A. Tobis", "title": "Independent Sets from an Algebraic Perspective", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the basic problem of counting independent sets in a\ngraph and, in particular, the problem of counting antichains in a finite poset,\nfrom an algebraic perspective. We show that neither independence polynomials of\nbipartite Cohen-Macaulay graphs nor Hilbert series of initial ideals of radical\nzero-dimensional complete intersections ideals, can be evaluated in polynomial\ntime, unless #P=P. Moreover, we present a family of radical zero-dimensional\ncomplete intersection ideals J_P associated to a finite poset P, for which we\ndescribe a universal Gr\\\"obner basis. This implies that the bottleneck in\ncomputing the dimension of the quotient by J_P (that is, the number of zeros of\nJ_P) using Gr\\\"obner methods lies in the description of the standard monomials.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 04:17:16 GMT"}, {"version": "v2", "created": "Tue, 3 May 2011 11:13:31 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Dickenstein", "Alicia", ""], ["Tobis", "Enrique A.", ""]]}, {"id": "1003.3619", "submitter": "Boris Ryabko", "authors": "Boris Ryabko", "title": "Using Information Theory to Study the Efficiency and Capacity of\n  Computers and Similar Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problems of estimating the computer efficiency and the\ncomputer capacity. We define the computer efficiency and capacity and suggest a\nmethod for their estimation, based on the analysis of processor instructions\nand kinds of accessible memory. It is shown how the suggested method can be\napplied to estimate the computer capacity. In particular, this consideration\ngives a new look at the organization of the memory of a computer. Obtained\nresults can be of some interest for practical applications\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 16:13:37 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Ryabko", "Boris", ""]]}, {"id": "1003.3704", "submitter": "Peiyush Jain", "authors": "Peiyush Jain", "title": "On a variant of Monotone NAE-3SAT and the Triangle-Free Cut problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define a restricted version of Monotone NAE-3SAT and show\nthat it remains NP-Complete even under that restriction. We expect this result\nwould be useful in proving NP-Completeness results for problems on\n$k$-colourable graphs ($k \\ge 5$). We also prove the NP-Completeness of the\nTriangle-Free Cut problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 02:45:38 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2010 19:32:37 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Jain", "Peiyush", ""]]}, {"id": "1003.3879", "submitter": "David Richerby", "authors": "Martin Dyer and David Richerby", "title": "An Effective Dichotomy for the Counting Constraint Satisfaction Problem", "comments": "31 pages. Corrected some errors from previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bulatov (2008) gave a dichotomy for the counting constraint satisfaction\nproblem #CSP. A problem from #CSP is characterised by a constraint language,\nwhich is a fixed, finite set of relations over a finite domain D. An instance\nof the problem uses these relations to constrain the variables in a larger set.\nBulatov showed that the problem of counting the satisfying assignments of\ninstances of any problem from #CSP is either in polynomial time (FP) or is\n#P-complete. His proof draws heavily on techniques from universal algebra and\ncannot be understood without a secure grasp of that field. We give an\nelementary proof of Bulatov's dichotomy, based on succinct representations,\nwhich we call frames, of a class of highly structured relations, which we call\nstrongly rectangular. We show that these are precisely the relations which are\ninvariant under a Mal'tsev polymorphism. En route, we give a simplification of\na decision algorithm for strongly rectangular constraint languages, due to\nBulatov and Dalmau (2006). We establish a new criterion for the #CSP dichotomy,\nwhich we call strong balance, and we prove that this property is decidable. In\nfact, we establish membership in NP. Thus, we show that the dichotomy is\neffective, resolving the most important open question concerning the #CSP\ndichotomy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2010 20:09:09 GMT"}, {"version": "v2", "created": "Tue, 24 Aug 2010 18:00:41 GMT"}, {"version": "v3", "created": "Sun, 31 Oct 2010 11:22:57 GMT"}, {"version": "v4", "created": "Wed, 17 Aug 2011 15:35:10 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Dyer", "Martin", ""], ["Richerby", "David", ""]]}, {"id": "1003.4029", "submitter": "Yakir Reshef", "authors": "Yakir Reshef, Salil Vadhan", "title": "On Extractors and Exposure-Resilient Functions for Sublogarithmic\n  Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deterministic extractors for oblivious bit-fixing sources (a.k.a.\nresilient functions) and exposure-resilient functions with small min-entropy:\nof the function's n input bits, k << n bits are uniformly random and unknown to\nthe adversary. We simplify and improve an explicit construction of extractors\nfor bit-fixing sources with sublogarithmic k due to Kamp and Zuckerman (SICOMP\n2006), achieving error exponentially small in k rather than polynomially small\nin k. Our main result is that when k is sublogarithmic in n, the short output\nlength of this construction (O(log k) output bits) is optimal for extractors\ncomputable by a large class of space-bounded streaming algorithms.\n  Next, we show that a random function is an extractor for oblivious bit-fixing\nsources with high probability if and only if k is superlogarithmic in n,\nsuggesting that our main result may apply more generally. In contrast, we show\nthat a random function is a static (resp. adaptive) exposure-resilient function\nwith high probability even if k is as small as a constant (resp. log log n). No\nexplicit exposure-resilient functions achieving these parameters are known.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 21:22:56 GMT"}, {"version": "v2", "created": "Sat, 29 May 2010 12:05:04 GMT"}, {"version": "v3", "created": "Sat, 11 Dec 2010 20:07:15 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Reshef", "Yakir", ""], ["Vadhan", "Salil", ""]]}, {"id": "1003.4679", "submitter": "Alexey Pospelov", "authors": "Alexey Pospelov", "title": "Bounds for Bilinear Complexity of Noncommutative Group Algebras", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of multiplication in noncommutative group algebras\nwhich is closely related to the complexity of matrix multiplication. We\ncharacterize such semisimple group algebras of the minimal bilinear complexity\nand show nontrivial lower bounds for the rest of the group algebras. These\nlower bounds are built on the top of Bl\\\"aser's results for semisimple algebras\nand algebras with large radical and the lower bound for arbitrary associative\nalgebras due to Alder and Strassen. We also show subquadratic upper bounds for\nall group algebras turning into \"almost linear\" provided the exponent of matrix\nmultiplication equals 2.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2010 16:07:11 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Pospelov", "Alexey", ""]]}, {"id": "1003.4719", "submitter": "Giorgi Japaridze", "authors": "Giorgi Japaridze", "title": "Introduction to clarithmetic I", "comments": null, "journal-ref": "Information and Computation 209 (2011), pp. 1312-1354", "doi": "10.1016/j.ic.2011.07.002", "report-no": null, "categories": "cs.LO cs.CC math.LO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Clarithmetic\" is a generic name for formal number theories similar to Peano\narithmetic, but based on computability logic (see\nhttp://www.cis.upenn.edu/~giorgi/cl.html) instead of the more traditional\nclassical or intuitionistic logics. Formulas of clarithmetical theories\nrepresent interactive computational problems, and their \"truth\" is understood\nas existence of an algorithmic solution. Imposing various complexity\nconstraints on such solutions yields various versions of clarithmetic. The\npresent paper introduces a system of clarithmetic for polynomial time\ncomputability, which is shown to be sound and complete. Sound in the sense that\nevery theorem T of the system represents an interactive number-theoretic\ncomputational problem with a polynomial time solution and, furthermore, such a\nsolution can be efficiently extracted from a proof of T. And complete in the\nsense that every interactive number-theoretic problem with a polynomial time\nsolution is represented by some theorem T of the system. The paper is written\nin a semitutorial style and targets readers with no prior familiarity with\ncomputability logic.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2010 19:33:44 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2010 06:29:45 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2010 15:45:55 GMT"}, {"version": "v4", "created": "Tue, 12 Jul 2011 13:58:06 GMT"}, {"version": "v5", "created": "Sun, 24 Jul 2011 11:50:45 GMT"}, {"version": "v6", "created": "Tue, 23 Aug 2011 12:10:10 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Japaridze", "Giorgi", ""]]}, {"id": "1003.4865", "submitter": "Oleg Verbitsky", "authors": "Oleg Pikhurko and Oleg Verbitsky", "title": "Logical complexity of graphs: a survey", "comments": "57 pages; 2 figures. This version contains an appendix with an\n  improvement of Theorem 4.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the definability of finite graphs in first-order logic with two\nrelation symbols for adjacency and equality of vertices. The logical depth\n$D(G)$ of a graph $G$ is equal to the minimum quantifier depth of a sentence\ndefining $G$ up to isomorphism. The logical width $W(G)$ is the minimum number\nof variables occurring in such a sentence. The logical length $L(G)$ is the\nlength of a shortest defining sentence. We survey known estimates for these\ngraph parameters and discuss their relations to other topics (such as the\nefficiency of the Weisfeiler-Lehman algorithm in isomorphism testing, the\nevolution of a random graph, quantitative characteristics of the zero-one law,\nor the contribution of Frank Ramsey to the research on Hilbert's\nEntscheidungsproblem). Also, we trace the behavior of the descriptive\ncomplexity of a graph as the logic becomes more restrictive (for example, only\ndefinitions with a bounded number of variables or quantifier alternations are\nallowed) or more expressible (after powering with counting quantifiers).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 11:48:59 GMT"}, {"version": "v2", "created": "Wed, 22 Dec 2010 13:00:27 GMT"}, {"version": "v3", "created": "Tue, 8 Feb 2011 13:47:29 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2013 08:29:29 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Pikhurko", "Oleg", ""], ["Verbitsky", "Oleg", ""]]}, {"id": "1003.5831", "submitter": "Matthew Szudzik", "authors": "Matthew P. Szudzik (Carnegie Mellon)", "title": "The Computable Universe Hypothesis", "comments": "33 pages, 0 figures; minor changes", "journal-ref": "A Computable Universe, World Scientific, 2013, pp. 479-523", "doi": "10.1142/9789814374309_0025", "report-no": null, "categories": "math.LO cs.CC cs.LO math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When can a model of a physical system be regarded as computable? We provide\nthe definition of a computable physical model to answer this question. The\nconnection between our definition and Kreisel's notion of a mechanistic theory\nis discussed, and several examples of computable physical models are given,\nincluding models which feature discrete motion, a model which features\nnon-discrete continuous motion, and probabilistic models such as radioactive\ndecay. We show how computable physical models on effective topological spaces\ncan be formulated using the theory of type-two effectivity (TTE). Various\ncommon operations on computable physical models are described, such as the\noperation of coarse-graining and the formation of statistical ensembles. The\ndefinition of a computable physical model also allows for a precise\nformalization of the computable universe hypothesis--the claim that all the\nlaws of physics are computable.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2010 18:28:26 GMT"}, {"version": "v2", "created": "Mon, 16 Aug 2010 12:34:25 GMT"}, {"version": "v3", "created": "Fri, 17 Dec 2010 16:14:51 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2012 01:55:08 GMT"}, {"version": "v5", "created": "Fri, 27 Jan 2012 20:52:16 GMT"}, {"version": "v6", "created": "Wed, 7 Aug 2013 20:28:31 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Szudzik", "Matthew P.", "", "Carnegie Mellon"]]}]