[{"id": "1611.00028", "submitter": "Zhengjun Cao", "authors": "Zhengjun Cao and Lihua Liu", "title": "A Note On One Realization of a Scalable Shor Algorithm", "comments": "6 pages, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, Monz, et al. [arXiv:1507.08852] have reported the\ndemonstration of factoring 15 using a scalable Shor algorithm with an ion-trap\nquantum computer. In this note, we remark that the report is somewhat\nmisleading because there are three flaws in the proposed circuit diagram of\nShor algorithm. We also remark that the principles behind the demonstration\nhave not been explained properly, including its correctness and complexity.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 02:04:42 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Cao", "Zhengjun", ""], ["Liu", "Lihua", ""]]}, {"id": "1611.00510", "submitter": "Yuki Takeuchi", "authors": "Yuki Takeuchi, Yasuhiro Takahashi", "title": "Ancilla-driven instantaneous quantum polynomial time circuit for quantum\n  supremacy", "comments": "6 pages, 4 figures", "journal-ref": "Phys. Rev. A 94, 062336 (2016)", "doi": "10.1103/PhysRevA.94.062336", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instantaneous quantum polynomial time (IQP) is a model of (probably)\nnon-universal quantum computation. Since it has been proven that IQP circuits\nare unlikely to be simulated classically up to a multiplicative error and an\nerror in the $l_1$ norm, IQP is considered as one of the promising classes that\ndemonstrates quantum supremacy. Although IQP circuits can be realized more\neasily than a universal quantum computer, demonstrating quantum supremacy is\nstill difficult. It is therefore desired to find subclasses of IQP that are\neasy to implement. In this paper, by imposing some restrictions on IQP, we\npropose ancilla-driven IQP (ADIQP) as the subclass of commuting quantum\ncomputation suitable for many experimental settings. We show that even though\nADIQP circuits are strictly weaker than IQP circuits in a sense, they are also\nhard to simulate classically up to a multiplicative error and an error in the\n$l_1$ norm. Moreover, the properties of ADIQP make it easy to investigate the\nverifiability of ADIQP circuits and the difficulties in realizing ADIQP\ncircuits.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 09:00:15 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 12:43:24 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Takeuchi", "Yuki", ""], ["Takahashi", "Yasuhiro", ""]]}, {"id": "1611.00783", "submitter": "William Hoza", "authors": "William M. Hoza, Adam R. Klivans", "title": "Preserving Randomness for Adaptive Algorithms", "comments": "To appear in RANDOM 2018. 32 pages, 2 figures. Added sections 1.5.3\n  and 7.1, changed terminology, fixed typos, improved presentation, added\n  appendix C, simplified abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose $\\mathsf{Est}$ is a randomized estimation algorithm that uses $n$\nrandom bits and outputs values in $\\mathbb{R}^d$. We show how to execute\n$\\mathsf{Est}$ on $k$ adaptively chosen inputs using only $n + O(k \\log(d +\n1))$ random bits instead of the trivial $nk$ (at the cost of mild increases in\nthe error and failure probability). Our algorithm combines a variant of the INW\npseudorandom generator (STOC '94) with a new scheme for shifting and rounding\nthe outputs of $\\mathsf{Est}$. We prove that modifying the outputs of\n$\\mathsf{Est}$ is necessary in this setting, and furthermore, our algorithm's\nrandomness complexity is near-optimal in the case $d \\leq O(1)$. As an\napplication, we give a randomness-efficient version of the Goldreich-Levin\nalgorithm; our algorithm finds all Fourier coefficients with absolute value at\nleast $\\theta$ of a function $F: \\{0, 1\\}^n \\to \\{-1, 1\\}$ using $O(n \\log n)\n\\cdot \\text{poly}(1/\\theta)$ queries to $F$ and $O(n)$ random bits (independent\nof $\\theta$), improving previous work by Bshouty et al. (JCSS '04).\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 20:06:31 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 20:48:30 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 04:16:22 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 19:16:52 GMT"}, {"version": "v5", "created": "Wed, 13 Jun 2018 16:32:22 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Hoza", "William M.", ""], ["Klivans", "Adam R.", ""]]}, {"id": "1611.00827", "submitter": "Fulvio Gesmundo", "authors": "Fulvio Gesmundo, Christian Ikenmeyer, Greta Panova", "title": "Geometric complexity theory and matrix powering", "comments": "21 pages. Final version to appear: Differential Geometry and its\n  Applications - Special issue in Geometry and Complexity Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valiant's famous determinant versus permanent problem is the flagship problem\nin algebraic complexity theory. Mulmuley and Sohoni (Siam J Comput 2001, 2008)\nintroduced geometric complexity theory, an approach to study this and related\nproblems via algebraic geometry and representation theory. Their approach works\nby multiplying the permanent polynomial with a high power of a linear form (a\nprocess called padding) and then comparing the orbit closures of the\ndeterminant and the padded permanent. This padding was recently used heavily to\nshow no-go results for the method of shifted partial derivatives (Efremenko,\nLandsberg, Schenck, Weyman, 2016) and for geometric complexity theory\n(Ikenmeyer Panova, FOCS 2016 and B\\\"urgisser, Ikenmeyer Panova, FOCS 2016).\nFollowing a classical homogenization result of Nisan (STOC 1991) we replace the\ndeterminant in geometric complexity theory with the trace of a variable matrix\npower. This gives an equivalent but much cleaner homogeneous formulation of\ngeometric complexity theory in which the padding is removed. This radically\nchanges the representation theoretic questions involved to prove complexity\nlower bounds. We prove that in this homogeneous formulation there are no orbit\noccurrence obstructions that prove even superlinear lower bounds on the\ncomplexity of the permanent. This is the first no-go result in geometric\ncomplexity theory that rules out superlinear lower bounds in some model.\nInterestingly---in contrast to the determinant---the trace of a variable matrix\npower is not uniquely determined by its stabilizer.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 22:06:09 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:12:25 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Gesmundo", "Fulvio", ""], ["Ikenmeyer", "Christian", ""], ["Panova", "Greta", ""]]}, {"id": "1611.00886", "submitter": "Marcel Jackson G", "authors": "Lucy Ham and Marcel Jackson", "title": "All or nothing: toward a promise problem dichotomy for constraint\n  problems", "comments": "Updated from version 1 to include new results. Updated from version 2\n  by some amendments and streamlined arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite constraint language $\\mathscr{R}$ is a finite set of relations over\nsome finite domain $A$. We show that intractability of the constraint\nsatisfaction problem $\\operatorname{CSP}(\\mathscr{R})$ can, in all known cases,\nbe replaced by an infinite hierarchy of intractable promise problems of\nincreasingly disparate promise conditions: where instances are guaranteed to\neither have no solutions at all, or to be $k$-robustly satisfiable (for any\nfixed $k$), meaning that every \"reasonable\" partial instantiation on~$k$\nvariables extends to a solution. For example, subject to the assumption\n$\\texttt{P}\\neq \\texttt{NP}$, then for any~$k$, we show that there is no\npolynomial time algorithm that can distinguish non-$3$-colourable graphs, from\nthose for which any reasonable $3$-colouring of any $k$ of the vertices can\nextend to a full $3$-colouring. Our main result shows that an analogous\nstatement holds for all known intractable constraint problems over fixed finite\nconstraint languages.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 05:40:08 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 02:52:03 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 12:53:38 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ham", "Lucy", ""], ["Jackson", "Marcel", ""]]}, {"id": "1611.00898", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Low Rank Approximation with Entrywise $\\ell_1$-Norm Error", "comments": "STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the $\\ell_1$-low rank approximation problem, where for a given $n\n\\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to\noutput a rank-$k$ matrix $\\widehat{A}$ for which\n  $$\\|A-\\widehat{A}\\|_1 \\leq \\alpha \\cdot \\min_{\\textrm{rank-}k\\textrm{\nmatrices}~A'}\\|A-A'\\|_1,$$ where for an $n \\times d$ matrix $C$, we let\n$\\|C\\|_1 = \\sum_{i=1}^n \\sum_{j=1}^d |C_{i,j}|$. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is\nindicated in models where Gaussian assumptions on the noise may not apply. The\nproblem was shown to be NP-hard by Gillis and Vavasis and a number of\nheuristics have been proposed. It was asked in multiple places if there are any\napproximation algorithms.\n  We give the first provable approximation algorithms for $\\ell_1$-low rank\napproximation, showing that it is possible to achieve approximation factor\n$\\alpha = (\\log d) \\cdot \\mathrm{poly}(k)$ in $\\mathrm{nnz}(A) + (n+d)\n\\mathrm{poly}(k)$ time, where $\\mathrm{nnz}(A)$ denotes the number of non-zero\nentries of $A$. If $k$ is constant, we further improve the approximation ratio\nto $O(1)$ with a $\\mathrm{poly}(nd)$-time algorithm. Under the Exponential Time\nHypothesis, we show there is no $\\mathrm{poly}(nd)$-time algorithm achieving a\n$(1+\\frac{1}{\\log^{1+\\gamma}(nd)})$-approximation, for $\\gamma > 0$ an\narbitrarily small constant, even when $k = 1$.\n  We give a number of additional results for $\\ell_1$-low rank approximation:\nnearly tight upper and lower bounds for column subset selection, CUR\ndecompositions, extensions to low rank approximation with respect to\n$\\ell_p$-norms for $1 \\leq p < 2$ and earthmover distance, low-communication\ndistributed protocols and low-memory streaming algorithms, algorithms with\nlimited randomness, and bicriteria algorithms. We also give a preliminary\nempirical evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 07:13:20 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:57:43 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1611.00911", "submitter": "Kasper Green Larsen", "authors": "Kasper Eenberg, Kasper Green Larsen, Huacheng Yu", "title": "DecreaseKeys are Expensive for External Memory Priority Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest open problems in external memory data structures is the\npriority queue problem with DecreaseKey operations. If only Insert and\nExtractMin operations need to be supported, one can design a comparison-based\npriority queue performing $O((N/B)\\lg_{M/B} N)$ I/Os over a sequence of $N$\noperations, where $B$ is the disk block size in number of words and $M$ is the\nmain memory size in number of words. This matches the lower bound for\ncomparison-based sorting and is hence optimal for comparison-based priority\nqueues. However, if we also need to support DecreaseKeys, the performance of\nthe best known priority queue is only $O((N/B) \\lg_2 N)$ I/Os. The big open\nquestion is whether a degradation in performance really is necessary. We answer\nthis question affirmatively by proving a lower bound of $\\Omega((N/B) \\lg_{\\lg\nN} B)$ I/Os for processing a sequence of $N$ intermixed Insert, ExtraxtMin and\nDecreaseKey operations. Our lower bound is proved in the cell probe model and\nthus holds also for non-comparison-based priority queues.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 08:30:38 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Eenberg", "Kasper", ""], ["Larsen", "Kasper Green", ""], ["Yu", "Huacheng", ""]]}, {"id": "1611.00918", "submitter": "Kasper Green Larsen", "authors": "Karl Bringmann, Allan Gr{\\o}nlund, Kasper Green Larsen", "title": "A Dichotomy for Regular Expression Membership Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regular expression membership testing: Given a regular expression of\nsize $m$ and a string of size $n$, decide whether the string is in the language\ndescribed by the regular expression. Its classic $O(nm)$ algorithm is one of\nthe big success stories of the 70s, which allowed pattern matching to develop\ninto the standard tool that it is today.\n  Many special cases of pattern matching have been studied that can be solved\nfaster than in quadratic time. However, a systematic study of tractable cases\nwas made possible only recently, with the first conditional lower bounds\nreported by Backurs and Indyk [FOCS'16]. Restricted to any \"type\" of\nhomogeneous regular expressions of depth 2 or 3, they either presented a\nnear-linear time algorithm or a quadratic conditional lower bound, with one\nexception known as the Word Break problem.\n  In this paper we complete their work as follows:\n  1) We present two almost-linear time algorithms that generalize all known\nalmost-linear time algorithms for special cases of regular expression\nmembership testing.\n  2) We classify all types, except for the Word Break problem, into\nalmost-linear time or quadratic time assuming the Strong Exponential Time\nHypothesis. This extends the classification from depth 2 and 3 to any constant\ndepth.\n  3) For the Word Break problem we give an improved $\\tilde{O}(n m^{1/3} + m)$\nalgorithm. Surprisingly, we also prove a matching conditional lower bound for\ncombinatorial algorithms. This establishes Word Break as the only intermediate\nproblem.\n  In total, we prove matching upper and lower bounds for any type of\nbounded-depth homogeneous regular expressions, which yields a full dichotomy\nfor regular expression membership testing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 08:55:07 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 06:44:04 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bringmann", "Karl", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1611.00934", "submitter": "Janosch Fuchs", "authors": "Hans-Joachim B\\\"ockenhauer, Janosch Fuchs, Ulla Karhum\\\"aki, Walter\n  Unger", "title": "Online Exploration of Rectangular Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of exploring unknown environments with\nautonomous agents. We model the environment as a graph with edge weights and\nanalyze the task of visiting all vertices of the graph at least once. The\nhardness of this task heavily depends on the knowledge and the capabilities of\nthe agent. In our model, the agent sees the whole graph in advance, but does\nnot know the weights of the edges. As soon as it arrives in a vertex, it can\nsee the weights of all the outgoing edges. We consider the special case of two\ndifferent edge weights $1$ and $k$ and prove that the problem remains hard even\nin this case. We prove a lower bound of $11/9$ on the competitive ratio of any\ndeterministic strategy for exploring a ladder graph and complement this result\nby a $4$-competitive algorithm. All of these results hold for undirected\ngraphs. Exploring directed graphs, where the direction of the edges is not\nknown beforehand, seems to be much harder. Here, we prove that a natural greedy\nstrategy has a linear lower bound on the competitive ratio both in ladders and\nsquare grids.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 09:48:13 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["B\u00f6ckenhauer", "Hans-Joachim", ""], ["Fuchs", "Janosch", ""], ["Karhum\u00e4ki", "Ulla", ""], ["Unger", "Walter", ""]]}, {"id": "1611.00975", "submitter": "Jiabao Lin", "authors": "Jiabao Lin and Hanpin Wang", "title": "The Complexity of Holant Problems over Boolean Domain with Non-negative\n  Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holant problem is a general framework to study the computational complexity\nof counting problems. We prove a complexity dichotomy theorem for Holant\nproblems over Boolean domain with non-negative weights. It is the first\ncomplete Holant dichotomy where constraint functions are not necessarily\nsymmetric.\n  Holant problems are indeed read-twice $\\#$CSPs. Intuitively, some $\\#$CSPs\nthat are $\\#$P-hard become tractable when restricting to read-twice instances.\nTo capture them, we introduce the Block-rank-one condition. It turns out that\nthe condition leads to a clear separation. If a function set $\\mathcal{F}$\nsatisfies the condition, then $\\mathcal{F}$ is of affine type or product type.\nOtherwise (a) $\\mathrm{Holant}(\\mathcal{F})$ is $\\#$P-hard; or (b) every\nfunction in $\\mathcal{F}$ is a tensor product of functions of arity at most 2;\nor (c) $\\mathcal{F}$ is transformable to a product type by some real orthogonal\nmatrix. Holographic transformations play an important role in both the hardness\nproof and the characterization of tractability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 12:23:59 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 05:50:55 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Lin", "Jiabao", ""], ["Wang", "Hanpin", ""]]}, {"id": "1611.01029", "submitter": "Sumit Kumar Jha", "authors": "Sumit Kumar Jha", "title": "On the Sum of Linear Coefficients of a Boolean Valued Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $f:\\{-1,1\\}^{n}\\rightarrow \\{-1,1\\}$ be a Boolean valued function having\ntotal degree $d$. Then a conjecture due to Servedio and Gopalan asserts that\n$\\sum_{i=1}^{n}\\widehat{f}(i)\\leq \\sum_{j=1}^{d}\\widehat{\\text{Maj}}_{d}(j)$\nwhere $\\text{Maj}_{d}$ is the majority function on $d$ bits. Here we give some\nalternative formalisms of this conjecture involving the discrete derivative\noperators on $f$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 14:10:17 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 09:06:47 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Jha", "Sumit Kumar", ""]]}, {"id": "1611.01090", "submitter": "Reinhard Pichler", "authors": "Wolfgang Fischl, Georg Gottlob and Reinhard Pichler", "title": "General and Fractional Hypertree Decompositions: Hard and Easy Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypertree decompositions, as well as the more powerful generalized hypertree\ndecompositions (GHDs), and the yet more general fractional hypertree\ndecompositions (FHD) are hypergraph decomposition methods successfully used for\nanswering conjunctive queries and for the solution of constraint satisfaction\nproblems. Every hypergraph H has a width relative to each of these\ndecomposition methods: its hypertree width hw(H), its generalized hypertree\nwidth ghw(H), and its fractional hypertree width fhw(H), respectively.\n  It is known that hw(H) <= k can be checked in polynomial time for fixed k,\nwhile checking ghw(H) <= k is NP-complete for any k greater than or equal to 3.\nThe complexity of checking fhw(H) <= k for a fixed k has been open for more\nthan a decade.\n  We settle this open problem by showing that checking fhw(H) <= k is\nNP-complete, even for k=2. The same construction allows us to prove also the\nNP-completeness of checking ghw(H) <= k for k=2. After proving these hardness\nresults, we identify meaningful restrictions, for which checking for bounded\nghw or fhw becomes tractable.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 16:54:37 GMT"}, {"version": "v10", "created": "Tue, 21 May 2019 08:22:25 GMT"}, {"version": "v11", "created": "Thu, 18 Jul 2019 11:38:17 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 17:48:37 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 14:49:35 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 11:26:00 GMT"}, {"version": "v5", "created": "Fri, 23 Jun 2017 08:55:29 GMT"}, {"version": "v6", "created": "Mon, 28 Aug 2017 09:29:25 GMT"}, {"version": "v7", "created": "Tue, 29 Aug 2017 11:13:24 GMT"}, {"version": "v8", "created": "Wed, 25 Jul 2018 14:20:36 GMT"}, {"version": "v9", "created": "Tue, 20 Nov 2018 10:50:34 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Fischl", "Wolfgang", ""], ["Gottlob", "Georg", ""], ["Pichler", "Reinhard", ""]]}, {"id": "1611.01190", "submitter": "Igor Carboni Oliveira", "authors": "Igor C. Oliveira, Rahul Santhanam", "title": "Conspiracies between Learning Algorithms, Circuit Lower Bounds and\n  Pseudorandomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove several results giving new and stronger connections between\nlearning, circuit lower bounds and pseudorandomness. Among other results, we\nshow a generic learning speedup lemma, equivalences between various learning\nmodels in the exponential time and subexponential time regimes, a dichotomy\nbetween learning and pseudorandomness, consequences of non-trivial learning for\ncircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,\nand NC$^1$-hardness for the Minimum Circuit Size Problem.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 21:08:38 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Oliveira", "Igor C.", ""], ["Santhanam", "Rahul", ""]]}, {"id": "1611.01291", "submitter": "Navid Talebanfard", "authors": "Pavel Pudl\\'ak, Dominik Scheder, Navid Talebanfard", "title": "Tighter Hard Instances for PPSZ", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2017.85", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct uniquely satisfiable $k$-CNF formulas that are hard for the\nalgorithm PPSZ. Firstly, we construct graph-instances on which \"weak PPSZ\" has\nsavings of at most $(2 + \\epsilon) / k$; the saving of an algorithm on an input\nformula with $n$ variables is the largest $\\gamma$ such that the algorithm\nsucceeds (i.e. finds a satisfying assignment) with probability at least $2^{ -\n(1 - \\gamma) n}$. Since PPSZ (both weak and strong) is known to have savings of\nat least $\\frac{\\pi^2 + o(1)}{6k}$, this is optimal up to the constant factor.\nIn particular, for $k=3$, our upper bound is $2^{0.333\\dots n}$, which is\nfairly close to the lower bound $2^{0.386\\dots n}$ of Hertli [SIAM J.\nComput.'14]. We also construct instances based on linear systems over\n$\\mathbb{F}_2$ for which strong PPSZ has savings of at most\n$O\\left(\\frac{\\log(k)}{k}\\right)$. This is only a $\\log(k)$ factor away from\nthe optimal bound. Our constructions improve previous savings upper bound of\n$O\\left(\\frac{\\log^2(k)}{k}\\right)$ due to Chen et al. [SODA'13].\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 09:27:34 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Pudl\u00e1k", "Pavel", ""], ["Scheder", "Dominik", ""], ["Talebanfard", "Navid", ""]]}, {"id": "1611.01328", "submitter": "Thorsten Wissmann", "authors": "Olaf Beyersdorff, Leroy Chew, Meena Mahajan, Anil Shukla", "title": "Feasible Interpolation for QBF Resolution Calculi", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 2 (June 8,\n  2017) lmcs:3702", "doi": "10.23638/LMCS-13(2:7)2017", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sharp contrast to classical proof complexity we are currently short of\nlower bound techniques for QBF proof systems. In this paper we establish the\nfeasible interpolation technique for all resolution-based QBF systems, whether\nmodelling CDCL or expansion-based solving. This both provides the first general\nlower bound method for QBF proof systems as well as largely extends the scope\nof classical feasible interpolation. We apply our technique to obtain new\nexponential lower bounds to all resolution-based QBF systems for a new class of\nQBF formulas based on the clique problem. Finally, we show how feasible\ninterpolation relates to the recently established lower bound method based on\nstrategy extraction.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 11:13:17 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 11:11:14 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 16:13:36 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Beyersdorff", "Olaf", ""], ["Chew", "Leroy", ""], ["Mahajan", "Meena", ""], ["Shukla", "Anil", ""]]}, {"id": "1611.01344", "submitter": "Shaull Almagor", "authors": "Shaull Almagor, Jo\\\"el Ouaknine, James Worrell", "title": "The Polytope-Collision Problem", "comments": "20 pages, 1 figure. Submitted to STOC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Orbit Problem consists of determining, given a matrix $A\\in\n\\mathbb{R}^{d\\times d}$ and vectors $x,y\\in \\mathbb{R}^d$, whether there exists\n$n\\in \\mathbb{N}$ such that $A^n=y$. This problem was shown to be decidable in\na seminal work of Kannan and Lipton in the 1980s. Subsequently, Kannan and\nLipton noted that the Orbit Problem becomes considerably harder when the target\n$y$ is replaced with a subspace of $\\mathbb{R}^d$. Recently, it was shown that\nthe problem is decidable for vector-space targets of dimension at most three,\nfollowed by another development showing that the problem is in PSPACE for\npolytope targets of dimension at most three. In this work, we take a dual look\nat the problem, and consider the case where the initial vector $x$ is replaced\nwith a polytope $P_1$, and the target is a polytope $P_2$. Then, the question\nis whether there exists $n\\in \\mathbb{N}$ such that $A^n P_1\\cap P_2\\neq\n\\emptyset$. We show that the problem can be decided in PSPACE for dimension at\nmost three. As in previous works, decidability in the case of higher dimensions\nis left open, as the problem is known to be hard for long-standing\nnumber-theoretic open problems.\n  Our proof begins by formulating the problem as the satisfiability of a\nparametrized family of sentences in the existential first-order theory of\nreal-closed fields. Then, after removing quantifiers, we are left with\ninstances of simultaneous positivity of sums of exponentials. Using techniques\nfrom transcendental number theory, and separation bounds on algebraic numbers,\nwe are able to solve such instances in PSPACE.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 12:00:41 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Almagor", "Shaull", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""]]}, {"id": "1611.01491", "submitter": "Anirbit Mukherjee", "authors": "Raman Arora, Amitabh Basu, Poorya Mianjy and Anirbit Mukherjee", "title": "Understanding Deep Neural Networks with Rectified Linear Units", "comments": "The poly(data) exact training algorithm has been improved to now be\n  applicable to any single hidden layer R^n-> R ReLU DNN and there is a cleaner\n  pseudocode for it given on page 8. Also now on page 7 there is a more precise\n  description about when and how the Zonotope construction improves on the\n  Theorem 4 of this paper, arXiv:1402.1869", "journal-ref": "ICLR 2028", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:54:50 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 20:25:56 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 17:38:11 GMT"}, {"version": "v4", "created": "Mon, 29 May 2017 20:06:50 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 17:17:14 GMT"}, {"version": "v6", "created": "Wed, 28 Feb 2018 02:23:47 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Arora", "Raman", ""], ["Basu", "Amitabh", ""], ["Mianjy", "Poorya", ""], ["Mukherjee", "Anirbit", ""]]}, {"id": "1611.01696", "submitter": "Jackson Abascal", "authors": "Jackson Abascal, Lane A. Hemaspaandra, Shir Maimon, and Daniel Rubery", "title": "Closure and Nonclosure Properties of the Compressible and Rankable Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.FL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rankable and compressible sets have been studied for more than a quarter\nof a century, ever since Allender [1] and Goldberg and Sipser [6] introduced\nthe formal study of polynomial-time ranking. Yet even after all that time,\nwhether the rankable and compressible sets are closed under the most important\nboolean and other operations remains essentially unexplored. The present paper\nstudies these questions for both polynomial-time and recursion-theoretic\ncompression and ranking, and for almost every case arrives at a Closed, a\nNot-Closed, or a Closed-Iff-Well-Known-Complexity-Classes-Collapse result for\nthe given operation. Even though compression and ranking classes are capturing\nsomething quite natural about the structure of sets, it turns out that they are\nquite fragile with respect to closure properties, and many fail to possess even\nthe most basic of closure properties. For example, we show that with respect to\nthe join (aka disjoint union) operation: the P-rankable sets are not closed,\nwhether the semistrongly P-rankable sets are closed is closely linked to\nwhether P = UP $\\cap$ coUP, and the strongly P-rankable sets are closed.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 20:31:58 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 14:50:46 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 03:44:04 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 21:37:30 GMT"}, {"version": "v5", "created": "Tue, 24 Jul 2018 20:56:20 GMT"}, {"version": "v6", "created": "Wed, 31 Oct 2018 00:28:13 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Abascal", "Jackson", ""], ["Hemaspaandra", "Lane A.", ""], ["Maimon", "Shir", ""], ["Rubery", "Daniel", ""]]}, {"id": "1611.01706", "submitter": "Eleni Bakali", "authors": "Eleni Bakali", "title": "Self-reducible with easy decision version counting problems admit\n  additive error approximation. Connections to counting complexity, exponential\n  time complexity, and circuit lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the class of counting problems,i.e. functions in $\\#$P, which are\nself reducible, and have easy decision version, i.e. for every input it is easy\nto decide if the value of the function $f(x)$ is zero. For example,\n$\\#$independent-sets of all sizes, is such a problem, and one of the hardest of\nthis class, since it is equivalent to $\\#$SAT under multiplicative\napproximation preserving reductions.\n  Using these two powerful properties, self reducibility and easy decision, we\nprove that all problems/ functions $f$ in this class can be approximated in\nprobabilistic polynomial time within an absolute exponential error\n$\\epsilon\\cdot 2^{n'}, \\forall\\epsilon>0$, which for many of those problems\n(when $n'=n+$constant) implies additive approximation to the fraction\n$f(x)/2^n$. (Where $n'$ is the amount of non-determinism of some associated\nNPTM).\n  Moreover we show that for all these problems we can have multiplicative error\nto the value $f(x)$, of any desired accuracy (i.e. a RAS), in time of order\n$2^{2n'/3}poly(n)$, which is strictly smaller than exhaustive search. We also\nshow that $f(x)<g(x)$ can be decided deterministically in time $g(x)poly(n),\n\\forall g$.\n  Finally we show that the Circuit Acceptance Probability Problem, which is\nrelated to derandomization and circuit lower bounds, can be solved with high\nprobability and in polynomial time, for the family of all circuits for which\nthe problems of counting either satisfying or unsatisfying assignments belong\nto TotP (which is the Karp-closure of self reducible problems with easy\ndecision version).\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 22:55:45 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bakali", "Eleni", ""]]}, {"id": "1611.01710", "submitter": "Stephen Gismondi", "authors": "E. R. Swart, S. J. Gismondi, N. R. Swart, C. E. Bell, A. Lee", "title": "Deciding Graph non-Hamiltonicity via a Closure Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matching and LP based heuristic algorithm that decides graph\nnon-Hamiltonicity. Each of the $n!$ Hamilton cycles in a complete directed\ngraph on $n+1$ vertices corresponds with each of the $n!$ $n$-permutation\nmatrices $P$, such that $p_{u,i}=1$ if and only if the $i^{th}$ arc in a cycle\nenters vertex $u$, starting and ending at vertex $n+1$. A graph instance ($G$)\nis initially coded as exclusion set $E$, whose members are pairs of components\nof $P$, $\\{p_{u,i} ,p_{v,i+1}\\}, i=1,n-1$, for each arc $(u,v)$ not in $G$. For\neach $\\{p_{u,i} ,p_{v,i+1}\\}\\in E$, the set of $P$ satisfying\n$p_{u,i}=p_{v,i+1}=1$ correspond with a set of cycles not in $G$. Accounting\nfor all arcs not in $G$, $E$ codes precisely the set of cycles not in $G$. A\ndoubly stochastic-like $\\mathcal{O}$($n^4$) formulation of the Hamilton cycle\ndecision problem is then constructed. Each $\\{p_{u,i} ,p_{v,j}\\}$ is coded as\nvariable $q_{u,i,v,j}$ such that the set of integer extrema is the set of all\npermutations. We model $G$ by setting each $q_{u,i,v,j}=0$ in correspondence\nwith each $\\{p_{u,i} ,p_{v,j}\\}\\in E$ such that for non-Hamiltonian $G$,\ninteger solutions cannot exist. We recognize non-Hamiltonicity by iteratively\ndeducing additional $q_{u,i,v,j}$ that can be set zero and expanding $E$ until\nthe formulation becomes infeasible, in which case we recognize that no integer\nsolutions exists i.e. $G$ is decided non-Hamiltonian. Over 100 non-Hamiltonian\ngraphs (10 through 104 vertices) and 2000 randomized 31 vertex non-Hamiltonian\ngraphs are tested and correctly decided non-Hamiltonian. For Hamiltonian $G$,\nthe complement of $E$ provides information about covers of matchings, perhaps\nuseful in searching for cycles. We also present an example where the algorithm\nfails to deduce any integral value for any $q_{u,i,v,j}$ i.e. $G$ is undecided.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 23:38:23 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Swart", "E. R.", ""], ["Gismondi", "S. J.", ""], ["Swart", "N. R.", ""], ["Bell", "C. E.", ""], ["Lee", "A.", ""]]}, {"id": "1611.01823", "submitter": "Cornelius Brand", "authors": "Cornelius Brand, Marc Roth", "title": "Parameterized counting of trees, forests and matroid bases", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the complexity of counting trees, forests and bases of\nmatroids from a parameterized point of view. It turns out that the problems of\ncomputing the number of trees and forests with $k$ edges are $\\# W[1]$-hard\nwhen parameterized by $k$. Together with the recent algorithm for deterministic\nmatrix truncation by Lokshtanov et al. (ICALP 2015), the hardness result for\n$k$-forests implies $\\# W[1]$-hardness of the problem of counting bases of a\nmatroid when parameterized by rank or nullity, even if the matroid is\nrestricted to be representable over a field of characteristic $2$. We\ncomplement this result by pointing out that the problem becomes fixed parameter\ntractable for matroids represented over a fixed finite field.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 18:53:56 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Brand", "Cornelius", ""], ["Roth", "Marc", ""]]}, {"id": "1611.02315", "submitter": "Jacob Steinhardt", "authors": "Moses Charikar and Jacob Steinhardt and Gregory Valiant", "title": "Learning from Untrusted Data", "comments": "Updated based on STOC camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of theoretical results in machine learning and statistics\nassume that the available training data is a reasonably reliable reflection of\nthe phenomena to be learned or estimated. Similarly, the majority of machine\nlearning and statistical techniques used in practice are brittle to the\npresence of large amounts of biased or malicious data. In this work we consider\ntwo frameworks in which to study estimation, learning, and optimization in the\npresence of significant fractions of arbitrary data.\n  The first framework, list-decodable learning, asks whether it is possible to\nreturn a list of answers, with the guarantee that at least one of them is\naccurate. For example, given a dataset of $n$ points for which an unknown\nsubset of $\\alpha n$ points are drawn from a distribution of interest, and no\nassumptions are made about the remaining $(1-\\alpha)n$ points, is it possible\nto return a list of $\\operatorname{poly}(1/\\alpha)$ answers, one of which is\ncorrect? The second framework, which we term the semi-verified learning model,\nconsiders the extent to which a small dataset of trusted data (drawn from the\ndistribution in question) can be leveraged to enable the accurate extraction of\ninformation from a much larger but untrusted dataset (of which only an\n$\\alpha$-fraction is drawn from the distribution).\n  We show strong positive results in both settings, and provide an algorithm\nfor robust learning in a very general stochastic optimization setting. This\ngeneral result has immediate implications for robust estimation in a number of\nsettings, including for robustly estimating the mean of distributions with\nbounded second moments, robustly learning mixtures of such distributions, and\nrobustly finding planted partitions in random graphs in which significant\nportions of the graph have been perturbed by an adversary.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 17:48:31 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Charikar", "Moses", ""], ["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""]]}, {"id": "1611.03069", "submitter": "Venkata Gandikota", "authors": "Venkata Gandikota, Badih Ghazi, Elena Grigorescu", "title": "NP-Hardness of Reed-Solomon Decoding, and the Prouhet-Tarry-Escott\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing the complexity of {\\em Bounded Distance Decoding} for\nReed-Solomon codes is a fundamental open problem in coding theory, explicitly\nasked by Guruswami and Vardy (IEEE Trans. Inf. Theory, 2005). The problem is\nmotivated by the large current gap between the regime when it is NP-hard, and\nthe regime when it is efficiently solvable (i.e., the Johnson radius).\n  We show the first NP-hardness results for asymptotically smaller decoding\nradii than the maximum likelihood decoding radius of Guruswami and Vardy.\nSpecifically, for Reed-Solomon codes of length $N$ and dimension $K=O(N)$, we\nshow that it is NP-hard to decode more than $ N-K- c\\frac{\\log N}{\\log\\log N}$\nerrors (with $c>0$ an absolute constant). Moreover, we show that the problem is\nNP-hard under quasipolynomial-time reductions for an error amount $> N-K-\nc\\log{N}$ (with $c>0$ an absolute constant).\n  These results follow from the NP-hardness of a generalization of the\nclassical Subset Sum problem to higher moments, called {\\em Moments Subset\nSum}, which has been a known open problem, and which may be of independent\ninterest.\n  We further reveal a strong connection with the well-studied\nProuhet-Tarry-Escott problem in Number Theory, which turns out to capture a\nmain barrier in extending our techniques. We believe the Prouhet-Tarry-Escott\nproblem deserves further study in the theoretical computer science community.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 20:15:55 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Gandikota", "Venkata", ""], ["Ghazi", "Badih", ""], ["Grigorescu", "Elena", ""]]}, {"id": "1611.03473", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures", "comments": "Changes from v1: Revised presentation. Added more applications of the\n  technique (SQ lower bounds for robust sparse mean estimation and robust\n  covariance estimation in spectral norm). Sharpened testing lower bound to\n  linear in the dimension (compared to nearly-linear in first version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 20:32:48 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:48:34 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1611.03739", "submitter": "Hendrik Molter", "authors": "Henning Fernau, Till Fluschnik, Danny Hermelin, Andreas Krebs, Hendrik\n  Molter, Rolf Niedermeier", "title": "Diminishable Parameterized Problems and Strict Polynomial Kernelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization---a mathematical key concept for provably effective\npolynomial-time preprocessing of NP-hard problems---plays a central role in\nparameterized complexity and has triggered an extensive line of research. This\nis in part due to a lower bounds framework that allows to exclude\npolynomial-size kernels under the assumption of NP $\\nsubseteq$ coNP$/$poly. In\nthis paper we consider a restricted yet natural variant of kernelization,\nnamely strict kernelization, where one is not allowed to increase the parameter\nof the reduced instance (the kernel) by more than an additive constant.\n  Building on earlier work of Chen, Flum, and M\\\"{u}ller [Theory Comput. Syst.\n2011] and developing a general and remarkably simple framework, we show that a\nvariety of FPT problems does not admit strict polynomial kernels under the\nweaker assumption of P $\\neq$ NP. In particular, we show that various\n(multicolored) graph problems and Turing machine computation problems do not\nadmit strict polynomial kernels unless P $=$ NP. To this end, a key concept we\nuse are diminishable problems; these are parameterized problems that allow to\ndecrease the parameter of the input instance by at least one in polynomial\ntime, thereby outputting an equivalent problem instance. Finally, we study a\nrelaxation of the notion of strict kernels and reveal its limitations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:17:01 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 17:06:48 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 17:11:01 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Fernau", "Henning", ""], ["Fluschnik", "Till", ""], ["Hermelin", "Danny", ""], ["Krebs", "Andreas", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1611.04178", "submitter": "Luca Grilli", "authors": "Luca Grilli", "title": "On the $\\mathcal{NP}$-hardness of GRacSim Drawing and k-SEFE Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of two problems in simultaneous graph drawing. The\nfirst problem, GRacSim Drawing, asks for finding a simultaneous geometric\nembedding of two graphs such that only crossings at right angles are allowed.\nThe second problem, k-SEFE, is a restricted version of the topological\nsimultaneous embedding with fixed edges (SEFE) problem, for two planar graphs,\nin which every private edge may receive at most $k$ crossings, where $k$ is a\nprescribed positive integer. We show that GRacSim Drawing is\n$\\mathcal{NP}$-hard and that k-SEFE is $\\mathcal{NP}$-complete. The\n$\\mathcal{NP}$-hardness of both problems is proved using two similar reductions\nfrom 3-Partition.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 20:06:33 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Grilli", "Luca", ""]]}, {"id": "1611.04843", "submitter": "Sergey Volkov", "authors": "Sergey Volkov", "title": "Finite Bases with Respect to the Superposition in Classes of Elementary\n  Recursive Functions, dissertation", "comments": "translated from Russian by Dina Kunets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a thesis that was defended in 2009 at Lomonosov Moscow State\nUniversity.\n  In Chapter 1:\n  1. It is proved that that the class of lower (Skolem) elementary functions is\nthe set of all polynomial-bounded functions that can be obtained by a\ncomposition of $x+1$, $xy$, $\\max(x-y,0)$, $x\\wedge y$, $\\lfloor x/y \\rfloor$,\nand one exponential function ($2^x$ or $x^y$) using formulas that have no more\nthan 2 floors with respect to an exponent (for example, $(x+y)^{xy+z}+1$ has 2\nfloors, $2^{2^x}$ has 3 floors). Here $x\\wedge y$ is a bitwise AND of $x$ and\n$y$.\n  2. It is proved that $\\{x+y,\\ \\max(x-y,0),\\ x\\wedge y,\\ \\lfloor x/y \\rfloor,\\\n2^{\\lfloor \\log_2 x \\rfloor^2}\\}$ and $\\{x+y,\\ \\max(x-y,0),\\ x\\wedge y,\\\n\\lfloor x/y \\rfloor,\\ x^{\\lfloor \\log_2 y \\rfloor}\\}$ are composition bases in\nthe functional version of the uniform $\\mathrm{TC}^0$ (also known as\n$\\mathrm{FOM}$).\n  3. The hierarchy of classes exhausting the class of elementary functions is\ndescribed in terms of compositions with restrictions on a number of floors in a\nformula.\n  The results of Chapter 1 are published in:\n  1) Volkov S.A. An exponential expansion of the Skolem-elementary functions,\nand bounded superpositions of simple arithmetic functions (in Russian),\nMathematical Problems of Cybernetics, Moscow, Fizmatlit, 2007, vol. 16, pp.\n163-190\n  2) doi:10.1134/S1064562407040217\n  In Chapter 2 a simple composition basis in the class ${\\cal E}^2$ of\nGrzegorczyk hierarchy is described. This result is published in DOI:\n10.1515/156939206779238436\n  In Chapter 3 it is proved that the group of permutations\n$\\mathrm{Gr}(Q)=\\{f:\\ f,f^{-1}\\in Q\\}$ is generated by two permutations for\nmany classes $Q$. For example, this is proved for $Q=\\mathrm{FP}$, where\n$\\mathrm{FP}$ is the class of all polynomial-time computable functions (of the\nlength of input). The results of chapter 3 are published in DOI:\n10.1515/DMA.2008.046\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 11:34:55 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 18:17:32 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 20:42:21 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Volkov", "Sergey", ""]]}, {"id": "1611.04999", "submitter": "Cyrus Rashtchian", "authors": "Paul Beame and Cyrus Rashtchian", "title": "Massively-Parallel Similarity Join, Edge-Isoperimetry, and Distance\n  Correlations on the Hypercube", "comments": "23 pages, plus references and appendix. To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed protocols for finding all pairs of similar vectors in a\nlarge dataset. Our results pertain to a variety of discrete metrics, and we\ngive concrete instantiations for Hamming distance. In particular, we give\nimproved upper bounds on the overhead required for similarity defined by\nHamming distance $r>1$ and prove a lower bound showing qualitative optimality\nof the overhead required for similarity over any Hamming distance $r$. Our main\nconceptual contribution is a connection between similarity search algorithms\nand certain graph-theoretic quantities. For our upper bounds, we exhibit a\ngeneral method for designing one-round protocols using edge-isoperimetric\nshapes in similarity graphs. For our lower bounds, we define a new\ncombinatorial optimization problem, which can be stated in purely\ngraph-theoretic terms yet also captures the core of the analysis in previous\ntheoretical work on distributed similarity joins. As one of our main technical\nresults, we prove new bounds on distance correlations in subsets of the Hamming\ncube.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 19:36:28 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Beame", "Paul", ""], ["Rashtchian", "Cyrus", ""]]}, {"id": "1611.05086", "submitter": "Daniel Valenzuela", "authors": "Romeo Rizzi, Massimo Cairo, Veli M\\\"akinen, Alexandru I. Tomescu,\n  Daniel Valenzuela", "title": "Hardness of Covering Alignment: Phase Transition in Post-Sequence\n  Genomics", "comments": null, "journal-ref": "IEEE/ACM Trans. on Computational Biology and Bioinformatics, 30\n  April 2018", "doi": "10.1109/TCBB.2018.2831691", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covering alignment problems arise from recent developments in genomics; so\ncalled pan-genome graphs are replacing reference genomes, and advances in\nhaplotyping enable full content of diploid genomes to be used as basis of\nsequence analysis. In this paper, we show that the computational complexity\nwill change for natural extensions of alignments to pan-genome representations\nand to diploid genomes. More broadly, our approach can also be seen as a\nminimal extension of sequence alignment to labelled directed acyclic graphs\n(labeled DAGs). Namely, we show that finding a \\emph{covering alignment} of two\nlabeled DAGs is NP-hard even on binary alphabets. A covering alignment asks for\ntwo paths $R_1$ (red) and $G_1$ (green) in DAG $D_1$ and two paths $R_2$ (red)\nand $G_2$ (green) in DAG $D_2$ that cover the nodes of the graphs and maximize\nthe sum of the global alignment scores:\n$\\mathsf{as}(\\mathsf{sp}(R_1),\\mathsf{sp}(R_2))+\\mathsf{as}(\\mathsf{sp}(G_1),\\mathsf{sp}(G_2))$,\nwhere $\\mathsf{sp}(P)$ is the concatenation of labels on the path $P$.\nPair-wise alignment of haplotype sequences forming a diploid chromosome can be\nconverted to a two-path coverable labelled DAG, and then the covering alignment\nmodels the similarity of two diploids over arbitrary recombinations. We also\ngive a reduction to the other direction, to show that such a\nrecombination-oblivious diploid alignment is NP-hard on alphabets of size $3$.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 22:47:19 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 17:48:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Rizzi", "Romeo", ""], ["Cairo", "Massimo", ""], ["M\u00e4kinen", "Veli", ""], ["Tomescu", "Alexandru I.", ""], ["Valenzuela", "Daniel", ""]]}, {"id": "1611.05558", "submitter": "Josh Alman", "authors": "Josh Alman and Ryan Williams", "title": "Probabilistic Rank and Matrix Rigidity", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": "In ACM Symposium on Theory of Computing (STOC), 2017", "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a notion of probabilistic rank and probabilistic sign-rank of a\nmatrix, which measures the extent to which a matrix can be probabilistically\nrepresented by low-rank matrices. We demonstrate several connections with\nmatrix rigidity, communication complexity, and circuit lower bounds, including:\n  The Walsh-Hadamard Transform is Not Very Rigid. We give surprising upper\nbounds on the rigidity of a family of matrices whose rigidity has been\nextensively studied, and was conjectured to be highly rigid. For the $2^n\n\\times 2^n$ Walsh-Hadamard transform $H_n$ (a.k.a. Sylvester matrices, or the\ncommunication matrix of Inner Product mod 2), we show how to modify only\n$2^{\\epsilon n}$ entries in each row and make the rank drop below\n$2^{n(1-\\Omega(\\epsilon^2/\\log(1/\\epsilon)))}$, for all $\\epsilon > 0$, over\nany field. That is, it is not possible to prove arithmetic circuit lower bounds\non Hadamard matrices, via L. Valiant's matrix rigidity approach. We also show\nnon-trivial rigidity upper bounds for $H_n$ with smaller target rank.\n  Matrix Rigidity and Threshold Circuit Lower Bounds. We give new consequences\nof rigid matrices for Boolean circuit complexity. We show that explicit $n\n\\times n$ Boolean matrices which maintain rank at least $2^{(\\log\nn)^{1-\\delta}}$ after $n^2/2^{(\\log n)^{\\delta/2}}$ modified entries would\nyield a function lacking sub-quadratic-size $AC^0$ circuits with two layers of\narbitrary linear threshold gates. We also prove that explicit 0/1 matrices over\n$\\mathbb{R}$ which are modestly more rigid than the best known rigidity lower\nbounds for sign-rank would imply strong lower bounds for the infamously\ndifficult class $THR\\circ THR$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 04:12:19 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 21:11:32 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Ryan", ""]]}, {"id": "1611.05564", "submitter": "Mark Zhandry", "authors": "Mark Zhandry", "title": "A Note on Quantum-Secure PRPs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to construct pseudorandom permutations (PRPs) that remain secure\neven if the adversary can query the permutation on a quantum superposition of\ninputs. Such PRPs are called \\emph{quantum-secure}. Our construction combines a\nquantum-secure pseudorandom \\emph{function} together with constructions of\n\\emph{classical} format preserving encryption. By combining known results, we\nobtain the first quantum-secure PRP in this model whose security relies only on\nthe existence of one-way functions. Previously, to the best of the author's\nknowledge, quantum security of PRPs had to be assumed, and there were no prior\nsecurity reductions to simpler primitives, let alone one-way functions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 05:09:48 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 18:17:26 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Zhandry", "Mark", ""]]}, {"id": "1611.05633", "submitter": "Slavcho Shtrakov", "authors": "Slavcho Shtrakov", "title": "Minor complexities of finite operations", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present a new class of complexity measures, induced by a new\ndata structure for representing $k$-valued functions (operations), called minor\ndecision diagram. The results are presented in terms of Multi-Valued Logic\ncircuits (MVL-circuits), ordered decision diagrams, formulas and minor\ndecomposition trees.\n  When assigning values to some variables in a function $f$ the resulting\nfunction is a subfunction of $f$, and when identifying some variables the\nresulting function is a minor of $f$. A set $M$ of essential variables in $f$\nis separable if there is a subfunction of $f$, whose set of essential variables\nis $M$. The essential arity gap $gap(f)$ of the function $f$ is the minimum\nnumber of essential variables in $f$ which become fictive when identifying\ndistinct essential variables in $f$. We prove that, if a function $f$ has\nnon-trivial arity gap ($gap(f)\\ge 2$), then all sets of essential variables in\n$f$ are separable. We define equivalence relations which classify the functions\nof $k$-valued logic into classes with the same minor complexities. These\nrelations induce transformation groups which are compared with the subgroups of\nthe restricted affine group (RAG) and the groups determined by the equivalence\nrelations with respect to the subfunctions, implementations and separable sets\nin functions. These methods provide a detailed classification of $n$-ary\n$k$-valued functions for small values of $n$ and $k$.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 10:59:53 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Shtrakov", "Slavcho", ""]]}, {"id": "1611.05641", "submitter": "\\\"Amin Baumeler", "authors": "\\\"Amin Baumeler and Stefan Wolf", "title": "Computational tameness of classical non-causal models", "comments": "7 pages, 3 figures, 1 algorithm, revised", "journal-ref": "Proceedings of the Royal Society A 474, 20170698, 2018", "doi": "10.1098/rspa.2017.0698", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the computational power of the non-causal circuit model, i.e.,\nthe circuit model where the assumption of a global causal order is replaced by\nthe assumption of logical consistency, is completely characterized by the\ncomplexity class~$\\operatorname{\\mathsf{UP}}\\cap\\operatorname{\\mathsf{coUP}}$.\nAn example of a problem in that class is factorization. Our result implies that\nclassical deterministic closed timelike curves (CTCs) cannot efficiently solve\nproblems that lie outside of that class. Thus, in stark contrast to other CTC\nmodels, these CTCs cannot efficiently\nsolve~$\\operatorname{\\mathsf{NP-complete}}$ problems,\nunless~$\\operatorname{\\mathsf{NP}}=\\operatorname{\\mathsf{UP}}\\cap\\operatorname{\\mathsf{coUP}}=\\operatorname{\\mathsf{coNP}}$,\nwhich lets their existence in nature appear less implausible. This result gives\na new characterization\nof~$\\operatorname{\\mathsf{UP}}\\cap\\operatorname{\\mathsf{coUP}}$ in terms of\nfixed points.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 11:47:11 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 11:41:13 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Baumeler", "\u00c4min", ""], ["Wolf", "Stefan", ""]]}, {"id": "1611.05754", "submitter": "Robin Kothari", "authors": "Anurag Anshu, Shalev Ben-David, Ankit Garg, Rahul Jain, Robin Kothari,\n  Troy Lee", "title": "Separating quantum communication and approximate rank", "comments": "34 pages", "journal-ref": "32nd Conference on Computational Complexity (CCC 2017), Leibniz\n  International Proceedings in Informatics (LIPIcs) 79, pp. 24:1-24:33 (2017)", "doi": "10.4230/LIPIcs.CCC.2017.24", "report-no": "MIT-CTP #4857", "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the best lower bound methods for the quantum communication complexity\nof a function H (with or without shared entanglement) is the logarithm of the\napproximate rank of the communication matrix of H. This measure is essentially\nequivalent to the approximate gamma_2 norm and generalized discrepancy, and\nsubsumes several other lower bounds. All known lower bounds on quantum\ncommunication complexity in the general unbounded-round model can be shown via\nthe logarithm of approximate rank, and it was an open problem to give any\nseparation at all between quantum communication complexity and the logarithm of\nthe approximate rank.\n  In this work we provide the first such separation: We exhibit a total\nfunction H with quantum communication complexity almost quadratically larger\nthan the logarithm of its approximate rank. We construct H using the\ncommunication lookup function framework of Anshu et al. (FOCS 2016) based on\nthe cheat sheet framework of Aaronson et al. (STOC 2016). From a starting\nfunction F, this framework defines a new function H=F_G. Our main technical\nresult is a lower bound on the quantum communication complexity of F_G in terms\nof the discrepancy of F, which we do via quantum information theoretic\narguments. We show the upper bound on the approximate rank of F_G by relating\nit to the Boolean circuit size of the starting function F.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:05:02 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Anshu", "Anurag", ""], ["Ben-David", "Shalev", ""], ["Garg", "Ankit", ""], ["Jain", "Rahul", ""], ["Kothari", "Robin", ""], ["Lee", "Troy", ""]]}, {"id": "1611.05819", "submitter": "Christopher Porter", "authors": "Cameron Fraize and Christopher P. Porter", "title": "Kolmogorov complexity and generalized length functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kolmogorov complexity measures the algorithmic complexity of a finite binary\nstring $\\sigma$ in terms of the length of the shortest description $\\sigma^*$\nof $\\sigma$. Traditionally, the length of a string is taken to measure the\namount of information contained in the string. However, we may also view the\nlength of $\\sigma$ as a measure of the cost of producing $\\sigma$, which\npermits one to generalize the notion of length, wherein the cost of producing a\n0 or a 1 can vary in some prescribed manner.\n  In this article, we initiate the study of this generalization of length based\non the above information cost interpretation. We also modify the definition of\nKolmogorov complexity to use such generalized length functions instead of\nstandard length. We further investigate conditions under which the notion of\ncomplexity defined in terms of a given generalized length function preserves\nsome essential properties of Kolmogorov complexity. We focus on a specific\nclass of generalized length functions that are intimately related to a specific\nsubcollection of Bernoulli $p$-measures, namely those corresponding to the\nunique computable real $p\\in(0,1)$ such that $p^k=1-p$, for integers $k\\geq 1$.\nWe then study randomness with respect to such measures, by proving a\ngeneralization version of the classic Levin-Schnorr theorem that involves\n$k$-length functions and then proving subsequent results that involve effective\ndimension and entropy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 19:07:44 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 18:55:40 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Fraize", "Cameron", ""], ["Porter", "Christopher P.", ""]]}, {"id": "1611.05991", "submitter": "Pasin Manurangsi", "authors": "Pasin Manurangsi", "title": "Almost-Polynomial Ratio ETH-Hardness of Approximating Densest\n  $k$-Subgraph", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Densest $k$-Subgraph problem, given an undirected graph $G$ and an\ninteger $k$, the goal is to find a subgraph of $G$ on $k$ vertices that\ncontains maximum number of edges. Even though the state-of-the-art algorithm\nfor the problem achieves only $O(n^{1/4 + \\varepsilon})$ approximation ratio\n(Bhaskara et al., 2010), previous attempts at proving hardness of\napproximation, including those under average case assumptions, fail to achieve\na polynomial ratio; the best ratios ruled out under any worst case assumption\nand any average case assumption are only any constant (Raghavendra and Steurer,\n2010) and $2^{\\Omega(\\log^{2/3} n)}$ (Alon et al., 2011) respectively.\n  In this work, we show, assuming the exponential time hypothesis (ETH), that\nthere is no polynomial-time algorithm that approximates Densest $k$-Subgraph to\nwithin $n^{1/(\\log \\log n)^c}$ factor of the optimum, where $c > 0$ is a\nuniversal constant independent of $n$. In addition, our result has \"perfect\ncompleteness\", meaning that we prove that it is ETH-hard to even distinguish\nbetween the case in which $G$ contains a $k$-clique and the case in which every\ninduced $k$-subgraph of $G$ has density at most $1/n^{-1/(\\log \\log n)^c}$ in\npolynomial time.\n  Moreover, if we make a stronger assumption that there is some constant\n$\\varepsilon > 0$ such that no subexponential-time algorithm can distinguish\nbetween a satisfiable 3SAT formula and one which is only $(1 -\n\\varepsilon)$-satisfiable (also known as Gap-ETH), then the ratio above can be\nimproved to $n^{f(n)}$ for any function $f$ whose limit is zero as $n$ goes to\ninfinity (i.e. $f \\in o(1)$).\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 07:05:00 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 08:23:21 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Manurangsi", "Pasin", ""]]}, {"id": "1611.06385", "submitter": "Jop Bri\\\"et", "authors": "Jop Bri\\\"et", "title": "On Embeddings of $\\ell_1^k$ from Locally Decodable Codes", "comments": "Appeared earlier on ECCC (http://eccc.hpi-web.de/report/2015/086/).\n  This version has a slightly shorter abstract and slightly edited\n  introduction. Removed left-over notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any $q$-query locally decodable code (LDC) gives a copy of\n$\\ell_1^k$ with small distortion in the Banach space of $q$-linear forms on\n$\\ell_{p_1}^N\\times\\cdots\\times\\ell_{p_q}^N$, provided $1/p_1 + \\cdots + 1/p_q\n\\leq 1$ and where $k$, $N$, and the distortion are simple functions of the code\nparameters. We exhibit the copy of $\\ell_1^k$ by constructing a basis for it\ndirectly from \"smooth\" LDC decoders. Based on this, we give alternative proofs\nfor known lower bounds on the length of 2-query LDCs. Using similar techniques,\nwe reprove known lower bounds for larger $q$. We also discuss the relation with\nan alternative proof, due to Pisier, of a result of Naor, Regev, and the author\non cotype properties of projective tensor products of $\\ell_p$ spaces.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 15:39:20 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 12:04:37 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Bri\u00ebt", "Jop", ""]]}, {"id": "1611.06593", "submitter": "Tony Huynh", "authors": "Yohann Benchetrit, Samuel Fiorini, Tony Huynh and Stefan Weltge", "title": "Characterizing Polytopes Contained in the $0/1$-Cube with Bounded\n  Chv\\'atal-Gomory Rank", "comments": "10 pages. Changed term 'pitch' to 'notch'. Removed 'Extended\n  Formulations' section since those results have been subsumed by\n  https://arxiv.org/abs/1711.01358", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $S \\subseteq \\{0,1\\}^n$ and $R$ be any polytope contained in $[0,1]^n$\nwith $R \\cap \\{0,1\\}^n = S$. We prove that $R$ has bounded Chv\\'atal-Gomory\nrank (CG-rank) provided that $S$ has bounded notch and bounded gap, where the\nnotch is the minimum integer $p$ such that all $p$-dimensional faces of the\n$0/1$-cube have a nonempty intersection with $S$, and the gap is a measure of\nthe size of the facet coefficients of $\\mathsf{conv}(S)$.\n  Let $H[\\bar{S}]$ denote the subgraph of the $n$-cube induced by the vertices\nnot in $S$. We prove that if $H[\\bar{S}]$ does not contain a subdivision of a\nlarge complete graph, then both the notch and the gap are bounded. By our main\nresult, this implies that the CG-rank of $R$ is bounded as a function of the\ntreewidth of $H[\\bar{S}]$. We also prove that if $S$ has notch $3$, then the\nCG-rank of $R$ is always bounded. Both results generalize a recent theorem of\nCornu\\'ejols and Lee, who proved that the CG-rank is bounded by a constant if\nthe treewidth of $H[\\bar{S}]$ is at most $2$.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 21:10:07 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 14:29:40 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 20:11:51 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Benchetrit", "Yohann", ""], ["Fiorini", "Samuel", ""], ["Huynh", "Tony", ""], ["Weltge", "Stefan", ""]]}, {"id": "1611.06632", "submitter": "Aaron Potechin", "authors": "Aaron Potechin", "title": "A Note on Amortized Branching Program Complexity", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that while almost all functions require exponential\nsize branching programs to compute, for all functions $f$ there is a branching\nprogram computing a doubly exponential number of copies of $f$ which has linear\nsize per copy of $f$. This result disproves a conjecture about non-uniform\ncatalytic computation, rules out a certain type of bottleneck argument for\nproving non-monotone space lower bounds, and can be thought of as a\nconstructive analogue of Razborov's result that submodular complexity measures\nhave maximum value $O(n)$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 01:58:12 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 01:36:14 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Potechin", "Aaron", ""]]}, {"id": "1611.06650", "submitter": "Hamed Hatami", "authors": "Yuval Dagan, Yuval Filmus, Hamed Hatami, Yaqiao Li", "title": "Trading information complexity for error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the standard two-party communication model. The central problem\nstudied in this article is how much one can save in information complexity by\nallowing an error of $\\epsilon$.\n  For arbitrary functions, we obtain lower bounds and upper bounds indicating a\ngain that is of order $\\Omega(h(\\epsilon))$ and $O(h(\\sqrt{\\epsilon}))$. Here\n$h$ denotes the binary entropy function. We analyze the case of the two-bit AND\nfunction in detail to show that for this function the gain is\n$\\Theta(h(\\epsilon))$. This answers a question of [M. Braverman, A. Garg, D.\nPankratov, and O. Weinstein, From information to exact communication (extended\nabstract), STOC'13].\n  We obtain sharp bounds for the set disjointness function of order $n$. For\nthe case of the distributional error, we introduce a new protocol that achieves\na gain of $\\Theta(\\sqrt{h(\\epsilon)})$ provided that $n$ is sufficiently large.\nWe apply these results to answer another of question of Braverman et al.\nregarding the randomized communication complexity of the set disjointness\nfunction.\n  Answering a question of [Mark Braverman, Interactive information complexity,\nSTOC'12], we apply our analysis of the set disjointness function to establish a\ngap between the two different notions of the prior-free information cost. This\nimplies that amortized randomized communication complexity is not necessarily\nequal to the amortized distributional communication complexity with respect to\nthe hardest distribution.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:12:07 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Dagan", "Yuval", ""], ["Filmus", "Yuval", ""], ["Hatami", "Hamed", ""], ["Li", "Yaqiao", ""]]}, {"id": "1611.06980", "submitter": "Sivakanth Gopi", "authors": "Arnab Bhattacharyya, Sivakanth Gopi, Avishay Tal", "title": "Lower bounds for 2-query LCCs over large alphabet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A locally correctable code (LCC) is an error correcting code that allows\ncorrection of any arbitrary coordinate of a corrupted codeword by querying only\na few coordinates. We show that any {\\em zero-error} $2$-query locally\ncorrectable code $\\mathcal{C}: \\{0,1\\}^k \\to \\Sigma^n$ that can correct a\nconstant fraction of corrupted symbols must have $n \\geq \\exp(k/\\log|\\Sigma|)$.\nWe say that an LCC is zero-error if there exists a non-adaptive corrector\nalgorithm that succeeds with probability $1$ when the input is an uncorrupted\ncodeword. All known constructions of LCCs are zero-error.\n  Our result is tight upto constant factors in the exponent. The only previous\nlower bound on the length of 2-query LCCs over large alphabet was\n$\\Omega\\left((k/\\log|\\Sigma|)^2\\right)$ due to Katz and Trevisan (STOC 2000).\nOur bound implies that zero-error LCCs cannot yield $2$-server private\ninformation retrieval (PIR) schemes with sub-polynomial communication. Since\nthere exists a $2$-server PIR scheme with sub-polynomial communication (STOC\n2015) based on a zero-error $2$-query locally decodable code (LDC), we also\nobtain a separation between LDCs and LCCs over large alphabet.\n  For our proof of the result, we need a new decomposition lemma for directed\ngraphs that may be of independent interest. Given a dense directed graph $G$,\nour decomposition uses the directed version of Szemer\\'edi regularity lemma due\nto Alon and Shapira (STOC 2003) to partition almost all of $G$ into a constant\nnumber of subgraphs which are either edge-expanding or empty.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:57:35 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 19:16:20 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gopi", "Sivakanth", ""], ["Tal", "Avishay", ""]]}, {"id": "1611.07008", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "Fine-Grained Complexity and Conditional Hardness for Sparse Graphs", "comments": "abstract updated; There is no update to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fine-grained complexity of sparse graph problems that\ncurrently have $\\tilde{O}(mn)$ time algorithms, where m is the number of edges\nand n is the number of vertices in the input graph. This class includes several\nimportant path problems on both directed and undirected graphs, including APSP,\nMWC (minimum weight cycle), and Eccentricities, which is the problem of\ncomputing, for each vertex in the graph, the length of a longest shortest path\nstarting at that vertex.\n  We introduce the notion of a sparse reduction which preserves the sparsity of\ngraphs, and we present near linear-time sparse reductions between various pairs\nof graph problems in the $\\tilde{O}(mn)$ class. Surprisingly, very few of the\nknown nontrivial reductions between problems in the $\\tilde{O}(mn)$ class are\nsparse reductions. In the directed case, our results give a partial order on a\nlarge collection of problems in the $\\tilde{O}(mn)$ class (along with some\nequivalences). In the undirected case we give two nontrivial sparse reductions:\nfrom MWC to APSP, and from unweighted ANSC (all nodes shortest cycles) to APSP.\nThe latter reduction also gives an improved algorithm for ANSC (for dense\ngraphs).\n  We propose the MWC Conjecture, a new conditional hardness conjecture that the\nweight of a minimum weight cycle in a directed graph cannot be computed in time\npolynomially smaller than mn. Our sparse reductions for directed path problems\nin the $\\tilde{O}(mn)$ class establish that several problems in this class,\nincluding 2-SiSP (second simple shortest path), Radius, and Eccentricities, are\nMWCC hard. We also identify Eccentricities as a key problem in the\n$\\tilde{O}(mn)$ class which is simultaneously MWCC-hard, SETH-hard and\nk-DSH-hard, where SETH is the Strong Exponential Time Hypothesis, and k-DSH is\nthe hypothesis that a dominating set of size k cannot be computed in time\npolynomially smaller than n^k.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:53:21 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:24:17 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 01:51:38 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1611.07144", "submitter": "David Harvey", "authors": "David Harvey and Joris van der Hoeven", "title": "Faster integer multiplication using plain vanilla FFT primes", "comments": "14 pages, to appear in Mathematics of Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming a conjectural upper bound for the least prime in an arithmetic\nprogression, we show that n-bit integers may be multiplied in O(n log n\n4^(log^* n)) bit operations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 04:35:14 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 05:01:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Harvey", "David", ""], ["van der Hoeven", "Joris", ""]]}, {"id": "1611.07235", "submitter": "S Raja", "authors": "Vikraman Arvind, Pushkar Joglekar, Partha Mukhopadhyay, S Raja", "title": "Identity Testing for +-Regular Noncommutative Arithmetic Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient randomized polynomial identity test for noncommutative\npolynomials given by noncommutative arithmetic circuits remains an open\nproblem. The main bottleneck to applying known techniques is that a\nnoncommutative circuit of size $s$ can compute a polynomial of degree\nexponential in $s$ with a double-exponential number of nonzero monomials. In\nthis paper, we report some progress by dealing with two natural subcases (both\nallow for polynomials of exponential degree and a double exponential number of\nmonomials): (1) We consider \\emph{$+$-regular} noncommutative circuits: these\nare homogeneous noncommutative circuits with the additional property that all\nthe $+$-gates are layered, and in each $+$-layer all gates have the same\nsyntactic degree. We give a \\emph{white-box} polynomial-time deterministic\npolynomial identity test for such circuits. Our algorithm combines some new\nstructural results for $+$-regular circuits with known results for\nnoncommutative ABP identity testing [RS05PIT], rank bound of commutative depth\nthree identities [SS13], and equivalence testing problem for words [Loh15,\nMSU97, Pla94]. (2) Next, we consider $\\Sigma\\Pi^*\\Sigma$ noncommutative\ncircuits: these are noncommutative circuits with layered $+$-gates such that\nthere are only two layers of $+$-gates. These $+$-layers are the output\n$+$-gate and linear forms at the bottom layer; between the $+$-layers the\ncircuit could have any number of $\\times$ gates. We given an efficient\nrandomized \\emph{black-box} identity testing problem for $\\Sigma\\Pi^*\\Sigma$\ncircuits. In particular, we show if $f\\in F<Z>$ is a nonzero noncommutative\npolynomial computed by a $\\Sigma\\Pi^*\\Sigma$ circuit of size $s$, then $f$\ncannot be a polynomial identity for the matrix algebra $\\mathbb{M}_s(F)$, where\nthe field $F$ is a sufficiently large extension of $F$ depending on the degree\nof $f$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:19:33 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Arvind", "Vikraman", ""], ["Joglekar", "Pushkar", ""], ["Mukhopadhyay", "Partha", ""], ["Raja", "S", ""]]}, {"id": "1611.07724", "submitter": "Frank Gurski", "authors": "Carolin Albrecht, Frank Gurski, Jochen Rethmann, Eda Yilmaz", "title": "Knapsack Problems: A Parameterized Point of View", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knapsack problem (KP) is a very famous NP-hard problem in combinatorial\noptimization. Also its generalization to multiple dimensions named\nd-dimensional knapsack problem (d-KP) and to multiple knapsacks named multiple\nknapsack problem (MKP) are well known problems. Since KP, d-KP, and MKP are\ninteger-valued problems defined on inputs of various informations, we study the\nfixed-parameter tractability of these problems. The idea behind fixed-parameter\ntractability is to split the complexity into two parts - one part that depends\npurely on the size of the input, and one part that depends on some parameter of\nthe problem that tends to be small in practice. Further we consider the closely\nrelated question, whether the sizes and the values can be reduced, such that\ntheir bit-length is bounded polynomially or even constantly in a given\nparameter, i.e. the existence of kernelizations is studied. We discuss the\nfollowing parameters: the number of items, the threshold value for the profit,\nthe sizes, the profits, the number d of dimensions, and the number m of\nknapsacks. We also consider the connection of parameterized knapsack problems\nto linear programming, approximation, and pseudo-polynomial algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:23:46 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Albrecht", "Carolin", ""], ["Gurski", "Frank", ""], ["Rethmann", "Jochen", ""], ["Yilmaz", "Eda", ""]]}, {"id": "1611.08326", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Detecting communities is hard, and counting them is even harder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 00:09:46 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1611.08400", "submitter": "Dirk Oliver Theis", "authors": "Mozhgan Pourmoradnasseri and Dirk Oliver Theis", "title": "Nondeterministic Communication Complexity of Random Boolean Functions", "comments": "Version with proofs (in the appendix). Extended abstract appeared in\n  Proceedings of Theory and Applications of Models of Computation, TAMC, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nondeterministic communication complexity and related concepts\n(fooling sets, fractional covering number) of random functions $f\\colon X\\times\nY \\to \\{0,1\\}$ where each value is chosen to be 1 independently with\nprobability $p=p(n)$, $n := |X|=|Y|$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 09:44:11 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 18:39:05 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Pourmoradnasseri", "Mozhgan", ""], ["Theis", "Dirk Oliver", ""]]}, {"id": "1611.08680", "submitter": "Jan Krajicek", "authors": "Jan Krajicek", "title": "Randomized feasible interpolation and monotone circuits with a local\n  oracle", "comments": "Preliminary version November 2016", "journal-ref": "J. of Mathematical Logic, Vol.18., No. 2, 1850012 (2018)", "doi": "10.1142/S0219061318500125", "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the feasible interpolation theorem for semantic derivations\nfrom K.(1997) by allowing randomized protocols (protocols in the sense of\nK.(1997). We also introduce an extension of the monotone circuit model,\nmonotone circuits with a local oracle (CLOs), that does correspond to\ncommunication protocols for the monotone Karchmer-Wigderson multi-function\n$KW^m[U,V]$ making errors. The new randomized feasible interpolation thus shows\nthat a short semantic derivation (from a certain class of derivations larger\nthan in the original method) of the disjointness of $U, V$, $U$ closed upwards,\nyields a small randomized protocol for and hence a small monotone CLO\nseparating the sets. To establish a lower bound for monotone CLOs separating\ntwo NP sets, one closed upwards, is an open problem.\n  This research is motivated by the open problem to establish a lower bound for\nproof system $R(LIN)$ operating with clauses formed by linear Boolean functions\nover the 2-element field. The new randomized feasible interpolation applies to\nthis proof system and also to the semantic versions of the cutting planes proof\nsystem CP (as randomized protocols efficiently simulate protocols with small\nreal communication complexity of K.(1998)), to small width resolution over CP,\nR(CP), and to random resolution RR of Buss, Kolodziejczyk and Thapen (2014).\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 09:08:13 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 11:03:05 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 09:08:10 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 12:07:49 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Krajicek", "Jan", ""]]}, {"id": "1611.08757", "submitter": "Rebecca Hoberg", "authors": "Rebecca Hoberg, Harishchandra Ramadas, Thomas Rothvoss and Xin Yang", "title": "Number Balancing is as hard as Minkowski's Theorem and Shortest Vector", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number balancing (NBP) problem is the following: given real numbers\n$a_1,\\ldots,a_n \\in [0,1]$, find two disjoint subsets $I_1,I_2 \\subseteq [n]$\nso that the difference $|\\sum_{i \\in I_1}a_i - \\sum_{i \\in I_2}a_i|$ of their\nsums is minimized. An application of the pigeonhole principle shows that there\nis always a solution where the difference is at most $O(\\frac{\\sqrt{n}}{2^n})$.\nFinding the minimum, however, is NP-hard. In polynomial time,the differencing\nalgorithm by Karmarkar and Karp from 1982 can produce a solution with\ndifference at most $n^{-\\Theta(\\log n)}$, but no further improvement has been\nmade since then.\n  In this paper, we show a relationship between NBP and Minkowski's Theorem.\nFirst we show that an approximate oracle for Minkowski's Theorem gives an\napproximate NBP oracle. Perhaps more surprisingly, we show that an approximate\nNBP oracle gives an approximate Minkowski oracle. In particular, we prove that\nany polynomial time algorithm that guarantees a solution of difference at most\n$2^{\\sqrt{n}} / 2^{n}$ would give a polynomial approximation for Minkowski as\nwell as a polynomial factor approximation algorithm for the Shortest Vector\nProblem.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:59:07 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 19:13:15 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Hoberg", "Rebecca", ""], ["Ramadas", "Harishchandra", ""], ["Rothvoss", "Thomas", ""], ["Yang", "Xin", ""]]}, {"id": "1611.08842", "submitter": "Dmitry Gavinsky", "authors": "Dmitry Gavinsky", "title": "The communication complexity of the inevitable intersection problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set disjointness is a central problem in communication complexity. Here Alice\nand Bob each receive a subset of an n-element universe, and they need to decide\nwhether their inputs intersect or not. The communication complexity of this\nproblem is relatively well understood, and in most models, including $-$ most\nfamously $-$ interactive randomised communication with bounded error, the\nproblem requires much communication.\n  In this work we were looking for a variation of the set disjointness problem,\nas natural and simple as possible, for which the known lower bound methods\nwould fail, and thus a new approach would be required in order to understand\nits complexity. The problem that we have found is a relational one: each player\nreceives a subset as input, and the goal is to find an element that belongs to\nboth players. We call it inevitable intersection.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 13:47:16 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 00:54:02 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 01:28:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gavinsky", "Dmitry", ""]]}, {"id": "1611.08927", "submitter": "Zack Fitzsimmons", "authors": "Zack Fitzsimmons and Edith Hemaspaandra", "title": "High-Multiplicity Election Problems", "comments": "A preliminary version of this paper (arXiv:1611.08927v1) was titled\n  \"The Complexity of Succinct Elections.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational study of elections generally assumes that the preferences\nof the electorate come in as a list of votes. Depending on the context, it may\nbe much more natural to represent the list succinctly, as the distinct votes of\nthe electorate and their counts, i.e., high-multiplicity representation. We\nconsider how this representation affects the complexity of election problems.\nHigh-multiplicity representation may be exponentially smaller than standard\nrepresentation, and so many polynomial-time algorithms for election problems in\nstandard representation become exponential-time. Surprisingly, for\npolynomial-time election problems, we are often able to either adapt the same\napproach or provide new algorithms to show that these problems remain\npolynomial-time in the high-multiplicity case; this is in sharp contrast to the\ncase where each voter has a weight, where the complexity usually increases. In\nthe process we explore the relationship between high-multiplicity scheduling\nand manipulation of high-multiplicity elections. And we show that for any fixed\nset of job lengths, high-multiplicity scheduling on uniform parallel machines\nis in P, which was previously known for only two job lengths. We did not find\nany natural case where a polynomial-time election problem does not remain in P\nwhen moving to high-multiplicity representation. However, we found one natural\nNP-hard election problem where the complexity does increase, namely winner\ndetermination for Kemeny elections.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 22:39:53 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 21:54:14 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 21:35:58 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 14:49:48 GMT"}, {"version": "v5", "created": "Thu, 24 Jun 2021 17:32:17 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Fitzsimmons", "Zack", ""], ["Hemaspaandra", "Edith", ""]]}, {"id": "1611.08946", "submitter": "Dave Touchette", "authors": "Anurag Anshu, Dave Touchette, Penghui Yao, and Nengkun Yu", "title": "Exponential Separation of Quantum Communication and Classical\n  Information", "comments": "v1, 36 pages, 3 figures", "journal-ref": "Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of\n  Computing, STOC 2017", "doi": "10.1145/3055399.3055401", "report-no": null, "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit a Boolean function for which the quantum communication complexity\nis exponentially larger than the classical information complexity. An\nexponential separation in the other direction was already known from the work\nof Kerenidis et. al. [SICOMP 44, pp. 1550-1572], hence our work implies that\nthese two complexity measures are incomparable. As classical information\ncomplexity is an upper bound on quantum information complexity, which in turn\nis equal to amortized quantum communication complexity, our work implies that a\ntight direct sum result for distributional quantum communication complexity\ncannot hold. The function we use to present such a separation is the Symmetric\nk-ary Pointer Jumping function introduced by Rao and Sinha [ECCC TR15-057],\nwhose classical communication complexity is exponentially larger than its\nclassical information complexity. In this paper, we show that the quantum\ncommunication complexity of this function is polynomially equivalent to its\nclassical communication complexity. The high-level idea behind our proof is\narguably the simplest so far for such an exponential separation between\ninformation and communication, driven by a sequence of round-elimination\narguments, allowing us to simplify further the approach of Rao and Sinha.\n  As another application of the techniques that we develop, we give a simple\nproof for an optimal trade-off between Alice's and Bob's communication while\ncomputing the related Greater-Than function on n bits: say Bob communicates at\nmost b bits, then Alice must send n/exp(O(b)) bits to Bob. This holds even when\nallowing pre-shared entanglement. We also present a classical protocol\nachieving this bound.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 00:57:58 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Anshu", "Anurag", ""], ["Touchette", "Dave", ""], ["Yao", "Penghui", ""], ["Yu", "Nengkun", ""]]}, {"id": "1611.09274", "submitter": "Juan Bermejo-Vega", "authors": "Juan Bermejo-Vega", "title": "Normalizer Circuits and Quantum Computation", "comments": "PhD thesis, Technical University of Munich (2016). Please cite\n  original papers if possible. Appendix E contains unpublished work on Gaussian\n  unitaries. If you spot typos/omissions please email me at JLastNames at\n  posteo dot net. Source: http://bit.ly/2gMdHn3. Related video talk:\n  https://www.perimeterinstitute.ca/videos/toy-theory-quantum-speed-ups-based-stabilizer-formalism\n  Posted on my birthday", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Abridged abstract.) In this thesis we introduce new models of quantum\ncomputation to study the emergence of quantum speed-up in quantum computer\nalgorithms.\n  Our first contribution is a formalism of restricted quantum operations, named\nnormalizer circuit formalism, based on algebraic extensions of the qubit\nClifford gates (CNOT, Hadamard and $\\pi/4$-phase gates): a normalizer circuit\nconsists of quantum Fourier transforms (QFTs), automorphism gates and quadratic\nphase gates associated to a set $G$, which is either an abelian group or\nabelian hypergroup. Though Clifford circuits are efficiently classically\nsimulable, we show that normalizer circuit models encompass Shor's celebrated\nfactoring algorithm and the quantum algorithms for abelian Hidden Subgroup\nProblems. We develop classical-simulation techniques to characterize under\nwhich scenarios normalizer circuits provide quantum speed-ups. Finally, we\ndevise new quantum algorithms for finding hidden hyperstructures. The results\noffer new insights into the source of quantum speed-ups for several algebraic\nproblems.\n  Our second contribution is an algebraic (group- and hypergroup-theoretic)\nframework for describing quantum many-body states and classically simulating\nquantum circuits. Our framework extends Gottesman's Pauli Stabilizer Formalism\n(PSF), wherein quantum states are written as joint eigenspaces of stabilizer\ngroups of commuting Pauli operators: while the PSF is valid for qubit/qudit\nsystems, our formalism can be applied to discrete- and continuous-variable\nsystems, hybrid settings, and anyonic systems. These results enlarge the known\nfamilies of quantum processes that can be efficiently classically simulated.\n  This thesis also establishes a precise connection between Shor's quantum\nalgorithm and the stabilizer formalism, revealing a common mathematical\nstructure in several quantum speed-ups and error-correcting codes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 18:30:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bermejo-Vega", "Juan", ""]]}, {"id": "1611.09296", "submitter": "Saeed Akhoondian Amiri", "authors": "Saeed Akhoondian Amiri, Szymon Dudycz, Stefan Schmid, Sebastian\n  Wiederrecht", "title": "Congestion-Free Rerouting of Flows on DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM cs.NI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changing a given configuration in a graph into another one is known as a re-\nconfiguration problem. Such problems have recently received much interest in\nthe context of algorithmic graph theory. We initiate the theoretical study of\nthe following reconfiguration problem: How to reroute $k$ unsplittable flows of\na certain demand in a capacitated network from their current paths to their\nrespective new paths, in a congestion-free manner? This problem finds immediate\napplications, e.g., in traffic engineering in computer networks. We show that\nthe problem is generally NP-hard already for $k = 2$ flows, which motivates us\nto study rerouting on a most basic class of flow graphs, namely DAGs.\nInterestingly, we find that for general $k$, deciding whether an unsplittable\nmulti-commodity flow rerouting schedule exists, is NP-hard even on DAGs. Both\nNP-hardness proofs are non-trivial. Our main contribution is a polynomial-time\n(fixed parameter tractable) algorithm to solve the route update problem for a\nbounded number of flows on DAGs. At the heart of our algorithm lies a novel\ndecomposition of the flow network that allows us to express and resolve\nreconfiguration dependencies among flows.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 19:25:04 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 17:02:21 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 09:14:17 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Dudycz", "Szymon", ""], ["Schmid", "Stefan", ""], ["Wiederrecht", "Sebastian", ""]]}, {"id": "1611.09541", "submitter": "Jan Philipp W\\\"achter", "authors": "Daniele D'Angeli, Emanuele Rodaro, Jan Philipp W\\\"achter", "title": "On the Complexity of the Word Problem for Automaton Semigroups and\n  Automaton Groups", "comments": null, "journal-ref": "Advances in Applied Mathematics, Volume 90, September 2017, Pages\n  160-187, ISSN 0196-8858", "doi": "10.1016/j.aam.2017.05.008", "report-no": null, "categories": "cs.FL cs.CC math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the word problem for automaton semigroups and\nautomaton groups from a complexity point of view. As an intermediate concept\nbetween automaton semigroups and automaton groups, we introduce\nautomaton-inverse semigroups, which are generated by partial, yet invertible\nautomata. We show that there is an automaton-inverse semigroup and, thus, an\nautomaton semigroup with a PSPACE-complete word problem. We also show that\nthere is an automaton group for which the word problem with a single rational\nconstraint is PSPACE-complete. Additionally, we provide simpler constructions\nfor the uniform word problems of these classes. For the uniform word problem\nfor automaton groups (without rational constraints), we show NL-hardness.\nFinally, we investigate a question asked by Cain about a better upper bound for\nthe length of a word on which two distinct elements of an automaton semigroup\nmust act differently.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 09:52:40 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 08:45:08 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["D'Angeli", "Daniele", ""], ["Rodaro", "Emanuele", ""], ["W\u00e4chter", "Jan Philipp", ""]]}, {"id": "1611.10258", "submitter": "Varun Kanade", "authors": "Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler", "title": "Reliably Learning the ReLU in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first dimension-efficient algorithms for learning Rectified\nLinear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto\n\\max(0, \\mathbf{w} \\cdot \\mathbf{x})$ with $\\mathbf{w} \\in \\mathbb{S}^{n-1}$.\nOur algorithm works in the challenging Reliable Agnostic learning model of\nKalai, Kanade, and Mansour (2009) where the learner is given access to a\ndistribution $\\cal{D}$ on labeled examples but the labeling may be arbitrary.\nWe construct a hypothesis that simultaneously minimizes the false-positive rate\nand the loss on inputs given positive labels by $\\cal{D}$, for any convex,\nbounded, and Lipschitz loss function.\n  The algorithm runs in polynomial-time (in $n$) with respect to any\ndistribution on $\\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for\nany error parameter $\\epsilon = \\Omega(1/\\log n)$ (this yields a PTAS for a\nquestion raised by F. Bach on the complexity of maximizing ReLUs). These\nresults are in contrast to known efficient algorithms for reliably learning\nlinear threshold functions, where $\\epsilon$ must be $\\Omega(1)$ and strong\nassumptions are required on the marginal distribution. We can compose our\nresults to obtain the first set of efficient algorithms for learning\nconstant-depth networks of ReLUs.\n  Our techniques combine kernel methods and polynomial approximations with a\n\"dual-loss\" approach to convex programming. As a byproduct we obtain a number\nof applications including the first set of efficient algorithms for \"convex\npiecewise-linear fitting\" and the first efficient algorithms for noisy\npolynomial reconstruction of low-weight polynomials on the unit sphere.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:42:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Goel", "Surbhi", ""], ["Kanade", "Varun", ""], ["Klivans", "Adam", ""], ["Thaler", "Justin", ""]]}, {"id": "1611.10319", "submitter": "Jayson Lynch", "authors": "Erik D. Demaine, Joshua Lockhart, Jayson Lynch", "title": "The Computational Complexity of Portal and Other 3D Video Games", "comments": "24 pages. Based on and overlaps with work in the MEng thesis of\n  Jayson Lynch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify the computational complexity of the popular video games Portal\nand Portal 2. We isolate individual mechanics of the game and prove\nNP-hardness, PSPACE-completeness, or (pseudo)polynomiality depending on the\nspecific game mechanics allowed. One of our proofs generalizes to prove\nNP-hardness of many other video games such as Half-Life 2, Halo, Doom, Elder\nScrolls, Fallout, Grand Theft Auto, Left 4 Dead, Mass Effect, Deus Ex, Metal\nGear Solid, and Resident Evil.\n  These results build on the established literature on the complexity of video\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:20:47 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Demaine", "Erik D.", ""], ["Lockhart", "Joshua", ""], ["Lynch", "Jayson", ""]]}, {"id": "1611.10334", "submitter": "Christoph Rauch", "authors": "Cynthia Kop and Jakob Grue Simonsen", "title": "Complexity Hierarchies and Higher-order Cons-free Term Rewriting", "comments": "extended version of a paper submitted to FSCD 2016. arXiv admin note:\n  substantial text overlap with arXiv:1604.08936", "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 3 (August 7,\n  2017) lmcs:3847", "doi": "10.23638/LMCS-13(3:8)2017", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructor rewriting systems are said to be cons-free if, roughly,\nconstructor terms in the right-hand sides of rules are subterms of the\nleft-hand sides; the computational intuition is that rules cannot build new\ndata structures. In programming language research, cons-free languages have\nbeen used to characterize hierarchies of computational complexity classes; in\nterm rewriting, cons-free first-order TRSs have been used to characterize the\nclass PTIME.\n  We investigate cons-free higher-order term rewriting systems, the complexity\nclasses they characterize, and how these depend on the type order of the\nsystems. We prove that, for every K $\\geq$ 1, left-linear cons-free systems\nwith type order K characterize E$^K$TIME if unrestricted evaluation is used\n(i.e., the system does not have a fixed reduction strategy).\n  The main difference with prior work in implicit complexity is that (i) our\nresults hold for non-orthogonal term rewriting systems with no assumptions on\nreduction strategy, (ii) we consequently obtain much larger classes for each\ntype order (E$^K$TIME versus EXP$^{K-1}$TIME), and (iii) results for cons-free\nterm rewriting systems have previously only been obtained for K = 1, and with\nadditional syntactic restrictions besides cons-freeness and left-linearity.\n  Our results are among the first implicit characterizations of the hierarchy E\n= E$^1$TIME $\\subsetneq$ E$^2$TIME $\\subsetneq$ ... Our work confirms prior\nresults that having full non-determinism (via overlapping rules) does not\ndirectly allow for characterization of non-deterministic complexity classes\nlike NE. We also show that non-determinism makes the classes characterized\nhighly sensitive to minor syntactic changes like admitting product types or\nnon-left-linear rules.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:02:40 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 11:43:13 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 14:06:34 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Kop", "Cynthia", ""], ["Simonsen", "Jakob Grue", ""]]}]