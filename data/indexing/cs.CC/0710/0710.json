[{"id": "0710.0318", "submitter": "Alexander Tiskin", "authors": "Vladimir Deineko and Alexander Tiskin", "title": "Fast minimum-weight double-tree shortcutting for Metric TSP: Is the best\n  one good enough?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metric Traveling Salesman Problem (TSP) is a classical NP-hard\noptimization problem. The double-tree shortcutting method for Metric TSP yields\nan exponentially-sized space of TSP tours, each of which approximates the\noptimal solution within at most a factor of 2. We consider the problem of\nfinding among these tours the one that gives the closest approximation, i.e.\\\nthe \\emph{minimum-weight double-tree shortcutting}. Burkard et al. gave an\nalgorithm for this problem, running in time $O(n^3+2^d n^2)$ and memory $O(2^d\nn^2)$, where $d$ is the maximum node degree in the rooted minimum spanning\ntree. We give an improved algorithm for the case of small $d$ (including planar\nEuclidean TSP, where $d \\leq 4$), running in time $O(4^d n^2)$ and memory\n$O(4^d n)$. This improvement allows one to solve the problem on much larger\ninstances than previously attempted. Our computational experiments suggest that\nin terms of the time-quality tradeoff, the minimum-weight double-tree\nshortcutting method provides one of the best known tour-constructing\nheuristics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2007 15:25:18 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2009 16:17:30 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2009 16:17:25 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Deineko", "Vladimir", ""], ["Tiskin", "Alexander", ""]]}, {"id": "0710.0360", "submitter": "Sylvain Perifel", "authors": "Pascal Koiran (LIP), Sylvain Perifel (LIP)", "title": "Interpolation in Valiant's theory", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": null, "abstract": "  We investigate the following question: if a polynomial can be evaluated at\nrational points by a polynomial-time boolean algorithm, does it have a\npolynomial-size arithmetic circuit? We argue that this question is certainly\ndifficult. Answering it negatively would indeed imply that the constant-free\nversions of the algebraic complexity classes VP and VNP defined by Valiant are\ndifferent. Answering this question positively would imply a transfer theorem\nfrom boolean to algebraic complexity. Our proof method relies on Lagrange\ninterpolation and on recent results connecting the (boolean) counting hierarchy\nto algebraic complexity classes. As a byproduct we obtain two additional\nresults: (i) The constant-free, degree-unbounded version of Valiant's\nhypothesis that VP and VNP differ implies the degree-bounded version. This\nresult was previously known to hold for fields of positive characteristic only.\n(ii) If exponential sums of easy to compute polynomials can be computed\nefficiently, then the same is true of exponential products. We point out an\napplication of this result to the P=NP problem in the Blum-Shub-Smale model of\ncomputation over the field of complex numbers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2007 18:58:19 GMT"}], "update_date": "2007-10-02", "authors_parsed": [["Koiran", "Pascal", "", "LIP"], ["Perifel", "Sylvain", "", "LIP"]]}, {"id": "0710.0539", "submitter": "Anthony A. Ruffa", "authors": "Anthony A. Ruffa", "title": "A Novel Solution to the ATT48 Benchmark Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": null, "abstract": "  A solution to the benchmark ATT48 Traveling Salesman Problem (from the\nTSPLIB95 library) results from isolating the set of vertices into ten\nopen-ended zones with nine lengthwise boundaries. In each zone, a\nminimum-length Hamiltonian Path (HP) is found for each combination of boundary\nvertices, leading to an approximation for the minimum-length Hamiltonian Cycle\n(HC). Determination of the optimal HPs for subsequent zones has the effect of\nautomatically filtering out non-optimal HPs from earlier zones. Although the\noptimal HC for ATT48 involves only two crossing edges between all zones (with\none exception), adding inter-zone edges can accommodate more complex problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2007 14:26:33 GMT"}], "update_date": "2007-10-03", "authors_parsed": [["Ruffa", "Anthony A.", ""]]}, {"id": "0710.0748", "submitter": "M Sabu THAMPI", "authors": "Murali Krishna P, Sabu .M Thampi", "title": "A Fast Heuristic Algorithm Based on Verification and Elimination Methods\n  for Maximum Clique Problem", "comments": "06 pages,01 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC", "license": null, "abstract": "  A clique in an undirected graph G= (V, E) is a subset V' V of vertices, each\npair of which is connected by an edge in E. The clique problem is an\noptimization problem of finding a clique of maximum size in graph. The clique\nproblem is NP-Complete. We have succeeded in developing a fast algorithm for\nmaximum clique problem by employing the method of verification and elimination.\nFor a graph of size N there are 2N sub graphs, which may be cliques and hence\nverifying all of them, will take a long time. Idea is to eliminate a major\nnumber of sub graphs, which cannot be cliques and verifying only the remaining\nsub graphs. This heuristic algorithm runs in polynomial time and executes\nsuccessfully for several examples when applied to random graphs and DIMACS\nbenchmark graphs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 09:33:20 GMT"}], "update_date": "2007-10-04", "authors_parsed": [["P", "Murali Krishna", ""], ["Thampi", "Sabu . M", ""]]}, {"id": "0710.0805", "submitter": "Elitza Maneva", "authors": "Elitza Maneva and Alistair Sinclair", "title": "On the Satisfiability Threshold and Clustering of Solutions of Random\n  3-SAT Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the structure of satisfying assignments of a random 3-SAT formula.\nIn particular, we show that a random formula of density 4.453 or higher almost\nsurely has no non-trivial \"core\" assignments. Core assignments are certain\npartial assignments that can be extended to satisfying assignments, and have\nbeen studied recently in connection with the Survey Propagation heuristic for\nrandom SAT. Their existence implies the presence of clusters of solutions, and\nthey have been shown to exist with high probability below the satisfiability\nthreshold for k-SAT with k>8, by Achlioptas and Ricci-Tersenghi, STOC 2006. Our\nresult implies that either this does not hold for 3-SAT or the threshold\ndensity for satisfiability in 3-SAT lies below 4.453.\n  The main technical tool that we use is a novel simple application of the\nfirst moment method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2007 19:04:44 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2008 23:01:27 GMT"}, {"version": "v3", "created": "Sun, 31 Aug 2008 18:27:03 GMT"}], "update_date": "2008-09-01", "authors_parsed": [["Maneva", "Elitza", ""], ["Sinclair", "Alistair", ""]]}, {"id": "0710.1153", "submitter": "Patrick Baillot", "authors": "Vincent Atassi, Patrick Baillot, Kazushige Terui", "title": "Verification of Ptime Reducibility for system F Terms: Type Inference\n  in<br> Dual Light Affine Logic", "comments": "32 pages, 8 figures", "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 4 (November\n  15, 2007) lmcs:1234", "doi": "10.2168/LMCS-3(4:10)2007", "report-no": null, "categories": "cs.LO cs.CC", "license": null, "abstract": "  In a previous work Baillot and Terui introduced Dual light affine logic\n(DLAL) as a variant of Light linear logic suitable for guaranteeing complexity\nproperties on lambda calculus terms: all typable terms can be evaluated in\npolynomial time by beta reduction and all Ptime functions can be represented.\nIn the present work we address the problem of typing lambda-terms in\nsecond-order DLAL. For that we give a procedure which, starting with a term\ntyped in system F, determines whether it is typable in DLAL and outputs a\nconcrete typing if there exists any. We show that our procedure can be run in\ntime polynomial in the size of the original Church typed system F term.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2007 09:24:31 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2007 11:02:50 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Atassi", "Vincent", ""], ["Baillot", "Patrick", ""], ["Terui", "Kazushige", ""]]}, {"id": "0710.1879", "submitter": "Ning Chen", "authors": "Ning Chen and Zhiyuan Yan", "title": "Cyclotomic FFTs with Reduced Additive Complexities Based on a Novel\n  Common Subexpression Elimination Algorithm", "comments": "11 pages, submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a novel common subexpression elimination\n(CSE) algorithm for matrix-vector multiplications over characteristic-2 fields.\nAs opposed to previously proposed CSE algorithms, which usually focus on\ncomplexity savings due to recurrences of subexpressions, our CSE algorithm\nachieves two types of complexity reductions, differential savings and\nrecurrence savings, by taking advantage of the cancelation property of\ncharacteristic-2 fields. Using our CSE algorithm, we reduce the additive\ncomplexities of cyclotomic fast Fourier transforms (CFFTs). Using a weighted\nsum of the numbers of multiplications and additions as a metric, our CFFTs\nachieve smaller total complexities than previously proposed CFFTs and other\nFFTs, requiring both fewer multiplications and fewer additions in many cases.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2007 08:13:44 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2007 18:09:03 GMT"}, {"version": "v3", "created": "Thu, 8 May 2008 03:23:10 GMT"}, {"version": "v4", "created": "Thu, 23 Oct 2008 06:18:31 GMT"}], "update_date": "2008-10-23", "authors_parsed": [["Chen", "Ning", ""], ["Yan", "Zhiyuan", ""]]}, {"id": "0710.2139", "submitter": "Ashkan Aazami", "authors": "Ashkan Aazami, Michael D. Stilp", "title": "Approximation algorithms and hardness for domination with propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": null, "abstract": "  The power dominating set (PDS) problem is the following extension of the\nwell-known dominating set problem: find a smallest-size set of nodes $S$ that\npower dominates all the nodes, where a node $v$ is power dominated if (1) $v$\nis in $S$ or $v$ has a neighbor in $S$, or (2) $v$ has a neighbor $w$ such that\n$w$ and all of its neighbors except $v$ are power dominated. We show a hardness\nof approximation threshold of $2^{\\log^{1-\\epsilon}{n}}$ in contrast to the\nlogarithmic hardness for the dominating set problem. We give an $O(\\sqrt{n})$\napproximation algorithm for planar graphs, and show that our methods cannot\nimprove on this approximation guarantee. Finally, we initiate the study of PDS\non directed graphs, and show the same hardness threshold of\n$2^{\\log^{1-\\epsilon}{n}}$ for directed \\emph{acyclic} graphs. Also we show\nthat the directed PDS problem can be solved optimally in linear time if the\nunderlying undirected graph has bounded tree-width.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2007 23:30:49 GMT"}], "update_date": "2007-10-12", "authors_parsed": [["Aazami", "Ashkan", ""], ["Stilp", "Michael D.", ""]]}, {"id": "0710.2732", "submitter": "Marie-Annick Guillemer", "authors": "Dima Grigoriev (IRMAR)", "title": "Probabilistic communication complexity over the reals", "comments": null, "journal-ref": null, "doi": null, "report-no": "07-60", "categories": "cs.CC", "license": null, "abstract": "  Deterministic and probabilistic communication protocols are introduced in\nwhich parties can exchange the values of polynomials (rather than bits in the\nusual setting). It is established a sharp lower bound $2n$ on the communication\ncomplexity of recognizing the $2n$-dimensional orthant, on the other hand the\nprobabilistic communication complexity of its recognizing does not exceed 4. A\npolyhedron and a union of hyperplanes are constructed in $\\RR^{2n}$ for which a\nlower bound $n/2$ on the probabilistic communication complexity of recognizing\neach is proved. As a consequence this bound holds also for the EMPTINESS and\nthe KNAPSACK problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2007 08:03:38 GMT"}], "update_date": "2007-10-16", "authors_parsed": [["Grigoriev", "Dima", "", "IRMAR"]]}, {"id": "0710.3519", "submitter": "Jan Foniok", "authors": "Jan Foniok", "title": "P-matrix recognition is co-NP-complete", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": null, "abstract": "  This is a summary of the proof by G.E. Coxson that P-matrix recognition is\nco-NP-complete. The result follows by a reduction from the MAX CUT problem\nusing results of S. Poljak and J. Rohn.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2007 14:14:26 GMT"}], "update_date": "2007-10-19", "authors_parsed": [["Foniok", "Jan", ""]]}, {"id": "0710.3642", "submitter": "Florent Bouchez", "authors": "Florent Bouchez (LIP), Alain Darte (LIP), Fabrice Rastello (LIP)", "title": "On the Complexity of Spill Everywhere under SSA Form", "comments": "10 pages", "journal-ref": "ACM SIGPLAN Notices Issue 7, Volume 42 (2007) 103 - 112", "doi": "10.1145/1254766.1254782", "report-no": null, "categories": "cs.DS cs.CC", "license": null, "abstract": "  Compilation for embedded processors can be either aggressive (time consuming\ncross-compilation) or just in time (embedded and usually dynamic). The\nheuristics used in dynamic compilation are highly constrained by limited\nresources, time and memory in particular. Recent results on the SSA form open\npromising directions for the design of new register allocation heuristics for\nembedded systems and especially for embedded compilation. In particular,\nheuristics based on tree scan with two separated phases -- one for spilling,\nthen one for coloring/coalescing -- seem good candidates for designing\nmemory-friendly, fast, and competitive register allocators. Still, also because\nof the side effect on power consumption, the minimization of loads and stores\noverhead (spilling problem) is an important issue. This paper provides an\nexhaustive study of the complexity of the ``spill everywhere'' problem in the\ncontext of the SSA form. Unfortunately, conversely to our initial hopes, many\nof the questions we raised lead to NP-completeness results. We identify some\npolynomial cases but that are impractical in JIT context. Nevertheless, they\ncan give hints to simplify formulations for the design of aggressive\nallocators.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2007 07:24:58 GMT"}], "update_date": "2009-09-18", "authors_parsed": [["Bouchez", "Florent", "", "LIP"], ["Darte", "Alain", "", "LIP"], ["Rastello", "Fabrice", "", "LIP"]]}, {"id": "0710.3804", "submitter": "Thierry Mora", "authors": "Thierry Mora and Lenka Zdeborova", "title": "Random subcubes as a toy model for constraint satisfaction problems", "comments": "21 pages, 4 figures", "journal-ref": "J. Stat. Phys. 131, n. 6 (2008), 1121-1138", "doi": "10.1007/s10955-008-9543-x", "report-no": null, "categories": "cs.CC cond-mat.dis-nn", "license": null, "abstract": "  We present an exactly solvable random-subcube model inspired by the structure\nof hard constraint satisfaction and optimization problems. Our model reproduces\nthe structure of the solution space of the random k-satisfiability and\nk-coloring problems, and undergoes the same phase transitions as these\nproblems. The comparison becomes quantitative in the large-k limit. Distance\nproperties, as well the x-satisfiability threshold, are studied. The model is\nalso generalized to define a continuous energy landscape useful for studying\nseveral aspects of glassy dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2007 23:33:18 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2008 16:32:21 GMT"}], "update_date": "2008-05-23", "authors_parsed": [["Mora", "Thierry", ""], ["Zdeborova", "Lenka", ""]]}, {"id": "0710.3961", "submitter": "Victor Korotkikh", "authors": "Victor Korotkikh and Galina Korotkikh", "title": "On a New Type of Information Processing for Efficient Management of\n  Complex Systems", "comments": "5 pages, 2 figures, to be presented at the International Conference\n  on Complex Systems, Boston, October 28 - November 2, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": null, "abstract": "  It is a challenge to manage complex systems efficiently without confronting\nNP-hard problems. To address the situation we suggest to use self-organization\nprocesses of prime integer relations for information processing.\nSelf-organization processes of prime integer relations define correlation\nstructures of a complex system and can be equivalently represented by\ntransformations of two-dimensional geometrical patterns determining the\ndynamics of the system and revealing its structural complexity. Computational\nexperiments raise the possibility of an optimality condition of complex systems\npresenting the structural complexity of a system as a key to its optimization.\n  From this perspective the optimization of a system could be all about the\ncontrol of the structural complexity of the system to make it consistent with\nthe structural complexity of the problem. The experiments also indicate that\nthe performance of a complex system may behave as a concave function of the\nstructural complexity. Therefore, once the structural complexity could be\ncontrolled as a single entity, the optimization of a complex system would be\npotentially reduced to a one-dimensional concave optimization irrespective of\nthe number of variables involved its description. This might open a way to a\nnew type of information processing for efficient management of complex systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2007 01:10:02 GMT"}], "update_date": "2007-10-23", "authors_parsed": [["Korotkikh", "Victor", ""], ["Korotkikh", "Galina", ""]]}, {"id": "0710.4272", "submitter": "Leslie Ann Goldberg", "authors": "Martin Dyer, Leslie Ann Goldberg and Mark Jerrum", "title": "An approximation trichotomy for Boolean #CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a trichotomy theorem for the complexity of approximately counting the\nnumber of satisfying assignments of a Boolean CSP instance. Such problems are\nparameterised by a constraint language specifying the relations that may be\nused in constraints. If every relation in the constraint language is affine\nthen the number of satisfying assignments can be exactly counted in polynomial\ntime. Otherwise, if every relation in the constraint language is in the\nco-clone IM_2 from Post's lattice, then the problem of counting satisfying\nassignments is complete with respect to approximation-preserving reductions in\nthe complexity class #RH\\Pi_1. This means that the problem of approximately\ncounting satisfying assignments of such a CSP instance is equivalent in\ncomplexity to several other known counting problems, including the problem of\napproximately counting the number of independent sets in a bipartite graph. For\nevery other fixed constraint language, the problem is complete for #P with\nrespect to approximation-preserving reductions, meaning that there is no fully\npolynomial randomised approximation scheme for counting satisfying assignments\nunless NP=RP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2007 14:35:05 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2009 21:14:30 GMT"}], "update_date": "2009-07-23", "authors_parsed": [["Dyer", "Martin", ""], ["Goldberg", "Leslie Ann", ""], ["Jerrum", "Mark", ""]]}, {"id": "0710.4508", "submitter": "Gregorio Malajovich", "authors": "Felipe Cucker, Teresa Krick, Gregorio Malajovich and Mario Wschebor", "title": "A Numerical Algorithm for Zero Counting. I: Complexity and Accuracy", "comments": "We made minor but necessary improvements in the presentation", "journal-ref": "Journal of Complexity 24 Issues 5-6, pp 582-605 (Oct-Dec 2008)", "doi": "10.1016/j.jco.2008.03.001", "report-no": null, "categories": "cs.CC cs.NA cs.SC math.NA", "license": null, "abstract": "  We describe an algorithm to count the number of distinct real zeros of a\npolynomial (square) system f. The algorithm performs O(n D kappa(f)) iterations\nwhere n is the number of polynomials (as well as the dimension of the ambient\nspace), D is a bound on the polynomials' degree, and kappa(f) is a condition\nnumber for the system. Each iteration uses an exponential number of operations.\nThe algorithm uses finite-precision arithmetic and a polynomial bound for the\nprecision required to ensure the returned output is correct is exhibited. This\nbound is a major feature of our algorithm since it is in contrast with the\nexponential precision required by the existing (symbolic) algorithms for\ncounting real zeros. The algorithm parallelizes well in the sense that each\niteration can be computed in parallel polynomial time with an exponential\nnumber of processors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2007 16:33:07 GMT"}, {"version": "v2", "created": "Wed, 19 Mar 2008 20:28:15 GMT"}], "update_date": "2010-07-12", "authors_parsed": [["Cucker", "Felipe", ""], ["Krick", "Teresa", ""], ["Malajovich", "Gregorio", ""], ["Wschebor", "Mario", ""]]}, {"id": "0710.4680", "submitter": "EDA Publishing Association", "authors": "Diana Marculescu", "title": "Energy Bounds for Fault-Tolerant Nanoscale Designs", "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)", "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)", "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": null, "abstract": "  The problem of determining lower bounds for the energy cost of a given\nnanoscale design is addressed via a complexity theory-based approach. This\npaper provides a theoretical framework that is able to assess the trade-offs\nexisting in nanoscale designs between the amount of redundancy needed for a\ngiven level of resilience to errors and the associated energy cost. Circuit\nsize, logic depth and error resilience are analyzed and brought together in a\ntheoretical framework that can be seamlessly integrated with automated\nsynthesis tools and can guide the design process of nanoscale systems comprised\nof failure prone devices. The impact of redundancy addition on the switching\nenergy and its relationship with leakage energy is modeled in detail. Results\nshow that 99% error resilience is possible for fault-tolerant designs, but at\nthe expense of at least 40% more energy if individual gates fail independently\nwith probability of 1%.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2007 09:04:27 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Marculescu", "Diana", ""]]}, {"id": "0710.5338", "submitter": "Toshiya Itoh", "authors": "Toshiya Itoh and Osamu Watanabe", "title": "Weighted Random Popular Matchings", "comments": "13 pages, 2 figures", "journal-ref": "Random Structures and Algorithms, 37(4), pp.477-494, 2010", "doi": "10.1002/rsa.20316", "report-no": null, "categories": "cs.DM cs.CC", "license": null, "abstract": "  For a set A of n applicants and a set I of m items, we consider a problem of\ncomputing a matching of applicants to items, i.e., a function M mapping A to I;\nhere we assume that each applicant $x \\in A$ provides a preference list on\nitems in I. We say that an applicant $x \\in A$ prefers an item p than an item q\nif p is located at a higher position than q in its preference list, and we say\nthat x prefers a matching M over a matching M' if x prefers M(x) over M'(x).\nFor a given matching problem A, I, and preference lists, we say that M is more\npopular than M' if the number of applicants preferring M over M' is larger than\nthat of applicants preferring M' over M, and M is called a popular matching if\nthere is no other matching that is more popular than M. Here we consider the\nsituation that A is partitioned into $A_{1},A_{2},...,A_{k}$, and that each\n$A_{i}$ is assigned a weight $w_{i}>0$ such that w_{1}>w_{2}>...>w_{k}>0$. For\nsuch a matching problem, we say that M is more popular than M' if the total\nweight of applicants preferring M over M' is larger than that of applicants\npreferring M' over M, and we call M an k-weighted popular matching if there is\nno other matching that is more popular than M. In this paper, we analyze the\n2-weighted matching problem, and we show that (lower bound) if\n$m/n^{4/3}=o(1)$, then a random instance of the 2-weighted matching problem\nwith $w_{1} \\geq 2w_{2}$ has a 2-weighted popular matching with probability\no(1); and (upper bound) if $n^{4/3}/m = o(1)$, then a random instance of the\n2-weighted matching problem with $w_{1} \\geq 2w_{2}$ has a 2-weighted popular\nmatching with probability 1-o(1).\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2007 04:41:03 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Itoh", "Toshiya", ""], ["Watanabe", "Osamu", ""]]}, {"id": "0710.5582", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis, Christos Papadimitriou", "title": "Computing Equilibria in Anonymous Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DM", "license": null, "abstract": "  We present efficient approximation algorithms for finding Nash equilibria in\nanonymous games, that is, games in which the players utilities, though\ndifferent, do not differentiate between other players. Our results pertain to\nsuch games with many players but few strategies. We show that any such game has\nan approximate pure Nash equilibrium, computable in polynomial time, with\napproximation O(s^2 L), where s is the number of strategies and L is the\nLipschitz constant of the utilities. Finally, we show that there is a PTAS for\nfinding an epsilon\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2007 07:32:37 GMT"}], "update_date": "2007-10-31", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Papadimitriou", "Christos", ""]]}]