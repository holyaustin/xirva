[{"id": "1509.00092", "submitter": "Raghu Meka", "authors": "Raghu Meka", "title": "Explicit resilient functions matching Ajtai-Linial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function on n variables is q-resilient if for any subset of at most\nq variables, the function is very likely to be determined by a uniformly random\nassignment to the remaining n-q variables; in other words, no coalition of at\nmost q variables has significant influence on the function. Resilient functions\nhave been extensively studied with a variety of applications in cryptography,\ndistributed computing, and pseudorandomness. The best known balanced resilient\nfunction on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2\nn))-resilient. However, the construction of Ajtai and Linial is by the\nprobabilistic method and does not give an efficiently computable function.\n  In this work we give an explicit monotone depth three almost-balanced Boolean\nfunction on n bits that is Omega(n/(log^2 n))-resilient matching the work of\nAjtai and Linial. The best previous explicit construction due to Meka [Meka09]\n(which only gives a logarithmic depth function) and Chattopadhyay and\nZuckermman [CZ15] were only n^{1-c}-resilient for any constant c < 1. Our\nconstruction and analysis are motivated by (and simplifies parts of) the recent\nbreakthrough of [CZ15] giving explicit two-sources extractors for\npolylogarithmic min-entropy; a key ingredient in their result was the\nconstruction of explicit constant-depth resilient functions.\n  An important ingredient in our construction is a new randomness optimal\noblivious sampler which preserves moment generating functions of sums of\nvariables and could be useful elsewhere.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 23:27:06 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 21:03:47 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Meka", "Raghu", ""]]}, {"id": "1509.00099", "submitter": "T\\'omas Magn\\'usson", "authors": "Bjarki \\'Ag\\'ust Gu{\\dh}mundsson, T\\'omas Ken Magn\\'usson, Bj\\\"orn\n  Orri S{\\ae}mundsson", "title": "Bounds and Fixed-Parameter Algorithms for Weighted Improper Coloring\n  (Extended Version)", "comments": "18 pages, 5 figures, extended version for additional proofs in\n  appendix for 16th Italian Conference on Theoretical Computer Science 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the weighted improper coloring problem, a generalization of\ndefective coloring. We present some hardness results and in particular we show\nthat weighted improper coloring is not fixed-parameter tractable when\nparameterized by pathwidth. We generalize bounds for defective coloring to\nweighted improper coloring and give a bound for weighted improper coloring in\nterms of the sum of edge weights. Finally we give fixed-parameter algorithms\nfor weighted improper coloring both when parameterized by treewidth and maximum\ndegree and when parameterized by treewidth and precision of edge weights. In\nparticular, we obtain a linear-time algorithm for weighted improper coloring of\ninterval graphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 00:15:32 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Gu\u00f0mundsson", "Bjarki \u00c1g\u00fast", ""], ["Magn\u00fasson", "T\u00f3mas Ken", ""], ["S\u00e6mundsson", "Bj\u00f6rn Orri", ""]]}, {"id": "1509.00279", "submitter": "Daniel Augot", "authors": "Daniel Augot (GRACE), Fran\\c{c}oise Levy-Dit-Vehel (ENSTA ParisTech\n  UMA), Man Cuong Ng\\^o (GRACE)", "title": "Information Sets of Multiplicity Codes", "comments": "International Symposium on Information Theory, Jun 2015, Hong-Kong,\n  China. IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here provide a method for systematic encoding of the Multiplicity codes\nintroduced by Kopparty, Saraf and Yekhanin in 2011. The construction is built\non an idea of Kop-party. We properly define information sets for these codes\nand give detailed proofs of the validity of Kopparty's construction, that use\ngenerating functions. We also give a complexity estimate of the associated\nencoding algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 13:27:41 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Augot", "Daniel", "", "GRACE"], ["Levy-Dit-Vehel", "Fran\u00e7oise", "", "ENSTA ParisTech\n  UMA"], ["Ng\u00f4", "Man Cuong", "", "GRACE"]]}, {"id": "1509.00524", "submitter": "Jason Rute", "authors": "Joseph S. Miller, Jason Rute", "title": "Energy randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy randomness is a notion of partial randomness introduced by\nDiamondstone and Kjos-Hanssen to characterize the sequences that can be\nelements of a Martin-L\\\"of random closed set (in the sense of Barmpalias,\nBrodhead, Cenzer, Dashti, and Weber). It has also been applied by Allen,\nBienvenu, and Slaman to the characterization of the possible zero times of a\nMartin-L\\\"of random Brownian motion. In this paper, we show that $X \\in\n2^\\omega$ is $s$-energy random if and only if $\\sum_{n\\in\\omega} 2^{sn -\nKM(X\\upharpoonright n)} < \\infty$, providing a characterization of energy\nrandomness via a priori complexity $KM$. This is related to a question of\nAllen, Bienvenu, and Slaman.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 23:36:59 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Miller", "Joseph S.", ""], ["Rute", "Jason", ""]]}, {"id": "1509.01014", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata and Yuichi Yoshida", "title": "On the Equivalence among Problems of Bounded Width", "comments": "accepted to ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology, called decomposition-based\nreductions, for showing the equivalence among various problems of\nbounded-width.\n  First, we show that the following are equivalent for any $\\alpha > 0$:\n  * SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * 3-SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * Max 2-SAT can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time,\n  * Independent Set can be solved in $O^*(2^{\\alpha \\mathrm{tw}})$ time, and\n  * Independent Set can be solved in $O^*(2^{\\alpha \\mathrm{cw}})$ time, where\ntw and cw are the tree-width and clique-width of the instance, respectively.\n  Then, we introduce a new parameterized complexity class EPNL, which includes\nSet Cover and Directed Hamiltonicity, and show that SAT, 3-SAT, Max 2-SAT, and\nIndependent Set parameterized by path-width are EPNL-complete. This implies\nthat if one of these EPNL-complete problems can be solved in $O^*(c^k)$ time,\nthen any problem in EPNL can be solved in $O^*(c^k)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:58:50 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Iwata", "Yoichi", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1509.01866", "submitter": "Richard Taylor Dr", "authors": "Richard Taylor", "title": "Approximation of the Quadratic Knapsack Problem", "comments": "8 pages one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any given $\\epsilon>0$ we provide an algorithm for the Quadratic Knapsack\nProblem that has an approximation ratio within $O(n^{2/5+\\epsilon})$ and a run\ntime within $O(n^{9/\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 23:21:39 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 01:12:25 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Taylor", "Richard", ""]]}, {"id": "1509.02348", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "On the complexity of piecewise affine system identification", "comments": "Automatica, International Federation of Automatic Control, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides results regarding the computational complexity of hybrid\nsystem identification. More precisely, we focus on the estimation of piecewise\naffine (PWA) maps from input-output data and analyze the complexity of\ncomputing a global minimizer of the error. Previous work showed that a global\nsolution could be obtained for continuous PWA maps with a worst-case complexity\nexponential in the number of data. In this paper, we show how global optimality\ncan be reached for a slightly more general class of possibly discontinuous PWA\nmaps with a complexity only polynomial in the number of data, however with an\nexponential complexity with respect to the data dimension. This result is\nobtained via an analysis of the intrinsic classification subproblem of\nassociating the data points to the different modes. In addition, we prove that\nthe problem is NP-hard, and thus that the exponential complexity in the\ndimension is a natural expectation for any exact algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 13:03:19 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1509.02503", "submitter": "J. M. Landsberg", "authors": "J.M. Landsberg", "title": "An introduction to geometric complexity theory", "comments": "Draft of article to appear in the Newsletter of the European\n  Mathematical Society. 9 pages in original, arXiv version has extra spaces due\n  to arXiv processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC math.DG math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I survey methods from differential geometry, algebraic geometry and\nrepresentation theory relevant for the permanent v. determinant problem from\ncomputer science, an algebraic analog of the P v. NP problem.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 19:16:33 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Landsberg", "J. M.", ""]]}, {"id": "1509.02557", "submitter": "Timothy Riley", "authors": "W. Dison, E. Einstein and T.R. Riley", "title": "Taming the hydra: the word problem and extreme integer compression", "comments": "63 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finitely presented group, the word problem asks for an algorithm which\ndeclares whether or not words on the generators represent the identity. The\nDehn function is a complexity measure of a direct attack on the word problem by\napplying the defining relations. Dison & Riley showed that a \"hydra phenomenon\"\ngives rise to novel groups with extremely fast growing (Ackermannian) Dehn\nfunctions. Here we show that nevertheless, there are efficient (polynomial\ntime) solutions to the word problems of these groups. Our main innovation is a\nmeans of computing efficiently with enormous integers which are represented in\ncompressed forms by strings of Ackermann functions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 21:03:24 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Dison", "W.", ""], ["Einstein", "E.", ""], ["Riley", "T. R.", ""]]}, {"id": "1509.02673", "submitter": "Mihai Oltean", "authors": "Mihai Oltean, Oana Muntean", "title": "Solving NP-complete problems with delayed signals: an overview of\n  current research directions", "comments": "in Proceedings of 1st international workshop on Optical\n  SuperComputing, LNCS 5172, pp. 115-128, Springer-Verlag, 2008, more info at:\n  http://www.cs.ubbcluj.ro/~moltean/optical", "journal-ref": null, "doi": "10.1007/978-3-540-85673-3_10", "report-no": null, "categories": "cs.ET cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we summarize the existing principles for building\nunconventional computing devices that involve delayed signals for encoding\nsolutions to NP-complete problems. We are interested in the following aspects:\nthe properties of the signal, the operations performed within the devices, some\ncomponents required for the physical implementation, precision required for\ncorrectly reading the solution and the decrease in the signal's strength. Six\nproblems have been solved so far by using the above enumerated principles:\nHamiltonian path, travelling salesman, bounded and unbounded subset sum,\nDiophantine equations and exact cover. For the hardware implementation several\ntypes of signals can be used: light, electric power, sound, electro-magnetic\netc.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 08:14:52 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Oltean", "Mihai", ""], ["Muntean", "Oana", ""]]}, {"id": "1509.02683", "submitter": "Tom C. van der Zanden", "authors": "Tom C. van der Zanden", "title": "Parameterized Complexity of Graph Constraint Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph constraint logic is a framework introduced by Hearn and Demaine, which\nprovides several problems that are often a convenient starting point for\nreductions. We study the parameterized complexity of Constraint Graph\nSatisfiability and both bounded and unbounded versions of Nondeterministic\nConstraint Logic (NCL) with respect to solution length, treewidth and maximum\ndegree of the underlying constraint graph as parameters. As a main result we\nshow that restricted NCL remains PSPACE-complete on graphs of bounded\nbandwidth, strengthening Hearn and Demaine's framework. This allows us to\nimprove upon existing results obtained by reduction from NCL. We show that\nreconfiguration versions of several classical graph problems (including\nindependent set, feedback vertex set and dominating set) are PSPACE-complete on\nplanar graphs of bounded bandwidth and that Rush Hour, generalized to $k\\times\nn$ boards, is PSPACE-complete even when $k$ is at most a constant.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 09:04:21 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["van der Zanden", "Tom C.", ""]]}, {"id": "1509.03014", "submitter": "EPTCS", "authors": "Naohi Eguchi", "title": "Formalizing Termination Proofs under Polynomial Quasi-interpretations", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 33-47", "doi": "10.4204/EPTCS.191.5", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usual termination proofs for a functional program require to check all the\npossible reduction paths. Due to an exponential gap between the height and size\nof such the reduction tree, no naive formalization of termination proofs yields\na connection to the polynomial complexity of the given program. We solve this\nproblem employing the notion of minimal function graph, a set of pairs of a\nterm and its normal form, which is defined as the least fixed point of a\nmonotone operator. We show that termination proofs for programs reducing under\nlexicographic path orders (LPOs for short) and polynomially quasi-interpretable\ncan be optimally performed in a weak fragment of Peano arithmetic. This yields\nan alternative proof of the fact that every function computed by an\nLPO-terminating, polynomially quasi-interpretable program is computable in\npolynomial space. The formalization is indeed optimal since every\npolynomial-space computable function can be computed by such a program. The\ncrucial observation is that inductive definitions of minimal function graphs\nunder LPO-terminating programs can be approximated with transfinite induction\nalong LPOs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 05:31:31 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Eguchi", "Naohi", ""]]}, {"id": "1509.03057", "submitter": "Tomoyuki Yamakami", "authors": "Tomoyuki Yamakami", "title": "The World of Combinatorial Fuzzy Problems and the Efficiency of Fuzzy\n  Approximation Algorithms", "comments": "A4, 10pt, 10 pages. This extended abstract already appeared in the\n  Proceedings of the Joint 7th International Conference on Soft Computing and\n  Intelligent Systems (SCIS 2014) and 15th International Symposium on Advanced\n  Intelligent Systems (ISIS 2014), December 3-6, 2014, Institute of Electrical\n  and Electronics Engineers (IEEE), pp. 29-35, 2014", "journal-ref": null, "doi": "10.1109/SCIS-ISIS.2014.7044695", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-examine a practical aspect of combinatorial fuzzy problems of various\ntypes, including search, counting, optimization, and decision problems. We are\nfocused only on those fuzzy problems that take series of fuzzy input objects\nand produce fuzzy values. To solve such problems efficiently, we design fast\nfuzzy algorithms, which are modeled by polynomial-time deterministic fuzzy\nTuring machines equipped with read-only auxiliary tapes and write-only output\ntapes and also modeled by polynomial-size fuzzy circuits composed of fuzzy\ngates. We also introduce fuzzy proof verification systems to model the\nfuzzification of nondeterminism. Those models help us identify four complexity\nclasses: Fuzzy-FPA of fuzzy functions, Fuzzy-PA and Fuzzy-NPA of fuzzy decision\nproblems, and Fuzzy-NPAO of fuzzy optimization problems. Based on a relative\napproximation scheme targeting fuzzy membership degree, we formulate two\nnotions of \"reducibility\" in order to compare the computational complexity of\ntwo fuzzy problems. These reducibility notions make it possible to locate the\nmost difficult fuzzy problems in Fuzzy-NPA and in Fuzzy-NPAO.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 09:07:31 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Yamakami", "Tomoyuki", ""]]}, {"id": "1509.03543", "submitter": "Mark Jerrum", "authors": "Leslie Ann Goldberg and Mark Jerrum", "title": "The complexity of counting locally maximal satisfying assignments of\n  Boolean CSPs", "comments": "V2 adds contextual material relating the results obtained here to\n  earlier work in a different but related setting. The technical content is\n  unchanged. V3 (this version) incorporates minor revisions. The title has been\n  changed to better reflect what is novel in this work. This version has been\n  accepted for publication in Theoretical Computer Science. 19 pages", "journal-ref": null, "doi": "10.1016/j.tcs.2016.04.008", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of the problem of counting the\nmaximal satisfying assignments of a Constraint Satisfaction Problem (CSP) over\nthe Boolean domain {0,1}. A satisfying assignment is maximal if any new\nassignment which is obtained from it by changing a 0 to a 1 is unsatisfying.\nFor each constraint language Gamma, #MaximalCSP(Gamma) denotes the problem of\ncounting the maximal satisfying assignments, given an input CSP with\nconstraints in Gamma. We give a complexity dichotomy for the problem of exactly\ncounting the maximal satisfying assignments and a complexity trichotomy for the\nproblem of approximately counting them. Relative to the problem #CSP(Gamma),\nwhich is the problem of counting all satisfying assignments, the maximal\nversion can sometimes be easier but never harder. This finding contrasts with\nthe recent discovery that approximately counting maximal independent sets in a\nbipartite graph is harder (under the usual complexity-theoretic assumptions)\nthan counting all independent sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 15:07:08 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 12:23:09 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 01:07:32 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Goldberg", "Leslie Ann", ""], ["Jerrum", "Mark", ""]]}, {"id": "1509.03712", "submitter": "Ugur Kucuk", "authors": "U\\u{g}ur K\\\"u\\c{c}\\\"uk, A. C. Cem Say, Abuzer Yakary{\\i}lmaz", "title": "Inkdots as advice for finite automata", "comments": "14 pages", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  3, Automata, Logic and Semantics (September 26, 2017) dmtcs:3941", "doi": "10.23638/DMTCS-19-3-1", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine inkdots placed on the input string as a way of providing advice to\nfinite automata, and establish the relations between this model and the\npreviously studied models of advised finite automata. The existence of an\ninfinite hierarchy of classes of languages that can be recognized with the help\nof increasing numbers of inkdots as advice is shown. The effects of different\nforms of advice on the succinctness of the advised machines are examined. We\nalso study randomly placed inkdots as advice to probabilistic finite automata,\nand demonstrate the superiority of this model over its deterministic version.\nEven very slowly growing amounts of space can become a resource of meaningful\nuse if the underlying advised model is extended with access to secondary\nmemory, while it is famously known that such small amounts of space are not\nuseful for unadvised one-way Turing machines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 06:41:00 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 19:05:47 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 14:12:30 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["K\u00fc\u00e7\u00fck", "U\u011fur", ""], ["Say", "A. C. Cem", ""], ["Yakary\u0131lmaz", "Abuzer", ""]]}, {"id": "1509.03976", "submitter": "Mikael Gast", "authors": "Mikael Gast, Mathias Hauptmann and Marek Karpinski", "title": "Approximability of TSP on Power Law Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the special case of Graphic TSP where the underlying\ngraph is a power law graph (PLG). We give a refined analysis of some of the\ncurrent best approximation algorithms and show that an improved approximation\nratio can be achieved for certain ranges of the power law exponent $\\beta$. For\nthe value of power law exponent $\\beta=1.5$ we obtain an approximation ratio of\n$1.34$ for Graphic TSP. Moreover we study the $(1,2)$-TSP with the underlying\ngraph of $1$-edges being a PLG. We show improved approximation ratios in the\ncase of underlying deterministic PLGs for $\\beta$ greater than $1.666$. For\nunderlying random PLGs we further improve the analysis and show even better\nexpected approximation ratio for the range of $\\beta$ between $1$ and $3.5$. On\nthe other hand we prove the first explicit inapproximability bounds for\n$(1,2)$-TSP for an underlying power law graph.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:50:27 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1509.04764", "submitter": "Mary Wootters", "authors": "Venkatesan Guruswami, Mary Wootters", "title": "Repairing Reed-Solomon Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of Reed-Solomon (RS) codes for the \\em exact repair\nproblem \\em in distributed storage. Our main result is that, in some parameter\nregimes, Reed-Solomon codes are optimal regenerating codes, among MDS codes\nwith linear repair schemes. Moreover, we give a characterization of MDS codes\nwith linear repair schemes which holds in any parameter regime, and which can\nbe used to give non-trivial repair schemes for RS codes in other settings.\n  More precisely, we show that for $k$-dimensional RS codes whose evaluation\npoints are a finite field of size $n$, there are exact repair schemes with\nbandwidth $(n-1)\\log((n-1)/(n-k))$ bits, and that this is optimal for any MDS\ncode with a linear repair scheme. In contrast, the naive (commonly implemented)\nrepair algorithm for this RS code has bandwidth $k\\log(n)$ bits. When the\nentire field is used as evaluation points, the number of nodes $n$ is much\nlarger than the number of bits per node (which is $O(\\log(n))$), and so this\nresult holds only when the degree of sub-packetization is small. However, our\nmethod applies in any parameter regime, and to illustrate this for high levels\nof sub-packetization we give an improved repair scheme for a specific\n(14,10)-RS code used in the Facebook Hadoop Analytics cluster.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 22:52:50 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 14:56:14 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Wootters", "Mary", ""]]}, {"id": "1509.05065", "submitter": "Fernando Brandao", "authors": "Fernando G.S.L. Brandao, Aram W. Harrow", "title": "Estimating operator norms using covering nets", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several polynomial- and quasipolynomial-time approximation schemes\nfor a large class of generalized operator norms. Special cases include the\n$2\\rightarrow q$ norm of matrices for $q>2$, the support function of the set of\nseparable quantum states, finding the least noisy output of\nentanglement-breaking quantum channels, and approximating the injective tensor\nnorm for a map between two Banach spaces whose factorization norm through\n$\\ell_1^n$ is bounded.\n  These reproduce and in some cases improve upon the performance of previous\nalgorithms by Brand\\~ao-Christandl-Yard and followup work, which were based on\nthe Sum-of-Squares hierarchy and whose analysis used techniques from quantum\ninformation such as the monogamy principle of entanglement. Our algorithms, by\ncontrast, are based on brute force enumeration over carefully chosen covering\nnets. These have the advantage of using less memory, having much simpler proofs\nand giving new geometric insights into the problem. Net-based algorithms for\nsimilar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in\neach case with a run-time that is exponential in the rank of some matrix. We\nachieve polynomial or quasipolynomial runtimes by using the much smaller nets\nthat exist in $\\ell_1$ spaces. This principle has been used in learning theory,\nwhere it is known as Maurey's empirical method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 21:08:30 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Brandao", "Fernando G. S. L.", ""], ["Harrow", "Aram W.", ""]]}, {"id": "1509.05572", "submitter": "Kitty Meeks", "authors": "Kitty Meeks", "title": "Randomised enumeration of small witnesses using a decision oracle", "comments": "To appear in Algorithmica. Author final version, incorporating\n  reviewer comments. An extended abstract of part of this work appeared in proc\n  IPEC '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems involve determining whether a universe of $n$\nelements contains a witness consisting of $k$ elements which have some\nspecified property. In this paper we investigate the relationship between the\ndecision and enumeration versions of such problems: efficient methods are known\nfor transforming a decision algorithm into a search procedure that finds a\nsingle witness, but even finding a second witness is not so straightforward in\ngeneral. We show that, if the decision version of the problem can be solved in\ntime $f(k) \\cdot poly(n)$, there is a randomised algorithm which enumerates all\nwitnesses in time $e^{k + o(k)} \\cdot f(k) \\cdot poly(n) \\cdot N$, where $N$ is\nthe total number of witnesses. If the decision version of the problem is solved\nby a randomised algorithm which may return false negatives, then the same\nmethod allows us to output a list of witnesses in which any given witness will\nbe included with high probability. The enumeration algorithm also gives rise to\nan efficient algorithm to count the total number of witnesses when this number\nis small.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 10:20:24 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 09:54:51 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 11:04:55 GMT"}, {"version": "v4", "created": "Thu, 4 Jan 2018 10:46:20 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Meeks", "Kitty", ""]]}, {"id": "1509.05623", "submitter": "Arnaud Mary", "authors": "Arnaud Mary and Yann Strozecki", "title": "Efficient enumeration of solutions produced by closure operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we address the problem of generating all elements obtained by\nthe saturation of an initial set by some operations. More precisely, we prove\nthat we can generate the closure by polymorphisms of a boolean relation with a\npolynomial delay. This implies for instance that we can compute with polynomial\ndelay the closure of a family of sets by any set of \"set operations\" (e.g. by\nunion, intersection, difference, symmetric difference$\\dots$). To do so, we\nprove that for any set of operations $\\mathcal{F}$, one can decide in\npolynomial time whether an elements belongs to the closure by $\\mathcal{F}$ of\na family of sets. When the relation is over a domain larger than two elements,\nour generic enumeration method fails for some cases, since the associated\ndecision problem is $NP$-hard and we provide an alternative algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 13:33:19 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 07:08:46 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Mary", "Arnaud", ""], ["Strozecki", "Yann", ""]]}, {"id": "1509.05637", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "The Shortest Path Problem with Edge Information Reuse is NP-Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the following variation of the single-source shortest path\nproblem is NP-complete. Let a weighted, directed, acyclic graph $G=(V,E,w)$\nwith source and sink vertices $s$ and $t$ be given. Let in addition a mapping\n$f$ on $E$ be given that associates information with the edges (e.g., a\npointer), such that $f(e)=f(e')$ means that edges $e$ and $e'$ carry the same\ninformation; for such edges it is required that $w(e)=w(e')$. The length of a\nsimple $st$ path $U$ is the sum of the weights of the edges on $U$ but edges\nwith $f(e)=f(e')$ are counted only once. The problem is to determine a shortest\nsuch $st$ path. We call this problem the \\emph{edge information reuse shortest\npath problem}. It is NP-complete by reduction from 3SAT.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 14:17:59 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 14:39:53 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 10:34:08 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1509.05662", "submitter": "Ching-Lueh Chang", "authors": "Ching-Lueh Chang", "title": "Metric $1$-median selection: Query complexity vs. approximation ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of finding a point in a metric space\n$(\\{1,2,\\ldots,n\\},d)$ with the minimum average distance to other points. We\nshow that this problem has no deterministic $o(n^{1+1/(h-1)})$-query\n$(2h-\\Omega(1))$-approximation algorithms for any constant\n$h\\in\\mathbb{Z}^+\\setminus\\{1\\}$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:23:32 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Chang", "Ching-Lueh", ""]]}, {"id": "1509.05725", "submitter": "Sebastian Ordyniak", "authors": "Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider,\n  and Stanislav \\v{Z}ivn\\'y", "title": "Backdoors into Heterogeneous Classes of SAT and CSP", "comments": "to appear in JCSS, full version of an AAAI 2014 paper", "journal-ref": "Journal of Computer and System Sciences 85 38-56 (2017)", "doi": "10.1016/j.jcss.2016.10.007", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the classical notion of strong and weak backdoor sets\nfor SAT and CSP by allowing that different instantiations of the backdoor\nvariables result in instances that belong to different base classes; the union\nof the base classes forms a heterogeneous base class. Backdoor sets to\nheterogeneous base classes can be much smaller than backdoor sets to\nhomogeneous ones, hence they are much more desirable but possibly harder to\nfind. We draw a detailed complexity landscape for the problem of detecting\nstrong and weak backdoor sets into heterogeneous base classes for SAT and CSP.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 17:39:33 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 09:49:56 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Gaspers", "Serge", ""], ["Misra", "Neeldhara", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""], ["\u017divn\u00fd", "Stanislav", ""]]}, {"id": "1509.05806", "submitter": "Juan Bermejo-Vega", "authors": "Juan Bermejo-Vega, Kevin C. Zatloukal", "title": "Abelian Hypergroups and Quantum Computation", "comments": "41 pages + 6 pages appendices. Added references and corrected typos\n  in this version (sections 1-2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a connection, described here for the first time, between the\nhidden normal subgroup problem (HNSP) and abelian hypergroups (algebraic\nobjects that model collisions of physical particles), we develop a stabilizer\nformalism using abelian hypergroups and an associated classical simulation\ntheorem (a la Gottesman-Knill). Using these tools, we develop the first\nprovably efficient quantum algorithm for finding hidden subhypergroups of\nnilpotent abelian hypergroups and, via the aforementioned connection, a new,\nhypergroup-based algorithm for the HNSP on nilpotent groups. We also give\nefficient methods for manipulating non-unitary, non-monomial stabilizers and an\nadaptive Fourier sampling technique of general interest.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:44:22 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 15:19:00 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Bermejo-Vega", "Juan", ""], ["Zatloukal", "Kevin C.", ""]]}, {"id": "1509.05896", "submitter": "Marcin Wrochna", "authors": "Micha{\\l} Pilipczuk, Marcin Wrochna", "title": "On space efficiency of algorithms working on structural decompositions\n  of graphs", "comments": "An extended abstract appeared in the proceedings of STACS'16. The new\n  version is augmented with a space-efficient algorithm for Dominating Set\n  using the Chinese remainder theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming on path and tree decompositions of graphs is a technique\nthat is ubiquitous in the field of parameterized and exponential-time\nalgorithms. However, one of its drawbacks is that the space usage is\nexponential in the decomposition's width. Following the work of Allender et al.\n[Theory of Computing, '14], we investigate whether this space complexity\nexplosion is unavoidable. Using the idea of reparameterization of Cai and\nJuedes [J. Comput. Syst. Sci., '03], we prove that the question is closely\nrelated to a conjecture that the Longest Common Subsequence problem\nparameterized by the number of input strings does not admit an algorithm that\nsimultaneously uses XP time and FPT space. Moreover, we complete the complexity\nlandscape sketched for pathwidth and treewidth by Allender et al. by\nconsidering the parameter tree-depth. We prove that computations on tree-depth\ndecompositions correspond to a model of non-deterministic machines that work in\npolynomial time and logarithmic space, with access to an auxiliary stack of\nmaximum height equal to the decomposition's depth. Together with the results of\nAllender et al., this describes a hierarchy of complexity classes for\npolynomial-time non-deterministic machines with different restrictions on the\naccess to working space, which mirrors the classic relations between treewidth,\npathwidth, and tree-depth.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 14:20:05 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 09:12:34 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "1509.06257", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Communication Complexity (for Algorithm Designers)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document collects the lecture notes from my course \"Communication\nComplexity (for Algorithm Designers),'' taught at Stanford in the winter\nquarter of 2015. The two primary goals of the course are: 1. Learn several\ncanonical problems that have proved the most useful for proving lower bounds\n(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds\nfor fundamental algorithmic problems to communication complexity lower bounds.\nAlong the way, we'll also: 3. Get exposure to lots of cool computational models\nand some famous results about them --- data streams and linear sketches,\ncompressive sensing, space-query time trade-offs in data structures,\nsublinear-time algorithms, and the extension complexity of linear programs. 4.\nScratch the surface of techniques for proving communication complexity lower\nbounds (fooling sets, corruption bounds, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 14:59:05 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "1509.06338", "submitter": "Hector Zenil", "authors": "Hector Zenil, James A.R. Marshall and Jesper Tegn\\'er", "title": "Approximations of Algorithmic and Structural Complexity Validate\n  Cognitive-behavioural Experimental Results", "comments": "28 pages, 7 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply methods for estimating the algorithmic complexity of sequences to\nbehavioural sequences of three landmark studies of animal behavior each of\nincreasing sophistication, including foraging communication by ants, flight\npatterns of fruit flies, and tactical deception and competition strategies in\nrodents. In each case, we demonstrate that approximations of Logical Depth and\nKolmogorv-Chaitin complexity capture and validate previously reported results,\nin contrast to other measures such as Shannon Entropy, compression or ad hoc.\nOur method is practically useful when dealing with short sequences, such as\nthose often encountered in cognitive-behavioural research. Our analysis\nsupports and reveals non-random behavior (LD and K complexity) in flies even in\nthe absence of external stimuli, and confirms the \"stochastic\" behaviour of\ntransgenic rats when faced that they cannot defeat by counter prediction. The\nmethod constitutes a formal approach for testing hypotheses about the\nmechanisms underlying animal behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 18:59:08 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 11:00:12 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 08:25:43 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2015 12:37:19 GMT"}, {"version": "v5", "created": "Sat, 26 Sep 2015 09:47:26 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zenil", "Hector", ""], ["Marshall", "James A. R.", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1509.06357", "submitter": "Daniel Paulusma", "authors": "Paul Bonsma, Daniel Paulusma", "title": "Using Contracted Solution Graphs for Solving Reconfiguration Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in a general setting a dynamic programming method for solving\nreconfiguration problems. Our method is based on contracted solution graphs,\nwhich are obtained from solution graphs by performing an appropriate series of\nedge contractions that decrease the graph size without losing any critical\ninformation needed to solve the reconfiguration problem under consideration.\nOur general framework captures the approach behind known reconfiguration\nresults of Bonsma (2012) and Hatanaka, Ito and Zhou (2014). As a third example,\nwe apply the method to the following problem: given two $k$-colorings $\\alpha$\nand $\\beta$ of a graph $G$, can $\\alpha$ be modified into $\\beta$ by recoloring\none vertex of $G$ at a time, while maintaining a $k$-coloring throughout? This\nproblem is known to be PSPACE-hard even for bipartite planar graphs and $k=4$.\nBy applying our method in combination with a thorough exploitation of the graph\nstructure we obtain a polynomial time algorithm for $(k-2)$-connected chordal\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 19:35:26 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 18:29:57 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Bonsma", "Paul", ""], ["Paulusma", "Daniel", ""]]}, {"id": "1509.06361", "submitter": "Supartha Podder", "authors": "Raghav Kulkarni, Supartha Podder", "title": "Quantum Query Complexity of Subgraph Isomorphism and Homomorphism", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $H$ be a fixed graph on $n$ vertices. Let $f_H(G) = 1$ iff the input\ngraph $G$ on $n$ vertices contains $H$ as a (not necessarily induced) subgraph.\nLet $\\alpha_H$ denote the cardinality of a maximum independent set of $H$. In\nthis paper we show:\n  \\[Q(f_H) = \\Omega\\left(\\sqrt{\\alpha_H \\cdot n}\\right),\\] where $Q(f_H)$\ndenotes the quantum query complexity of $f_H$.\n  As a consequence we obtain a lower bounds for $Q(f_H)$ in terms of several\nother parameters of $H$ such as the average degree, minimum vertex cover,\nchromatic number, and the critical probability.\n  We also use the above bound to show that $Q(f_H) = \\Omega(n^{3/4})$ for any\n$H$, improving on the previously best known bound of $\\Omega(n^{2/3})$. Until\nvery recently, it was believed that the quantum query complexity is at least\nsquare root of the randomized one. Our $\\Omega(n^{3/4})$ bound for $Q(f_H)$\nmatches the square root of the current best known bound for the randomized\nquery complexity of $f_H$, which is $\\Omega(n^{3/2})$ due to Gr\\\"oger.\nInterestingly, the randomized bound of $\\Omega(\\alpha_H \\cdot n)$ for $f_H$\nstill remains open.\n  We also study the Subgraph Homomorphism Problem, denoted by $f_{[H]}$, and\nshow that $Q(f_{[H]}) = \\Omega(n)$.\n  Finally we extend our results to the $3$-uniform hypergraphs. In particular,\nwe show an $\\Omega(n^{4/5})$ bound for quantum query complexity of the Subgraph\nIsomorphism, improving on the previously known $\\Omega(n^{3/4})$ bound. For the\nSubgraph Homomorphism, we obtain an $\\Omega(n^{3/2})$ bound for the same.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 19:54:51 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 02:53:42 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Kulkarni", "Raghav", ""], ["Podder", "Supartha", ""]]}, {"id": "1509.06984", "submitter": "Max Bannach", "authors": "Max Bannach, Christoph Stockhusen, Till Tantau", "title": "Fast Parallel Fixed-Parameter Algorithms via Color Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fixed-parameter algorithms have been successfully applied to solve numerous\ndifficult problems within acceptable time bounds on large inputs. However, most\nfixed-parameter algorithms are inherently \\emph{sequential} and, thus, make no\nuse of the parallel hardware present in modern computers. We show that parallel\nfixed-parameter algorithms do not only exist for numerous parameterized\nproblems from the literature -- including vertex cover, packing problems,\ncluster editing, cutting vertices, finding embeddings, or finding matchings --\nbut that there are parallel algorithms working in \\emph{constant} time or at\nleast in time \\emph{depending only on the parameter} (and not on the size of\nthe input) for these problems. Phrased in terms of complexity classes, we place\nnumerous natural parameterized problems in parameterized versions of AC$^0$. On\na more technical level, we show how the \\emph{color coding} method can be\nimplemented in constant time and apply it to embedding problems for graphs of\nbounded tree-width or tree-depth and to model checking first-order formulas in\ngraphs of bounded degree.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 13:55:36 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Bannach", "Max", ""], ["Stockhusen", "Christoph", ""], ["Tantau", "Till", ""]]}, {"id": "1509.07200", "submitter": "EPTCS", "authors": "Christian Wurm (Universit\\\"at D\\\"usseldorf)", "title": "Synchronous Subsequentiality and Approximations to Undecidable Problems", "comments": "In Proceedings GandALF 2015, arXiv:1509.06858", "journal-ref": "EPTCS 193, 2015, pp. 58-72", "doi": "10.4204/EPTCS.193.5", "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the class of synchronous subsequential relations, a subclass of\nthe synchronous relations which embodies some properties of subsequential\nrelations. If we take relations of this class as forming the possible\ntransitions of an infinite automaton, then most decision problems (apart from\nmembership) still remain undecidable (as they are for synchronous and\nsubsequential rational relations), but on the positive side, they can be\napproximated in a meaningful way we make precise in this paper. This might make\nthe class useful for some applications, and might serve to establish an\nintermediate position in the trade-off between issues of expressivity and\n(un)decidability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 01:52:37 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Wurm", "Christian", "", "Universit\u00e4t D\u00fcsseldorf"]]}, {"id": "1509.07276", "submitter": "Harumichi Nishimura", "authors": "Keisuke Fujii, Hirotada Kobayashi, Tomoyuki Morimae, Harumichi\n  Nishimura, Shuhei Tamate, Seiichiro Tani", "title": "Power of Quantum Computation with Few Clean Qubits", "comments": "44 pages + cover page; the results in Section 8 are overlapping with\n  the main results in arXiv:1409.6777", "journal-ref": "Proceedings of 43rd International Colloquium on Automata,\n  Languages, and Programming (ICALP 2016), pp. 13:1-13:14", "doi": "10.4230/LIPIcs.ICALP.2016.13", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the power of polynomial-time quantum computation in\nwhich only a very limited number of qubits are initially clean in the |0>\nstate, and all the remaining qubits are initially in the totally mixed state.\nNo initializations of qubits are allowed during the computation, nor\nintermediate measurements. The main results of this paper are unexpectedly\nstrong error-reducible properties of such quantum computations. It is proved\nthat any problem solvable by a polynomial-time quantum computation with\none-sided bounded error that uses logarithmically many clean qubits can also be\nsolvable with exponentially small one-sided error using just two clean qubits,\nand with polynomially small one-sided error using just one clean qubit. It is\nfurther proved in the case of two-sided bounded error that any problem solvable\nby such a computation with a constant gap between completeness and soundness\nusing logarithmically many clean qubits can also be solvable with exponentially\nsmall two-sided error using just two clean qubits. If only one clean qubit is\navailable, the problem is again still solvable with exponentially small error\nin one of the completeness and soundness and polynomially small error in the\nother. As an immediate consequence of the above result for the two-sided-error\ncase, it follows that the TRACE ESTIMATION problem defined with fixed constant\nthreshold parameters is complete for the classes of problems solvable by\npolynomial-time quantum computations with completeness 2/3 and soundness 1/3\nusing logarithmically many clean qubits and just one clean qubit. The\ntechniques used for proving the error-reduction results may be of independent\ninterest in themselves, and one of the technical tools can also be used to show\nthe hardness of weak classical simulations of one-clean-qubit computations\n(i.e., DQC1 computations).\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 08:48:44 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Fujii", "Keisuke", ""], ["Kobayashi", "Hirotada", ""], ["Morimae", "Tomoyuki", ""], ["Nishimura", "Harumichi", ""], ["Tamate", "Shuhei", ""], ["Tani", "Seiichiro", ""]]}, {"id": "1509.07437", "submitter": "Astrid Pieterse", "authors": "Bart M.P. Jansen and Astrid Pieterse", "title": "Sparsification Upper and Lower Bounds for Graph Problems and\n  Not-All-Equal SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several sparsification lower and upper bounds for classic problems\nin graph theory and logic. For the problems 4-Coloring, (Directed) Hamiltonian\nCycle, and (Connected) Dominating Set, we prove that there is no\npolynomial-time algorithm that reduces any n-vertex input to an equivalent\ninstance, of an arbitrary problem, with bitsize O(n^{2-e}) for e > 0, unless NP\nis in coNP/poly and the polynomial-time hierarchy collapses. These results\nimply that existing linear-vertex kernels for k-Nonblocker and k-Max Leaf\nSpanning Tree (the parametric duals of (Connected) Dominating Set) cannot be\nimproved to have O(k^{2-e}) edges, unless NP is in coNP/poly. We also present a\npositive result and exhibit a non-trivial sparsification algorithm for\nd-Not-All-Equal SAT. We give an algorithm that reduces an n-variable input with\nclauses of size at most d to an equivalent input with O(n^{d-1}) clauses, for\nany fixed d. Our algorithm is based on a linear-algebraic proof of Lovasz that\nbounds the number of hyperedges in critically 3-chromatic d-uniform n-vertex\nhypergraphs by n choose d-1. We show that our kernel is tight under the\nassumption that NP is not a subset of coNP/poly.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 17:02:31 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1509.07466", "submitter": "Henry Yuen", "authors": "Mohammad Bavarian, Thomas Vidick, Henry Yuen", "title": "Anchored parallel repetition for nonlocal games", "comments": "42 pages. Original version was published as \"Hardness amplification\n  for entangled games via anchoring\" in the proceedings of Symposium on Theory\n  of Computing 2017. This version is a revision to give more details on the\n  proof of the quantum parallel repetition result. Classical multiplayer\n  parallel repetition results no longer included, but can still be found in\n  arXiv:1509.07466v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple transformation on two-player nonlocal games, called\n\"anchoring\", and prove an exponential-decay parallel repetition theorem for all\nanchored games in the setting of quantum entangled players. This transformation\nis inspired in part by the Feige-Kilian transformation (SICOMP 2000), and has\nthe property that if the quantum value of the original game $G$ is $v$ then the\nquantum value of the anchored game $G_\\bot$ is $1 - (1 - \\alpha)^2 \\cdot (1 -\nv)$ where $\\alpha$ is a parameter of the transformation. In particular the\nanchored game has quantum value $1$ if and only if the original game $G$ has\nquantum value $1$. This provides the first gap amplification technique for\ngeneral two-player nonlocal games that achieves exponential decay of the\nquantum value.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 18:27:12 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 01:29:47 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bavarian", "Mohammad", ""], ["Vidick", "Thomas", ""], ["Yuen", "Henry", ""]]}, {"id": "1509.07476", "submitter": "Li-Yang Tan", "authors": "Xi Chen, Igor C. Oliveira, Rocco A. Servedio, Li-Yang Tan", "title": "Near-optimal small-depth lower bounds for small distance connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any depth-$d$ circuit for determining whether an $n$-node graph\nhas an $s$-to-$t$ path of length at most $k$ must have size\n$n^{\\Omega(k^{1/d}/d)}$. The previous best circuit size lower bounds for this\nproblem were $n^{k^{\\exp(-O(d))}}$ (due to Beame, Impagliazzo, and Pitassi\n[BIP98]) and $n^{\\Omega((\\log k)/d)}$ (following from a recent formula size\nlower bound of Rossman [Ros14]). Our lower bound is quite close to optimal,\nsince a simple construction gives depth-$d$ circuits of size $n^{O(k^{2/d})}$\nfor this problem (and strengthening our bound even to $n^{k^{\\Omega(1/d)}}$\nwould require proving that undirected connectivity is not in $\\mathsf{NC^1}.$)\n  Our proof is by reduction to a new lower bound on the size of small-depth\ncircuits computing a skewed variant of the \"Sipser functions\" that have played\nan important role in classical circuit lower bounds [Sip83, Yao85, H{\\aa}s86].\nA key ingredient in our proof of the required lower bound for these Sipser-like\nfunctions is the use of \\emph{random projections}, an extension of random\nrestrictions which were recently employed in [RST15]. Random projections allow\nus to obtain sharper quantitative bounds while employing simpler arguments,\nboth conceptually and technically, than in the previous works [Ajt89, BPU92,\nBIP98, Ros14].\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 19:01:34 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chen", "Xi", ""], ["Oliveira", "Igor C.", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1509.07588", "submitter": "Dmitry Chistikov", "authors": "Dmitry Chistikov and Szabolcs Iv\\'an and Anna Lubiw and Jeffrey\n  Shallit", "title": "Fractional coverings, greedy coverings, and rectifier networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rectifier network is a directed acyclic graph with distinguished sources\nand sinks; it is said to compute a Boolean matrix $M$ that has a $1$ in the\nentry $(i,j)$ iff there is a path from the $j$th source to the $i$th sink. The\nsmallest number of edges in a rectifier network computing $M$ is a classic\ncomplexity measure on matrices, which has been studied for more than half a\ncentury.\n  We explore two well-known techniques that have hitherto found little to no\napplications in this theory. Both of them build on a basic fact that depth-$2$\nrectifier networks are essentially weighted coverings of Boolean matrices with\nrectangles. We obtain new results by using fractional and greedy coverings\n(defined in the standard way).\n  First, we show that all fractional coverings of the so-called full triangular\nmatrix have cost at least $n\\log n$. This provides (a fortiori) a new proof of\nthe tight lower bound on its depth-$2$ complexity (the exact value has been\nknown since 1965, but previous proofs are based on different arguments).\nSecond, we show that the greedy heuristic is instrumental in tightening the\nupper bound on the depth-$2$ complexity of the Kneser-Sierpi\\'nski\n(disjointness) matrix. The previous upper bound is $O(n^{1.28})$, and we\nimprove it to $O(n^{1.17})$, while the best known lower bound is\n$\\Omega(n^{1.16})$. Third, using fractional coverings, we obtain a form of\ndirect product theorem that gives a lower bound on unbounded-depth complexity\nof Kronecker (tensor) products of matrices. In this case, the greedy heuristic\nshows (by an argument due to Lov\\'asz) that our result is only a logarithmic\nfactor away from the \"full\" direct product theorem. Our second and third\nresults constitute progress on open problem 7.3 and resolve, up to a\nlogarithmic factor, open problem 7.5 from a recent book by Jukna and Sergeev\n(in Foundations and Trends in Theoretical Computer Science (2013)).\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 05:29:18 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 01:39:14 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Chistikov", "Dmitry", ""], ["Iv\u00e1n", "Szabolcs", ""], ["Lubiw", "Anna", ""], ["Shallit", "Jeffrey", ""]]}, {"id": "1509.07687", "submitter": "Chiel Ten Brinke", "authors": "Chiel B. Ten Brinke, Frank J. P. van Houten, and Hans L. Bodlaender", "title": "Practical Algorithms for Linear Boolean-width", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a number of new exact algorithms and heuristics to\ncompute linear boolean decompositions, and experimentally evaluate these\nalgorithms. The experimental evaluation shows that significant improvements can\nbe made with respect to running time without increasing the width of the\ngenerated decompositions. We also evaluated dynamic programming algorithms on\nlinear boolean decompositions for several vertex subset problems. This\nevaluation shows that such algorithms are often much faster (up to several\norders of magnitude) compared to theoretical worst case bounds.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 11:42:03 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Brinke", "Chiel B. Ten", ""], ["van Houten", "Frank J. P.", ""], ["Bodlaender", "Hans L.", ""]]}, {"id": "1509.07766", "submitter": "Or Sattath", "authors": "Or Sattath, Siddhardh C. Morampudi, Christopher R. Laumann, and\n  Roderich Moessner", "title": "When a local Hamiltonian must be frustration-free", "comments": null, "journal-ref": "Proc. Natl. Acad. Sci. 113 (23), 6433-6437 (2016)", "doi": "10.1073/pnas.1519833113", "report-no": null, "categories": "quant-ph cond-mat.stat-mech cond-mat.str-el cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad range of quantum optimisation problems can be phrased as the question\nwhether a specific system has a ground state at zero energy, i.e.\\ whether its\nHamiltonian is frustration free. Frustration-free Hamiltonians, in turn, play a\ncentral role for constructing and understanding new phases of matter in quantum\nmany-body physics. Unfortunately, determining whether this is the case is known\nto be a complexity-theoretically intractable problem. This makes it highly\ndesirable to search for efficient heuristics and algorithms in order to, at\nleast, partially answer this question. Here we prove a general criterion - a\nsufficient condition - under which a local Hamiltonian is guaranteed to be\nfrustration free by lifting Shearer's theorem from classical probability theory\nto the quantum world. Remarkably, evaluating this condition proceeds via a\nfully classical analysis of a hard-core lattice gas at negative fugacity on the\nHamiltonian's interaction graph which, as a statistical mechanics problem, is\nof interest in its own right. We concretely apply this criterion to local\nHamiltonians on various regular lattices, while bringing to bear the tools of\nspin glass physics which permit us to obtain new bounds on the SAT/UNSAT\ntransition in random quantum satisfiability. These also lead us to natural\nconjectures for when such bounds will be tight, as well as to a novel notion of\nuniversality for these computer science problems. Besides providing concrete\nalgorithms leading to detailed and quantitative insights, this underscores the\npower of marrying classical statistical mechanics with quantum computation and\ncomplexity theory.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 15:51:40 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 11:39:01 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Sattath", "Or", ""], ["Morampudi", "Siddhardh C.", ""], ["Laumann", "Christopher R.", ""], ["Moessner", "Roderich", ""]]}, {"id": "1509.07789", "submitter": "Niel de Beaudrap", "authors": "Niel de Beaudrap", "title": "On exact counting and quasi-quantum complexity", "comments": "22 pages, 6 figures. Revised draft of a submission to TQC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present characterisations of \"exact\" gap-definable classes, in terms of\nindeterministic models of computation which slightly modify the standard model\nof quantum computation. This follows on work of Aaronson\n[arXiv:quant-ph/0412187], who shows that the counting class PP can be\ncharacterised in terms of bounded-error \"quantum\" algorithms which use\ninvertible (and possibly non-unitary) transformations, or postselections on\nevents of non-zero probability. Our work considers similar modifications of the\nquantum computational model, but in the setting of exact algorithms, and\nalgorithms with zero error and constant success probability. We show that the\ngap-definable counting classes [J. Comput. Syst. Sci. 48 (1994), p.116] which\nbound exact and zero-error quantum algorithms can be characterised in terms of\n\"quantum-like\" algorithms involving nonunitary gates, and that postselection\nand nonunitarity have equivalent power for exact quantum computation only if\nthese classes collapse.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 16:54:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["de Beaudrap", "Niel", ""]]}, {"id": "1509.08123", "submitter": "Dana Moshkovitz", "authors": "Ofer Grossman, Dana Moshkovitz", "title": "Amplification and Derandomization Without Slowdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for decreasing the error probability of randomized\nalgorithms and for converting randomized algorithms to deterministic\n(non-uniform) algorithms. Unlike most existing techniques that involve\nrepetition of the randomized algorithm and hence a slowdown, our techniques\nproduce algorithms with a similar run-time to the original randomized\nalgorithms. The amplification technique is related to a certain stochastic\nmulti-armed bandit problem. The derandomization technique - which is the main\ncontribution of this work - points to an intriguing connection between\nderandomization and sketching/sparsification.\n  We demonstrate the techniques by showing applications to Max-Cut on dense\ngraphs, approximate clique on graphs that contain a large clique, constraint\nsatisfaction problems on dense bipartite graphs and the list decoding to unique\ndecoding problem for the Reed-Muller code.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 19:01:47 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Grossman", "Ofer", ""], ["Moshkovitz", "Dana", ""]]}, {"id": "1509.08251", "submitter": "Paul Bonsma", "authors": "Christoph Berkholz, Paul Bonsma, Martin Grohe", "title": "Tight Lower and Upper Bounds for the Complexity of Canonical Colour\n  Refinement", "comments": "An extended abstract of this paper appeared in the proceedings of\n  ESA'13, LNCS 8125, pp. 145-156", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An assignment of colours to the vertices of a graph is stable if any two\nvertices of the same colour have identically coloured neighbourhoods. The goal\nof colour refinement is to find a stable colouring that uses a minimum number\nof colours. This is a widely used subroutine for graph isomorphism testing\nalgorithms, since any automorphism needs to be colour preserving. We give an\n$O((m+n)\\log n)$ algorithm for finding a canonical version of such a stable\ncolouring, on graphs with $n$ vertices and $m$ edges. We show that no faster\nalgorithm is possible, under some modest assumptions about the type of\nalgorithm, which captures all known colour refinement algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 09:36:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Berkholz", "Christoph", ""], ["Bonsma", "Paul", ""], ["Grohe", "Martin", ""]]}, {"id": "1509.08628", "submitter": "Patrick Scharpfenecker", "authors": "Britta Dorn, Dominikus Kr\\\"uger, and Patrick Scharpfenecker", "title": "Often harder than in the Constructive Case: Destructive Bribery in\n  CP-nets", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the destructive bribery problem---an external\nagent tries to prevent a disliked candidate from winning by bribery\nactions---in voting over combinatorial domains, where the set of candidates is\nthe Cartesian product of several issues. This problem is related to the concept\nof the margin of victory of an election which constitutes a measure of\nrobustness of the election outcome and plays an important role in the context\nof electronic voting. In our setting, voters have conditional preferences over\nassignments to these issues, modelled by CP-nets. We settle the complexity of\nall combinations of this problem based on distinctions of four voting rules,\nfive cost schemes, three bribery actions, weighted and unweighted voters, as\nwell as the negative and the non-negative scenario. We show that almost all of\nthese cases are NP-complete or NP-hard for weighted votes while approximately\nhalf of the cases can be solved in polynomial time for unweighted votes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:17:34 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Dorn", "Britta", ""], ["Kr\u00fcger", "Dominikus", ""], ["Scharpfenecker", "Patrick", ""]]}, {"id": "1509.08825", "submitter": "Donald Stull", "authors": "Xiang Huang, D. M. Stull", "title": "Polynomial Space Randomness in Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the interaction between polynomial space randomness and a\nfundamental result of analysis, the Lebesgue differentiation theorem. We\ngeneralize Ko's framework for polynomial space computability in $\\mathbb{R}^n$\nto define \\textit{weakly pspace-random} points, a new variant of polynomial\nspace randomness. We show that the Lebesgue differentiation theorem holds for\nevery weakly pspace-random point.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 16:00:05 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 15:19:03 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Huang", "Xiang", ""], ["Stull", "D. M.", ""]]}, {"id": "1509.08896", "submitter": "Holden Lee", "authors": "Holden Lee", "title": "Quadratic polynomials of small modulus cannot represent OR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An open problem in complexity theory is to find the minimal degree of a\npolynomial representing the $n$-bit OR function modulo composite $m$. This\nproblem is related to understanding the power of circuits with $\\text{MOD}_m$\ngates where $m$ is composite. The OR function is of particular interest because\nit is the simplest function not amenable to bounds from communication\ncomplexity. Tardos and Barrington established a lower bound of $\\Omega((\\log\nn)^{O_m(1)})$, and Barrington, Beigel, and Rudich established an upper bound of\n$n^{O_m(1)}$. No progress has been made on closing this gap for twenty years,\nand progress will likely require new techniques.\n  We make progress on this question viewed from a different perspective: rather\nthan fixing the modulus $m$ and bounding the minimum degree $d$ in terms of the\nnumber of variables $n$, we fix the degree $d$ and bound $n$ in terms of the\nmodulus $m$. For degree $d=2$, we prove a quasipolynomial bound of $n\\le\nm^{O(d)}\\le m^{O(\\log m)}$, improving the previous best bound of $2^{O(m)}$\nimplied by Tardos and Barrington's general bound.\n  To understand the computational power of quadratic polynomials modulo $m$, we\nintroduce a certain dichotomy which may be of independent interest. Namely, we\ndefine a notion of boolean rank of a quadratic polynomial $f$ and relate it to\nthe notion of diagonal rigidity. Using additive combinatorics, we show that\nwhen the rank is low, $f(\\mathbf x)=0$ must have many solutions. Using\ntechniques from exponential sums, we show that when the rank of $f$ is high,\n$f$ is close to equidistributed. In either case, $f$ cannot represent the OR\nfunction in many variables.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 19:24:51 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 19:42:46 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Lee", "Holden", ""]]}, {"id": "1509.09236", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Stephen A. Vavasis", "title": "On the Complexity of Robust PCA and $\\ell_1$-norm Low-Rank Matrix\n  Approximation", "comments": "16 pages, some typos corrected", "journal-ref": "Mathematics of Operations Research 43 (4), pp. 1072-1084, 2018", "doi": "10.1287/moor.2017.0895", "report-no": null, "categories": "cs.LG cs.CC math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank matrix approximation problem with respect to the component-wise\n$\\ell_1$-norm ($\\ell_1$-LRA), which is closely related to robust principal\ncomponent analysis (PCA), has become a very popular tool in data mining and\nmachine learning. Robust PCA aims at recovering a low-rank matrix that was\nperturbed with sparse noise, with applications for example in\nforeground-background video separation. Although $\\ell_1$-LRA is strongly\nbelieved to be NP-hard, there is, to the best of our knowledge, no formal proof\nof this fact. In this paper, we prove that $\\ell_1$-LRA is NP-hard, already in\nthe rank-one case, using a reduction from MAX CUT. Our derivations draw\ninteresting connections between $\\ell_1$-LRA and several other well-known\nproblems, namely, robust PCA, $\\ell_0$-LRA, binary matrix factorization, a\nparticular densest bipartite subgraph problem, the computation of the cut norm\nof $\\{-1,+1\\}$ matrices, and the discrete basis problem, which we all prove to\nbe NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:05:50 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2015 16:48:44 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 10:12:17 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1509.09271", "submitter": "Andrew M. Childs", "authors": "Andrew M. Childs, Wim van Dam, Shih-Han Hung, Igor E. Shparlinski", "title": "Optimal quantum algorithm for polynomial interpolation", "comments": "17 pages, minor improvements, added conjecture about multivariate\n  interpolation", "journal-ref": "Proceedings of the 43rd International Colloquium on Automata,\n  Languages, and Programming (ICALP 2016), pp. 16:1-16:13 (2016)", "doi": "10.4230/LIPIcs.ICALP.2016.16", "report-no": null, "categories": "quant-ph cs.CC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the number of quantum queries required to determine the\ncoefficients of a degree-d polynomial over GF(q). A lower bound shown\nindependently by Kane and Kutin and by Meyer and Pommersheim shows that d/2+1/2\nquantum queries are needed to solve this problem with bounded error, whereas an\nalgorithm of Boneh and Zhandry shows that d quantum queries are sufficient. We\nshow that the lower bound is achievable: d/2+1/2 quantum queries suffice to\ndetermine the polynomial with bounded error. Furthermore, we show that d/2+1\nqueries suffice to achieve probability approaching 1 for large q. These upper\nbounds improve results of Boneh and Zhandry on the insecurity of cryptographic\nprotocols against quantum attacks. We also show that our algorithm's success\nprobability as a function of the number of queries is precisely optimal.\nFurthermore, the algorithm can be implemented with gate complexity poly(log q)\nwith negligible decrease in the success probability. We end with a conjecture\nabout the quantum query complexity of multivariate polynomial interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 17:47:39 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 18:36:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Childs", "Andrew M.", ""], ["van Dam", "Wim", ""], ["Hung", "Shih-Han", ""], ["Shparlinski", "Igor E.", ""]]}]