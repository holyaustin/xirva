[{"id": "1412.0279", "submitter": "Valery Shchesnovich", "authors": "Valery S. Shchesnovich", "title": "Boson-Sampling with non-interacting fermions", "comments": "Minor misprints corrected. 19 pages; 3 figures (one colored)", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.other cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the conditions under which identical particles in unitary linear\nnetworks behave as the other species, i.e. bosons as fermions and fermions as\nbosons. It is found that the Boson-Sampling computer of Aaronson & Arkhipov can\nbe implemented in an interference experiment with non-interacting fermions in\nan appropriately entangled state. Moreover, a scheme is proposed which\nsimulates the scattershot version of the Boson-Sampling computer by preparing,\non the fly, the required entangled state of fermions from an unentangled one.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 20:44:39 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 17:05:31 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 13:57:27 GMT"}, {"version": "v4", "created": "Sun, 22 Mar 2015 17:21:30 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Shchesnovich", "Valery S.", ""]]}, {"id": "1412.0348", "submitter": "Arturs Backurs", "authors": "Arturs Backurs, Piotr Indyk", "title": "Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\n  SETH is false)", "comments": "STOC'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance (a.k.a. the Levenshtein distance) between two strings is\ndefined as the minimum number of insertions, deletions or substitutions of\nsymbols needed to transform one string into another. The problem of computing\nthe edit distance between two strings is a classical computational task, with a\nwell-known algorithm based on dynamic programming. Unfortunately, all known\nalgorithms for this problem run in nearly quadratic time.\n  In this paper we provide evidence that the near-quadratic running time bounds\nknown for the problem of computing edit distance might be tight. Specifically,\nwe show that, if the edit distance can be computed in time $O(n^{2-\\delta})$\nfor some constant $\\delta>0$, then the satisfiability of conjunctive normal\nform formulas with $N$ variables and $M$ clauses can be solved in time\n$M^{O(1)} 2^{(1-\\epsilon)N}$ for a constant $\\epsilon>0$. The latter result\nwould violate the Strong Exponential Time Hypothesis, which postulates that\nsuch algorithms do not exist.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 04:57:06 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 21:13:21 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 17:11:08 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 18:01:17 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""]]}, {"id": "1412.0356", "submitter": "Bahman Kalantari", "authors": "Bahman Kalantari", "title": "An Algorithmic Separating Hyperplane Theorem and Its Applications", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first prove a new separating hyperplane theorem characterizing when a pair\nof compact convex subsets $K, K'$ of the Euclidean space intersect, and when\nthey are disjoint. The theorem is distinct from classical separation theorems.\nIt generalizes the {\\it distance duality} proved in our earlier work for\ntesting the membership of a distinguished point in the convex hull of a finite\npoint set. Next by utilizing the theorem, we develop a substantially\ngeneralized and stronger version of the {\\it Triangle Algorithm} introduced in\nthe previous work to perform any of the following three tasks: (1) To compute a\npair $(p,p') \\in K \\times K'$, where either the Euclidean distance $d(p,p')$ is\nto within a prescribed tolerance, or the orthogonal bisecting hyperplane of the\nline segment $pp'$ separates the two sets; (2) When $K$ and $K'$ are disjoint,\nto compute $(p,p') \\in K \\times K'$ so that $d(p,p')$ approximates $d(K,K')$ to\nwithin a prescribed tolerance; (3) When $K$ and $K'$ are disjoint, to compute a\npair of parallel supporting hyperplanes $H,H'$ so that $d(H,H')$ is to within a\nprescribed tolerance of the optimal margin. The worst-case complexity of each\niteration is solving a linear objective over $K$ or $K'$. The resulting\nalgorithm is a fully polynomial-time approximation scheme for such important\nspecial cases as when $K$ and $K'$ are convex hulls of finite points sets, or\nthe intersection of a finite number of halfspaces. The results find many\ntheoretical and practical applications, such as in machine learning,\nstatistics, linear, quadratic and convex programming. In particular, in a\nseparate article we report on a comparison of the Triangle Algorithm and SMO\nfor solving the hard margin problem. In future work we extend the applications\nto combinatorial and NP-complete problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 05:48:30 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 19:33:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Kalantari", "Bahman", ""]]}, {"id": "1412.0423", "submitter": "Christian Engels", "authors": "Christian Engels", "title": "Dichotomy Theorems for Homomorphism Polynomials of Graph Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will show dichotomy theorems for the computation of\npolynomials corresponding to evaluation of graph homomorphisms in Valiant's\nmodel. We are given a fixed graph $H$ and want to find all graphs, from some\ngraph class, homomorphic to this $H$. These graphs will be encoded by a family\nof polynomials.\n  We give dichotomies for the polynomials for cycles, cliques, trees,\nouterplanar graphs, planar graphs and graphs of bounded genus.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:01:14 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Engels", "Christian", ""]]}, {"id": "1412.0449", "submitter": "Zhaohui Wei", "authors": "Zhaohui Wei and Zhangqi Yin", "title": "The Generation Cost of Bipartite Quantum States under LOCC", "comments": "This paper has been withdrawn by the authors as similar results have\n  been found by Francesco Buscemi and Nilanjana Datta", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a realistic setting of quantum tasks that generate shared\nbipartite quantum states. Suppose \\alice and \\bob are located at different\nplaces and need to produce a target shared quantum state $\\rho$. In order to\nsave quantum communication, they can choose to share a proper smaller quantum\nstate $\\sigma$ first, and then turn $\\sigma$ to $\\rho$ by performing only local\nquantum operations and classical communications (LOCC). We hope $\\sigma$ is the\noptimal such that the quantum communication needed is as little as possible,\nwhich is called the generation cost of $\\rho$. In this paper, for an arbitrary\nbipartite $\\rho$, we characterize its generation cost completely by proving\nthat it is exactly equivalent to the logarithm of the Schmidt number of $\\rho$.\nSimilar quantum schemes where classical communication is not allowed have\nactually been considered. By comparing the two settings, we are able to look\ninto the role that classical communication plays in these fundamental tasks,\nwhere we exhibit some instances in which classical communication is not helpful\ncompletely.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 12:23:06 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 04:34:52 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Wei", "Zhaohui", ""], ["Yin", "Zhangqi", ""]]}, {"id": "1412.0784", "submitter": "Linus Hamilton", "authors": "Linus Hamilton", "title": "Braid is undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Braid is a 2008 puzzle game centered around the ability to reverse time. We\nshow that Braid can simulate an arbitrary computation. Our construction makes\nno use of Braid's unique time mechanics, and therefore may apply to many other\nvideo games.\n  We also show that a plausible \"bounded\" variant of Braid lies within\n2-EXPSPACE. Our proof relies on a technical lemma about Turing machines which\nmay be of independent interest. Namely, define a braidlike Turing machine to be\na Turing machine that, when it writes to the tape, deletes all data on the tape\nto the right of the head. We prove that deciding the behavior of such a machine\nlies in EXPSPACE.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 05:29:18 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Hamilton", "Linus", ""]]}, {"id": "1412.0795", "submitter": "Guangda Hu", "authors": "Zeev Dvir and Guangda Hu", "title": "Sylvester-Gallai for Arrangements of Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study arrangements of $k$-dimensional subspaces\n$V_1,\\ldots,V_n \\subset \\mathbb{C}^\\ell$. Our main result shows that, if every\npair $V_{a},V_b$ of subspaces is contained in a dependent triple (a triple\n$V_{a},V_b,V_c$ contained in a $2k$-dimensional space), then the entire\narrangement must be contained in a subspace whose dimension depends only on $k$\n(and not on $n$). The theorem holds under the assumption that $V_a \\cap V_b =\n\\{0\\}$ for every pair (otherwise it is false). This generalizes the\nSylvester-Gallai theorem (or Kelly's theorem for complex numbers), which proves\nthe $k=1$ case. Our proof also handles arrangements in which we have many pairs\n(instead of all) appearing in dependent triples, generalizing the quantitative\nresults of Barak et. al. [BDWY-pnas].\n  One of the main ingredients in the proof is a strengthening of a Theorem of\nBarthe [Bar98] (from the $k=1$ to $k>1$ case) proving the existence of a linear\nmap that makes the angles between pairs of subspaces large on average. Such a\nmapping can be found, unless there is an obstruction in the form of a low\ndimensional subspace intersecting many of the spaces in the arrangement (in\nwhich case one can use a different argument to prove the main theorem).\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 06:47:44 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Dvir", "Zeev", ""], ["Hu", "Guangda", ""]]}, {"id": "1412.0864", "submitter": "Yinglei Song", "authors": "Yinglei Song", "title": "On the Induced Matching Problem in Hamiltonian Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parameterized complexity and inapproximability of\nthe {\\sc Induced Matching} problem in hamiltonian bipartite graphs. We show\nthat, given a hamiltonian cycle in a hamiltonian bipartite graph, the problem\nis W[1]-hard and cannot be solved in time $n^{o(k^{\\frac{1}{2}})}$ unless\nW[1]=FPT, where $n$ is the number of vertices in the graph. In addition, we\nshow that unless NP=P, the maximum induced matching in a hamiltonian graph\ncannot be approximated within a ratio of $n^{1-\\epsilon}$, where $n$ is the\nnumber of vertices in the graph. For a bipartite hamiltonian graph in $n$\nvertices, it is NP-hard to approximate its maximum induced matching based on a\nhamiltonian cycle of the graph within a ratio of $n^{\\frac{1}{4}-\\epsilon}$,\nwhere $n$ is the number of vertices in the graph and $\\epsilon$ is any positive\nconstant.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 11:17:54 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 02:35:02 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Song", "Yinglei", ""]]}, {"id": "1412.0969", "submitter": "Ruta Mehta", "authors": "Ruta Mehta, Vijay V. Vazirani, and Sadra Yazdanbod", "title": "Settling Some Open Problems on 2-Player Symmetric Nash Equilibria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, researchers have studied the complexity of several decision\nversions of Nash equilibrium in (symmetric) two-player games (bimatrix games).\nTo the best of our knowledge, the last remaining open problem of this sort is\nthe following; it was stated by Papadimitriou in 2007: find a non-symmetric\nNash equilibrium (NE) in a symmetric game. We show that this problem is\nNP-complete and the problem of counting the number of non-symmetric NE in a\nsymmetric game is #P-complete.\n  In 2005, Kannan and Theobald defined the \"rank of a bimatrix game\"\nrepresented by matrices (A, B) to be rank(A+B) and asked whether a NE can be\ncomputed in rank 1 games in polynomial time. Observe that the rank 0 case is\nprecisely the zero sum case, for which a polynomial time algorithm follows from\nvon Neumann's reduction of such games to linear programming. In 2011, Adsul et.\nal. obtained an algorithm for rank 1 games; however, it does not solve the case\nof symmetric rank 1 games. We resolve this problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 16:40:50 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Mehta", "Ruta", ""], ["Vazirani", "Vijay V.", ""], ["Yazdanbod", "Sadra", ""]]}, {"id": "1412.1124", "submitter": "Dmitry Itsykson", "authors": "Dmitry Itsykson, Anna Malova, Vsevolod Oparin, Dmitry Sokolov", "title": "Tree-like resolution complexity of two planar problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two CSP problems: the first CSP encodes 2D Sperner's lemma for\nthe standard triangulation of the right triangle on $n^2$ small triangles; the\nsecond CSP encodes the fact that it is impossible to match cells of $n \\times\nn$ square to arrows (two horizontal, two vertical and four diagonal) such that\narrows in two cells with a common edge differ by at most $45^\\circ$, and all\narrows on the boundary of the square do not look outside (this fact is a\ncorollary of the Brower's fixed point theorem). We prove that the tree-like\nresolution complexities of these CSPs are $2^{\\Theta(n)}$. For Sperner's lemma\nour result implies $\\Omega(n)$ lower bound on the number of request to colors\nof vertices that is enough to make in order to find a trichromatic triangle;\nthis lower bound was originally proved by Crescenzi and Silvestri.\n  CSP based on Sperner's lemma is related with the $\\rm PPAD$-complete problem.\nWe show that CSP corresponding to arrows is also related with a $\\rm\nPPAD$-complete problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 22:37:17 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Itsykson", "Dmitry", ""], ["Malova", "Anna", ""], ["Oparin", "Vsevolod", ""], ["Sokolov", "Dmitry", ""]]}, {"id": "1412.1130", "submitter": "Will Rosenbaum", "authors": "Rafail Ostrovsky, Will Rosenbaum", "title": "It's Not Easy Being Three: The Approximability of Three-Dimensional\n  Stable Matching Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1976, Knuth asked if the stable marriage problem (SMP) can be generalized\nto marriages consisting of 3 genders. In 1988, Alkan showed that the natural\ngeneralization of SMP to 3 genders ($3$GSM) need not admit a stable marriage.\nThree years later, Ng and Hirschberg proved that it is NP-complete to determine\nif given preferences admit a stable marriage. They further prove an analogous\nresult for the $3$ person stable assignment ($3$PSA) problem.\n  In light of Ng and Hirschberg's NP-hardness result for $3$GSM and $3$PSA, we\ninitiate the study of approximate versions of these problems. In particular, we\ndescribe two optimization variants of $3$GSM and $3$PSA: maximally stable\nmarriage/matching (MSM) and maximum stable submarriage/submatching (MSS). We\nshow that both variants are NP-hard to approximate within some fixed constant\nfactor. Conversely, we describe a simple polynomial time algorithm which\ncomputes constant factor approximations for the maximally stable marriage and\nmatching problems. Thus both variants of MSM are APX-complete.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 23:14:41 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1412.1188", "submitter": "Murray Elder", "authors": "Benjamin A. Burton and Murray Elder and Arkadius Kalka and Stephan\n  Tillmann", "title": "2-manifold recognition is in logspace", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the homeomorphism problem for 2-manifolds can be decided in\nlogspace. The proof relies on Reingold's logspace solution to the undirected\n$s,t$-connectivity problem in graphs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 04:55:03 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Burton", "Benjamin A.", ""], ["Elder", "Murray", ""], ["Kalka", "Arkadius", ""], ["Tillmann", "Stephan", ""]]}, {"id": "1412.1229", "submitter": "Feng Pan Dr", "authors": "Feng Pan", "title": "SAT is a problem with exponential complexity measured by negentropy", "comments": "This paper has been withdrawn by the author due to a crucial naive\n  error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the reason why entropy reduction (negentropy) can be used to\nmeasure the complexity of any computation was first elaborated both in the\naspect of mathematics and informational physics. In the same time the\nequivalence of computation and information was clearly stated. Then the\ncomplexities of three specific problems: logical compare, sorting and SAT, were\nanalyzed and measured. The result showed SAT was a problem with exponential\ncomplexity which naturally leads to the conclusion that no efficient algorithm\nexists to solve it. That's to say: NP!=P.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 08:36:38 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 12:29:43 GMT"}, {"version": "v3", "created": "Fri, 16 Jan 2015 00:41:40 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 13:22:38 GMT"}, {"version": "v5", "created": "Sun, 13 Sep 2015 00:16:22 GMT"}, {"version": "v6", "created": "Mon, 21 Sep 2015 05:38:17 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Pan", "Feng", ""]]}, {"id": "1412.1505", "submitter": "Guy Van den Broeck", "authors": "Paul Beame, Guy Van den Broeck, Eric Gribkoff, Dan Suciu", "title": "Symmetric Weighted First-Order Model Counting", "comments": "To appear at PODS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FO Model Counting problem (FOMC) is the following: given a sentence\n$\\Phi$ in FO and a number $n$, compute the number of models of $\\Phi$ over a\ndomain of size $n$; the Weighted variant (WFOMC) generalizes the problem by\nassociating a weight to each tuple and defining the weight of a model to be the\nproduct of weights of its tuples. In this paper we study the complexity of the\nsymmetric WFOMC, where all tuples of a given relation have the same weight. Our\nmotivation comes from an important application, inference in Knowledge Bases\nwith soft constraints, like Markov Logic Networks, but the problem is also of\nindependent theoretical interest. We study both the data complexity, and the\ncombined complexity of FOMC and WFOMC. For the data complexity we prove the\nexistence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the\nexistence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also\nprove that all $\\gamma$-acyclic queries have polynomial time data complexity.\nFor the combined complexity, we prove that, for every fragment FO$^{k}$, $k\\geq\n2$, the combined complexity of FOMC (or WFOMC) is #P-complete.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 22:03:52 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 13:29:54 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 14:58:14 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Beame", "Paul", ""], ["Broeck", "Guy Van den", ""], ["Gribkoff", "Eric", ""], ["Suciu", "Dan", ""]]}, {"id": "1412.1543", "submitter": "George Mertzios", "authors": "Archontia C. Giannopoulou and George B. Mertzios", "title": "New Geometric Representations and Domination Problems on Tolerance and\n  Multitolerance Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tolerance graphs model interval relations in such a way that intervals can\ntolerate a certain amount of overlap without being in conflict. In one of the\nmost natural generalizations of tolerance graphs with direct applications in\nthe comparison of DNA sequences from different organisms, namely multitolerance\ngraphs, two tolerances are allowed for each interval - one from the left and\none from the right side. Several efficient algorithms for optimization problems\nthat are NP-hard in general graphs have been designed for tolerance and\nmultitolerance graphs. In spite of this progress, the complexity status of some\nfundamental algorithmic problems on tolerance and multitolerance graphs, such\nas the dominating set problem, remained unresolved until now, three decades\nafter the introduction of tolerance graphs. In this article we introduce two\nnew geometric representations for tolerance and multitolerance graphs, given by\npoints and line segments in the plane. Apart from being important on their own,\nthese new representations prove to be a powerful tool for deriving both\nhardness results and polynomial time algorithms. Using them, we surprisingly\nprove that the dominating set problem can be solved in polynomial time on\ntolerance graphs and that it is APX-hard on multitolerance graphs, solving thus\na longstanding open problem. This problem is the first one that has been\ndiscovered with a different complexity status in these two graph classes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 02:39:01 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 15:19:30 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Giannopoulou", "Archontia C.", ""], ["Mertzios", "George B.", ""]]}, {"id": "1412.2457", "submitter": "Thomas Steinke", "authors": "Mark Bun and Thomas Steinke", "title": "Weighted Polynomial Approximations: Limits for Learning and\n  Pseudorandomness", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial approximations to boolean functions have led to many positive\nresults in computer science. In particular, polynomial approximations to the\nsign function underly algorithms for agnostically learning halfspaces, as well\nas pseudorandom generators for halfspaces. In this work, we investigate the\nlimits of these techniques by proving inapproximability results for the sign\nfunction.\n  Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput.\n2008) shows that halfspaces can be learned with respect to log-concave\ndistributions on $\\mathbb{R}^n$ in the challenging agnostic learning model. The\npower of this algorithm relies on the fact that under log-concave\ndistributions, halfspaces can be approximated arbitrarily well by low-degree\npolynomials. We ask whether this technique can be extended beyond log-concave\ndistributions, and establish a negative result. We show that polynomials of any\ndegree cannot approximate the sign function to within arbitrarily low error for\na large class of non-log-concave distributions on the real line, including\nthose with densities proportional to $\\exp(-|x|^{0.99})$.\n  Secondly, we investigate the derandomization of Chernoff-type concentration\ninequalities. Chernoff-type tail bounds on sums of independent random variables\nhave pervasive applications in theoretical computer science. Schmidt et al.\n(SIAM J. Discrete Math. 1995) showed that these inequalities can be established\nfor sums of random variables with only $O(\\log(1/\\delta))$-wise independence,\nfor a tail probability of $\\delta$. We show that their results are tight up to\nconstant factors.\n  These results rely on techniques from weighted approximation theory, which\nstudies how well functions on the real line can be approximated by polynomials\nunder various distributions. We believe that these techniques will have further\napplications in other areas of computer science.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 05:58:56 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""]]}, {"id": "1412.2460", "submitter": "Supriya Krishnamurthy", "authors": "Supriya Krishnamurthy and Sumedha", "title": "An alternate view of complexity in k-SAT problems", "comments": "5 pages, 3 figures, typos corrected", "journal-ref": "Phys. Rev. E 92, 042144(2015)", "doi": "10.1103/PhysRevE.92.042144", "report-no": null, "categories": "cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The satisfiability threshold for constraint satisfaction problems is that\nvalue of the ratio of constraints (or clauses) to variables, above which the\nprobability that a random instance of the problem has a solution is zero in the\nlarge system limit. Two different approaches to obtaining this threshold have\nbeen discussed in the literature - using first or second-moment methods which\ngive rigorous bounds or using the non-rigorous but powerful replica-symmetry\nbreaking (RSB) approach, which gives very accurate predictions on random\ngraphs. In this paper, we lay out a different route to obtaining this threshold\non a Bethe lattice. We need make no assumptions about the solution-space\nstructure, a key assumption in the RSB approach. Despite this, our expressions\nand threshold values exactly match the best predictions of the cavity method\nunder the 1-RSB assumption. Our method hence provides alternate interpretations\nas well as motivations for the key equations in the RSB approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 06:04:47 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 09:34:25 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Krishnamurthy", "Supriya", ""], ["Sumedha", "", ""]]}, {"id": "1412.2470", "submitter": "Nikhil Balaji", "authors": "Nikhil Balaji and Samir Datta", "title": "Bounded Treewidth and Space-Efficient Linear Algebra", "comments": "Replaces http://arxiv.org/abs/1312.7468", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a recent result of Elberfeld, Jakoby and Tantau showing that\n$\\mathsf{MSO}$ properties are Logspace computable on graphs of bounded\ntree-width, we consider the complexity of computing the determinant of the\nadjacency matrix of a bounded tree-width graph and as our main result prove\nthat it is in Logspace. It is important to notice that the determinant is\nneither an $\\mathsf{MSO}$-property nor counts the number of solutions of an\n$\\mathsf{MSO}$-predicate. This technique yields Logspace algorithms for\ncounting the number of spanning arborescences and directed Euler tours in\nbounded tree-width digraphs.\n  We demonstrate some linear algebraic applications of the determinant\nalgorithm by describing Logspace procedures for the characteristic polynomial,\nthe powers of a weighted bounded tree-width graph and feasibility of a system\nof linear equations where the underlying bipartite graph has bounded\ntree-width.\n  Finally, we complement our upper bounds by proving $\\mathsf{L}$-hardness of\nthe problems of computing the determinant, and of powering a bounded tree-width\nmatrix. We also show the $\\mathsf{GapL}$-hardness of Iterated Matrix\nMultiplication where each matrix has bounded tree-width.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 07:54:24 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Balaji", "Nikhil", ""], ["Datta", "Samir", ""]]}, {"id": "1412.2497", "submitter": "Mika\\\"el Rabie", "authors": "Olivier Bournez, Johanne Cohen and Mika\\\"el Rabie", "title": "Homonym Population Protocols, or Providing a Small Space of Computation\n  Using a Few Identifiers", "comments": "Instead of updating the paper, we actually submitted a new version on\n  arXiv, meaning that this one is not the latest version of our results\n  available on arXiv. (The good version is Homonym Population Protocols\n  [arXiv:1602.03540])", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols have been introduced by Angluin et al. as a model in\nwhich n passively mobile anonymous finite-state agents stably compute a\npredicate on the multiset of their inputs via interactions by pairs. The model\nhas been extended by Guerraoui and Ruppert to yield the community protocol\nmodels where agents have unique identifiers but may only store a finite number\nof the identifiers they already heard about. The population protocol models can\nonly compute semi-linear predicates, whereas in the community protocol model\nthe whole community of agents provides collectively the power of a Turing\nmachine with a O(n log n) space. We consider variations on the above models and\nwe obtain a whole landscape that covers and extends already known results: By\nconsidering the case of homonyms, that is to say the case when several agents\nmay share the same identifier, we provide a hierarchy that goes from the case\nof no identifier (i.e. a single one for all, i.e. the population protocol\nmodel) to the case of unique identifiers (i.e. community protocol model). We\nobtain in particular that any Turing Machine on space O(logO(1) n) can be\nsimulated with at least O(logO(1) n) identifiers, a result filling a gap left\nopen in all previous studies. Our results also extend and revisit in particular\nthe hierarchy provided by Chatzigiannakis et al. on population protocols\ncarrying Turing Machines on limited space, solving the problem of the gap left\nby this work between per-agent space o(log log n) (proved to be equivalent to\npopulation protocols) and O(log n) (proved to be equivalent to Turing\nmachines).\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 09:56:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 11:58:01 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Bournez", "Olivier", ""], ["Cohen", "Johanne", ""], ["Rabie", "Mika\u00ebl", ""]]}, {"id": "1412.2559", "submitter": "Martin Milani\\v{c}", "authors": "Ferdinando Cicalese, Martin Milani\\v{c}, Romeo Rizzi", "title": "On the complexity of the vector connectivity problem", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a relaxation of the Vector Domination problem called Vector\nConnectivity (VecCon). Given a graph $G$ with a requirement $r(v)$ for each\nvertex $v$, VecCon asks for a minimum cardinality set $S$ of vertices such that\nevery vertex $v\\in V\\setminus S$ is connected to $S$ via $r(v)$ disjoint paths.\nIn the paper introducing the problem, Boros et al. [Networks, 2014] gave\npolynomial-time solutions for VecCon in trees, cographs, and split graphs, and\nshowed that the problem can be approximated in polynomial time on $n$-vertex\ngraphs to within a factor of $\\log n+2$, leaving open the question of whether\nthe problem is NP-hard on general graphs. We show that VecCon is APX-hard in\ngeneral graphs, and NP-hard in planar bipartite graphs and in planar line\ngraphs. We also generalize the polynomial result for trees by solving the\nproblem for block graphs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 13:54:57 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Milani\u010d", "Martin", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1412.3095", "submitter": "Mathijs de Weerdt", "authors": "Jan Elffers and Mathijs de Weerdt", "title": "Scheduling with two non-unit task lengths is NP-complete", "comments": "12 pages, 5 figures; the proof in this version has been thoroughly\n  revised (compared to the first version) to make the argumentation easier to\n  follow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-preemptive job scheduling problem with release times and deadlines on\na single machine is fundamental to many scheduling problems. We parameterize\nthis problem by the set of job lengths the jobs can have. The case where all\njob lengths are identical is known to be solvable in polynomial time. We prove\nthat the problem with two job lengths is NP-complete, except for the case in\nwhich the short jobs have unit job length, which was already known to be\nefficiently solvable. The proof uses a reduction from satisfiability to an\nauxiliary scheduling problem that includes a set of paired jobs that each have\nboth an early and a late deadline, and of which at least one should be\nscheduled before the early deadline. This reduction is enabled by not only\nthese pairwise dependencies between jobs, but also by dependencies introduced\nby specifically constructed sets of jobs which have deadlines close to each\nother. The auxiliary scheduling problem in its turn can be reduced to the\nscheduling problem with two job lengths by representing each pair of jobs with\ntwo deadlines by four different jobs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:41:36 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 10:45:25 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Elffers", "Jan", ""], ["de Weerdt", "Mathijs", ""]]}, {"id": "1412.3246", "submitter": "J", "authors": "J\\'an Pich (Charles University in Prague)", "title": "Logical strength of complexity theory and a formalization of the PCP\n  theorem in bounded arithmetic", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 2 (June 16,\n  2015) lmcs:1568", "doi": "10.2168/LMCS-11(2:8)2015", "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several known formalizations of theorems from computational\ncomplexity in bounded arithmetic and formalize the PCP theorem in the theory\nPV1 (no formalization of this theorem was known). This includes a formalization\nof the existence and of some properties of the (n,d,{\\lambda})-graphs in PV1.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 10:23:05 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 07:24:31 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Pich", "J\u00e1n", "", "Charles University in Prague"]]}, {"id": "1412.3334", "submitter": "Kei Uchizawa Dr.", "authors": "Takehiro Ito, Yota Otachi, Toshiki Saitoh, Hisayuki Satoh, Akira\n  Suzuki, Kei Uchizawa, Ryuhei Uehara, Katsuhisa Yamanaka and Xiao Zhou", "title": "Computational Complexity of Competitive Diffusion on (Un)weighted Graphs", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.GT cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:12:10 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Ito", "Takehiro", ""], ["Otachi", "Yota", ""], ["Saitoh", "Toshiki", ""], ["Satoh", "Hisayuki", ""], ["Suzuki", "Akira", ""], ["Uchizawa", "Kei", ""], ["Uehara", "Ryuhei", ""], ["Yamanaka", "Katsuhisa", ""], ["Zhou", "Xiao", ""]]}, {"id": "1412.3335", "submitter": "Narendra Karmarkar Dr", "authors": "Narendra Karmarkar", "title": "Towards a Broader View of Theory of Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beginning with the projectively invariant method for linear programming,\ninterior point methods have led to powerful algorithms for many difficult\ncomputing problems, in combinatorial optimization, logic, number theory and\nnon-convex optimization. Algorithms for convex optimization benefitted from\nmany pre-established ideas from classical mathematics, but non-convex problems\nrequire new concepts. Lecture series I am presenting at the conference on\nFoundations of Computational Mathematics, 2014, outlines some of these\nconcepts{computational models based on the concept of the continuum, algorithms\ninvariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on\nco-ordinate representation, extended proof systems for more efficient\ncertificates of optimality, extensions of Grassmanns extension theory,\nefficient evaluation methods for the effect of exponential number of\nconstraints, theory of connected sets based on graded connectivity, theory of\ncurved spaces adapted to the problem data, and concept of relatively algebraic\nsets in curved space. Since this conference does not have a proceedings, the\npurpose of this article is to provide the material being presented at the\nconference in more widely accessible form.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:16:23 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Karmarkar", "Narendra", ""]]}, {"id": "1412.3359", "submitter": "Qi Duan", "authors": "Qi Duan, Haadi Jafarian, Ehab Al-Shaer and Jinhui Xu", "title": "On DDoS Attack Related Minimum Cut Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two important extensions of the classical minimum cut\nproblem, called {\\em Connectivity Preserving Minimum Cut (CPMC)} problem and\n{\\em Threshold Minimum Cut (TMC)} problem, which have important applications in\nlarge-scale DDoS attacks. In CPMC problem, a minimum cut is sought to separate\na of source from a destination node and meanwhile preserve the connectivity\nbetween the source and its partner node(s). The CPMC problem also has important\napplications in many other areas such as emergency responding, image\nprocessing, pattern recognition, and medical sciences. In TMC problem, a\nminimum cut is sought to isolate a target node from a threshold number of\npartner nodes. TMC problem is an important special case of network inhibition\nproblem and has important applications in network security. We show that the\ngeneral CPMC problem cannot be approximated within $logn$ unless $NP=P$ has\nquasi-polynomial algorithms. We also show that a special case of two group CPMC\nproblem in planar graphs can be solved in polynomial time. The corollary of\nthis result is that the network diversion problem in planar graphs is in $P$, a\npreviously open problem. We show that the threshold minimum node cut (TMNC)\nproblem can be approximated within ratio $O(\\sqrt{n})$ and the threshold\nminimum edge cut problem (TMEC) can be approximated within ratio\n$O(\\log^2{n})$. \\emph{We also answer another long standing open problem: the\nhardness of the network inhibition problem and network interdiction problem. We\nshow that both of them cannot be approximated within any constant ratio. unless\n$NP \\nsubseteq \\cap_{\\delta>0} BPTIME(2^{n^{\\delta}})$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 16:35:32 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 14:33:11 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Duan", "Qi", ""], ["Jafarian", "Haadi", ""], ["Al-Shaer", "Ehab", ""], ["Xu", "Jinhui", ""]]}, {"id": "1412.3377", "submitter": "Niall Murphy", "authors": "Niall Murphy and Damien Woods", "title": "Uniformity is weaker than semi-uniformity for some membrane systems", "comments": "28 pages, 1 figure", "journal-ref": "Fundamenta Informaticae, 134(1-2):129-152. 2014", "doi": "10.3233/FI-2014-1095", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate computing models that are presented as families of finite\ncomputing devices with a uniformity condition on the entire family. Examples of\nsuch models include Boolean circuits, membrane systems, DNA computers, chemical\nreaction networks and tile assembly systems, and there are many others.\nHowever, in such models there are actually two distinct kinds of uniformity\ncondition. The first is the most common and well-understood, where each input\nlength is mapped to a single computing device (e.g. a Boolean circuit) that\ncomputes on the finite set of inputs of that length. The second, called\nsemi-uniformity, is where each input is mapped to a computing device for that\ninput (e.g. a circuit with the input encoded as constants). The former notion\nis well-known and used in Boolean circuit complexity, while the latter notion\nis frequently found in literature on nature-inspired computation from the past\n20 years or so.\n  Are these two notions distinct? For many models it has been found that these\nnotions are in fact the same, in the sense that the choice of uniformity or\nsemi-uniformity leads to characterisations of the same complexity classes. In\nother related work, we showed that these notions are actually distinct for\ncertain classes of Boolean circuits. Here, we give analogous results for\nmembrane systems by showing that certain classes of uniform membrane systems\nare strictly weaker than the analogous semi-uniform classes. This solves a\nknown open problem in the theory of membrane systems. We then go on to present\nresults towards characterising the power of these semi-uniform and uniform\nmembrane models in terms of NL and languages reducible to the unary languages\nin NL, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 17:30:22 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Murphy", "Niall", ""], ["Woods", "Damien", ""]]}, {"id": "1412.3570", "submitter": "Bruno Grenet", "authors": "Bruno Grenet", "title": "Bounded-degree factors of lacunary multivariate polynomials", "comments": "31 pages; Long version of arXiv:1401.4720 with simplified proofs", "journal-ref": "Journal of Symbolic Computation 75, pages 171-192, 2016", "doi": "10.1016/j.jsc.2015.11.013", "report-no": null, "categories": "cs.SC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method for computing bounded-degree factors\nof lacunary multivariate polynomials. In particular for polynomials over number\nfields, we give a new algorithm that takes as input a multivariate polynomial f\nin lacunary representation and a degree bound d and computes the irreducible\nfactors of degree at most d of f in time polynomial in the lacunary size of f\nand in d. Our algorithm, which is valid for any field of zero characteristic,\nis based on a new gap theorem that enables reducing the problem to several\ninstances of (a) the univariate case and (b) low-degree multivariate\nfactorization.\n  The reduction algorithms we propose are elementary in that they only\nmanipulate the exponent vectors of the input polynomial. The proof of\ncorrectness and the complexity bounds rely on the Newton polytope of the\npolynomial, where the underlying valued field consists of Puiseux series in a\nsingle variable.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 08:41:03 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 10:45:16 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Grenet", "Bruno", ""]]}, {"id": "1412.3955", "submitter": "Dimitrios Thilikos", "authors": "Petr A. Golovach, Marcin Kami\\'nski, Spyridon Maniatis, Dimitrios M.\n  Thilikos", "title": "The Parameterized Complexity of Graph Cyclability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclability of a graph is the maximum integer $k$ for which every $k$\nvertices lie on a cycle. The algorithmic version of the problem, given a graph\n$G$ and a non-negative integer $k,$ decide whether the cyclability of $G$ is at\nleast $k,$ is {\\sf NP}-hard. We study the parametrized complexity of this\nproblem. We prove that this problem, parameterized by $k,$ is ${\\sf\nco\\mbox{-}W[1]}$-hard and that its does not admit a polynomial kernel on planar\ngraphs, unless ${\\sf NP}\\subseteq{\\sf co}\\mbox{-}{\\sf NP}/{\\sf poly}$. On the\npositive side, we give an {\\sf FPT} algorithm for planar graphs that runs in\ntime $2^{2^{O(k^2\\log k)}}\\cdot n^2$. Our algorithm is based on a series of\ngraph-theoretical results on cyclic linkages in planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 11:43:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 14:43:22 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Golovach", "Petr A.", ""], ["Kami\u0144ski", "Marcin", ""], ["Maniatis", "Spyridon", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1412.4188", "submitter": "Jan Kyn\\v{c}l", "authors": "Jan Kyn\\v{c}l, Bernard Lidick\\'y and Tom\\'a\\v{s} Vysko\\v{c}il", "title": "Irreversible 2-conversion set in graphs of bounded degree", "comments": "18 pages, 12 figures; journal version", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  3, Graph Theory (September 26, 2017) dmtcs:3952", "doi": "10.23638/DMTCS-19-3-5", "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An irreversible $k$-threshold process (also a $k$-neighbor bootstrap\npercolation) is a dynamic process on a graph where vertices change color from\nwhite to black if they have at least $k$ black neighbors. An irreversible\n$k$-conversion set of a graph $G$ is a subset $S$ of vertices of $G$ such that\nthe irreversible $k$-threshold process starting with $S$ black eventually\nchanges all vertices of $G$ to black. We show that deciding the existence of an\nirreversible 2-conversion set of a given size is NP-complete, even for graphs\nof maximum degree 4, which answers a question of Dreyer and Roberts.\nConversely, we show that for graphs of maximum degree 3, the minimum size of an\nirreversible 2-conversion set can be computed in polynomial time. Moreover, we\nfind an optimal irreversible 3-conversion set for the toroidal grid,\nsimplifying constructions of Pike and Zou.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 03:35:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 02:39:17 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 18:12:56 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 14:30:44 GMT"}, {"version": "v5", "created": "Fri, 22 Sep 2017 21:04:18 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kyn\u010dl", "Jan", ""], ["Lidick\u00fd", "Bernard", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""]]}, {"id": "1412.4203", "submitter": "Nikolaos Kariotoglou", "authors": "Nikolaos Kariotoglou, Kostas Margellos and John Lygeros", "title": "On the computational complexity and generalization properties of\n  multi-stage and recursive scenario programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the computational complexity and feasibility properties of\nscenario based techniques for uncertain optimization programs. We consider\ndifferent solution alternatives ranging from the standard scenario approach to\nrecursive variants, and compare feasibility as a function of the total\ncomputation burden. We identify trade-offs between the different methods\ndepending on the problem structure and the desired probability of constraint\nsatisfaction. Our motivation for this work stems from the applicability and\ncomplexity reduction when making decisions by means of recursive algorithms. We\nillustrate our results on an example from the area of approximate dynamic\nprogramming\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 08:07:59 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Kariotoglou", "Nikolaos", ""], ["Margellos", "Kostas", ""], ["Lygeros", "John", ""]]}, {"id": "1412.4259", "submitter": "Michael Blondin", "authors": "Michael Blondin, Alain Finkel, Stefan G\\\"oller, Christoph Haase,\n  Pierre McKenzie", "title": "Reachability in Two-Dimensional Vector Addition Systems with States is\n  PSPACE-complete", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": "10.1109/LICS.2015.14", "report-no": null, "categories": "cs.FL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the complexity of the reachability problem for vector addition\nsystems with states (VASS) is a long-standing open problem in computer science.\nLong known to be decidable, the problem to this day lacks any complexity upper\nbound whatsoever. In this paper, reachability for two-dimensional VASS is shown\nPSPACE-complete. This improves on a previously known doubly exponential time\nbound established by Howell, Rosier, Huynh and Yen in 1986. The coverability\nand boundedness problems are also noted to be PSPACE-complete. In addition,\nsome complexity results are given for the reachability problem in\ntwo-dimensional VASS and in integer VASS when numbers are encoded in unary.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 17:10:34 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Blondin", "Michael", ""], ["Finkel", "Alain", ""], ["G\u00f6ller", "Stefan", ""], ["Haase", "Christoph", ""], ["McKenzie", "Pierre", ""]]}, {"id": "1412.4273", "submitter": "Maciej Drwal", "authors": "Maciej Drwal, Roman Rischke", "title": "Complexity of interval minmax regret scheduling on parallel identical\n  machines with total completion time criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of scheduling jobs on parallel\nidentical machines, where the processing times of jobs are uncertain: only\ninterval bounds of processing times are known. The optimality criterion of a\nschedule is the total completion time. In order to cope with the uncertainty,\nwe consider the maximum regret objective and we seek a schedule that performs\nwell under all possible instantiations of processing times. Although the\ndeterministic version of the considered problem is solvable in polynomial time,\nthe minmax regret version is known to be weakly NP-hard even for a single\nmachine, and strongly NP-hard for parallel unrelated machines. In this paper,\nwe show that the problem is strongly NP-hard also in the case of parallel\nidentical machines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 19:18:48 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 09:15:05 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 11:57:03 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Drwal", "Maciej", ""], ["Rischke", "Roman", ""]]}, {"id": "1412.4413", "submitter": "Jop Bri\\\"et", "authors": "Jop Bri\\\"et, Oded Regev, and Rishi Saket", "title": "Tight Hardness of the Non-commutative Grothendieck Problem", "comments": "19 pages. Changes compared to v1: strengthened results to NP-hardness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for any $\\varepsilon > 0$ it is NP-hard to approximate the\nnon-commutative Grothendieck problem to within a factor $1/2 + \\varepsilon$,\nwhich matches the approximation ratio of the algorithm of Naor, Regev, and\nVidick (STOC'13). Our proof uses an embedding of $\\ell_2$ into the space of\nmatrices endowed with the trace norm with the property that the image of\nstandard basis vectors is longer than that of unit vectors with no large\ncoordinates.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 21:33:29 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 16:19:19 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Bri\u00ebt", "Jop", ""], ["Regev", "Oded", ""], ["Saket", "Rishi", ""]]}, {"id": "1412.4719", "submitter": "Abhishek Bhowmick", "authors": "Abhishek Bhowmick, Shachar Lovett", "title": "Nonclassical polynomials as a barrier to polynomial lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constructing explicit functions which cannot be approximated\nby low degree polynomials has been extensively studied in computational\ncomplexity, motivated by applications in circuit lower bounds,\npseudo-randomness, constructions of Ramsey graphs and locally decodable codes.\nStill, most of the known lower bounds become trivial for polynomials of\nsuper-logarithmic degree. Here, we suggest a new barrier explaining this\nphenomenon. We show that many of the existing lower bound proof techniques\nextend to nonclassical polynomials, an extension of classical polynomials which\narose in higher order Fourier analysis. Moreover, these techniques are tight\nfor nonclassical polynomials of logarithmic degree.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 18:51:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Bhowmick", "Abhishek", ""], ["Lovett", "Shachar", ""]]}, {"id": "1412.4832", "submitter": "Howard Karloff", "authors": "Dean Foster, Howard Karloff, and Justin Thaler", "title": "Variable Selection is Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for sparse linear regression is the problem of finding,\ngiven an m x p matrix B and a target vector y, a sparse vector x such that Bx\napproximately equals y. Assuming a standard complexity hypothesis, we show that\nno polynomial-time algorithm can find a k'-sparse x with ||Bx-y||^2<=h(m,p),\nwhere k'=k*2^{log^{1-delta} p} and h(m,p)<=p^(C_1)*m^(1-C_2), where delta>0,\nC_1>0,C_2>0 are arbitrary. This is true even under the promise that there is an\nunknown k-sparse vector x^* satisfying Bx^*=y. We prove a similar result for a\nstatistical version of the problem in which the data are corrupted by noise.\n  To the authors' knowledge, these are the first hardness results for sparse\nregression that apply when the algorithm simultaneously has k'>k and h(m,p)>0.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 23:15:40 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Foster", "Dean", ""], ["Karloff", "Howard", ""], ["Thaler", "Justin", ""]]}, {"id": "1412.4904", "submitter": "Hartmut Klauck", "authors": "Hartmut Klauck and Supartha Podder", "title": "New Bounds for the Garden-Hose Model", "comments": "In FSTTCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show new results about the garden-hose model. Our main results include\nimproved lower bounds based on non-deterministic communication complexity\n(leading to the previously unknown $\\Theta(n)$ bounds for Inner Product mod 2\nand Disjointness), as well as an $O(n\\cdot \\log^3 n)$ upper bound for the\nDistributed Majority function (previously conjectured to have quadratic\ncomplexity). We show an efficient simulation of formulae made of AND, OR, XOR\ngates in the garden-hose model, which implies that lower bounds on the\ngarden-hose complexity $GH(f)$ of the order $\\Omega(n^{2+\\epsilon})$ will be\nhard to obtain for explicit functions. Furthermore we study a time-bounded\nvariant of the model, in which even modest savings in time can lead to\nexponential lower bounds on the size of garden-hose protocols.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 08:03:58 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Klauck", "Hartmut", ""], ["Podder", "Supartha", ""]]}, {"id": "1412.5484", "submitter": "Sheela Devadas", "authors": "Sheela Devadas, Ronitt Rubinfeld", "title": "A Self-Tester for Linear Functions over the Integers with an Elementary\n  Proof of Correctness", "comments": null, "journal-ref": null, "doi": "10.1007/s00224-015-9639-z", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple, self-contained proofs of correctness for algorithms for\nlinearity testing and program checking of linear functions on finite subsets of\nintegers represented as n-bit numbers. In addition we explore a generalization\nof self-testing to homomorphisms on a multidimensional vector space. We show\nthat our self-testing algorithm for the univariate case can be directly\ngeneralized to vector space domains. The number of queries made by our\nalgorithms is independent of domain size.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 17:07:47 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 18:59:50 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 22:31:21 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Devadas", "Sheela", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1412.5655", "submitter": "Xi Chen", "authors": "Xi Chen and Rocco A. Servedio and Li-Yang Tan", "title": "New algorithms and lower bounds for monotonicity testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing whether an unknown Boolean function $f$ is\nmonotone versus $\\epsilon$-far from every monotone function. The two main\nresults of this paper are a new lower bound and a new algorithm for this\nwell-studied problem.\n  Lower bound: We prove an $\\tilde{\\Omega}(n^{1/5})$ lower bound on the query\ncomplexity of any non-adaptive two-sided error algorithm for testing whether an\nunknown Boolean function $f$ is monotone versus constant-far from monotone.\nThis gives an exponential improvement on the previous lower bound of\n$\\Omega(\\log n)$ due to Fischer et al. [FLN+02]. We show that the same lower\nbound holds for monotonicity testing of Boolean-valued functions over hypergrid\ndomains $\\{1,\\ldots,m\\}^n$ for all $m\\ge 2$.\n  Upper bound: We give an $\\tilde{O}(n^{5/6})\\text{poly}(1/\\epsilon)$-query\nalgorithm that tests whether an unknown Boolean function $f$ is monotone versus\n$\\epsilon$-far from monotone. Our algorithm, which is non-adaptive and makes\none-sided error, is a modified version of the algorithm of Chakrabarty and\nSeshadhri [CS13a], which makes $\\tilde{O}(n^{7/8})\\text{poly}(1/\\epsilon)$\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:26:10 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Chen", "Xi", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1412.5657", "submitter": "Xi Chen", "authors": "Xi Chen and Anindya De and Rocco A. Servedio and Li-Yang Tan", "title": "Boolean function monotonicity testing requires (almost) $n^{1/2}$\n  non-adaptive queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a lower bound of $\\Omega(n^{1/2 - c})$, for all $c>0$, on the query\ncomplexity of (two-sided error) non-adaptive algorithms for testing whether an\n$n$-variable Boolean function is monotone versus constant-far from monotone.\nThis improves a $\\tilde{\\Omega}(n^{1/5})$ lower bound for the same problem that\nwas recently given in [CST14] and is very close to $\\Omega(n^{1/2})$, which we\nconjecture is the optimal lower bound for this model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:37:54 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Chen", "Xi", ""], ["De", "Anindya", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1412.5893", "submitter": "Pavel Hrubes", "authors": "Pavel Hrube\\v{s}", "title": "On families of anticommuting matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $e_{1},\\dots, e_{k}$ be complex $n\\times n$ matrices such that\n$e_{i}e_{j}=-e_{j}e_{i}$ whenever $i\\not=j$. We conjecture that\n$\\hbox{rk}(e_{1}^{2})+\\hbox{rk}(e_{2}^{2})+\\cdots+\\hbox{rk}(e_{k}^{2})\\leq\nO(n\\log n)$, and prove some results in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:32:24 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Hrube\u0161", "Pavel", ""]]}, {"id": "1412.6265", "submitter": "Amit Daniely", "authors": "Amit Daniely and Michael Schapira and Gal Shahaf", "title": "Inapproximability of Truthful Mechanisms via Generalizations of the VC\n  Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic mechanism design (AMD) studies the delicate interplay between\ncomputational efficiency, truthfulness, and optimality. We focus on AMD's\nparadigmatic problem: combinatorial auctions. We present a new generalization\nof the VC dimension to multivalued collections of functions, which encompasses\nthe classical VC dimension, Natarajan dimension, and Steele dimension. We\npresent a corresponding generalization of the Sauer-Shelah Lemma and harness\nthis VC machinery to establish inapproximability results for deterministic\ntruthful mechanisms. Our results essentially unify all inapproximability\nresults for deterministic truthful mechanisms for combinatorial auctions to\ndate and establish new separation gaps between truthful and non-truthful\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 09:57:07 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2015 09:48:33 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Daniely", "Amit", ""], ["Schapira", "Michael", ""], ["Shahaf", "Gal", ""]]}, {"id": "1412.6268", "submitter": "Jiyou Li", "authors": "Jiyou Li and Chu Luo", "title": "The simplified weighted sum function and its average sensitivity", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we simplify the definition of the weighted sum Boolean function\nwhich used to be inconvenient to compute and use. We show that the new function\nhas essentially the same properties as the previous one. In particular, the\nbound on the average sensitivity of the weighted sum Boolean function remains\nunchanged after the simplification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:08:59 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Li", "Jiyou", ""], ["Luo", "Chu", ""]]}, {"id": "1412.6396", "submitter": "Till Tantau", "authors": "Till Tantau", "title": "Existential Second-Order Logic Over Graphs: A Complete\n  Complexity-Theoretic Classification", "comments": "Technical report version of a STACS 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive complexity theory aims at inferring a problem's computational\ncomplexity from the syntactic complexity of its description. A cornerstone of\nthis theory is Fagin's Theorem, by which a graph property is expressible in\nexistential second-order logic (ESO logic) if, and only if, it is in NP. A\nnatural question, from the theory's point of view, is which syntactic fragments\nof ESO logic also still characterize NP. Research on this question has\nculminated in a dichotomy result by Gottlob, Kolatis, and Schwentick: for each\npossible quantifier prefix of an ESO formula, the resulting prefix class either\ncontains an NP-complete problem or is contained in P. However, the exact\ncomplexity of the prefix classes inside P remained elusive. In the present\npaper, we clear up the picture by showing that for each prefix class of ESO\nlogic, its reduction closure under first-order reductions is either FO, L, NL,\nor NP. For undirected, self-loop-free graphs two containment results are\nespecially challenging to prove: containment in L for the prefix $\\exists R_1\n\\cdots \\exists R_n \\forall x \\exists y$ and containment in FO for the prefix\n$\\exists M \\forall x \\exists y$ for monadic $M$. The complex argument by\nGottlob, Kolatis, and Schwentick concerning polynomial time needs to be\ncarefully reexamined and either combined with the logspace version of\nCourcelle's Theorem or directly improved to first-order computations. A\ndifferent challenge is posed by formulas with the prefix $\\exists M \\forall\nx\\forall y$: We show that they express special constraint satisfaction problems\nthat lie in L.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:51:33 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Tantau", "Till", ""]]}, {"id": "1412.6507", "submitter": "Adam Bouland", "authors": "Scott Aaronson, Adam Bouland, Joseph Fitzsimons, Mitchell Lee", "title": "The space \"just above\" BQP", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the space \"just above\" BQP by defining a complexity class PDQP\n(Product Dynamical Quantum Polynomial time) which is larger than BQP but does\nnot contain NP relative to an oracle. The class is defined by imagining that\nquantum computers can perform measurements that do not collapse the\nwavefunction. This (non-physical) model of computation can efficiently solve\nproblems such as Graph Isomorphism and Approximate Shortest Vector which are\nbelieved to be intractable for quantum computers. Furthermore, it can search an\nunstructured N-element list in $\\tilde O$(N^{1/3}) time, but no faster than\n{\\Omega}(N^{1/4}), and hence cannot solve NP-hard problems in a black box\nmanner. In short, this model of computation is more powerful than standard\nquantum computation, but only slightly so.\n  Our work is inspired by previous work of Aaronson on the power of sampling\nthe histories of hidden variables. However Aaronson's work contains an error in\nits proof of the lower bound for search, and hence it is unclear whether or not\nhis model allows for search in logarithmic time. Our work can be viewed as a\nconceptual simplification of Aaronson's approach, with a provable polynomial\nlower bound for search.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:06:58 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Aaronson", "Scott", ""], ["Bouland", "Adam", ""], ["Fitzsimons", "Joseph", ""], ["Lee", "Mitchell", ""]]}, {"id": "1412.6641", "submitter": "Omid Etesami", "authors": "Salman Beigi, Omid Etesami, and Amin Gohari", "title": "Deterministic Randomness Extraction from Generalized and Distributed\n  Santha-Vazirani Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Santha-Vazirani (SV) source is a sequence of random bits where the\nconditional distribution of each bit, given the previous bits, can be partially\ncontrolled by an adversary. Santha and Vazirani show that deterministic\nrandomness extraction from these sources is impossible. In this paper, we study\nthe generalization of SV sources for non-binary sequences. We show that unlike\nthe binary case, deterministic randomness extraction in the generalized case is\nsometimes possible. We present a necessary condition and a sufficient condition\nfor the possibility of deterministic randomness extraction. These two\nconditions coincide in \"non-degenerate\" cases.\n  Next, we turn to a distributed setting. In this setting the SV source\nconsists of a random sequence of pairs $(a_1, b_1), (a_2, b_2), \\ldots$\ndistributed between two parties, where the first party receives $a_i$'s and the\nsecond one receives $b_i$'s. The goal of the two parties is to extract common\nrandomness without communication. Using the notion of maximal correlation, we\nprove a necessary condition and a sufficient condition for the possibility of\ncommon randomness extraction from these sources. Based on these two conditions,\nthe problem of common randomness extraction essentially reduces to the problem\nof randomness extraction from (non-distributed) SV sources. This result\ngeneralizes results of G\\'acs and K\\\"orner, and Witsenhausen about common\nrandomness extraction from i.i.d. sources to adversarial sources.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 10:49:39 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Beigi", "Salman", ""], ["Etesami", "Omid", ""], ["Gohari", "Amin", ""]]}, {"id": "1412.6761", "submitter": "Masaki Nakanishi", "authors": "Masaki Nakanishi, Abuzer Yakary{\\i}lmaz and Aida Gainutdinova", "title": "New results on classical and quantum counter automata", "comments": "21 pages", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 21 no.\n  4, Automata, Logic and Semantics (September 27, 2019) dmtcs:5788", "doi": "10.23638/DMTCS-21-4-12", "report-no": null, "categories": "cs.FL cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that one-way quantum one-counter automaton with zero-error is more\npowerful than its probabilistic counterpart on promise problems. Then, we\nobtain a similar separation result between Las Vegas one-way probabilistic\none-counter automaton and one-way deterministic one-counter automaton.\n  We also obtain new results on classical counter automata regarding language\nrecognition. It was conjectured that one-way probabilistic one blind-counter\nautomata cannot recognize Kleene closure of equality language [A. Yakaryilmaz:\nSuperiority of one-way and realtime quantum machines. RAIRO - Theor. Inf. and\nApplic. 46(4): 615-641 (2012)]. We show that this conjecture is false, and also\nshow several separation results for blind/non-blind counter automata.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 10:41:03 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 10:13:01 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 02:06:47 GMT"}, {"version": "v4", "created": "Wed, 25 Sep 2019 01:44:36 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Nakanishi", "Masaki", ""], ["Yakary\u0131lmaz", "Abuzer", ""], ["Gainutdinova", "Aida", ""]]}, {"id": "1412.6787", "submitter": "Kees Middelburg", "authors": "J. A. Bergstra, C. A. Middelburg", "title": "Instruction sequence size complexity of parity", "comments": "12 pages, the preliminaries are largely the same as the preliminaries\n  in arXiv:1312.1812 [cs.PL] and some earlier papers; 13 pages, minor errors\n  corrected; 13 pages, presentation improved; 14 pages, remarks about related\n  work added; 14 pages, presentation improved", "journal-ref": "Fundamenta Informaticae, 149(3):297--309, 2016", "doi": "10.3233/FI-2016-1450", "report-no": null, "categories": "cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each Boolean function can be computed by a single-pass instruction sequence\nthat contains only instructions to set and get the content of Boolean\nregisters, forward jump instructions, and a termination instruction. Auxiliary\nBoolean registers are not necessary for this. In the current paper, we show\nthat, in the case of the parity functions, shorter instruction sequences are\npossible with the use of an auxiliary Boolean register in the presence of\ninstructions to complement the content of auxiliary Boolean registers. This\nresult supports, in a setting where programs are instruction sequences acting\non Boolean registers, a basic intuition behind the storage of auxiliary data,\nnamely the intuition that this makes possible a reduction of the size of a\nprogram.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 14:29:29 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 07:44:17 GMT"}, {"version": "v3", "created": "Sat, 28 May 2016 09:38:30 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 08:29:13 GMT"}, {"version": "v5", "created": "Fri, 15 Jul 2016 09:44:32 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Bergstra", "J. A.", ""], ["Middelburg", "C. A.", ""]]}, {"id": "1412.6935", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Benjamin Sach", "title": "Time Bounds for Streaming Problems", "comments": "31 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1207.1885", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give tight cell-probe bounds for the time to compute convolution,\nmultiplication and Hamming distance in a stream. The cell probe model is a\nparticularly strong computational model and subsumes, for example, the popular\nword RAM model.\n  We first consider online convolution where the task is to output the inner\nproduct between a fixed $n$-dimensional vector and a vector of the $n$ most\nrecent values from a stream. One symbol of the stream arrives at a time and the\neach output must be computed before the next symbols arrives.\n  Next we show bounds for online multiplication where the stream consists of\npairs of digits, one from each of two $n$ digit numbers that are to be\nmultiplied. One pair arrives at a time and the task is to output a single new\ndigit from the product before the next pair of digits arrives.\n  Finally we look at the online Hamming distance problem where the Hamming\ndistance is outputted instead of the inner product.\n  For each of these three problems, we give a lower bound of\n$\\Omega(\\frac{\\delta}{w}\\log n)$ time on average per output, where $\\delta$ is\nthe number of bits needed to represent an input symbol and $w$ is the cell or\nword size. We argue that these bound are in fact tight within the cell probe\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 11:35:53 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Sach", "Benjamin", ""]]}, {"id": "1412.7373", "submitter": "Abhishek Bhowmick", "authors": "Abhishek Bhowmick, Th\\'ai Ho\\`ang L\\^e", "title": "On primitive elements in finite fields of low characteristic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of constructing a small subset of a finite field\ncontaining primitive elements of the field. Given a finite field,\n$\\mathbb{F}_{q^n}$, small $q$ and large $n$, we show that the set of all low\ndegree polynomials contains the expected number of primitive elements.\n  The main theorem we prove is a bound for character sums over short intervals\nin function fields. Our result is unconditional and slightly better than what\nis known (conditionally under GRH) in the integer case and might be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:05:53 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Bhowmick", "Abhishek", ""], ["L\u00ea", "Th\u00e1i Ho\u00e0ng", ""]]}, {"id": "1412.7979", "submitter": "Daniel Dadush", "authors": "Kai-Min Chung, Daniel Dadush, Feng-Hao Liu, Chris Peikert", "title": "On the Lattice Smoothing Parameter Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smoothing parameter $\\eta_{\\epsilon}(\\mathcal{L})$ of a Euclidean lattice\n$\\mathcal{L}$, introduced by Micciancio and Regev (FOCS'04; SICOMP'07), is\n(informally) the smallest amount of Gaussian noise that \"smooths out\" the\ndiscrete structure of $\\mathcal{L}$ (up to error $\\epsilon$). It plays a\ncentral role in the best known worst-case/average-case reductions for lattice\nproblems, a wealth of lattice-based cryptographic constructions, and\n(implicitly) the tightest known transference theorems for fundamental lattice\nquantities.\n  In this work we initiate a study of the complexity of approximating the\nsmoothing parameter to within a factor $\\gamma$, denoted $\\gamma$-${\\rm\nGapSPP}$. We show that (for $\\epsilon = 1/{\\rm poly}(n)$): $(2+o(1))$-${\\rm\nGapSPP} \\in {\\rm AM}$, via a Gaussian analogue of the classic\nGoldreich-Goldwasser protocol (STOC'98); $(1+o(1))$-${\\rm GapSPP} \\in {\\rm\ncoAM}$, via a careful application of the Goldwasser-Sipser (STOC'86) set size\nlower bound protocol to thin spherical shells; $(2+o(1))$-${\\rm GapSPP} \\in\n{\\rm SZK} \\subseteq {\\rm AM} \\cap {\\rm coAM}$ (where ${\\rm SZK}$ is the class\nof problems having statistical zero-knowledge proofs), by constructing a\nsuitable instance-dependent commitment scheme (for a slightly worse\n$o(1)$-term); $(1+o(1))$-${\\rm GapSPP}$ can be solved in deterministic\n$2^{O(n)} {\\rm polylog}(1/\\epsilon)$ time and $2^{O(n)}$ space.\n  As an application, we demonstrate a tighter worst-case to average-case\nreduction for basing cryptography on the worst-case hardness of the ${\\rm\nGapSPP}$ problem, with $\\tilde{O}(\\sqrt{n})$ smaller approximation factor than\nthe ${\\rm GapSVP}$ problem.\n  Central to our results are two novel, and nearly tight, characterizations of\nthe magnitude of discrete Gaussian sums.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 19:50:58 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Chung", "Kai-Min", ""], ["Dadush", "Daniel", ""], ["Liu", "Feng-Hao", ""], ["Peikert", "Chris", ""]]}, {"id": "1412.8671", "submitter": "Ciar\\'an M. Lee", "authors": "Ciar\\'an M. Lee, Jonathan Barrett", "title": "Computation in generalised probabilistic theories", "comments": "14+9 pages. Comments welcome", "journal-ref": "New J. Phys. 17 (2015) 083001", "doi": "10.1088/1367-2630/17/8/083001", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the existence of an efficient quantum algorithm for factoring, it is\nlikely that quantum computation is intrinsically more powerful than classical\ncomputation. At present, the best upper bound known for the power of quantum\ncomputation is that BQP is in AWPP. This work investigates limits on\ncomputational power that are imposed by physical principles. To this end, we\ndefine a circuit-based model of computation in a class of operationally-defined\ntheories more general than quantum theory, and ask: what is the minimal set of\nphysical assumptions under which the above inclusion still holds? We show that\ngiven only an assumption of tomographic locality (roughly, that multipartite\nstates can be characterised by local measurements), efficient computations are\ncontained in AWPP. This inclusion still holds even without assuming a basic\nnotion of causality (where the notion is, roughly, that probabilities for\noutcomes cannot depend on future measurement choices). Following Aaronson, we\nextend the computational model by allowing post-selection on measurement\noutcomes. Aaronson showed that the corresponding quantum complexity class is\nequal to PP. Given only the assumption of tomographic locality, the inclusion\nin PP still holds for post-selected computation in general theories. Thus in a\nworld with post-selection, quantum theory is optimal for computation in the\nspace of all general theories. We then consider if relativised complexity\nresults can be obtained for general theories. It is not clear how to define a\nsensible notion of an oracle in the general framework that reduces to the\nstandard notion in the quantum case. Nevertheless, it is possible to define\ncomputation relative to a `classical oracle'. Then, we show there exists a\nclassical oracle relative to which efficient computation in any theory\nsatisfying the causality assumption and tomographic locality does not include\nNP.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 16:13:30 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 18:31:07 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Lee", "Ciar\u00e1n M.", ""], ["Barrett", "Jonathan", ""]]}, {"id": "1412.8746", "submitter": "Iddo Tzameret", "authors": "Fu Li, Iddo Tzameret, Zhengyu Wang", "title": "Characterizing Propositional Proofs as Non-Commutative Formulas", "comments": "Extended abstract appeared in Proc. of CCC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does every Boolean tautology have a short propositional-calculus proof? Here,\na propositional calculus (i.e. Frege) proof is a proof starting from a set of\naxioms and deriving new Boolean formulas using a set of fixed sound derivation\nrules. Establishing any super-polynomial size lower bound on Frege proofs (in\nterms of the size of the formula proved) is a major open problem in proof\ncomplexity, and among a handful of fundamental hardness questions in complexity\ntheory by and large. Non-commutative arithmetic formulas, on the other hand,\nconstitute a quite weak computational model, for which exponential-size lower\nbounds were shown already back in 1991 by Nisan [Nis91] who used a particularly\ntransparent argument.\n  In this work we show that Frege lower bounds in fact follow from\ncorresponding size lower bounds on non-commutative formulas computing certain\npolynomials (and that such lower bounds on non-commutative formulas must exist,\nunless NP=coNP). More precisely, we demonstrate a natural association between\ntautologies $T$ to non-commutative polynomials $p$, such that: if $T$ has a\npolynomial-size Frege proof then $p$ has a polynomial-size non-commutative\narithmetic formula; and conversely, when $T$ is a DNF, if $p$ has a\npolynomial-size non-commutative arithmetic formula over $GF(2)$ then $T$ has a\nFrege proof of quasi-polynomial size.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 19:48:55 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 03:56:07 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 01:19:05 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2015 17:23:44 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Li", "Fu", ""], ["Tzameret", "Iddo", ""], ["Wang", "Zhengyu", ""]]}]