[{"id": "1211.0071", "submitter": "Leonid A. Levin", "authors": "Leonid A. Levin", "title": "Randomness and Non-determinism", "comments": "1992 talk at ASL meeting", "journal-ref": "Journal of Symbolic Logic, 58/3:1102-1103, 1993", "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponentiation makes the difference between the bit-size of this line and the\nnumber (<< 2^{300}) of particles in the known Universe. The expulsion of\nexponential time algorithms from Computer Theory in the 60's broke its\numbilical cord from Mathematical Logic. It created a deep gap between\ndeterministic computation and -- formerly its unremarkable tools -- randomness\nand non-determinism. Little did we learn in the past decades about the power of\neither of these two basic \"freedoms\" of computation, but some vague pattern is\nemerging in relationships between them. The pattern of similar techniques\ninstrumental for quite different results in this area seems even more\ninteresting. Ideas like multilinear and low-degree multivariate polynomials,\nFourier transformation over low-periodic groups seem very illuminating. The\ntalk surveyed some recent results. One of them, given in a stronger form than\npreviously published, is described below.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 01:09:48 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Levin", "Leonid A.", ""]]}, {"id": "1211.0330", "submitter": "Shubhangi Saraf", "authors": "Zeev Dvir, Shubhangi Saraf, Avi Wigderson", "title": "Improved rank bounds for design matrices and a new proof of Kelly's\n  theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the rank of complex sparse matrices in which the supports of\ndifferent columns have small intersections. The rank of these matrices, called\ndesign matrices, was the focus of a recent work by Barak et. al. (BDWY11) in\nwhich they were used to answer questions regarding point configurations. In\nthis work we derive near-optimal rank bounds for these matrices and use them to\nobtain asymptotically tight bounds in many of the geometric applications. As a\nconsequence of our improved analysis, we also obtain a new, linear algebraic,\nproof of Kelly's theorem, which is the complex analog of the Sylvester-Gallai\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 00:07:29 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Dvir", "Zeev", ""], ["Saraf", "Shubhangi", ""], ["Wigderson", "Avi", ""]]}, {"id": "1211.0331", "submitter": "Shubhangi Saraf", "authors": "Albert Ai, Zeev Dvir, Shubhangi Saraf, Avi Wigderson", "title": "Sylvester-Gallai type theorems for approximate collinearity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study questions in incidence geometry where the precise position of points\nis `blurry' (e.g. due to noise, inaccuracy or error). Thus lines are replaced\nby narrow tubes, and more generally affine subspaces are replaced by their\nsmall neighborhood. We show that the presence of a sufficiently large number of\napproximately collinear triples in a set of points in d dimensional complex\nspace implies that the points are close to a low dimensional affine subspace.\nThis can be viewed as a stable variant of the Sylvester-Gallai theorem and its\nextensions.\n  Building on the recently found connection between Sylvester-Gallai type\ntheorems and complex Locally Correctable Codes (LCCs), we define the new notion\nof stable LCCs, in which the (local) correction procedure can also handle small\nperturbations in the euclidean metric. We prove that such stable codes with\nconstant query complexity do not exist. No impossibility results were known in\nany such local setting for more than 2 queries.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 00:19:26 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Ai", "Albert", ""], ["Dvir", "Zeev", ""], ["Saraf", "Shubhangi", ""], ["Wigderson", "Avi", ""]]}, {"id": "1211.0517", "submitter": "Prathapasinghe Dharmawansa", "authors": "Prathapasinghe Dharmawansa, Matthew McKay, and Yang Chen", "title": "Distributions of Demmel and Related Condition Numbers", "comments": "To appear in SIAM Journal on Matrix Analysis and Applications (SIMAX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random matrix $\\mathbf{A}\\in\\mathbb{C}^{m\\times n}$ ($m \\geq n$)\ncontaining independent complex Gaussian entries with zero mean and unit\nvariance, and let $0<\\lambda_1\\leq \\lambda_{2}\\leq ...\\leq \\lambda_n<\\infty$\ndenote the eigenvalues of $\\mathbf{A}^{*}\\mathbf{A}$ where $(\\cdot)^*$\nrepresents conjugate-transpose. This paper investigates the distribution of the\nrandom variables $\\frac{\\sum_{j=1}^n \\lambda_j}{\\lambda_k}$, for $k = 1$ and $k\n= 2$. These two variables are related to certain condition number metrics,\nincluding the so-called Demmel condition number, which have been shown to arise\nin a variety of applications. For both cases, we derive new exact expressions\nfor the probability densities, and establish the asymptotic behavior as the\nmatrix dimensions grow large. In particular, it is shown that as $n$ and $m$\ntend to infinity with their difference fixed, both densities scale on the order\nof $n^3$. After suitable transformations, we establish exact expressions for\nthe asymptotic densities, obtaining simple closed-form expressions in some\ncases. Our results generalize the work of Edelman on the Demmel condition\nnumber for the case $m = n$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 18:38:29 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Dharmawansa", "Prathapasinghe", ""], ["McKay", "Matthew", ""], ["Chen", "Yang", ""]]}, {"id": "1211.0606", "submitter": "Michael Vyalyi", "authors": "Mikhail N. Vyalyi", "title": "On complexity of regular realizability problems", "comments": "Submitted to Problems of Information Transmission. Corrected and\n  extended version, main results are the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regular realizability (RR) problem is testing nonemptiness of intersection\nof some fixed language (filter) with given regular language. We study here\ncomplexity of RR problems. It appears that for any language L there exists RR\nproblem equivalent to L under disjunctive reductions on nondeterministic log\nspace. It implies that for any level of polynomial hierarchy there exists\ncomplete RR problem under polynomial reductions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 08:53:57 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2012 09:17:13 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Vyalyi", "Mikhail N.", ""]]}, {"id": "1211.0665", "submitter": "Pascal Koiran", "authors": "Pascal Koiran (LIP), Anastasios Zouzias", "title": "Hidden cliques and the certification of the restricted isometry property", "comments": "arXiv admin note: substantial text overlap with arXiv:1103.4984", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing is a technique for finding sparse solutions to\nunderdetermined linear systems. This technique relies on properties of the\nsensing matrix such as the restricted isometry property. Sensing matrices that\nsatisfy this property with optimal parameters are mainly obtained via\nprobabilistic arguments. Deciding whether a given matrix satisfies the\nrestricted isometry property is a non-trivial computational problem. Indeed, we\nshow in this paper that restricted isometry parameters cannot be approximated\nin polynomial time within any constant factor under the assumption that the\nhidden clique problem is hard. Moreover, on the positive side we propose an\nimprovement on the brute-force enumeration algorithm for checking the\nrestricted isometry property.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 07:24:50 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Koiran", "Pascal", "", "LIP"], ["Zouzias", "Anastasios", ""]]}, {"id": "1211.0721", "submitter": "Andris Ambainis", "authors": "Andris Ambainis", "title": "Superlinear advantage for exact quantum algorithms", "comments": "20 pages, v6: small number of small corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum algorithm is exact if, on any input data, it outputs the correct\nanswer with certainty (probability 1). A key question is: how big is the\nadvantage of exact quantum algorithms over their classical counterparts:\ndeterministic algorithms. For total Boolean functions in the query model, the\nbiggest known gap was just a factor of 2: PARITY of N inputs bits requires $N$\nqueries classically but can be computed with N/2 queries by an exact quantum\nalgorithm.\n  We present the first example of a Boolean function f(x_1, ..., x_N) for which\nexact quantum algorithms have superlinear advantage over the deterministic\nalgorithms. Any deterministic algorithm that computes our function must use N\nqueries but an exact quantum algorithm can compute it with O(N^{0.8675...})\nqueries.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 21:20:38 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2012 17:44:26 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2012 11:19:47 GMT"}, {"version": "v4", "created": "Sun, 17 Mar 2013 13:54:32 GMT"}, {"version": "v5", "created": "Thu, 27 Feb 2014 21:54:28 GMT"}, {"version": "v6", "created": "Wed, 9 Jul 2014 20:02:34 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Ambainis", "Andris", ""]]}, {"id": "1211.1001", "submitter": "Anindya De", "authors": "Anindya De, Elchanan Mossel and Joe Neeman", "title": "Majority is Stablest : Discrete and SoS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Majority is Stablest Theorem has numerous applications in hardness of\napproximation and social choice theory. We give a new proof of the Majority is\nStablest Theorem by induction on the dimension of the discrete cube. Unlike the\nprevious proof, it uses neither the \"invariance principle\" nor Borell's result\nin Gaussian space. The new proof is general enough to include all previous\nvariants of majority is stablest such as \"it ain't over until it's over\" and\n\"Majority is most predictable\". Moreover, the new proof allows us to derive a\nproof of Majority is Stablest in a constant level of the Sum of Squares\nhierarchy.This implies in particular that Khot-Vishnoi instance of Max-Cut does\nnot provide a gap instance for the Lasserre hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 20:52:21 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 06:26:54 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["De", "Anindya", ""], ["Mossel", "Elchanan", ""], ["Neeman", "Joe", ""]]}, {"id": "1211.1041", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Ankur Moitra", "title": "Algorithms and Hardness for Robust Subspace Recovery", "comments": "Appeared in Proceedings of COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental problem in unsupervised learning called\n\\emph{subspace recovery}: given a collection of $m$ points in $\\mathbb{R}^n$,\nif many but not necessarily all of these points are contained in a\n$d$-dimensional subspace $T$ can we find it? The points contained in $T$ are\ncalled {\\em inliers} and the remaining points are {\\em outliers}. This problem\nhas received considerable attention in computer science and in statistics. Yet\nefficient algorithms from computer science are not robust to {\\em adversarial}\noutliers, and the estimators from robust statistics are hard to compute in high\ndimensions.\n  Are there algorithms for subspace recovery that are both robust to outliers\nand efficient? We give an algorithm that finds $T$ when it contains more than a\n$\\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimator\nis both easy to compute and well-behaved when there are a constant fraction of\noutliers. We prove that it is Small Set Expansion hard to find $T$ when the\nfraction of errors is any larger, thus giving evidence that our estimator is an\n{\\em optimal} compromise between efficiency and robustness.\n  As it turns out, this basic problem has a surprising number of connections to\nother areas including small set expansion, matroid theory and functional\nanalysis that we make use of here.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 21:39:22 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 14:32:57 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 21:51:26 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Hardt", "Moritz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1211.1109", "submitter": "Raghu Meka", "authors": "Daniel Kane, Raghu Meka", "title": "A PRG for Lipschitz Functions of Polynomials with Applications to\n  Sparsest Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give improved pseudorandom generators (PRGs) for Lipschitz functions of\nlow-degree polynomials over the hypercube. These are functions of the form\npsi(P(x)), where P is a low-degree polynomial and psi is a function with small\nLipschitz constant. PRGs for smooth functions of low-degree polynomials have\nreceived a lot of attention recently and play an important role in constructing\nPRGs for the natural class of polynomial threshold functions. In spite of the\nrecent progress, no nontrivial PRGs were known for fooling Lipschitz functions\nof degree O(log n) polynomials even for constant error rate. In this work, we\ngive the first such generator obtaining a seed-length of (log\nn)\\tilde{O}(d^2/eps^2) for fooling degree d polynomials with error eps.\nPrevious generators had an exponential dependence on the degree.\n  We use our PRG to get better integrality gap instances for sparsest cut, a\nfundamental problem in graph theory with many applications in graph\noptimization. We give an instance of uniform sparsest cut for which a powerful\nsemi-definite relaxation (SDP) first introduced by Goemans and Linial and\nstudied in the seminal work of Arora, Rao and Vazirani has an integrality gap\nof exp(\\Omega((log log n)^{1/2})). Understanding the performance of the\nGoemans-Linial SDP for uniform sparsest cut is an important open problem in\napproximation algorithms and metric embeddings and our work gives a\nnear-exponential improvement over previous lower bounds which achieved a gap of\n\\Omega(log log n).\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 04:51:27 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Kane", "Daniel", ""], ["Meka", "Raghu", ""]]}, {"id": "1211.1302", "submitter": "Hector Zenil", "authors": "Fernando Soler-Toscano, Hector Zenil, Jean-Paul Delahaye and Nicolas\n  Gauvrit", "title": "Calculating Kolmogorov Complexity from the Output Frequency\n  Distributions of Small Turing Machines", "comments": "26 pages, 9 figures, 8 tables. Additional material can be found at\n  the Algorithmic Nature Group website at http://www.algorithmicnature.org. An\n  Online Algorithmic Complexity Calculator implementing this technique and\n  making the data available to the research community is accessible at\n  http://www.complexitycalculator.com. Corresponding author: HZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing on various notions from theoretical computer science, we present a\nnovel numerical approach, motivated by the notion of algorithmic probability,\nto the problem of approximating the Kolmogorov-Chaitin complexity of short\nstrings. The method is an alternative to the traditional lossless compression\nalgorithms, which it may complement, the two being serviceable for different\nstring lengths. We provide a thorough analysis for all $\\sum_{n=1}^{11} 2^n$\nbinary strings of length $n<12$ and for most strings of length $12\\leq n\n\\leq16$ by running all $\\sim 2.5 \\times 10^{13}$ Turing machines with 5 states\nand 2 symbols ($8\\times 22^9$ with reduction techniques) using the most\nstandard formalism of Turing machines, used in for example the Busy Beaver\nproblem. We address the question of stability and error estimation, the\nsensitivity of the continued application of the method for wider coverage and\nbetter accuracy, and provide statistical evidence suggesting robustness. As\nwith compression algorithms, this work promises to deliver a range of\napplications, and to provide insight into the question of complexity\ncalculation of finite (and short) strings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 16:39:17 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 15:58:18 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Soler-Toscano", "Fernando", ""], ["Zenil", "Hector", ""], ["Delahaye", "Jean-Paul", ""], ["Gauvrit", "Nicolas", ""]]}, {"id": "1211.1490", "submitter": "Rodrigo Silveira", "authors": "Jos\\'e Miguel D\\'iaz-B\\'a\\~nez and Matias Korman and Pablo\n  P\\'erez-Lantero and Alexander Pilz and Carlos Seara and Rodrigo I. Silveira", "title": "New results on stabbing segments with a polygon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural variation of the concept of stabbing a segment by a\nsimple polygon: a segment is stabbed by a simple polygon $\\mathcal{P}$ if at\nleast one of its two endpoints is contained in $\\mathcal{P}$. A segment set $S$\nis stabbed by $\\mathcal{P}$ if every segment of $S$ is stabbed by\n$\\mathcal{P}$. We show that if $S$ is a set of pairwise disjoint segments, the\nproblem of computing the minimum perimeter polygon stabbing $S$ can be solved\nin polynomial time. We also prove that for general segments the problem is\nNP-hard. Further, an adaptation of our polynomial-time algorithm solves an open\nproblem posed by L\\\"offler and van Kreveld [Algorithmica 56(2), 236--269\n(2010)] about finding a maximum perimeter convex hull for a set of imprecise\npoints modeled as line segments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 09:22:51 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2012 08:53:10 GMT"}, {"version": "v3", "created": "Fri, 20 Jun 2014 15:12:38 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["D\u00edaz-B\u00e1\u00f1ez", "Jos\u00e9 Miguel", ""], ["Korman", "Matias", ""], ["P\u00e9rez-Lantero", "Pablo", ""], ["Pilz", "Alexander", ""], ["Seara", "Carlos", ""], ["Silveira", "Rodrigo I.", ""]]}, {"id": "1211.1505", "submitter": "Jesper Nederlof", "authors": "Hans L. Bodlaender, Marek Cygan, Stefan Kratsch and Jesper Nederlof", "title": "Solving weighted and counting variants of connectivity problems\n  parameterized by treewidth deterministically in single exponential time", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many local graph problems, like Vertex Cover and\nDominating Set, can be solved in 2^{O(tw)}|V|^{O(1)} time for graphs G=(V,E)\nwith a given tree decomposition of width tw. However, for nonlocal problems,\nlike the fundamental class of connectivity problems, for a long time we did not\nknow how to do this faster than tw^{O(tw)}|V|^{O(1)}. Recently, Cygan et al.\n(FOCS 2011) presented Monte Carlo algorithms for a wide range of connectivity\nproblems running in time $c^{tw}|V|^{O(1)} for a small constant c, e.g., for\nHamiltonian Cycle and Steiner tree. Naturally, this raises the question whether\nrandomization is necessary to achieve this runtime; furthermore, it is\ndesirable to also solve counting and weighted versions (the latter without\nincurring a pseudo-polynomial cost in terms of the weights).\n  We present two new approaches rooted in linear algebra, based on matrix rank\nand determinants, which provide deterministic c^{tw}|V|^{O(1)} time algorithms,\nalso for weighted and counting versions. For example, in this time we can solve\nthe traveling salesman problem or count the number of Hamiltonian cycles. The\nrank-based ideas provide a rather general approach for speeding up even\nstraightforward dynamic programming formulations by identifying \"small\" sets of\nrepresentative partial solutions; we focus on the case of expressing\nconnectivity via sets of partitions, but the essential ideas should have\nfurther applications. The determinant-based approach uses the matrix tree\ntheorem for deriving closed formulas for counting versions of connectivity\nproblems; we show how to evaluate those formulas via dynamic programming.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 10:38:52 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["Cygan", "Marek", ""], ["Kratsch", "Stefan", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1211.1506", "submitter": "Jesper Nederlof", "authors": "Marek Cygan, Stefan Kratsch and Jesper Nederlof", "title": "Fast Hamiltonicity checking via bases of perfect matchings", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an even integer t \\geq 2, the Matchings Connecivity matrix H_t is a\nmatrix that has rows and columns both labeled by all perfect matchings of the\ncomplete graph K_t on t vertices; an entry H_t[M_1,M_2] is 1 if M_1\\cup M_2 is\na Hamiltonian cycle and 0 otherwise. Motivated by the computational study of\nthe Hamiltonicity problem, we present three results on the structure of H_t: We\nfirst show that H_t has rank at most 2^{t/2-1} over GF(2) via an appropriate\nfactorization that explicitly provides families of matchings X_t forming bases\nfor H_t. Second, we show how to quickly change representation between such\nbases. Third, we notice that the sets of matchings X_t induce permutation\nmatrices within H_t.\n  Subsequently, we use the factorization to obtain an 1.888^n n^{O(1)} time\nMonte Carlo algorithm that solves the Hamiltonicity problem in directed\nbipartite graphs. Our algorithm as well counts the number of Hamiltonian cycles\nmodulo two in directed bipartite or undirected graphs in the same time bound.\nMoreover, we use the fast basis change algorithm from the second result to\npresent a Monte Carlo algorithm that given an undirected graph on n vertices\nalong with a path decomposition of width at most pw decides Hamiltonicity in\n(2+\\sqrt{2})^{pw}n^{O(1)} time. Finally, we use the third result to show that\nfor every \\epsilon >0 this cannot be improved to\n(2+\\sqrt{2}-\\epsilon)^{pw}n^{O(1)} time unless the Strong Exponential Time\nHypothesis fails, i.e., a faster algorithm for this problem would imply the\nbreakthrough result of a (2-\\epsilon)^n time algorithm for CNF-Sat.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 10:39:24 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Cygan", "Marek", ""], ["Kratsch", "Stefan", ""], ["Nederlof", "Jesper", ""]]}, {"id": "1211.1636", "submitter": "Andr\\'e Nichterlein", "authors": "Sepp Hartung and Andr\\'e Nichterlein", "title": "On the Parameterized and Approximation Hardness of Metric Dimension", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard Metric Dimension problem is to decide for a given graph G and a\npositive integer k whether there is a vertex subset of size at most k that\nseparates all vertex pairs in G. Herein, a vertex v separates a pair {u,w} if\nthe distance (length of a shortest path) between v and u is different from the\ndistance of v and w. We give a polynomial-time computable reduction from the\nBipartite Dominating Set problem to Metric Dimension on maximum degree three\ngraphs such that there is a one-to-one correspondence between the solution sets\nof both problems. There are two main consequences of this: First, it proves\nthat Metric Dimension on maximum degree three graphs is W[2]-complete with\nrespect to the parameter k. This answers an open question concerning the\nparameterized complexity of Metric Dimension posed by D\\'iaz et al. [ESA'12]\nand already mentioned by Lokshtanov [Dagstuhl seminar, 2009]. Additionally, it\nimplies that Metric Dimension cannot be solved in n^{o(k)} time, unless the\nassumption FPT \\neq W[1] fails. This proves that a trivial n^{O(k)} algorithm\nis probably asymptotically optimal.\n  Second, as Bipartite Dominating Set is inapproximable within o(log n), it\nfollows that Metric Dimension on maximum degree three graphs is also\ninapproximable by a factor of o(log n), unless NP=P. This strengthens the\nresult of Hauptmann et al. [JDA 2012] who proved APX-hardness on bounded-degree\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 18:50:44 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Hartung", "Sepp", ""], ["Nichterlein", "Andr\u00e9", ""]]}, {"id": "1211.1722", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "Inverse problems in approximate uniform generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of \\emph{inverse} problems in approximate uniform\ngeneration, focusing on uniform generation of satisfying assignments of various\ntypes of Boolean functions. In such an inverse problem, the algorithm is given\nuniform random satisfying assignments of an unknown function $f$ belonging to a\nclass $\\C$ of Boolean functions, and the goal is to output a probability\ndistribution $D$ which is $\\epsilon$-close, in total variation distance, to the\nuniform distribution over $f^{-1}(1)$.\n  Positive results: We prove a general positive result establishing sufficient\nconditions for efficient inverse approximate uniform generation for a class\n$\\C$. We define a new type of algorithm called a \\emph{densifier} for $\\C$, and\nshow (roughly speaking) how to combine (i) a densifier, (ii) an approximate\ncounting / uniform generation algorithm, and (iii) a Statistical Query learning\nalgorithm, to obtain an inverse approximate uniform generation algorithm. We\napply this general result to obtain a poly$(n,1/\\eps)$-time algorithm for the\nclass of halfspaces; and a quasipoly$(n,1/\\eps)$-time algorithm for the class\nof $\\poly(n)$-size DNF formulas.\n  Negative results: We prove a general negative result establishing that the\nexistence of certain types of signature schemes in cryptography implies the\nhardness of certain inverse approximate uniform generation problems. This\nimplies that there are no {subexponential}-time inverse approximate uniform\ngeneration algorithms for 3-CNF formulas; for intersections of two halfspaces;\nfor degree-2 polynomial threshold functions; and for monotone 2-CNF formulas.\n  Finally, we show that there is no general relationship between the complexity\nof the \"forward\" approximate uniform generation problem and the complexity of\nthe inverse problem for a class $\\C$ -- it is possible for either one to be\neasy while the other is hard.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 23:12:00 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1211.1878", "submitter": "Joost Joosten", "authors": "Joost J. Joosten", "title": "On the necessity of complexity", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wolfram's Principle of Computational Equivalence (PCE) implies that universal\ncomplexity abounds in nature. This paper comprises three sections. In the first\nsection we consider the question why there are so many universal phenomena\naround. So, in a sense, we week a driving force behind the PCE if any. We\npostulate a principle GNS that we call the Generalized Natural Selection\nPrinciple that together with the Church-Turing Thesis is seen to be equivalent\nto a weak version of PCE. In the second section we ask the question why we do\nnot observe any phenomena that are complex but not-universal. We choose a\ncognitive setting to embark on this question and make some analogies with\nformal logic. In the third and final section we report on a case study where we\nsee rich structures arise everywhere.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 10:43:32 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Joosten", "Joost J.", ""]]}, {"id": "1211.1958", "submitter": "Yuan Zhou", "authors": "Ryan O'Donnell, Yuan Zhou", "title": "Approximability and proof complexity", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with the proof-complexity of certifying that\noptimization problems do \\emph{not} have good solutions. Specifically we\nconsider bounded-degree \"Sum of Squares\" (SOS) proofs, a powerful algebraic\nproof system introduced in 1999 by Grigoriev and Vorobjov. Work of Shor,\nLasserre, and Parrilo shows that this proof system is automatizable using\nsemidefinite programming (SDP), meaning that any $n$-variable degree-$d$ proof\ncan be found in time $n^{O(d)}$. Furthermore, the SDP is dual to the well-known\nLasserre SDP hierarchy, meaning that the \"$d/2$-round Lasserre value\" of an\noptimization problem is equal to the best bound provable using a degree-$d$ SOS\nproof. These ideas were exploited in a recent paper by Barak et al.\\ (STOC\n2012) which shows that the known \"hard instances\" for the Unique-Games problem\nare in fact solved close to optimally by a constant level of the Lasserre SDP\nhierarchy.\n  We continue the study of the power of SOS proofs in the context of difficult\noptimization problems. In particular, we show that the Balanced-Separator\nintegrality gap instances proposed by Devanur et al.\\ can have their optimal\nvalue certified by a degree-4 SOS proof. The key ingredient is an SOS proof of\nthe KKL Theorem. We also investigate the extent to which the Khot--Vishnoi\nMax-Cut integrality gap instances can have their optimum value certified by an\nSOS proof. We show they can be certified to within a factor .952 ($> .878$)\nusing a constant-degree proof. These investigations also raise an interesting\nmathematical question: is there a constant-degree SOS proof of the Central\nLimit Theorem?\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 19:59:30 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["O'Donnell", "Ryan", ""], ["Zhou", "Yuan", ""]]}, {"id": "1211.2376", "submitter": "Piyush Srivastava", "authors": "Alistair Sinclair, Piyush Srivastava", "title": "Lee-Yang theorems and the complexity of computing averages", "comments": "30 pages, 3 figures. This version adds definitions of\n  complexity-theoretic terms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cond-mat.stat-mech cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of computing average quantities related to spin\nsystems, such as the mean magnetization and susceptibility in the ferromagnetic\nIsing model, and the average dimer count (or average size of a matching) in the\nmonomer-dimer model. By establishing connections between the complexity of\ncomputing these averages and the location of the complex zeros of the partition\nfunction, we show that these averages are #P-hard to compute. In case of the\nIsing model, our approach requires us to prove an extension of the famous\nLee-Yang Theorem from the 1950s.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 07:05:39 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 04:19:44 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Sinclair", "Alistair", ""], ["Srivastava", "Piyush", ""]]}, {"id": "1211.2483", "submitter": "Giovanni Viglietta", "authors": "Giovanni Viglietta", "title": "Guarding and Searching Polyhedra", "comments": "Ph.D. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the Art Gallery Problem and the Searchlight Scheduling Problem in\n3-dimensional polyhedral environments, putting special emphasis on edge guards\nand orthogonal polyhedra.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 00:02:29 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Viglietta", "Giovanni", ""]]}, {"id": "1211.2627", "submitter": "Martin Lackner", "authors": "G\\'abor Erd\\'elyi, Martin Lackner and Andreas Pfandler", "title": "Computational Aspects of Nearly Single-Peaked Electorates", "comments": "Published in the Journal of Artificial Intelligence Research (JAIR).\n  A short version of this paper appeared in the proceedings of the\n  Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2013). An\n  even earlier version appeared in the proceedings of the Fourth International\n  Workshop on Computational Social Choice 2012 (COMSOC 2012)", "journal-ref": "Journal of Artificial Intelligence Research (JAIR), 58: 297-337\n  (2017)", "doi": "10.1613/jair.5210", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation, bribery, and control are well-studied ways of changing the\noutcome of an election. Many voting rules are, in the general case,\ncomputationally resistant to some of these manipulative actions. However when\nrestricted to single-peaked electorates, these rules suddenly become easy to\nmanipulate. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra studied the\ncomputational complexity of strategic behavior in nearly single-peaked\nelectorates. These are electorates that are not single-peaked but close to it\naccording to some distance measure.\n  In this paper we introduce several new distance measures regarding\nsingle-peakedness. We prove that determining whether a given profile is nearly\nsingle-peaked is NP-complete in many cases. For one case we present a\npolynomial-time algorithm. In case the single-peaked axis is given, we show\nthat determining the distance is always possible in polynomial time.\nFurthermore, we explore the relations between the new notions introduced in\nthis paper and existing notions from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 14:24:55 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2013 23:03:52 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 13:46:47 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 10:18:37 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Erd\u00e9lyi", "G\u00e1bor", ""], ["Lackner", "Martin", ""], ["Pfandler", "Andreas", ""]]}, {"id": "1211.2664", "submitter": "Cl\\'ement Canonne", "authors": "Clement Canonne, Dana Ron, Rocco A. Servedio", "title": "Testing probability distributions using conditional samples", "comments": "Significant changes on Section 9 (detailing and expanding the proof\n  of Theorem 16). Several clarifications and typos fixed in various places", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new framework for property testing of probability distributions,\nby considering distribution testing algorithms that have access to a\nconditional sampling oracle.* This is an oracle that takes as input a subset $S\n\\subseteq [N]$ of the domain $[N]$ of the unknown probability distribution $D$\nand returns a draw from the conditional probability distribution $D$ restricted\nto $S$. This new model allows considerable flexibility in the design of\ndistribution testing algorithms; in particular, testing algorithms in this\nmodel can be adaptive.\n  We study a wide range of natural distribution testing problems in this new\nframework and some of its variants, giving both upper and lower bounds on query\ncomplexity. These problems include testing whether $D$ is the uniform\ndistribution $\\mathcal{U}$; testing whether $D = D^\\ast$ for an explicitly\nprovided $D^\\ast$; testing whether two unknown distributions $D_1$ and $D_2$\nare equivalent; and estimating the variation distance between $D$ and the\nuniform distribution. At a high level our main finding is that the new\n\"conditional sampling\" framework we consider is a powerful one: while all the\nproblems mentioned above have $\\Omega(\\sqrt{N})$ sample complexity in the\nstandard model (and in some cases the complexity must be almost linear in $N$),\nwe give $\\mathrm{poly}(\\log N, 1/\\varepsilon)$-query algorithms (and in some\ncases $\\mathrm{poly}(1/\\varepsilon)$-query algorithms independent of $N$) for\nall these problems in our conditional sampling setting.\n  *Independently from our work, Chakraborty et al. also considered this\nframework. We discuss their work in Subsection [1.4].\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 15:39:28 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 18:23:16 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Canonne", "Clement", ""], ["Ron", "Dana", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1211.3439", "submitter": "Devendra Desai", "authors": "Aditya Bhaskara, Devendra Desai, Srikanth Srinivasan", "title": "Optimal Hitting Sets for Combinatorial Shapes", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing explicit Hitting sets for\nCombinatorial Shapes, a class of statistical tests first studied by Gopalan,\nMeka, Reingold, and Zuckerman (STOC 2011). These generalize many well-studied\nclasses of tests, including symmetric functions and combinatorial rectangles.\nGeneralizing results of Linial, Luby, Saks, and Zuckerman (Combinatorica 1997)\nand Rabani and Shpilka (SICOMP 2010), we construct hitting sets for\nCombinatorial Shapes of size polynomial in the alphabet, dimension, and the\ninverse of the error parameter. This is optimal up to polynomial factors. The\nbest previous hitting sets came from the Pseudorandom Generator construction of\nGopalan et al., and in particular had size that was quasipolynomial in the\ninverse of the error parameter.\n  Our construction builds on natural variants of the constructions of Linial et\nal. and Rabani and Shpilka. In the process, we construct fractional perfect\nhash families and hitting sets for combinatorial rectangles with stronger\nguarantees. These might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 21:45:31 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Desai", "Devendra", ""], ["Srinivasan", "Srikanth", ""]]}, {"id": "1211.3492", "submitter": "Natalia L. Malinina", "authors": "Natalia L. Malinina", "title": "On the principal impossibility to prove P=NP", "comments": "20 pages, 21 figures. arXiv admin note: substantial text overlap with\n  arXiv:1210.6088, arXiv:1007.1059", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The material of the article is devoted to the most complicated and\ninteresting problem -- a problem of P = NP?. This research was presented to\nmathematical community in Hyderabad during International Congress of\nMathematicians. But there it was published in a very brief form, so this\narticle is an attempt to give those, who are interested in the problem, my\nreasoning on the theme. It is not a proof in full, because it is very difficult\nto prove something, which is not provable, but it seems that these reasoning\nwill help us to understand the problem of the combinatorial explosion more\ndeeply and to realize in full all the problems to which we are going because of\nthe combinatorial explosion. Maybe we will realize that the combinatorial\nexplosion is somehow a law, such a law, which influences the World, as Newton's\nlaw of gravitation influences the fall of each thing.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 05:11:56 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Malinina", "Natalia L.", ""]]}, {"id": "1211.4891", "submitter": "Hector Zenil", "authors": "Fernando Soler-Toscano, Hector Zenil, Jean-Paul Delahaye and Nicolas\n  Gauvrit", "title": "Correspondence and Independence of Numerical Evaluations of Algorithmic\n  Information Measures", "comments": "22 pages, 8 images. This article draws heavily from arXiv:1211.1302", "journal-ref": null, "doi": "10.3233/COM-13019", "report-no": null, "categories": "cs.IT cs.CC cs.FL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that real-value approximations of Kolmogorov-Chaitin (K_m) using the\nalgorithmic Coding theorem as calculated from the output frequency of a large\nset of small deterministic Turing machines with up to 5 states (and 2 symbols),\nis in agreement with the number of instructions used by the Turing machines\nproducing s, which is consistent with strict integer-value program-size\ncomplexity. Nevertheless, K_m proves to be a finer-grained measure and a\npotential alternative approach to lossless compression algorithms for small\nentities, where compression fails. We also show that neither K_m nor the number\nof instructions used shows any correlation with Bennett's Logical Depth LD(s)\nother than what's predicted by the theory. The agreement between theory and\nnumerical calculations shows that despite the undecidability of these\ntheoretical measures, approximations are stable and meaningful, even for small\nprograms and for short strings. We also announce a first Beta version of an\nOnline Algorithmic Complexity Calculator (OACC), based on a combination of\ntheoretical concepts, as a numerical implementation of the Coding Theorem\nMethod.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 22:05:06 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 10:27:08 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2013 18:59:19 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Soler-Toscano", "Fernando", ""], ["Zenil", "Hector", ""], ["Delahaye", "Jean-Paul", ""], ["Gauvrit", "Nicolas", ""]]}, {"id": "1211.4918", "submitter": "Xi Chen", "authors": "Xi Chen and Dimitris Paparas and Mihalis Yannakakis", "title": "The Complexity of Non-Monotone Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of non-monotone utilities, which covers a wide\nvariety of utility functions in economic theory. We then prove that it is\nPPAD-hard to compute an approximate Arrow-Debreu market equilibrium in markets\nwith linear and non-monotone utilities. Building on this result, we settle the\nlong-standing open problem regarding the computation of an approximate\nArrow-Debreu market equilibrium in markets with CES utility functions, by\nproving that it is PPAD-complete when the Constant Elasticity of Substitution\nparameter \\rho is any constant less than -1.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 02:41:14 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Chen", "Xi", ""], ["Paparas", "Dimitris", ""], ["Yannakakis", "Mihalis", ""]]}, {"id": "1211.4949", "submitter": "Shinnosuke Seki", "authors": "Shinnosuke Seki and Yasushi Okuno", "title": "On the behavior of tile assembly system at high temperatures", "comments": "This paper is an extended version of the following paper: S. Seki and\n  Y. Okuno. On the behavior of tile assembly system at high temperatures. In\n  CiE 2012: How the World Computes - Turing Centenary Conference and 8th\n  Conference on Computability in Europe, LNCS 7318, pages 549-559, Springer,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviors of Winfree's tile assembly systems (TASs) at high temperatures are\ninvestigated in combination with integer programming of a specific form called\nthreshold programming. First, we propose a way to build bridges from the\nBoolean satisfiability problem (SAT) to threshold programming, and further to\nTAS's behavior, in order to prove the NP-hardness of optimizing temperatures of\nTASs that behave in a way given as input. These bridges will take us further to\ntwo important results on the behavior of TASs at high temperatures. The first\nsays that arbitrarily high temperatures are required to assemble some shape by\na TAS of \"reasonable\" size. The second is that for any temperature at least 4\ngiven as a parameter, it is NP-hard to find the minimum size TAS that\nself-assembles a given shape and works at the given temperature or below.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 07:28:42 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Seki", "Shinnosuke", ""], ["Okuno", "Yasushi", ""]]}, {"id": "1211.4974", "submitter": "Martin Ziegler", "authors": "Akitoshi Kawamura and Norbert Th. M\\\"uller and Carsten R\\\"osnick and\n  Martin Ziegler", "title": "Parameterized Uniform Complexity in Numerics: from Smooth to Analytic,\n  from NP-hard to Polytime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthesis of classical Computational Complexity Theory with Recursive\nAnalysis provides a quantitative foundation to reliable numerics. Here the\noperators of maximization, integration, and solving ordinary differential\nequations are known to map (even high-order differentiable) polynomial-time\ncomputable functions to instances which are `hard' for classical complexity\nclasses NP, #P, and CH; but, restricted to analytic functions, map\npolynomial-time computable ones to polynomial-time computable ones --\nnon-uniformly!\n  We investigate the uniform parameterized complexity of the above operators in\nthe setting of Weihrauch's TTE and its second-order extension due to\nKawamura&Cook (2010). That is, we explore which (both continuous and discrete,\nfirst and second order) information and parameters on some given f is\nsufficient to obtain similar data on Max(f) and int(f); and within what running\ntime, in terms of these parameters and the guaranteed output precision 2^(-n).\n  It turns out that Gevrey's hierarchy of functions climbing from analytic to\nsmooth corresponds to the computational complexity of maximization growing from\npolytime to NP-hard. Proof techniques involve mainly the Theory of (discrete)\nComputation, Hard Analysis, and Information-Based Complexity.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 09:23:14 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Kawamura", "Akitoshi", ""], ["M\u00fcller", "Norbert Th.", ""], ["R\u00f6snick", "Carsten", ""], ["Ziegler", "Martin", ""]]}, {"id": "1211.5544", "submitter": "Holger Petersen", "authors": "Holger Petersen", "title": "A Note on Kolmogorov-Uspensky Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving an open problem stated by Shvachko, it is shown that a language which\nis not real-time recognizable by some variants of pointer machines can be\naccepted by a Kolmogorov-Uspensky machine in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 16:37:37 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Petersen", "Holger", ""]]}, {"id": "1211.5718", "submitter": "Elad Haramaty", "authors": "Elad Haramaty and Madhu Sudan", "title": "Deterministic Compression with Uncertain Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of compression of information when the source of the\ninformation and the destination do not agree on the prior, i.e., the\ndistribution from which the information is being generated. This setting was\nconsidered previously by Kalai et al. (ICS 2011) who suggested that this was a\nnatural model for human communication, and efficient schemes for compression\nhere could give insights into the behavior of natural languages. Kalai et al.\ngave a compression scheme with nearly optimal performance, assuming the source\nand destination share some uniform randomness. In this work we explore the need\nfor this randomness, and give some non-trivial upper bounds on the\ndeterministic communication complexity for this problem. In the process we\nintroduce a new family of structured graphs of constant fractional chromatic\nnumber whose (integral) chromatic number turns out to be a key component in the\nanalysis of the communication complexity. We provide some non-trivial upper\nbounds on the chromatic number of these graphs to get our upper bound, while\nusing lower bounds on variants of these graphs to prove lower bounds for some\nnatural approaches to solve the communication complexity question. Tight\nanalysis of communication complexity of our problems and the chromatic number\nof the underlying graphs remains open.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 02:28:22 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Haramaty", "Elad", ""], ["Sudan", "Madhu", ""]]}, {"id": "1211.5729", "submitter": "Xiaojun Zhu", "authors": "Xiaojun Zhu, Qun Li, Weizhen Mao, Guihai Chen", "title": "Online Vector Scheduling and Generalized Load Balancing", "comments": "This work has been accepted to JPDC. Please refer to\n  http://dx.doi.org/10.1016/j.jpdc.2013.12.006", "journal-ref": null, "doi": "10.1016/j.jpdc.2013.12.006", "report-no": "WM-CS-2012-01", "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time reduction from vector scheduling problem (VS) to\ngeneralized load balancing problem (GLB). This reduction gives the first\nnon-trivial online algorithm for VS where vectors come in an online fashion.\nThe online algorithm is very simple in that each vector only needs to minimize\nthe $L_{\\ln(md)}$ norm of the resulting load when it comes, where $m$ is the\nnumber of partitions and $d$ is the dimension of vectors. It has an\napproximation bound of $e\\log(md)$, which is in $O(\\ln(md))$, so it also\nimproves the $O(\\ln^2d)$ bound of the existing polynomial time algorithm for\nVS. Additionally, the reduction shows that GLB does not have constant\napproximation algorithms that run in polynomial time unless $P=NP$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 05:20:29 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 08:53:59 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Zhu", "Xiaojun", ""], ["Li", "Qun", ""], ["Mao", "Weizhen", ""], ["Chen", "Guihai", ""]]}, {"id": "1211.5773", "submitter": "Koji Kobayashi", "authors": "Koji Kobayashi", "title": "Circuit complexity and Problem structure in Hamming space", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes about relation between circuit complexity and accept\ninputs structure in Hamming space by using almost all monotone circuit that\nemulate deterministic Turing machine (DTM).\n  Circuit family that emulate DTM are almost all monotone circuit family except\nsome NOT-gate which connect input variables (like negation normal form (NNF)).\nTherefore, we can analyze DTM limitation by using this NNF Circuit family.\n  NNF circuit have symmetry of OR-gate input line, so NNF circuit cannot\nidentify from OR-gate output line which of OR-gate input line is 1. So NNF\ncircuit family cannot compute sandwich structure effectively (Sandwich\nstructure is two accept inputs that sandwich reject inputs in Hamming space).\nNNF circuit have to use unique AND-gate to identify each different vector of\nsandwich structure. That is, we can measure problem complexity by counting\ndifferent vectors.\n  Some decision problem have characteristic in sandwich structure. Different\nvectors of Negate HornSAT problem are at most constant length because we can\ndelete constant part of each negative literal in Horn clauses by using definite\nclauses. Therefore, number of these different vector is at most polynomial\nsize. The other hand, we can design high complexity problem with almost perfct\nnonlinear (APN) function.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 16:22:03 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 00:48:27 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 12:22:44 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 01:41:20 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Kobayashi", "Koji", ""]]}, {"id": "1211.6120", "submitter": "Mark Wilde", "authors": "Patrick Hayden, Kevin Milner, and Mark M. Wilde", "title": "Two-message quantum interactive proofs and the quantum separability\n  problem", "comments": "34 pages, 6 figures; v2: technical improvements and new result for\n  the multipartite quantum separability problem; v3: minor changes to address\n  referee comments, accepted for presentation at the 2013 IEEE Conference on\n  Computational Complexity; v4: changed problem names; v5: updated references\n  and added a paragraph to the conclusion to connect with prior work on\n  separability testing", "journal-ref": "Proceedings of the 28th IEEE Conference on Computational\n  Complexity, pages 156-167, Palo Alto, California, June 2013", "doi": "10.1109/CCC.2013.24", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a polynomial-time mixed-state quantum circuit, described as a\nsequence of local unitary interactions followed by a partial trace, generates a\nquantum state shared between two parties. One might then wonder, does this\nquantum circuit produce a state that is separable or entangled? Here, we give\nevidence that it is computationally hard to decide the answer to this question,\neven if one has access to the power of quantum computation. We begin by\nexhibiting a two-message quantum interactive proof system that can decide the\nanswer to a promise version of the question. We then prove that the promise\nproblem is hard for the class of promise problems with \"quantum statistical\nzero knowledge\" (QSZK) proof systems by demonstrating a polynomial-time Karp\nreduction from the QSZK-complete promise problem \"quantum state\ndistinguishability\" to our quantum separability problem. By exploiting Knill's\nefficient encoding of a matrix description of a state into a description of a\ncircuit to generate the state, we can show that our promise problem is NP-hard\nwith respect to Cook reductions. Thus, the quantum separability problem (as\nphrased above) constitutes the first nontrivial promise problem decidable by a\ntwo-message quantum interactive proof system while being hard for both NP and\nQSZK. We also consider a variant of the problem, in which a given\npolynomial-time mixed-state quantum circuit accepts a quantum state as input,\nand the question is to decide if there is an input to this circuit which makes\nits output separable across some bipartite cut. We prove that this problem is a\ncomplete promise problem for the class QIP of problems decidable by quantum\ninteractive proof systems. Finally, we show that a two-message quantum\ninteractive proof system can also decide a multipartite generalization of the\nquantum separability problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 21:00:29 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 18:38:06 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2013 18:04:43 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2013 07:57:46 GMT"}, {"version": "v5", "created": "Fri, 6 Sep 2013 23:42:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hayden", "Patrick", ""], ["Milner", "Kevin", ""], ["Wilde", "Mark M.", ""]]}, {"id": "1211.6320", "submitter": "Massarenti Alex", "authors": "Alex Massarenti, Emanuele Raviolo", "title": "On the rank of $n\\times n$ matrix multiplication", "comments": "10 pages. New version, title and main result changed. Linear Algebra\n  and its Applications 2013", "journal-ref": null, "doi": "10.1016/j.laa.2013.01.031", "report-no": null, "categories": "cs.CC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every $p\\leq n$ positive integer we obtain the lower bound\n$(3-\\frac{1}{p+1})n^2-\\big(2\\binom{2p}{p+1}-\\binom{2p-2}{p-1}+2\\big)n$ for the\nrank of the $n\\times n$ matrix multiplication. This bound improves the previous\none $(3-\\frac{1}{p+1})n^2-\\big(1+2p\\binom{2p}{p}\\big)n$ due to Landsberg.\nFurthermore our bound improves the classic bound $\\frac{5}{2}n^2-3n$, due to\nBl\\\"aser, for every $n\\geq 132$. Finally, for $p = 2$, with a sligtly different\nstrategy we menage to obtain the lower bound $\\frac{8}{3}n^2-7n$ which improves\nBl\\\"aser's bound for any $n\\geq 24$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 14:58:37 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 19:02:06 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Massarenti", "Alex", ""], ["Raviolo", "Emanuele", ""]]}, {"id": "1211.6656", "submitter": "Bruno Escoffier", "authors": "Bruno Escoffier, EunJung Kim and Vangelis Th. Paschos", "title": "Subexponential and FPT-time Inapproximability of Independent Set and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-parameter algorithms, approximation algorithms and moderately\nexponential algorithms are three major approaches to algorithms design. While\neach of them being very active in its own, there is an increasing attention to\nthe connection between different approaches. In particular, whether Maximum\nIndependent Set would be better approximable once endowed with\nsubexponential-time or FPT-time is a central question. In this paper, we\npresent a strong link between the linear PCP conjecture and the\ninapproximability, thus partially answering this question.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 16:56:19 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Escoffier", "Bruno", ""], ["Kim", "EunJung", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1211.6724", "submitter": "Bhaskar DasGupta", "authors": "Bhaskar DasGupta and Lakshmi Kaligounder", "title": "On Approximating Graph Bipartization via Node Deletion", "comments": "Although the results are correct, it was pointed out that the results\n  follow from some previously known results. Accordingly, this version of the\n  paper is withdrawn by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the results are correct, it was pointed out that the results follow\nfrom some previously known results. Accordingly, this version of the paper is\nwithdrawn by the authors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 20:05:10 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 22:10:19 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["DasGupta", "Bhaskar", ""], ["Kaligounder", "Lakshmi", ""]]}, {"id": "1211.6997", "submitter": "Cristopher Moore", "authors": "Varsha Dani, Josep Diaz, Thomas Hayes, and Cristopher Moore", "title": "The Power of Choice for Random Satisfiability", "comments": "typo fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Achlioptas processes for k-SAT formulas. We create a semi-random\nformula with n variables and m clauses, where each clause is a choice, made\non-line, between two or more uniformly random clauses. Our goal is to delay the\nsatisfiability/unsatisfiability transition, keeping the formula satisfiable up\nto densities m/n beyond the satisfiability threshold alpha_k for random k-SAT.\nWe show that three choices suffice to raise the threshold for any k >= 3, and\nthat two choices suffice for all 3 <= k <= 25. We also show that two choices\nsuffice to lower the threshold for all k >= 3, making the formula unsatisfiable\nat a density below alpha_k.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 17:30:38 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2012 16:33:17 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Dani", "Varsha", ""], ["Diaz", "Josep", ""], ["Hayes", "Thomas", ""], ["Moore", "Cristopher", ""]]}, {"id": "1211.7138", "submitter": "Steven Heilman", "authors": "Steven Heilman", "title": "Euclidean Partitions Optimizing Noise Stability", "comments": "40 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.FA math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Standard Simplex Conjecture of Isaksson and Mossel asks for the partition\n$\\{A_{i}\\}_{i=1}^{k}$ of $\\mathbb{R}^{n}$ into $k\\leq n+1$ pieces of equal\nGaussian measure of optimal noise stability. That is, for $\\rho>0$, we maximize\n$$\n\\sum_{i=1}^{k}\\int_{\\mathbb{R}^{n}}\\int_{\\mathbb{R}^{n}}1_{A_{i}}(x)1_{A_{i}}(x\\rho+y\\sqrt{1-\\rho^{2}})\ne^{-(x_{1}^{2}+\\cdots+x_{n}^{2})/2}e^{-(y_{1}^{2}+\\cdots+y_{n}^{2})/2}dxdy. $$\nIsaksson and Mossel guessed the best partition for this problem and proved some\napplications of their conjecture. For example, the Standard Simplex Conjecture\nimplies the Plurality is Stablest Conjecture. For $k=3,n\\geq2$ and\n$0<\\rho<\\rho_{0}(k,n)$, we prove the Standard Simplex Conjecture. The full\nconjecture has applications to theoretical computer science, and to geometric\nmulti-bubble problems (after Isaksson and Mossel).\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 02:18:43 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 18:10:02 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Heilman", "Steven", ""]]}, {"id": "1211.7161", "submitter": "Michael Soltys", "authors": "Sam Buss, Michael Soltys", "title": "Unshuffling a Square is NP-Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A shuffle of two strings is formed by interleaving the characters into a new\nstring, keeping the characters of each string in order. A string is a square if\nit is a shuffle of two identical strings. There is a known polynomial time\ndynamic programming algorithm to determine if a given string z is the shuffle\nof two given strings x,y; however, it has been an open question whether there\nis a polynomial time algorithm to determine if a given string z is a square. We\nresolve this by proving that this problem is NP-complete via a many-one\nreduction from 3- Partition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 06:14:06 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Buss", "Sam", ""], ["Soltys", "Michael", ""]]}, {"id": "1211.7346", "submitter": "H. G\\\"okalp Demirci", "authors": "H. G\\\"okalp Demirci and A. C. Cem Say", "title": "Checking generalized debates with small space and randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model of probabilistic debate checking, where a silent\nresource-bounded verifier reads a dialogue about the membership of the string\nin the language under consideration between a prover and a refuter. Our model\ncombines and generalizes the concepts of one-way interactive proof systems,\ngames of incomplete information, and probabilistically checkable\ncomplete-information debate systems. We consider debates of partial and zero\ninformation, where the prover is prevented from seeing some or all of the\nmessages of the refuter, as well as those of complete information. The classes\nof languages with debates checkable by verifiers operating under severe bounds\non the memory and randomness are studied.\n  We give full characterizations of versions of these classes corresponding to\nsimultaneous bounds of O(1) space and O(1) random bits, and of logarithmic\nspace and polynomial time. It turns out that constant-space verifiers, which\ncan only check complete-information debates for regular languages\ndeterministically, can check for membership in any language in P when allowed\nto use a constant number of random bits. Similar increases also occur for zero-\nand partial- information debates, from NSPACE(n) to PSPACE, and from E to\nEXPTIME, respectively. Adding logarithmic space to these constant-randomness\nverifiers does not change their power. When logspace debate checkers are\nrestricted to run in polynomial time without a bound on the number of random\nbits, the class of debatable languages equals PSPACE for all debate types. We\nalso present a result on the hardness of approximating the quantified max word\nproblem for matrices that is a corollary of this characterization.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 19:25:03 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Demirci", "H. G\u00f6kalp", ""], ["Say", "A. C. Cem", ""]]}]