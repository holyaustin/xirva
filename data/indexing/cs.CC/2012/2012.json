[{"id": "2012.00079", "submitter": "Micha{\\l} Pilipczuk", "authors": "Eduard Eiben and Robert Ganian and Du\\v{s}an Knop and Sebastian\n  Ordyniak and Micha{\\l} Pilipczuk and Marcin Wrochna", "title": "Integer Programming and Incidence Treedepth", "comments": "11 pages, 1 figure. This is an extended version of an article that\n  appeared at IPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a strong connection has been shown between the tractability of\ninteger programming (IP) with bounded coefficients on the one side and the\nstructure of its constraint matrix on the other side. To that end, integer\nlinear programming is fixed-parameter tractable with respect to the primal (or\ndual) treedepth of the Gaifman graph of its constraint matrix and the largest\ncoefficient (in absolute value). Motivated by this, Kouteck\\'y, Levin, and Onn\n[ICALP 2018] asked whether it is possible to extend these result to a more\nbroader class of integer linear programs. More formally, is integer linear\nprogramming fixed-parameter tractable with respect to the incidence treedepth\nof its constraint matrix and the largest coefficient (in absolute value)?\n  We answer this question in negative. In particular, we prove that deciding\nthe feasibility of a system in the standard form, ${A\\mathbf{x} = \\mathbf{b}},\n{\\mathbf{l} \\le \\mathbf{x} \\le \\mathbf{u}}$, is $\\mathsf{NP}$-hard even when\nthe absolute value of any coefficient in $A$ is 1 and the incidence treedepth\nof $A$ is 5. Consequently, it is not possible to decide feasibility in\npolynomial time even if both the assumed parameters are constant, unless\n$\\mathsf{P}=\\mathsf{NP}$. Moreover, we complement this intractability result by\nshowing tractability for natural and only slightly more restrictive settings,\nnamely: (1) treedepth with an additional bound on either the maximum arity of\nconstraints or the maximum number of occurrences of variables and (2) the\nvertex cover number.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:03:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Knop", "Du\u0161an", ""], ["Ordyniak", "Sebastian", ""], ["Pilipczuk", "Micha\u0142", ""], ["Wrochna", "Marcin", ""]]}, {"id": "2012.00193", "submitter": "Muhammad Usman", "authors": "Muhammad Usman", "title": "Lightweight Encryption for the Low Powered IoT Devices", "comments": "This is a short survey of lightweight encryption algorithms used in\n  IoT, submitted as an assignment for the graduate course titled \"Internet of\n  Things\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The internet of things refers to the network of devices connected to the\ninternet and can communicate with each other. The term things is to refer\nnon-conventional devices that are usually not connected to the internet. The\nnetwork of such devices or things is growing at an enormous rate. The security\nand privacy of the data flowing through these things is a major concern. The\ndevices are low powered and the conventional encryption algorithms are not\nsuitable to be employed on these devices. In this correspondence a survey of\nthe contemporary lightweight encryption algorithms suitable for use in the IoT\nenvironment has been presented.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:59:33 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:15:27 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Usman", "Muhammad", ""]]}, {"id": "2012.00330", "submitter": "Abhijit S. Mudigonda", "authors": "Abhijit S. Mudigonda, R. Ryan Williams", "title": "Time-Space Lower Bounds for Simulating Proof Systems with Quantum and\n  Randomized Verifiers", "comments": "38 pages, 5 figures. To appear in ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A line of work initiated by Fortnow in 1997 has proven model-independent\ntime-space lower bounds for the $\\mathsf{SAT}$ problem and related problems\nwithin the polynomial-time hierarchy. For example, for the $\\mathsf{SAT}$\nproblem, the state-of-the-art is that the problem cannot be solved by\nrandom-access machines in $n^c$ time and $n^{o(1)}$ space simultaneously for $c\n< 2\\cos(\\frac{\\pi}{7}) \\approx 1.801$.\n  We extend this lower bound approach to the quantum and randomized domains.\nCombining Grover's algorithm with components from $\\mathsf{SAT}$ time-space\nlower bounds, we show that there are problems verifiable in $O(n)$ time with\nquantum Merlin-Arthur protocols that cannot be solved in $n^c$ time and\n$n^{o(1)}$ space simultaneously for $c < \\frac{3+\\sqrt{3}}{2} \\approx 2.366$, a\nsuper-quadratic time lower bound. This result and the prior work on\n$\\mathsf{SAT}$ can both be viewed as consequences of a more general formula for\ntime lower bounds against small-space algorithms, whose asymptotics we study in\nfull.\n  We also show lower bounds against randomized algorithms: there are problems\nverifiable in $O(n)$ time with (classical) Merlin-Arthur protocols that cannot\nbe solved in $n^c$ randomized time and $n^{o(1)}$ space simultaneously for $c <\n1.465$, improving a result of Diehl. For quantum Merlin-Arthur protocols, the\nlower bound in this setting can be improved to $c < 1.5$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:25:55 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 21:49:55 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mudigonda", "Abhijit S.", ""], ["Williams", "R. Ryan", ""]]}, {"id": "2012.01085", "submitter": "Youming Qiao", "authors": "Joshua A. Grochow, Youming Qiao, Gang Tang", "title": "Average-case algorithms for testing isomorphism of polynomials,\n  algebras, and multilinear forms", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problems of testing isomorphism of polynomials, algebras, and\nmultilinear forms. Our first main results are average-case algorithms for these\nproblems. For example, we develop an algorithm that takes two cubic forms $f,\ng\\in \\mathbb{F}_q[x_1,\\dots, x_n]$, and decides whether $f$ and $g$ are\nisomorphic in time $q^{O(n)}$ for most $f$. This average-case setting has\ndirect practical implications, having been studied in multivariate cryptography\nsince the 1990s. Our second result concerns the complexity of testing\nequivalence of alternating trilinear forms. This problem is of interest in both\nmathematics and cryptography. We show that this problem is polynomial-time\nequivalent to testing equivalence of symmetric trilinear forms, by showing that\nthey are both Tensor Isomorphism-complete (Grochow-Qiao, ITCS, 2021), therefore\nis equivalent to testing isomorphism of cubic forms over most fields.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:20:06 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Qiao", "Youming", ""], ["Tang", "Gang", ""]]}, {"id": "2012.01199", "submitter": "Johannes Greiner", "authors": "Manuel Bodirsky, Johannes Greiner", "title": "Tractable Combinations of Theories via Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a first-order theory $T$, the Constraint Satisfaction Problem of $T$ is\nthe computational problem of deciding whether a given conjunction of atomic\nformulas is satisfiable in some model of $T$. In this article we develop\nsufficient conditions for polynomial-time tractability of the constraint\nsatisfaction problem for the union of two theories with disjoint relational\nsignatures. To this end, we introduce the concept of sampling for theories and\nshow that samplings can be applied to examples which are not covered by the\nseminal result of Nelson and Oppen.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:33:34 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Greiner", "Johannes", ""]]}, {"id": "2012.01226", "submitter": "Hans Bodlaender", "authors": "Hans L. Bodlaender", "title": "Parameterized complexity of Bandwidth of Caterpillars and Weighted Path\n  Emulation", "comments": "31 pages; 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we show that Bandwidth is hard for the complexity class $W[t]$\nfor all $t\\in {\\bf N}$, even for caterpillars with hair length at most three.\nAs intermediate problem, we introduce the Weighted Path Emulation problem:\ngiven a vertex-weighted path $P_N$ and integer $M$, decide if there exists a\nmapping of the vertices of $P_N$ to a path $P_M$, such that adjacent vertices\nare mapped to adjacent or equal vertices, and such that the total weight of the\nimage of a vertex from $P_M$ equals an integer $c$. We show that {\\sc Weighted\nPath Emulation}, with $c$ as parameter, is hard for $W[t]$ for all $t\\in {\\bf\nN}$, and is strongly NP-complete. We also show that Directed Bandwidth is hard\nfor $W[t]$ for all $t\\in {\\bf N}$, for directed acyclic graphs whose underlying\nundirected graph is a caterpillar.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:14:33 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Bodlaender", "Hans L.", ""]]}, {"id": "2012.01420", "submitter": "Ananth Goyal", "authors": "Ananth Goyal", "title": "Constructing Segmented Differentiable Quadratics to Determine\n  Algorithmic Run Times and Model Non-Polynomial Functions", "comments": "Regeneron Science Talent Search (STS) Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.FA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an approach to determine the continual progression of algorithmic\nefficiency, as an alternative to standard calculations of time complexity,\nlikely, but not exclusively, when dealing with data structures with unknown\nmaximum indexes and with algorithms that are dependent on multiple variables\napart from just input size. The proposed method can effectively determine the\nrun time behavior $F$ at any given index $x$ , as well as $\\frac{\\partial\nF}{\\partial x}$, as a function of only one or multiple arguments, by combining\n$\\frac{n}{2}$ quadratic segments, based upon the principles of Lagrangian\nPolynomials and their respective secant lines. Although the approach used is\ndesigned for analyzing the efficacy of computational algorithms, the proposed\nmethod can be used within the pure mathematical field as a novel way to\nconstruct non-polynomial functions, such as $\\log_2{n}$ or $\\frac{n+1}{n-2}$,\nas a series of segmented differentiable quadratics to model functional behavior\nand reoccurring natural patterns. After testing, our method had an average\naccuracy of above of 99\\% with regard to functional resemblance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:22:49 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Goyal", "Ananth", ""]]}, {"id": "2012.01530", "submitter": "Siddharth Bhandari", "authors": "Siddharth Bhandari, Prahladh Harsha, Mrinal Kumar and Madhu Sudan", "title": "Decoding Multivariate Multiplicity Codes on Product Sets", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DM math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The multiplicity Schwartz-Zippel lemma bounds the total multiplicity of\nzeroes of a multivariate polynomial on a product set. This lemma motivates the\nmultiplicity codes of Kopparty, Saraf and Yekhanin [J. ACM, 2014], who showed\nhow to use this lemma to construct high-rate locally-decodable codes. However,\nthe algorithmic results about these codes crucially rely on the fact that the\npolynomials are evaluated on a vector space and not an arbitrary product set.\n  In this work, we show how to decode multivariate multiplicity codes of large\nmultiplicities in polynomial time over finite product sets (over fields of\nlarge characteristic and zero characteristic). Previously such decoding\nalgorithms were not known even for a positive fraction of errors. In contrast,\nour work goes all the way to the distance of the code and in particular exceeds\nboth the unique decoding bound and the Johnson bound. For errors exceeding the\nJohnson bound, even combinatorial list-decodablity of these codes was not\nknown.\n  Our algorithm is an application of the classical polynomial method directly\nto the multivariate setting. In particular, we do not rely on a reduction from\nthe multivariate to the univariate case as is typical of many of the existing\nresults on decoding codes based on multivariate polynomials. However, a vanilla\napplication of the polynomial method in the multivariate setting does not yield\na polynomial upper bound on the list size. We obtain a polynomial bound on the\nlist size by taking an alternative view of multivariate multiplicity codes. In\nthis view, we glue all the partial derivatives of the same order together using\na fresh set $z$ of variables. We then apply the polynomial method by viewing\nthis as a problem over the field $\\mathbb{F}(z)$ of rational functions in $z$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:12:07 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bhandari", "Siddharth", ""], ["Harsha", "Prahladh", ""], ["Kumar", "Mrinal", ""], ["Sudan", "Madhu", ""]]}, {"id": "2012.01880", "submitter": "Arnab Maiti", "authors": "Arnab Maiti, Palash Dey", "title": "On Parameterized Complexity of Binary Networked Public Goods Game", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Binary Networked Public Goods game, every player needs to decide if\nshe participates in a public project whose utility is shared equally by the\ncommunity. We study the problem of deciding if there exists a pure strategy\nNash equilibrium (PSNE) in such games. The problem is already known to be\nNP-complete. We provide fine-grained analysis of this problem under the lens of\nparameterized complexity theory. We consider various natural graph parameters\nand show either W[1]-hardness or exhibit an FPT algorithm. We finally exhibit\nsome special graph classes, for example path, cycle, bi-clique, complete graph,\netc., which always have a PSNE if the utility function of the players are fully\nhomogeneous.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:50:42 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 16:37:16 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 10:53:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Maiti", "Arnab", ""], ["Dey", "Palash", ""]]}, {"id": "2012.01920", "submitter": "Igor Carboni Oliveira", "authors": "Srinivasan Arunachalam, Alex B. Grilo, Tom Gur, Igor C. Oliveira,\n  Aarthi Sundaram", "title": "Quantum learning algorithms imply circuit lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the first general connection between the design of quantum\nalgorithms and circuit lower bounds. Specifically, let $\\mathfrak{C}$ be a\nclass of polynomial-size concepts, and suppose that $\\mathfrak{C}$ can be\nPAC-learned with membership queries under the uniform distribution with error\n$1/2 - \\gamma$ by a time $T$ quantum algorithm. We prove that if $\\gamma^2\n\\cdot T \\ll 2^n/n$, then $\\mathsf{BQE} \\nsubseteq \\mathfrak{C}$, where\n$\\mathsf{BQE} = \\mathsf{BQTIME}[2^{O(n)}]$ is an exponential-time analogue of\n$\\mathsf{BQP}$. This result is optimal in both $\\gamma$ and $T$, since it is\nnot hard to learn any class $\\mathfrak{C}$ of functions in (classical) time $T\n= 2^n$ (with no error), or in quantum time $T = \\mathsf{poly}(n)$ with error at\nmost $1/2 - \\Omega(2^{-n/2})$ via Fourier sampling. In other words, even a\nmarginal improvement on these generic learning algorithms would lead to major\nconsequences in complexity theory.\n  Our proof builds on several works in learning theory, pseudorandomness, and\ncomputational complexity, and crucially, on a connection between non-trivial\nclassical learning algorithms and circuit lower bounds established by Oliveira\nand Santhanam (CCC 2017). Extending their approach to quantum learning\nalgorithms turns out to create significant challenges. To achieve that, we show\namong other results how pseudorandom generators imply learning-to-lower-bound\nconnections in a generic fashion, construct the first conditional pseudorandom\ngenerator secure against uniform quantum computations, and extend the local\nlist-decoding algorithm of Impagliazzo, Jaiswal, Kabanets and Wigderson (SICOMP\n2010) to quantum circuits via a delicate analysis. We believe that these\ncontributions are of independent interest and might find other applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:03:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Arunachalam", "Srinivasan", ""], ["Grilo", "Alex B.", ""], ["Gur", "Tom", ""], ["Oliveira", "Igor C.", ""], ["Sundaram", "Aarthi", ""]]}, {"id": "2012.02210", "submitter": "Yuval Filmus", "authors": "Yuval Filmus, Or Meir, Avishay Tal", "title": "Shrinkage under Random Projections, and Cubic Formula Lower Bounds for\n  $\\mathbf{AC}^0$", "comments": "Full version of ITCS 2021 paper, 42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  H\\r{a}stad showed that any De Morgan formula (composed of AND, OR and NOT\ngates) shrinks by a factor of $O(p^{2})$ under a random restriction that leaves\neach variable alive independently with probability $p$ [SICOMP, 1998]. Using\nthis result, he gave an $\\widetilde{\\Omega}(n^{3})$ formula size lower bound\nfor the Andreev function, which, up to lower order improvements, remains the\nstate-of-the-art lower bound for any explicit function. In this work, we extend\nthe shrinkage result of H\\r{a}stad to hold under a far wider family of random\nrestrictions and their generalization -- random projections. Based on our\nshrinkage results, we obtain an $\\widetilde{\\Omega}(n^{3})$ formula size lower\nbound for an explicit function computed in $\\mathbf{AC}^0$. This improves upon\nthe best known formula size lower bounds for $\\mathbf{AC}^0$, that were only\nquadratic prior to our work. In addition, we prove that the KRW conjecture\n[Karchmer et al., Computational Complexity 5(3/4), 1995] holds for inner\nfunctions for which the unweighted quantum adversary bound is tight. In\nparticular, this holds for inner functions with a tight Khrapchenko bound. Our\nrandom projections are tailor-made to the function's structure so that the\nfunction maintains structure even under projection -- using such projections is\nnecessary, as standard random restrictions simplify $\\mathbf{AC}^0$ circuits.\nIn contrast, we show that any De Morgan formula shrinks by a quadratic factor\nunder our random projections, allowing us to prove the cubic lower bound. Our\nproof techniques build on the proof of H\\r{a}stad for the simpler case of\nbalanced formulas. This allows for a significantly simpler proof at the cost of\nslightly worse parameters. As such, when specialized to the case of $p$-random\nrestrictions, our proof can be used as an exposition of H\\r{a}stad's result.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 19:00:55 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 15:35:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Filmus", "Yuval", ""], ["Meir", "Or", ""], ["Tal", "Avishay", ""]]}, {"id": "2012.02243", "submitter": "Dmitriy Kunisky", "authors": "Afonso S. Bandeira, Dmitriy Kunisky, Alexander S. Wein", "title": "Average-Case Integrality Gap for Non-Negative Principal Component\n  Analysis", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Montanari and Richard (2015) asked whether a natural semidefinite programming\n(SDP) relaxation can effectively optimize $\\mathbf{x}^{\\top}\\mathbf{W}\n\\mathbf{x}$ over $\\|\\mathbf{x}\\| = 1$ with $x_i \\geq 0$ for all coordinates\n$i$, where $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ is drawn from the Gaussian\northogonal ensemble (GOE) or a spiked matrix model. In small numerical\nexperiments, this SDP appears to be tight for the GOE, producing a rank-one\noptimal matrix solution aligned with the optimal vector $\\mathbf{x}$. We prove,\nhowever, that as $n \\to \\infty$ the SDP is not tight, and certifies an upper\nbound asymptotically no better than the simple spectral bound\n$\\lambda_{\\max}(\\mathbf{W})$ on this objective function. We also provide\nevidence, using tools from recent literature on hypothesis testing with\nlow-degree polynomials, that no subexponential-time certification algorithm can\nimprove on this behavior. Finally, we present further numerical experiments\nestimating how large $n$ would need to be before this limiting behavior becomes\nevident, providing a cautionary example against extrapolating asymptotics of\nSDPs in high dimension from their efficacy in small \"laptop scale\"\ncomputations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 20:19:56 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2012.02335", "submitter": "Manaswi Paraashar", "authors": "Sourav Chakraborty, Nikhil S. Mande, Rajat Mittal, Tulasimohan Molli,\n  Manaswi Paraashar, Swagato Sanyal", "title": "Tight Chang's-lemma-type bounds for Boolean functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chang's lemma (Duke Mathematical Journal, 2002) is a classical result with\napplications across several areas in mathematics and computer science. For a\nBoolean function $f$ that takes values in {-1,1} let $r(f)$ denote its Fourier\nrank. For each positive threshold $t$, Chang's lemma provides a lower bound on\n$wt(f):=\\Pr[f(x)=-1]$ in terms of the dimension of the span of its characters\nwith Fourier coefficients of magnitude at least $1/t$. We examine the tightness\nof Chang's lemma w.r.t. the following three natural settings of the threshold:\n  - the Fourier sparsity of $f$, denoted $k(f)$,\n  - the Fourier max-supp-entropy of $f$, denoted $k'(f)$, defined to be $\\max\n\\{1/|\\hat{f}(S)| : \\hat{f}(S) \\neq 0\\}$,\n  - the Fourier max-rank-entropy of $f$, denoted $k''(f)$, defined to be the\nminimum $t$ such that characters whose Fourier coefficients are at least $1/t$\nin absolute value span a space of dimension $r(f)$.\n  We prove new lower bounds on $wt(f)$ in terms of these measures. One of our\nlower bounds subsumes and refines the previously best known upper bound on\n$r(f)$ in terms of $k(f)$ by Sanyal (ToC, 2019). Another lower bound is based\non our improvement of a bound by Chattopadhyay, Hatami, Lovett and Tal (ITCS,\n2019) on the sum of the absolute values of the level-$1$ Fourier coefficients.\nWe also show that Chang's lemma for the these choices of the threshold is\nasymptotically outperformed by our bounds for most settings of the parameters\ninvolved.\n  Next, we show that our bounds are tight for a wide range of the parameters\ninvolved, by constructing functions (which are modifications of the Addressing\nfunction) witnessing their tightness. Finally we construct Boolean functions\n$f$ for which\n  - our lower bounds asymptotically match $wt(f)$, and\n  - for any choice of the threshold $t$, the lower bound obtained from Chang's\nlemma is asymptotically smaller than $wt(f)$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:55:43 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 16:16:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Mande", "Nikhil S.", ""], ["Mittal", "Rajat", ""], ["Molli", "Tulasimohan", ""], ["Paraashar", "Manaswi", ""], ["Sanyal", "Swagato", ""]]}, {"id": "2012.02513", "submitter": "Adrien Richard", "authors": "Florian Bridoux, Am\\'elia Durbec, K\\'evin Perrot, Adrien Richard", "title": "Complexity of fixed point counting problems in Boolean Networks", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM q-bio.MN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Boolean network (BN) with $n$ components is a discrete dynamical system\ndescribed by the successive iterations of a function $f:\\{0,1\\}^n \\to\n\\{0,1\\}^n$. This model finds applications in biology, where fixed points play a\ncentral role. For example, in genetic regulations, they correspond to cell\nphenotypes. In this context, experiments reveal the existence of positive or\nnegative influences among components: component $i$ has a positive (resp.\nnegative) influence on component $j$ meaning that $j$ tends to mimic (resp.\nnegate) $i$. The digraph of influences is called signed interaction digraph\n(SID), and one SID may correspond to a large number of BNs (which is, in\naverage, doubly exponential according to $n$). The present work opens a new\nperspective on the well-established study of fixed points in BNs. When\nbiologists discover the SID of a BN they do not know, they may ask: given that\nSID, can it correspond to a BN having at least/at most $k$ fixed points?\nDepending on the input, we prove that these problems are in $\\textrm{P}$ or\ncomplete for $\\textrm{NP}$, $\\textrm{NP}^{\\textrm{NP}}$,\n$\\textrm{NP}^{\\textrm{#P}}$ or $\\textrm{NEXPTIME}$. In particular, we prove\nthat it is $\\textrm{NP}$-complete (resp. $\\textrm{NEXPTIME}$-complete) to\ndecide if a given SID can correspond to a BN having at least two fixed points\n(resp. no fixed point).\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:45:51 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bridoux", "Florian", ""], ["Durbec", "Am\u00e9lia", ""], ["Perrot", "K\u00e9vin", ""], ["Richard", "Adrien", ""]]}, {"id": "2012.02619", "submitter": "Mohamed-Bachir Belaid", "authors": "Christian Bessiere, Mohamed-Bachir Belaid, Nadjib Lazaar", "title": "Computational Complexity of Three Central Problems in Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Itemset mining is one of the most studied tasks in knowledge discovery. In\nthis paper we analyze the computational complexity of three central itemset\nmining problems. We prove that mining confident rules with a given item in the\nhead is NP-hard. We prove that mining high utility itemsets is NP-hard. We\nfinally prove that mining maximal or closed itemsets is coNP-hard as soon as\nthe users can specify constraints on the kind of itemsets they are interested\nin.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:26:21 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 09:57:19 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 11:17:14 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Bessiere", "Christian", ""], ["Belaid", "Mohamed-Bachir", ""], ["Lazaar", "Nadjib", ""]]}, {"id": "2012.03367", "submitter": "Moshe Vardi", "authors": "James E. Newman and Moshe Y. Vardi", "title": "FPRAS Approximation of the Matrix Permanent in Practice", "comments": "This article is based on an MS thesis by the first author, submitted\n  to Rice University on June 12, 2020. Research partially supported by NSF\n  Grant no. IIS-1527668", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The matrix permanent belongs to the complexity class #P-Complete. It is\ngenerally believed to be computationally infeasible for large problem sizes,\nand significant research has been done on approximation algorithms for the\nmatrix permanent. We present an implementation and detailed runtime analysis of\none such Markov Chain Monte Carlo (MCMC) based Fully Polynomial Randomized\nApproximation Scheme (FPRAS) for the matrix permanent, which has previously\nonly been described theoretically and with big-Oh runtime analysis. We\ndemonstrate by analysis and experiment that the constant factors hidden by\nprevious big-Oh analyses result in computational infeasibility.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:25:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Newman", "James E.", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "2012.03415", "submitter": "Shalev Ben-David", "authors": "Anurag Anshu, Shalev Ben-David, Srijita Kundu", "title": "On Query-to-Communication Lifting for Adversary Bounds", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate query-to-communication lifting theorems for models related to\nthe quantum adversary bounds. Our results are as follows:\n  1. We show that the classical adversary bound lifts to a lower bound on\nrandomized communication complexity with a constant-sized gadget. We also show\nthat the classical adversary bound is a strictly stronger lower bound technique\nthan the previously-lifted measure known as critical block sensitivity, making\nour lifting theorem one of the strongest lifting theorems for randomized\ncommunication complexity using a constant-sized gadget.\n  2. Turning to quantum models, we show a connection between lifting theorems\nfor quantum adversary bounds and secure 2-party quantum computation in a\ncertain \"honest-but-curious\" model. Under the assumption that such secure\n2-party computation is impossible, we show that a simplified version of the\npositive-weight adversary bound lifts to a quantum communication lower bound\nusing a constant-sized gadget. We also give an unconditional lifting theorem\nwhich lower bounds bounded-round quantum communication protocols.\n  3. Finally, we give some new results in query complexity. We show that the\nclassical adversary and the positive-weight quantum adversary are quadratically\nrelated. We also show that the positive-weight quantum adversary is never\nlarger than the square of the approximate degree. Both relations hold even for\npartial functions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 02:10:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Anshu", "Anurag", ""], ["Ben-David", "Shalev", ""], ["Kundu", "Srijita", ""]]}, {"id": "2012.03883", "submitter": "Bruno P. Cavalar", "authors": "Bruno Pasqualotto Cavalar, Mrinal Kumar, Benjamin Rossman", "title": "Monotone Circuit Lower Bounds from Robust Sunflowers", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Robust sunflowers are a generalization of combinatorial sunflowers that have\napplications in monotone circuit complexity, DNF sparsification, randomness\nextractors, and recent advances on the Erd\\H{o}s-Rado sunflower conjecture. The\nrecent breakthrough of Alweiss, Lovett, Wu and Zhang gives an improved bound on\nthe maximum size of a $w$-set system that excludes a robust sunflower. In this\npaper, we use this result to obtain an $\\exp(n^{1/2-o(1)})$ lower bound on the\nmonotone circuit size of an explicit $n$-variate monotone function, improving\nthe previous best known $\\exp(n^{1/3-o(1)})$ due to Andreev and Harnik and Raz.\nWe also show an $\\exp(\\Omega(n))$ lower bound on the monotone arithmetic\ncircuit size of a related polynomial. Finally, we introduce a notion of robust\nclique-sunflowers and use this to prove an $n^{\\Omega(k)}$ lower bound on the\nmonotone circuit size of the CLIQUE function for all $k \\le n^{1/3-o(1)}$,\nstrengthening the bound of Alon and Boppana.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:00:32 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cavalar", "Bruno Pasqualotto", ""], ["Kumar", "Mrinal", ""], ["Rossman", "Benjamin", ""]]}, {"id": "2012.03923", "submitter": "Nathaniel Harms", "authors": "Eric Blais, Renato Ferreira Pinto Jr., Nathaniel Harms", "title": "VC Dimension and Distribution-Free Sample-Based Testing", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of determining which classes of functions can be\ntested more efficiently than they can be learned, in the distribution-free\nsample-based model that corresponds to the standard PAC learning setting. Our\nmain result shows that while VC dimension by itself does not always provide\ntight bounds on the number of samples required to test a class of functions in\nthis model, it can be combined with a closely-related variant that we call\n\"lower VC\" (or LVC) dimension to obtain strong lower bounds on this sample\ncomplexity.\n  We use this result to obtain strong and in many cases nearly optimal lower\nbounds on the sample complexity for testing unions of intervals, halfspaces,\nintersections of halfspaces, polynomial threshold functions, and decision\ntrees. Conversely, we show that two natural classes of functions, juntas and\nmonotone functions, can be tested with a number of samples that is polynomially\nsmaller than the number of samples required for PAC learning.\n  Finally, we also use the connection between VC dimension and property testing\nto establish new lower bounds for testing radius clusterability and testing\nfeasibility of linear constraint systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:50:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Blais", "Eric", ""], ["Pinto", "Renato Ferreira", "Jr."], ["Harms", "Nathaniel", ""]]}, {"id": "2012.03938", "submitter": "Yossi Rozantsev", "authors": "Yossi Rozantsev", "title": "The Local Structure of Bounded Degree Graphs", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $G=(V,E)$ be a simple graph with maximum degree $d$. For an integer\n$k\\in\\mathbb{N}$, the $k$-disc of a vertex $v\\in V$ is defined as the rooted\nsubgraph of $G$ that is induced by all vertices whose distance to $v$ is at\nmost $k$. The $k$-disc frequency distribution vector of $G$, denoted by\n$\\text{freq}_{k}(G)$, is a vector indexed by all isomorphism types of rooted\n$k$-discs. For each such isomorphism type $\\Gamma$, the corresponding entry in\n$\\text{freq}_{k}(G)$ counts the fraction of vertices in $V$ that have a\n$k$-disc isomorphic to $\\Gamma$. In a sense, $\\text{freq}_{k}(G)$ is one way to\nrepresent the \"local structure\" of $G$. The graph $G$ can be arbitrarily large,\nand so a natural question is whether given $\\text{freq}_{k}(G)$ it is possible\nto construct a small graph $H$, whose size is independent of $|V|$, such that\n$H$ has a similar local structure. N. Alon proved that for any $\\epsilon>0$\nthere always exists a graph $H$ whose size is independent of $|V|$ and whose\nfrequency vector satisfies\n$||\\text{freq}_{k}(G)-\\text{freq}_{k}(H)||_{1}\\le\\epsilon$. However, his proof\nis only existential and does not imply that there is a deterministic algorithm\nto construct such a graph $H$. He gave the open problem of finding an explicit\ndeterministic algorithm that finds $H$, or proving that no such algorithm\nexists. Our main result is that Alon's problem is undecidable if and only if a\nmuch more general problem (involving directed edges and edge colors) is\nundecidable. We also prove that both problems are decidable for the special\ncase when $G$ is a path. We show that the local structure of any directed\nedge-colored path $G$ can be approximated by a suitable fixed-size directed\nedge-colored path $H$ and we give explicit bound on the size of $H$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 06:59:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Rozantsev", "Yossi", ""]]}, {"id": "2012.04211", "submitter": "Guangsheng Ma", "authors": "Guangsheng Ma and Hongbo Li", "title": "Quantum Fully Homomorphic Encryption with neither Clifford Gate\n  Decomposition nor Real Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel quantum fully homomorphic encryption (QFHE) scheme, which\nallows to perform conditional rotations with the control bit in encrypted form.\nIn our scheme, any quantum circuit can be directly evaluated with no need to\ndecompose into Clifford/non-Clifford gates, nor to be transformed into real\nrepresentation. Our scheme is able to evaluate quantum Fourier transform\nalgorithm faster than previous QFHE schemes in the worst case.\n  The security of our scheme relies on the hardness of the underlying quantum\ncapable FHE scheme, and the latter sets its security on the learning with\nerrors problem and the circular security assumption.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:54:02 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 13:57:41 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 10:52:17 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 10:43:30 GMT"}, {"version": "v5", "created": "Tue, 18 May 2021 04:49:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ma", "Guangsheng", ""], ["Li", "Hongbo", ""]]}, {"id": "2012.04327", "submitter": "Yakov Babichenko", "authors": "Yakov Babichenko and Aviad Rubinstein", "title": "Settling the complexity of Nash equilibrium in congestion games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider (i) the problem of finding a (possibly mixed) Nash equilibrium in\ncongestion games, and (ii) the problem of finding an (exponential precision)\nfixed point of the gradient descent dynamics of a smooth function $f:[0,1]^n\n\\rightarrow \\mathbb{R}$. We prove that these problems are equivalent. Our\nresult holds for various explicit descriptions of $f$, ranging from (almost\ngeneral) arithmetic circuits, to degree-$5$ polynomials. By a very recent\nresult of [Fearnley, Goldberg, Hollender, Savani '20] this implies that these\nproblems are PPAD$\\cap$PLS-complete. As a corollary, we also obtain the\nfollowing equivalence of complexity classes: CCLS = PPAD$\\cap$PLS.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 10:00:45 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:53:53 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Babichenko", "Yakov", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "2012.04437", "submitter": "Lev Gordeev", "authors": "L. Gordeev, E. H. Haeusler", "title": "On proof theory in computer science", "comments": "arXiv admin note: text overlap with arXiv:2011.09262", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The subject logic in computer science should entail proof theoretic\napplications. So the question arises whether open problems in computational\ncomplexity can be solved by advanced proof theoretic techniques. In particular,\nconsider the complexity classes NP, coNP and PSPACE. It is well-known that NP\nand coNP are contained in PSPACE, but till recently precise characterization of\nthese relationships remained open. Now [2], [3] (see also [4]) presented proofs\nof corresponding equalities NP = coNP = PSPACE. These results were obtained by\nappropriate proof theoretic tree-to-dag compressing techniques to be briefy\nexplained below.\n  [2] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE,\nStudia Logica (107) (1): 55{83 (2019)\n  [3] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE II,\nBulletin of the Section of Logic (49) (3): 213{230 (2020)\nhttp://dx.doi.org/10.18788/0138-0680.2020.16\n  [4] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE II:\nAddendum, arXiv:2011.09262 (2020)\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:09:55 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Gordeev", "L.", ""], ["Haeusler", "E. H.", ""]]}, {"id": "2012.04679", "submitter": "J. M. Landsberg", "authors": "Runshi Geng and J.M. Landsberg", "title": "On the geometry of geometric rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We make a geometric study of the Geometric Rank of tensors recently\nintroduced by Kopparty et al. Results include classification of tensors with\ndegenerate geometric rank in $C^3\\otimes C^3\\otimes C^3$, classification of\ntensors with geometric rank two, and showing that upper bounds on geometric\nrank imply lower bounds on tensor rank.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:07:14 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 20:31:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Geng", "Runshi", ""], ["Landsberg", "J. M.", ""]]}, {"id": "2012.04868", "submitter": "J. Maurice Rojas", "authors": "J. Maurice Rojas", "title": "Counting Real Roots in Polynomial-Time for Systems Supported on Circuits", "comments": "29 pages, 1 figure, accepted for presentation at MEGA (Effective\n  Methods in Algebraic Geometry) 2021. You can see a recording of my talk at\n  MEGA 2021 (June 9, 2021) at this YouTube link:\n  https://www.youtube.com/watch?v=KKKmTctxbs4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suppose $A=\\{a_1,\\ldots,a_{n+2}\\}\\subset\\mathbb{Z}^n$ has cardinality $n+2$,\nwith all the coordinates of the $a_j$ having absolute value at most $d$, and\nthe $a_j$ do not all lie in the same affine hyperplane. Suppose\n$F=(f_1,\\ldots,f_n)$ is an $n\\times n$ polynomial system with generic integer\ncoefficients at most $H$ in absolute value, and $A$ the union of the sets of\nexponent vectors of the $f_i$. We give the first algorithm that, for any fixed\n$n$, counts exactly the number of real roots of $F$ in in time polynomial in\n$\\log(dH)$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:11:20 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 05:35:35 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 17:33:24 GMT"}, {"version": "v4", "created": "Sun, 3 Jan 2021 23:23:52 GMT"}, {"version": "v5", "created": "Fri, 11 Jun 2021 16:37:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rojas", "J. Maurice", ""]]}, {"id": "2012.05086", "submitter": "Mohammad Mansour", "authors": "Mohammad M. Mansour", "title": "Low-Complexity Soft-Output MIMO Detectors Based on Optimal Channel\n  Puncturing", "comments": "arXiv admin note: text overlap with arXiv:2001.10904", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Channel puncturing transforms a multiple-input multiple-output (MIMO) channel\ninto a sparse lower-triangular form using the so-called WL decomposition scheme\nin order to reduce tree-based detection complexity. We propose computationally\nefficient soft-output detectors based on two forms of channel puncturing:\naugmented and two-sided. The augmented WL detector (AWLD) employs a punctured\nchannel derived by triangularizing the true channel in augmented form, followed\nby leftsided Gaussian elimination. The two-sided WL detector (dubbed WLZ)\nemploys right-sided reduction and left-sided elimination to puncture the\nchannel. We prove that augmented channel puncturing is optimal in maximizing\nthe lower-bound on the achievable information rate (AIR) based on a new\nmismatched detection model. We show that the AWLD decomposes into an MMSE\nprefilter and channel gain compensation stages, followed by a regular WL\ndetector (WLD) that computes least-squares softdecision estimates. Similarly,\nWLZ decomposes into a pre-processing reduction step followed by WLD. AWLD\nattains the same performance as the existing AIR-based partial marginalization\n(PM) detector, but with less computational complexity. We empirically show that\nWLZ attains the best complexityperformance tradeoff among tree-based detectors.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:40:27 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Mansour", "Mohammad M.", ""]]}, {"id": "2012.05132", "submitter": "Benjamin Rossman", "authors": "Benjamin Rossman", "title": "Shrinkage of Decision Lists and DNF Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish nearly tight bounds on the expected shrinkage of decision lists\nand DNF formulas under the $p$-random restriction $\\mathbf R_p$ for all values\nof $p \\in [0,1]$. For a function $f$ with domain $\\{0,1\\}^n$, let\n$\\mathrm{DL}(f)$ denote the minimum size of a decision list that computes $f$.\nWe show that \\[\n  \\mathbb E[\\ \\mathrm{DL}(f{\\upharpoonright}\\mathbf R_p)\\ ] \\le\n  \\mathrm{DL}(f)^{\\log_{2/(1-p)}(\\frac{1+p}{1-p})}. \\] For example, this bound\nis $\\sqrt{\\mathrm{DL}(f)}$ when $p = \\sqrt{5}-2 \\approx 0.24$. For Boolean\nfunctions $f$, we obtain the same shrinkage bound with respect to DNF formula\nsize plus $1$ (i.e., replacing $\\mathrm{DL}(\\cdot)$ with\n$\\mathrm{DNF}(\\cdot)+1$ on both sides of the inequality).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:09:56 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 21:21:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rossman", "Benjamin", ""]]}, {"id": "2012.05233", "submitter": "Nikhil Mande", "authors": "Sourav Chakraborty, Arkadev Chattopadhyay, Peter H{\\o}yer, Nikhil S.\n  Mande, Manaswi Paraashar, Ronald de Wolf", "title": "Symmetry and Quantum Query-to-Communication Simulation", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Buhrman, Cleve and Wigderson (STOC'98) showed that for every Boolean function\nf : {-1,1}^n to {-1,1} and G in {AND_2, XOR_2}, the bounded-error quantum\ncommunication complexity of the composed function f o G equals O(Q(f) log n),\nwhere Q(f) denotes the bounded-error quantum query complexity of f. This is in\ncontrast with the classical setting, where it is easy to show that R^{cc}(f o\nG) < 2 R(f), where R^{cc} and R denote bounded-error communication and query\ncomplexity, respectively. Chakraborty et al. (CCC'20) exhibited a total\nfunction for which the log n overhead in the BCW simulation is required. We\nimprove upon their result in several ways.\n  We show that the log n overhead is not required when f is symmetric,\ngeneralizing a result of Aaronson and Ambainis for the Set-Disjointness\nfunction (Theory of Computing'05). This upper bound assumes a shared entangled\nstate, though for most symmetric functions the assumed number of entangled\nqubits is less than the communication and hence could be part of the\ncommunication. To prove this, we design an efficient distributed version of\nnoisy amplitude amplification that allows us to prove the result when f is the\nOR function.\n  In view of our first result, one may ask whether the log n overhead in the\nBCW simulation can be avoided even when f is transitive. We give a strong\nnegative answer by showing that the log n overhead is still necessary for some\ntransitive functions even when we allow the quantum communication protocol an\nerror probability that can be arbitrarily close to 1/2.\n  We also give, among other things, a general recipe to construct functions for\nwhich the log n overhead is required in the BCW simulation in the bounded-error\ncommunication model, even if the parties are allowed to share an arbitrary\nprior entangled state for free.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:58:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Chattopadhyay", "Arkadev", ""], ["H\u00f8yer", "Peter", ""], ["Mande", "Nikhil S.", ""], ["Paraashar", "Manaswi", ""], ["de Wolf", "Ronald", ""]]}, {"id": "2012.05398", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Enric Boix-Adsera", "title": "Hardness results for Multimarginal Optimal Transport problems", "comments": "For expository purposes, some of these results were moved from v1 of\n  arXiv 2008.03006. The current drafts of these papers have no overlapping\n  results. arXiv admin note: text overlap with arXiv:2008.03006", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimarginal Optimal Transport (MOT) is the problem of linear programming\nover joint probability distributions with fixed marginals. A key issue in many\napplications is the complexity of solving MOT: the linear program has\nexponential size in the number of marginals k and their support sizes n. A\nrecent line of work has shown that MOT is poly(n,k)-time solvable for certain\nfamilies of costs that have poly(n,k)-size implicit representations. However,\nit is unclear what further families of costs this line of algorithmic research\ncan encompass. In order to understand these fundamental limitations, this paper\ninitiates the study of intractability results for MOT.\n  Our main technical contribution is developing a toolkit for proving\nNP-hardness and inapproximability results for MOT problems. We demonstrate this\ntoolkit by using it to establish the intractability of a number of MOT problems\nstudied in the literature that have resisted previous algorithmic efforts. For\ninstance, we provide evidence that repulsive costs make MOT intractable by\nshowing that several such problems of interest are NP-hard to solve--even\napproximately.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:36:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Boix-Adsera", "Enric", ""]]}, {"id": "2012.05460", "submitter": "Matthew Coudron", "authors": "Nolan J. Coble and Matthew Coudron", "title": "Quasi-polynomial time approximation of output probabilities of\n  geometrically-local, shallow quantum circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a classical algorithm that, for any 3D geometrically-local,\npolylogarithmic-depth quantum circuit $C$ acting on $n$ qubits, and any bit\nstring $x\\in\\{0,1\\}^n$, can compute the quantity $|< x |C|0^{\\otimes n}>|^2$ to\nwithin any inverse-polynomial additive error in quasi-polynomial time. It is\nknown that it is $\\#P$-hard to compute this same quantity to within $2^{-n^2}$\nadditive error [Mov20, KMM21]. The previous best known algorithm for this\nproblem used $O(2^{n^{1/3}}\\text{poly}(1/\\epsilon))$ time to compute\nprobabilities to within additive error $\\epsilon$ [BGM20]. Notably, the [BGM20]\npaper included an elegant polynomial time algorithm for this estimation task\nrestricted to 2D circuits, which makes a novel use of 1D Matrix Product States\n(MPS) carefully tailored to the 2D geometry of the circuit in question.\nSurprisingly, it is not clear that it is possible to extend this use of MPS to\naddress the case of 3D circuits in polynomial time. This raises a natural\nquestion as to whether the computational complexity of the 3D problem might be\ndrastically higher than that of the 2D problem. In this work we address this\nquestion by exhibiting a quasi-polynomial time algorithm for the 3D case. In\norder to surpass the technical barriers encountered by previously known\ntechniques we are forced to pursue a novel approach.\n  Our algorithm has a Divide-and-Conquer structure, demonstrating how to\napproximate the desired quantity via several instantiations of the same problem\ntype, each involving 3D-local circuits on about half the number of qubits as\nthe original. This division step is then applied recursively, expressing the\noriginal quantity as a weighted combination of smaller and smaller 3D-local\nquantum circuits. A central technical challenge is to control correlations\narising from entanglement that may exist between the different circuit\n``pieces\" produced this way.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:19:29 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 00:11:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Coble", "Nolan J.", ""], ["Coudron", "Matthew", ""]]}, {"id": "2012.05682", "submitter": "Johannes Greiner", "authors": "Manuel Bodirsky, Johannes Greiner, Jakub Rydval", "title": "Tractable Combinations of Temporal CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constraint satisfaction problem (CSP) of a first-order theory $T$ is the\ncomputational problem of deciding whether a given conjunction of atomic\nformulas is satisfiable in some model of $T$. We study the computational\ncomplexity of $\\mathrm{CSP}(T_1 \\cup T_2)$ where $T_1$ and $T_2$ are theories\nwith disjoint finite relational signatures. We prove that if $T_1$ and $T_2$\nare the theories of temporal structures, i.e., structures where all relations\nhave a first-order definition in $(\\mathbb{Q};<)$, then $\\mathrm{CSP}(T_1 \\cup\nT_2)$ is in P or NP-complete. To this end we prove a purely algebraic statement\nabout the structure of the lattice of locally closed clones over the domain\n${\\mathbb Q}$ that contain $\\mathrm{Aut}(\\mathbb{Q};<)$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:08:34 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 09:11:43 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Greiner", "Johannes", ""], ["Rydval", "Jakub", ""]]}, {"id": "2012.06304", "submitter": "Latif Salum", "authors": "Latif Salum", "title": "Quantified X3SAT: P = NP = PSPACE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper shows that P = NP via one-in-three (or exactly-1) 3SAT, and that\nNP = PSPACE\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:36:07 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 12:36:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Salum", "Latif", ""]]}, {"id": "2012.06370", "submitter": "Sarah Winkler", "authors": "Sarah Winkler and Georg Moser", "title": "Runtime Complexity Analysis of Logically Constrained Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logically constrained rewrite systems (LCTRSs) are a versatile and efficient\nrewriting formalism that can be used to model programs from various programming\nparadigms, as well as simplification systems in compilers and SMT solvers. In\nthis paper, we investigate techniques to analyse the worst-case runtime\ncomplexity of LCTRSs. For that, we exploit synergies between previously\ndeveloped decomposition techniques for standard term rewriting by Avanzini et\nal. in conjunction with alternating time and size bound approximations for\ninteger programs by Brockschmidt et al. and adapt these techniques suitably to\nLCTRSs. Furthermore, we provide novel modularization techniques to exploit loop\nbounds from recurrence equations which yield sublinear bounds. We have\nimplemented the method in TCT to test the viability of our method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:19:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Winkler", "Sarah", ""], ["Moser", "Georg", ""]]}, {"id": "2012.06713", "submitter": "Sami Davies", "authors": "Sami Davies, Miklos Z. Racz, Cyrus Rashtchian, Benjamin G. Schiffer", "title": "Approximate Trace Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the usual trace reconstruction problem, the goal is to exactly reconstruct\nan unknown string of length $n$ after it passes through a deletion channel many\ntimes independently, producing a set of traces (i.e., random subsequences of\nthe string). We consider the relaxed problem of approximate reconstruction.\nHere, the goal is to output a string that is close to the original one in edit\ndistance while using much fewer traces than is needed for exact reconstruction.\nWe present several algorithms that can approximately reconstruct strings that\nbelong to certain classes, where the estimate is within $n/\\mathrm{polylog}(n)$\nedit distance, and where we only use $\\mathrm{polylog}(n)$ traces (or sometimes\njust a single trace). These classes contain strings that require a linear\nnumber of traces for exact reconstruction and which are quite different from a\ntypical random string. From a technical point of view, our algorithms\napproximately reconstruct consecutive substrings of the unknown string by\naligning dense regions of traces and using a run of a suitable length to\napproximate each region. To complement our algorithms, we present a general\nblack-box lower bound for approximate reconstruction, building on a lower bound\nfor distinguishing between two candidate input strings in the worst case. In\nparticular, this shows that approximating to within $n^{1/3 - \\delta}$ edit\ndistance requires $n^{1 + 3\\delta/2}/\\mathrm{polylog}(n)$ traces for $0< \\delta\n< 1/3$ in the worst case.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 03:34:26 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:27:11 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Davies", "Sami", ""], ["Racz", "Miklos Z.", ""], ["Rashtchian", "Cyrus", ""], ["Schiffer", "Benjamin G.", ""]]}, {"id": "2012.06779", "submitter": "Gaurav Sood", "authors": "Olaf Beyersdorff, Joshua Blinkhorn, Meena Mahajan, Tom\\'a\\v{s} Peitl,\n  Gaurav Sood", "title": "Hard QBFs for Merge Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first proof size lower bounds for the proof system Merge\nResolution (MRes [Olaf Beyersdorff et al., 2020]), a refutational proof system\nfor prenex quantified Boolean formulas (QBF) with a CNF matrix. Unlike most QBF\nresolution systems in the literature, proofs in MRes consist of resolution\nsteps together with information on countermodels, which are syntactically\nstored in the proofs as merge maps. As demonstrated in [Olaf Beyersdorff et\nal., 2020], this makes MRes quite powerful: it has strategy extraction by\ndesign and allows short proofs for formulas which are hard for classical QBF\nresolution systems.\n  Here we show the first exponential lower bounds for MRes, thereby uncovering\nlimitations of MRes. Technically, the results are either transferred from\nbounds from circuit complexity (for restricted versions of MRes) or directly\nobtained by combinatorial arguments (for full MRes). Our results imply that the\nMRes approach is largely orthogonal to other QBF resolution models such as the\nQCDCL resolution systems QRes and QURes and the expansion systems\n$\\forall$Exp+Res and IR.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 10:39:14 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Beyersdorff", "Olaf", ""], ["Blinkhorn", "Joshua", ""], ["Mahajan", "Meena", ""], ["Peitl", "Tom\u00e1\u0161", ""], ["Sood", "Gaurav", ""]]}, {"id": "2012.07056", "submitter": "Anamay Tengse", "authors": "Mrinal Kumar, C. Ramya, Ramprasad Saptharishi, Anamay Tengse", "title": "If VNP is hard, then so are equations for it", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assuming that the Permanent polynomial requires algebraic circuits of\nexponential size, we show that the class VNP does not have efficiently\ncomputable equations. In other words, any nonzero polynomial that vanishes on\nthe coefficient vectors of all polynomials in the class VNP requires algebraic\ncircuits of super-polynomial size.\n  In a recent work of Chatterjee and the authors (FOCS 2020), it was shown that\nthe subclasses of VP and VNP consisting of polynomials with bounded integer\ncoefficients do have equations with small algebraic circuits. Their work left\nopen the possibility that these results could perhaps be extended to all of VP\nor VNP. The results in this paper show that assuming the hardness of Permanent,\nat least for VNP, allowing polynomials with large coefficients does indeed\nincur a significant blow up in the circuit complexity of equations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:17:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kumar", "Mrinal", ""], ["Ramya", "C.", ""], ["Saptharishi", "Ramprasad", ""], ["Tengse", "Anamay", ""]]}, {"id": "2012.07497", "submitter": "Saulo Queiroz", "authors": "Saulo Queiroz and Jo\\~ao P. Vilela and Edmundo Monteiro", "title": "Is FFT Fast Enough for Beyond-5G Communications?", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the complexity and throughput limits of the Fast\nFourier Transform (FFT) algorithm having in mind the unprecedented number of\npoints (subcarriers) $N$ expected in future waveforms. Based on the\nspectro-computational analysis, we verify that the FFT complexity to process an\n$N$-subcarrier symbol grows faster than the number of bits in the symbol. Thus,\nthe useful throughput of FFT nullifies as $N$ grows. Also, because FFT demands\n$N$ to be a power of two $2^i$ (for some $i>0$), the spectrum widening causes\nthe FFT complexity to grow exponentially on $i$, i.e. $O(2^ii)$. To overcome\nthese limitations, we propose the Parameterized DFT (PDFT) algorithm, which\nbuilds on the parameterized complexity technique and the classic $O(N^2)$ DFT\nalgorithm to replace an $N$-point DFT into $N/n$ ($n>0$) smaller $n$-point\nDFTs. By setting $n=\\Theta(1)$, we get a $O(N)$ algorithm whose resulting\nwaveform matches OFDM in its vectorized form (i.e., Vector OFDM) but with the\n$N=2^i$ constraint relaxed. Besides, we also show that PDFT becomes\nmultiplierless for $n=2$, requiring only $\\Theta(N)$ complex sums. We believe\nour results constitute a relevant step towards the practical deployment of\nfuture extremely wide multicarrier signals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:31:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Queiroz", "Saulo", ""], ["Vilela", "Jo\u00e3o P.", ""], ["Monteiro", "Edmundo", ""]]}, {"id": "2012.07514", "submitter": "Vladimir Naidenko G.", "authors": "Vladimir Naidenko", "title": "Logical characterizations of computational complexity classes", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Descriptive complexity theory is an important area in the study of\ncomputational complexity. In this direction, it is possible to describe\ncombinatorial problems exclusively by logical methods, without resorting to the\nuse of complicated algorithms. The first work in this direction was written in\n1974 by the American mathematician Fagin. The article describes the development\nof methods of the theory of descriptive complexity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:48:24 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Naidenko", "Vladimir", ""]]}, {"id": "2012.07556", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Erik D. Demaine, Andrei Gonczi, Dylan H. Hendrickson,\n  Adam Hesterberg, Matias Korman, Oliver Korten, Jayson Lynch, Irene Parada,\n  Vera Sacrist\\'an", "title": "Characterizing Universal Reconfigurability of Modular Pivoting Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give both efficient algorithms and hardness results for reconfiguring\nbetween two connected configurations of modules in the hexagonal grid. The\nreconfiguration moves that we consider are \"pivots\", where a hexagonal module\nrotates around a vertex shared with another module. Following prior work on\nmodular robots, we define two natural sets of hexagon pivoting moves of\nincreasing power: restricted and monkey moves. When we allow both moves, we\npresent the first universal reconfiguration algorithm, which transforms between\nany two connected configurations using $O(n^3)$ monkey moves. This result\nstrongly contrasts the analogous problem for squares, where there are rigid\nexamples that do not have a single pivoting move preserving connectivity. On\nthe other hand, if we only allow restricted moves, we prove that the\nreconfiguration problem becomes PSPACE-complete. Moreover, we show that, in\ncontrast to hexagons, the reconfiguration problem for pivoting squares is\nPSPACE-complete regardless of the set of pivoting moves allowed. In the\nprocess, we strengthen the reduction framework of Demaine et al. [FUN'18] that\nwe consider of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:21:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Demaine", "Erik D.", ""], ["Gonczi", "Andrei", ""], ["Hendrickson", "Dylan H.", ""], ["Hesterberg", "Adam", ""], ["Korman", "Matias", ""], ["Korten", "Oliver", ""], ["Lynch", "Jayson", ""], ["Parada", "Irene", ""], ["Sacrist\u00e1n", "Vera", ""]]}, {"id": "2012.07678", "submitter": "Alapan Chaudhuri", "authors": "Zeeshan Ahmed, Alapan Chaudhuri, Kunwar Shaanjeet Singh Grover, Hrishi\n  Narayanan, Manasvi Vaidyula and Shreyas Pradhan", "title": "Games and Computational Complexity", "comments": "We thank Erik Demaine and Giovanni Viglietta, both of whose original\n  works served as more than inspiration for this article. Dated Dec 16, 71\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers are known to solve a wide spectrum of problems, however not all\nproblems are computationally solvable. Further, the solvable problems\nthemselves vary on the amount of computational resources they require for being\nsolved. The rigorous analysis of problems and assigning them to complexity\nclasses what makes up the immense field of complexity theory.\n  Do protein folding and sudoku have something in common? It might not seem so\nbut complexity theory tells us that if we had an algorithm that could solve\nsudoku efficiently then we could adapt it to predict for protein folding. This\nsame property is held by classic platformer games such as Super Mario Bros,\nwhich was proven to be NP-complete by Erik Demaine et. al.\n  This article attempts to review the analysis of classical platformer games.\nHere, we explore the field of complexity theory through a broad survey of\nliterature and then use it to prove that that solving a generalized level in\nthe game \"Celeste\" is NP-complete. Later, we also show how a small change in it\nmakes the game presumably harder to compute. Various abstractions and\nformalisms related to modelling of games in general (namely game theory and\nconstraint logic) and 2D platformer video games, including the generalized\nmeta-theorems originally formulated by Giovanni Viglietta are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 16:26:25 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 10:17:53 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ahmed", "Zeeshan", ""], ["Chaudhuri", "Alapan", ""], ["Grover", "Kunwar Shaanjeet Singh", ""], ["Narayanan", "Hrishi", ""], ["Vaidyula", "Manasvi", ""], ["Pradhan", "Shreyas", ""]]}, {"id": "2012.07774", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Small Covers for Near-Zero Sets of Polynomials and Learning Latent\n  Variable Models", "comments": "Full version of FOCS'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V$ be any vector space of multivariate degree-$d$ homogeneous\npolynomials with co-dimension at most $k$, and $S$ be the set of points where\nall polynomials in $V$ {\\em nearly} vanish. We establish a qualitatively\noptimal upper bound on the size of $\\epsilon$-covers for $S$, in the\n$\\ell_2$-norm. Roughly speaking, we show that there exists an $\\epsilon$-cover\nfor $S$ of cardinality $M = (k/\\epsilon)^{O_d(k^{1/d})}$. Our result is\nconstructive yielding an algorithm to compute such an $\\epsilon$-cover that\nruns in time $\\mathrm{poly}(M)$.\n  Building on our structural result, we obtain significantly improved learning\nalgorithms for several fundamental high-dimensional probabilistic models with\nhidden variables. These include density and parameter estimation for\n$k$-mixtures of spherical Gaussians (with known common covariance), PAC\nlearning one-hidden-layer ReLU networks with $k$ hidden units (under the\nGaussian distribution), density and parameter estimation for $k$-mixtures of\nlinear regressions (with Gaussian covariates), and parameter estimation for\n$k$-mixtures of hyperplanes. Our algorithms run in time {\\em quasi-polynomial}\nin the parameter $k$. Previous algorithms for these problems had running times\nexponential in $k^{\\Omega(1)}$.\n  At a high-level our algorithms for all these learning problems work as\nfollows: By computing the low-degree moments of the hidden parameters, we are\nable to find a vector space of polynomials that nearly vanish on the unknown\nparameters. Our structural result allows us to compute a quasi-polynomial sized\ncover for the set of hidden parameters, which we exploit in our learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:14:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.07804", "submitter": "Sivakanth Gopi", "authors": "Sivakanth Gopi, Venkatesan Guruswami", "title": "Improved Maximally Recoverable LRCs using Skew Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT math.RA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over\n$\\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$\nlocal groups each of size $r$. Each local group satisfies `$a$' local parity\nchecks to recover from `$a$' erasures in that local group and there are further\n$h$ global parity checks to provide fault tolerance from more global erasure\npatterns. Such an LRC is Maximally Recoverable (MR), if it offers the best\nblend of locality and global erasure resilience -- namely it can correct all\nerasure patterns whose recovery is information-theoretically feasible given the\nlocality structure (these are precisely patterns with up to `$a$' erasures in\neach local group and an additional $h$ erasures anywhere in the codeword).\n  Random constructions can easily show the existence of MR LRCs over very large\nfields, but a major algebraic challenge is to construct MR LRCs, or even show\ntheir existence, over smaller fields, as well as understand inherent lower\nbounds on their field size. We give an explicit construction of\n$(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by\n$\\left(O\\left(\\max\\{r,n/r\\}\\right)\\right)^{\\min\\{h,r-a\\}}$. This improves upon\nknown constructions in many relevant parameter ranges. Moreover, it matches the\nlower bound from Gopi et al. (2020) in an interesting range of parameters where\n$r=\\Theta(\\sqrt{n})$, $r-a=\\Theta(\\sqrt{n})$ and $h$ is a fixed constant with\n$h\\le a+2$, achieving the optimal field size of $\\Theta_{h}(n^{h/2}).$\n  Our construction is based on the theory of skew polynomials. We believe skew\npolynomials should have further applications in coding and complexity theory;\nas a small illustration we show how to capture algebraic results underlying\nlist decoding folded Reed-Solomon and multiplicity codes in a unified way\nwithin this theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:38:40 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 21:45:22 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gopi", "Sivakanth", ""], ["Guruswami", "Venkatesan", ""]]}, {"id": "2012.07833", "submitter": "Edward Haeusler", "authors": "Edward Hermann Haeusler", "title": "Going from the huge to the small: Efficient succinct representation of\n  proofs in Minimal implicational logic", "comments": "Companion to arXiv:2009.09802v1. This versions explains better\n  Lemma14. This Lemma now is Lemma16. Two new lemmas were introduced in the new\n  version to help writing the better explanation on former Lemma14. Some reader\n  asked a better explanation. I am grateful to prof. Lew Gordeev to have raised\n  questions that help me to deliver this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A previous article shows that any linear height bounded normal proof of a\ntautology in the Natural Deduction for Minimal implicational logic\n$M_{\\supset}$ is as huge as it is redundant. More precisely, any proof in a\nfamily of super-polynomially sized and linearly height bounded proofs have a\nsub-derivation that occurs super-polynomially many times in it. In this\narticle, we show that by collapsing all the repeated sub-derivations we obtain\na smaller structure, a rooted Directed Acyclic Graph (r-DAG), that is\npolynomially upper-bounded on the size of $\\alpha$ and it is a certificate that\n$\\alpha$ is a tautology that can be verified in polynomial time. In other\nwords, for every huge proof of a tautology in $M_{\\supset}$, we obtain a\nsuccinct certificate for its validity. Moreover, we show an algorithm able to\ncheck this validity in polynomial time on the certificate's size. Comments on\nhow the results in this article are related to a proof of the conjecture\n$NP=CoNP$ appears in conclusion.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:13:58 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:19:51 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Haeusler", "Edward Hermann", ""]]}, {"id": "2012.08231", "submitter": "Ruo Ando", "authors": "Ruo Ando, Yoshiyasu Takefuji", "title": "A new perspective of paramodulation complexity by solving massive 8\n  puzzles", "comments": "arXiv admin note: text overlap with arXiv:2011.00775", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sliding puzzle is a combination puzzle where a player slide pieces along\ncertain routes on a board to reach a certain end-configuration. In this paper,\nwe propose a novel measurement of complexity of massive sliding puzzles with\nparamodulation which is an inference method of automated reasoning. It turned\nout that by counting the number of clauses yielded with paramodulation, we can\nevaluate the difficulty of each puzzle. In experiment, we have generated 100 *\n8 puzzles which passed the solvability checking by countering inversions. By\ndoing this, we can distinguish the complexity of 8 puzzles with the number of\ngenerated with paramodulation. For example, board [2,3,6,1,7,8,5,4, hole] is\nthe easiest with score 3008 and board [6,5,8,7,4,3,2,1, hole] is the most\ndifficult with score 48653. Besides, we have succeeded to obverse several\nlayers of complexity (the number of clauses generated) in 100 puzzles. We can\nconclude that proposal method can provide a new perspective of paramodulation\ncomplexity concerning sliding block puzzles.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:47:47 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Ando", "Ruo", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "2012.08735", "submitter": "Li-Yang Tan", "authors": "Guy Blanc, Jane Lange, Li-Yang Tan", "title": "Testing and reconstruction via decision trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sublinear and local computation algorithms for decision trees,\nfocusing on testing and reconstruction. Our first result is a tester that runs\nin $\\mathrm{poly}(\\log s, 1/\\varepsilon)\\cdot n\\log n$ time, makes\n$\\mathrm{poly}(\\log s,1/\\varepsilon)\\cdot \\log n$ queries to an unknown\nfunction $f$, and:\n  $\\circ$ Accepts if $f$ is $\\varepsilon$-close to a size-$s$ decision tree;\n  $\\circ$ Rejects if $f$ is $\\Omega(\\varepsilon)$-far from decision trees of\nsize $s^{\\tilde{O}((\\log s)^2/\\varepsilon^2)}$.\n  Existing testers distinguish size-$s$ decision trees from those that are\n$\\varepsilon$-far from from size-$s$ decision trees in\n$\\mathrm{poly}(s^s,1/\\varepsilon)\\cdot n$ time with $\\tilde{O}(s/\\varepsilon)$\nqueries. We therefore solve an incomparable problem, but achieve\ndoubly-exponential-in-$s$ and exponential-in-$s$ improvements in time and query\ncomplexities respectively. We obtain our tester by designing a reconstruction\nalgorithm for decision trees: given query access to a function $f$ that is\nclose to a small decision tree, this algorithm provides fast query access to a\nsmall decision tree that is close to $f$. By known relationships, our results\nyield reconstruction algorithms for numerous other boolean function properties\n-- Fourier degree, randomized and quantum query complexities, certificate\ncomplexity, sensitivity, etc. -- which in turn yield new testers for these\nproperties. Finally, we give a hardness result for testing whether an unknown\nfunction is $\\varepsilon$-close-to or $\\Omega(\\varepsilon)$-far-from size-$s$\ndecision trees. We show that an efficient algorithm for this task would yield\nan efficient algorithm for properly learning decision trees, a central open\nproblem of learning theory. It has long been known that proper learning\nalgorithms for any class $\\mathcal{H}$ yield property testers for\n$\\mathcal{H}$; this provides an example of a converse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 04:18:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Blanc", "Guy", ""], ["Lange", "Jane", ""], ["Tan", "Li-Yang", ""]]}, {"id": "2012.09376", "submitter": "Qisheng Wang", "authors": "Qisheng Wang and Mingsheng Ying", "title": "Quantum Algorithm for Lexicographically Minimal String Rotation", "comments": "40 pages, 6 algorithms, minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexicographically minimal string rotation (LMSR) is a problem to find the\nminimal one among all rotations of a string in the lexicographical order, which\nis widely used in equality checking of graphs, polygons, automata and chemical\nstructures.\n  In this paper, we propose an $O(n^{3/4})$ quantum query algorithm for LMSR.\nIn particular, the algorithm has average-case query complexity $O(\\sqrt n \\log\nn)$, which is shown to be asymptotically optimal up to a polylogarithmic\nfactor, compared with its $\\Omega\\left(\\sqrt{n/\\log n}\\right)$ lower bound.\nFurthermore, we claim that our quantum algorithm outperforms any (classical)\nrandomized algorithms in both worst-case and average-case query complexities by\nshowing that every (classical) randomized algorithm for LMSR has worst-case\nquery complexity $\\Omega(n)$ and average-case query complexity $\\Omega(n/\\log\nn)$.\n  Our quantum algorithm for LMSR is developed in a framework of nested quantum\nalgorithms, based on two new results: (i) an $O(\\sqrt{n})$ (optimal) quantum\nminimum finding on bounded-error quantum oracles; and (ii) its $O\\left(\\sqrt{n\n\\log(1/\\varepsilon)}\\right)$ (optimal) error reduction. As a byproduct, we\nobtain some better upper bounds of independent interest: (i) $O(\\sqrt{N})$\n(optimal) for constant-depth MIN-MAX trees on $N$ variables; and (ii)\n$O(\\sqrt{n \\log m})$ for pattern matching which removes\n$\\operatorname{polylog}(n)$ factors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 03:13:45 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 12:45:42 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wang", "Qisheng", ""], ["Ying", "Mingsheng", ""]]}, {"id": "2012.09476", "submitter": "Albert Atserias", "authors": "Albert Atserias, Ilario Bonacina, Susanna F. de Rezende, Massimo\n  Lauria, Jakob Nordstr\\\"om, and Alexander Razborov", "title": "Clique Is Hard on Average for Regular Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove that for $k \\ll \\sqrt[4]{n}$ regular resolution requires length\n$n^{\\Omega(k)}$ to establish that an Erd\\H{o}s-R\\'enyi graph with appropriately\nchosen edge density does not contain a $k$-clique. This lower bound is optimal\nup to the multiplicative constant in the exponent, and also implies\nunconditional $n^{\\Omega(k)}$ lower bounds on running time for several\nstate-of-the-art algorithms for finding maximum cliques in graphs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 10:02:34 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Atserias", "Albert", ""], ["Bonacina", "Ilario", ""], ["de Rezende", "Susanna F.", ""], ["Lauria", "Massimo", ""], ["Nordstr\u00f6m", "Jakob", ""], ["Razborov", "Alexander", ""]]}, {"id": "2012.09720", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Hardness of Learning Halfspaces with Massart Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of PAC learning halfspaces in the presence of Massart\n(bounded) noise. Specifically, given labeled examples $(x, y)$ from a\ndistribution $D$ on $\\mathbb{R}^{n} \\times \\{ \\pm 1\\}$ such that the marginal\ndistribution on $x$ is arbitrary and the labels are generated by an unknown\nhalfspace corrupted with Massart noise at rate $\\eta<1/2$, we want to compute a\nhypothesis with small misclassification error. Characterizing the efficient\nlearnability of halfspaces in the Massart model has remained a longstanding\nopen problem in learning theory.\n  Recent work gave a polynomial-time learning algorithm for this problem with\nerror $\\eta+\\epsilon$. This error upper bound can be far from the\ninformation-theoretically optimal bound of $\\mathrm{OPT}+\\epsilon$. More recent\nwork showed that {\\em exact learning}, i.e., achieving error\n$\\mathrm{OPT}+\\epsilon$, is hard in the Statistical Query (SQ) model. In this\nwork, we show that there is an exponential gap between the\ninformation-theoretically optimal error and the best error that can be achieved\nby a polynomial-time SQ algorithm. In particular, our lower bound implies that\nno efficient SQ algorithm can approximate the optimal error within any\npolynomial factor.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:43:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.09770", "submitter": "Daniel Paulusma", "authors": "Barnaby Martin and Dani\\\"el Paulusma and Siani Smith", "title": "Hard Problems That Quickly Become Very Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph class is hereditary if it is closed under vertex deletion. We give\nexamples of NP-hard, PSPACE-complete and NEXPTIME-complete problems that become\nconstant-time solvable for every hereditary graph class that is not equal to\nthe class of all graphs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:25:23 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Martin", "Barnaby", ""], ["Paulusma", "Dani\u00ebl", ""], ["Smith", "Siani", ""]]}, {"id": "2012.09804", "submitter": "Alexsander Andrade De Melo", "authors": "Celina M. H. de Figueiredo, Alexsander A. de Melo, Fabiano S.\n  Oliveira, and Ana Silva", "title": "Maximum cut on interval graphs of interval count four is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of the MaxCut problem restricted to interval\ngraphs has been open since the 80's, being one of the problems proposed by\nJohnson on his Ongoing Guide to NP-completeness, and has been settled as\nNP-complete only recently by Adhikary, Bose, Mukherjee and Roy. On the other\nhand, many flawed proofs of polynomiality for MaxCut on the more restrictive\nclass of proper/unit interval graphs (or graphs with interval count 1) have\nbeen presented along the years, and the classification of the problem is still\nnot known. In this paper, we present the first NP-completeness proof for MaxCut\nwhen restricted to interval graphs with bounded interval count, namely graphs\nwith interval count 4.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:11:34 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 22:16:54 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 21:45:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["de Figueiredo", "Celina M. H.", ""], ["de Melo", "Alexsander A.", ""], ["Oliveira", "Fabiano S.", ""], ["Silva", "Ana", ""]]}, {"id": "2012.09814", "submitter": "Daniel Paulusma", "authors": "Petr A. Golovach and Dani\\\"el Paulusma and Erik Jan van Leeuwen", "title": "Induced Disjoint Paths in AT-free Graphs", "comments": "An extended abstract of this paper appeared in the proceedings of\n  SWAT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paths $P_1,\\ldots,P_k$ in a graph $G=(V,E)$ are mutually induced if any two\ndistinct $P_i$ and $P_j$ have neither common vertices nor adjacent vertices\n(except perhaps their end-vertices). The Induced Disjoint Paths problem is to\ndecide if a graph $G$ with $k$ pairs of specified vertices $(s_i,t_i)$ contains\n$k$ mutually induced paths $P_i$ such that each $P_i$ connects $s_i$ and $t_i$.\nThis is a classical graph problem that is NP-complete even for $k=2$. We study\nit for AT-free graphs.\n  Unlike its subclasses of permutation graphs and cocomparability graphs, the\nclass of AT-free graphs has no geometric intersection model. However, by a new,\nstructural analysis of the behaviour of Induced Disjoint Paths for AT-free\ngraphs, we prove that it can be solved in polynomial time for AT-free graphs\neven when $k$ is part of the input. This is in contrast to the situation for\nother well-known graph classes, such as planar graphs, claw-free graphs, or\nmore recently, (theta,wheel)-free graphs, for which such a result only holds if\n$k$ is fixed.\n  As a consequence of our main result, the problem of deciding if a given\nAT-free graph contains a fixed graph $H$ as an induced topological minor admits\na polynomial-time algorithm. In addition, we show that such an algorithm is\nessentially optimal by proving that the problem is W[1]-hard with parameter\n$|V_H|$, even on a subclass of AT-free graph, namely cobipartite graphs. We\nalso show that the problems $k$-in-a-Path and $k$-in-a-Tree are polynomial-time\nsolvable on AT-free graphs even if $k$ is part of the input. These problems are\nto test if a graph has an induced path or induced tree, respectively, spanning\n$k$ given vertices.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:32:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Golovach", "Petr A.", ""], ["Paulusma", "Dani\u00ebl", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "2012.10976", "submitter": "Stasys Jukna", "authors": "Stasys Jukna", "title": "Notes on Hazard-Free Circuits", "comments": "18 pages, 1 figure, 1 table; submitted to SIAM J. Discrete Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constructing hazard-free Boolean circuits (those avoiding\nelectronic glitches) dates back to the 1940s and is an important problem in\ncircuit design and even in cybersecurity. We show that a DeMorgan circuit is\nhazard-free if and only if the circuit produces (purely syntactically) all\nprime implicants as well as all prime implicates of the Boolean function it\ncomputes. This extends to arbitrary DeMorgan circuits a classical result of\nEichelberger [IBM J. Res. Develop., 9 (1965)] showing this property for special\ndepth-two circuits. Via an amazingly simple proof, we also strengthen a recent\nresult Ikenmeyer et al. [J. ACM, 66:4 (2019)]: not only the complexities of\nhazard-free and monotone circuits for monotone Boolean functions do coincide,\nbut every optimal hazard-free circuit for a monotone Boolean function must be\nmonotone. Then we show that hazard-free circuit complexity of a very simple\n(non-monotone) Boolean function is super-polynomially larger than its\nunrestricted circuit complexity. This function accepts a Boolean n x n matrix\niff every row and every column has exactly one 1-entry. Finally, we show that\nevery Boolean function of n variables can be computed by a hazard-free circuit\nof size O(2^n/n).\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 16:59:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Jukna", "Stasys", ""]]}, {"id": "2012.11266", "submitter": "Subin Pulari", "authors": "Satyadev Nandakumar and Subin Pulari", "title": "Ergodic Theorems for PSPACE functions and their converses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of effective pointwise ergodic theorems in\nresource-bounded settings. Classically, the convergence of the ergodic averages\nfor integrable functions can be arbitrarily slow. In contrast, we show that for\na class of PSPACE L1 functions, and a class of PSPACE computable\nmeasure-preserving ergodic transformations, the ergodic average exists for all\nPSPACE randoms and is equal to the space average on every EXP random. We\nestablish a partial converse that PSPACE non-randomness can be characterized as\nnon-convergence of ergodic averages. Further, we prove that there is a class of\nresource-bounded randoms, viz. SUBEXP-space randoms, on which the corresponding\nergodic theorem has an exact converse - a point x is SUBEXP-space random if and\nonly if the corresponding effective ergodic theorem holds for x.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 11:43:23 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 10:46:36 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Nandakumar", "Satyadev", ""], ["Pulari", "Subin", ""]]}, {"id": "2012.11742", "submitter": "Micha{\\l} Pilipczuk", "authors": "Jana Cslovjecsek and Friedrich Eisenbrand and Micha{\\l} Pilipczuk and\n  Moritz Venzin and Robert Weismantel", "title": "Efficient sequential and parallel algorithms for multistage stochastic\n  integer programming using proximity", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving integer programs of the form $\\min\n\\{\\,c^\\intercal x\\ \\colon\\ Ax=b, x\\geq 0\\}$, where $A$ is a multistage\nstochastic matrix in the following sense: the primal treedepth of $A$ is\nbounded by a parameter $d$, which means that the columns of $A$ can be\norganized into a rooted forest of depth at most $d$ so that columns not bound\nby the ancestor/descendant relation in the forest do not have non-zero entries\nin the same row. We give an algorithm that solves this problem in\nfixed-parameter time $f(d,\\|A\\|_{\\infty})\\cdot n\\log^{O(2^d)} n$, where $f$ is\na computable function and $n$ is the number of rows of $A$. The algorithm works\nin the strong model, where the running time only measures unit arithmetic\noperations on the input numbers and does not depend on their bitlength. This is\nthe first fpt algorithm for multistage stochastic integer programming to\nachieve almost linear running time in the strong sense. For the case of\ntwo-stage stochastic integer programs, our algorithm works in time\n$2^{(2\\|A\\|_\\infty)^{O(r(r+s))}}\\cdot n\\log^{O(rs)} n$. The algorithm can be\nalso parallelized: we give an implementation in the PRAM model that achieves\nrunning time $f(d,\\|A\\|_{\\infty})\\cdot \\log^{O(2^d)} n$ using $n$ processors.\n  The main conceptual ingredient in our algorithms is a new proximity result\nfor multistage stochastic integer programs. We prove that if we consider an\ninteger program $P$, say with a constraint matrix $A$, then for every optimum\nsolution to the linear relaxation of $P$ there exists an optimum (integral)\nsolution to $P$ that lies, in the $\\ell_{\\infty}$-norm, within distance bounded\nby a function of $\\|A\\|_{\\infty}$ and the primal treedepth of $A$. On the way\nto achieve this result, we prove a generalization and considerable improvement\nof a structural result of Klein for multistage stochastic integer programs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:21:50 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cslovjecsek", "Jana", ""], ["Eisenbrand", "Friedrich", ""], ["Pilipczuk", "Micha\u0142", ""], ["Venzin", "Moritz", ""], ["Weismantel", "Robert", ""]]}, {"id": "2012.12216", "submitter": "Shivam Nadimpalli", "authors": "Anindya De, Shivam Nadimpalli, Rocco A. Servedio", "title": "Quantitative Correlation Inequalities via Semigroup Interpolation", "comments": "37 pages, conference version to appear in ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most correlation inequalities for high-dimensional functions in the\nliterature, such as the Fortuin-Kasteleyn-Ginibre (FKG) inequality and the\ncelebrated Gaussian Correlation Inequality of Royen, are qualitative statements\nwhich establish that any two functions of a certain type have non-negative\ncorrelation. In this work we give a general approach that can be used to\nbootstrap many qualitative correlation inequalities for functions over product\nspaces into quantitative statements. The approach combines a new extremal\nresult about power series, proved using complex analysis, with harmonic\nanalysis of functions over product spaces. We instantiate this general approach\nin several different concrete settings to obtain a range of new and\nnear-optimal quantitative correlation inequalities, including:\n  $\\bullet$ A quantitative version of Royen's celebrated Gaussian Correlation\nInequality. Royen (2014) confirmed a conjecture, open for 40 years, stating\nthat any two symmetric, convex sets must be non-negatively correlated under any\ncentered Gaussian distribution. We give a lower bound on the correlation in\nterms of the vector of degree-2 Hermite coefficients of the two convex sets,\nanalogous to the correlation bound for monotone Boolean functions over\n$\\{0,1\\}^n$ obtained by Talagrand (1996).\n  $\\bullet$ A quantitative version of the well-known FKG inequality for\nmonotone functions over any finite product probability space, generalizing the\nquantitative correlation bound for monotone Boolean functions over $\\{0,1\\}^n$\nobtained by Talagrand (1996). The only prior generalization of which we are\naware is due to Keller (2008, 2009, 2012), which extended Talagrand's result to\nproduct distributions over $\\{0,1\\}^n$. We also give two different quantitative\nversions of the FKG inequality for monotone functions over the continuous\ndomain $[0,1]^n$, answering a question of Keller (2009).\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:03:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["De", "Anindya", ""], ["Nadimpalli", "Shivam", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "2012.12594", "submitter": "Christian Schulz", "authors": "Faisal Abu-Khzam, Sebastian Lamm, Matthias Mnich, Alexander Noe,\n  Christian Schulz, and Darren Strash", "title": "Recent Advances in Practical Data Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, significant advances have been made in the design\nand analysis of fixed-parameter algorithms for a wide variety of\ngraph-theoretic problems. This has resulted in an algorithmic toolbox that is\nby now well-established. However, these theoretical algorithmic ideas have\nreceived very little attention from the practical perspective. We survey recent\ntrends in data reduction engineering results for selected problems. Moreover,\nwe describe concrete techniques that may be useful for future implementations\nin the area and give open problems and research questions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:52:29 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 10:11:13 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 07:57:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Abu-Khzam", "Faisal", ""], ["Lamm", "Sebastian", ""], ["Mnich", "Matthias", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""]]}, {"id": "2012.12717", "submitter": "James David Watson", "authors": "James D. Watson, Johannes Bausch, Sevag Gharibian", "title": "The Complexity of Translationally Invariant Problems beyond Ground State\n  Energies", "comments": "58 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.str-el cs.CC math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that three fundamental questions regarding local Hamiltonians --\napproximating the ground state energy (the Local Hamiltonian problem),\nsimulating local measurements on the ground space (APX-SIM), and deciding if\nthe low energy space has an energy barrier (GSCON) -- are $\\mathsf{QMA}$-hard,\n$\\mathsf{P}^{\\mathsf{QMA}[log]}$-hard and $\\mathsf{QCMA}$-hard, respectively,\nmeaning they are likely intractable even on a quantum computer. Yet while\nhardness for the Local Hamiltonian problem is known to hold even for\ntranslationally-invariant systems, it is not yet known whether APX-SIM and\nGSCON remain hard in such \"simple\" systems. In this work, we show that the\ntranslationally invariant versions of both APX-SIM and GSCON remain\nintractable, namely are $\\mathsf{P}^{\\mathsf{QMA}_{\\mathsf{EXP}}}$- and\n$\\mathsf{QCMA}_{\\mathsf{EXP}}$-complete, respectively. Each of these results is\nattained by giving a respective generic \"lifting theorem\" for producing\nhardness results. For APX-SIM, for example, we give a framework for \"lifting\"\nany abstract local circuit-to-Hamiltonian mapping $H$ (satisfying mild\nassumptions) to hardness of APX-SIM on the family of Hamiltonians produced by\n$H$, while preserving the structural and geometric properties of $H$ (e.g.\ntranslation invariance, geometry, locality, etc). Each result also leverages\ncounterintuitive properties of our constructions: for APX-SIM, we \"compress\"\nthe answers to polynomially many parallel queries to a QMA oracle into a single\nqubit. For GSCON, we give a hardness construction robust against highly\nnon-local unitaries, i.e. even if the adversary acts on all but one qudit in\nthe system in each step.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:44:57 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Watson", "James D.", ""], ["Bausch", "Johannes", ""], ["Gharibian", "Sevag", ""]]}, {"id": "2012.12798", "submitter": "Stasys Jukna", "authors": "Stasys Jukna", "title": "Coin Flipping in Dynamic Programming is Almost Useless", "comments": "25 pages, 1 table", "journal-ref": "ACM Trans. on Computation Theory (2020) 26 pages, Article 17", "doi": "10.1145/3397476", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider probabilistic circuits working over the real numbers, and using\narbitrary semialgebraic functions of bounded description complexity as gates.\nIn particular, such circuits can use all arithmetic operations +, -, x, /,\noptimization operations min and max, conditional branching (if-then-else), and\nmany more. We show that probabilistic circuits using any of these operations as\ngates can be simulated by deterministic circuits with only about a quadratical\nblowup in size. A not much larger blow up in circuit size is also shown when\nderandomizing approximating circuits. The algorithmic consequence, motivating\nthe title, is that randomness cannot substantially speed up dynamic programming\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:58:49 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jukna", "Stasys", ""]]}, {"id": "2012.12828", "submitter": "Eva Miranda", "authors": "Robert Cardona, Eva Miranda, Daniel Peralta-Salas and Francisco Presas", "title": "Constructing Turing complete Euler flows in dimension $3$", "comments": "minor changes, 16 pages, 2 figures", "journal-ref": "Proceedings of the National Academy of Sciences May 2021, 118 (19)\n  e2026818118", "doi": "10.1073/pnas.2026818118", "report-no": null, "categories": "math.DS cs.CC math.AP math.SG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can every physical system simulate any Turing machine? This is a classical\nproblem which is intimately connected with the undecidability of certain\nphysical phenomena. Concerning fluid flows, Moore asked in [15] if\nhydrodynamics is capable of performing computations. More recently, Tao\nlaunched a programme based on the Turing completeness of the Euler equations to\naddress the blow up problem in the Navier-Stokes equations. In this direction,\nthe undecidability of some physical systems has been studied in recent years,\nfrom the quantum gap problem [7] to quantum field theories [11]. To the best of\nour knowledge, the existence of undecidable particle paths of 3D fluid flows\nhas remained an elusive open problem since Moore's works in the early 1990's.\nIn this article we construct a Turing complete stationary Euler flow on a\nRiemannian $S^3$ and speculate on its implications concerning Tao's approach to\nthe blow up problem in the Navier-Stokes equations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:43:44 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:58:38 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cardona", "Robert", ""], ["Miranda", "Eva", ""], ["Peralta-Salas", "Daniel", ""], ["Presas", "Francisco", ""]]}, {"id": "2012.12831", "submitter": "Stasys Jukna", "authors": "Stasys Jukna and Hannes Seiwert", "title": "Approximation Limitations of Pure Dynamic Programming", "comments": null, "journal-ref": "SIAM J. on Computing 49:1 (2020) 170-207", "doi": "10.1137/18M1196339", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first, even super-polynomial, lower bounds on the size of\ntropical (min,+) and (max,+) circuits approximating given optimization\nproblems. Many classical dynamic programming (DP) algorithms for optimization\nproblems are pure in that they only use the basic min, max, + operations in\ntheir recursion equations. Tropical circuits constitute a rigorous mathematical\nmodel for this class of algorithms. An algorithmic consequence of our lower\nbounds for tropical circuits is that the approximation powers of pure DP\nalgorithms and greedy algorithms are incomparable. That pure DP algorithms can\nhardly beat greedy in approximation, is long known. New in this consequence is\nthat also the converse holds.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:50:02 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jukna", "Stasys", ""], ["Seiwert", "Hannes", ""]]}, {"id": "2012.12838", "submitter": "Stasys Jukna", "authors": "Stasys Jukna and Hannes Seiwert", "title": "Sorting Can Exponentially Speed Up Pure Dynamic Programming", "comments": null, "journal-ref": "Information Processing Letters 159-160 (2020), article Nr. 105962", "doi": "10.1016/j.ipl.2020.105962", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many discrete minimization problems, including various versions of the\nshortest path problem, can be efficiently solved by dynamic programming (DP)\nalgorithms that are \"pure\" in that they only perform basic operations, as min,\nmax, +, but no conditional branchings via if-then-else in their recursion\nequations. It is known that any pure (min,+) DP algorithm solving the minimum\nweight spanning tree problem on undirected n-vertex graphs must perform at\nleast $2^{\\Omega(\\sqrt{n})}$ operations. We show that this problem can be\nsolved by a pure (min,max,+) DP algorithm performing only $O(n^3)$ operations.\nThe algorithm is essentially a (min,max) algorithm: addition operations are\nonly used to output the final values. The presence of both min and max\noperations means that now DP algorithms can sort: this explains the title of\nthe paper.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:03:33 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jukna", "Stasys", ""], ["Seiwert", "Hannes", ""]]}, {"id": "2012.13329", "submitter": "Arda Sahiner", "authors": "Arda Sahiner, Tolga Ergen, John Pauly and Mert Pilanci", "title": "Vector-output ReLU Neural Network Problems are Copositive Programs:\n  Convex Analysis of Two Layer Networks and Polynomial-time Algorithms", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the convex semi-infinite dual of the two-layer vector-output ReLU\nneural network training problem. This semi-infinite dual admits a finite\ndimensional representation, but its support is over a convex set which is\ndifficult to characterize. In particular, we demonstrate that the non-convex\nneural network training problem is equivalent to a finite-dimensional convex\ncopositive program. Our work is the first to identify this strong connection\nbetween the global optima of neural networks and those of copositive programs.\nWe thus demonstrate how neural networks implicitly attempt to solve copositive\nprograms via semi-nonnegative matrix factorization, and draw key insights from\nthis formulation. We describe the first algorithms for provably finding the\nglobal minimum of the vector output neural network training problem, which are\npolynomial in the number of samples for a fixed data rank, yet exponential in\nthe dimension. However, in the case of convolutional architectures, the\ncomputational complexity is exponential in only the filter size and polynomial\nin all other parameters. We describe the circumstances in which we can find the\nglobal optimum of this neural network training problem exactly with\nsoft-thresholded SVD, and provide a copositive relaxation which is guaranteed\nto be exact for certain classes of problems, and which corresponds with the\nsolution of Stochastic Gradient Descent in practice.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:03:30 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sahiner", "Arda", ""], ["Ergen", "Tolga", ""], ["Pauly", "John", ""], ["Pilanci", "Mert", ""]]}, {"id": "2012.14095", "submitter": "J\\'an Pich", "authors": "J\\'an Pich", "title": "Learning algorithms from circuit lower bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit known constructions of efficient learning algorithms from various\nnotions of constructive circuit lower bounds such as distinguishers breaking\npseudorandom generators or efficient witnessing algorithms which find errors of\nsmall circuits attempting to compute hard functions. As our main result we\nprove that if it is possible to find efficiently, in a particular interactive\nway, errors of many p-size circuits attempting to solve hard problems, then\np-size circuits can be PAC learned over the uniform distribution with\nmembership queries by circuits of subexponential size. The opposite implication\nholds as well. This provides a new characterisation of learning algorithms and\nextends the natural proofs barrier of Razborov and Rudich. The proof is based\non a method of exploiting Nisan-Wigderson generators introduced by\nKraj\\'{i}\\v{c}ek (2010) and used to analyze complexity of circuit lower bounds\nin bounded arithmetic.\n  An interesting consequence of known constructions of learning algorithms from\ncircuit lower bounds is a learning speedup of Oliveira and Santhanam (2016). We\npresent an alternative proof of this phenomenon and discuss its potential to\nadvance the program of hardness magnification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 04:47:36 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Pich", "J\u00e1n", ""]]}, {"id": "2012.14236", "submitter": "Themistoklis Melissourgos", "authors": "Argyrios Deligkas, John Fearnley, Themistoklis Melissourgos", "title": "Pizza Sharing is PPA-hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of computing solutions for the\nstraight-cut and square-cut pizza sharing problems. We show that finding an\napproximate solution is PPA-hard for the straight-cut problem, and PPA-complete\nfor the square-cut problem, while finding an exact solution for the square-cut\nproblem is FIXP-hard and in BU. Our PPA-hardness results apply even when all\nmass distributions are unions of non-overlapping squares, and our FIXP-hardness\nresult applies even when all mass distributions are unions of weighted squares\nand right-angled triangles. We also show that decision variants of the\nsquare-cut problem are hard: we show that the approximate problem is\nNP-complete, and the exact problem is ETR-complete.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:10:30 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 03:15:23 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Deligkas", "Argyrios", ""], ["Fearnley", "John", ""], ["Melissourgos", "Themistoklis", ""]]}, {"id": "2012.14412", "submitter": "Jeroen Zuiddam", "authors": "Matthias Christandl, Vladimir Lysikov, Jeroen Zuiddam", "title": "Weighted Slice Rank and a Minimax Correspondence to Strassen's Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.RT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural and computational understanding of tensors is the driving force\nbehind faster matrix multiplication algorithms, the unraveling of quantum\nentanglement, and the breakthrough on the cap set problem. Strassen's\nasymptotic spectra program (SFCS 1986) characterizes optimal matrix\nmultiplication algorithms through monotone functionals. Our work advances and\nmakes novel connections among two recent developments in the study of tensors,\nnamely (1) the slice rank of tensors, a notion of rank for tensors that emerged\nfrom the resolution of the cap set problem (Ann. of Math. 2017), and (2) the\nquantum functionals of tensors (STOC 2018), monotone functionals defined as\noptimizations over moment polytopes. More precisely, we introduce an extension\nof slice rank that we call weighted slice rank and we develop a minimax\ncorrespondence between the asymptotic weighted slice rank and the quantum\nfunctionals. Weighted slice rank encapsulates different notions of\nbipartiteness of quantum entanglement.\n  The correspondence allows us to give a rank-type characterization of the\nquantum functionals. Moreover, whereas the original definition of the quantum\nfunctionals only works over the complex numbers, this new characterization can\nbe extended to all fields. Thereby, in addition to gaining deeper understanding\nof Strassen's theory for the complex numbers, we obtain a proposal for quantum\nfunctionals over other fields. The finite field case is crucial for\ncombinatorial and algorithmic problems where the field can be optimized over.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:49:23 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Christandl", "Matthias", ""], ["Lysikov", "Vladimir", ""], ["Zuiddam", "Jeroen", ""]]}, {"id": "2012.15347", "submitter": "Ilya Shapirovsky", "authors": "Ilya B. Shapirovsky", "title": "Satisfiability problems on sums of Kripke frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the operation of sum on Kripke frames, where a family of\nframes-summands is indexed by elements of another frame. In many cases, the\nmodal logic of sums inherits the finite model property and decidability from\nthe modal logic of summands. In this paper we show that, under a general\ncondition, the satisfiability problem on sums is polynomial space Turing\nreducible to the satisfiability problem on summands. In particular, for many\nmodal logics decidability in PSPACE is an immediate corollary from the semantic\ncharacterization of the logic.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:28:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Shapirovsky", "Ilya B.", ""]]}]