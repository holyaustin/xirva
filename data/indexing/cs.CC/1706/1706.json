[{"id": "1706.00053", "submitter": "Ramesh Krishnan S. Pallavoor", "authors": "Roksana Baleshzar, Deeparnab Chakrabarty, Ramesh Krishnan S.\n  Pallavoor, Sofya Raskhodnikova, C. Seshadhri", "title": "A Lower Bound for Nonadaptive, One-Sided Error Testing of Unateness of\n  Boolean Functions over the Hypercube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function $f:\\{0,1\\}^d \\mapsto \\{0,1\\}$ is unate if, along each\ncoordinate, the function is either nondecreasing or nonincreasing. In this\nnote, we prove that any nonadaptive, one-sided error unateness tester must make\n$\\Omega(\\frac{d}{\\log d})$ queries. This result improves upon the\n$\\Omega(\\frac{d}{\\log^2 d})$ lower bound for the same class of testers due to\nChen et al. (STOC, 2017).\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:16:46 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Baleshzar", "Roksana", ""], ["Chakrabarty", "Deeparnab", ""], ["Pallavoor", "Ramesh Krishnan S.", ""], ["Raskhodnikova", "Sofya", ""], ["Seshadhri", "C.", ""]]}, {"id": "1706.00078", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Yaroslav Shitov", "title": "Low-Rank Matrix Approximation in the Infinity Norm", "comments": "12 pages, 3 tables", "journal-ref": "Linear Algebra and its Applications 581, pp. 367-382, 2019", "doi": "10.1016/j.laa.2019.07.017", "report-no": null, "categories": "cs.CC cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank matrix approximation problem with respect to the entry-wise\n$\\ell_{\\infty}$-norm is the following: given a matrix $M$ and a factorization\nrank $r$, find a matrix $X$ whose rank is at most $r$ and that minimizes\n$\\max_{i,j} |M_{ij} - X_{ij}|$. In this paper, we prove that the decision\nvariant of this problem for $r=1$ is NP-complete using a reduction from the\nproblem `not all equal 3SAT'. We also analyze several cases when the problem\ncan be solved in polynomial time, and propose a simple practical heuristic\nalgorithm which we apply on the problem of the recovery of a quantized low-rank\nmatrix.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:32:51 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Gillis", "Nicolas", ""], ["Shitov", "Yaroslav", ""]]}, {"id": "1706.00080", "submitter": "Vladimir Podolskii", "authors": "Dima Grigoriev, Vladimir V. Podolskii", "title": "Tropical Combinatorial Nullstellensatz and Sparse Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical algebra emerges in many fields of mathematics such as algebraic\ngeometry, mathematical physics and combinatorial optimization. In part, its\nimportance is related to the fact that it makes various parameters of\nmathematical objects computationally accessible. Tropical polynomials play a\nfundamental role in this, especially for the case of algebraic geometry. On the\nother hand, many algebraic questions behind tropical polynomials remain open.\nIn this paper we address four basic questions on tropical polynomials closely\nrelated to their computational properties:\n  1. Given a polynomial with a certain support (set of monomials) and a\n(finite) set of inputs, when is it possible for the polynomial to vanish on all\nthese inputs?\n  2. A more precise question, given a polynomial with a certain support and a\n(finite) set of inputs, how many roots can this polynomial have on this set of\ninputs?\n  3. Given an integer $k$, for which $s$ there is a set of $s$ inputs such that\nany non-zero polynomial with at most $k$ monomials has a non-root among these\ninputs?\n  4. How many integer roots can have a one variable polynomial given by a\ntropical algebraic circuit?\n  In the classical algebra well-known results in the direction of these\nquestions are Combinatorial Nullstellensatz due to N. Alon, J. Schwartz - R.\nZippel Lemma and Universal Testing Set for sparse polynomials respectively. The\nclassical analog of the last question is known as $\\tau$-conjecture due to M.\nShub - S. Smale. In this paper we provide results on these four questions for\ntropical polynomials.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:40:01 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 13:27:43 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 07:25:30 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Grigoriev", "Dima", ""], ["Podolskii", "Vladimir V.", ""]]}, {"id": "1706.00335", "submitter": "Srijita Kundu", "authors": "Anurag Anshu, Dmitry Gavinsky, Rahul Jain, Srijita Kundu, Troy Lee,\n  Priyanka Mukhopadhyay, Miklos Santha and Swagato Sanyal", "title": "A Composition Theorem for Randomized Query Complexity", "comments": "11 pages; version 2, minor errors corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let the randomized query complexity of a relation for error probability\n$\\epsilon$ be denoted by $R_\\epsilon(\\cdot)$. We prove that for any relation $f\n\\subseteq \\{0,1\\}^n \\times \\mathcal{R}$ and Boolean function $g:\\{0,1\\}^m\n\\rightarrow \\{0,1\\}$, $R_{1/3}(f\\circ g^n) = \\Omega(R_{4/9}(f)\\cdot\nR_{1/2-1/n^4}(g))$, where $f \\circ g^n$ is the relation obtained by composing\n$f$ and $g$. We also show that $R_{1/3}\\left(f \\circ \\left(g^\\oplus_{O(\\log\nn)}\\right)^n\\right)=\\Omega(\\log n \\cdot R_{4/9}(f) \\cdot R_{1/3}(g))$, where\n$g^\\oplus_{O(\\log n)}$ is the function obtained by composing the xor function\non $O(\\log n)$ bits and $g^t$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:09:27 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 13:53:47 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Anshu", "Anurag", ""], ["Gavinsky", "Dmitry", ""], ["Jain", "Rahul", ""], ["Kundu", "Srijita", ""], ["Lee", "Troy", ""], ["Mukhopadhyay", "Priyanka", ""], ["Santha", "Miklos", ""], ["Sanyal", "Swagato", ""]]}, {"id": "1706.00617", "submitter": "Christophe Paul", "authors": "Florian Barbero, Christophe Paul, Micha{\\l} Pilipczuk", "title": "Exploring the complexity of layout parameters in tournaments and\n  semi-complete digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple digraph is semi-complete if for any two of its vertices $u$ and $v$,\nat least one of the arcs $(u,v)$ and $(v,u)$ is present. We study the\ncomplexity of computing two layout parameters of semi-complete digraphs:\ncutwidth and optimal linear arrangement (OLA). We prove that: (1) Both\nparameters are $\\mathsf{NP}$-hard to compute and the known exact and\nparameterized algorithms for them have essentially optimal running times,\nassuming the Exponential Time Hypothesis; (2) The cutwidth parameter admits a\nquadratic Turing kernel, whereas it does not admit any polynomial kernel unless\n$\\mathsf{NP}\\subseteq \\mathsf{coNP}/\\textrm{poly}$. By contrast, OLA admits a\nlinear kernel. These results essentially complete the complexity analysis of\ncomputing cutwidth and OLA on semi-complete digraphs. Our techniques can be\nalso used to analyze the sizes of minimal obstructions for having small\ncutwidth under the induced subdigraph relation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 10:23:12 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Barbero", "Florian", ""], ["Paul", "Christophe", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1706.00911", "submitter": "Mehdy Roayaei", "authors": "Mehdy Roayaei, MohammadReza Razzazi", "title": "Inferring protein-protein interaction and protein-DNA interaction\n  directions based on cause-effect pairs in undirected and mixed networks", "comments": "11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: Given an undirected (mixed) network and a\nset of ordered source-target, or cause-effect pairs, direct all edges so as to\nmaximize the number of pairs that admit a directed source-target path. This is\ncalled maximum graph orientation problem, and has applications in understanding\ninteractions in protein-protein interaction networks and protein-DNA\ninteraction networks. We have studied the problem on both undirected and mixed\nnetworks. In the undirected case, we determine the parameterized complexity of\nthe problem (for non-fixed and fixed paths) with respect to the number of\nsatisfied pairs, which has been an open problem. Also, we present an exact\nalgorithm which outperforms the previous algorithms on trees with bounded\nnumber of leaves. In addition, we present a parameterized-approximation\nalgorithm with respect to a parameter named the number of backbones of a tree.\nIn the mixed case, we present polynomial-time algorithms for the problem on\npaths and cycles, and an FPT-algorithm based on the combined parameter the\nnumber of arcs and the number of pairs on general graphs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 08:16:01 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Roayaei", "Mehdy", ""], ["Razzazi", "MohammadReza", ""]]}, {"id": "1706.01627", "submitter": "Silv\\`ere Gangloff", "authors": "Silv\\`ere Gangloff (I2M), Mathieu Sablik (IMT)", "title": "Quantified block gluing, aperiodicity and entropy of multidimensional\n  SFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is possible to define mixing properties for subshifts according to the\nintensity which allows to concatenate two rectangular blocks. We study the\ninterplay between this intensity and computational properties. In particular we\nprove that there exists linearly block gluing subshift of finite type which are\naperiodic and that all right-recursively enumerable positive number can be\nrealized as entropy of linearly block gluing Z 2-subshift of finite type. Like\nlinearly block gluing imply transitivity, this last point answer a question\nasked in [HM10] about the characterization of the entropy of transitive\nsubshift of finite type.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 06:57:34 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 18:59:00 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 06:36:15 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Gangloff", "Silv\u00e8re", "", "I2M"], ["Sablik", "Mathieu", "", "IMT"]]}, {"id": "1706.02200", "submitter": "Rodolphe Giroudeau", "authors": "Rodolphe Giroudeau (MAORE), Jean-Claude K\\\"onig (MAORE), Benoit\n  Darties (Le2i, MAORE), Gilles Simonin", "title": "Bounds and approximation results for scheduling coupled-tasks with\n  compatibility constraints", "comments": null, "journal-ref": "15th International Conference on Project Management and\n  Scheduling, pp.94-97, 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to propose some lower and upper bounds for the\ncoupled-tasks scheduling problem in presence of compatibility constraints\naccording to classical complexity hypothesis ($\\mathcal{P} \\neq \\mathcal{NP}$,\n$\\mathcal{ETH}$). Moreover, we develop an efficient polynomial-time\napproximation algorithm for the specific case for which the topology describing\nthe compatibility constraints is a quasi split-graph.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:14:28 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"], ["Darties", "Benoit", "", "Le2i, MAORE"], ["Simonin", "Gilles", ""]]}, {"id": "1706.02202", "submitter": "Benoit Darties", "authors": "Gilles Simonin (MAORE), Benoit Darties (Le2i, MAORE), Rodolphe\n  Giroudeau (MAORE), Jean-Claude K\\\"onig (MAORE)", "title": "Isomorphic coupled-task scheduling problem with compatibility\n  constraints on a single processor", "comments": null, "journal-ref": "Journal of Scheduling, Springer Verlag, 2011, 14 (5), pp.501-509", "doi": "10.1007/s10951-010-0193-x", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem presented in this paper is a generalization of the usual\ncoupled-tasks scheduling problem in presence of compatibility constraints. The\nreason behind this study is the data acquisition problem for a submarine\ntorpedo. We investigate a particular configuration for coupled tasks (any task\nis divided into two sub-tasks separated by an idle time), in which the idle\ntime of a coupled task is equal to the sum of durations of its two sub-tasks.\nWe prove -completeness of the minimization of the schedule length, we show that\nfinding a solution to our problem amounts to solving a graph problem, which in\nitself is close to the minimum-disjoint-path cover (min-DCP) problem. We design\na (3a+2b)/(2a+2b)-approximation, where a and b (the processing time of the two\nsub-tasks) are two input data such as a>b>0, and that leads to a ratio between\n3/2 and 5/4. Using a polynomial-time algorithm developed for some class of\ngraph of min-DCP, we show that the ratio decreases to 1.37 .\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:14:50 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Simonin", "Gilles", "", "MAORE"], ["Darties", "Benoit", "", "Le2i, MAORE"], ["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"]]}, {"id": "1706.02205", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer and T. J. Sullivan and Houman Owhadi", "title": "Compression, inversion, and approximate PCA of dense kernel matrices at\n  near-linear computational complexity", "comments": "52 pages. A high level summary of this work can be found under\n  https://f-t-s.github.io/projects/cholesky/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.DS cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense kernel matrices $\\Theta \\in \\mathbb{R}^{N \\times N}$ obtained from\npoint evaluations of a covariance function $G$ at locations $\\{ x_{i} \\}_{1\n\\leq i \\leq N} \\subset \\mathbb{R}^{d}$ arise in statistics, machine learning,\nand numerical analysis. For covariance functions that are Green's functions of\nelliptic boundary value problems and homogeneously-distributed sampling points,\nwe show how to identify a subset $S \\subset \\{ 1 , \\dots , N \\}^2$, with $\\# S\n= O ( N \\log (N) \\log^{d} ( N /\\epsilon ) )$, such that the zero fill-in\nincomplete Cholesky factorisation of the sparse matrix $\\Theta_{ij} 1_{( i, j )\n\\in S}$ is an $\\epsilon$-approximation of $\\Theta$. This factorisation can\nprovably be obtained in complexity $O ( N \\log( N ) \\log^{d}( N /\\epsilon) )$\nin space and $O ( N \\log^{2}( N ) \\log^{2d}( N /\\epsilon) )$ in time, improving\nupon the state of the art for general elliptic operators; we further present\nnumerical evidence that $d$ can be taken to be the intrinsic dimension of the\ndata set rather than that of the ambient space. The algorithm only needs to\nknow the spatial configuration of the $x_{i}$ and does not require an analytic\nrepresentation of $G$. Furthermore, this factorization straightforwardly\nprovides an approximate sparse PCA with optimal rate of convergence in the\noperator norm. Hence, by using only subsampling and the incomplete Cholesky\nfactorization, we obtain, at nearly linear complexity, the compression,\ninversion and approximate PCA of a large class of covariance matrices. By\ninverting the order of the Cholesky factorization we also obtain a solver for\nelliptic PDE with complexity $O ( N \\log^{d}( N /\\epsilon) )$ in space and $O (\nN \\log^{2d}( N /\\epsilon) )$ in time, improving upon the state of the art for\ngeneral elliptic operators.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:26:14 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 22:22:57 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 19:36:49 GMT"}, {"version": "v4", "created": "Wed, 6 May 2020 20:11:23 GMT"}, {"version": "v5", "created": "Fri, 30 Oct 2020 18:34:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Sullivan", "T. J.", ""], ["Owhadi", "Houman", ""]]}, {"id": "1706.02207", "submitter": "Adi Shraibman", "authors": "Nati Linial and and Toniann Pitassi and Adi Shraibman", "title": "On The Communication Complexity of High-Dimensional Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multiparty communication complexity of high dimensional\npermutations, in the Number On the Forehead (NOF) model. This model is due to\nChandra, Furst and Lipton (CFL) who also gave a nontrivial protocol for the\nExactly-n problem where three players receive integer inputs and need to decide\nif their inputs sum to a given integer $n$. There is a considerable body of\nliterature dealing with the same problem, where $(\\mathbb{N},+)$ is replaced by\nsome other abelian group. Our work can be viewed as a far-reaching extension of\nthis line of work.\n  We show that the known lower bounds for that group-theoretic problem apply to\nall high dimensional permutations. We introduce new proof techniques that\nappeal to recent advances in Additive Combinatorics and Ramsey theory. We\nreveal new and unexpected connections between the NOF communication complexity\nof high dimensional permutations and a variety of well known and thoroughly\nstudied problems in combinatorics.\n  Previous protocols for Exactly-n all rely on the construction of large sets\nof integers without a 3-term arithmetic progression. No direct algorithmic\nprotocol was previously known for the problem, and we provide the first such\nalgorithm. This suggests new ways to significantly improve the CFL protocol.\n  Many new open questions are presented throughout.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:28:30 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 19:29:44 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 11:27:36 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Linial", "Nati", ""], ["Pitassi", "and Toniann", ""], ["Shraibman", "Adi", ""]]}, {"id": "1706.02214", "submitter": "Benoit Darties", "authors": "Benoit Darties (Le2i), Rodolphe Giroudeau (MAORE), Jean-Claude K\\\"onig\n  (MAORE), Gilles Simonin", "title": "Some complexity and approximation results for coupled-tasks scheduling\n  problem according to topology", "comments": null, "journal-ref": "RAIRO - Operations Research, EDP Sciences, 2016, 50, pp.781 - 795", "doi": "10.1051/ro/2016034", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the makespan minimization coupled-tasks problem in presence of\ncompatibility constraints with a specified topology. In particular, we focus on\nstretched coupled-tasks, i.e. coupled-tasks having the same sub-tasks execution\ntime and idle time duration. We study several problems in framework of classic\ncomplexity and approximation for which the compatibility graph is bipartite\n(star, chain,. . .). In such a context, we design some efficient\npolynomial-time approximation algorithms for an intractable scheduling problem\naccording to some parameters.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:37:35 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Darties", "Benoit", "", "Le2i"], ["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"], ["Simonin", "Gilles", ""]]}, {"id": "1706.02277", "submitter": "Adi Shraibman", "authors": "Adi Shraibman", "title": "A Note on Multiparty Communication Complexity and the Hales-Jewett\n  Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For integers $n$ and $k$, the density Hales-Jewett number $c_{n,k}$ is\ndefined as the maximal size of a subset of $[k]^n$ that contains no\ncombinatorial line. We show that for $k \\ge 3$ the density Hales-Jewett number\n$c_{n,k}$ is equal to the maximal size of a cylinder intersection in the\nproblem $Part_{n,k}$ of testing whether $k$ subsets of $[n]$ form a partition.\nIt follows that the communication complexity, in the Number On the Forehead\n(NOF) model, of $Part_{n,k}$, is equal to the minimal size of a partition of\n$[k]^n$ into subsets that do not contain a combinatorial line. Thus, the bound\nin \\cite{chattopadhyay2007languages} on $Part_{n,k}$ using the Hales-Jewett\ntheorem is in fact tight, and the density Hales-Jewett number can be thought of\nas a quantity in communication complexity. This gives a new angle to this well\nstudied quantity.\n  As a simple application we prove a lower bound on $c_{n,k}$, similar to the\nlower bound in \\cite{polymath2010moser} which is roughly $c_{n,k}/k^n \\ge\n\\exp(-O(\\log n)^{1/\\lceil \\log_2 k\\rceil})$. This lower bound follows from a\nprotocol for $Part_{n,k}$. It is interesting to better understand the\ncommunication complexity of $Part_{n,k}$ as this will also lead to the better\nunderstanding of the Hales-Jewett number. The main purpose of this note is to\nmotivate this study.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:39:28 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 16:13:12 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Shraibman", "Adi", ""]]}, {"id": "1706.03451", "submitter": "Dejan Delic", "authors": "Dejan Deli\\'c", "title": "Constraint Satisfaction Problem Dichotomy for Finite Templates: a Proof\n  Via Consistency Checks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central problems in the study of parametrized constraint\nsatisfaction problems is the Dichotomy Conjecture by T. Feder and M. Vardi\nstating that the constraint satisfaction problem (CSP) over a fixed, finite\nconstraint language is either solvable in polynomial time or\n\\textsc{NP}-complete. The conjecture was verified in certain special cases\n(domains with a relatively small number of elements, constraint languages\ncontaining all unary relations, etc.) In this article, we present a proof of\nthe Dichotomy Conjecture via local consistency and AF- consistency checks. In\nfact, we show that, for every Taylor domain, which is\n$(2\\lceil\\frac{K}{2}\\rceil,3\\lceil\\frac{K}{2}\\rceil)$-consistent, where $K$ is\nthe largest arity of a relation in the constraint language, we can define\npolynomially many proper subinstances such that, the original instance of the\nCSP is solvable if, and only if, the problem has a solution in one of those\nsubinstances.. Finally, a solution is constructed using the combination of SLAC\n(Singleton Linear Arc Consistency), introduced by M. Kozik, and AF-consistency.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 03:30:39 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 19:25:14 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 13:53:21 GMT"}, {"version": "v4", "created": "Mon, 4 Sep 2017 21:52:17 GMT"}, {"version": "v5", "created": "Mon, 25 Sep 2017 04:21:14 GMT"}, {"version": "v6", "created": "Sat, 9 Dec 2017 22:47:27 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Deli\u0107", "Dejan", ""]]}, {"id": "1706.03750", "submitter": "Konrad Dabrowski", "authors": "Konrad K. Dabrowski and Dani\\\"el Paulusma", "title": "Contracting Bipartite Graphs to Paths and Cycles", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing if a given graph $G$ contains the $k$-vertex path $P_k$ as a minor or\nas an induced minor is trivial for every fixed integer $k\\geq 1$. However, the\nsituation changes for the problem of checking if a graph can be modified into\n$P_k$ by using only edge contractions. In this case the problem is known to be\nNP-complete even if $k=4$. This led to an intensive investigation for testing\ncontractibility on restricted graph classes. We focus on bipartite graphs.\nHeggernes, van 't Hof, L\\'{e}v\\^{e}que and Paul proved that the problem stays\nNP-complete for bipartite graphs if $k=6$. We strengthen their result from\n$k=6$ to $k=5$. We also show that the problem of contracting a bipartite graph\nto the $6$-vertex cycle $C_6$ is NP-complete. The cyclicity of a graph is the\nlength of the longest cycle the graph can be contracted to. As a consequence of\nour second result, determining the cyclicity of a bipartite graph is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:36:36 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Dabrowski", "Konrad K.", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1706.03951", "submitter": "Shmuel Onn", "authors": "Antoine Deza, Asaf Levin, Syed M. Meesum, Shmuel Onn", "title": "Optimization over Degree Sequences", "comments": null, "journal-ref": "SIAM Journal on Discrete Mathematics, 32:2067--2079, 2018", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem of optimizing arbitrary functions over\ndegree sequences of hypergraphs and multihypergraphs. We show that over\nmultihypergraphs the problem can be solved in polynomial time. For hypergraphs,\nwe show that deciding if a given sequence is the degree sequence of a\n3-hypergraph is NP-complete, thereby solving a 30 year long open problem. This\nimplies that optimization over hypergraphs is hard already for simple concave\nfunctions. In contrast, we show that for graphs, if the functions at vertices\nare the same, then the problem is polynomial time solvable. We also provide\npositive results for convex optimization over multihypergraphs and graphs and\nexploit connections to degree sequence polytopes and threshold graphs. We then\nelaborate on connections to the emerging theory of shifted combinatorial\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:21:50 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 10:44:01 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Deza", "Antoine", ""], ["Levin", "Asaf", ""], ["Meesum", "Syed M.", ""], ["Onn", "Shmuel", ""]]}, {"id": "1706.04225", "submitter": "Joel Friedman", "authors": "Joel Friedman", "title": "Inner Rank and Lower Bounds for Matrix Multiplication", "comments": "Errors in many of the results (starting with those of Section 4) due\n  to an \"exchange of indices.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a notion of {\\em inner rank} as a tool for obtaining lower bounds\non the rank of matrix multiplication tensors. We use it to give a short proof\nthat the border rank (and therefore rank) of the tensor associated with\n$n\\times n$ matrix multiplication over an arbitrary field is at least\n$2n^2-n+1$. While inner rank does not provide improvements to currently known\nlower bounds, we argue that this notion merits further study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 19:04:00 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 18:39:49 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Friedman", "Joel", ""]]}, {"id": "1706.04582", "submitter": "David Narv\\'aez", "authors": "Lane A. Hemaspaandra and David E. Narv\\'aez", "title": "Existence versus Exploitation: The Opacity of Backbones and Backdoors\n  Under a Weak Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoors and backbones of Boolean formulas are hidden structural properties.\nA natural goal, already in part realized, is that solver algorithms seek to\nobtain substantially better performance by exploiting these structures.\n  However, the present paper is not intended to improve the performance of SAT\nsolvers, but rather is a cautionary paper. In particular, the theme of this\npaper is that there is a potential chasm between the existence of such\nstructures in the Boolean formula and being able to effectively exploit them.\nThis does not mean that these structures are not useful to solvers. It does\nmean that one must be very careful not to assume that it is computationally\neasy to go from the existence of a structure to being able to get one's hands\non it and/or being able to exploit the structure.\n  For example, in this paper we show that, under the assumption that P $\\neq$\nNP, there are easily recognizable families of Boolean formulas with strong\nbackdoors that are easy to find, yet for which it is hard (in fact,\nNP-complete) to determine whether the formulas are satisfiable. We also show\nthat, also under the assumption P $\\neq$ NP, there are easily recognizable sets\nof Boolean formulas for which it is hard (in fact, NP-complete) to determine\nwhether they have a large backbone.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:46:01 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 21:45:55 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 17:54:53 GMT"}, {"version": "v4", "created": "Tue, 24 Oct 2017 15:51:57 GMT"}, {"version": "v5", "created": "Sat, 10 Mar 2018 16:44:58 GMT"}, {"version": "v6", "created": "Tue, 24 Apr 2018 00:47:04 GMT"}, {"version": "v7", "created": "Tue, 3 Jul 2018 18:44:09 GMT"}, {"version": "v8", "created": "Thu, 1 Nov 2018 19:06:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Hemaspaandra", "Lane A.", ""], ["Narv\u00e1ez", "David E.", ""]]}, {"id": "1706.04641", "submitter": "Dhiraj Holden", "authors": "Shafi Goldwasser and Ofer Grossman and Dhiraj Holden", "title": "Pseudo-deterministic Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce pseudo-deterministic interactive proofs (psdAM): interactive\nproof systems for search problems where the verifier is guaranteed with high\nprobability to output the same output on different executions. As in the case\nwith classical interactive proofs, the verifier is a probabilistic polynomial\ntime algorithm interacting with an untrusted powerful prover.\n  We view pseudo-deterministic interactive proofs as an extension of the study\nof pseudo-deterministic randomized polynomial time algorithms: the goal of the\nlatter is to find canonical solutions to search problems whereas the goal of\nthe former is to prove that a solution to a search problem is canonical to a\nprobabilistic polynomial time verifier. Alternatively, one may think of the\npowerful prover as aiding the probabilistic polynomial time verifier to find\ncanonical solutions to search problems, with high probability over the\nrandomness of the verifier. The challenge is that pseudo-determinism should\nhold not only with respect to the randomness, but also with respect to the\nprover: a malicious prover should not be able to cause the verifier to output a\nsolution other than the unique canonical one.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 19:14:27 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Goldwasser", "Shafi", ""], ["Grossman", "Ofer", ""], ["Holden", "Dhiraj", ""]]}, {"id": "1706.05193", "submitter": "Sebastien Tixeuil", "authors": "Arnaud Sangnier (IRIF), Nathalie Sznajder (MoVe), Maria Potop-Butucaru\n  (NPA, LINCS), S\\'ebastien Tixeuil (NPA, IUF, LINCS)", "title": "Parameterized Verification of Algorithms for Oblivious Robots on a Ring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study verification problems for autonomous swarms of mobile robots that\nself-organize and cooperate to solve global objectives. In particular, we focus\nin this paper on the model proposed by Suzuki and Yamashita of anonymous robots\nevolving in a discrete space with a finite number of locations (here, a ring).\nA large number of algorithms have been proposed working for rings whose size is\nnot a priori fixed and can be hence considered as a parameter. Handmade\ncorrectness proofs of these algorithms have been shown to be error-prone, and\nrecent attention had been given to the application of formal methods to\nautomatically prove those. Our work is the first to study the verification\nproblem of such algorithms in the parameter-ized case. We show that safety and\nreachability problems are undecidable for robots evolving asynchronously. On\nthe positive side, we show that safety properties are decidable in the\nsynchronous case, as well as in the asynchronous case for a particular class of\nalgorithms. Several properties on the protocol can be decided as well. Decision\nprocedures rely on an encoding in Presburger arithmetics formulae that can be\nverified by an SMT-solver. Feasibility of our approach is demonstrated by the\nencoding of several case studies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 09:38:21 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Sangnier", "Arnaud", "", "IRIF"], ["Sznajder", "Nathalie", "", "MoVe"], ["Potop-Butucaru", "Maria", "", "NPA, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF, LINCS"]]}, {"id": "1706.05295", "submitter": "Eun Jee Lee", "authors": "Emmanuel Abbe, Sanjeev Kulkarni, Eun Jee Lee", "title": "Nonbacktracking Bounds on the Influence in Independent Cascade Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops upper and lower bounds on the influence measure in a\nnetwork, more precisely, the expected number of nodes that a seed set can\ninfluence in the independent cascade model. In particular, our bounds exploit\nnonbacktracking walks, Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and\nare computed by message passing implementation. Nonbacktracking walks have\nrecently allowed for headways in community detection, and this paper shows that\ntheir use can also impact the influence computation. Further, we provide a knob\nto control the trade-off between the efficiency and the accuracy of the bounds.\nFinally, the tightness of the bounds is illustrated with simulations on various\nnetwork models.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 00:09:46 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 05:48:49 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Kulkarni", "Sanjeev", ""], ["Lee", "Eun Jee", ""]]}, {"id": "1706.05556", "submitter": "Erik Waingarten", "authors": "Xi Chen and Rocco A. Servedio and Li-Yang Tan and Erik Waingarten", "title": "Adaptivity is exponentially powerful for testing monotonicity of\n  halfspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a $\\mathrm{poly}(\\log n, 1/\\epsilon)$-query adaptive algorithm for\ntesting whether an unknown Boolean function $f: \\{-1,1\\}^n \\to \\{-1,1\\}$, which\nis promised to be a halfspace, is monotone versus $\\epsilon$-far from monotone.\nSince non-adaptive algorithms are known to require almost $\\Omega(n^{1/2})$\nqueries to test whether an unknown halfspace is monotone versus far from\nmonotone, this shows that adaptivity enables an exponential improvement in the\nquery complexity of monotonicity testing for halfspaces.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 16:11:30 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Chen", "Xi", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""], ["Waingarten", "Erik", ""]]}, {"id": "1706.05738", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Alistair Stewart", "title": "Fourier-Based Testing for Families of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the general problem of testing whether an unknown distribution\nbelongs to a specified family of distributions. More specifically, given a\ndistribution family $\\mathcal{P}$ and sample access to an unknown discrete\ndistribution $\\mathbf{P}$, we want to distinguish (with high probability)\nbetween the case that $\\mathbf{P} \\in \\mathcal{P}$ and the case that\n$\\mathbf{P}$ is $\\epsilon$-far, in total variation distance, from every\ndistribution in $\\mathcal{P}$. This is the prototypical hypothesis testing\nproblem that has received significant attention in statistics and, more\nrecently, in theoretical computer science.\n  The sample complexity of this general inference task depends on the\nunderlying family $\\mathcal{P}$. The gold standard in distribution property\ntesting is to design sample-optimal and computationally efficient algorithms\nfor this task. The main contribution of this work is a simple and general\ntesting technique that is applicable to all distribution families whose Fourier\nspectrum satisfies a certain approximate sparsity property. To the best of our\nknowledge, ours is the first use of the Fourier transform in the context of\ndistribution testing.\n  We apply our Fourier-based framework to obtain near sample-optimal and\ncomputationally efficient testers for the following fundamental distribution\nfamilies: Sums of Independent Integer Random Variables (SIIRVs), Poisson\nMultinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For\nthe first two, ours are the first non-trivial testers in the literature, vastly\ngeneralizing previous work on testing Poisson Binomial Distributions. For the\nthird, our tester improves on prior work in both sample and time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 22:28:20 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:05:23 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Stewart", "Alistair", ""]]}, {"id": "1706.05902", "submitter": "Victor Lagerkvist Mr", "authors": "Peter Jonsson, Victor Lagerkvist, Biman Roy", "title": "Time Complexity of Constraint Satisfaction via Universal Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential-time hypothesis (ETH) states that 3-SAT is not solvable in\nsubexponential time, i.e. not solvable in O(c^n) time for arbitrary c > 1,\nwhere n denotes the number of variables. Problems like k-SAT can be viewed as\nspecial cases of the constraint satisfaction problem (CSP), which is the\nproblem of determining whether a set of constraints is satisfiable. In this\npaper we study thef worst-case time complexity of NP-complete CSPs. Our main\ninterest is in the CSP problem parameterized by a constraint language Gamma\n(CSP(Gamma)), and how the choice of Gamma affects the time complexity. It is\nbelieved that CSP(Gamma) is either tractable or NP-complete, and the algebraic\nCSP dichotomy conjecture gives a sharp delineation of these two classes based\non algebraic properties of constraint languages. Under this conjecture and the\nETH, we first rule out the existence of subexponential algorithms for\nfinite-domain NP-complete CSP(Gamma) problems. This result also extends to\ncertain infinite-domain CSPs and structurally restricted CSP(Gamma) problems.\nWe then begin a study of the complexity of NP-complete CSPs where one is\nallowed to arbitrarily restrict the values of individual variables, which is a\nvery well-studied subclass of CSPs. For such CSPs with finite domain D, we\nidentify a relation SD such that (1) CSP({SD}) is NP-complete and (2) if\nCSP(Gamma) over D is NP-complete and solvable in O(c^n) time, then CSP({SD}) is\nsolvable in O(c^n) time, too. Hence, the time complexity of CSP({SD}) is a\nlower bound for all CSPs of this particular kind. We also prove that the\ncomplexity of CSP({SD}) is decreasing when |D| increases, unless the ETH is\nfalse. This implies, for instance, that for every c>1 there exists a\nfinite-domain Gamma such that CSP(Gamma) is NP-complete and solvable in O(c^n)\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 12:26:49 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Jonsson", "Peter", ""], ["Lagerkvist", "Victor", ""], ["Roy", "Biman", ""]]}, {"id": "1706.05906", "submitter": "Markus Schmid", "authors": "Katrin Casel, Henning Fernau, Alexander Grigoriev, Markus L. Schmid\n  and Sue Whitesides", "title": "Combinatorial Properties and Recognition of Unit Square Visibility\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit square (grid) visibility graphs (USV and USGV, resp.) are described by\naxis-parallel visibility between unit squares placed (on integer grid\ncoordinates) in the plane. We investigate combinatorial properties of these\ngraph classes and the hardness of variants of the recognition problem, i.e.,\nthe problem of representing USGV with fixed visibilities within small area and,\nfor USV, the general recognition problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 12:48:09 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 09:57:39 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Casel", "Katrin", ""], ["Fernau", "Henning", ""], ["Grigoriev", "Alexander", ""], ["Schmid", "Markus L.", ""], ["Whitesides", "Sue", ""]]}, {"id": "1706.05941", "submitter": "Victor Lagerkvist Dr.", "authors": "Victor Lagerkvist, Magnus Wahlstr\\\"om", "title": "Kernelization of Constraint Satisfaction Problems: A Study through\n  Universal Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kernelization algorithm for a computational problem is a procedure which\ncompresses an instance into an equivalent instance whose size is bounded with\nrespect to a complexity parameter. For the Boolean satisfiability problem\n(SAT), and the constraint satisfaction problem (CSP), there exist many results\nconcerning upper and lower bounds for kernelizability of specific problems, but\nit is safe to say that we lack general methods to determine whether a given SAT\nproblem admits a kernel of a particular size. This could be contrasted to the\ncurrently flourishing research program of determining the classical complexity\nof finite-domain CSP problems, where almost all non-trivial tractable classes\nhave been identified with the help of algebraic properties. In this paper, we\ntake an algebraic approach to the problem of characterizing the kernelization\nlimits of NP-hard SAT and CSP problems, parameterized by the number of\nvariables. Our main focus is on problems admitting linear kernels, as has,\nsomewhat surprisingly, previously been shown to exist. We show that a CSP\nproblem has a kernel with O(n) constraints if it can be embedded (via a domain\nextension) into a CSP problem which is preserved by a Maltsev operation. We\nalso study extensions of this towards SAT and CSP problems with kernels with\nO(n^c) constraints, c>1, based on embeddings into CSP problems preserved by a\nk-edge operation, k > c. These results follow via a variant of the celebrated\nfew subpowers algorithm. In the complementary direction, we give indication\nthat the Maltsev condition might be a complete characterization of SAT problems\nwith linear kernels, by showing that an algebraic condition that is shared by\nall problems with a Maltsev embedding is also necessary for the existence of a\nlinear kernel unless NP is included in co-NP/poly.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 13:46:54 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Lagerkvist", "Victor", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1706.06407", "submitter": "Aviad Rubinstein", "authors": "Amir Abboud, Aviad Rubinstein and Ryan Williams", "title": "Distributed PCP Theorems for Hardness of Approximation in P", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new distributed model of probabilistically checkable proofs\n(PCP). A satisfying assignment $x \\in \\{0,1\\}^n$ to a CNF formula $\\varphi$ is\nshared between two parties, where Alice knows $x_1, \\dots, x_{n/2}$, Bob knows\n$x_{n/2+1},\\dots,x_n$, and both parties know $\\varphi$. The goal is to have\nAlice and Bob jointly write a PCP that $x$ satisfies $\\varphi$, while\nexchanging little or no information. Unfortunately, this model as-is does not\nallow for nontrivial query complexity. Instead, we focus on a non-deterministic\nvariant, where the players are helped by Merlin, a third party who knows all of\n$x$.\n  Using our framework, we obtain, for the first time, PCP-like reductions from\nthe Strong Exponential Time Hypothesis (SETH) to approximation problems in P.\nIn particular, under SETH we show that there are no truly-subquadratic\napproximation algorithms for Bichromatic Maximum Inner Product over\n{0,1}-vectors, Bichromatic LCS Closest Pair over permutations, Approximate\nRegular Expression Matching, and Diameter in Product Metric. All our\ninapproximability factors are nearly-tight. In particular, for the first two\nproblems we obtain nearly-polynomial factors of $2^{(\\log n)^{1-o(1)}}$; only\n$(1+o(1))$-factor lower bounds (under SETH) were known before.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:16:30 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 20:53:15 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Abboud", "Amir", ""], ["Rubinstein", "Aviad", ""], ["Williams", "Ryan", ""]]}, {"id": "1706.06467", "submitter": "Marie MacCaig Dr", "authors": "Stephane Gaubert and Marie MacCaig", "title": "Approximating the Volume of Tropical Polytopes is Difficult", "comments": null, "journal-ref": "International Journal of Algebra and Computation, 29(02):357--389,\n  2019", "doi": "10.1142/S0218196719500061", "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of counting the number of integer points in\ntropical polytopes, and the complexity of calculating their volume. We study\nthe tropical analogue of the outer parallel body and establish bounds for its\nvolume. We deduce that there is no approximation algorithm of factor\n$\\alpha=2^{\\text{poly}(m,n)}$ for the volume of a tropical polytope given by\n$n$ vertices in a space of dimension $m$, unless P$=$NP. Neither is there such\nan approximation algorithm for counting the number of integer points in\ntropical polytopes described by vertices. If follows that approximating these\nvalues for tropical polytopes is more difficult than for classical polytopes.\nOur proofs use a reduction from the problem of calculating the tropical rank.\nFor tropical polytopes described by inequalities we prove that counting the\nnumber of integer points and calculating the volume are $\\#$P-hard.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:23:32 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Gaubert", "Stephane", ""], ["MacCaig", "Marie", ""]]}, {"id": "1706.06708", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Sarah Eisenstat, Mikhail Rudoy", "title": "Solving the Rubik's Cube Optimally is NP-complete", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2018.24", "report-no": null, "categories": "cs.CC cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that optimally solving an $n \\times n \\times n$\nRubik's Cube is NP-complete by reducing from the Hamiltonian Cycle problem in\nsquare grid graphs. This improves the previous result that optimally solving an\n$n \\times n \\times n$ Rubik's Cube with missing stickers is NP-complete. We\nprove this result first for the simpler case of the Rubik's Square---an $n\n\\times n \\times 1$ generalization of the Rubik's Cube---and then proceed with a\nsimilar but more complicated proof for the Rubik's Cube case.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 00:02:45 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 07:33:24 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Demaine", "Erik D.", ""], ["Eisenstat", "Sarah", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.06900", "submitter": "Michal Parnas", "authors": "Michal Parnas and Adi Shraibman", "title": "The Augmentation Property of Binary Matrices for the Binary and Boolean\n  Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the Augmentation property for binary matrices with respect to\ndifferent rank functions. A matrix $A$ has the Augmentation property for a\ngiven rank function, if for any subset of column vectors $x_1,...,x_t$ for for\nwhich the rank of $A$ does not increase when augmented separately with each of\nthe vectors $x_i$, $1\\leq i \\leq t$, it also holds that the rank does not\nincrease when augmenting $A$ with all vectors $x_1,...,x_t$ simultaneously.\nThis property holds trivially for the usual linear rank over the reals, but as\nwe show, things change significantly when considering the binary and boolean\nrank of a matrix.\n  We prove a necessary and sufficient condition for this property to hold under\nthe binary and boolean rank of binary matrices. Namely, a matrix has the\nAugmentation property for these rank functions if and only if it has a unique\nbase that spans all other bases of the matrix with respect to the given rank\nfunction. For the binary rank, we also present a concrete characterization of a\nfamily of matrices that has the Augmentation property. This characterization is\nbased on the possible types of linear dependencies between rows of $V$, in\noptimal binary decompositions of the matrix as $A=U\\cdot V$.\n  Furthermore, we use the Augmentation property to construct simple families of\nmatrices, for which there is a gap between their real and binary rank and\nbetween their real and boolean rank.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:35:40 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Parnas", "Michal", ""], ["Shraibman", "Adi", ""]]}, {"id": "1706.07406", "submitter": "Noam Goldberg", "authors": "Noam Goldberg and Gabor Rudolf", "title": "On the Complexity and Approximation of the Maximum Expected Value\n  All-or-Nothing Subset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unconstrained nonlinear binary optimization problem of selecting a maximum\nexpected value subset of items is considered. Each item is associated with a\nprofit and probability. Each of the items succeeds or fails independently with\nthe given probabilities, and the profit is obtained in the event that all\nselected items succeed. The objective is to select a subset that maximizes the\ntotal value times the product of probabilities of the chosen items. The problem\nis proven NP-hard by a nontrivial reduction from subset sum. Then we develop a\nfully polynomial time approximation scheme (FPTAS) for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:19:45 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Goldberg", "Noam", ""], ["Rudolf", "Gabor", ""]]}, {"id": "1706.07890", "submitter": "Andrew Drucker", "authors": "Andrew Drucker", "title": "A Note on a Communication Game", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a communication game, and a conjecture about this game, whose\nproof would imply the well-known Sensitivity Conjecture asserting a polynomial\nrelation between sensitivity and block sensitivity for Boolean functions. The\nauthor defined this game and observed the connection in Dec. 2013 - Jan. 2014.\nThe game and connection were independently discovered by Gilmer, Kouck\\'y, and\nSaks, who also established further results about the game (not proved by us)\nand published their results in ITCS '15 [GKS15].\n  This note records our independent work, including some observations that did\nnot appear in [GKS15]. Namely, the main conjecture about this communication\ngame would imply not only the Sensitivity Conjecture, but also a stronger\nhypothesis raised by Chung, F\\\"uredi, Graham, and Seymour [CFGS88]; and,\nanother related conjecture we pose about a \"query-bounded\" variant of our\ncommunication game would suffice to answer a question of Aaronson, Ambainis,\nBalodis, and Bavarian [AABB14] about the query complexity of the \"Weak Parity\"\nproblem---a question whose resolution was previously shown by [AABB14] to\nfollow from a proof of the Chung et al. hypothesis.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 01:36:33 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Drucker", "Andrew", ""]]}, {"id": "1706.07900", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Mikhail Rudoy", "title": "Tree-Residue Vertex-Breaking: a new tool for proving hardness", "comments": "37 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new problem called Tree-Residue Vertex-Breaking\n(TRVB): given a multigraph $G$ some of whose vertices are marked \"breakable,\"\nis it possible to convert $G$ into a tree via a sequence of \"vertex-breaking\"\noperations (replacing a degree-$k$ breakable vertex by $k$ degree-$1$ vertices,\ndisconnecting the $k$ incident edges)?\n  We characterize the computational complexity of TRVB with any combination of\nthe following additional constraints: $G$ must be planar, $G$ must be a simple\ngraph, the degree of every breakable vertex must belong to an allowed list $B$,\nand the degree of every unbreakable vertex must belong to an allowed list $U$.\nThe two results which we expect to be most generally applicable are that (1)\nTRVB is polynomially solvable when breakable vertices are restricted to have\ndegree at most $3$; and (2) for any $k \\ge 4$, TRVB is NP-complete when the\ngiven multigraph is restricted to be planar and to consist entirely of\ndegree-$k$ breakable vertices. To demonstrate the use of TRVB, we give a simple\nproof of the known result that Hamiltonicity in max-degree-$3$ square grid\ngraphs is NP-hard.\n  We also demonstrate a connection between TRVB and the Hypergraph Spanning\nTree problem. This connection allows us to show that the Hypergraph Spanning\nTree problem in $k$-uniform $2$-regular hypergraphs is NP-complete for any $k\n\\ge 4$, even when the incidence graph of the hypergraph is planar.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 03:32:42 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 03:45:50 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 05:17:03 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.08023", "submitter": "Xu Zhiqiang", "authors": "Heng Zhou and Zhiqiang Xu", "title": "On generalizations of $p$-sets and their applications", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.CC cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $p$-set, which is in a simple analytic form, is well distributed in unit\ncubes. The well-known Weil's exponential sum theorem presents an upper bound of\nthe exponential sum over the $p$-set. Based on the result, one shows that the\n$p$-set performs well in numerical integration, in compressed sensing as well\nas in UQ. However, $p$-set is somewhat rigid since the cardinality of the\n$p$-set is a prime $p$ and the set only depends on the prime number $p$. The\npurpose of this paper is to present generalizations of $p$-sets, say\n$\\mathcal{P}_{d,p}^{{\\mathbf a},\\epsilon}$, which is more flexible.\nParticularly, when a prime number $p$ is given, we have many different choices\nof the new $p$-sets. Under the assumption that Goldbach conjecture holds, for\nany even number $m$, we present a point set, say ${\\mathcal L}_{p,q}$, with\ncardinality $m-1$ by combining two different new $p$-sets, which overcomes a\nmajor bottleneck of the $p$-set. We also present the upper bounds of the\nexponential sums over $\\mathcal{P}_{d,p}^{{\\mathbf a},\\epsilon}$ and ${\\mathcal\nL}_{p,q}$, which imply these sets have many potential applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 02:03:35 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhou", "Heng", ""], ["Xu", "Zhiqiang", ""]]}, {"id": "1706.08050", "submitter": "Matthew  Johnson", "authors": "Nina Chiarelli, Tatiana R. Hartinger, Matthew Johnson, Martin\n  Milani\\v{c}, Dani\\\"el Paulusma", "title": "Minimum Connected Transversals in Graphs: New Hardness Results and\n  Tractable Cases Using the Price of Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a systematic study in the computational complexity of the\nconnected variant of three related transversal problems: Vertex Cover, Feedback\nVertex Set, and Odd Cycle Transversal. Just like their original counterparts,\nthese variants are NP-complete for general graphs. A graph $G$ is $H$-free for\nsome graph $H$ if $G$ contains no induced subgraph isomorphic to $H$. It is\nknown that Connected Vertex Cover is NP-complete even for $H$-free graphs if\n$H$ contains a claw or a cycle. We show that the two other connected variants\nalso remain NP-complete if $H$ contains a cycle or claw. In the remaining case\n$H$ is a linear forest. We show that Connected Vertex Cover, Connected Feedback\nVertex Set, and Connected Odd Cycle Transversal are polynomial-time solvable\nfor $sP_2$-free graphs for every constant $s\\geq 1$. For proving these results\nwe use known results on the price of connectivity for vertex cover, feedback\nvertex set, and odd cycle transversal. This is the first application of the\nprice of connectivity that results in polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 08:00:56 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 09:01:08 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 15:27:57 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Chiarelli", "Nina", ""], ["Hartinger", "Tatiana R.", ""], ["Johnson", "Matthew", ""], ["Milani\u010d", "Martin", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1706.08414", "submitter": "Marc Roth", "authors": "Marc Roth", "title": "Counting Restricted Homomorphisms via M\\\"obius Inversion over Matroid\n  Lattices", "comments": "26 pages, 5 figures, ESA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework for the complexity classification of parameterized\ncounting problems that can be formulated as the summation over the numbers of\nhomomorphisms from small pattern graphs H_1,...,H_l to a big host graph G with\nthe restriction that the coefficients correspond to evaluations of the M\\\"obius\nfunction over the lattice of a graphic matroid. This generalizes the idea of\nCurticapean, Dell and Marx [STOC 17] who used a result of Lov\\'asz stating that\nthe number of subgraph embeddings from a graph H to a graph G can be expressed\nas such a sum over the lattice of partitions of H. In the first step we\nintroduce what we call graphically restricted homomorphisms that, inter alia,\ngeneralize subgraph embeddings as well as locally injective homomorphisms. We\nprovide a complete parameterized complexity dichotomy for counting such\nhomomorphisms, that is, we identify classes of patterns for which the problem\nis fixed-parameter tractable (FPT), including an algorithm, and prove that all\nother pattern classes lead to #W[1]-hard problems. The main ingredients of the\nproof are the complexity classification of linear combinations of homomorphisms\ndue to Curticapean, Dell and Marx [STOC 17] as well as a corollary of Rota's\nNBC Theorem which states that the sign of the M\\\"obius function over a\ngeometric lattice only depends on the rank of its arguments. We use the general\ntheorem to classify the complexity of counting locally injective homomorphisms\nas well as homomorphisms that are injective in the r-neighborhood for constant\nr. Furthermore, we show that the former has \"real\" FPT cases by considering the\nsubgraph counting problem restricted to trees on both sides. Finally we show\nthat the dichotomy for counting graphically restricted homomorphisms readily\nextends to so-called linear combinations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:44:48 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Roth", "Marc", ""]]}, {"id": "1706.08431", "submitter": "Ralf Rothenberger", "authors": "Tobias Friedrich, Anton Krohmer, Ralf Rothenberger, Thomas Sauerwald,\n  Andrew M. Sutton", "title": "Bounds on the Satisfiability Threshold for Power Law Distributed Random\n  SAT", "comments": "17 pages", "journal-ref": "25th Annual European Symposium on Algorithms (ESA), 2017,\n  37:1-37:15", "doi": "10.4230/LIPIcs.ESA.2017.37", "report-no": null, "categories": "cs.DM cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Propositional satisfiability (SAT) is one of the most fundamental problems in\ncomputer science. The worst-case hardness of SAT lies at the core of\ncomputational complexity theory. The average-case analysis of SAT has triggered\nthe development of sophisticated rigorous and non-rigorous techniques for\nanalyzing random structures.\n  Despite a long line of research and substantial progress, nearly all\ntheoretical work on random SAT assumes a uniform distribution on the variables.\nIn contrast, real-world instances often exhibit large fluctuations in variable\noccurrence. This can be modeled by a scale-free distribution of the variables,\nwhich results in distributions closer to industrial SAT instances.\n  We study random k-SAT on n variables, $m=\\Theta(n)$ clauses, and a power law\ndistribution on the variable occurrences with exponent $\\beta$. We observe a\nsatisfiability threshold at $\\beta=(2k-1)/(k-1)$. This threshold is tight in\nthe sense that instances with $\\beta\\le(2k-1)/(k-1)-\\varepsilon$ for any\nconstant $\\varepsilon>0$ are unsatisfiable with high probability (w.h.p.). For\n$\\beta\\geq(2k-1)/(k-1)+\\varepsilon$, the picture is reminiscent of the uniform\ncase: instances are satisfiable w.h.p. for sufficiently small constant\nclause-variable ratios $m/n$; they are unsatisfiable above a ratio $m/n$ that\ndepends on $\\beta$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:12:25 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Friedrich", "Tobias", ""], ["Krohmer", "Anton", ""], ["Rothenberger", "Ralf", ""], ["Sauerwald", "Thomas", ""], ["Sutton", "Andrew M.", ""]]}, {"id": "1706.08468", "submitter": "Marius Zimand", "authors": "Marius Zimand", "title": "Distributed compression through the lens of algorithmic information\n  theory: a primer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed compression is the task of compressing correlated data by several\nparties, each one possessing one piece of data and acting separately. The\nclassical Slepian-Wolf theorem (D. Slepian, J. K. Wolf, IEEE Transactions on\nInf. Theory, 1973) shows that if data is generated by independent draws from a\njoint distribution, that is by a memoryless stochastic process, then\ndistributed compression can achieve the same compression rates as centralized\ncompression when the parties act together. Recently, the author (M. Zimand,\nSTOC 2017) has obtained an analogue version of the Slepian-Wolf theorem in the\nframework of Algorithmic Information Theory (also known as Kolmogorov\ncomplexity). The advantage over the classical theorem, is that the AIT version\nworks for individual strings, without any assumption regarding the generative\nprocess. The only requirement is that the parties know the complexity profile\nof the input strings, which is a simple quantitative measure of the data\ncorrelation. The goal of this paper is to present in an accessible form that\nomits some technical details the main ideas from the reference (M. Zimand, STOC\n2017).\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:41:04 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zimand", "Marius", ""]]}, {"id": "1706.08786", "submitter": "Leslie Ann Goldberg", "authors": "Jacob Focke, Leslie Ann Goldberg, Stanislav Zivny", "title": "The Complexity of Counting Surjective Homomorphisms and Compactions", "comments": "Minor revisions, to appear in SIDMA", "journal-ref": "SIAM Journal on Discrete Mathematics 33(2) 1006-1043 (2019)", "doi": "10.1137/17M1153182", "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A homomorphism from a graph G to a graph H is a function from the vertices of\nG to the vertices of H that preserves edges. A homomorphism is surjective if it\nuses all of the vertices of H and it is a compaction if it uses all of the\nvertices of H and all of the non-loop edges of H. Hell and Nesetril gave a\ncomplete characterisation of the complexity of deciding whether there is a\nhomomorphism from an input graph G to a fixed graph H. A complete\ncharacterisation is not known for surjective homomorphisms or for compactions,\nthough there are many interesting results. Dyer and Greenhill gave a complete\ncharacterisation of the complexity of counting homomorphisms from an input\ngraph G to a fixed graph H. In this paper, we give a complete characterisation\nof the complexity of counting surjective homomorphisms from an input graph G to\na fixed graph H and we also give a complete characterisation of the complexity\nof counting compactions from an input graph G to a fixed graph H. In an\naddendum we use our characterisations to point out a dichotomy for the\ncomplexity of the respective approximate counting problems (in the connected\ncase).\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 11:25:11 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 09:43:07 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 10:13:29 GMT"}, {"version": "v4", "created": "Tue, 14 Nov 2017 14:00:31 GMT"}, {"version": "v5", "created": "Tue, 9 Apr 2019 07:06:30 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Focke", "Jacob", ""], ["Goldberg", "Leslie Ann", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1706.09043", "submitter": "Daniel Paulusma", "authors": "Dani\\\"el Paulusma and Christophe Picouleau and Bernard Ries", "title": "Critical Vertices and Edges in $H$-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vertex or edge in a graph is critical if its deletion reduces the chromatic\nnumber of the graph by 1. We consider the problems of deciding whether a graph\nhas a critical vertex or edge, respectively. We give a complexity dichotomy for\nboth problems restricted to $H$-free graphs, that is, graphs with no induced\nsubgraph isomorphic to $H$. Moreover, we show that an edge is critical if and\nonly if its contraction reduces the chromatic number by 1. Hence, we also\nobtain a complexity dichotomy for the problem of deciding if a graph has an\nedge whose contraction reduces the chromatic number by 1.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 20:58:26 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Paulusma", "Dani\u00ebl", ""], ["Picouleau", "Christophe", ""], ["Ries", "Bernard", ""]]}, {"id": "1706.09052", "submitter": "Daniel Paulusma", "authors": "\\\"Oznur Ya\\c{s}ar Diner and Dani\\\"el Paulusma and Christophe Picouleau\n  and Bernard Ries", "title": "Contraction and Deletion Blockers for Perfect Graphs and $H$-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem: for given integers $d$, $k$ and graph $G$,\ncan we reduce some fixed graph parameter $\\pi$ of $G$ by at least $d$ via at\nmost $k$ graph operations from some fixed set $S$? As parameters we take the\nchromatic number $\\chi$, clique number $\\omega$ and independence number\n$\\alpha$, and as operations we choose the edge contraction ec and vertex\ndeletion vd. We determine the complexity of this problem for $S=\\{\\mbox{ec}\\}$\nand $S=\\{\\mbox{vd}\\}$ and $\\pi\\in \\{\\chi,\\omega,\\alpha\\}$ for a number of\nsubclasses of perfect graphs. We use these results to determine the complexity\nof the problem for $S=\\{\\mbox{ec}\\}$ and $S=\\{\\mbox{vd}\\}$ and $\\pi\\in\n\\{\\chi,\\omega,\\alpha\\}$ restricted to $H$-free graphs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 21:16:25 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Diner", "\u00d6znur Ya\u015far", ""], ["Paulusma", "Dani\u00ebl", ""], ["Picouleau", "Christophe", ""], ["Ries", "Bernard", ""]]}, {"id": "1706.09066", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Victor A. Campos, Ana Karolinna Maia, Ignasi Sau,\n  Ana Silva", "title": "On the complexity of finding internally vertex-disjoint long directed\n  paths", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two positive integers $k$ and $\\ell$, a $(k \\times \\ell)$-spindle is the\nunion of $k$ pairwise internally vertex-disjoint directed paths with $\\ell$\narcs between two vertices $u$ and $v$. We are interested in the (parameterized)\ncomplexity of several problems consisting in deciding whether a given digraph\ncontains a subdivision of a spindle, which generalize both the Maximum Flow and\nLongest Path problems. We obtain the following complexity dichotomy: for a\nfixed $\\ell \\geq 1$, finding the largest $k$ such that an input digraph $G$\ncontains a subdivision of a $(k \\times \\ell)$-spindle is polynomial-time\nsolvable if $\\ell \\leq 3$, and NP-hard otherwise. We place special emphasis on\nfinding spindles with exactly two paths and present FPT algorithms that are\nasymptotically optimal under the ETH. These algorithms are based on the\ntechnique of representative families in matroids, and use also color-coding as\na subroutine. Finally, we study the case where the input graph is acyclic, and\npresent several algorithmic and hardness results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 22:17:20 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Campos", "Victor A.", ""], ["Maia", "Ana Karolinna", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "1706.09156", "submitter": "Lihua Lei", "authors": "Lihua Lei, Cheng Ju, Jianbo Chen and Michael I. Jordan", "title": "Non-convex Finite-Sum Optimization Via SCSG Methods", "comments": "Add Lemma B.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of algorithms, as variants of the stochastically\ncontrolled stochastic gradient (SCSG) methods (Lei and Jordan, 2016), for the\nsmooth non-convex finite-sum optimization problem. Assuming the smoothness of\neach component, the complexity of SCSG to reach a stationary point with\n$\\mathbb{E} \\|\\nabla f(x)\\|^{2}\\le \\epsilon$ is $O\\left (\\min\\{\\epsilon^{-5/3},\n\\epsilon^{-1}n^{2/3}\\}\\right)$, which strictly outperforms the stochastic\ngradient descent. Moreover, SCSG is never worse than the state-of-the-art\nmethods based on variance reduction and it significantly outperforms them when\nthe target accuracy is low. A similar acceleration is also achieved when the\nfunctions satisfy the Polyak-Lojasiewicz condition. Empirical experiments\ndemonstrate that SCSG outperforms stochastic gradient methods on training\nmulti-layers neural networks in terms of both training and validation loss.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 07:54:02 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 06:31:56 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 05:42:13 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 04:13:12 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Lei", "Lihua", ""], ["Ju", "Cheng", ""], ["Chen", "Jianbo", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1706.09169", "submitter": "Martin Avanzini", "authors": "Martin Avanzini and Ugo Dal Lago", "title": "Automating Sized Type Inference for Complexity Analysis (Technical\n  Report)", "comments": "Technical report of http://dx.doi.org/10.1145/3110287", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces a new methodology for the complexity analysis of\nhigher-order functional programs, which is based on three ingredients: a\npowerful type system for size analysis and a sound type inference procedure for\nit, a ticking monadic transformation, and constraint solving. Noticeably, the\npresented methodology can be fully automated, and is able to analyse a series\nof examples which cannot be handled by most competitor methodologies. This is\npossible due to the choice of adopting an abstract index language and index\npolymorphism at higher ranks. A prototype implementation is available.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 08:44:10 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Avanzini", "Martin", ""], ["Lago", "Ugo Dal", ""]]}, {"id": "1706.09230", "submitter": "Caishi Fang", "authors": "Caishi Fang", "title": "Accelerations for Graph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two main results. First, by only one conjecture\n(Conjecture 2.9) for recognizing a vertex symmetric graph, which is the hardest\ntask for our problem, we construct an algorithm for finding an isomorphism\nbetween two graphs in polynomial time $ O(n^{3}) $. Second, without that\nconjecture, we prove the algorithm to be of quasi-polynomial time $\nO(n^{1.5\\log n}) $. The conjectures in this paper are correct for all graphs of\nsize no larger than $ 5 $ and all graphs we have encountered. At least the\nconjecture for determining if a graph is vertex symmetric is quite true\nintuitively. We are not able to prove them by hand, so we have planned to find\npossible counterexamples by a computer. We also introduce new concepts like\ncollapse pattern and collapse tomography, which play important roles in our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 06:03:16 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Fang", "Caishi", ""]]}, {"id": "1706.09279", "submitter": "Chris Cade", "authors": "Chris Cade and Ashley Montanaro", "title": "The Quantum Complexity of Computing Schatten $p$-norms", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quantum complexity of computing Schatten $p$-norms and\nrelated quantities, and find that the problem of estimating these quantities is\nclosely related to the one clean qubit model of computation. We show that the\nproblem of approximating $\\text{Tr}\\, (|A|^p)$ for a log-local $n$-qubit\nHamiltonian $A$ and $p=\\text{poly}(n)$, up to a suitable level of accuracy, is\ncontained in DQC1; and that approximating this quantity up to a somewhat higher\nlevel of accuracy is DQC1-hard. In some cases the level of accuracy achieved by\nthe quantum algorithm is substantially better than a natural classical\nalgorithm for the problem. The same problem can be solved for arbitrary sparse\nmatrices in BQP. One application of the algorithm is the approximate\ncomputation of the energy of a graph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:27:00 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Cade", "Chris", ""], ["Montanaro", "Ashley", ""]]}, {"id": "1706.09339", "submitter": "Amer Mouawad", "authors": "Eduard Eiben, Mithilesh Kumar, Amer E. Mouawad, Fahad Panolan, and\n  Sebastian Siebertz", "title": "Lossy Kernels for Connected Dominating Set on Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $\\alpha > 1$, an $\\alpha$-approximate (bi-)kernel is a polynomial-time\nalgorithm that takes as input an instance $(I, k)$ of a problem $\\mathcal{Q}$\nand outputs an instance $(I',k')$ (of a problem $\\mathcal{Q}'$) of size bounded\nby a function of $k$ such that, for every $c\\geq 1$, a $c$-approximate solution\nfor the new instance can be turned into a $(c\\cdot\\alpha)$-approximate solution\nof the original instance in polynomial time. This framework of lossy\nkernelization was recently introduced by Lokshtanov et al. We study Connected\nDominating Set (and its distance-$r$ variant) parameterized by solution size on\nsparse graph classes like biclique-free graphs, classes of bounded expansion,\nand nowhere dense classes. We prove that for every $\\alpha>1$, Connected\nDominating Set admits a polynomial-size $\\alpha$-approximate (bi-)kernel on all\nthe aforementioned classes. Our results are in sharp contrast to the\nkernelization complexity of Connected Dominating Set, which is known to not\nadmit a polynomial kernel even on $2$-degenerate graphs and graphs of bounded\nexpansion, unless $\\textsf{NP} \\subseteq \\textsf{coNP/poly}$. We complement our\nresults by the following conditional lower bound. We show that if a class\n$\\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for\nsome value of $r\\in\\mathbb{N}$ there cannot exist an $\\alpha$-approximate\nbi-kernel for the (Connected) Distance-$r$ Dominating Set problem on\n$\\mathcal{C}$ for any $\\alpha>1$ (assuming the Gap Exponential Time\nHypothesis).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 15:55:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 17:16:54 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Eiben", "Eduard", ""], ["Kumar", "Mithilesh", ""], ["Mouawad", "Amer E.", ""], ["Panolan", "Fahad", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1706.09356", "submitter": "Pawe{\\l} Naroski", "authors": "Zbigniew Lonc, Pawe{\\l} Naroski, and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Tight Euler tours in uniform hypergraphs - computational aspects", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  3, Analysis of Algorithms (September 26, 2017) dmtcs:3934", "doi": "10.23638/DMTCS-19-3-2", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By a tight tour in a $k$-uniform hypergraph $H$ we mean any sequence of its\nvertices $(w_0,w_1,\\ldots,w_{s-1})$ such that for all $i=0,\\ldots,s-1$ the set\n$e_i=\\{w_i,w_{i+1}\\ldots,w_{i+k-1}\\}$ is an edge of $H$ (where operations on\nindices are computed modulo $s$) and the sets $e_i$ for $i=0,\\ldots,s-1$ are\npairwise different. A tight tour in $H$ is a tight Euler tour if it contains\nall edges of $H$. We prove that the problem of deciding if a given $3$-uniform\nhypergraph has a tight Euler tour is NP-complete, and that it cannot be solved\nin time $2^{o(m)}$ (where $m$ is the number of edges in the input hypergraph),\nunless the ETH fails. We also present an exact exponential algorithm for the\nproblem, whose time complexity matches this lower bound, and the space\ncomplexity is polynomial. In fact, this algorithm solves a more general problem\nof computing the number of tight Euler tours in a given uniform hypergraph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 16:42:34 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:57:19 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Lonc", "Zbigniew", ""], ["Naroski", "Pawe\u0142", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1706.09362", "submitter": "Adam Freilich", "authors": "Xi Chen, Adam Freilich, Rocco A. Servedio, Timothy Sun", "title": "Sample-based high-dimensional convexity testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the problem of high-dimensional convexity testing, there is an unknown set\n$S \\subseteq \\mathbb{R}^n$ which is promised to be either convex or\n$\\varepsilon$-far from every convex body with respect to the standard\nmultivariate normal distribution $\\mathcal{N}(0, 1)^n$. The job of a testing\nalgorithm is then to distinguish between these two cases while making as few\ninspections of the set $S$ as possible.\n  In this work we consider sample-based testing algorithms, in which the\ntesting algorithm only has access to labeled samples\n$(\\boldsymbol{x},S(\\boldsymbol{x}))$ where each $\\boldsymbol{x}$ is\nindependently drawn from $\\mathcal{N}(0, 1)^n$. We give nearly matching sample\ncomplexity upper and lower bounds for both one-sided and two-sided convexity\ntesting algorithms in this framework. For constant $\\varepsilon$, our results\nshow that the sample complexity of one-sided convexity testing is\n$2^{\\tilde{\\Theta}(n)}$ samples, while for two-sided convexity testing it is\n$2^{\\tilde{\\Theta}(\\sqrt{n})}$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 16:58:50 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Chen", "Xi", ""], ["Freilich", "Adam", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Timothy", ""]]}, {"id": "1706.09370", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte, Markus Hecher, Michael Morak, Stefan Woltran", "title": "DynASP2.5: Dynamic Programming on Tree Decompositions in Action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vibrant theoretical research area are efficient exact parameterized\nalgorithms. Very recent solving competitions such as the PACE challenge show\nthat there is also increasing practical interest in the parameterized\nalgorithms community. An important research question is whether dedicated\nparameterized exact algorithms exhibit certain practical relevance and one can\neven beat well-established problem solvers. We consider the logic-based\ndeclarative modeling language and problem solving framework Answer Set\nProgramming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based\nalgorithms. An ASP solver (DynASP2), which is based on a classical dynamic\nprogramming on tree decompositions, has been published very recently.\nUnfortunately, DynASP2 can outperform modern ASP solvers on programs of small\ntreewidth only if the question of interest is to count the number of solutions.\nIn this paper, we describe underlying concepts of our new implementation\n(DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers\neven for finding just one solution when solving problems as the Steiner tree\nproblem that have been modeled in ASP on graphs with low treewidth. Our\nimplementation is based on a novel approach that we call multi-pass dynamic\nprogramming (M-DPSINC).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:20:24 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1706.09391", "submitter": "Ralph Christian Bottesch", "authors": "Ralph Christian Bottesch", "title": "Relativization and Interactive Proof Systems in Parameterized Complexity\n  Theory", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.IPEC.2017.9", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some classical complexity-theoretic techniques to Parameterized\nComplexity. First, we study relativization for the machine models that were\nused by Chen, Flum, and Grohe (2005) to characterize a number of parameterized\ncomplexity classes. Here we obtain a new and non-trivial characterization of\nthe A-Hierarchy in terms of oracle machines, and parameterize a famous result\nof Baker, Gill, and Solovay (1975), by proving that, relative to specific\noracles, FPT and A[1] can either coincide or differ (a similar statement holds\nfor FPT and W[P]). Second, we initiate the study of interactive proof systems\nin the parameterized setting, and show that every problem in the class AW[SAT]\nhas a proof system with \"short\" interactions, in the sense that the number of\nrounds is upper-bounded in terms of the parameter value alone.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:55:37 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 14:07:53 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bottesch", "Ralph Christian", ""]]}, {"id": "1706.09393", "submitter": "Markus Hecher", "authors": "Johannes K. Fichte, Markus Hecher, Irina Schindler", "title": "Default Logic and Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Reiter's propositional default logic when the\ntreewidth of a certain graph representation (semi-primal graph) of the input\ntheory is bounded. We establish a dynamic programming algorithm on tree\ndecompositions that decides whether a theory has a consistent stable extension\n(Ext). Our algorithm can even be used to enumerate all generating defaults\n(ExtEnum) that lead to stable extensions.\n  We show that our algorithm decides Ext in linear time in the input theory and\ntriple exponential time in the treewidth (so-called fixed-parameter linear\nalgorithm).\n  Further, our algorithm solves ExtEnum with a pre-computation step that is\nlinear in the input theory and triple exponential in the treewidth followed by\na linear delay to output solutions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:57:02 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 11:03:22 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Schindler", "Irina", ""]]}, {"id": "1706.09608", "submitter": "Arnaud Mary", "authors": "Nicolas Bousquet, Arnaud Mary, Aline Parreau", "title": "Token Jumping in minor-closed classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two $k$-independent sets $I$ and $J$ of a graph $G$, one can ask if it\nis possible to transform the one into the other in such a way that, at any\nstep, we replace one vertex of the current independent set by another while\nkeeping the property of being independent. Deciding this problem, known as the\nToken Jumping (TJ) reconfiguration problem, is PSPACE-complete even on planar\ngraphs. Ito et al. proved in 2014 that the problem is FPT parameterized by $k$\nif the input graph is $K_{3,\\ell}$-free.\n  We prove that the result of Ito et al. can be extended to any\n$K_{\\ell,\\ell}$-free graphs. In other words, if $G$ is a $K_{\\ell,\\ell}$-free\ngraph, then it is possible to decide in FPT-time if $I$ can be transformed into\n$J$. As a by product, the TJ-reconfiguration problem is FPT in many well-known\nclasses of graphs such as any minor-free class.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:33:30 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Mary", "Arnaud", ""], ["Parreau", "Aline", ""]]}, {"id": "1706.09696", "submitter": "Simone Santini", "authors": "Jaun Casanova, Simone Santini", "title": "On the relation between representations and computability", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental results in computability is the existence of\nwell-defined functions that cannot be computed. In this paper we study the\neffects of data representation on computability; we show that, while for each\npossible way of representing data there exist incomputable functions, the\ncomputability of a specific abstract function is never an absolute property,\nbut depends on the representation used for the function domain. We examine the\nscope of this dependency and provide mathematical criteria to favour some\nrepresentations over others. As we shall show, there are strong reasons to\nsuggest that computational enumerability should be an additional axiom for\ncomputation models. We analyze the link between the techniques and effects of\nrepresentation changes and those of oracle machines, showing an important\nconnection between their hierarchies. Finally, these notions enable us to gain\na new insight on the Church-Turing thesis: its interpretation as the underlying\nalgebraic structure to which computation is invariant.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 11:47:21 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Casanova", "Jaun", ""], ["Santini", "Simone", ""]]}, {"id": "1706.09748", "submitter": "Will Rosenbaum", "authors": "Talya Eden, Will Rosenbaum", "title": "On Sampling Edges Almost Uniformly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling an edge almost uniformly from an unknown\ngraph, $G = (V, E)$. Access to the graph is provided via queries of the\nfollowing types: (1) uniform vertex queries, (2) degree queries, and (3)\nneighbor queries. We describe an algorithm that returns a random edge $e \\in E$\nusing $\\tilde{O}(n / \\sqrt{\\varepsilon m})$ queries in expectation, where $n =\n|V|$ is the number of vertices, and $m = |E|$ is the number of edges, such that\neach edge $e$ is sampled with probability $(1 \\pm \\varepsilon)/m$. We prove\nthat our algorithm is optimal in the sense that any algorithm that samples an\nedge from an almost-uniform distribution must perform $\\Omega(n / \\sqrt{m})$\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 13:31:36 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Eden", "Talya", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1706.09854", "submitter": "Mateus Ara\\'ujo", "authors": "Mateus Ara\\'ujo, Philippe Allard Gu\\'erin, \\\"Amin Baumeler", "title": "Quantum computation with indefinite causal structures", "comments": "11 + 5 pages, no figures, 16 circuits. Corrected equations (33)-(36)", "journal-ref": "Phys. Rev. A 96, 052315 (2017)", "doi": "10.1103/PhysRevA.96.052315", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to study the physical plausibility of closed timelike curves (CTCs)\nis to examine their computational power. This has been done for Deutschian CTCs\n(D-CTCs) and post-selection CTCs (P-CTCs), with the result that they allow for\nthe efficient solution of problems in PSPACE and PP, respectively. Since these\nare extremely powerful complexity classes, which are not expected to be\nsolvable in reality, this can be taken as evidence that these models for CTCs\nare pathological. This problem is closely related to the nonlinearity of this\nmodels, which also allows for example cloning quantum states, in the case of\nD-CTCs, or distinguishing non-orthogonal quantum states, in the case of P-CTCs.\nIn contrast, the process matrix formalism allows one to model indefinite causal\nstructures in a linear way, getting rid of these effects, and raising the\npossibility that its computational power is rather tame. In this paper we show\nthat process matrices correspond to a linear particular case of P-CTCs, and\ntherefore that its computational power is upperbounded by that of PP. We show,\nfurthermore, a family of processes that can violate causal inequalities but\nnevertheless can be simulated by a causally ordered quantum circuit with only a\nconstant overhead, showing that indefinite causality is not necessarily hard to\nsimulate.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:03:55 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 05:47:50 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 16:20:54 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ara\u00fajo", "Mateus", ""], ["Gu\u00e9rin", "Philippe Allard", ""], ["Baumeler", "\u00c4min", ""]]}, {"id": "1706.10028", "submitter": "Zeyu Guo", "authors": "Zeyu Guo", "title": "$\\mathcal{P}$-schemes and Deterministic Polynomial Factoring over Finite\n  Fields", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.SC math.GR math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of mathematical objects called $\\mathcal{P}$-schemes,\nwhere $\\mathcal{P}$ is a poset of subgroups of a finite group $G$. A\n$\\mathcal{P}$-scheme is a collection of partitions of the right coset spaces\n$H\\backslash G$, indexed by $H\\in\\mathcal{P}$, that satisfies a list of axioms.\nThese objects generalize the classical notion of association schemes as well as\nthe notion of $m$-schemes (Ivanyos et al. 2009).\n  Based on $\\mathcal{P}$-schemes, we develop a unifying framework for the\nproblem of deterministic factoring of univariate polynomials over finite fields\nunder the generalized Riemann hypothesis (GRH).\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 05:42:59 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 09:44:03 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Guo", "Zeyu", ""]]}, {"id": "1706.10046", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Mikhail Rudoy", "title": "Hamiltonicity is Hard in Thin or Polygonal Grid Graphs, but Easy in Thin\n  Polygonal Grid Graphs", "comments": "25 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2007, Arkin et al. initiated a systematic study of the complexity of the\nHamiltonian cycle problem on square, triangular, or hexagonal grid graphs,\nrestricted to polygonal, thin, superthin, degree-bounded, or solid grid graphs.\nThey solved many combinations of these problems, proving them either\npolynomially solvable or NP-complete, but left three combinations open. In this\npaper, we prove two of these unsolved combinations to be NP-complete:\nHamiltonicity of Square Polygonal Grid Graphs and Hamiltonicity of Hexagonal\nThin Grid Graphs. We also consider a new restriction, where the grid graph is\nboth thin and polygonal, and prove that Hamiltonicity then becomes polynomially\nsolvable for square, triangular, and hexagonal grid graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 07:37:53 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.10110", "submitter": "Kasper Green Larsen", "authors": "Casper Benjamin Freksen and Kasper Green Larsen", "title": "On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss lemma is one of the corner stone results in\ndimensionality reduction. It says that given $N$, for any set of $N$ vectors $X\n\\subset \\mathbb{R}^n$, there exists a mapping $f : X \\to \\mathbb{R}^m$ such\nthat $f(X)$ preserves all pairwise distances between vectors in $X$ to within\n$(1 \\pm \\varepsilon)$ if $m = O(\\varepsilon^{-2} \\lg N)$. Much effort has gone\ninto developing fast embedding algorithms, with the Fast Johnson-Lindenstrauss\ntransform of Ailon and Chazelle being one of the most well-known techniques.\nThe current fastest algorithm that yields the optimal $m =\nO(\\varepsilon^{-2}\\lg N)$ dimensions has an embedding time of $O(n \\lg n +\n\\varepsilon^{-2} \\lg^3 N)$. An exciting approach towards improving this, due to\nHinrichs and Vyb\\'iral, is to use a random $m \\times n$ Toeplitz matrix for the\nembedding. Using Fast Fourier Transform, the embedding of a vector can then be\ncomputed in $O(n \\lg m)$ time. The big question is of course whether $m =\nO(\\varepsilon^{-2} \\lg N)$ dimensions suffice for this technique. If so, this\nwould end a decades long quest to obtain faster and faster\nJohnson-Lindenstrauss transforms. The current best analysis of the embedding of\nHinrichs and Vyb\\'iral shows that $m = O(\\varepsilon^{-2}\\lg^2 N)$ dimensions\nsuffices. The main result of this paper, is a proof that this analysis\nunfortunately cannot be tightened any further, i.e., there exists a set of $N$\nvectors requiring $m = \\Omega(\\varepsilon^{-2} \\lg^2 N)$ for the Toeplitz\napproach to work.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:46:28 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:23:55 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Freksen", "Casper Benjamin", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1706.10114", "submitter": "May Szedl\\'ak", "authors": "Komei Fukuda and May Szedlak", "title": "On the Complexity of Polytopes in $LI(2)$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider polytopes given by systems of $n$ inequalities in\n$d$ variables, where every inequality has at most two variables with nonzero\ncoefficient. We denote this family by $LI(2)$. We show that despite of the easy\nalgebraic structure, polytopes in $LI(2)$ can have high complexity. We\nconstruct a polytope in $LI(2)$, whose number of vertices is almost the number\nof vertices of the dual cyclic polytope, the difference is a multiplicative\nfactor of depending on $d$ and in particular independent of $n$. Moreover we\nshow that the dual cyclic polytope can not be realized in $LI(2)$.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:51:58 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Fukuda", "Komei", ""], ["Szedlak", "May", ""]]}, {"id": "1706.10153", "submitter": "Ruhollah Majdoddin", "authors": "Ruhollah Majdoddin", "title": "Parameterized Complexity of CSP for Infinite Constraint Languages", "comments": "Minor corrections in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameterized Constraint Satisfaction Problem for infinite\nconstraint languages. The parameters that we study are weight of the satisfying\nassignment, number of constraints, maximum number of occurrences of a variable\nin the instance, and maximum number of occurrences of a variable in each\nconstraint. A dichotomy theorem is already known for finite constraint\nlanguages with the weight parameter. We prove some general theorems that show,\nas new results, that some well-known problems are fixed-parameter tractable and\nsome others are in W[1].\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 11:49:41 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 15:18:08 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 15:56:39 GMT"}, {"version": "v4", "created": "Wed, 9 Aug 2017 12:51:07 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Majdoddin", "Ruhollah", ""]]}]