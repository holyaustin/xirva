[{"id": "1410.0245", "submitter": "Jeremy Kun", "authors": "Benjamin Fish and Jeremy Kun and \\'Ad\\'am D\\'aniel Lelkes and Lev\n  Reyzin and Gy\\\"orgy Tur\\'an", "title": "On the Computational Complexity of MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study MapReduce computations from a complexity-theoretic\nperspective. First, we formulate a uniform version of the MRC model of Karloff\net al. (2010). We then show that the class of regular languages, and moreover\nall of sublogarithmic space, lies in constant round MRC. This result also\napplies to the MPC model of Andoni et al. (2014). In addition, we prove that,\nconditioned on a variant of the Exponential Time Hypothesis, there are strict\nhierarchies within MRC so that increasing the number of rounds or the amount of\ntime per processor increases the power of MRC. To the best of our knowledge we\nare the first to approach the MapReduce model with complexity-theoretic\ntechniques, and our work lays the foundation for further analysis relating\nMapReduce to established complexity classes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 14:44:01 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 18:43:00 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Fish", "Benjamin", ""], ["Kun", "Jeremy", ""], ["Lelkes", "\u00c1d\u00e1m D\u00e1niel", ""], ["Reyzin", "Lev", ""], ["Tur\u00e1n", "Gy\u00f6rgy", ""]]}, {"id": "1410.0390", "submitter": "Peter Richtarik", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Simple Complexity Analysis of Simplified Direct Search", "comments": "21 pages, 5 algorithms, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of unconstrained minimization of a smooth function in\nthe derivative-free setting using. In particular, we propose and study a\nsimplified variant of the direct search method (of direction type), which we\ncall simplified direct search (SDS). Unlike standard direct search methods,\nwhich depend on a large number of parameters that need to be tuned, SDS depends\non a single scalar parameter only.\n  Despite relevant research activity in direct search methods spanning several\ndecades, complexity guarantees---bounds on the number of function evaluations\nneeded to find an approximate solution---were not established until very\nrecently. In this paper we give a surprisingly brief and unified analysis of\nSDS for nonconvex, convex and strongly convex functions. We match the existing\ncomplexity results for direct search in their dependence on the problem\ndimension ($n$) and error tolerance ($\\epsilon$), but the overall bounds are\nsimpler, easier to interpret, and have better dependence on other problem\nparameters. In particular, we show that for the set of directions formed by the\nstandard coordinate vectors and their negatives, the number of function\nevaluations needed to find an $\\epsilon$-solution is $O(n^2 /\\epsilon)$ (resp.\n$O(n^2 \\log(1/\\epsilon))$) for the problem of minimizing a convex (resp.\nstrongly convex) smooth function. In the nonconvex smooth case, the bound is\n$O(n^2/\\epsilon^2)$, with the goal being the reduction of the norm of the\ngradient below $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 21:42:04 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 12:09:47 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1410.0589", "submitter": "Mateus de Oliveira Oliveira", "authors": "Mateus de Oliveira Oliveira", "title": "An Algorithmic Metatheorem for Directed Treewidth", "comments": "41 pages, 6 figures, Accepted to Discrete Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of directed treewidth was introduced by Johnson, Robertson,\nSeymour and Thomas [Journal of Combinatorial Theory, Series B, Vol 82, 2001] as\na first step towards an algorithmic metatheory for digraphs. They showed that\nsome NP-complete properties such as Hamiltonicity can be decided in polynomial\ntime on digraphs of constant directed treewidth. Nevertheless, despite more\nthan one decade of intensive research, the list of hard combinatorial problems\nthat are known to be solvable in polynomial time when restricted to digraphs of\nconstant directed treewidth has remained scarce. In this work we enrich this\nlist by providing for the first time an algorithmic metatheorem connecting the\nmonadic second order logic of graphs to directed treewidth. We show that most\nof the known positive algorithmic results for digraphs of constant directed\ntreewidth can be reformulated in terms of our metatheorem. Additionally, we\nshow how to use our metatheorem to provide polynomial time algorithms for two\nclasses of combinatorial problems that have not yet been studied in the context\nof directed width measures. More precisely, for each fixed $k,w \\in\n\\mathbb{N}$, we show how to count in polynomial time on digraphs of directed\ntreewidth $w$, the number of minimum spanning strong subgraphs that are the\nunion of $k$ directed paths, and the number of maximal subgraphs that are the\nunion of $k$ directed paths and satisfy a given minor closed property. To prove\nour metatheorem we devise two technical tools which we believe to be of\nindependent interest. First, we introduce the notion of tree-zig-zag number of\na digraph, a new directed width measure that is at most a constant times\ndirected treewidth. Second, we introduce the notion of $z$-saturated tree slice\nlanguage, a new formalism for the specification and manipulation of infinite\nsets of digraphs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 15:27:00 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 07:16:23 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Oliveira", "Mateus de Oliveira", ""]]}, {"id": "1410.0855", "submitter": "Bart M. P. Jansen", "authors": "Bart M. P. Jansen and D\\'aniel Marx", "title": "Characterizing the easy-to-find subgraphs from the viewpoint of\n  polynomial-time algorithms, kernels, and Turing kernels", "comments": "69 pages, extended abstract to appear in the proceedings of SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two fundamental problems related to finding subgraphs: (1) given\ngraphs G and H, Subgraph Test asks if H is isomorphic to a subgraph of G, (2)\ngiven graphs G, H, and an integer t, Packing asks if G contains t\nvertex-disjoint subgraphs isomorphic to H. For every graph class F, let\nF-Subgraph Test and F-Packing be the special cases of the two problems where H\nis restricted to be in F. Our goal is to study which classes F make the two\nproblems tractable in one of the following senses:\n  * (randomized) polynomial-time solvable,\n  * admits a polynomial (many-one) kernel, or\n  * admits a polynomial Turing kernel (that is, has an adaptive polynomial-time\nprocedure that reduces the problem to a polynomial number of instances, each of\nwhich has size bounded polynomially by the size of the solution).\n  We identify a simple combinatorial property such that if a hereditary class F\nhas this property, then F-Packing admits a polynomial kernel, and has no\npolynomial (many-one) kernel otherwise, unless the polynomial hierarchy\ncollapses. Furthermore, if F does not have this property, then F-Packing is\neither WK[1]-hard, W[1]-hard, or Long Path-hard, giving evidence that it does\nnot admit polynomial Turing kernels either.\n  For F-Subgraph Test, we show that if every graph of a hereditary class F\nsatisfies the property that it is possible to delete a bounded number of\nvertices such that every remaining component has size at most two, then\nF-Subgraph Test is solvable in randomized polynomial time and it is NP-hard\notherwise. We introduce a combinatorial property called (a,b,c,d)-splittability\nand show that if every graph in a hereditary class F has this property, then\nF-Subgraph Test admits a polynomial Turing kernel and it is WK[1]-hard,\nW[1]-hard, or Long Path-hard, otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 14:04:19 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1410.0932", "submitter": "Cedric Yen-Yu Lin", "authors": "Cedric Yen-Yu Lin and Han-Hsuan Lin", "title": "Upper bounds on quantum query complexity inspired by the Elitzur-Vaidman\n  bomb tester", "comments": "32 pages. Minor revisions and corrections. Regev and Schiff's proof\n  that P(OR) = \\Omega(N) removed", "journal-ref": null, "doi": null, "report-no": "MIT-CTP/4592", "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the Elitzur-Vaidman bomb testing problem [arXiv:hep-th/9305002],\nwe introduce a new query complexity model, which we call bomb query complexity\n$B(f)$. We investigate its relationship with the usual quantum query complexity\n$Q(f)$, and show that $B(f)=\\Theta(Q(f)^2)$.\n  This result gives a new method to upper bound the quantum query complexity:\nwe give a method of finding bomb query algorithms from classical algorithms,\nwhich then provide nonconstructive upper bounds on $Q(f)=\\Theta(\\sqrt{B(f)})$.\nWe subsequently were able to give explicit quantum algorithms matching our\nupper bound method. We apply this method on the single-source shortest paths\nproblem on unweighted graphs, obtaining an algorithm with $O(n^{1.5})$ quantum\nquery complexity, improving the best known algorithm of $O(n^{1.5}\\sqrt{\\log\nn})$ [arXiv:quant-ph/0606127]. Applying this method to the maximum bipartite\nmatching problem gives an $O(n^{1.75})$ algorithm, improving the best known\ntrivial $O(n^2)$ upper bound.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 18:14:22 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 23:23:41 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Lin", "Cedric Yen-Yu", ""], ["Lin", "Han-Hsuan", ""]]}, {"id": "1410.1318", "submitter": "Magnus Gausdal Find", "authors": "Joan Boyar, Magnus Gausdal Find", "title": "Constructive Relationships Between Algebraic Thickness and Normality", "comments": "Final version published in FCT'2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between two measures of Boolean functions;\n\\emph{algebraic thickness} and \\emph{normality}. For a function $f$, the\nalgebraic thickness is a variant of the \\emph{sparsity}, the number of nonzero\ncoefficients in the unique GF(2) polynomial representing $f$, and the normality\nis the largest dimension of an affine subspace on which $f$ is constant. We\nshow that for $0 < \\epsilon<2$, any function with algebraic thickness\n$n^{3-\\epsilon}$ is constant on some affine subspace of dimension\n$\\Omega\\left(n^{\\frac{\\epsilon}{2}}\\right)$. Furthermore, we give an algorithm\nfor finding such a subspace. We show that this is at most a factor of\n$\\Theta(\\sqrt{n})$ from the best guaranteed, and when restricted to the\ntechnique used, is at most a factor of $\\Theta(\\sqrt{\\log n})$ from the best\nguaranteed. We also show that a concrete function, majority, has algebraic\nthickness $\\Omega\\left(2^{n^{1/6}}\\right)$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 11:07:53 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 13:37:26 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Boyar", "Joan", ""], ["Find", "Magnus Gausdal", ""]]}, {"id": "1410.1653", "submitter": "S\\'andor Kisfaludi-Bak", "authors": "Zolt\\'an Kir\\'aly, S\\'andor Kisfaludi-Bak", "title": "Notes on dual-critical graphs", "comments": "10 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define dual-critical graphs as graphs having an acyclic orientation, where\nthe indegrees are odd except for the unique source. We have very limited\nknowledge about the complexity of dual-criticality testing. By the definition\nthe problem is in NP, and a result of Bal\\'azs and Christian Szegedy provides a\nrandomized polynomial algorithm, which relies on formal matrix rank computing.\nIt is unknown whether dual-criticality test can be done in deterministic\npolynomial time. Moreover, the question of being in co-NP is also open.\n  We give equivalent descriptions for dual-critical graphs in the general case,\nand further equivalent descriptions in the special cases of planar graphs and\n3-regular graphs. These descriptions provide polynomial algorithms for these\nspecial classes. We also give an FPT algorithm for a relaxed version of\ndual-criticality called $k$-dual-criticality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 09:44:06 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Kir\u00e1ly", "Zolt\u00e1n", ""], ["Kisfaludi-Bak", "S\u00e1ndor", ""]]}, {"id": "1410.2075", "submitter": "Oliver Schaudt", "authors": "Van Bang Le and Andrea Oversberg and Oliver Schaudt", "title": "Squares of $3$-sun-free split graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The square of a graph $G$, denoted by $G^2$, is obtained from $G$ by putting\nan edge between two distinct vertices whenever their distance is two. Then $G$\nis called a square root of $G^2$. Deciding whether a given graph has a square\nroot is known to be NP-complete, even if the root is required to be a split\ngraph, that is, a graph in which the vertex set can be partitioned into a\nstable set and a clique.\n  We give a wide range of polynomial time solvable cases for the problem of\nrecognizing if a given graph is the square of some special kind of split graph.\nTo the best of our knowledge, our result properly contains all previously known\nsuch cases. Our polynomial time algorithms are build on a structural\ninvestigation of graphs that admit a split square root that is 3-sun-free, and\nmay pave the way toward a dichotomy theorem for recognizing squares of\n(3-sun-free) split graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 12:05:59 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Le", "Van Bang", ""], ["Oversberg", "Andrea", ""], ["Schaudt", "Oliver", ""]]}, {"id": "1410.2371", "submitter": "Steven Kelk", "authors": "Leo van Iersel, Steven Kelk, Nela Lekic, Simone Linz", "title": "Satisfying ternary permutation constraints by multiple linear orders or\n  phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ternary permutation constraint satisfaction problem (CSP) is specified by a\nsubset Pi of the symmetric group S_3. An instance of such a problem consists of\na set of variables V and a set of constraints C, where each constraint is an\nordered triple of distinct elements from V. The goal is to construct a linear\norder alpha on V such that, for each constraint (a,b,c) in C, the ordering of\na,b,c induced by alpha is in Pi. Excluding symmetries and trivial cases there\nare 11 such problems, and their complexity is well known. Here we consider the\nvariant of the problem, denoted 2-Pi, where we are allowed to construct two\nlinear orders alpha and beta and each constraint needs to be satisfied by at\nleast one of the two. We give a full complexity classification of all 11 2-Pi\nproblems, observing that in the switch from one to two linear orders the\ncomplexity landscape changes quite abruptly and that hardness proofs become\nrather intricate. We then focus on one of the 11 problems in particular, which\nis closely related to the '2-Caterpillar Compatibility' problem in the\nphylogenetics literature. We show that this particular CSP remains hard on\nthree linear orders, and also in the biologically relevant case when we swap\nthree linear orders for three phylogenetic trees, yielding the '3-Tree\nCompatibility' problem. Due to the biological relevance of this problem we also\ngive extremal results concerning the minimum number of trees required, in the\nworst case, to satisfy a set of rooted triplet constraints on n leaf labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 07:38:55 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["van Iersel", "Leo", ""], ["Kelk", "Steven", ""], ["Lekic", "Nela", ""], ["Linz", "Simone", ""]]}, {"id": "1410.2456", "submitter": "Olga Podolskaya", "authors": "Olga Podolskaya", "title": "On Circuit Complexity of Parity and Majority Functions in Antichain\n  Basis", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the circuit complexity of boolean functions in a certain infinite\nbasis. The basis consists of all functions that take value $1$ on antichains\nover the boolean cube. We prove that the circuit complexity of the parity\nfunction and the majority function of $n$ variables in this basis is $\\lfloor\n\\frac{n+1}{2} \\rfloor$ and $\\left\\lfloor \\frac{n}{2} \\right \\rfloor +1$\nrespectively. We show that the asymptotic of the maximum complexity of\n$n$-variable boolean functions in this basis equals $n.$\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 13:43:30 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Podolskaya", "Olga", ""]]}, {"id": "1410.2652", "submitter": "Lane A. Hemaspaandra", "authors": "Gabor Erdelyi, Edith Hemaspaandra, Lane A. Hemaspaandra", "title": "More Natural Models of Electoral Control by Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Control\" studies attempts to set the outcome of elections through the\naddition, deletion, or partition of voters or candidates. The set of benchmark\ncontrol types was largely set in the seminal 1992 paper by Bartholdi, Tovey,\nand Trick that introduced control, and there now is a large literature studying\nhow many of the benchmark types various election systems are vulnerable to,\ni.e., have polynomial-time attack algorithms for.\n  However, although the longstanding benchmark models of addition and deletion\nmodel relatively well the real-world settings that inspire them, the\nlongstanding benchmark models of partition model settings that are arguably\nquite distant from those they seek to capture.\n  In this paper, we introduce--and for some important cases analyze the\ncomplexity of--new partition models that seek to better capture many real-world\npartition settings. In particular, in many partition settings one wants the two\nparts of the partition to be of (almost) equal size, or is partitioning into\nmore than two parts, or has groups of actors who must be placed in the same\npart of the partition. Our hope is that having these new partition types will\nallow studies of control attacks to include such models that more realistically\ncapture many settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 23:22:49 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Erdelyi", "Gabor", ""], ["Hemaspaandra", "Edith", ""], ["Hemaspaandra", "Lane A.", ""]]}, {"id": "1410.2882", "submitter": "Alex B. Grilo", "authors": "Alex B. Grilo, Iordanis Kerenidis, Jamie Sikora", "title": "QMA with subset state witnesses", "comments": null, "journal-ref": "Chicago Journal of Theoretical Computer Science, no. 4, 2016", "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class QMA plays a fundamental role in quantum complexity theory and it\nhas found surprising connections to condensed matter physics and in particular\nin the study of the minimum energy of quantum systems. In this paper, we\nfurther investigate the class QMA and its related class QCMA by asking what\nmakes quantum witnesses potentially more powerful than classical ones. We\nprovide a definition of a new class, SQMA, where we restrict the possible\nquantum witnesses to the \"simpler\" subset states, i.e. a uniform superposition\nover the elements of a subset of n-bit strings. Surprisingly, we prove that\nthis class is equal to QMA, hence providing a new characterisation of the class\nQMA. We also prove the analogous result for QMA(2) and describe a new complete\nproblem for QMA and a stronger lower bound for the class QMA$_1$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 19:50:32 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 13:01:43 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Grilo", "Alex B.", ""], ["Kerenidis", "Iordanis", ""], ["Sikora", "Jamie", ""]]}, {"id": "1410.3277", "submitter": "Peter Hertling", "authors": "Peter Hertling (Universitaet der Bundeswehr Muenchen), Christoph\n  Spandl (Universitaet der Bundeswehr Muenchen)", "title": "Computing a Solution of Feigenbaum's Functional Equation in Polynomial\n  Time", "comments": "CCA 2012, Cambridge, UK, 24-27 June 2012", "journal-ref": "Logical Methods in Computer Science, Volume 10, Issue 4 (December\n  9, 2014) lmcs:984", "doi": "10.2168/LMCS-10(4:7)2014", "report-no": null, "categories": "math.DS cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lanford has shown that Feigenbaum's functional equation has an analytic\nsolution. We show that this solution is a polynomial time computable function.\nThis implies in particular that the so-called first Feigenbaum constant is a\npolynomial time computable real number.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 12:21:36 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 17:13:50 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 10:01:05 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Hertling", "Peter", "", "Universitaet der Bundeswehr Muenchen"], ["Spandl", "Christoph", "", "Universitaet der Bundeswehr Muenchen"]]}, {"id": "1410.3375", "submitter": "Kitty Meeks", "authors": "Mark Jerrum and Kitty Meeks", "title": "The parameterised complexity of counting even and odd induced subgraphs", "comments": "Author final version, to appear in Combinatorica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of counting, in a given graph, the number of induced\nk-vertex subgraphs which have an even number of edges, and also the\ncomplementary problem of counting the k-vertex induced subgraphs having an odd\nnumber of edges. We demonstrate that both problems are #W[1]-hard when\nparameterised by k, in fact proving a somewhat stronger result about counting\nsubgraphs with a property that only holds for some subset of k-vertex subgraphs\nwhich have an even (respectively odd) number of edges. On the other hand, we\nshow that the problems of counting even and odd k-vertex induced subgraphs both\nadmit an FPTRAS. These approximation schemes are based on a surprising\nstructural result, which exploits ideas from Ramsey theory.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 16:08:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 13:56:14 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Jerrum", "Mark", ""], ["Meeks", "Kitty", ""]]}, {"id": "1410.3820", "submitter": "Marc van Kreveld", "authors": "Hans L. Bodlaender and Marc van Kreveld", "title": "Google Scholar makes it Hard - the complexity of organizing one's\n  publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With Google Scholar, scientists can maintain their publications on personal\nprofile pages, while the citations to these works are automatically collected\nand counted. Maintenance of publications is done manually by the researcher\nherself, and involves deleting erroneous ones, merging ones that are the same\nbut which were not recognized as the same, adding forgotten co-authors, and\ncorrecting titles of papers and venues. The publications are presented on pages\nwith 20 or 100 papers in the web page interface from 2012--2014. The interface\ndoes not allow a scientist to merge two version of a paper if they appear on\ndifferent pages. This not only implies that a scientist who wants to merge\ncertain subsets of publications will sometimes be unable to do so, but also, we\nshow in this note that the decision problem to determine if it is possible to\nmerge given subsets of papers is NP-complete.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 09:09:47 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Bodlaender", "Hans L.", ""], ["van Kreveld", "Marc", ""]]}, {"id": "1410.4044", "submitter": "Arne Meier", "authors": "Martin L\\\"uck and Arne Meier and Irina Schindler", "title": "Parameterized Complexity of CTL: A Generalization of Courcelle's Theorem", "comments": "Conference version: \"L\\\"uck, Meier, Schindler. Parameterized\n  Complexity of CTL: A Generalization of Courcelle's Theorem. Language and\n  Automata Theory and Applications - 9th International Conference, LATA 2015,\n  Nice, France. Lecture Notes in Computer Science, Volume 8977, pp. 549-560,\n  Springer\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an almost complete classification of the parameterized complexity\nof all operator fragments of the satisfiability problem in computation tree\nlogic CTL. The investigated parameterization is the sum of temporal depth and\nstructural pathwidth. The classification shows a dichotomy between W[1]-hard\nand fixed-parameter tractable fragments. The only real operator fragment which\nis confirmed to be in FPT is the fragment containing solely AX. Also we prove a\ngeneralization of Courcelle's theorem to infinite signatures which will be used\nto proof the FPT-membership case.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 12:49:40 GMT"}, {"version": "v2", "created": "Sun, 23 Nov 2014 15:47:20 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2015 15:26:51 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["L\u00fcck", "Martin", ""], ["Meier", "Arne", ""], ["Schindler", "Irina", ""]]}, {"id": "1410.4135", "submitter": "Adam Case", "authors": "Adam Case and Jack H. Lutz", "title": "Mutual Dimension", "comments": "This article is 29 pages and has been submitted to ACM Transactions\n  on Computation Theory. A preliminary version of part of this material was\n  reported at the 2013 Symposium on Theoretical Aspects of Computer Science in\n  Kiel, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the lower and upper mutual dimensions $mdim(x:y)$ and $Mdim(x:y)$\nbetween any two points $x$ and $y$ in Euclidean space. Intuitively these are\nthe lower and upper densities of the algorithmic information shared by $x$ and\n$y$. We show that these quantities satisfy the main desiderata for a\nsatisfactory measure of mutual algorithmic information. Our main theorem, the\ndata processing inequality for mutual dimension, says that, if $f:\\mathbb{R}^m\n\\rightarrow \\mathbb{R}^n$ is computable and Lipschitz, then the inequalities\n$mdim(f(x):y) \\leq mdim(x:y)$ and $Mdim(f(x):y) \\leq Mdim(x:y)$ hold for all $x\n\\in \\mathbb{R}^m$ and $y \\in \\mathbb{R}^t$. We use this inequality and related\ninequalities that we prove in like fashion to establish conditions under which\nvarious classes of computable functions on Euclidean space preserve or\notherwise transform mutual dimensions between points.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 16:57:38 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Case", "Adam", ""], ["Lutz", "Jack H.", ""]]}, {"id": "1410.4241", "submitter": "Badih Ghazi", "authors": "Badih Ghazi, Euiwoong Lee", "title": "LP/SDP Hierarchy Lower Bounds for Decoding Random LDPC Codes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random (dv,dc)-regular LDPC codes are well-known to achieve the Shannon\ncapacity of the binary symmetric channel (for sufficiently large dv and dc)\nunder exponential time decoding. However, polynomial time algorithms are only\nknown to correct a much smaller fraction of errors. One of the most powerful\npolynomial-time algorithms with a formal analysis is the LP decoding algorithm\nof Feldman et al. which is known to correct an Omega(1/dc) fraction of errors.\nIn this work, we show that fairly powerful extensions of LP decoding, based on\nthe Sherali-Adams and Lasserre hierarchies, fail to correct much more errors\nthan the basic LP-decoder. In particular, we show that:\n  1) For any values of dv and dc, a linear number of rounds of the\nSherali-Adams LP hierarchy cannot correct more than an O(1/dc) fraction of\nerrors on a random (dv,dc)-regular LDPC code.\n  2) For any value of dv and infinitely many values of dc, a linear number of\nrounds of the Lasserre SDP hierarchy cannot correct more than an O(1/dc)\nfraction of errors on a random (dv,dc)-regular LDPC code.\n  Our proofs use a new stretching and collapsing technique that allows us to\nleverage recent progress in the study of the limitations of LP/SDP hierarchies\nfor Maximum Constraint Satisfaction Problems (Max-CSPs). The problem then\nreduces to the construction of special balanced pairwise independent\ndistributions for Sherali-Adams and special cosets of balanced pairwise\nindependent subgroups for Lasserre.\n  Some of our techniques are more generally applicable to a large class of\nBoolean CSPs called Min-Ones. In particular, for k-Hypergraph Vertex Cover, we\nobtain an improved integrality gap of $k-1-\\epsilon$ that holds after a\n\\emph{linear} number of rounds of the Lasserre hierarchy, for any k = q+1 with\nq an arbitrary prime power. The best previous gap for a linear number of rounds\nwas equal to $2-\\epsilon$ and due to Schoenebeck.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 21:45:59 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Ghazi", "Badih", ""], ["Lee", "Euiwoong", ""]]}, {"id": "1410.4607", "submitter": "Hai-Jun Zhou", "authors": "Jin-Hua Zhao, Yusupjan Habibulla, and Hai-Jun Zhou", "title": "Statistical Mechanics of the Minimum Dominating Set Problem", "comments": "Extensively revised (final version to be published in Journal of\n  Statistical Physics). 19 pages in total", "journal-ref": "Journal of Statistical Physics 159: 1154--1174 (2015)", "doi": "10.1007/s10955-015-1220-2", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum dominating set problem has wide applications in network science\nand related fields. It consists of assembling a node set of global minimum size\nsuch that any node of the network is either in this set or is adjacent to at\nleast one node of this set. Although this is a difficult optimization problem\nin general, we show it can be exactly solved by a generalized leaf-removal\nprocess if the network contains no core. If the network has an extensive core,\nwe estimate the size of minimum dominating sets by a mean-field theory and\nimplement a belief-propagation algorithm to obtain near-optimal solutions. Our\nalgorithms also perform well on real-world network instances.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 23:57:04 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 07:10:39 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Zhao", "Jin-Hua", ""], ["Habibulla", "Yusupjan", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1410.4925", "submitter": "Tuomo Kauranne", "authors": "Tuomo Kauranne", "title": "Finitely unstable theories and computational complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity class $NP$ can be logically characterized both through\nexistential second order logic $SO\\exists$, as proven by Fagin, and through\nsimulating a Turing machine via the satisfiability problem of propositional\nlogic SAT, as proven by Cook. Both theorems involve encoding a Turing machine\nby a formula in the corresponding logic and stating that a model of this\nformula exists if and only if the Turing machine halts, i.e. the formula is\nsatisfiable iff the Turing machine accepts its input. Trakhtenbrot's theorem\ndoes the same in first order logic $FO$. Such different orders of encoding are\npossible because the set of all possible configurations of any Turing machine\nup to any given finite time instant can be defined by a finite set of\npropositional variables, or is locally represented by a model of fixed finite\nsize. In the current paper, we first encode such time-limited computations of a\ndeterministic Turing machine (DTM) in first order logic. We then take a closer\nlook at DTMs that solve SAT. When the length of the input string to such a DTM\nthat contains effectively encoded instances of SAT is parameterized by the\nnatural number $M$, we proceed to show that the corresponding $FO$ theory\n$SAT_M$ has a lower bound on the size of its models that grows almost\nexponentially with $M$. This lower bound on model size also translates into a\nlower bound on the deterministic time complexity of SAT.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 09:13:25 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Kauranne", "Tuomo", ""]]}, {"id": "1410.5000", "submitter": "Jad Hamza", "authors": "Jad Hamza", "title": "On the complexity of Linearizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was shown in Alur et al. [1] that the problem of verifying finite\nconcurrent systems through Linearizability is in EXPSPACE. However, there was\nstill a complexity gap between the easy to obtain PSPACE lower bound and the\nEXPSPACE upper bound. We show in this paper that Linearizability is\nEXPSPACE-complete.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 20:17:36 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 12:28:49 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Hamza", "Jad", ""]]}, {"id": "1410.5037", "submitter": "Jonni Virtema", "authors": "Juha Kontinen, Antti Kuusisto, Jonni Virtema", "title": "Decidability of predicate logics with team semantics", "comments": "Extended version of a MFCS 2016 article. Changes on the earlier arXiv\n  version: title changed, added the result on validity of two-variable\n  dependence logic, restructuring", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of predicate logics based on team semantics. We show\nthat the satisfiability problems of two-variable independence logic and\ninclusion logic are both NEXPTIME-complete. Furthermore, we show that the\nvalidity problem of two-variable dependence logic is undecidable, thereby\nsolving an open problem from the team semantics literature. We also briefly\nanalyse the complexity of the Bernays-Sch\\\"onfinkel-Ramsey prefix classes of\ndependence logic.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 05:14:54 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 10:49:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kontinen", "Juha", ""], ["Kuusisto", "Antti", ""], ["Virtema", "Jonni", ""]]}, {"id": "1410.5053", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida", "title": "Gowers Norm, Function Limits, and Parameter Estimation", "comments": "arXiv admin note: text overlap with arXiv:1212.3849, arXiv:1308.4108\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{f_i:\\mathbb{F}_p^i \\to \\{0,1\\}\\}$ be a sequence of functions, where\n$p$ is a fixed prime and $\\mathbb{F}_p$ is the finite field of order $p$. The\nlimit of the sequence can be syntactically defined using the notion of\nultralimit. Inspired by the Gowers norm, we introduce a metric over limits of\nfunction sequences, and study properties of it. One application of this metric\nis that it provides a characterization of affine-invariant parameters of\nfunctions that are constant-query estimable. Using this characterization, we\nshow that the property of being a function of a constant number of low-degree\npolynomials and a constant number of factored polynomials (of arbitrary\ndegrees) is constant-query testable if it is closed under blowing-up. Examples\nof this property include the property of having a constant spectral norm and\ndegree-structural properties with rank conditions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 09:50:04 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 05:28:07 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Yoshida", "Yuichi", ""]]}, {"id": "1410.5152", "submitter": "Shanghua Teng", "authors": "Christian Borgs and Jennifer Chayes and Adrian Marple and Shang-Hua\n  Teng", "title": "Fixed-Points of Social Choice: An Axiomatic Approach to Network\n  Communities", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.DS cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first social choice theory approach to the question of what\nconstitutes a community in a social network. Inspired by the classic\npreferences models in social choice theory, we start from an abstract social\nnetwork framework, called preference networks; these consist of a finite set of\nmembers where each member has a total-ranking preference of all members in the\nset.\n  Within this framework, we develop two complementary approaches to\naxiomatically study the formation and structures of communities. (1) We apply\nsocial choice theory and define communities indirectly by postulating that they\nare fixed points of a preference aggregation function obeying certain desirable\naxioms. (2) We directly postulate desirable axioms for communities without\nreference to preference aggregation, leading to eight natural community axioms.\n  These approaches allow us to formulate and analyze community rules. We prove\na taxonomy theorem that provides a structural characterization of the family of\ncommunity rules that satisfies all eight axioms. The structure is actually\nquite beautiful: these community rules form a bounded lattice under the natural\nintersection and union operations. Our structural theorem is complemented with\na complexity result: while identifying a community by the most selective rule\nof the lattice is in P, deciding if a subset is a community by the most\ncomprehensive rule of the lattice is coNP-complete. Our studies also shed light\non the limitations of defining community rules solely based on preference\naggregation: any aggregation function satisfying Arrow's IIA axiom, or based on\ncommonly used aggregation schemes like the Borda count or generalizations\nthereof, lead to communities which violate at least one of our community\naxioms. Finally, we give a polynomial-time rule consistent with seven axioms\nand weakly satisfying the eighth axiom.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 04:37:23 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Marple", "Adrian", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1410.5169", "submitter": "Vikram Nathan", "authors": "Michael Mitzenmacher and Vikram Nathan", "title": "Hardness of Peeling with Stashes", "comments": "12 pages (including title/abstract), 6 JPEG black/white figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of several algorithms and data structures can be framed as a\npeeling process on a random hypergraph: vertices with degree less than k and\ntheir adjacent edges are removed until no vertices of degree less than k are\nleft. Often the question is whether the remaining hypergraph, the k-core, is\nempty or not. In some settings, it may be possible to remove either vertices or\nedges from the hypergraph before peeling, at some cost. For example, in hashing\napplications where keys correspond to edges and buckets to vertices, one might\nuse an additional side data structure, commonly referred to as a stash, to\nseparately handle some keys in order to avoid collisions. The natural question\nin such cases is to find the minimum number of edges (or vertices) that need to\nbe stashed in order to realize an empty k-core. We show that both these\nproblems are NP-complete for all $k \\geq 2$ on graphs and regular hypergraphs,\nwith the sole exception being that the edge variant of stashing is solvable in\npolynomial time for $k = 2$ on standard (2-uniform) graphs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 07:29:19 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 22:54:48 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Mitzenmacher", "Michael", ""], ["Nathan", "Vikram", ""]]}, {"id": "1410.5186", "submitter": "Dominikus Kr\\\"uger", "authors": "Britta Dorn and Dominikus Kr\\\"uger", "title": "On the Hardness of Bribery Variants in Voting with CP-Nets", "comments": "improved readability; identified Cheapest Subsets to be the\n  enumeration variant of K.th Largest Subset, so we renamed it to K-Smallest\n  Subsets and point to the literatur; some more typos fixed", "journal-ref": null, "doi": "10.1007/s10472-015-9469-3", "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We continue previous work by Mattei et al. (Mattei, N., Pini, M., Rossi, F.,\nVenable, K.: Bribery in voting with CP-nets. Ann. of Math. and Artif. Intell.\npp. 1--26 (2013)) in which they study the computational complexity of bribery\nschemes when voters have conditional preferences that are modeled by CP-nets.\nFor most of the cases they considered, they could show that the bribery problem\nis solvable in polynomial time. Some cases remained open---we solve two of them\nand extend the previous results to the case that voters are weighted. Moreover,\nwe consider negative (weighted) bribery in CP-nets, when the briber is not\nallowed to pay voters to vote for his preferred candidate.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 08:23:00 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 15:06:50 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 12:26:22 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Dorn", "Britta", ""], ["Kr\u00fcger", "Dominikus", ""]]}, {"id": "1410.5191", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Mark Jones, and Magnus Wahlstrom", "title": "Structural Parameterizations of the Mixed Chinese Postman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Mixed Chinese Postman Problem (MCPP), given a weighted mixed graph $G$\n($G$ may have both edges and arcs), our aim is to find a minimum weight closed\nwalk traversing each edge and arc at least once. The MCPP parameterized by the\nnumber of edges in $G$ or the number of arcs in $G$ is fixed-parameter\ntractable as proved by van Bevern {\\em et al.} (in press) and Gutin, Jones and\nSheng (ESA 2014), respectively. In this paper, we consider the unweighted\nversion of MCPP. Solving an open question of van Bevern {\\em et al.} (in\npress), we show that somewhat unexpectedly MCPP parameterized by the\n(undirected) treewidth of $G$ is W[1]-hard. In fact, we prove that even the\nMCPP parameterized by the pathwidth of $G$ is W[1]-hard. On the positive side,\nwe show that the unweighted version of MCPP parameterized by tree-depth is\nfixed-parameter tractable. We are unaware of any natural graph parameters\nbetween pathwidth and tree-depth and so our results provide a dichotomy of the\ncomplexity of MCPP. Furthermore, we believe that MCPP is the first problem\nknown to be W[1]-hard with respect to treewidth but FPT with respect to\ntree-depth.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 08:37:11 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 19:12:30 GMT"}, {"version": "v3", "created": "Sun, 25 Jan 2015 10:35:57 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "1410.5297", "submitter": "Delaram Kahrobaei", "authors": "Bren Cavallo and Delaram Kahrobaei", "title": "A Polynomial Time Algorithm For The Conjugacy Decision and Search\n  Problems in Free Abelian-by-Infinite Cyclic Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a polynomial time algorithm that solves both the\nconjugacy decision and search problems in free abelian-by-infinite cyclic\ngroups where the input is elements in normal form. We do this by adapting the\nwork of Bogopolski, Martino, Maslakova, and Ventura in\n\\cite{bogopolski2006conjugacy} and Bogopolski, Martino, and Ventura in\n\\cite{bogopolski2010orbit}, to free abelian-by-infinite cyclic groups, and in\ncertain cases apply a polynomial time algorithm for the orbit problem over\n$\\Z^n$ by Kannan and Lipton.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 14:42:46 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Cavallo", "Bren", ""], ["Kahrobaei", "Delaram", ""]]}, {"id": "1410.5369", "submitter": "Hubie Chen", "authors": "Hubie Chen", "title": "Proof Complexity Modulo the Polynomial Hierarchy: Understanding\n  Alternation as a Source of Hardness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study a framework in which one can present alternation-based\nlower bounds on proof length in proof systems for quantified Boolean formulas.\nA key notion in this framework is that of proof system ensemble, which is\n(essentially) a sequence of proof systems where, for each, proof checking can\nbe performed in the polynomial hierarchy. We introduce a proof system ensemble\ncalled relaxing QU-res which is based on the established proof system\nQU-resolution. Our main results include an exponential separation of the\ntree-like and general versions of relaxing QU-res, and an exponential lower\nbound for relaxing QU-res; these are analogs of classical results in\npropositional proof complexity.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 17:43:24 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 15:18:08 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 16:06:47 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Chen", "Hubie", ""]]}, {"id": "1410.5555", "submitter": "Mikhail Tikhomirov", "authors": "Mikhail Tikhomirov", "title": "On computational complexity of length embeddability of graphs", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ is embeddable in $\\mathbb{R}^d$ if vertices of $G$ can be\nassigned with points of $\\mathbb{R}^d$ in such a way that all pairs of adjacent\nvertices are at the distance 1. We show that verifying embeddability of a given\ngraph in $\\mathbb{R}^d$ is NP-hard in the case $d > 2$ for all reasonable\nnotions of embeddability.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 07:01:54 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Tikhomirov", "Mikhail", ""]]}, {"id": "1410.5792", "submitter": "Andrey Bogomolov", "authors": "Andrey Bogomolov, Bruno Lepri, Fabio Pianesi", "title": "Generalized Compression Dictionary Distance as Universal Similarity\n  Measure", "comments": "2014 Conference on Big Data from Space (BiDS 14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new similarity measure based on information theoretic measures\nwhich is superior than Normalized Compression Distance for clustering problems\nand inherits the useful properties of conditional Kolmogorov complexity. We\nshow that Normalized Compression Dictionary Size and Normalized Compression\nDictionary Entropy are computationally more efficient, as the need to perform\nthe compression itself is eliminated. Also they scale linearly with exponential\nvector size growth and are content independent. We show that normalized\ncompression dictionary distance is compressor independent, if limited to\nlossless compressors, which gives space for optimizations and implementation\nspeed improvement for real-time and big data applications. The introduced\nmeasure is applicable for machine learning tasks of parameter-free unsupervised\nclustering, supervised learning such as classification and regression, feature\nselection, and is applicable for big data problems with order of magnitude\nspeed increase.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 19:17:19 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bogomolov", "Andrey", ""], ["Lepri", "Bruno", ""], ["Pianesi", "Fabio", ""]]}, {"id": "1410.5845", "submitter": "Michael O'Brien", "authors": "Aaron Adcock, Erik D. Demaine, Martin L. Demaine, Michael P. O'Brien,\n  Felix Reidl, Fernando S\\'anchez Villaamil, and Blair D. Sullivan", "title": "Zig-Zag Numberlink is NP-Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When can $t$ terminal pairs in an $m \\times n$ grid be connected by $t$\nvertex-disjoint paths that cover all vertices of the grid? We prove that this\nproblem is NP-complete. Our hardness result can be compared to two previous\nNP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices''\nconstraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted\nto have the fewest possible corners within their homotopy class. The latter\nrestriction is a common form of the famous Nikoli puzzle \\emph{Numberlink}; our\nproblem is another common form of Numberlink, sometimes called \\emph{Zig-Zag\nNumberlink} and popularized by the smartphone app \\emph{Flow Free}.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 20:27:20 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Adcock", "Aaron", ""], ["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["O'Brien", "Michael P.", ""], ["Reidl", "Felix", ""], ["Villaamil", "Fernando S\u00e1nchez", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1410.6072", "submitter": "Lek-Heng Lim", "authors": "Shmuel Friedland and Lek-Heng Lim", "title": "Nuclear Norm of Higher-Order Tensors", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish several mathematical and computational properties of the nuclear\nnorm for higher-order tensors. We show that like tensor rank, tensor nuclear\nnorm is dependent on the choice of base field --- the value of the nuclear norm\nof a real 3-tensor depends on whether we regard it as a real 3-tensor or a\ncomplex 3-tensor with real entries. We show that every tensor has a nuclear\nnorm attaining decomposition and every symmetric tensor has a symmetric nuclear\nnorm attaining decomposition. There is a corresponding notion of nuclear rank\nthat, unlike tensor rank, is upper semicontinuous. We establish an analogue of\nBanach's theorem for tensor spectral norm and Comon's conjecture for tensor\nrank --- for a symmetric tensor, its symmetric nuclear norm always equals its\nnuclear norm. We show that computing tensor nuclear norm is NP-hard in several\nsense. Deciding weak membership in the nuclear norm unit ball of 3-tensors is\nNP-hard, as is finding an $\\varepsilon$-approximation of nuclear norm for\n3-tensors. In addition, the problem of computing spectral or nuclear norm of a\n4-tensor is NP-hard, even if we restrict the 4-tensor to be bi-Hermitian,\nbisymmetric, positive semidefinite, nonnegative valued, or all of the above. We\ndiscuss some simple polynomial-time approximation bounds. As an aside, we show\nthat the nuclear $(p,q)$-norm of a matrix is NP-hard in general but can be\ncomputed in polynomial-time if $p=1$, $q = 1$, or $p=q=2$, with closed-form\nexpressions for the nuclear $(1,q)$- and $(p,1)$-norms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 15:23:45 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 05:18:54 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 03:21:47 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Friedland", "Shmuel", ""], ["Lim", "Lek-Heng", ""]]}, {"id": "1410.6121", "submitter": "Branislav Nikolic", "authors": "J. K. Freericks, B. K. Nikolic, and O. Frieder", "title": "The Nonequilibrium Many-Body Problem as a paradigm for extreme data\n  science", "comments": "33 pages, 7 figures, invited review for Int. J. Mod. Phys. B;\n  published version with additional references", "journal-ref": "Int J. Mod. Phys. B 28, 1430021 (2014)", "doi": "10.1142/S0217979214300217", "report-no": null, "categories": "cond-mat.str-el cond-mat.stat-mech cs.CC cs.CE math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating big data pervades much of physics. But some problems, which we\ncall extreme data problems, are too large to be treated within big data\nscience. The nonequilibrium quantum many-body problem on a lattice is just such\na problem, where the Hilbert space grows exponentially with system size and\nrapidly becomes too large to fit on any computer (and can be effectively\nthought of as an infinite-sized data set). Nevertheless, much progress has been\nmade with computational methods on this problem, which serve as a paradigm for\nhow one can approach and attack extreme data problems. In addition, viewing\nthese physics problems from a computer-science perspective leads to new\napproaches that can be tried to solve them more accurately and for longer\ntimes. We review a number of these different ideas here.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 17:55:53 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 18:21:56 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Freericks", "J. K.", ""], ["Nikolic", "B. K.", ""], ["Frieder", "O.", ""]]}, {"id": "1410.6298", "submitter": "Erika De Benedetti", "authors": "Erika De Benedetti and Simona Ronchi Della Rocca", "title": "A type assignment for lambda-calculus complete both for FPTIME and\n  strong normalization", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the aims of Implicit Computational Complexity is the design of\nprogramming languages with bounded computational complexity; indeed,\nguaranteeing and certifying a limited resources usage is of central importance\nfor various aspects of computer science. One of the more promising approaches\nto this aim is based on the use of lambda-calculus as paradigmatic programming\nlanguage and the design of type assignment systems for lambda-terms, where\ntypes guarantee both the functional correctness and the complexity bound. Here\nwe propose a system of stratified types, inspired by intersection types, where\nintersection is a non-associative operator. The system, called STR, is correct\nand complete for polynomial time computations; moreover, all the strongly\nnormalizing terms are typed in it, thus increasing the typing power with\nrespect to the previous proposals. Moreover, STR enjoys a stronger expressivity\nwith respect to the previous system STA, since it allows to type a restricted\nversion of iteration.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 09:19:36 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["De Benedetti", "Erika", ""], ["Della Rocca", "Simona Ronchi", ""]]}, {"id": "1410.6396", "submitter": "Marzio De Biasi", "authors": "Marzio De Biasi", "title": "Permutation Reconstruction from Differences", "comments": "22 pages, appears in The Electronic Journal of Combinatorics 21(4)\n  (2014); #P4.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the problem of reconstructing a permutation\n$\\pi_1,\\dotsc,\\pi_n$ of the integers $[1\\dotso n]$ given the absolute\ndifferences $|\\pi_{i+1}-\\pi_i|$, $i = 1,\\dotsc,n-1$ is NP-complete. As an\nintermediate step we first prove the NP-completeness of the decision version of\na new puzzle game that we call Crazy Frog Puzzle. The permutation\nreconstruction from differences is one of the simplest combinatorial problems\nthat have been proved to be computationally intractable.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 15:28:42 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 10:22:18 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["De Biasi", "Marzio", ""]]}, {"id": "1410.6400", "submitter": "Tobias Friedrich", "authors": "Nikolaos Fountoulakis, Tobias Friedrich, Danny Hermelin", "title": "On the Average-case Complexity of Parameterized Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-Clique problem is a fundamental combinatorial problem that plays a\nprominent role in classical as well as in parameterized complexity theory. It\nis among the most well-known NP-complete and W[1]-complete problems. Moreover,\nits average-case complexity analysis has created a long thread of research\nalready since the 1970s. Here, we continue this line of research by studying\nthe dependence of the average-case complexity of the k-Clique problem on the\nparameter k. To this end, we define two natural parameterized analogs of\nefficient average-case algorithms. We then show that k-Clique admits both\nanalogues for Erd\\H{o}s-R\\'{e}nyi random graphs of arbitrary density. We also\nshow that k-Clique is unlikely to admit neither of these analogs for some\nspecific computable input distribution.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 15:33:01 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Fountoulakis", "Nikolaos", ""], ["Friedrich", "Tobias", ""], ["Hermelin", "Danny", ""]]}, {"id": "1410.6663", "submitter": "Laurent Bulteau", "authors": "Laurent Bulteau, Gustavo Sacomoto, Blerina Sinaimeri", "title": "Computing an Evolutionary Ordering is Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that computing an evolutionary ordering of a family of sets, i.e. an\nordering where each set intersects with --but is not included in-- the union\nearlier sets, is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 12:39:08 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Bulteau", "Laurent", ""], ["Sacomoto", "Gustavo", ""], ["Sinaimeri", "Blerina", ""]]}, {"id": "1410.6806", "submitter": "David Rolnick", "authors": "Jes\\'us A. De Loera, Susan Margulies, Michael Pernpeintner, Eric\n  Riedl, David Rolnick, Gwen Spencer, Despina Stasi, Jon Swenson", "title": "Gr\\\"obner Bases and Nullstellens\\\"atze for Graph-Coloring Ideals", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC math.AC math.AG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a well-known family of polynomial ideals encoding the problem of\ngraph-$k$-colorability. Our paper describes how the inherent combinatorial\nstructure of the ideals implies several interesting algebraic properties.\nSpecifically, we provide lower bounds on the difficulty of computing Gr\\\"obner\nbases and Nullstellensatz certificates for the coloring ideals of general\ngraphs. For chordal graphs, however, we explicitly describe a Gr\\\"obner basis\nfor the coloring ideal, and provide a polynomial-time algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 19:25:05 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["De Loera", "Jes\u00fas A.", ""], ["Margulies", "Susan", ""], ["Pernpeintner", "Michael", ""], ["Riedl", "Eric", ""], ["Rolnick", "David", ""], ["Spencer", "Gwen", ""], ["Stasi", "Despina", ""], ["Swenson", "Jon", ""]]}, {"id": "1410.7082", "submitter": "Aleksandr Maksimenko", "authors": "Aleksandr Maksimenko", "title": "Complexity of LP in Terms of the Face Lattice", "comments": "11 pages", "journal-ref": "Journal of Applied and Industrial Mathematics, 10(3), pp 370-379,\n  2016", "doi": "10.1134/S1990478916030078", "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a finite set in $Z^d$. We consider the problem of optimizing\nlinear function $f(x) = c^T x$ on $X$, where $c\\in Z^d$ is an input vector. We\ncall it a problem $X$. A problem $X$ is related with linear program\n$\\max\\limits_{x \\in P} f(x)$, where polytope $P$ is a convex hull of $X$. The\nkey parameters for evaluating the complexity of a problem $X$ are the dimension\n$d$, the cardinality $|X|$, and the encoding size $S(X) = \\log_2\n\\left(\\max\\limits_{x\\in X} \\|x\\|_{\\infty}\\right)$. We show that if the (time\nand space) complexity of some algorithm $A$ for solving a problem $X$ is\ndefined only in terms of combinatorial structure of $P$ and the size $S(X)$,\nthen for every $d$ and $n$ there exists polynomially (in $d$, $\\log n$, and\n$S$) solvable problem $Y$ with $\\dim Y = d$, $|Y| = n$, such that the algorithm\n$A$ requires exponential time or space for solving $Y$.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 20:55:24 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Maksimenko", "Aleksandr", ""]]}, {"id": "1410.7237", "submitter": "Tobias Jacobs", "authors": "Tobias Jacobs and Salvatore Longo", "title": "A New Perspective on the Windows Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Windows Scheduling Problem, also known as the Pinwheel Problem, is to\nschedule periodic jobs subject to their processing frequency demands. Instances\nare given as a set of jobs that have to be processed infinitely often such that\nthe time interval between two consecutive executions of the same job j is no\nlonger than the job's given period $p_j$.\n  The key contribution of this work is a new interpretation of the problem\nvariant with exact periods, where the time interval between consecutive\nexecutions must be strictly $p_j$. We show that this version is equivalent to a\nnatural combinatorial problem we call Partial Coding. Reductions in both\ndirections can be realized in polynomial time, so that both hardness proofs and\nalgorithms for Partial Coding transfer to Windows Scheduling.\n  Applying this new perspective, we obtain a number of new results regarding\nthe computational complexity of various Windows Scheduling Problem variants. We\nprove that even the case of one processor and unit-length jobs does not admit a\npseudo-polynomial time algorithm unless SAT can be solved by a randomized\nmethod in expected quasi-polynomial time. This result also extends to the case\nof inexact periods, which answers a question that has remained open for more\nthan two decades. Furthermore, we report an error found in a hardness proof\npreviously given for the multi-machine case without machine migration, and we\nshow that this variant reduces to the single-machine case. Finally, we prove\nthat even with unit-length jobs the problem is co-NP-hard when jobs are allowed\nto migrate between machines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:49:35 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Jacobs", "Tobias", ""], ["Longo", "Salvatore", ""]]}, {"id": "1410.7253", "submitter": "Ariel Gabizon", "authors": "Abhishek Bhowmick, Ariel Gabizon, Th\\'ai Ho\\`ang L\\^e, David Zuckerman", "title": "Deterministic Extractors for Additive Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model of a weakly random source that admits randomness\nextraction. Our model of additive sources includes such natural sources as\nuniform distributions on arithmetic progressions (APs), generalized arithmetic\nprogressions (GAPs), and Bohr sets, each of which generalizes affine sources.\nWe give an explicit extractor for additive sources with linear min-entropy over\nboth $\\mathbb{Z}_p$ and $\\mathbb{Z}_p^n$, for large prime $p$, although our\nresults over $\\mathbb{Z}_p^n$ require that the source further satisfy a\nlist-decodability condition. As a corollary, we obtain explicit extractors for\nAPs, GAPs, and Bohr sources with linear min-entropy, although again our results\nover $\\mathbb{Z}_p^n$ require the list-decodability condition. We further\nexplore special cases of additive sources. We improve previous constructions of\nline sources (affine sources of dimension 1), requiring a field of size linear\nin $n$, rather than $\\Omega(n^2)$ by Gabizon and Raz. This beats the\nnon-explicit bound of $\\Theta(n \\log n)$ obtained by the probabilistic method.\nWe then generalize this result to APs and GAPs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 14:29:33 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Bhowmick", "Abhishek", ""], ["Gabizon", "Ariel", ""], ["L\u00ea", "Th\u00e1i Ho\u00e0ng", ""], ["Zuckerman", "David", ""]]}, {"id": "1410.7328", "submitter": "Paul Vitanyi", "authors": "P.M.B. Vitanyi (CWI and University of Amsterdam)", "title": "Exact Expression For Information Distance", "comments": "6 pages LaTeX. added material and corrected it", "journal-ref": "IEEE Trans. Inform. Theory, 63:8(2017), 4725-4728", "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.CV cs.DM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information distance can be defined not only between two strings but also in\na finite multiset of strings of cardinality greater than two. We give an\nelementary proof for expressing the information distance in terms of plain\nKolmogorov complexity. It is exact since for each cardinality of the multiset\nthe lower bound for some multiset equals the upper bound for all multisets up\nto a constant additive term.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 17:46:57 GMT"}, {"version": "v10", "created": "Tue, 11 Jul 2017 17:07:32 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 17:53:43 GMT"}, {"version": "v3", "created": "Wed, 29 Oct 2014 17:25:56 GMT"}, {"version": "v4", "created": "Thu, 30 Oct 2014 18:19:10 GMT"}, {"version": "v5", "created": "Fri, 31 Oct 2014 16:31:08 GMT"}, {"version": "v6", "created": "Mon, 9 Feb 2015 16:49:52 GMT"}, {"version": "v7", "created": "Mon, 1 Jun 2015 16:37:02 GMT"}, {"version": "v8", "created": "Tue, 2 Jun 2015 15:36:38 GMT"}, {"version": "v9", "created": "Fri, 2 Oct 2015 16:05:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Vitanyi", "P. M. B.", "", "CWI and University of Amsterdam"]]}, {"id": "1410.7583", "submitter": "Romain Hollanders", "authors": "Romain Hollanders, Bal\\'azs Gerencs\\'er, Jean-Charles Delvenne and\n  Rapha\\\"el M. Jungers", "title": "Improved bound on the worst case complexity of Policy Iteration", "comments": "11 pages, 1 figure, submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Markov Decision Processes (MDPs) is a recurrent task in engineering.\nEven though it is known that solutions for minimizing the infinite horizon\nexpected reward can be found in polynomial time using Linear Programming\ntechniques, iterative methods like the Policy Iteration algorithm (PI) remain\nusually the most efficient in practice. This method is guaranteed to converge\nin a finite number of steps. Unfortunately, it is known that it may require an\nexponential number of steps in the size of the problem to converge. On the\nother hand, many open questions remain considering the actual worst case\ncomplexity. In this work, we provide the first improvement over the fifteen\nyears old upper bound from Mansour & Singh (1999) by showing that PI requires\nat most k/(k-1)*k^n/n + o(k^n/n) iterations to converge, where n is the number\nof states of the MDP and k is the maximum number of actions per state. Perhaps\nmore importantly, we also show that this bound is optimal for an important\nrelaxation of the problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 11:07:54 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Hollanders", "Romain", ""], ["Gerencs\u00e9r", "Bal\u00e1zs", ""], ["Delvenne", "Jean-Charles", ""], ["Jungers", "Rapha\u00ebl M.", ""]]}, {"id": "1410.8202", "submitter": "Jesko H\\\"uttenhain", "authors": "Jesko H\\\"uttenhain and Christian Ikenmeyer", "title": "Binary Determinantal Complexity", "comments": "10 pages, C source code for the computation available as ancillary\n  files", "journal-ref": "Linear Algebra and its Applications, 504:559-573, 2016", "doi": "10.1016/j.laa.2016.04.027", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that for writing the 3 by 3 permanent polynomial as a determinant of\na matrix consisting only of zeros, ones, and variables as entries, a 7 by 7\nmatrix is required. Our proof is computer based and uses the enumeration of\nbipartite graphs. Furthermore, we analyze sequences of polynomials that are\ndeterminants of polynomially sized matrices consisting only of zeros, ones, and\nvariables. We show that these are exactly the sequences in the complexity class\nof constant free polynomially sized (weakly) skew circuits.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 23:57:38 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 22:20:31 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["H\u00fcttenhain", "Jesko", ""], ["Ikenmeyer", "Christian", ""]]}, {"id": "1410.8253", "submitter": "Pascal Van Hentenryck", "authors": "Karsten Lehmann, Alban Grastien, and Pascal Van Hentenryck", "title": "AC-Feasibility on Tree Networks is NP-Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed significant interest in convex relaxations of the\npower flows, several papers showing that the second-order cone relaxation is\ntight for tree networks under various conditions on loads or voltages. This\npaper shows that AC-feasibility, i.e., to find whether some generator dispatch\ncan satisfy a given demand, is NP-Hard for tree networks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 04:50:34 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Lehmann", "Karsten", ""], ["Grastien", "Alban", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1410.8420", "submitter": "Cl\\'ement Canonne", "authors": "Eric Blais, Cl\\'ement L. Canonne, Igor C. Oliveira, Rocco A. Servedio\n  and Li-Yang Tan", "title": "Learning circuits with few negations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotone Boolean functions, and the monotone Boolean circuits that compute\nthem, have been intensively studied in complexity theory. In this paper we\nstudy the structure of Boolean functions in terms of the minimum number of\nnegations in any circuit computing them, a complexity measure that interpolates\nbetween monotone functions and the class of all functions. We study this\ngeneralization of monotonicity from the vantage point of learning theory,\ngiving near-matching upper and lower bounds on the uniform-distribution\nlearnability of circuits in terms of the number of negations they contain. Our\nupper bounds are based on a new structural characterization of negation-limited\ncircuits that extends a classical result of A. A. Markov. Our lower bounds,\nwhich employ Fourier-analytic tools from hardness amplification, give new\nresults even for circuits with no negations (i.e. monotone functions).\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 16:10:26 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Blais", "Eric", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Oliveira", "Igor C.", ""], ["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1410.8585", "submitter": "J. M. Landsberg", "authors": "Shrawan Kumar and J.M. Landsberg", "title": "Connections between conjectures of Alon-Tarsi, Hadamard-Howe, and\n  integrals over the special unitary group", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC math.CO math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the Alon-Tarsi conjecture on Latin squares is equivalent to a very\nspecial case of a conjecture made independently by Hadamard and Howe, and to\nthe non-vanishing of some interesting integrals over SU(n). Our investigations\nwere motivated by geometric complexity theory.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:54:57 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Kumar", "Shrawan", ""], ["Landsberg", "J. M.", ""]]}, {"id": "1410.8594", "submitter": "Santiago Figueira", "authors": "Javier Almarza and Santiago Figueira", "title": "Normality in non-integer bases and polynomial time randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that if $x\\in[0,1]$ is polynomial time random (i.e. no polynomial\ntime computable martingale succeeds on the binary fractional expansion of $x$)\nthen $x$ is normal in any integer base greater than one. We show that if $x$ is\npolynomial time random and $\\beta>1$ is Pisot, then $x$ is \"normal in base\n$\\beta$\", in the sense that the sequence $(x\\beta^n)_{n\\in\\mathbb{N}}$ is\nuniformly distributed modulo one. We work with the notion of \"$P$-martingale\",\na generalization of martingales to non-uniform distributions, and show that a\nsequence over a finite alphabet is distributed according to an irreducible,\ninvariant Markov measure~$P$ if an only if no $P$-martingale whose betting\nfactors are computed by a deterministic finite automaton succeeds on it. This\nis a generalization of Schnorr and Stimm's characterization of normal sequences\nin integer bases. Our results use tools and techniques from symbolic dynamics,\ntogether with automata theory and algorithmic randomness.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 00:11:57 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Almarza", "Javier", ""], ["Figueira", "Santiago", ""]]}, {"id": "1410.8816", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun, Sebastian Pokutta, Daniel Zink", "title": "Affine reductions for LPs and SDPs", "comments": "Minor updates, correction of typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a reduction mechanism for LP and SDP formulations that degrades\napproximation factors in a controlled fashion. Our reduction mechanism is a\nminor restriction of classical reductions establishing inapproximability in the\ncontext of PCP theorems. As a consequence we establish strong linear\nprogramming inapproximability (for LPs with a polynomial number of constraints)\nfor many problems. In particular we obtain a $3/2-\\varepsilon$\ninapproximability for VertexCover answering an open question in\n[arXiv:1309.0563] and we answer a weak version of our sparse graph conjecture\nposed in [arXiv:1311.4001] showing an inapproximability factor of\n$1/2+\\varepsilon$ for bounded degree IndependentSet. In the case of SDPs, we\nobtain inapproximability results for these problems relative to the\nSDP-inapproximability of MaxCUT. Moreover, using our reduction framework we are\nable to reproduce various results for CSPs from [arXiv:1309.0563] via simple\nreductions from Max-2-XOR.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 17:12:00 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 20:45:34 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 20:45:19 GMT"}, {"version": "v4", "created": "Tue, 12 May 2015 16:14:08 GMT"}, {"version": "v5", "created": "Tue, 19 Jan 2016 15:52:56 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Pokutta", "Sebastian", ""], ["Zink", "Daniel", ""]]}, {"id": "1410.8819", "submitter": "Manuel Sorge", "authors": "Stefan Kratsch and Manuel Sorge", "title": "On Kernelization and Approximation for the Vector Connectivity Problem", "comments": "Non-constructive Kernelization argument, improved technical details\n  of signatures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Vector Connectivity problem we are given an undirected graph\n$G=(V,E)$, a demand function $\\phi\\colon V\\to\\{0,\\ldots,d\\}$, and an integer\n$k$. The question is whether there exists a set $S$ of at most $k$ vertices\nsuch that every vertex $v\\in V\\setminus S$ has at least $\\phi(v)$\nvertex-disjoint paths to $S$; this abstractly captures questions about placing\nservers or warehouses relative to demands. The problem is \\NP-hard already for\ninstances with $d=4$ (Cicalese et al., arXiv '14), admits a log-factor\napproximation (Boros et al., Networks '14), and is fixed-parameter tractable in\nterms of~$k$ (Lokshtanov, unpublished '14). We prove several results regarding\nkernelization and approximation for Vector Connectivity and the variant Vector\n$d$-Connectivity where the upper bound $d$ on demands is a fixed constant. For\nVector $d$-Connectivity we give a factor $d$-approximation algorithm and\nconstruct a vertex-linear kernelization, i.e., an efficient reduction to an\nequivalent instance with $f(d)k=O(k)$ vertices. For Vector Connectivity we have\na factor $\\text{opt}$-approximation and we can show that it has no\nkernelization to size polynomial in $k$ or even $k+d$ unless\n$\\mathsf{NP\\subseteq coNP/poly}$, making $f(d)\\operatorname{poly}(k)$ optimal\nfor Vector $d$-Connectivity. Finally, we provide a write-up for fixed-parameter\ntractability of Vector Connectivity($k$) by giving an alternative FPT algorithm\nbased on matroid intersection.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 17:19:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 12:01:35 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Kratsch", "Stefan", ""], ["Sorge", "Manuel", ""]]}, {"id": "1410.8852", "submitter": "Jonathan Kochems", "authors": "Jonathan Kochems and C.-H. Luke Ong", "title": "Decidable Models of Recursive Asynchronous Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronously communicating pushdown systems (ACPS) that satisfy the\nempty-stack constraint (a pushdown process may receive only when its stack is\nempty) are a popular decidable model for recursive programs with asynchronous\natomic procedure calls. We study a relaxation of the empty-stack constraint for\nACPS that permits concurrency and communication actions at any stack height,\ncalled the shaped stack constraint, thus enabling a larger class of concurrent\nprograms to be modelled. We establish a close connection between ACPS with\nshaped stacks and a novel extension of Petri nets: Nets with Nested Coloured\nTokens (NNCTs). Tokens in NNCTs are of two types: simple and complex. Complex\ntokens carry an arbitrary number of coloured tokens. The rules of NNCT can\nsynchronise complex and simple tokens, inject coloured tokens into a complex\ntoken, and eject all tokens of a specified set of colours to predefined places.\nWe show that the coverability problem for NNCTs is Tower-complete. To our\nknowledge, NNCT is the first extension of Petri nets, in the class of nets with\nan infinite set of token types, that has primitive recursive coverability. This\nresult implies Tower-completeness of coverability for ACPS with shaped stacks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:01:39 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 22:08:13 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kochems", "Jonathan", ""], ["Ong", "C. -H. Luke", ""]]}]