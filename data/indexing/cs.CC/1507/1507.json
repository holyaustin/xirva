[{"id": "1507.00026", "submitter": "Qin Zhang", "authors": "Qin Zhang", "title": "On the Communication Complexity of Distributed Clustering", "comments": "This paper has been withdrawn by the author due to the fact that\n  simpler proofs exist; see arXiv:1702.00196", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a first set of communication lower bounds for\ndistributed clustering problems, in particular, for k-center, k-median and\nk-means. When the input is distributed across a large number of machines and\nthe number of clusters k is small, our lower bounds match the current best\nupper bounds up to a logarithmic factor. We have designed a new composition\nframework in our proofs for multiparty number-in-hand communication complexity\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:29:01 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 17:23:06 GMT"}, {"version": "v3", "created": "Thu, 2 Feb 2017 01:48:20 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Zhang", "Qin", ""]]}, {"id": "1507.00177", "submitter": "Ramprasad Saptharishi", "authors": "Mrinal Kumar, Ramprasad Saptharishi", "title": "An exponential lower bound for homogeneous depth-5 circuits over finite\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we show exponential lower bounds for the class of homogeneous\ndepth-$5$ circuits over all small finite fields. More formally, we show that\nthere is an explicit family $\\{P_d : d \\in \\mathbb{N}\\}$ of polynomials in\n$\\mathsf{VNP}$, where $P_d$ is of degree $d$ in $n = d^{O(1)}$ variables, such\nthat over all finite fields $\\mathbb{F}_q$, any homogeneous depth-$5$ circuit\nwhich computes $P_d$ must have size at least $\\exp(\\Omega_q(\\sqrt{d}))$.\n  To the best of our knowledge, this is the first super-polynomial lower bound\nfor this class for any field $\\mathbb{F}_q \\neq \\mathbb{F}_2$.\n  Our proof builds up on the ideas developed on the way to proving lower bounds\nfor homogeneous depth-$4$ circuits [GKKS13, FLMS13, KLSS14, KS14] and for\nnon-homogeneous depth-$3$ circuits over finite fields [GK98, GR00]. Our key\ninsight is to look at the space of shifted partial derivatives of a polynomial\nas a space of functions from $\\mathbb{F}_q^n \\rightarrow \\mathbb{F}_q$ as\nopposed to looking at them as a space of formal polynomials and builds over a\ntighter analysis of the lower bound of Kumar and Saraf [KS14].\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:39:47 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Kumar", "Mrinal", ""], ["Saptharishi", "Ramprasad", ""]]}, {"id": "1507.00213", "submitter": "Zhaohui Wei", "authors": "Jamie Sikora and Antonios Varvitsiotis and Zhaohui Wei", "title": "Minimum Dimension of a Hilbert Space Needed to Generate a Quantum\n  Correlation", "comments": "5 pages", "journal-ref": "Phys. Rev. Lett. 117, 060401 (2016)", "doi": "10.1103/PhysRevLett.117.060401", "report-no": null, "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a two-party correlation that can be generated by performing local\nmeasurements on a bipartite quantum system. A question of fundamental\nimportance is to understand how many resources, which we quantify by the\ndimension of the underlying quantum system, are needed to reproduce this\ncorrelation. In this Letter, we identify an easy-to-compute lower bound on the\nsmallest Hilbert space dimension needed to generate a given two-party quantum\ncorrelation. We show that our bound is tight on many well-known correlations\nand discuss how it can rule out correlations of having a finite-dimensional\nquantum representation. We show that our bound is multiplicative under product\ncorrelations and also that it can witness the non-convexity of certain\nrestricted-dimensional quantum correlations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:53:07 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 15:06:08 GMT"}, {"version": "v3", "created": "Tue, 25 Oct 2016 12:34:33 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Sikora", "Jamie", ""], ["Varvitsiotis", "Antonios", ""], ["Wei", "Zhaohui", ""]]}, {"id": "1507.00432", "submitter": "Stacey Jeffery", "authors": "Tsuyoshi Ito, Stacey Jeffery", "title": "Approximate Span Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Span programs are a model of computation that have been used to design\nquantum algorithms, mainly in the query model. For any decision problem, there\nexists a span program that leads to an algorithm with optimal quantum query\ncomplexity, but finding such an algorithm is generally challenging.\n  We consider new ways of designing quantum algorithms using span programs. We\nshow how any span program that decides a problem $f$ can also be used to decide\n\"property testing\" versions of $f$, or more generally, approximate the span\nprogram witness size, a property of the input related to $f$. For example,\nusing our techniques, the span program for OR, which can be used to design an\noptimal algorithm for the OR function, can also be used to design optimal\nalgorithms for: threshold functions, in which we want to decide if the Hamming\nweight of a string is above a threshold or far below, given the promise that\none of these is true; and approximate counting, in which we want to estimate\nthe Hamming weight of the input. We achieve these results by relaxing the\nrequirement that 1-inputs hit some target exactly in the span program, which\ncould make design of span programs easier.\n  We also give an exposition of span program structure, which increases the\nunderstanding of this important model. One implication is alternative\nalgorithms for estimating the witness size when the phase gap of a certain\nunitary can be lower bounded. We show how to lower bound this phase gap in some\ncases.\n  As applications, we give the first upper bounds in the adjacency query model\non the quantum time complexity of estimating the effective resistance between\n$s$ and $t$, $R_{s,t}(G)$, of $\\tilde\nO(\\frac{1}{\\epsilon^{3/2}}n\\sqrt{R_{s,t}(G)})$, and, when $\\mu$ is a lower\nbound on $\\lambda_2(G)$, by our phase gap lower bound, we can obtain $\\tilde\nO(\\frac{1}{\\epsilon}n\\sqrt{R_{s,t}(G)/\\mu})$, both using $O(\\log n)$ space.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 05:45:53 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Ito", "Tsuyoshi", ""], ["Jeffery", "Stacey", ""]]}, {"id": "1507.00674", "submitter": "Cibele Freire", "authors": "Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou", "title": "A Characterization of the Complexity of Resilience and Responsibility\n  for Self-join-free Conjunctive Queries", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research thrusts in the area of data management have focused on\nunderstanding how changes in the data affect the output of a view or standing\nquery. Example applications are explaining query results, propagating updates\nthrough views, and anonymizing datasets. These applications usually rely on\nunderstanding how interventions in a database impact the output of a query. An\nimportant aspect of this analysis is the problem of deleting a minimum number\nof tuples from the input tables to make a given Boolean query false. We refer\nto this problem as \"the resilience of a query\" and show its connections to the\nwell-studied problems of deletion propagation and causal responsibility. In\nthis paper, we study the complexity of resilience for self-join-free\nconjunctive queries, and also make several contributions to previous known\nresults for the problems of deletion propagation with source side-effects and\ncausal responsibility: (1) We define the notion of resilience and provide a\ncomplete dichotomy for the class of self-join-free conjunctive queries with\narbitrary functional dependencies; this dichotomy also extends and generalizes\nprevious tractability results on deletion propagation with source side-effects.\n(2) We formalize the connection between resilience and causal responsibility,\nand show that resilience has a larger class of tractable queries than\nresponsibility. (3) We identify a mistake in a previous dichotomy for the\nproblem of causal responsibility and offer a revised characterization based on\nnew, simpler, and more intuitive notions. (4) Finally, we extend the dichotomy\nfor causal responsibility in two ways: (a) we treat cases where the input\ntables contain functional dependencies, and (b) we compute responsibility for a\nset of tuples specified via wildcards.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 17:45:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Freire", "Cibele", ""], ["Gatterbauer", "Wolfgang", ""], ["Immerman", "Neil", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1507.00739", "submitter": "Ashley Montanaro", "authors": "Aram W. Harrow and Ashley Montanaro", "title": "Extremal eigenvalues of local Hamiltonians", "comments": "5 pages; v4: uses standard journal style", "journal-ref": "Quantum 1, 6 (2017)", "doi": "10.22331/q-2017-04-25-6", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We apply classical algorithms for approximately solving constraint\nsatisfaction problems to find bounds on extremal eigenvalues of local\nHamiltonians. We consider spin Hamiltonians for which we have an upper bound on\nthe number of terms in which each spin participates, and find extensive bounds\nfor the operator norm and ground-state energy of such Hamiltonians under this\nconstraint. In each case the bound is achieved by a product state which can be\nfound efficiently using a classical algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 20:01:56 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 09:01:26 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 14:52:02 GMT"}, {"version": "v4", "created": "Fri, 21 Apr 2017 15:24:16 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Harrow", "Aram W.", ""], ["Montanaro", "Ashley", ""]]}, {"id": "1507.00829", "submitter": "Oanh Nguyen", "authors": "Raghu Meka, Oanh Nguyen, Van Vu", "title": "Anti-concentration for polynomials of independent random variables", "comments": "Theorem 1.7 on p-biased distribution and Theorem 2.5 on lower bound\n  for approximating OR function are modified due to some errors in the previous\n  version. Theorem 1.8 on general distributions and an application on graph\n  theory are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove anti-concentration results for polynomials of independent random\nvariables with arbitrary degree. Our results extend the classical\nLittlewood-Offord result for linear polynomials, and improve several earlier\nestimates.\n  We discuss applications in two different areas. In complexity theory, we\nprove near optimal lower bounds for computing the Parity, addressing a\nchallenge in complexity theory posed by Razborov and Viola, and also address a\nproblem concerning OR functions. In random graph theory, we derive a general\nanti-concentration result on the number of copies of a fixed graph in a random\ngraph.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:58:13 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 07:25:57 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 06:40:12 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2015 23:45:02 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Meka", "Raghu", ""], ["Nguyen", "Oanh", ""], ["Vu", "Van", ""]]}, {"id": "1507.00843", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Optimal linear Bernoulli factories for small mean problems", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 07:56:55 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 20:56:56 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "1507.00931", "submitter": "Michael Pinsker", "authors": "Michael Pinsker", "title": "Algebraic and model theoretic methods in constraint satisfaction", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is related to the tutorials I gave at the Banff International\nResearch Station and within a \"Doc-course\" at Charles University Prague in the\nfall of 2014. It describes my current research and some of the most important\nopen questions related to it.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:00:25 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Pinsker", "Michael", ""]]}, {"id": "1507.01083", "submitter": "Jean-Guillaume Dumas", "authors": "Jean-Guillaume Dumas, Erich Kaltofen (NCSU), Emmanuel Thom\\'e\n  (CARAMEL)", "title": "Interactive certificate for the verification of Wiedemann's Krylov\n  sequence: application to the certification of the determinant, the minimal\n  and the characteristic polynomials of sparse matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certificates to a linear algebra computation are additional data structures\nfor each output, which can be used by a-possibly randomized- verification\nalgorithm that proves the correctness of each output. Wiede-mann's algorithm\nprojects the Krylov sequence obtained by repeatedly multiplying a vector by a\nmatrix to obtain a linearly recurrent sequence. The minimal polynomial of this\nsequence divides the minimal polynomial of the matrix. For instance, if the\n$n\\times n$ input matrix is sparse with n 1+o(1) non-zero entries, the\ncomputation of the sequence is quadratic in the dimension of the matrix while\nthe computation of the minimal polynomial is n 1+o(1), once that projected\nKrylov sequence is obtained. In this paper we give algorithms that compute\ncertificates for the Krylov sequence of sparse or structured $n\\times n$\nmatrices over an abstract field, whose Monte Carlo verification complexity can\nbe made essentially linear. As an application this gives certificates for the\ndeterminant, the minimal and characteristic polynomials of sparse or structured\nmatrices at the same cost.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 09:02:37 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Dumas", "Jean-Guillaume", "", "NCSU"], ["Kaltofen", "Erich", "", "NCSU"], ["Thom\u00e9", "Emmanuel", "", "CARAMEL"]]}, {"id": "1507.01159", "submitter": "Euiwoong Lee", "authors": "Euiwoong Lee", "title": "APX-Hardness of Maximizing Nash Social Welfare with Indivisible Items", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating a set of indivisible items to agents with\nadditive utilities to maximize the Nash social welfare. Cole and Gkatzelis\nrecently proved that this problem admits a constant factor approximation. We\ncomplement their result by showing that this problem is APX-hard.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 02:14:46 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Lee", "Euiwoong", ""]]}, {"id": "1507.01776", "submitter": "Robert Powell", "authors": "Robert Powell and Andrei Krokhin", "title": "A Reduction from Valued CSP to Min Cost Homomorphism Problem for\n  Digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a valued constraint satisfaction problem (VCSP), the goal is to find an\nassignment of labels to variables that minimizes a given sum of functions. Each\nfunction in the sum depends on a subset of variables, takes values which are\nrational numbers or infinity, and is chosen from a fixed finite set of\nfunctions called a constraint language. The case when all functions take only\nvalues 0 and infinity is known as the constraint satisfaction problem (CSP). It\nis known that any CSP with fixed constraint language is polynomial-time\nequivalent to one where the constraint language contains a single binary\nrelation (i.e. a digraph). A recent proof of this by Bulin et al. gives such a\nreduction that preserves most of the algebraic properties of the constraint\nlanguage that are known to characterize the complexity of the corresponding\nCSP. We adapt this proof to the more general setting of VCSP to show that each\nVCSP with a fixed finite (valued) constraint language is equivalent to one\nwhere the constraint language consists of one $\\{0,\\infty\\}$-valued binary\nfunction (i.e. a digraph) and one finite-valued unary function, the latter\nproblem known as the (extended) Minimum Cost Homomorphism Problem for digraphs.\nWe also show that our reduction preserves some important algebraic properties\nof the (valued) constraint language.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 12:23:23 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Powell", "Robert", ""], ["Krokhin", "Andrei", ""]]}, {"id": "1507.01906", "submitter": "Ashkan Norouzi-Fard", "authors": "Abbas Bazzi and Ashkan Norouzi-Fard", "title": "Towards Tight Lower Bounds for Scheduling Problems", "comments": "25 pages, 3 figures, To appear in the Proceedings of the 23rd Annual\n  European Symposium on Algorithms 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a close connection between structural hardness for $k$-partite graphs\nand tight inapproximability results for scheduling problems with precedence\nconstraints. Assuming a natural but nontrivial generalisation of the bipartite\nstructural hardness result of Bansal and Khot, we obtain a hardness of\n$2-\\epsilon$ for the problem of minimising the makespan for scheduling\nprecedence-constrained jobs with preemption on identical parallel machines.\nThis matches the best approximation guarantee for this problem. Assuming the\nsame hypothesis, we also obtain a super constant inapproximability result for\nthe problem of scheduling precedence-constrained jobs on related parallel\nmachines, making progress towards settling an open question in both lists of\nten open questions by Williamson and Shmoys, and by Schuurman and Woeginger.\n  The study of structural hardness of $k$-partite graphs is of independent\ninterest, as it captures the intrinsic hardness for a large family of\nscheduling problems. Other than the ones already mentioned, this generalisation\nalso implies tight inapproximability to the problem of minimising the weighted\ncompletion time for precedence-constrained jobs on a single machine, and the\nproblem of minimising the makespan of precedence-constrained jobs on identical\nparallel machine, and hence unifying the results of Bansal and Khot, and\nSvensson, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 18:04:24 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Bazzi", "Abbas", ""], ["Norouzi-Fard", "Ashkan", ""]]}, {"id": "1507.01917", "submitter": "Youming Qiao", "authors": "Joshua A. Grochow and Youming Qiao", "title": "Polynomial-time isomorphism test of groups that are tame extensions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.GR math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new polynomial-time algorithms for testing isomorphism of a class of\ngroups given by multiplication tables (GpI). Two results (Cannon & Holt, J.\nSymb. Comput. 2003; Babai, Codenotti & Qiao, ICALP 2012) imply that GpI reduces\nto the following: given groups G, H with characteristic subgroups of the same\ntype and isomorphic to $\\mathbb{Z}_p^d$, and given the coset of isomorphisms\n$Iso(G/\\mathbb{Z}_p^d, H/\\mathbb{Z}_p^d)$, compute Iso(G, H) in time poly(|G|).\nBabai & Qiao (STACS 2012) solved this problem when a Sylow p-subgroup of\n$G/\\mathbb{Z}_p^d$ is trivial. In this paper, we solve the preceding problem in\nthe so-called \"tame\" case, i.e., when a Sylow p-subgroup of $G/\\mathbb{Z}_p^d$\nis cyclic, dihedral, semi-dihedral, or generalized quaternion. These cases\ncorrespond exactly to the group algebra\n$\\overline{\\mathbb{F}}_p[G/\\mathbb{Z}_p^d]$ being of tame type, as in the\ncelebrated tame-wild dichotomy in representation theory. We then solve new\ncases of GpI in polynomial time.\n  Our result relies crucially on the divide-and-conquer strategy proposed\nearlier by the authors (CCC 2014), which splits GpI into two problems, one on\ngroup actions (representations), and one on group cohomology. Based on this\nstrategy, we combine permutation group and representation algorithms with new\nmathematical results, including bounds on the number of indecomposable\nrepresentations of groups in the tame case, and on the size of their cohomology\ngroups.\n  Finally, we note that when a group extension is not tame, the preceding\nbounds do not hold. This suggests a precise sense in which the tame-wild\ndichotomy from representation theory may also be a dividing line between the\n(currently) easy and hard instances of GpI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 18:46:49 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 02:20:15 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Qiao", "Youming", ""]]}, {"id": "1507.01988", "submitter": "Abuzer Yakaryilmaz", "authors": "Andris Ambainis and Abuzer Yakary{\\i}lmaz", "title": "Automata and Quantum Computing", "comments": "33 pages. A revised and updated version (June 2018). To appear in\n  Automata: From Mathematics to Applications edited by Jean-\\'Eric Pin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing is a new model of computation, based on quantum physics.\nQuantum computers can be exponentially faster than conventional computers for\nproblems such as factoring. Besides full-scale quantum computers, more\nrestricted models such as quantum versions of finite automata have been\nstudied. In this paper, we survey various models of quantum finite automata and\ntheir properties. We also provide some open questions and new directions for\nresearchers.\n  Keywords: quantum finite automata, probabilistic finite automata,\nnondeterminism, bounded error, unbounded error, state complexity, decidability\nand undecidability, computational complexity\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 23:40:48 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 10:21:57 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Ambainis", "Andris", ""], ["Yakary\u0131lmaz", "Abuzer", ""]]}, {"id": "1507.02000", "submitter": "Guanghui Lan", "authors": "Guanghui Lan, Yi Zhou", "title": "An optimal randomized incremental gradient method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of finite-sum convex optimization problems\nwhose objective function is given by the summation of $m$ ($\\ge 1$) smooth\ncomponents together with some other relatively simple terms. We first introduce\na deterministic primal-dual gradient (PDG) method that can achieve the optimal\nblack-box iteration complexity for solving these composite optimization\nproblems using a primal-dual termination criterion. Our major contribution is\nto develop a randomized primal-dual gradient (RPDG) method, which needs to\ncompute the gradient of only one randomly selected smooth component at each\niteration, but can possibly achieve better complexity than PDG in terms of the\ntotal number of gradient evaluations. More specifically, we show that the total\nnumber of gradient evaluations performed by RPDG can be ${\\cal O} (\\sqrt{m})$\ntimes smaller, both in expectation and with high probability, than those\nperformed by deterministic optimal first-order methods under favorable\nsituations. We also show that the complexity of the RPDG method is not\nimprovable by developing a new lower complexity bound for a general class of\nrandomized methods for solving large-scale finite-sum convex optimization\nproblems. Moreover, through the development of PDG and RPDG, we introduce a\nnovel game-theoretic interpretation for these optimal methods for convex\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 00:49:52 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 11:58:29 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 20:59:35 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Yi", ""]]}, {"id": "1507.02015", "submitter": "Pascal Koiran", "authors": "Ignacio Garcia-Marco (LIP), Pascal Koiran (LIP)", "title": "Lower Bounds by Birkhoff Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give lower bounds for the representation of real univariate\npolynomials as sums of powers of degree 1 polynomials. We present two families\nof polynomials of degree d such that the number of powers that are required in\nsuch a representation must be at least of order d. This is clearly optimal up\nto a constant factor. Previous lower bounds for this problem were only of order\n$\\Omega$($\\sqrt$ d), and were obtained from arguments based on Wronskian\ndeterminants and \"shifted derivatives.\" We obtain this improvement thanks to a\nnew lower bound method based on Birkhoff interpolation (also known as \"lacunary\npolynomial interpolation\").\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:10:11 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Garcia-Marco", "Ignacio", "", "LIP"], ["Koiran", "Pascal", "", "LIP"]]}, {"id": "1507.02067", "submitter": "Daniel Rudolf", "authors": "Christoph Aistleitner, Aicke Hinrichs, Daniel Rudolf", "title": "On the size of the largest empty box amidst a point set", "comments": "7 pages, accepted for publication in Discrete Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the largest empty axis-parallel box amidst a point\nconfiguration is a classical problem in computational geometry. It is known\nthat the volume of the largest empty box is of asymptotic order $1/n$ for\n$n\\to\\infty$ and fixed dimension $d$. However, it is natural to assume that the\nvolume of the largest empty box increases as $d$ gets larger. In the present\npaper we prove that this actually is the case: for every set of $n$ points in\n$[0, 1]^d$ there exists an empty box of volume at least $c_d n^{-1}$ , where\n$c_d \\to \\infty$ as $d\\to \\infty$. More precisely, $c_d$ is at least of order\nroughly $\\log d$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 08:38:46 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 16:40:49 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 10:21:28 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Aistleitner", "Christoph", ""], ["Hinrichs", "Aicke", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1507.02184", "submitter": "Sang-il Oum", "authors": "Jisu Jeong and Eun Jung Kim and Sang-il Oum", "title": "The \"art of trellis decoding\" is fixed-parameter tractable", "comments": "50 pages. Accepted to SODA 2016 under the title \"constructive\n  algorithms for path-width of matroids\". We added several figures to improve\n  its presentation. We found a mistake in the proof of Lemma 3.24 of the\n  previous version. In order to fix it, we changed some definitions in Section\n  3 and were able to recover our theorem", "journal-ref": "IEEE Trans. Inform. Theory, 63(11)(November 2017), pp. 7178-7205", "doi": "10.1109/TIT.2017.2740283", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n subspaces of a finite-dimensional vector space over a fixed finite\nfield $\\mathbb F$, we wish to find a linear layout $V_1,V_2,\\ldots,V_n$ of the\nsubspaces such that $\\dim((V_1+V_2+\\cdots+V_i) \\cap (V_{i+1}+\\cdots+V_n))\\le k$\nfor all i, such a linear layout is said to have width at most k. When\nrestricted to 1-dimensional subspaces, this problem is equivalent to computing\nthe trellis-width (or minimum trellis state-complexity) of a linear code in\ncoding theory and computing the path-width of an $\\mathbb F$-represented\nmatroid in matroid theory.\n  We present a fixed-parameter tractable algorithm to construct a linear layout\nof width at most k, if it exists, for input subspaces of a finite-dimensional\nvector space over $\\mathbb F$. As corollaries, we obtain a fixed-parameter\ntractable algorithm to produce a path-decomposition of width at most k for an\ninput $\\mathbb F$-represented matroid of path-width at most k, and a\nfixed-parameter tractable algorithm to find a linear rank-decomposition of\nwidth at most k for an input graph of linear rank-width at most k. In both\ncorollaries, no such algorithms were known previously.\n  It was previously known that a fixed-parameter tractable algorithm exists for\nthe decision version of the problem for matroid path-width, a theorem by\nGeelen, Gerards, and Whittle~(2002) implies that for each fixed finite field\n$\\mathbb F$, there are finitely many forbidden $\\mathbb F$-representable minors\nfor the class of matroids of path-width at most k. An algorithm by\nHlin\\v{e}n\\'y (2006) can detect a minor in an input $\\mathbb F$-represented\nmatroid of bounded branch-width. However, this indirect approach would not\nproduce an actual path-decomposition. Our algorithm is the first one to\nconstruct such a path-decomposition and does not depend on the finiteness of\nforbidden minors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:57:05 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 19:54:46 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 07:31:59 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 08:40:45 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Jeong", "Jisu", ""], ["Kim", "Eun Jung", ""], ["Oum", "Sang-il", ""]]}, {"id": "1507.02723", "submitter": "Holger Petersen", "authors": "Holger Petersen", "title": "An NL-Complete Puzzle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of a puzzle that turns out to be NL-complete.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 22:03:56 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Petersen", "Holger", ""]]}, {"id": "1507.02955", "submitter": "Michael Walter", "authors": "Christian Ikenmeyer and Ketan D. Mulmuley and Michael Walter", "title": "On vanishing of Kronecker coefficients", "comments": "43 pages, 1 figure", "journal-ref": null, "doi": "10.1007/s00037-017-0158-y", "report-no": null, "categories": "cs.CC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of deciding positivity of Kronecker coefficients is\nNP-hard. Previously, this problem was conjectured to be in P, just as for the\nLittlewood-Richardson coefficients. Our result establishes in a formal way that\nKronecker coefficients are more difficult than Littlewood-Richardson\ncoefficients, unless P=NP.\n  We also show that there exists a #P-formula for a particular subclass of\nKronecker coefficients whose positivity is NP-hard to decide. This is an\nevidence that, despite the hardness of the positivity problem, there may well\nexist a positive combinatorial formula for the Kronecker coefficients. Finding\nsuch a formula is a major open problem in representation theory and algebraic\ncombinatorics.\n  Finally, we consider the existence of the partition triples $(\\lambda, \\mu,\n\\pi)$ such that the Kronecker coefficient $k^\\lambda_{\\mu, \\pi} = 0$ but the\nKronecker coefficient $k^{l \\lambda}_{l \\mu, l \\pi} > 0$ for some integer\n$l>1$. Such \"holes\" are of great interest as they witness the failure of the\nsaturation property for the Kronecker coefficients, which is still poorly\nunderstood. Using insight from computational complexity theory, we turn our\nhardness proof into a positive result: We show that not only do there exist\nmany such triples, but they can also be found efficiently. Specifically, we\nshow that, for any $0<\\epsilon\\leq1$, there exists $0<a<1$ such that, for all\n$m$, there exist $\\Omega(2^{m^a})$ partition triples $(\\lambda,\\mu,\\mu)$ in the\nKronecker cone such that: (a) the Kronecker coefficient $k^\\lambda_{\\mu,\\mu}$\nis zero, (b) the height of $\\mu$ is $m$, (c) the height of $\\lambda$ is $\\le\nm^\\epsilon$, and (d) $|\\lambda|=|\\mu| \\le m^3$. The proof of the last result\nillustrates the effectiveness of the explicit proof strategy of GCT.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 16:19:53 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 17:32:52 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ikenmeyer", "Christian", ""], ["Mulmuley", "Ketan D.", ""], ["Walter", "Michael", ""]]}, {"id": "1507.03046", "submitter": "Diego Cifuentes", "authors": "Diego Cifuentes and Pablo A. Parrilo", "title": "An efficient tree decomposition method for permanents and mixed\n  discriminants", "comments": "32 pages, 4 figures", "journal-ref": "Linear Algebra and its Applications, Volume 493, 15 March 2016,\n  pages 45-81", "doi": "10.1016/j.laa.2015.12.004", "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:26:53 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Cifuentes", "Diego", ""], ["Parrilo", "Pablo A.", ""]]}, {"id": "1507.03113", "submitter": "Jack Murtagh", "authors": "Jack Murtagh, Salil Vadhan", "title": "The Complexity of Computing the Optimal Composition of Differential\n  Privacy", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of differential privacy, composition theorems (starting with the\noriginal paper of Dwork, McSherry, Nissim, and Smith (TCC'06)) bound the\ndegradation of privacy when composing several differentially private\nalgorithms. Kairouz, Oh, and Viswanath (ICML'15) showed how to compute the\noptimal bound for composing $k$ arbitrary $(\\epsilon,\\delta)$-differentially\nprivate algorithms. We characterize the optimal composition for the more\ngeneral case of $k$ arbitrary\n$(\\epsilon_{1},\\delta_{1}),\\ldots,(\\epsilon_{k},\\delta_{k})$-differentially\nprivate algorithms where the privacy parameters may differ for each algorithm\nin the composition. We show that computing the optimal composition in general\nis $\\#$P-complete. Since computing optimal composition exactly is infeasible\n(unless FP=$\\#$P), we give an approximation algorithm that computes the\ncomposition to arbitrary accuracy in polynomial time. The algorithm is a\nmodification of Dyer's dynamic programming approach to approximately counting\nsolutions to knapsack problems (STOC'03).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 14:58:34 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 17:38:59 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Murtagh", "Jack", ""], ["Vadhan", "Salil", ""]]}, {"id": "1507.03126", "submitter": "Oded Regev", "authors": "Andris Ambainis and Aleksandrs Belovs and Oded Regev and Ronald de\n  Wolf", "title": "Efficient Quantum Algorithms for (Gapped) Group Testing and Junta\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-junta testing problem, a tester has to efficiently decide whether\na given function $f:\\{0,1\\}^n\\rightarrow \\{0,1\\}$ is a $k$-junta (i.e., depends\non at most $k$ of its input bits) or is $\\epsilon$-far from any $k$-junta. Our\nmain result is a quantum algorithm for this problem with query complexity\n$\\tilde O(\\sqrt{k/\\epsilon})$ and time complexity $\\tilde\nO(n\\sqrt{k/\\epsilon})$. This quadratically improves over the query complexity\nof the previous best quantum junta tester, due to At\\i c\\i\\ and Servedio. Our\ntester is based on a new quantum algorithm for a gapped version of the\ncombinatorial group testing problem, with an up to quartic improvement over the\nquery complexity of the best classical algorithm. For our upper bound on the\ntime complexity we give a near-linear time implementation of a shallow variant\nof the quantum Fourier transform over the symmetric group, similar to the\nSchur-Weyl transform. We also prove a lower bound of $\\Omega(k^{1/3})$ queries\nfor junta-testing (for constant $\\epsilon$).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 16:53:57 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ambainis", "Andris", ""], ["Belovs", "Aleksandrs", ""], ["Regev", "Oded", ""], ["de Wolf", "Ronald", ""]]}, {"id": "1507.03166", "submitter": "Jo\\~ao Sousa Pinto", "authors": "Jo\\\"el Ouaknine, Jo\\~ao Sousa-Pinto, James Worrell", "title": "On the Polytope Escape Problem for Continuous Linear Dynamical Systems", "comments": "Accepted to HSCC 2017", "journal-ref": null, "doi": "10.1145/3049797.3049798", "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Polyhedral Escape Problem for continuous linear dynamical systems\nconsists of deciding, given an affine function $f: \\mathbb{R}^{d} \\rightarrow\n\\mathbb{R}^{d}$ and a convex polyhedron $\\mathcal{P} \\subseteq \\mathbb{R}^{d}$,\nwhether, for some initial point $\\boldsymbol{x}_{0}$ in $\\mathcal{P}$, the\ntrajectory of the unique solution to the differential equation\n$\\dot{\\boldsymbol{x}}(t)=f(\\boldsymbol{x}(t))$,\n$\\boldsymbol{x}(0)=\\boldsymbol{x}_{0}$, is entirely contained in $\\mathcal{P}$.\nWe show that this problem is decidable, by reducing it in polynomial time to\nthe decision version of linear programming with real algebraic coefficients,\nthus placing it in $\\exists \\mathbb{R}$, which lies between NP and PSPACE. Our\nalgorithm makes use of spectral techniques and relies among others on tools\nfrom Diophantine approximation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 22:56:01 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 19:11:42 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Ouaknine", "Jo\u00ebl", ""], ["Sousa-Pinto", "Jo\u00e3o", ""], ["Worrell", "James", ""]]}, {"id": "1507.03269", "submitter": "David Steurer", "authors": "Samuel B. Hopkins and Jonathan Shi and David Steurer", "title": "Tensor principal component analysis via sum-of-squares proofs", "comments": "published in Conference on Learning Theory (COLT) 2015 (submitted\n  February 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 20:30:09 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1507.03439", "submitter": "Michael Etscheid", "authors": "Michael Etscheid, Stefan Kratsch, Matthias Mnich, Heiko R\\\"oglin", "title": "Polynomial Kernels for Weighted Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelization is a formalization of efficient preprocessing for NP-hard\nproblems using the framework of parameterized complexity. Among open problems\nin kernelization it has been asked many times whether there are deterministic\npolynomial kernelizations for Subset Sum and Knapsack when parameterized by the\nnumber $n$ of items.\n  We answer both questions affirmatively by using an algorithm for compressing\nnumbers due to Frank and Tardos (Combinatorica 1987). This result had been\nfirst used by Marx and V\\'egh (ICALP 2013) in the context of kernelization. We\nfurther illustrate its applicability by giving polynomial kernels also for\nweighted versions of several well-studied parameterized problems. Furthermore,\nwhen parameterized by the different item sizes we obtain a polynomial\nkernelization for Subset Sum and an exponential kernelization for Knapsack.\nFinally, we also obtain kernelization results for polynomial integer programs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 13:15:14 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Etscheid", "Michael", ""], ["Kratsch", "Stefan", ""], ["Mnich", "Matthias", ""], ["R\u00f6glin", "Heiko", ""]]}, {"id": "1507.03546", "submitter": "Zi-Wen Liu", "authors": "Zi-Wen Liu, Christopher Perry, Yechao Zhu, Dax Enshan Koh, Scott\n  Aaronson", "title": "Doubly infinite separation of quantum information and communication", "comments": "16 pages, 2 figures. v4: minor errors fixed; close to published\n  version; v5: financial support info added", "journal-ref": "Phys. Rev. A 93, 012347 (2016)", "doi": "10.1103/PhysRevA.93.012347", "report-no": "MIT-CTP/4692", "categories": "quant-ph cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the existence of (one-way) communication tasks with a subconstant\nversus superconstant asymptotic gap, which we call \"doubly infinite,\" between\ntheir quantum information and communication complexities. We do so by studying\nthe exclusion game [C. Perry et al., Phys. Rev. Lett. 115, 030504 (2015)] for\nwhich there exist instances where the quantum information complexity tends to\nzero as the size of the input $n$ increases. By showing that the quantum\ncommunication complexity of these games scales at least logarithmically in $n$,\nwe obtain our result. We further show that the established lower bounds and\ngaps still hold even if we allow a small probability of error. However in this\ncase, the $n$-qubit quantum message of the zero-error strategy can be\ncompressed polynomially.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 18:49:52 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 23:41:52 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2015 20:40:11 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2016 22:12:29 GMT"}, {"version": "v5", "created": "Thu, 5 May 2016 20:23:20 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Liu", "Zi-Wen", ""], ["Perry", "Christopher", ""], ["Zhu", "Yechao", ""], ["Koh", "Dax Enshan", ""], ["Aaronson", "Scott", ""]]}, {"id": "1507.03558", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt\n  Rubinfeld", "title": "Testing Shape Restrictions of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of testing structured properties (classes) of discrete\ndistributions. Specifically, given sample access to an arbitrary distribution\n$D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish between\n$D\\in\\mathcal{P}$ and $\\ell_1(D,\\mathcal{P})>\\varepsilon$. We develop a general\nalgorithm for this question, which applies to a large range of\n\"shape-constrained\" properties, including monotone, log-concave, $t$-modal,\npiecewise-polynomial, and Poisson Binomial distributions. Moreover, for all\ncases considered, our algorithm has near-optimal sample complexity with regard\nto the domain size and is computationally efficient. For most of these classes,\nwe provide the first non-trivial tester in the literature. In addition, we also\ndescribe a generic method to prove lower bounds for this problem, and use it to\nshow our upper bounds are nearly tight. Finally, we extend some of our\ntechniques to tolerant testing, deriving nearly-tight upper and lower bounds\nfor the corresponding questions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 19:22:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 16:28:19 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 19:56:27 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1507.03844", "submitter": "Elis\\^angela Silva Dias", "authors": "Elis\\^angela Silva Dias, Diane Castonguay", "title": "Polynomial recognition of cluster algebras of finite type", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster algebras are a recent topic of study and have been shown to be a\nuseful tool to characterize structures in several knowledge fields. An\nimportant problem is to establish whether or not a given cluster algebra is of\nfinite type. Using the standard definition, the problem is infeasible since it\nuses mutations that can lead to an infinite process. Barot, Geiss and\nZelevinsky (2006) presented an easier way to verify if a given algebra is of\nfinite type, by testing that all chordless cycles of the graph related to the\nalgebra are cyclically oriented and that there exists a positive quasi-Cartan\ncompanion of the skew-symmetrizable matrix related to the algebra. We develop\nan algorithm that verifies these conditions and decides whether or not a\ncluster algebra is of finite type in polynomial time. The second part of the\nalgorithm is used to prove that the more general problem to decide if a matrix\nhas a positive quasi-Cartan companion is in NP.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 13:32:00 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Dias", "Elis\u00e2ngela Silva", ""], ["Castonguay", "Diane", ""]]}, {"id": "1507.03885", "submitter": "J\\=anis Iraids", "authors": "Kaspars Balodis, J\\=anis Iraids", "title": "Quantum Lower Bound for Graph Collision Implies Lower Bound for Triangle\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that an improvement to the best known quantum lower bound for\nGRAPH-COLLISION problem implies an improvement to the best known lower bound\nfor TRIANGLE problem in the quantum query complexity model. In GRAPH-COLLISION\nwe are given free access to a graph $(V,E)$ and access to a function\n$f:V\\rightarrow \\{0,1\\}$ as a black box. We are asked to determine if there\nexist $(u,v) \\in E$, such that $f(u)=f(v)=1$. In TRIANGLE we have a black box\naccess to an adjacency matrix of a graph and we have to determine if the graph\ncontains a triangle. For both of these problems the known lower bounds are\ntrivial ($\\Omega(\\sqrt{n})$ and $\\Omega(n)$, respectively) and there is no\nknown matching upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 15:20:24 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Balodis", "Kaspars", ""], ["Iraids", "J\u0101nis", ""]]}, {"id": "1507.04299", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Ilya Razenshteyn", "title": "Tight Lower Bounds for Data-Dependent Locality-Sensitive Hashing", "comments": "16 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a tight lower bound for the exponent $\\rho$ for data-dependent\nLocality-Sensitive Hashing schemes, recently used to design efficient solutions\nfor the $c$-approximate nearest neighbor search. In particular, our lower bound\nmatches the bound of $\\rho\\le \\frac{1}{2c-1}+o(1)$ for the $\\ell_1$ space,\nobtained via the recent algorithm from [Andoni-Razenshteyn, STOC'15].\n  In recent years it emerged that data-dependent hashing is strictly superior\nto the classical Locality-Sensitive Hashing, when the hash function is\ndata-independent. In the latter setting, the best exponent has been already\nknown: for the $\\ell_1$ space, the tight bound is $\\rho=1/c$, with the upper\nbound from [Indyk-Motwani, STOC'98] and the matching lower bound from\n[O'Donnell-Wu-Zhou, ITCS'11].\n  We prove that, even if the hashing is data-dependent, it must hold that\n$\\rho\\ge \\frac{1}{2c-1}-o(1)$. To prove the result, we need to formalize the\nexact notion of data-dependent hashing that also captures the complexity of the\nhash functions (in addition to their collision properties). Without restricting\nsuch complexity, we would allow for obviously infeasible solutions such as the\nVoronoi diagram of a dataset. To preclude such solutions, we require our hash\nfunctions to be succinct. This condition is satisfied by all the known\nalgorithmic results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 17:02:20 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Andoni", "Alexandr", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1507.04391", "submitter": "Vangelis Paschos", "authors": "Dimitris Fotakis and Michael Lampis and Vangelis Th. Paschos", "title": "Sub-exponential Approximation Schemes for CSPs: from Dense to Almost\n  Sparse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been known, since the classical work of (Arora, Karger,\nKarpinski, JCSS~99), that \\MC\\ admits a PTAS on dense graphs, and more\ngenerally, \\kCSP\\ admits a PTAS on \"dense\" instances with $\\Omega(n^k)$\nconstraints. In this paper we extend and generalize their exhaustive sampling\napproach, presenting a framework for $(1-\\eps)$-approximating any \\kCSP\\\nproblem in \\emph{sub-exponential} time while significantly relaxing the\ndenseness requirement on the input instance. Specifically, we prove that for\nany constants $\\delta \\in (0, 1]$ and $\\eps > 0$, we can approximate \\kCSP\\\nproblems with $\\Omega(n^{k-1+\\delta})$ constraints within a factor of\n$(1-\\eps)$ in time $2^{O(n^{1-\\delta}\\ln n /\\eps^3)}$. The framework is quite\ngeneral and includes classical optimization problems, such as \\MC, {\\sc\nMax}-DICUT, \\kSAT, and (with a slight extension) $k$-{\\sc Densest Subgraph}, as\nspecial cases. For \\MC\\ in particular (where $k=2$), it gives an approximation\nscheme that runs in time sub-exponential in $n$ even for \"almost-sparse\"\ninstances (graphs with $n^{1+\\delta}$ edges). We prove that our results are\nessentially best possible, assuming the ETH. First, the density requirement\ncannot be relaxed further: there exists a constant $r < 1$ such that for all\n$\\delta > 0$, \\kSAT\\ instances with $O(n^{k-1})$ clauses cannot be approximated\nwithin a ratio better than $r$ in time $2^{O(n^{1-\\delta})}$. Second, the\nrunning time of our algorithm is almost tight \\emph{for all densities}. Even\nfor \\MC\\ there exists $r<1$ such that for all $\\delta' > \\delta >0$, \\MC\\\ninstances with $n^{1+\\delta}$ edges cannot be approximated within a ratio\nbetter than $r$ in time $2^{n^{1-\\delta'}}$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 20:48:19 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Lampis", "Michael", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "1507.04461", "submitter": "Shantanu Sharma", "authors": "Foto Afrati, Shlomi Dolev, Ephraim Korach, Shantanu Sharma, Jeffrey D.\n  Ullman", "title": "Assignment Problems of Different-Sized Inputs in MapReduce", "comments": "This paper is accepted in ACM Transactions on Knowledge Discovery\n  from Data (TKDD), August 2016. Preliminary versions of this paper have\n  appeared in the proceeding of DISC 2014 and BeyondMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A MapReduce algorithm can be described by a mapping schema, which assigns\ninputs to a set of reducers, such that for each required output there exists a\nreducer that receives all the inputs that participate in the computation of\nthis output. Reducers have a capacity, which limits the sets of inputs that\nthey can be assigned. However, individual inputs may vary in terms of size. We\nconsider, for the first time, mapping schemas where input sizes are part of the\nconsiderations and restrictions. One of the significant parameters to optimize\nin any MapReduce job is communication cost between the map and reduce phases.\nThe communication cost can be optimized by minimizing the number of copies of\ninputs sent to the reducers. The communication cost is closely related to the\nnumber of reducers of constrained capacity that are used to accommodate\nappropriately the inputs, so that the requirement of how the inputs must meet\nin a reducer is satisfied. In this work, we consider a family of problems where\nit is required that each input meets with each other input in at least one\nreducer. We also consider a slightly different family of problems in which,\neach input of a list, X, is required to meet each input of another list, Y, in\nat least one reducer. We prove that finding an optimal mapping schema for these\nfamilies of problems is NP-hard, and present a bin-packing-based approximation\nalgorithm for finding a near optimal mapping schema.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 06:46:19 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 19:07:51 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Afrati", "Foto", ""], ["Dolev", "Shlomi", ""], ["Korach", "Ephraim", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1507.04500", "submitter": "Ale\\v{s} Bizjak", "authors": "John Fearnley and Rahul Savani", "title": "The Complexity of All-switches Strategy Improvement", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 4 (October\n  31, 2018) lmcs:4940", "doi": "10.23638/LMCS-14(4:9)2018", "report-no": null, "categories": "cs.DS cs.CC cs.GT cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategy improvement is a widely-used and well-studied class of algorithms\nfor solving graph-based infinite games. These algorithms are parameterized by a\nswitching rule, and one of the most natural rules is \"all switches\" which\nswitches as many edges as possible in each iteration. Continuing a recent line\nof work, we study all-switches strategy improvement from the perspective of\ncomputational complexity. We consider two natural decision problems, both of\nwhich have as input a game $G$, a starting strategy $s$, and an edge $e$. The\nproblems are: 1.) The edge switch problem, namely, is the edge $e$ ever\nswitched by all-switches strategy improvement when it is started from $s$ on\ngame $G$? 2.) The optimal strategy problem, namely, is the edge $e$ used in the\nfinal strategy that is found by strategy improvement when it is started from\n$s$ on game $G$? We show $\\mathtt{PSPACE}$-completeness of the edge switch\nproblem and optimal strategy problem for the following settings: Parity games\nwith the discrete strategy improvement algorithm of V\\\"oge and Jurdzi\\'nski;\nmean-payoff games with the gain-bias algorithm [14,37]; and discounted-payoff\ngames and simple stochastic games with their standard strategy improvement\nalgorithms. We also show $\\mathtt{PSPACE}$-completeness of an analogous problem\nto edge switch for the bottom-antipodal algorithm for finding the sink of an\nAcyclic Unique Sink Orientation on a cube.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 09:27:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 17:57:49 GMT"}, {"version": "v3", "created": "Sat, 23 Jun 2018 09:09:30 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 09:15:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Fearnley", "John", ""], ["Savani", "Rahul", ""]]}, {"id": "1507.04645", "submitter": "Amit Chakrabarti", "authors": "Amit Chakrabarti and Anthony Wirth", "title": "Incidence Geometries and the Pass Complexity of Semi-Streaming Set Cover", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set cover, over a universe of size $n$, may be modelled as a data-streaming\nproblem, where the $m$ sets that comprise the instance are to be read one by\none. A semi-streaming algorithm is allowed only $O(n\\, \\mathrm{poly}\\{\\log n,\n\\log m\\})$ space to process this stream. For each $p \\ge 1$, we give a very\nsimple deterministic algorithm that makes $p$ passes over the input stream and\nreturns an appropriately certified $(p+1)n^{1/(p+1)}$-approximation to the\noptimum set cover. More importantly, we proceed to show that this approximation\nfactor is essentially tight, by showing that a factor better than\n$0.99\\,n^{1/(p+1)}/(p+1)^2$ is unachievable for a $p$-pass semi-streaming\nalgorithm, even allowing randomisation. In particular, this implies that\nachieving a $\\Theta(\\log n)$-approximation requires $\\Omega(\\log n/\\log\\log n)$\npasses, which is tight up to the $\\log\\log n$ factor. These results extend to a\nrelaxation of the set cover problem where we are allowed to leave an\n$\\varepsilon$ fraction of the universe uncovered: the tight bounds on the best\napproximation factor achievable in $p$ passes turn out to be\n$\\Theta_p(\\min\\{n^{1/(p+1)}, \\varepsilon^{-1/p}\\})$. Our lower bounds are based\non a construction of a family of high-rank incidence geometries, which may be\nthought of as vast generalisations of affine planes. This construction, based\non algebraic techniques, appears flexible enough to find other applications and\nis therefore interesting in its own right.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:42:09 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Wirth", "Anthony", ""]]}, {"id": "1507.04820", "submitter": "Karsten Lehmann", "authors": "Karsten Lehmann and Alban Grastien and Pascal Van Hentenryck", "title": "The Complexity of Switching and FACTS Maximum-Potential-Flow Problems", "comments": "arXiv admin note: text overlap with arXiv:1411.4369", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This papers considers the problem of maximizing the load that can be served\nby a power network. We use the commonly accepted Linear DC power network model\nand consider wo configuration options: switching lines and using FACTS devices.\nWe present the first comprehensive complexity study of this optimization\nproblem. Our results show hat the problem is NP-complete and that there is no\nfully polynomial-time approximation scheme. For switching, these results extend\nto planar networks with a aximum-node degree of 3. Additionally, we demonstrate\nthat the optimization problems are still NP-hard if we restrict the network\nstructure to cacti with a maximum degree of 3.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 02:37:58 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Lehmann", "Karsten", ""], ["Grastien", "Alban", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1507.04907", "submitter": "Martin Kouteck\\'y", "authors": "Petr Kolman, Martin Kouteck\\'y, Hans Raj Tiwary", "title": "Extension Complexity, MSO Logic, and Treewidth", "comments": "Final version accepted by DMTCS", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 22 no.\n  4, Discrete Algorithms (October 1, 2020) dmtcs:6811", "doi": "10.23638/DMTCS-22-4-8", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the convex hull $P_{\\varphi}(G)$ of all satisfying assignments of\na given MSO formula $\\varphi$ on a given graph $G$. We show that there exists\nan extended formulation of the polytope $P_{\\varphi}(G)$ that can be described\nby $f(|\\varphi|,\\tau)\\cdot n$ inequalities, where $n$ is the number of vertices\nin $G$, $\\tau$ is the treewidth of $G$ and $f$ is a computable function\ndepending only on $\\varphi$ and $\\tau.$\n  In other words, we prove that the extension complexity of $P_{\\varphi}(G)$ is\nlinear in the size of the graph $G$, with a constant depending on the treewidth\nof $G$ and the formula $\\varphi$. This provides a very general yet very simple\nmeta-theorem about the extension complexity of polytopes related to a wide\nclass of problems and graphs. As a corollary of our main result, we obtain an\nanalogous result % for the weaker MSO$_1$ logic on the wider class of graphs of\nbounded cliquewidth.\n  Furthermore, we study our main geometric tool which we term the glued product\nof polytopes. While the glued product of polytopes has been known since the\n'90s, we are the first to show that it preserves decomposability and\nboundedness of treewidth of the constraint matrix. This implies that our\nextension of $P_\\varphi(G)$ is decomposable and has a constraint matrix of\nbounded treewidth; so far only few classes of polytopes are known to be\ndecomposable. These properties make our extension useful in the construction of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:31:53 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 19:24:43 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 02:30:32 GMT"}, {"version": "v4", "created": "Sun, 27 Nov 2016 17:08:24 GMT"}, {"version": "v5", "created": "Tue, 28 Feb 2017 11:25:46 GMT"}, {"version": "v6", "created": "Thu, 11 May 2017 20:12:44 GMT"}, {"version": "v7", "created": "Mon, 17 Jun 2019 13:45:25 GMT"}, {"version": "v8", "created": "Mon, 29 Jun 2020 18:02:14 GMT"}, {"version": "v9", "created": "Tue, 29 Sep 2020 15:08:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kolman", "Petr", ""], ["Kouteck\u00fd", "Martin", ""], ["Tiwary", "Hans Raj", ""]]}, {"id": "1507.05061", "submitter": "Ahmed Younes Dr.", "authors": "Ahmed Younes and Jonathan E. Rowe", "title": "A Polynomial Time Bounded-error Quantum Algorithm for Boolean\n  Satisfiability", "comments": "15 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1505.06284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to answer a long-standing open problem on the\nrelationship between NP and BQP. The paper shows that BQP contains NP by\nproposing a BQP quantum algorithm for the MAX-E3-SAT problem which is a\nfundamental NP-hard problem. Given an E3-CNF Boolean formula, the aim of the\nMAX-E3-SAT problem is to find the variable assignment that maximizes the number\nof satisfied clauses. The proposed algorithm runs in $O(m^2)$ for an E3-CNF\nBoolean formula with $m$ clauses and in the worst case runs in $O(n^6)$ for an\nE3-CNF Boolean formula with $n$ inputs. The proposed algorithm maximizes the\nset of satisfied clauses using a novel iterative partial negation and partial\nmeasurement technique. The algorithm is shown to achieve an arbitrary high\nprobability of success of $1-\\epsilon$ for small $\\epsilon>0$ using a\npolynomial resources. In addition to solving the MAX-E3-SAT problem, the\nproposed algorithm can also be used to decide if an E3-CNF Boolean formula is\nsatisfiable or not, which is an NP-complete problem, based on the maximum\nnumber of satisfied clauses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 10:22:11 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 01:24:50 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Younes", "Ahmed", ""], ["Rowe", "Jonathan E.", ""]]}, {"id": "1507.05106", "submitter": "Joshua Alman", "authors": "Josh Alman, Ryan Williams", "title": "Probabilistic Polynomials and Hamming Nearest Neighbors", "comments": "16 pages. To appear in 56th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2015)", "journal-ref": null, "doi": "10.1109/FOCS.2015.18", "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute any symmetric Boolean function on $n$ variables over\nany field (as well as the integers) with a probabilistic polynomial of degree\n$O(\\sqrt{n \\log(1/\\epsilon)})$ and error at most $\\epsilon$. The degree\ndependence on $n$ and $\\epsilon$ is optimal, matching a lower bound of Razborov\n(1987) and Smolensky (1987) for the MAJORITY function. The proof is\nconstructive: a low-degree polynomial can be efficiently sampled from the\ndistribution.\n  This polynomial construction is combined with other algebraic ideas to give\nthe first subquadratic time algorithm for computing a (worst-case) batch of\nHamming distances in superlogarithmic dimensions, exactly. To illustrate, let\n$c(n) : \\mathbb{N} \\rightarrow \\mathbb{N}$. Suppose we are given a database $D$\nof $n$ vectors in $\\{0,1\\}^{c(n) \\log n}$ and a collection of $n$ query vectors\n$Q$ in the same dimension. For all $u \\in Q$, we wish to compute a $v \\in D$\nwith minimum Hamming distance from $u$. We solve this problem in $n^{2-1/O(c(n)\n\\log^2 c(n))}$ randomized time. Hence, the problem is in \"truly subquadratic\"\ntime for $O(\\log n)$ dimensions, and in subquadratic time for $d = o((\\log^2\nn)/(\\log \\log n)^2)$. We apply the algorithm to computing pairs with maximum\ninner product, closest pair in $\\ell_1$ for vectors with bounded integer\nentries, and pairs with maximum Jaccard coefficients.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 20:26:56 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Alman", "Josh", ""], ["Williams", "Ryan", ""]]}, {"id": "1507.05136", "submitter": "Tselil Schramm", "authors": "Prasad Raghavendra and Tselil Schramm", "title": "Tight Lower Bounds for Planted Clique in the Degree-4 SOS Program", "comments": "This paper appeared in SODA 2016, in a merged manuscript with the\n  paper of Hopkins, Kothari and Potechin: http://arxiv.org/abs/1507.05230", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a lower bound of $\\tilde{\\Omega}(\\sqrt{n})$ for the degree-4\nSum-of-Squares SDP relaxation for the planted clique problem. Specifically, we\nshow that on an Erd\\\"os-R\\'enyi graph $G(n,\\tfrac{1}{2})$, with high\nprobability there is a feasible point for the degree-4 SOS relaxation of the\nclique problem with an objective value of $\\tilde{\\Omega}(\\sqrt{n})$, so that\nthe program cannot distinguish between a random graph and a random graph with a\nplanted clique of size $\\tilde{O}(\\sqrt{n})$. This bound is tight.\n  We build on the works of Deshpande and Montanari and Meka et al., who give\nlower bounds of $\\tilde{\\Omega}(n^{1/3})$ and $\\tilde{\\Omega}(n^{1/4})$\nrespectively. We improve on their results by making a perturbation to the SDP\nsolution proposed in their work, then showing that this perturbation remains\nPSD as the objective value approaches $\\tilde{\\Omega}(n^{1/2})$.\n  In an independent work, Hopkins, Kothari and Potechin [HKP15] have obtained a\nsimilar lower bound for the degree-$4$ SOS relaxation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 00:32:28 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 06:59:53 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 23:40:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Schramm", "Tselil", ""]]}, {"id": "1507.05185", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Qihang Lin, Rong Jin", "title": "Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a fast approximation method for {\\it large-scale\nhigh-dimensional} sparse least-squares regression problem by exploiting the\nJohnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional\nvectors into a low-dimensional space. In particular, we propose to apply the JL\ntransforms to the data matrix and the target vector and then to solve a sparse\nleast-squares problem on the compressed data with a {\\it slightly larger\nregularization parameter}. Theoretically, we establish the optimization error\nbound of the learned model for two different sparsity-inducing regularizers,\ni.e., the elastic net and the $\\ell_1$ norm. Compared with previous relevant\nwork, our analysis is {\\it non-asymptotic and exhibits more insights} on the\nbound, the sample complexity and the regularization. As an illustration, we\nalso provide an error bound of the {\\it Dantzig selector} under JL transforms.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 13:16:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Lin", "Qihang", ""], ["Jin", "Rong", ""]]}, {"id": "1507.05230", "submitter": "Pravesh Kothari", "authors": "Samuel B. Hopkins and Pravesh K. Kothari and Aaron Potechin", "title": "SoS and Planted Clique: Tight Analysis of MPW Moments at all Degrees and\n  an Optimal Lower Bound at Degree Four", "comments": "67 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding large cliques in random graphs and its \"planted\"\nvariant, where one wants to recover a clique of size $\\omega \\gg \\log{(n)}$\nadded to an \\Erdos-\\Renyi graph $G \\sim G(n,\\frac{1}{2})$, have been intensely\nstudied. Nevertheless, existing polynomial time algorithms can only recover\nplanted cliques of size $\\omega = \\Omega(\\sqrt{n})$. By contrast, information\ntheoretically, one can recover planted cliques so long as $\\omega \\gg\n\\log{(n)}$. In this work, we continue the investigation of algorithms from the\nsum of squares hierarchy for solving the planted clique problem begun by Meka,\nPotechin, and Wigderson (MPW, 2015) and Deshpande and Montanari (DM,2015). Our\nmain results improve upon both these previous works by showing:\n  1. Degree four SoS does not recover the planted clique unless $\\omega \\gg\n\\sqrt n poly \\log n$, improving upon the bound $\\omega \\gg n^{1/3}$ due to DM.\nA similar result was obtained independently by Raghavendra and Schramm (2015).\n  2. For $2 < d = o(\\sqrt{\\log{(n)}})$, degree $2d$ SoS does not recover the\nplanted clique unless $\\omega \\gg n^{1/(d + 1)} /(2^d poly \\log n)$, improving\nupon the bound due to MPW.\n  Our proof for the second result is based on a fine spectral analysis of the\ncertificate used in the prior works MPW,DM and Feige and Krauthgamer (2003) by\ndecomposing it along an appropriately chosen basis. Along the way, we develop\ncombinatorial tools to analyze the spectrum of random matrices with dependent\nentries and to understand the symmetries in the eigenspaces of the set\nsymmetric matrices inspired by work of Grigoriev (2001).\n  An argument of Kelner shows that the first result cannot be proved using the\nsame certificate. Rather, our proof involves constructing and analyzing a new\ncertificate that yields the nearly tight lower bound by \"correcting\" the\ncertificate of previous works.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 22:51:17 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Kothari", "Pravesh K.", ""], ["Potechin", "Aaron", ""]]}, {"id": "1507.05305", "submitter": "Noson S. Yanofsky", "authors": "Noson S. Yanofsky", "title": "Computability and Complexity of Categorical Structures", "comments": "36 pages. Some proofs were improved and typos were eliminated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine various categorical structures that can and cannot be constructed.\nWe show that total computable functions can be mimicked by constructible\nfunctors. More generally, whatever can be done by a Turing machine can be\nconstructed by categories. Since there are infinitary constructions in category\ntheory, it is shown that category theory is strictly more powerful than Turing\nmachines. In particular, categories can solve the Halting Problem for Turing\nmachines. We also show that categories can solve any problem in the arithmetic\nhierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 16:07:21 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 15:12:08 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Yanofsky", "Noson S.", ""]]}, {"id": "1507.05388", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte and Miroslaw Truszczynski and Stefan Woltran", "title": "Dual-normal Logic Programs - the Forgotten Class", "comments": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  31st International Conference on Logic Programming (ICLP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disjunctive Answer Set Programming is a powerful declarative programming\nparadigm with complexity beyond NP. Identifying classes of programs for which\nthe consistency problem is in NP is of interest from the theoretical standpoint\nand can potentially lead to improvements in the design of answer set\nprogramming solvers. One of such classes consists of dual-normal programs,\nwhere the number of positive body atoms in proper rules is at most one. Unlike\nother classes of programs, dual-normal programs have received little attention\nso far. In this paper we study this class. We relate dual-normal programs to\npropositional theories and to normal programs by presenting several\ninter-translations. With the translation from dual-normal to normal programs at\nhand, we introduce the novel class of body-cycle free programs, which are in\nmany respects dual to head-cycle free programs. We establish the expressive\npower of dual-normal programs in terms of SE- and UE-models, and compare them\nto normal programs. We also discuss the complexity of deciding whether\ndual-normal programs are strongly and uniformly equivalent.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 05:28:50 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Truszczynski", "Miroslaw", ""], ["Woltran", "Stefan", ""]]}, {"id": "1507.05485", "submitter": "Pierre Lairez", "authors": "Pierre Lairez", "title": "A deterministic algorithm to compute approximate roots of polynomial\n  systems in polynomial average time", "comments": null, "journal-ref": null, "doi": "10.1007/s10208-016-9319-7", "report-no": null, "categories": "math.NA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a deterministic algorithm that computes an approximate root of n\ncomplex polynomial equations in n unknowns in average polynomial time with\nrespect to the size of the input, in the Blum-Shub-Smale model with square\nroot. It rests upon a derandomization of an algorithm of Beltr\\'an and Pardo\nand gives a deterministic affirmative answer to Smale's 17th problem. The main\nidea is to make use of the randomness contained in the input itself.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:20:10 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 13:22:13 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 07:30:29 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Lairez", "Pierre", ""]]}, {"id": "1507.05592", "submitter": "Bill Fefferman", "authors": "Bill Fefferman and Chris Umans", "title": "The Power of Quantum Fourier Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of work initiated by Terhal and DiVincenzo and Bremner, Jozsa, and\nShepherd, shows that quantum computers can efficiently sample from probability\ndistributions that cannot be exactly sampled efficiently on a classical\ncomputer, unless the PH collapses. Aaronson and Arkhipov take this further by\nconsidering a distribution that can be sampled efficiently by linear optical\nquantum computation, that under two feasible conjectures, cannot even be\napproximately sampled classically within bounded total variation distance,\nunless the PH collapses.\n  In this work we use Quantum Fourier Sampling to construct a class of\ndistributions that can be sampled by a quantum computer. We then argue that\nthese distributions cannot be approximately sampled classically, unless the PH\ncollapses, under variants of the Aaronson and Arkhipov conjectures.\n  In particular, we show a general class of quantumly sampleable distributions\neach of which is based on an \"Efficiently Specifiable\" polynomial, for which a\nclassical approximate sampler implies an average-case approximation. This class\nof polynomials contains the Permanent but also includes, for example, the\nHamiltonian Cycle polynomial, and many other familiar #P-hard polynomials.\n  Although our construction, unlike that proposed by Aaronson and Arkhipov,\nlikely requires a universal quantum computer, we are able to use this\nadditional power to weaken the conjectures needed to prove approximate sampling\nhardness results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 18:59:36 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Fefferman", "Bill", ""], ["Umans", "Chris", ""]]}, {"id": "1507.05753", "submitter": "Deepak Ponvel Chermakani Mr", "authors": "Deepak Ponvel Chermakani", "title": "Optimal Aggregation of Blocks into Subproblems in Linear-Programs with\n  Block-Diagonal-Structure", "comments": "Added Theorem 8 to cover the case of the P function being linear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wall-clock-time is minimized for a solution to a linear-program with\nblock-diagonal-structure, by decomposing the linear-program into as many\nsmall-sized subproblems as possible, each block resulting in a separate\nsubproblem, when the number of available parallel-processing-units is at least\nequal to the number of blocks. This is not necessarily the case when the\nparallel processing capability is limited, causing multiple subproblems to be\nserially solved on the same processing-unit. In such a situation, it might be\nbetter to aggregate blocks into larger sized subproblems. The optimal\naggregation strategy depends on the computing-platform used, and minimizes the\naverage-case running time for the set of subproblems. We show that optimal\naggregation is NP-hard when blocks are of unequal size, and that optimal\naggregation can be achieved within polynomial-time when blocks are of equal\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 08:57:30 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 07:03:18 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chermakani", "Deepak Ponvel", ""]]}, {"id": "1507.05841", "submitter": "Martin Marinov Mr", "authors": "Martin Marinov and David Gregg", "title": "On the GI-Completeness of a Sorting Networks Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subitemset isomorphism problem is really important and there are\nexcellent practical solutions described in the literature. However, the\ncomputational complexity analysis and classification of the BZ (Bundala and\nZavodny) subitemset isomorphism problem is currently an open problem. In this\npaper we prove that checking whether two sorting networks are BZ isomorphic to\neach other is GI-Complete; the general GI (Graph Isomorphism) problem is known\nto be in NP and LWPP, but widely believed to be neither P nor NP-Complete;\nrecent research suggests that the problem is in QP. Moreover, we state the BZ\nsorting network isomorphism problem as a general isomorphism problem on\nitemsets --- because every sorting network is represented by Bundala and\nZavodny as an itemset. The complexity classification presented in this paper\napplies sorting networks, as well as the general itemset isomorphism problem.\nThe main consequence of our work is that currently no polynomial-time algorithm\nexists for solving the BZ sorting network subitemset isomorphism problem;\nhowever the CM (Choi and Moon) sorting network isomorphism problem can be\nefficiently solved in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 14:12:29 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 13:45:27 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 15:16:10 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Marinov", "Martin", ""], ["Gregg", "David", ""]]}, {"id": "1507.05950", "submitter": "Dimitris S. Papailiopoulos", "authors": "Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein", "title": "On the Worst-Case Approximability of Sparse PCA", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:34:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Chan", "Siu On", ""], ["Papailiopoulos", "Dimitris", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1507.06370", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Avi Wigderson", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 05:50:16 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ma", "Tengyu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1507.06736", "submitter": "Bubacarr Bah", "authors": "Bubacarr Bah, and Rachel Ward", "title": "The sample complexity of weighted sparse approximation", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2543211", "report-no": null, "categories": "math.NA cs.CC cs.IT math.FA math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Gaussian sampling matrices, we provide bounds on the minimal number of\nmeasurements $m$ required to achieve robust weighted sparse recovery guarantees\nin terms of how well a given prior model for the sparsity support aligns with\nthe true underlying support. Our main contribution is that for a sparse vector\n${\\bf x} \\in \\mathbb{R}^N$ supported on an unknown set $\\mathcal{S} \\subset\n\\{1, \\dots, N\\}$ with $|\\mathcal{S}|\\leq k$, if $\\mathcal{S}$ has\n\\emph{weighted cardinality} $\\omega(\\mathcal{S}) := \\sum_{j \\in \\mathcal{S}}\n\\omega_j^2$, and if the weights on $\\mathcal{S}^c$ exhibit mild growth,\n$\\omega_j^2 \\geq \\gamma \\log(j/\\omega(\\mathcal{S}))$ for $j\\in\\mathcal{S}^c$\nand $\\gamma > 0$, then the sample complexity for sparse recovery via weighted\n$\\ell_1$-minimization using weights $\\omega_j$ is linear in the weighted\nsparsity level, and $m = \\mathcal{O}(\\omega(\\mathcal{S})/\\gamma)$. This main\nresult is a generalization of special cases including a) the standard sparse\nrecovery setting where all weights $\\omega_j \\equiv 1$, and $m =\n\\mathcal{O}\\left(k\\log\\left(N/k\\right)\\right)$; b) the setting where the\nsupport is known a priori, and $m = \\mathcal{O}(k)$; and c) the setting of\nsparse recovery with prior information, and $m$ depends on how well the weights\nare aligned with the support set $\\mathcal{S}$. We further extend the results\nin case c) to the setting of additive noise. Our results are {\\em nonuniform}\nthat is they apply for a fixed support, unknown a priori, and the weights on\n$\\mathcal{S}$ do not all have to be smaller than the weights on $\\mathcal{S}^c$\nfor our recovery results to hold.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 04:22:05 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 17:54:49 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 04:39:13 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 05:09:14 GMT"}, {"version": "v5", "created": "Sat, 9 Apr 2016 13:03:19 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Bah", "Bubacarr", ""], ["Ward", "Rachel", ""]]}, {"id": "1507.06865", "submitter": "H\\\"useyin Akcan", "authors": "Yunus Can Bilge, Do\\u{g}ukan \\c{C}a\\u{g}atay, Beg\\\"um Gen\\c{c}, Mecit\n  Sar{\\i}, H\\\"useyin Akcan, Cem Evrendilek", "title": "All Colors Shortest Path Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All Colors Shortest Path problem defined on an undirected graph aims at\nfinding a shortest, possibly non-simple, path where every color occurs at least\nonce, assuming that each vertex in the graph is associated with a color known\nin advance. To the best of our knowledge, this paper is the first to define and\ninvestigate this problem. Even though the problem is computationally similar to\ngeneralized minimum spanning tree, and the generalized traveling salesman\nproblems, allowing for non-simple paths where a node may be visited multiple\ntimes makes All Colors Shortest Path problem novel and computationally unique.\nIn this paper we prove that All Colors Shortest Path problem is NP-hard, and\ndoes not lend itself to a constant factor approximation. We also propose\nseveral heuristic solutions for this problem based on LP-relaxation, simulated\nannealing, ant colony optimization, and genetic algorithm, and provide\nextensive simulations for a comparative analysis of them. The heuristics\npresented are not the standard implementations of the well known heuristic\nalgorithms, but rather sophisticated models tailored for the problem in hand.\nThis fact is acknowledged by the very promising results reported.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 14:57:02 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Bilge", "Yunus Can", ""], ["\u00c7a\u011fatay", "Do\u011fukan", ""], ["Gen\u00e7", "Beg\u00fcm", ""], ["Sar\u0131", "Mecit", ""], ["Akcan", "H\u00fcseyin", ""], ["Evrendilek", "Cem", ""]]}, {"id": "1507.06878", "submitter": "Shogo Nakajima", "authors": "Fran\\c{c}ois Le Gall and Shogo Nakajima", "title": "Quantum Algorithm for Triangle Finding in Sparse Graphs", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a quantum algorithm for triangle finding over sparse\ngraphs that improves over the previous best quantum algorithm for this task by\nBuhrman et al. [SIAM Journal on Computing, 2005]. Our algorithm is based on the\nrecent $\\tilde O(n^{5/4})$-query algorithm given by Le Gall [FOCS 2014] for\ntriangle finding over dense graphs (here $n$ denotes the number of vertices in\nthe graph). We show in particular that triangle finding can be solved with\n$O(n^{5/4-\\epsilon})$ queries for some constant $\\epsilon>0$ whenever the graph\nhas at most $O(n^{2-c})$ edges for some constant $c>0$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 15:16:31 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Nakajima", "Shogo", ""]]}, {"id": "1507.07459", "submitter": "Tim Oosterwijk", "authors": "Tim Oosterwijk", "title": "On local search and LP and SDP relaxations for k-Set Packing", "comments": "There is a mistake in the following line of Theorem 17: \"As an\n  induced subgraph of H with more edges than vertices constitutes an improving\n  set\". Therefore, the proofs of Theorem 17, and hence Theorems 19, 23 and 24,\n  are false. It is still open whether these theorems are true", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set packing is a fundamental problem that generalises some well-known\ncombinatorial optimization problems and knows a lot of applications. It is\nequivalent to hypergraph matching and it is strongly related to the maximum\nindependent set problem. In this thesis we study the k-set packing problem\nwhere given a universe U and a collection C of subsets over U, each of\ncardinality k, one needs to find the maximum collection of mutually disjoint\nsubsets. Local search techniques have proved to be successful in the search for\napproximation algorithms, both for the unweighted and the weighted version of\nthe problem where every subset in C is associated with a weight and the\nobjective is to maximise the sum of the weights. We make a survey of these\napproaches and give some background and intuition behind them. In particular,\nwe simplify the algebraic proof of the main lemma for the currently best\nweighted approximation algorithm of Berman ([Ber00]) into a proof that reveals\nmore intuition on what is really happening behind the math. The main result is\na new bound of k/3 + 1 + epsilon on the integrality gap for a polynomially\nsized LP relaxation for k-set packing by Chan and Lau ([CL10]) and the natural\nSDP relaxation [NOTE: see page iii]. We provide detailed proofs of lemmas\nneeded to prove this new bound and treat some background on related topics like\nsemidefinite programming and the Lovasz Theta function. Finally we have an\nextended discussion in which we suggest some possibilities for future research.\nWe discuss how the current results from the weighted approximation algorithms\nand the LP and SDP relaxations might be improved, the strong relation between\nset packing and the independent set problem and the difference between the\nweighted and the unweighted version of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 15:51:50 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Oosterwijk", "Tim", ""]]}, {"id": "1507.07581", "submitter": "Shant Boodaghians", "authors": "Shant Boodaghians and Adrian Vetta", "title": "Testing Consumer Rationality using Perfect Graphs and Oriented Discs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a consumer data-set, the axioms of revealed preference proffer a binary\ntest for rational behaviour. A natural (non-binary) measure of the degree of\nrationality exhibited by the consumer is the minimum number of data points\nwhose removal induces a rationalisable data-set.We study the computational\ncomplexity of the resultant consumer rationality problem in this paper. This\nproblem is, in the worst case, equivalent (in terms of approximation) to the\ndirected feedback vertex set problem. Our main result is to obtain an exact\nthreshold on the number of commodities that separates easy cases and hard\ncases. Specifically, for two-commodity markets the consumer rationality problem\nis polynomial time solvable; we prove this via a reduction to the vertex cover\nproblem on perfect graphs. For three-commodity markets, however, the problem is\nNP-complete; we prove thisusing a reduction from planar 3-SAT that is based\nupon oriented-disc drawings.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 20:31:51 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 23:02:29 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Boodaghians", "Shant", ""], ["Vetta", "Adrian", ""]]}, {"id": "1507.07856", "submitter": "Rahul Cs", "authors": "N.S. Narayanaswamy, C.S. Rahul", "title": "A Classification of Connected f -factor Problems inside NP", "comments": "15 pages, submitted to FSTTCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an undirected graph G = (V, E) with n vertices, and a function f : V ->\nN, we consider the problem of finding a connected f -factor in G. In this work\nwe design an algorithm to check for the existence of a connected f -factor, for\nthe case where f (v) >= n/g(n), for all v in V and g(n) is polylogarithmic in\nn. The running time of our algorithm is O(n^{2g(n)}. As a consequence of this\nalgorithm we conclude that the complexity of connected f -factor for the case\nwe consider is unlikely to be NP-Complete unless the Exponential Time\nHypothesis (ETH) is false. Secondly, under the assumption of the ETH, we show\nthat it is also unlikely to be in P for g(n) in O((log n)^{1+eps} ) for any\neps> 0. Therefore, our results show that for all eps> 0, connected f -factor\nfor f (v) >= n/O(log n)^{1+eps}) is in NP-Intermediate unless the ETH is false.\nFurther, for any constant c > 0, when g(n) = c, our algorithm for connected f\n-factor runs in polynomial time. Finally, we extend our algorithm to compute a\nminimum weight connected f -factor in edge weighted graphs in the same\nasymptotic time bounds.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 17:21:06 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Narayanaswamy", "N. S.", ""], ["Rahul", "C. S.", ""]]}, {"id": "1507.08235", "submitter": "Lilla T\\'othm\\'er\\'esz", "authors": "Lilla T\\'othm\\'er\\'esz", "title": "Algorithmic aspects of rotor-routing and the notion of linear\n  equivalence", "comments": null, "journal-ref": "Discrete Applied Mathematics 236: pp. 428-437. (2018)", "doi": null, "report-no": null, "categories": "math.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the analogue of linear equivalence of graph divisors for the\nrotor-router model, and use it to prove polynomial time computability of some\nproblems related to rotor-routing. Using the connection between linear\nequivalence for chip-firing and for rotor-routing, we give a simple proof for\nthe fact that the number of rotor-router unicycle-orbits equals the order of\nthe Picard group. We also show that the rotor-router action of the Picard group\non the set of spanning in-arborescences can be interpreted in terms of the\nlinear equivalence.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 17:42:40 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 10:18:50 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["T\u00f3thm\u00e9r\u00e9sz", "Lilla", ""]]}, {"id": "1507.08582", "submitter": "Giovanni Pighizzini", "authors": "Giovanni Pighizzini", "title": "One-Tape Turing Machine Variants and Language Recognition", "comments": "20 pages. This article will appear in the Complexity Theory Column of\n  the September 2015 issue of SIGACT News", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two restricted versions of one-tape Turing machines. Both\ncharacterize the class of context-free languages. In the first version,\nproposed by Hibbard in 1967 and called limited automata, each tape cell can be\nrewritten only in the first $d$ visits, for a fixed constant $d\\geq 2$.\nFurthermore, for $d=2$ deterministic limited automata are equivalent to\ndeterministic pushdown automata, namely they characterize deterministic\ncontext-free languages. Further restricting the possible operations, we\nconsider strongly limited automata. These models still characterize\ncontext-free languages. However, the deterministic version is less powerful\nthan the deterministic version of limited automata. In fact, there exist\ndeterministic context-free languages that are not accepted by any deterministic\nstrongly limited automaton.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:58:23 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Pighizzini", "Giovanni", ""]]}, {"id": "1507.08690", "submitter": "Holger Petersen", "authors": "Holger Petersen", "title": "The Complexity of Some Combinatorial Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the decision versions of the puzzles Knossos and The Hour-Glass\nare complete for NP.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 21:04:54 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Petersen", "Holger", ""]]}]