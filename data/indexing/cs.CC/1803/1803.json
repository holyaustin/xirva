[{"id": "1803.00618", "submitter": "Marcelo S. Reis", "authors": "Marcelo S. Reis", "title": "Finding steady-state solutions for ODE systems of zero, first and\n  homogeneous second-order chemical reactions is NP-hard", "comments": "8 pages, 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of modeling of cell signaling pathways, a relevant step is\nfinding steady-state solutions for ODE systems that describe the kinetics of a\nset of chemical reactions, especially sets composed of zero, first, and\nsecond-order reactions. To compute a steady-state solution, one must set the\nleft-hand side of each ODE as zero, hence obtaining a system of non-negative,\nquadratic polynomial equations. If all second-order reactions are homogeneous\nin respect to their reactants, then the obtained quadratic polynomial equation\nsystem will also have univariate monomials. Although it is a well-known fact\nthat finding a root of a quadratic polynomial equation system is a NP-hard\nproblem, it is not so easy to find a readily available proof of NP-hardness for\nspecial cases like the aforementioned one. Therefore, we provide here a\nself-contained proof that finding a root of non-negative, with univariate\nmonomials quadratic polynomial equation system (NUMQ-PES) is NP-hard. This\nresult implies that finding steady-state solutions for ODE systems of zero,\nfirst and homogeneous second-order chemical reactions is a NP-hard problem;\nhence, it is not a feasible approach to approximate non-homogeneous\nsecond-order reactions into homogeneous ones.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 14:33:36 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 13:48:34 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Reis", "Marcelo S.", ""]]}, {"id": "1803.00796", "submitter": "Karl Bringmann", "authors": "Amir Abboud, Arturs Backurs, Karl Bringmann, Marvin K\\\"unnemann", "title": "Fine-Grained Complexity of Analyzing Compressed Data: Quantifying\n  Improvements over Decompress-And-Solve", "comments": "Presented at FOCS'17. Full version. 63 pages", "journal-ref": null, "doi": "10.1109/FOCS.2017.26", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we analyze data without decompressing it? As our data keeps growing,\nunderstanding the time complexity of problems on compressed inputs, rather than\nin convenient uncompressed forms, becomes more and more relevant. Suppose we\nare given a compression of size $n$ of data that originally has size $N$, and\nwe want to solve a problem with time complexity $T(\\cdot)$. The naive strategy\nof \"decompress-and-solve\" gives time $T(N)$, whereas \"the gold standard\" is\ntime $T(n)$: to analyze the compression as efficiently as if the original data\nwas small.\n  We restrict our attention to data in the form of a string (text, files,\ngenomes, etc.) and study the most ubiquitous tasks. While the challenge might\nseem to depend heavily on the specific compression scheme, most methods of\npractical relevance (Lempel-Ziv-family, dictionary methods, and others) can be\nunified under the elegant notion of Grammar Compressions. A vast literature,\nacross many disciplines, established this as an influential notion for\nAlgorithm design.\n  We introduce a framework for proving (conditional) lower bounds in this\nfield, allowing us to assess whether decompress-and-solve can be improved, and\nby how much. Our main results are:\n  - The $O(nN\\sqrt{\\log{N/n}})$ bound for LCS and the $O(\\min\\{N \\log N, nM\\})$\nbound for Pattern Matching with Wildcards are optimal up to $N^{o(1)}$ factors,\nunder the Strong Exponential Time Hypothesis. (Here, $M$ denotes the\nuncompressed length of the compressed pattern.)\n  - Decompress-and-solve is essentially optimal for Context-Free Grammar\nParsing and RNA Folding, under the $k$-Clique conjecture.\n  - We give an algorithm showing that decompress-and-solve is not optimal for\nDisjointness.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:32:07 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "1803.00804", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Philip Wellnitz", "title": "Clique-Based Lower Bounds for Parsing Tree-Adjoining Grammars", "comments": "Presented at CPM'17. 15 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.CPM.2017.12", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-adjoining grammars are a generalization of context-free grammars that\nare well suited to model human languages and are thus popular in computational\nlinguistics. In the tree-adjoining grammar recognition problem, given a grammar\n$\\Gamma$ and a string $s$ of length $n$, the task is to decide whether $s$ can\nbe obtained from $\\Gamma$. Rajasekaran and Yooseph's parser (JCSS'98) solves\nthis problem in time $O(n^{2\\omega})$, where $\\omega < 2.373$ is the matrix\nmultiplication exponent. The best algorithms avoiding fast matrix\nmultiplication take time $O(n^6)$.\n  The first evidence for hardness was given by Satta (J. Comp. Linguist.'94):\nFor a more general parsing problem, any algorithm that avoids fast matrix\nmultiplication and is significantly faster than $O(|\\Gamma| n^6)$ in the case\nof $|\\Gamma| = \\Theta(n^{12})$ would imply a breakthrough for Boolean matrix\nmultiplication.\n  Following an approach by Abboud et al. (FOCS'15) for context-free grammar\nrecognition, in this paper we resolve many of the disadvantages of the previous\nlower bound. We show that, even on constant-size grammars, any improvement on\nRajasekaran and Yooseph's parser would imply a breakthrough for the $k$-Clique\nproblem. This establishes tree-adjoining grammar parsing as a practically\nrelevant problem with the unusual running time of $n^{2\\omega}$, up to lower\norder factors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:56:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bringmann", "Karl", ""], ["Wellnitz", "Philip", ""]]}, {"id": "1803.00882", "submitter": "Philipp Zschoche", "authors": "Till Fluschnik, Hendrik Molter, Rolf Niedermeier, Malte Renken, and\n  Philipp Zschoche", "title": "Temporal Graph Classes: A View Through Temporal Separators", "comments": "arXiv admin note: text overlap with arXiv:1711.00963", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of separating two distinct\nvertices s and z by vertex deletion in a temporal graph. In a temporal graph,\nthe vertex set is fixed but the edges have (discrete) time labels. Since the\ncorresponding Temporal (s, z)-Separation problem is NP-hard, it is natural to\ninvestigate whether relevant special cases exist that are computationally\ntractable. To this end, we study restrictions of the underlying (static)\ngraph---there we observe polynomial-time solvability in the case of bounded\ntreewidth---as well as restrictions concerning the \"temporal evolution\" along\nthe time steps. Systematically studying partially novel concepts in this\ndirection, we identify sharp borders between tractable and intractable cases.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 13:12:40 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 08:02:21 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Fluschnik", "Till", ""], ["Molter", "Hendrik", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""], ["Zschoche", "Philipp", ""]]}, {"id": "1803.00904", "submitter": "Aviad Rubinstein", "authors": "Aviad Rubinstein", "title": "Hardness of Approximate Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove conditional near-quadratic running time lower bounds for approximate\nBichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance.\nSpecifically, unless the Strong Exponential Time Hypothesis (SETH) is false,\nfor every $\\delta>0$ there exists a constant $\\epsilon>0$ such that computing a\n$(1+\\epsilon)$-approximation to the Bichromatic Closest Pair requires\n$n^{2-\\delta}$ time. In particular, this implies a near-linear query time for\nApproximate Nearest Neighbor search with polynomial preprocessing time.\n  Our reduction uses the Distributed PCP framework of [ARW'17], but obtains\nimproved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG\ncodes have been constructed in other settings before [BKKMS'16, BCGRS'17], but\nour construction is the first to yield new hardness results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 15:44:01 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Rubinstein", "Aviad", ""]]}, {"id": "1803.00938", "submitter": "Karl Bringmann", "authors": "Karl Bringmann, Marvin K\\\"unnemann", "title": "Multivariate Fine-Grained Complexity of Longest Common Subsequence", "comments": "Presented at SODA'18. Full Version. 66 pages", "journal-ref": null, "doi": "10.1137/1.9781611975031.79", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic combinatorial pattern matching problem of finding a\nlongest common subsequence (LCS). For strings $x$ and $y$ of length $n$, a\ntextbook algorithm solves LCS in time $O(n^2)$, but although much effort has\nbeen spent, no $O(n^{2-\\varepsilon})$-time algorithm is known. Recent work\nindeed shows that such an algorithm would refute the Strong Exponential Time\nHypothesis (SETH) [Abboud, Backurs, Vassilevska Williams + Bringmann,\nK\\\"unnemann FOCS'15].\n  Despite the quadratic-time barrier, for over 40 years an enduring scientific\ninterest continued to produce fast algorithms for LCS and its variations.\nParticular attention was put into identifying and exploiting input parameters\nthat yield strongly subquadratic time algorithms for special cases of interest,\ne.g., differential file comparison. This line of research was successfully\npursued until 1990, at which time significant improvements came to a halt. In\nthis paper, using the lens of fine-grained complexity, our goal is to (1)\njustify the lack of further improvements and (2) determine whether some special\ncases of LCS admit faster algorithms than currently known.\n  To this end, we provide a systematic study of the multivariate complexity of\nLCS, taking into account all parameters previously discussed in the literature:\nthe input size $n:=\\max\\{|x|,|y|\\}$, the length of the shorter string\n$m:=\\min\\{|x|,|y|\\}$, the length $L$ of an LCS of $x$ and $y$, the numbers of\ndeletions $\\delta := m-L$ and $\\Delta := n-L$, the alphabet size, as well as\nthe numbers of matching pairs $M$ and dominant pairs $d$. For any class of\ninstances defined by fixing each parameter individually to a polynomial in\nterms of the input size, we prove a SETH-based lower bound matching one of\nthree known algorithms. Specifically, we determine the optimal running time for\nLCS under SETH as $(n+\\min\\{d, \\delta \\Delta, \\delta m\\})^{1\\pm o(1)}$.\n  [...]\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:26:13 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""]]}, {"id": "1803.01519", "submitter": "Tom Gur", "authors": "Alessandro Chiesa, Michael A. Forbes, Tom Gur, Nicholas Spooner", "title": "Spatial Isolation Implies Zero Knowledge Even in a Quantum World", "comments": "55 pages. arXiv admin note: text overlap with arXiv:1704.02086", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero knowledge plays a central role in cryptography and complexity. The\nseminal work of Ben-Or et al. (STOC 1988) shows that zero knowledge can be\nachieved unconditionally for any language in NEXP, as long as one is willing to\nmake a suitable physical assumption: if the provers are spatially isolated,\nthen they can be assumed to be playing independent strategies. Quantum\nmechanics, however, tells us that this assumption is unrealistic, because\nspatially-isolated provers could share a quantum entangled state and realize a\nnon-local correlated strategy. The MIP* model captures this setting. In this\nwork we study the following question: does spatial isolation still suffice to\nunconditionally achieve zero knowledge even in the presence of quantum\nentanglement? We answer this question in the affirmative: we prove that every\nlanguage in NEXP has a 2-prover zero knowledge interactive proof that is sound\nagainst entangled provers; that is, NEXP \\subseteq ZK-MIP*. Our proof consists\nof constructing a zero knowledge interactive PCP with a strong algebraic\nstructure, and then lifting it to the MIP* model. This lifting relies on a new\nframework that builds on recent advances in low-degree testing against\nentangled strategies, and clearly separates classical and quantum tools. Our\nmain technical contribution consists of developing new algebraic techniques for\nobtaining unconditional zero knowledge; this includes a zero knowledge variant\nof the celebrated sumcheck protocol, a key building block in many probabilistic\nproof systems. A core component of our sumcheck protocol is a new algebraic\ncommitment scheme, whose analysis relies on algebraic complexity theory.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 07:01:36 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Chiesa", "Alessandro", ""], ["Forbes", "Michael A.", ""], ["Gur", "Tom", ""], ["Spooner", "Nicholas", ""]]}, {"id": "1803.02186", "submitter": "Hector Zenil", "authors": "Hector Zenil, Narsis A. Kiani, Jesper Tegn\\'er", "title": "Symmetry and Algorithmic Complexity of Polyominoes and Polyhedral Graphs", "comments": "18 pages, 4 figures + Appendix (1 figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DM cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a definition of algorithmic symmetry able to capture essential\naspects of geometric symmetry. We review, study and apply a method for\napproximating the algorithmic complexity (also known as Kolmogorov-Chaitin\ncomplexity) of graphs and networks based on the concept of Algorithmic\nProbability (AP). AP is a concept (and method) capable of recursively\nenumeration all properties of computable (causal) nature beyond statistical\nregularities. We explore the connections of algorithmic complexity---both\ntheoretical and numerical---with geometric properties mainly symmetry and\ntopology from an (algorithmic) information-theoretic perspective. We show that\napproximations to algorithmic complexity by lossless compression and an\nAlgorithmic Probability-based method can characterize properties of\npolyominoes, polytopes, regular and quasi-regular polyhedra as well as\npolyhedral networks, thereby demonstrating its profiling capabilities.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 16:04:43 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Zenil", "Hector", ""], ["Kiani", "Narsis A.", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1803.02289", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "Testing the complexity of a valued CSP language", "comments": "to appear in ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Valued Constraint Satisfaction Problem (VCSP) provides a common framework\nthat can express a wide range of discrete optimization problems. A VCSP\ninstance is given by a finite set of variables, a finite domain of labels, and\nan objective function to be minimized. This function is represented as a sum of\nterms where each term depends on a subset of the variables. To obtain different\nclasses of optimization problems, one can restrict all terms to come from a\nfixed set $\\Gamma$ of cost functions, called a language.\n  Recent breakthrough results have established a complete complexity\nclassification of such classes with respect to language $\\Gamma$: if all cost\nfunctions in $\\Gamma$ satisfy a certain algebraic condition then all\n$\\Gamma$-instances can be solved in polynomial time, otherwise the problem is\nNP-hard. Unfortunately, testing this condition for a given language $\\Gamma$ is\nknown to be NP-hard. We thus study exponential algorithms for this\nmeta-problem. We show that the tractability condition of a finite-valued\nlanguage $\\Gamma$ can be tested in $O(\\sqrt[3]{3}^{\\,|D|}\\cdot\npoly(size(\\Gamma)))$ time, where $D$ is the domain of $\\Gamma$ and\n$poly(\\cdot)$ is some fixed polynomial. We also obtain a matching lower bound\nunder the Strong Exponential Time Hypothesis (SETH). More precisely, we prove\nthat for any constant $\\delta<1$ there is no $O(\\sqrt[3]{3}^{\\,\\delta|D|})$\nalgorithm, assuming that SETH holds.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:38:08 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 15:51:01 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 20:31:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1803.02409", "submitter": "Christopher Purcell", "authors": "William Phan and Christopher Purcell", "title": "On the parameterized complexity of manipulating Top Trading Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of exchange when 1) agents are endowed with\nheterogeneous indivisible objects, and 2) there is no money. In general, no\nrule satisfies the three central properties Pareto-efficiency, individual\nrationality, and strategy-proofness \\cite{Sonmez1999}. Recently, it was shown\nthat Top Trading Cycles is $\\NP$-hard to manipulate \\cite{FujitaEA2015}, a\nrelaxation of strategy-proofness. However, parameterized complexity is a more\nappropriate framework for this and other economic settings. Certain aspects of\nthe problem - number of objects each agent brings to the table, goods up for\nauction, candidates in an election \\cite{consandlang2007}, legislative figures\nto influence \\cite{christian2007complexity} - may face natural bounds or are\nfixed as the problem grows. We take a parameterized complexity approach to\nindivisible goods exchange for the first time. Our results represent good and\nbad news for TTC. When the size of the endowments $k$ is a fixed constant, we\nshow that the computational task of manipulating TTC can be performed in\npolynomial time. On the other hand, we show that this parameterized problem is\n$\\W[1]$-hard, and therefore unlikely to be \\emph{fixed parameter tractable}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 20:00:14 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Phan", "William", ""], ["Purcell", "Christopher", ""]]}, {"id": "1803.02849", "submitter": "David Mass", "authors": "Tali Kaufman, David Mass", "title": "Good Distance Lattices from High Dimensional Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a new framework for constructing good distance lattices from high\ndimensional expanders. For error-correcting codes, which have a similar flavor\nas lattices, there is a known framework that yields good codes from expanders.\nHowever, prior to our work, there has been no framework that yields good\ndistance lattices directly from expanders. Interestingly, we need the notion of\nhigh dimensional expansion (and not only one dimensional expansion) for\nobtaining large distance lattices which are dense.\n  Our construction is obtained by proving the existence of bounded degree high\ndimensional cosystolic expanders over any ring, and in particular over\n$\\mathbb{Z}$. Previous bounded degree cosystolic expanders were known only over\n$\\mathbb{F}_2$. The proof of the cosystolic expansion over any ring is composed\nof two main steps, each of an independent interest: We show that coboundary\nexpansion over any ring of the links of a bounded degree complex implies that\nthe complex is a cosystolic expander over any ring. We then prove that all the\nlinks of Ramanujan complexes (which are called spherical buildings) are\ncoboundary expanders over any ring.\n  We follow the strategy of [LMM16] for proving that the spherical building is\na coboundary expander over any ring. Besides of generalizing their proof from\n$\\mathbb{F}_2$ to any ring, we present it in a detailed way, which might serve\nreaders with less background who wish to get into the field.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 19:30:20 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Kaufman", "Tali", ""], ["Mass", "David", ""]]}, {"id": "1803.03239", "submitter": "Michael P. Kim", "authors": "Michael P. Kim and Omer Reingold and Guy N. Rothblum", "title": "Fairness Through Computationally-Bounded Awareness", "comments": "Appears at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of fair classification within the versatile framework of\nDwork et al. [ITCS '12], which assumes the existence of a metric that measures\nsimilarity between pairs of individuals. Unlike earlier work, we do not assume\nthat the entire metric is known to the learning algorithm; instead, the learner\ncan query this arbitrary metric a bounded number of times. We propose a new\nnotion of fairness called metric multifairness and show how to achieve this\nnotion in our setting. Metric multifairness is parameterized by a similarity\nmetric $d$ on pairs of individuals to classify and a rich collection ${\\cal C}$\nof (possibly overlapping) \"comparison sets\" over pairs of individuals. At a\nhigh level, metric multifairness guarantees that similar subpopulations are\ntreated similarly, as long as these subpopulations are identified within the\nclass ${\\cal C}$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:23:17 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 18:23:37 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kim", "Michael P.", ""], ["Reingold", "Omer", ""], ["Rothblum", "Guy N.", ""]]}, {"id": "1803.03514", "submitter": "Torstein J. F. Str{\\o}mme", "authors": "Lars Jaffke, O-joung Kwon, Torstein J. F. Str{\\o}mme, Jan Arne Telle", "title": "Generalized distance domination problems and their complexity on graphs\n  of bounded mim-width", "comments": "Accepted at IPEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the family of $(\\sigma, \\rho)$-problems and locally checkable\nvertex partition problems to their distance versions, which naturally captures\nwell-known problems such as distance-$r$ dominating set and distance-$r$\nindependent set. We show that these distance problems are XP parameterized by\nthe structural parameter mim-width, and hence polynomial on graph classes where\nmim-width is bounded and quickly computable, such as $k$-trapezoid graphs,\nDilworth $k$-graphs, (circular) permutation graphs, interval graphs and their\ncomplements, convex graphs and their complements, $k$-polygon graphs, circular\narc graphs, complements of $d$-degenerate graphs, and $H$-graphs if given an\n$H$-representation. To supplement these findings, we show that many classes of\n(distance) $(\\sigma, \\rho)$-problems are W[1]-hard parameterized by mim-width +\nsolution size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 14:01:46 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 15:12:12 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Jaffke", "Lars", ""], ["Kwon", "O-joung", ""], ["Str\u00f8mme", "Torstein J. F.", ""], ["Telle", "Jan Arne", ""]]}, {"id": "1803.03600", "submitter": "Gregorio Malajovich", "authors": "Gregorio Malajovich and Mike Shub", "title": "A theory of NP-completeness and ill-conditioning for approximate real\n  computations", "comments": null, "journal-ref": "Journal of the ACM 66(4) Article 27 pp 1-38 (May 2019)", "doi": "10.1145/3321479", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a complexity theory for approximate real computations. We first\nproduce a theory for exact computations but with condition numbers. The input\nsize depends on a condition number, which is not assumed known by the machine.\nThe theory admits deterministic and nondeterministic polynomial time\nrecognizable problems. We prove that P is not NP in this theory if and only if\nP is not NP in the BSS theory over the reals.\n  Then we develop a theory with weak and strong approximate computations. This\ntheory is intended to model actual numerical computations that are usually\nperformed in floating point arithmetic. It admits classes P and NP and also an\nNP-complete problem. We relate the P vs NP question in this new theory to the\nclassical P vs NP problem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 17:06:54 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 12:17:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Malajovich", "Gregorio", ""], ["Shub", "Mike", ""]]}, {"id": "1803.03663", "submitter": "Daniel Paulusma", "authors": "Barnaby Martin, Daniel Paulusma, Erik Jan van Leeuwen", "title": "Disconnected Cuts in Claw-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disconnected cut of a connected graph is a vertex cut that itself also\ninduces a disconnected subgraph. The decision problem whether a graph has a\ndisconnected cut is called Disconnected Cut. This problem is closely related to\nseveral homomorphism and contraction problems, and fits in an extensive line of\nresearch on vertex cuts with additional properties. It is known that\nDisconnected Cut is NP-hard on general graphs, while polynomial-time algorithms\nare known for several graph classes. However, the complexity of the problem on\nclaw-free graphs remained an open question. Its connection to the complexity of\nthe problem to contract a claw-free graph to the 4-vertex cycle $C_4$ led Ito\net al. (TCS 2011) to explicitly ask to resolve this open question.\n  We prove that Disconnected Cut is polynomial-time solvable on claw-free\ngraphs, answering the question of Ito et al. The centerpiece of our result is a\nnovel decomposition theorem for claw-free graphs of diameter 2, which we\nbelieve is of independent interest and expands the research line initiated by\nChudnovsky and Seymour (JCTB 2007-2012) and Hermelin et al. (ICALP 2011). On\nour way to exploit this decomposition theorem, we characterize how disconnected\ncuts interact with certain cobipartite subgraphs, and prove two further novel\nalgorithmic results, namely Disconnected Cut is polynomial-time solvable on\ncircular-arc graphs and line graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 19:23:03 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Martin", "Barnaby", ""], ["Paulusma", "Daniel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1803.03708", "submitter": "Jeffrey Bosboom", "authors": "Jeffrey Bosboom, Erik D. Demaine, Mikhail Rudoy", "title": "Computational Complexity of Generalized Push Fight", "comments": "27 pages, 35 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational complexity of optimally playing the two-player\nboard game Push Fight, generalized to an arbitrary board and number of pieces.\nWe prove that the game is PSPACE-hard to decide who will win from a given\nposition, even for simple (almost rectangular) hole-free boards. We also\nanalyze the mate-in-1 problem: can the player win in a single turn? One turn in\nPush Fight consists of up to two \"moves\" followed by a mandatory \"push\". With\nthese rules, or generalizing the number of allowed moves to any constant, we\nshow mate-in-1 can be solved in polynomial time. If, however, the number of\nmoves per turn is part of the input, the problem becomes NP-complete. On the\nother hand, without any limit on the number of moves per turn, the problem\nbecomes polynomially solvable again.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 22:10:17 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Bosboom", "Jeffrey", ""], ["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1803.04025", "submitter": "Ofer Grossman", "authors": "Ofer Grossman, Yang P. Liu", "title": "Reproducibility and Pseudo-Determinism in Log-Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A curious property of randomized log-space search algorithms is that their\noutputs are often longer than their workspace. This leads to the question: how\ncan we reproduce the results of a randomized log space computation without\nstoring the output or randomness verbatim? Running the algorithm again with new\nrandom bits may result in a new (and potentially different) output.\n  We show that every problem in search-RL has a randomized log-space algorithm\nwhere the output can be reproduced. Specifically, we show that for every\nproblem in search-RL, there are a pair of log-space randomized algorithms A and\nB where for every input x, A will output some string t_x of size O(log n), such\nthat B when running on (x, t_x) will be pseudo-deterministic: that is, running\nB multiple times on the same input (x, t_x) will result in the same output on\nall executions with high probability. Thus, by storing only O(log n) bits in\nmemory, it is possible to reproduce the output of a randomized log-space\nalgorithm.\n  An algorithm is reproducible without storing any bits in memory (i.e.,\n|t_x|=0) if and only if it is pseudo-deterministic. We show\npseudo-deterministic algorithms for finding paths in undirected graphs and\nEulerian graphs using logarithmic space. Our algorithms are substantially\nfaster than the best known deterministic algorithms for finding paths in such\ngraphs in log-space.\n  The algorithm for search-RL has the additional property that its output, when\nviewed as a random variable depending on the randomness used by the algorithm,\nhas entropy O(log n).\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 19:39:54 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Grossman", "Ofer", ""], ["Liu", "Yang P.", ""]]}, {"id": "1803.04104", "submitter": "J. Maurice Rojas", "authors": "J. Maurice Rojas and Yuyu Zhu", "title": "Dedekind Zeta Zeroes and Faster Complex Dimension Computation", "comments": "11 pages, submitted to a conference. Some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC math.AG math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to earlier work of Koiran, it is known that the truth of the\nGeneralized Riemann Hypothesis (GRH) implies that the dimension of algebraic\nsets over the complex numbers can be determined within the\npolynomial-hierarchy. The truth of GRH thus provides a direct connection\nbetween a concrete algebraic geometry problem and the P vs.NP Problem, in a\nradically different direction from the geometric complexity theory approach to\nVP vs. VNP. We explore more plausible hypotheses yielding the same speed-up.\nOne minimalist hypothesis we derive involves improving the error term (as a\nfunction of the degree, coefficient height, and $x$) on the fraction of primes\n$p\\!\\leq\\!x$ for which a univariate polynomial has roots mod $p$. A second\nminimalist hypothesis involves sharpening current zero-free regions for\nDedekind zeta functions. Both our hypotheses allow failures of GRH but still\nenable complex dimension computation in the polynomial hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 03:27:45 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Rojas", "J. Maurice", ""], ["Zhu", "Yuyu", ""]]}, {"id": "1803.04402", "submitter": "Bill Fefferman", "authors": "Adam Bouland, Bill Fefferman, Chinmay Nirkhe, Umesh Vazirani", "title": "Quantum Supremacy and the Complexity of Random Circuit Sampling", "comments": null, "journal-ref": null, "doi": "10.1038/s41567-018-0318-2", "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical milestone on the path to useful quantum computers is quantum\nsupremacy - a demonstration of a quantum computation that is prohibitively hard\nfor classical computers. A leading near-term candidate, put forth by the\nGoogle/UCSB team, is sampling from the probability distributions of randomly\nchosen quantum circuits, which we call Random Circuit Sampling (RCS).\n  In this paper we study both the hardness and verification of RCS. While RCS\nwas defined with experimental realization in mind, we show complexity theoretic\nevidence of hardness that is on par with the strongest theoretical proposals\nfor supremacy. Specifically, we show that RCS satisfies an average-case\nhardness condition - computing output probabilities of typical quantum circuits\nis as hard as computing them in the worst-case, and therefore #P-hard. Our\nreduction exploits the polynomial structure in the output amplitudes of random\nquantum circuits, enabled by the Feynman path integral. In addition, it follows\nfrom known results that RCS satisfies an anti-concentration property, making it\nthe first supremacy proposal with both average-case hardness and\nanti-concentration.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 17:55:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Bouland", "Adam", ""], ["Fefferman", "Bill", ""], ["Nirkhe", "Chinmay", ""], ["Vazirani", "Umesh", ""]]}, {"id": "1803.04553", "submitter": "Li-Yang Tan", "authors": "Rocco A. Servedio and Li-Yang Tan", "title": "Luby--Veli\\v{c}kovi\\'c--Wigderson revisited: Improved correlation bounds\n  and pseudorandom generators for depth-two circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study correlation bounds and pseudorandom generators for depth-two\ncircuits that consist of a $\\mathsf{SYM}$-gate (computing an arbitrary\nsymmetric function) or $\\mathsf{THR}$-gate (computing an arbitrary linear\nthreshold function) that is fed by $S$ $\\mathsf{AND}$ gates. Such circuits were\nconsidered in early influential work on unconditional derandomization of Luby,\nVeli\\v{c}kovi\\'c, and Wigderson [LVW93], who gave the first non-trivial PRG\nwith seed length $2^{O(\\sqrt{\\log(S/\\varepsilon)})}$ that $\\varepsilon$-fools\nthese circuits.\n  In this work we obtain the first strict improvement of [LVW93]'s seed length:\nwe construct a PRG that $\\varepsilon$-fools size-$S$\n$\\{\\mathsf{SYM},\\mathsf{THR}\\} \\circ\\mathsf{AND}$ circuits over $\\{0,1\\}^n$\nwith seed length \\[ 2^{O(\\sqrt{\\log S })} + \\mathrm{polylog}(1/\\varepsilon), \\]\nan exponential (and near-optimal) improvement of the $\\varepsilon$-dependence\nof [LVW93]. The above PRG is actually a special case of a more general PRG\nwhich we establish for constant-depth circuits containing multiple\n$\\mathsf{SYM}$ or $\\mathsf{THR}$ gates, including as a special case\n$\\{\\mathsf{SYM},\\mathsf{THR}\\} \\circ \\mathsf{AC^0}$ circuits. These more\ngeneral results strengthen previous results of Viola [Vio06] and essentially\nstrengthen more recent results of Lovett and Srinivasan [LS11].\n  Our improved PRGs follow from improved correlation bounds, which are\ntransformed into PRGs via the Nisan--Wigderson \"hardness versus randomness\"\nparadigm [NW94]. The key to our improved correlation bounds is the use of a\nrecent powerful \\emph{multi-switching} lemma due to H{\\aa}stad [H{\\aa}s14].\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:12:00 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Servedio", "Rocco A.", ""], ["Tan", "Li-Yang", ""]]}, {"id": "1803.05380", "submitter": "Stasys Jukna", "authors": "Stasys Jukna and Hannes Seiwert", "title": "Greedy can beat pure dynamic programming", "comments": "The first structural claim of the Forest lemma (that the sets of\n  vertices in the connected components of all forests from one side must be the\n  same) in the previous version is just wrong. We now use entirely different\n  arguments to prove a slightly weaker version of the numerical claim of the\n  Forest lemma. The resulting lower bound is weaker, but still exponential", "journal-ref": "Inf. Proc. Letters 142 (2019) 90-95", "doi": "10.1016/j.ipl.2018.10.018", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamic programming algorithms for discrete 0-1 optimizationproblems are\n\"pure\" in that their recursion equations only use min/max and addition\noperations, and do not depend on actual input weights. The well-known greedy\nalgorithm of Kruskal solves the minimum weight spanning tree problem on\n$n$-vertex graphs using only $O(n^2\\log n)$ operations. We prove that any pure\nDP algorithm for this problem must perform $2^{\\Omega(\\sqrt{n})}$ operations.\nSince the greedy algorithm can also badly fail on some optimization problems,\neasily solvable by pure DP algorithms, our result shows that the computational\npowers of these two types of algorithms are incomparable.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:16:10 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 16:38:27 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jukna", "Stasys", ""], ["Seiwert", "Hannes", ""]]}, {"id": "1803.05498", "submitter": "Perrot K\\'evin", "authors": "Enrico Formenti, K\\'evin Perrot and Eric R\\'emila", "title": "Computational complexity of the avalanche problem on one dimensional\n  Kadanoff sandpiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove that the general avalanche problem AP is in NC, for\nthe Kadanoff sandpile model in one dimension, answering an open problem of\nFormenti, Goles and Martin in 2010. Thus adding one more item to the (slowly)\ngrowing list of dimension sensitive problems since in higher dimensions the\nproblem is P-complete (for monotone sandpiles).\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 20:16:30 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Formenti", "Enrico", ""], ["Perrot", "K\u00e9vin", ""], ["R\u00e9mila", "Eric", ""]]}, {"id": "1803.05718", "submitter": "Jon Lee", "authors": "Martin Kouteck\\'y, Jon Lee, Viswanath Nagarajan, Xiangkun Shen", "title": "Approximating Max-Cut under Graph-MSO Constraints", "comments": "arXiv admin note: text overlap with arXiv:1511.08152", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the max-cut and max-$k$-cut problems under graph-based\nconstraints. Our approach can handle any constraint specified using monadic\nsecond-order (MSO) logic on graphs of constant treewidth. We give a\n$\\frac{1}{2}$-approximation algorithm for this class of problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 13:07:07 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 23:19:55 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Kouteck\u00fd", "Martin", ""], ["Lee", "Jon", ""], ["Nagarajan", "Viswanath", ""], ["Shen", "Xiangkun", ""]]}, {"id": "1803.05933", "submitter": "Noam Solomon", "authors": "Chi-Ning Chou, Mrinal Kumar and Noam Solomon", "title": "Some Closure Results for Polynomial Factorization and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sequence of seminal results in the 80's, Kaltofen showed that the\ncomplexity class VP is closed under taking factors. A natural question in this\ncontext is to understand if other natural classes of multivariate polynomials,\nfor instance, arithmetic formulas, algebraic branching programs, bounded depth\narithmetic circuits or the class VNP, are closed under taking factors.\n  In this paper, we show that all factors of degree at most $\\log^a n$ of\npolynomials with poly(n) size depth $k$ circuits have poly(n) size circuits of\ndepth at most $O(k + a)$. This partially answers a question of\nShpilka-Yehudayoff and has applications to hardness-randomness tradeoffs for\nbounded depth arithmetic circuits. More precisely, this shows that a\nsuperpolynomial lower bound for bounded depth arithmetic circuits, for a family\nof explicit polynomials of degree poly$(\\log n)$ implies deterministic\nsub-exponential time algorithms for polynomial identity testing (PIT) for\nbounded depth arithmetic circuits. This is incomparable to a beautiful result\nof Dvir et al., where they showed that super-polynomial lower bounds for\nconstant depth arithmetic circuits for any explicit family of polynomials (of\npotentially high degree) implies sub-exponential time deterministic PIT for\nbounded depth circuits of bounded individual degree. Thus, we remove the\n\"bounded individual degree\" condition in [DSY09] at the cost of strengthening\nthe hardness assumption to hold for polynomials of low degree.\n  As direct applications of our techniques, we also show that the complexity\nclass VNP is closed under taking factors, thereby confirming a conjecture of\nB\\\"urgisser and get an alternate proof of the fact (first shown by Dutta et\nal.) that if a polynomial $Q$ of degree at most $d$ divides a polynomial $P$\ncomputable by a formula of size $s$, then $Q$ has a formula of size at most\npoly$(s, d^{\\log d}, deg(P))$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:20:42 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Kumar", "Mrinal", ""], ["Solomon", "Noam", ""]]}, {"id": "1803.06074", "submitter": "Haruka Mizuta", "authors": "Tesshu Hanaka and Takehiro Ito and Haruka Mizuta and Benjamin Moore\n  and Naomi Nishimura and Vijay Subramanya and Akira Suzuki and Krishna\n  Vaidyanathan", "title": "Reconfiguring spanning and induced subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph reconfiguration is a family of problems focusing on the reachability\nof the solution space in which feasible solutions are subgraphs, represented\neither as sets of vertices or sets of edges, satisfying a prescribed graph\nstructure property. Although there has been previous work that can be\ncategorized as subgraph reconfiguration, most of the related results appear\nunder the name of the property under consideration; for example, independent\nset, clique, and matching. In this paper, we systematically clarify the\ncomplexity status of subgraph reconfiguration with respect to graph structure\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 04:54:55 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Hanaka", "Tesshu", ""], ["Ito", "Takehiro", ""], ["Mizuta", "Haruka", ""], ["Moore", "Benjamin", ""], ["Nishimura", "Naomi", ""], ["Subramanya", "Vijay", ""], ["Suzuki", "Akira", ""], ["Vaidyanathan", "Krishna", ""]]}, {"id": "1803.06521", "submitter": "Sitan Chen", "authors": "Sitan Chen, Ankur Moitra", "title": "Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications", "comments": "62 pages; to appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:26:04 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 17:43:48 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Chen", "Sitan", ""], ["Moitra", "Ankur", ""]]}, {"id": "1803.06636", "submitter": "Igor Pak", "authors": "Igor Pak", "title": "Complexity problems in enumerative combinatorics", "comments": "31 pages; an expanded version of the ICM 2018 paper (Section 4 added,\n  refs expanded)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM math.HO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a broad survey of recent results in Enumerative Combinatorics and\ntheir complexity aspects.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 10:31:54 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 20:40:46 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pak", "Igor", ""]]}, {"id": "1803.06800", "submitter": "Joshua Grochow", "authors": "Joshua A. Grochow and Jamie Tucker-Foltz", "title": "Computational topology and the Unique Games Conjecture", "comments": "Full version of a conference paper in 34th International Symposium on\n  Computational Geometry (SoCG 2018)", "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2018.43", "report-no": null, "categories": "cs.CC cs.CG cs.DM math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covering spaces of graphs have long been useful for studying expanders (as\n\"graph lifts\") and unique games (as the \"label-extended graph\"). In this paper\nwe advocate for the thesis that there is a much deeper relationship between\ncomputational topology and the Unique Games Conjecture. Our starting point is\nLinial's 2005 observation that the only known problems whose inapproximability\nis equivalent to the Unique Games Conjecture - Unique Games and Max-2Lin - are\ninstances of Maximum Section of a Covering Space on graphs. We then observe\nthat the reduction between these two problems (Khot-Kindler-Mossel-O'Donnell,\nFOCS 2004; SICOMP, 2007) gives a well-defined map of covering spaces. We\nfurther prove that inapproximability for Maximum Section of a Covering Space on\n(cell decompositions of) closed 2-manifolds is also equivalent to the Unique\nGames Conjecture. This gives the first new \"Unique Games-complete\" problem in\nover a decade.\n  Our results partially settle an open question of Chen and Freedman (SODA\n2010; Disc. Comput. Geom., 2011) from computational topology, by showing that\ntheir question is almost equivalent to the Unique Games Conjecture. (The main\ndifference is that they ask for inapproximability over\n$\\mathbb{Z}/2\\mathbb{Z}$, and we show Unique Games-completeness over\n$\\mathbb{Z}/k\\mathbb{Z}$ for large $k$.) This equivalence comes from the fact\nthat when the structure group $G$ of the covering space is Abelian - or more\ngenerally for principal $G$-bundles - Maximum Section of a $G$-Covering Space\nis the same as the well-studied problem of 1-Homology Localization.\n  Although our most technically demanding result is an application of Unique\nGames to computational topology, we hope that our observations on the\ntopological nature of the Unique Games Conjecture will lead to applications of\nalgebraic topology to the Unique Games Conjecture in the future.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 04:15:41 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Grochow", "Joshua A.", ""], ["Tucker-Foltz", "Jamie", ""]]}, {"id": "1803.06878", "submitter": "Tom\\'a\\v{s} Masa\\v{r}\\'ik", "authors": "Du\\v{s}an Knop, Tom\\'a\\v{s} Masa\\v{r}\\'ik, Tom\\'a\\v{s} Toufar", "title": "Parameterized Complexity of Fair Vertex Evaluation Problems", "comments": "25 pages, 5 figures, presented at MFCS 2019", "journal-ref": null, "doi": "10.4230/LIPIcs.MFCS.2019.33", "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 11:49:02 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 21:53:20 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 17:10:55 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 10:56:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Knop", "Du\u0161an", ""], ["Masa\u0159\u00edk", "Tom\u00e1\u0161", ""], ["Toufar", "Tom\u00e1\u0161", ""]]}, {"id": "1803.07369", "submitter": "Ivan Zapreev", "authors": "Ivan S. Zapreev, Cees Verdier, Manuel Mazo Jr", "title": "Optimal Symbolic Controllers Determinization for BDD storage", "comments": "Supported by STW-EW as a part of the CADUSY project #13852. The short\n  version of this article has been accepted to ADHS'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controller synthesis techniques based on symbolic abstractions appeal by\nproducing correct-by-design controllers, under intricate behavioural\nconstraints. Yet, being relations between abstract states and inputs, such\ncontrollers are immense in size, which makes them futile for em- bedded\nplatforms. Control-synthesis tools such as PESSOA, SCOTS, and CoSyMA tackle the\nproblem by storing controllers as binary decision di- agrams (BDDs). However,\ndue to redundantly keeping multiple inputs per-state, the resulting controllers\nare still too large. In this work, we first show that choosing an optimal\ncontroller determinization is an NP- complete problem. Further, we consider the\npreviously known controller determinization technique and discuss its\nweaknesses. We suggest several new approaches to the problem, based on greedy\nalgorithms, symbolic regression, and (muli-terminal) BDDs. Finally, we\nempirically compare the techniques and show that some of the new algorithms can\nproduce up to 85% smaller controllers than those obtained with the previous\ntechnique.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 11:13:43 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Zapreev", "Ivan S.", ""], ["Verdier", "Cees", ""], ["Mazo", "Manuel", "Jr"]]}, {"id": "1803.07374", "submitter": "Filip Hanzely", "authors": "Filip Hanzely and Peter Richt\\'arik", "title": "Fastest Rates for Stochastic Mirror Descent Methods", "comments": "45 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative smoothness - a notion introduced by Birnbaum et al. (2011) and\nrediscovered by Bauschke et al. (2016) and Lu et al. (2016) - generalizes the\nstandard notion of smoothness typically used in the analysis of gradient type\nmethods. In this work we are taking ideas from well studied field of stochastic\nconvex optimization and using them in order to obtain faster algorithms for\nminimizing relatively smooth functions. We propose and analyze two new\nalgorithms: Relative Randomized Coordinate Descent (relRCD) and Relative\nStochastic Gradient Descent (relSGD), both generalizing famous algorithms in\nthe standard smooth setting. The methods we propose can be in fact seen as a\nparticular instances of stochastic mirror descent algorithms. One of them,\nrelRCD corresponds to the first stochastic variant of mirror descent algorithm\nwith linear convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 11:27:01 GMT"}], "update_date": "2018-03-25", "authors_parsed": [["Hanzely", "Filip", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1803.07376", "submitter": "Bernd Schuh", "authors": "Bernd Schuh", "title": "Sub-exponential Upper Bound for #XSAT of some CNF Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an upper bound on the number of models for exact satisfiability\n(XSAT) of arbitrary CNF formulas F. The bound can be calculated solely from the\ndistribution of positive and negated literals in the formula. For certain\nsubsets of CNF instances the new bound can be computed in sub-exponential time,\nnamely in at most O(exp(sqrt(n))) , where n is the number of variables of F. A\nwider class of SAT problems beyond XSAT is defined to which the method can be\nextended.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 11:30:48 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Schuh", "Bernd", ""]]}, {"id": "1803.07465", "submitter": "Dmitriy Zhuk", "authors": "Dmitriy Zhuk", "title": "A modification of the CSP algorithm for infinite languages", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.01914", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Satisfaction Problem on finite sets is known to be NP-complete in\ngeneral but certain restrictions on the constraint language can ensure\ntractability. It was proved that if a constraint language has a weak near\nunanimity polymorphism then the corresponding constraint satisfaction problem\nis tractable, otherwise it is NP-complete. In the paper we present a\nmodification of the algorithm that works in polynomial time even for infinite\nconstraint languages.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 21:28:47 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 10:59:13 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhuk", "Dmitriy", ""]]}, {"id": "1803.07683", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi, Jeffrey Zhang", "title": "On the Complexity of Testing Attainment of the Optimal Value in\n  Nonlinear Optimization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that unless P=NP, there exists no polynomial time (or even\npseudo-polynomial time) algorithm that can test whether the optimal value of a\nnonlinear optimization problem where the objective and constraints are given by\nlow-degree polynomials is attained. If the degrees of these polynomials are\nfixed, our results along with previously-known \"Frank-Wolfe type\" theorems\nimply that exactly one of two cases can occur: either the optimal value is\nattained on every instance, or it is strongly NP-hard to distinguish attainment\nfrom non-attainment. We also show that testing for some well-known sufficient\nconditions for attainment of the optimal value, such as coercivity of the\nobjective function and closedness and boundedness of the feasible set, is\nstrongly NP-hard. As a byproduct, our proofs imply that testing the Archimedean\nproperty of a quadratic module is strongly NP-hard, a property that is of\nindependent interest to the convergence of the Lasserre hierarchy. Finally, we\ngive semidefinite programming (SDP)-based sufficient conditions for attainment\nof the optimal value, in particular a new characterization of coercive\npolynomials that lends itself to an SDP hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 22:54:29 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 03:13:02 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Zhang", "Jeffrey", ""]]}, {"id": "1803.09370", "submitter": "Saket Saurabh", "authors": "Sushmita Gupta, Pranabendu Misra, Saket Saurabh and Meirav Zehavi", "title": "Popular Matching in Roommates Setting is NP-hard", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 23:53:25 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gupta", "Sushmita", ""], ["Misra", "Pranabendu", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1803.09717", "submitter": "Karthik C. S.", "authors": "Arnab Bhattacharyya, Suprovat Ghoshal, Karthik C. S., and Pasin\n  Manurangsi", "title": "Parameterized Intractability of Even Set and Shortest Vector Problem\n  from Gap-ETH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Even Set problem is a parameterized variant of the Minimum Distance\nProblem of linear codes over $\\mathbb F_2$, which can be stated as follows:\ngiven a generator matrix $\\mathbf A$ and an integer $k$, determine whether the\ncode generated by $\\mathbf A$ has distance at most $k$. Here, $k$ is the\nparameter of the problem. The question of whether $k$-Even Set is fixed\nparameter tractable (FPT) has been repeatedly raised in literature and has\nearned its place in Downey and Fellows' book (2013) as one of the \"most\ninfamous\" open problems in the field of Parameterized Complexity.\n  In this work, we show that $k$-Even Set does not admit FPT algorithms under\nthe (randomized) Gap Exponential Time Hypothesis (Gap-ETH) [Dinur'16,\nManurangsi-Raghavendra'16]. In fact, our result rules out not only exact FPT\nalgorithms, but also any constant factor FPT approximation algorithms for the\nproblem. Furthermore, our result holds even under the following weaker\nassumption, which is also known as the Parameterized Inapproximability\nHypothesis (PIH) [Lokshtanov et al.'17]: no (randomized) FPT algorithm can\ndistinguish a satisfiable 2CSP instance from one which is only\n$0.99$-satisfiable (where the parameter is the number of variables).\n  We also consider the parameterized $k$-Shortest Vector Problem (SVP), in\nwhich we are given a lattice whose basis vectors are integral and an integer\n$k$, and the goal is to determine whether the norm of the shortest vector (in\nthe $\\ell_p$ norm for some fixed $p$) is at most $k$. Similar to $k$-Even Set,\nthis problem is also a long-standing open problem in the field of Parameterized\nComplexity. We show that, for any $p > 1$, $k$-SVP is hard to approximate (in\nFPT time) to some constant factor, assuming PIH. Furthermore, for the case of\n$p = 2$, the inapproximability factor can be amplified to any constant.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:13:43 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Ghoshal", "Suprovat", ""], ["S.", "Karthik C.", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "1803.09941", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Zirui Zhou", "title": "Iteration-complexity of first-order augmented Lagrangian methods for\n  convex conic programming", "comments": "Needs substantial revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a class of convex conic programming. In particular,\nwe propose an inexact augmented Lagrangian (I-AL) method for solving this\nproblem, in which the augmented Lagrangian subproblems are solved approximately\nby a variant of Nesterov's optimal first-order method. We show that the total\nnumber of first-order iterations of the proposed I-AL method for computing an\n$\\epsilon$-KKT solution is at most $\\mathcal{O}(\\epsilon^{-7/4})$. We also\npropose a modified I-AL method and show that it has an improved\niteration-complexity $\\mathcal{O}(\\epsilon^{-1}\\log\\epsilon^{-1})$, which is so\nfar the lowest complexity bound among all first-order I-AL type of methods for\ncomputing an $\\epsilon$-KKT solution. Our complexity analysis of the I-AL\nmethods is mainly based on an analysis on inexact proximal point algorithm\n(PPA) and the link between the I-AL methods and inexact PPA. It is\nsubstantially different from the existing complexity analyses of the\nfirst-order I-AL methods in the literature, which typically regard the I-AL\nmethods as an inexact dual gradient method. Compared to the mostly related I-AL\nmethods \\cite{Lan16}, our modified I-AL method is more practically efficient\nand also applicable to a broader class of problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:48:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 03:51:08 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhou", "Zirui", ""]]}, {"id": "1803.09947", "submitter": "Ryuhei Mori", "authors": "Ryuhei Mori", "title": "Periodic Fourier representation of Boolean functions", "comments": "18 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a new type of Fourier-like representation of\nBoolean function $f\\colon\\{+1,-1\\}^n\\to\\{+1,-1\\}$ \\[ f(x) =\n\\cos\\left(\\pi\\sum_{S\\subseteq[n]}\\phi_S \\prod_{i\\in S} x_i\\right). \\] This\nrepresentation, which we call the periodic Fourier representation, of Boolean\nfunction is closely related to a certain type of multipartite Bell inequalities\nand non-adaptive measurement-based quantum computation with linear\nside-processing ($\\mathrm{NMQC}_\\oplus$). The minimum number of non-zero\ncoefficients in the above representation, which we call the periodic Fourier\nsparsity, is equal to the required number of qubits for the exact computation\nof $f$ by $\\mathrm{NMQC}_\\oplus$. Periodic Fourier representations are not\nunique, and can be directly obtained both from the Fourier representation and\nthe $\\mathbb{F}_2$-polynomial representation. In this work, we first show that\nBoolean functions related to $\\mathbb{Z}/4\\mathbb{Z}$-polynomial have small\nperiodic Fourier sparsities. Second, we show that the periodic Fourier sparsity\nis at least $2^{\\mathrm{deg}_{\\mathbb{F}_2}(f)}-1$, which means that\n$\\mathrm{NMQC}_\\oplus$ efficiently computes a Boolean function $f$ if and only\nif $\\mathbb{F}_2$-degree of $f$ is small. Furthermore, we show that any\nsymmetric Boolean function, e.g., $\\mathsf{AND}_n$, $\\mathsf{Mod}^3_n$,\n$\\mathsf{Maj}_n$, etc, can be exactly computed by depth-2\n$\\mathrm{NMQC}_\\oplus$ using a polynomial number of qubits, that implies\nexponential gaps between $\\mathrm{NMQC}_\\oplus$ and depth-2\n$\\mathrm{NMQC}_\\oplus$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:10:44 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 04:00:49 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 09:27:21 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Mori", "Ryuhei", ""]]}, {"id": "1803.09954", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii", "title": "Quantum speedup in stoquastic adiabatic quantum computation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cond-mat.stat-mech cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation provides exponential speedup for solving certain\nmathematical problems against classical computers. Motivated by current rapid\nexperimental progress on quantum computing devices, various models of quantum\ncomputation have been investigated to show quantum computational supremacy. At\na commercial side, quantum annealing machine realizes the quantum Ising model\nwith a transverse field and heuristically solves combinatorial optimization\nproblems. The computational power of this machine is closely related to\nadiabatic quantum computation (AQC) with a restricted type of Hamiltonians,\nnamely stoquastic Hamiltonians, and has been thought to be relatively less\npowerful compared to universal quantum computers. Little is known about\ncomputational quantum speedup nor advantage in AQC with stoquastic\nHamiltonians. Here we characterize computational capability of AQC with\nstoquastic Hamiltonians, which we call stoqAQC. We construct a concrete stoqAQC\nmodel, whose lowest energy gap is lower bounded polynomially, and hence the\nfinal state can be obtained in polynomial time. Then we show that it can\nsimulate universal quantum computation if adaptive single-qubit measurements in\nnon-standard bases are allowed on the final state. Even if the measurements are\nrestricted to non-adaptive measurements to respect the robustness of AQC, the\nproposed model exhibits quantum computational supremacy; classical simulation\nis impossible under complexity theoretical conjectures. Moreover, it is found\nthat such a stoqAQC model can simulate Shor's algorithm and solve the factoring\nproblem in polynomial time. We also propose how to overcome the measurement\nimperfections via quantum error correction within the stoqAQC model and also an\nexperimentally feasible verification scheme to test whether or not stoqAQC is\ndone faithfully.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:28:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Fujii", "Keisuke", ""]]}, {"id": "1803.09963", "submitter": "Wenxia Guo", "authors": "Wenxia Guo, Jin Wang, Majun He, Xiaoqin Ren, Wenhong Tian, Qingxian\n  Wang", "title": "An Efficient Method to Transform SAT problems to Binary Integer Linear\n  Programming Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational complexity theory, a decision problem is NP-complete when it\nis both in NP and NP-hard. Although a solution to a NP-complete can be verified\nquickly, there is no known algorithm to solve it in polynomial time. There\nexists a method to reduce a SAT (Satifiability) problem to Subset Sum Problem\n(SSP) in the literature, however, it can only be applied to small or medium\nsize problems. Our study is to find an efficient method to transform a SAT\nproblem to a mixed integer linear programming problem in larger size. Observing\nthe feature of variable-clauses constraints in SAT, we apply linear inequality\nmodel (LIM) to the problem and propose a method called LIMSAT. The new method\ncan work efficiently for very large size problem with thousands of variables\nand clauses in SAT tested using up-to-date benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:50:35 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Guo", "Wenxia", ""], ["Wang", "Jin", ""], ["He", "Majun", ""], ["Ren", "Xiaoqin", ""], ["Tian", "Wenhong", ""], ["Wang", "Qingxian", ""]]}, {"id": "1803.10184", "submitter": "Hamid Hoorfar", "authors": "Hamid Hoorfar and Alireza Bagheri", "title": "A New Optimal Algorithm for Computing the Visibility Area of a simple\n  Polygon from a Viewpoint", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple polygon $ \\mathcal {P} $ of $ n $ vertices in the Plane. We\nstudy the problem of computing the visibility area from a given viewpoint $ q $\ninside $ \\mathcal {P} $ where only sub-linear variables are allowed for working\nspace. Without any memory-constrained, this problem was previously solved in $\nO(n) $-time and $ O(n) $-variables space. In a newer research, the visibility\narea of a point be computed in $ O(n) $-time, using $ O(\\sqrt{n}) $ variables\nfor working space. In this paper, we present an optimal-time algorithm, using $\nO(c/\\log n) $ variables space for computing visibility area, where $ c<n $ is\nthe number of critical vertices. We keep the algorithm in the linear-time and\nreduce space as much as possible.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 17:07:26 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 15:07:44 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 17:28:27 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Hoorfar", "Hamid", ""], ["Bagheri", "Alireza", ""]]}, {"id": "1803.10267", "submitter": "Xiang Huang", "authors": "Xiang Huang, Titus H. Klinge, James I. Lathrop, Xiaoyuan Li, Jack H.\n  Lutz", "title": "Real-Time Computability of Real Numbers by Chemical Reaction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the class of real numbers that are computed in real time by\ndeterministic chemical reaction networks that are integral in the sense that\nall their reaction rate constants are positive integers. We say that such a\nreaction network computes a real number $\\alpha$ in real time if it has a\ndesignated species $X$ such that, when all species concentrations are set to\nzero at time $t = 0$, the concentration $x(t)$ of $X$ is within $2^{-t}$ of\n$|{\\alpha}|$ at all times $t \\ge 1$, and the concentrations of all other\nspecies are bounded. We show that every algebraic number and some\ntranscendental numbers are real time computable by chemical reaction networks\nin this sense. We discuss possible implications of this for the 1965\nHartmanis-Stearns conjecture, which says that no irrational algebraic number is\nreal time computable by a Turing machine.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 18:42:33 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Huang", "Xiang", ""], ["Klinge", "Titus H.", ""], ["Lathrop", "James I.", ""], ["Li", "Xiaoyuan", ""], ["Lutz", "Jack H.", ""]]}, {"id": "1803.10574", "submitter": "Serge Burckel Riviere", "authors": "Dr Serge Burckel", "title": "Non-Interlaced SAT is in P", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the NP-Complete problem SAT and the geometry of its instances.\nFor a particular type that we call {\\it non-interlaced formulas}, we propose a\npolynomial time algorithm for their resolution using graphs and matrices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:34:00 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 16:01:26 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Burckel", "Dr Serge", ""]]}, {"id": "1803.11134", "submitter": "Thorsten Wissmann", "authors": "Berit Gru{\\ss}ien", "title": "Capturing Polynomial Time using Modular Decomposition", "comments": "38 pages, 10 Figures. A preliminary version of this article appeared\n  in the Proceedings of the 32nd Annual ACM/IEEE Symposium on Logic in Computer\n  Science (LICS '17)", "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (March 5,\n  2019) lmcs:5256", "doi": "10.23638/LMCS-15(1:24)2019", "report-no": null, "categories": "cs.CC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The question of whether there is a logic that captures polynomial time is one\nof the main open problems in descriptive complexity theory and database theory.\nIn 2010 Grohe showed that fixed point logic with counting captures polynomial\ntime on all classes of graphs with excluded minors. We now consider classes of\ngraphs with excluded induced subgraphs. For such graph classes, an effective\ngraph decomposition, called modular decomposition, was introduced by Gallai in\n1976. The graphs that are non-decomposable with respect to modular\ndecomposition are called prime. We present a tool, the Modular Decomposition\nTheorem, that reduces (definable) canonization of a graph class C to\n(definable) canonization of the class of prime graphs of C that are colored\nwith binary relations on a linearly ordered set. By an application of the\nModular Decomposition Theorem, we show that fixed point logic with counting\ncaptures polynomial time on the class of permutation graphs. Within the proof\nof the Modular Decomposition Theorem, we show that the modular decomposition of\na graph is definable in symmetric transitive closure logic with counting. We\nobtain that the modular decomposition tree is computable in logarithmic space.\nIt follows that cograph recognition and cograph canonization is computable in\nlogarithmic space.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 16:19:14 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 17:49:48 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 14:11:28 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Gru\u00dfien", "Berit", ""]]}]