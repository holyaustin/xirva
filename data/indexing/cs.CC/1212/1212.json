[{"id": "1212.0025", "submitter": "Scott Aaronson", "authors": "Scott Aaronson and Travis Hance", "title": "Generalizing and Derandomizing Gurvits's Approximation Algorithm for the\n  Permanent", "comments": "19 pages, 1 figure, minor additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Around 2002, Leonid Gurvits gave a striking randomized algorithm to\napproximate the permanent of an n*n matrix A. The algorithm runs in\nO(n^2/eps^2) time, and approximates Per(A) to within eps*||A||^n additive\nerror. A major advantage of Gurvits's algorithm is that it works for arbitrary\nmatrices, not just for nonnegative matrices. This makes it highly relevant to\nquantum optics, where the permanents of bounded-norm complex matrices play a\ncentral role. Indeed, the existence of Gurvits's algorithm is why, in their\nrecent work on the hardness of quantum optics, Aaronson and Arkhipov (AA) had\nto talk about sampling problems rather than estimation problems.\n  In this paper, we improve Gurvits's algorithm in two ways. First, using an\nidea from quantum optics, we generalize the algorithm so that it yields a\nbetter approximation when the matrix A has either repeated rows or repeated\ncolumns. Translating back to quantum optics, this lets us classically estimate\nthe probability of any outcome of an AA-type experiment---even an outcome\ninvolving multiple photons \"bunched\" in the same mode---at least as well as\nthat probability can be estimated by the experiment itself. (This does not, of\ncourse, let us solve the AA sampling problem.) It also yields a general upper\nbound on the probabilities of \"bunched\" outcomes, which resolves a conjecture\nof Gurvits and might be of independent physical interest.\n  Second, we use eps-biased sets to derandomize Gurvits's algorithm, in the\nspecial case where the matrix A is nonnegative. More interestingly, we\ngeneralize the notion of eps-biased sets to the complex numbers, construct\n\"complex eps-biased sets,\" then use those sets to derandomize even our\ngeneralization of Gurvits's algorithm to the multirow/multicolumn case (again\nfor nonnegative A). Whether Gurvits's algorithm can be derandomized for general\nA remains an outstanding problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 22:10:11 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 23:06:12 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Aaronson", "Scott", ""], ["Hance", "Travis", ""]]}, {"id": "1212.0191", "submitter": "Koji Kobayashi", "authors": "Koji Kobayashi", "title": "Finite and infinite basis in P and NP", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provide new approach to solve P vs NP problem by using\ncardinality of bases function. About NP-Complete problems, we can divide to\ninfinite disjunction of P-Complete problems. These P-Complete problems are\nindependent of each other in disjunction. That is, NP-Complete problem is in\ninfinite dimension function space that bases are P-Complete. The other hand,\nany P-Complete problem have at most a finite number of P-Complete basis. The\nreason is that each P problems have at most finite number of Least fixed point\noperator. Therefore, we cannot describe NP-Complete problems in P. We can also\nprove this result from incompleteness of P.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 07:32:43 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 16:37:53 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 15:11:46 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Kobayashi", "Koji", ""]]}, {"id": "1212.0703", "submitter": "Kurt Mehlhorn", "authors": "Tomasz Jurkiewicz, Kurt Mehlhorn", "title": "The Cost of Address Translation", "comments": "A extended abstract of this paper was published in the proceedings of\n  ALENEX13, New Orleans, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computers are not random access machines (RAMs). They have a memory\nhierarchy, multiple cores, and virtual memory. In this paper, we address the\ncomputational cost of address translation in virtual memory. Starting point for\nour work is the observation that the analysis of some simple algorithms (random\nscan of an array, binary search, heapsort) in either the RAM model or the EM\nmodel (external memory model) does not correctly predict growth rates of actual\nrunning times. We propose the VAT model (virtual address translation) to\naccount for the cost of address translations and analyze the algorithms\nmentioned above and others in the model. The predictions agree with the\nmeasurements. We also analyze the VAT-cost of cache-oblivious algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 12:55:21 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 14:16:01 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Jurkiewicz", "Tomasz", ""], ["Mehlhorn", "Kurt", ""]]}, {"id": "1212.0752", "submitter": "Bundit Laekhanukit", "authors": "Bundit Laekhanukit", "title": "Parameters of Two-Prover-One-Round Game and The Hardness of Connectivity\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing parameters of Two-Prover-One-Round Game (2P1R) is an important\ntask in PCPs literature as it would imply a smaller PCP with the same or\nstronger soundness. While this is a basic question in PCPs community, the\nconnection between the parameters of PCPs and hardness of approximations is\nsometime obscure to approximation algorithm community. In this paper, we\ninvestigate the connection between the parameters of 2P1R and the hardness of\napproximating the class of so-called connectivity problems, which includes as\nsubclasses the survivable network design and (multi)cut problems. Based on\nrecent development on 2P1R by Chan (ECCC 2011) and several techniques in PCPs\nliterature, we improve hardness results of some connectivity problems that are\nin the form $k^\\sigma$, for some (very) small constant $\\sigma>0$, to hardness\nresults of the form $k^c$ for some explicit constant $c$, where $k$ is a\nconnectivity parameter. In addition, we show how to convert these hardness into\nhardness results of the form $D^{c'}$, where $D$ is the number of demand pairs\n(or the number of terminals).\n  Thus, we give improved hardness results of k^{1/2-\\epsilon} and\nk^{1/10-\\epsilon} for the root $k$-connectivity problem on directed and\nundirected graphs, k^{1/6-\\epsilon} for the vertex-connectivity survivable\nnetwork design problem on undirected graphs, and k^{1/6-\\epsilon} for the\nvertex-connectivity $k$-route cut problem on undirected graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 15:10:41 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 01:22:32 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Laekhanukit", "Bundit", ""]]}, {"id": "1212.1789", "submitter": "Jan Krajicek", "authors": "Jan Krajicek", "title": "On the computational complexity of finding hard tautologies", "comments": null, "journal-ref": "Bulletin of the London Mathematical Society, 46(1), (2014),\n  pp.111-125", "doi": "10.1112/blms/bdt071", "report-no": null, "categories": "math.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known (cf. K.-Pudl\\'ak 1989) that a polynomial time algorithm\nfinding tautologies hard for a propositional proof system $P$ exists iff $P$ is\nnot optimal. Such an algorithm takes $1^{(k)}$ and outputs a tautology $\\tau_k$\nof size at least $k$ such that $P$ is not p-bounded on the set of all\n$\\tau_k$'s.\n  We consider two more general search problems involving finding a hard\nformula, {\\bf Cert} and {\\bf Find}, motivated by two hypothetical situations:\nthat one can prove that $\\np \\neq co\\np$ and that no optimal proof system\nexists. In {\\bf Cert} one is asked to find a witness that a given\nnon-deterministic circuit with $k$ inputs does not define $TAUT \\cap \\kk$. In\n{\\bf Find}, given $1^{(k)}$ and a tautology $\\alpha$ of size at most $k^{c_0}$,\none should output a size $k$ tautology $\\beta$ that has no size $k^{c_1}$\n$P$-proof from substitution instances of $\\alpha$.\n  We shall prove, assuming the existence of an exponentially hard one-way\npermutation, that {\\bf Cert} cannot be solved by a time $2^{O(k)}$ algorithm.\nUsing a stronger hypothesis about the proof complexity of Nisan-Wigderson\ngenerator we show that both problems {\\bf Cert} and {\\bf Find} are actually\nonly partially defined for infinitely many $k$ (i.e. there are inputs\ncorresponding to $k$ for which the problem has no solution). The results are\nbased on interpreting the Nisan-Wigderson generator as a proof system.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 13:23:20 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 08:42:35 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Krajicek", "Jan", ""]]}, {"id": "1212.1881", "submitter": "Georg Gottlob", "authors": "Georg Gottlob", "title": "Deciding Monotone Duality and Identifying Frequent Itemsets in Quadratic\n  Logspace", "comments": "Preprint of a paper which appeared in: Proceedings of the 32nd ACM\n  SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS 2013,\n  New York, NY,USA, June 22-27,2013, pp.25-36", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monotone duality problem is defined as follows: Given two monotone\nformulas f and g in iredundant DNF, decide whether f and g are dual. This\nproblem is the same as duality testing for hypergraphs, that is, checking\nwhether a hypergraph H consists of precisely all minimal transversals of a\nsimple hypergraph G. By exploiting a recent problem-decomposition method by\nBoros and Makino (ICALP 2009), we show that duality testing for hypergraphs,\nand thus for monotone DNFs, is feasible in DSPACE[log^2 n], i.e., in quadratic\nlogspace. As the monotone duality problem is equivalent to a number of problems\nin the areas of databases, data mining, and knowledge discovery, the results\npresented here yield new complexity results for those problems, too. For\nexample, it follows from our results that whenever for a Boolean-valued\nrelation (whose attributes represent items), a number of maximal frequent\nitemsets and a number of minimal infrequent itemsets are known, then it can be\ndecided in quadratic logspace whether there exist additional frequent or\ninfrequent itemsets.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 11:36:09 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 18:16:02 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2013 16:55:31 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Gottlob", "Georg", ""]]}, {"id": "1212.1891", "submitter": "Ryan Williams", "authors": "Ryan Williams", "title": "Natural Proofs Versus Derandomization", "comments": "32 pages, major revision for special issue of STOC'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study connections between Natural Proofs, derandomization, and the problem\nof proving \"weak\" circuit lower bounds such as ${\\sf NEXP} \\not\\subset {\\sf\nTC^0}$. Natural Proofs have three properties: they are constructive (an\nefficient algorithm $A$ is embedded in them), have largeness ($A$ accepts a\nlarge fraction of strings), and are useful ($A$ rejects all strings which are\ntruth tables of small circuits). Strong circuit lower bounds that are\n\"naturalizing\" would contradict present cryptographic understanding, yet the\nvast majority of known circuit lower bound proofs are naturalizing. So it is\nimperative to understand how to pursue un-Natural Proofs. Some heuristic\narguments say constructivity should be circumventable: largeness is inherent in\nmany proof techniques, and it is probably our presently weak techniques that\nyield constructivity. We prove:\n  $\\bullet$ Constructivity is unavoidable, even for $\\sf NEXP$ lower bounds.\nInformally, we prove for all \"typical\" non-uniform circuit classes ${\\cal C}$,\n${\\sf NEXP} \\not\\subset {\\cal C}$ if and only if there is a polynomial-time\nalgorithm distinguishing some function from all functions computable by ${\\cal\nC}$-circuits. Hence ${\\sf NEXP} \\not\\subset {\\cal C}$ is equivalent to\nexhibiting a constructive property useful against ${\\cal C}$.\n  $\\bullet$ There are no $\\sf P$-natural properties useful against ${\\cal C}$\nif and only if randomized exponential time can be \"derandomized\" using truth\ntables of circuits from ${\\cal C}$ as random seeds. Therefore the task of\nproving there are no $\\sf P$-natural properties is inherently a derandomization\nproblem, weaker than but implied by the existence of strong pseudorandom\nfunctions.\n  These characterizations are applied to yield several new results, including\nimproved ${\\sf ACC}^0$ lower bounds and new unconditional derandomizations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 14:20:18 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 03:30:37 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 21:45:22 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Williams", "Ryan", ""]]}, {"id": "1212.1941", "submitter": "Vladimir Nikishkin", "authors": "Vladimir Nikishkin", "title": "Amortized communication complexity of an equality predicate", "comments": "12 pages, beta version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We study the communication complexity of a direct sum of independent copies\nof the equality predicate. We prove that the probabilistic communication\ncomplexity of this problem is equal to O(N); computational complexity of the\nproposed protocol is polynomial in size of inputs. Our protocol improves the\nresult achieved in 1995(Feder, Kushilevitz, Naor, Nisan). Our construction is\nbased on two techniques: Nisan's pseudorandom generator (1992) and Smith's\nstring synchronization algorithm (2007).\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 00:13:19 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2012 18:11:44 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Nikishkin", "Vladimir", ""]]}, {"id": "1212.1942", "submitter": "Sumedha", "authors": "Sumedha, Supriya Krishnamurthy and Sharmistha Sahoo", "title": "Balanced K-SAT and Biased random K-SAT on trees", "comments": "22 pages, 7 figures", "journal-ref": "Phys. Rev. E 87, 042130 (2013)", "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and solve some variations of the random K-satisfiability problem -\nbalanced K-SAT and biased random K-SAT - on a regular tree, using techniques we\nhave developed earlier(arXiv:1110.2065). In both these problems, as well as\nvariations of these that we have looked at, we find that the SAT-UNSAT\ntransition obtained on the Bethe lattice matches the exact threshold for the\nsame model on a random graph for K=2 and is very close to the numerical value\nobtained for K=3. For higher K it deviates from the numerical estimates of the\nsolvability threshold on random graphs, but is very close to the dynamical\n1-RSB threshold as obtained from the first non-trivial fixed point of the\nsurvey propagation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 00:20:45 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Sumedha", "", ""], ["Krishnamurthy", "Supriya", ""], ["Sahoo", "Sharmistha", ""]]}, {"id": "1212.2284", "submitter": "Tyson Williams", "authors": "Heng Guo and Tyson Williams", "title": "The Complexity of Planar Boolean #CSP with Complex Weights", "comments": "38 pages, 12 figures, preliminary version appeared at ICALP 2013.\n  arXiv admin note: text overlap with arXiv:1207.2354, arXiv:1008.0683 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a complexity dichotomy theorem for symmetric complex-weighted\nBoolean #CSP when the constraint graph of the input must be planar. The\nproblems that are #P-hard over general graphs but tractable over planar graphs\nare precisely those with a holographic reduction to matchgates. This\ngeneralizes a theorem of Cai, Lu, and Xia for the case of real weights. We also\nobtain a dichotomy theorem for a symmetric arity 4 signature with complex\nweights in the planar Holant framework, which we use in the proof of our #CSP\ndichotomy. In particular, we reduce the problem of evaluating the Tutte\npolynomial of a planar graph at the point (3,3) to counting the number of\nEulerian orientations over planar 4-regular graphs to show the latter is\n#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our\nproof techniques combine new ideas with refinements and extensions of existing\ntechniques. These include planar pairings, the recursive unary construction,\nthe anti-gadget technique, and pinning in the Hadamard basis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 03:05:01 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2013 15:30:43 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Guo", "Heng", ""], ["Williams", "Tyson", ""]]}, {"id": "1212.2549", "submitter": "Thatchaphol Saranurak", "authors": "Thatchaphol Saranurak, Gorav Jindal", "title": "Subtraction makes computing integers faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show some facts regarding the question whether, for any number $n$, the\nlength of the shortest Addition Multiplications Chain (AMC) computing $n$ is\npolynomial in the length of the shortest division-free Straight Line Program\n(SLP) that computes $n$.\n  If the answer to this question is \"yes\", then we can show a stronger upper\nbound for $\\mathrm{PosSLP}$, the important problem which essentially captures\nthe notion of efficient computation over the reals. If the answer is \"no\", then\nthis would demonstrate how subtraction helps generating integers\nsuper-polynomially faster, given that addition and multiplication can be done\nin unit time.\n  In this paper, we show that, for almost all numbers, AMCs and SLPs need same\nasymptotic length for computation. However, for one specific form of numbers,\nSLPs are strictly more powerful than AMCs by at least one step of computation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 17:36:29 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Saranurak", "Thatchaphol", ""], ["Jindal", "Gorav", ""]]}, {"id": "1212.3217", "submitter": "Joost Joosten", "authors": "J. J. Joosten", "title": "Complexity fits the fittest", "comments": "arXiv admin note: substantial text overlap with arXiv:1211.1878", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we shall relate computational complexity to the principle of\nnatural selection. We shall do this by giving a philosophical account of\ncomplexity versus universality. It seems sustainable to equate universal\nsystems to complex systems or at least to potentially complex systems. Post's\nproblem on the existence of (natural) intermediate degrees (between decidable\nand universal RE) then finds its analog in the Principle of Computional\nEquivalence (PCE). In this paper we address possible driving forces --if any--\nbehind PCE. Both the natural aspects as well as the cognitive ones are\ninvestigated. We postulate a principle GNS that we call the Generalized Natural\nSelection principle that together with the Church-Turing thesis is seen to be\nin close correspondence to a weak version of PCE. Next, we view our cognitive\ntoolkit in an evolutionary light and postulate a principle in analogy with\nFodor's language principle. In the final part of the paper we reflect on ways\nto provide circumstantial evidence for GNS by means of theorems, experiments\nor, simulations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 11:49:44 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Joosten", "J. J.", ""]]}, {"id": "1212.3282", "submitter": "EPTCS", "authors": "Niall Murphy (Universidad Polit\\'ecnica de Madrid), Damien Woods\n  (California Institute of Technology)", "title": "AND and/or OR: Uniform Polynomial-Size Circuits", "comments": "In Proceedings MCU 2013, arXiv:1309.1043", "journal-ref": "EPTCS 128, 2013, pp. 150-166", "doi": "10.4204/EPTCS.128.20", "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the complexity of uniform OR circuits and AND circuits of\npolynomial-size and depth. As their name suggests, OR circuits have OR gates as\ntheir computation gates, as well as the usual input, output and constant (0/1)\ngates. As is the norm for Boolean circuits, our circuits have multiple sink\ngates, which implies that an OR circuit computes an OR function on some subset\nof its input variables. Determining that subset amounts to solving a number of\nreachability questions on a polynomial-size directed graph (which input gates\nare connected to the output gate?), taken from a very sparse set of graphs.\nHowever, it is not obvious whether or not this (restricted) reachability\nproblem can be solved, by say, uniform AC^0 circuits (constant depth,\npolynomial-size, AND, OR, NOT gates). This is one reason why characterizing the\npower of these simple-looking circuits in terms of uniform classes turns out to\nbe intriguing. Another is that the model itself seems particularly natural and\nworthy of study.\n  Our goal is the systematic characterization of uniform polynomial-size OR\ncircuits, and AND circuits, in terms of known uniform machine-based complexity\nclasses. In particular, we consider the languages reducible to such uniform\nfamilies of OR circuits, and AND circuits, under a variety of reduction types.\nWe give upper and lower bounds on the computational power of these language\nclasses. We find that these complexity classes are closely related to tallyNL,\nthe set of unary languages within NL, and to sets reducible to tallyNL.\nSpecifically, for a variety of types of reductions (many-one, conjunctive truth\ntable, disjunctive truth table, truth table, Turing) we give characterizations\nof languages reducible to OR circuit classes in terms of languages reducible to\ntallyNL classes. Then, some of these OR classes are shown to coincide, and some\nare proven to be distinct. We give analogous results for AND circuits. Finally,\nfor many of our OR circuit classes, and analogous AND circuit classes, we prove\nwhether or not the two classes coincide, although we leave one such inclusion\nopen.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 19:35:12 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2013 08:08:19 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Murphy", "Niall", "", "Universidad Polit\u00e9cnica de Madrid"], ["Woods", "Damien", "", "California Institute of Technology"]]}, {"id": "1212.3380", "submitter": "Jed Yang", "authors": "Jed Yang", "title": "Rectangular tileability and complementary tileability are undecidable", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does a given a set of polyominoes tile some rectangle? We show that this\nproblem is undecidable. In a different direction, we also consider tiling a\ncofinite subset of the plane. The tileability is undecidable for many variants\nof this problem. However, we present an algorithm for testing whether the\ncomplement of a finite region is tileable by a set of rectangles.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 01:16:57 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Yang", "Jed", ""]]}, {"id": "1212.3418", "submitter": "Anissa Lamani", "authors": "Evangelos Bampas, Anissa Lamani (MIS), Franck Petit (LIP6), Mathieu\n  Valero (LIP6, INRIA Rocquencourt)", "title": "Self-Stabilizing Balancing Algorithm for Containment-Based Trees", "comments": "(2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containment-based trees encompass various handy structures such as B+-trees,\nR-trees and M-trees. They are widely used to build data indexes,\nrange-queryable overlays, publish/subscribe systems both in centralized and\ndistributed contexts. In addition to their versatility, their balanced shape\nensures an overall satisfactory performance. Re- cently, it has been shown that\ntheir distributed implementations can be fault-resilient. However, this\nrobustness is achieved at the cost of un-balancing the structure. While the\nstructure remains correct in terms of searchability, its performance can be\nsignificantly decreased. In this paper, we propose a distributed\nself-stabilizing algorithm to balance containment-based trees.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 09:17:50 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Bampas", "Evangelos", "", "MIS"], ["Lamani", "Anissa", "", "MIS"], ["Petit", "Franck", "", "LIP6"], ["Valero", "Mathieu", "", "LIP6, INRIA Rocquencourt"]]}, {"id": "1212.3471", "submitter": "Marek Karpinski", "authors": "Marek Karpinski, Andrzej Lingas, Dzmitry Sledneu", "title": "Optimal Cuts and Partitions in Tree Metrics in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial time dynamic programming algorithm for optimal\npartitions in the shortest path metric induced by a tree. This resolves, among\nother things, the exact complexity status of the optimal partition problems in\none dimensional geometric metric settings. Our method of solution could be also\nof independent interest in other applications. We discuss also an extension of\nour method to the class of metrics induced by the bounded treewidth graphs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 13:52:42 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Karpinski", "Marek", ""], ["Lingas", "Andrzej", ""], ["Sledneu", "Dzmitry", ""]]}, {"id": "1212.3517", "submitter": "Mikael Gast", "authors": "Mikael Gast, Mathias Hauptmann, Marek Karpinski", "title": "Inapproximability of Dominating Set in Power Law Graphs", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give logarithmic lower bounds for the approximability of the Minimum\nDominating Set problem in connected (alpha,beta)-Power Law Graphs. We give also\na best up to now upper approximation bound on the problem for the case of the\nparameters beta>2. We develop also a new functional method for proving lower\napproximation bounds and display a sharp phase transition between\napproximability and inapproximability of the underlying problem. This method\ncould also be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 16:31:32 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Gast", "Mikael", ""], ["Hauptmann", "Mathias", ""], ["Karpinski", "Marek", ""]]}, {"id": "1212.3849", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya, Eldar Fischer, Hamed Hatami, Pooya Hatami,\n  Shachar Lovett", "title": "Every locally characterized affine-invariant property is testable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F = F_p for any fixed prime p >= 2. An affine-invariant property is a\nproperty of functions on F^n that is closed under taking affine transformations\nof the domain. We prove that all affine-invariant property having local\ncharacterizations are testable. In fact, we show a proximity-oblivious test for\nany such property P, meaning that there is a test that, given an input function\nf, makes a constant number of queries to f, always accepts if f satisfies P,\nand rejects with positive probability if the distance between f and P is\nnonzero. More generally, we show that any affine-invariant property that is\nclosed under taking restrictions to subspaces and has bounded complexity is\ntestable.\n  We also prove that any property that can be described as the property of\ndecomposing into a known structure of low-degree polynomials is locally\ncharacterized and is, hence, testable. For example, whether a function is a\nproduct of two degree-d polynomials, whether a function splits into a product\nof d linear polynomials, and whether a function has low rank are all examples\nof degree-structural properties and are therefore locally characterized.\n  Our results depend on a new Gowers inverse theorem by Tao and Ziegler for low\ncharacteristic fields that decomposes any polynomial with large Gowers norm\ninto a function of low-degree non-classical polynomials. We establish a new\nequidistribution result for high rank non-classical polynomials that drives the\nproofs of both the testability results and the local characterization of\ndegree-structural properties.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 23:22:01 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 07:34:07 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Fischer", "Eldar", ""], ["Hatami", "Hamed", ""], ["Hatami", "Pooya", ""], ["Lovett", "Shachar", ""]]}, {"id": "1212.4129", "submitter": "Bundit Laekhanukit", "authors": "Parinya Chalermsook, Bundit Laekhanukit, Danupon Nanongkai", "title": "Graph Products Revisited: Tight Approximation Hardness of Induced\n  Matching, Poset Dimension and More", "comments": "Preliminary version is published in SODA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph product is a fundamental tool with rich applications in both graph\ntheory and theoretical computer science. It is usually studied in the form\n$f(G*H)$ where $G$ and $H$ are graphs, * is a graph product and $f$ is a graph\nproperty. For example, if $f$ is the independence number and * is the\ndisjunctive product, then the product is known to be multiplicative:\n$f(G*H)=f(G)f(H)$.\n  In this paper, we study graph products in the following non-standard form:\n$f((G\\oplus H)*J)$ where $G$, $H$ and $J$ are graphs, $\\oplus$ and * are two\ndifferent graph products and $f$ is a graph property. We show that if $f$ is\nthe induced and semi-induced matching number, then for some products $\\oplus$\nand *, it is subadditive in the sense that $f((G\\oplus H)*J)\\leq\nf(G*J)+f(H*J)$. Moreover, when $f$ is the poset dimension number, it is almost\nsubadditive.\n  As applications of this result (we only need $J=K_2$ here), we obtain tight\nhardness of approximation for various problems in discrete mathematics and\ncomputer science: bipartite induced and semi-induced matching (a.k.a. maximum\nexpanding sequences), poset dimension, maximum feasible subsystem with 0/1\ncoefficients, unit-demand min-buying and single-minded pricing, donation center\nlocation, boxicity, cubicity, threshold dimension and independent packing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 20:38:28 GMT"}, {"version": "v2", "created": "Sat, 18 Oct 2014 20:55:49 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Laekhanukit", "Bundit", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1212.4372", "submitter": "Widad Machmouchi", "authors": "Paul Beame, Raphael Clifford, Widad Machmouchi", "title": "Sliding Windows with Limited Storage", "comments": "The results of this paper are superceded by the paper at:\n  arXiv:1309.3690", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider time-space tradeoffs for exactly computing frequency moments and\norder statistics over sliding windows. Given an input of length 2n-1, the task\nis to output the function of each window of length n, giving n outputs in\ntotal. Computations over sliding windows are related to direct sum problems\nexcept that inputs to instances almost completely overlap.\n  We show an average case and randomized time-space tradeoff lower bound of TS\nin Omega(n^2) for multi-way branching programs, and hence standard RAM and\nword-RAM models, to compute the number of distinct elements, F_0, in sliding\nwindows over alphabet [n]. The same lower bound holds for computing the\nlow-order bit of F_0 and computing any frequency moment F_k for k not equal to\n1. We complement this lower bound with a TS in \\tilde O(n^2) deterministic RAM\nalgorithm for exactly computing F_k in sliding windows.\n  We show time-space separations between the complexity of sliding-window\nelement distinctness and that of sliding-window $F_0\\bmod 2$ computation. In\nparticular for alphabet [n] there is a very simple errorless sliding-window\nalgorithm for element distinctness that runs in O(n) time on average and uses\nO(log{n}) space.\n  We show that any algorithm for a single element distinctness instance can be\nextended to an algorithm for the sliding-window version of element distinctness\nwith at most a polylogarithmic increase in the time-space product.\n  Finally, we show that the sliding-window computation of order statistics such\nas the maximum and minimum can be computed with only a logarithmic increase in\ntime, but that a TS in Omega(n^2) lower bound holds for sliding-window\ncomputation of order statistics such as the median, a nearly linear increase in\ntime when space is small.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 14:50:45 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 21:30:55 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2013 02:32:31 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Beame", "Paul", ""], ["Clifford", "Raphael", ""], ["Machmouchi", "Widad", ""]]}, {"id": "1212.4548", "submitter": "Stefan Schneider", "authors": "Russell Impagliazzo, Ramamohan Paturi, Stefan Schneider", "title": "A Satisfiability Algorithm for Sparse Depth Two Threshold Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a nontrivial algorithm for the satisfiability problem for cn-wire\nthreshold circuits of depth two which is better than exhaustive search by a\nfactor 2^{sn} where s= 1/c^{O(c^2)}. We believe that this is the first\nnontrivial satisfiability algorithm for cn-wire threshold circuits of depth\ntwo. The independently interesting problem of the feasibility of sparse 0-1\ninteger linear programs is a special case. To our knowledge, our algorithm is\nthe first to achieve constant savings even for the special case of Integer\nLinear Programming. The key idea is to reduce the satisfiability problem to the\nVector Domination Problem, the problem of checking whether there are two\nvectors in a given collection of vectors such that one dominates the other\ncomponent-wise.\n  We also provide a satisfiability algorithm with constant savings for depth\ntwo circuits with symmetric gates where the total weighted fan-in is at most\ncn.\n  One of our motivations is proving strong lower bounds for TC^0 circuits,\nexploiting the connection (established by Williams) between satisfiability\nalgorithms and lower bounds. Our second motivation is to explore the connection\nbetween the expressive power of the circuits and the complexity of the\ncorresponding circuit satisfiability problem.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 01:13:45 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2013 20:51:47 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Impagliazzo", "Russell", ""], ["Paturi", "Ramamohan", ""], ["Schneider", "Stefan", ""]]}, {"id": "1212.4756", "submitter": "Matthew Patitz", "authors": "Erik D. Demaine, Martin L. Demaine, S\\'andor P. Fekete, Matthew J.\n  Patitz, Robert T. Schweller, Andrew Winslow, and Damien Woods", "title": "One Tile to Rule Them All: Simulating Any Turing Machine, Tile Assembly\n  System, or Tiling System with a Single Puzzle Piece", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the power of tile self-assembly models that extend\nthe well-studied abstract Tile Assembly Model (aTAM) by permitting tiles of\nshapes beyond unit squares. Our main result shows the surprising fact that any\naTAM system, consisting of many different tile types, can be simulated by a\nsingle tile type of a general shape. As a consequence, we obtain a single\nuniversal tile type of a single (constant-size) shape that serves as a\n\"universal tile machine\": the single universal tile type can simulate any\ndesired aTAM system when given a single seed assembly that encodes the desired\naTAM system. We also show how to adapt this result to convert any of a variety\nof plane tiling systems (such as Wang tiles) into a \"nearly\" plane tiling\nsystem with a single tile (but with small gaps between the tiles). All of these\nresults rely on the ability to both rotate and translate tiles; by contrast, we\nshow that a single nonrotatable tile, of arbitrary shape, can produce\nassemblies which either grow infinitely or cannot grow at all, implying\ndrastically limited computational power.\n  On the positive side, we show how to simulate arbitrary cellular automata for\na limited number of steps using a single nonrotatable tile and a linear-size\nseed assembly.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 17:37:27 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Patitz", "Matthew J.", ""], ["Schweller", "Robert T.", ""], ["Winslow", "Andrew", ""], ["Woods", "Damien", ""]]}, {"id": "1212.5097", "submitter": "Deepak Ponvel Chermakani Mr", "authors": "Deepak Ponvel Chermakani", "title": "NP-Hardness of optimizing the sum of Rational Linear Functions over an\n  Asymptotic-Linear-Program", "comments": "3 Pages, 4 Theorems, 1 Definition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We convert, within polynomial-time and sequential processing, an NP-Complete\nProblem into a real-variable problem of minimizing a sum of Rational Linear\nFunctions constrained by an Asymptotic-Linear-Program. The coefficients and\nconstants in the real-variable problem are 0, 1, -1, K, or -K, where K is the\ntime parameter that tends to positive infinity. The number of variables,\nconstraints, and rational linear functions in the objective, of the\nreal-variable problem is bounded by a polynomial function of the size of the\nNP-Complete Problem. The NP-Complete Problem has a feasible solution,\nif-and-only-if, the real-variable problem has a feasible optimal objective\nequal to zero. We thus show the strong NP-hardness of this real-variable\noptimization problem.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 17:04:17 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Chermakani", "Deepak Ponvel", ""]]}, {"id": "1212.5132", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "The Inverse Shapley Value Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $f$ a weighted voting scheme used by $n$ voters to choose between two\ncandidates, the $n$ \\emph{Shapley-Shubik Indices} (or {\\em Shapley values}) of\n$f$ provide a measure of how much control each voter can exert over the overall\noutcome of the vote. Shapley-Shubik indices were introduced by Lloyd Shapley\nand Martin Shubik in 1954 \\cite{SS54} and are widely studied in social choice\ntheory as a measure of the \"influence\" of voters. The \\emph{Inverse Shapley\nValue Problem} is the problem of designing a weighted voting scheme which\n(approximately) achieves a desired input vector of values for the\nShapley-Shubik indices. Despite much interest in this problem no provably\ncorrect and efficient algorithm was known prior to our work.\n  We give the first efficient algorithm with provable performance guarantees\nfor the Inverse Shapley Value Problem. For any constant $\\eps > 0$ our\nalgorithm runs in fixed poly$(n)$ time (the degree of the polynomial is\nindependent of $\\eps$) and has the following performance guarantee: given as\ninput a vector of desired Shapley values, if any \"reasonable\" weighted voting\nscheme (roughly, one in which the threshold is not too skewed) approximately\nmatches the desired vector of values to within some small error, then our\nalgorithm explicitly outputs a weighted voting scheme that achieves this vector\nof Shapley values to within error $\\eps.$ If there is a \"reasonable\" voting\nscheme in which all voting weights are integers at most $\\poly(n)$ that\napproximately achieves the desired Shapley values, then our algorithm runs in\ntime $\\poly(n)$ and outputs a weighted voting scheme that achieves the target\nvector of Shapley values to within error $\\eps=n^{-1/8}.$\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 16:38:11 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1212.5324", "submitter": "Li-Yang Tan", "authors": "Manuel Kauers, Ryan O'Donnell, Li-Yang Tan, Yuan Zhou", "title": "Hypercontractive inequalities via SOS, and the Frankl--R\\\"odl graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our main result is a formulation and proof of the reverse hypercontractive\ninequality in the sum-of-squares (SOS) proof system. As a consequence we show\nthat for any constant $0 < \\gamma \\leq 1/4$, the SOS/Lasserre SDP hierarchy at\ndegree $4\\lceil \\frac{1}{4\\gamma}\\rceil$ certifies the statement \"the maximum\nindependent set in the Frankl--R\\\"odl graph $\\mathrm{FR}^{n}_{\\gamma}$ has\nfractional size~$o(1)$\". Here $\\mathrm{FR}^{n}_{\\gamma} = (V,E)$ is the graph\nwith $V = \\{0,1\\}^n$ and $(x,y) \\in E$ whenever $\\Delta(x,y) = (1-\\gamma)n$ (an\neven integer). In particular, we show the degree-$4$ SOS algorithm certifies\nthe chromatic number lower bound \"$\\chi(\\mathrm{FR}^{n}_{1/4}) = \\omega(1)$\",\neven though $\\mathrm{FR}^{n}_{1/4}$ is the canonical integrality gap instance\nfor which standard SDP relaxations cannot even certify\n\"$\\chi(\\mathrm{FR}^{n}_{1/4}) > 3$\". Finally, we also give an SOS proof of (a\ngeneralization of) the sharp $(2,q)$-hypercontractive inequality for any even\ninteger $q$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 03:28:28 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 00:33:57 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 21:31:16 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kauers", "Manuel", ""], ["O'Donnell", "Ryan", ""], ["Tan", "Li-Yang", ""], ["Zhou", "Yuan", ""]]}, {"id": "1212.5645", "submitter": "Rajat Tandon", "authors": "Rajat Tandon", "title": "Algorithm to Compute Squares of 1st N Natural Numbers Without Using\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Processors may find some elementary operations to be faster than the others.\nAlthough an operation may be conceptually as simple as some other operation,\nthe processing speeds of the two can vary. A clever programmer will always try\nto choose the faster instructions for the job. This paper presents an algorithm\nto display squares of 1st N natural numbers without using multiplication (*\noperator). Instead, the same work can be done using addition (+ operator). The\nresults can also be used to compute the sum of those squares. If we compare the\nnormal method of computing the squares of 1st N natural numbers with this\nmethod, we can conclude that the algorithm discussed in the paper is more\noptimized in terms of time complexity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 02:06:21 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Tandon", "Rajat", ""]]}, {"id": "1212.5895", "submitter": "Mario Alviano", "authors": "Mario Alviano, Wolfgang Faber, Stefan Woltran", "title": "Complexity of super-coherence problems in ASP", "comments": "22 pages, 1 figure, journal paper", "journal-ref": "Theory and Practice of Logic Programming 14 (2014) 339-361", "doi": "10.1017/S147106841300001X", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting techniques from database theory in order to optimize Answer Set\nProgramming (ASP) systems, and in particular the grounding components of ASP\nsystems, is an important topic in ASP. In recent years, the Magic Set method\nhas received some interest in this setting, and a variant of it, called DMS,\nhas been proposed for ASP. However, this technique has a caveat, because it is\nnot correct (in the sense of being query-equivalent) for all ASP programs. In\nrecent work, a large fragment of ASP programs, referred to as super-coherent\nprograms, has been identified, for which DMS is correct. The fragment contains\nall programs which possess at least one answer set, no matter which set of\nfacts is added to them. Two open question remained: How complex is it to\ndetermine whether a given program is super-coherent? Does the restriction to\nsuper-coherent programs limit the problems that can be solved? Especially the\nfirst question turned out to be quite difficult to answer precisely. In this\npaper, we formally prove that deciding whether a propositional program is\nsuper-coherent is \\Pi^P_3-complete in the disjunctive case, while it is\n\\Pi^P_2-complete for normal programs. The hardness proofs are the difficult\npart in this endeavor: We proceed by characterizing the reductions by the\nmodels and reduct models which the ASP programs should have, and then provide\ninstantiations that meet the given specifications. Concerning the second\nquestion, we show that all relevant ASP reasoning tasks can be transformed into\ntasks over super-coherent programs, even though this transformation is more of\ntheoretical than practical interest.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 11:04:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alviano", "Mario", ""], ["Faber", "Wolfgang", ""], ["Woltran", "Stefan", ""]]}, {"id": "1212.6104", "submitter": "Jason Teutsch", "authors": "Jason Teutsch", "title": "Short lists for shortest descriptions in short time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to find a shortest description for a binary string? The\nwell-known answer is \"no, Kolmogorov complexity is not computable.\" Faced with\nthis barrier, one might instead seek a short list of candidates which includes\na laconic description. Remarkably such approximations exist. This paper\npresents an efficient algorithm which generates a polynomial-size list\ncontaining an optimal description for a given input string. Along the way, we\nemploy expander graphs and randomness dispersers to obtain an Explicit Online\nMatching Theorem for bipartite graphs and a refinement of Muchnik's Conditional\nComplexity Theorem. Our main result extends recent work by Bauwens, Mahklin,\nVereschchagin, and Zimand.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 00:15:53 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2012 08:30:22 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2013 01:27:30 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2013 07:44:58 GMT"}, {"version": "v5", "created": "Wed, 12 Feb 2014 21:38:28 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Teutsch", "Jason", ""]]}, {"id": "1212.6327", "submitter": "Marko Grgurovi\\v{c}", "authors": "Andrej Brodnik and Marko Grgurovi\\v{c}", "title": "Speeding up shortest path algorithms", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an arbitrary, non-negatively weighted, directed graph $G=(V,E)$ we\npresent an algorithm that computes all pairs shortest paths in time\n$\\mathcal{O}(m^* n + m \\lg n + nT_\\psi(m^*, n))$, where $m^*$ is the number of\ndifferent edges contained in shortest paths and $T_\\psi(m^*, n)$ is a running\ntime of an algorithm to solve a single-source shortest path problem (SSSP).\nThis is a substantial improvement over a trivial $n$ times application of\n$\\psi$ that runs in $\\mathcal{O}(nT_\\psi(m,n))$. In our algorithm we use $\\psi$\nas a black box and hence any improvement on $\\psi$ results also in improvement\nof our algorithm.\n  Furthermore, a combination of our method, Johnson's reweighting technique and\ntopological sorting results in an $\\mathcal{O}(m^*n + m \\lg n)$ all-pairs\nshortest path algorithm for arbitrarily-weighted directed acyclic graphs.\n  In addition, we also point out a connection between the complexity of a\ncertain sorting problem defined on shortest paths and SSSP.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 08:59:40 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Brodnik", "Andrej", ""], ["Grgurovi\u010d", "Marko", ""]]}, {"id": "1212.6567", "submitter": "Andr", "authors": "Martin Grohe (RWTH Aachen University, Germany), Berit Gru{\\ss}ien\n  (Humboldt-Universit\\\"at zu Berlin), Andr\\'e Hernich (Humboldt-Universit\\\"at\n  zu Berlin), Bastian Laubner (Humboldt-Universit\\\"at zu Berlin)", "title": "L-Recursion and a new Logic for Logarithmic Space", "comments": "44 pages, 10 figures. A preliminary version of this article appeared\n  in the Proceedings of the 25th International Workshop on Computer Science\n  Logic (CSL '11)", "journal-ref": "Logical Methods in Computer Science, Volume 9, Issue 1 (March 13,\n  2013) lmcs:938", "doi": "10.2168/LMCS-9(1:11)2013", "report-no": null, "categories": "cs.LO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend first-order logic with counting by a new operator that allows it to\nformalise a limited form of recursion which can be evaluated in logarithmic\nspace. The resulting logic LREC has a data complexity in LOGSPACE, and it\ndefines LOGSPACE-complete problems like deterministic reachability and Boolean\nformula evaluation. We prove that LREC is strictly more expressive than\ndeterministic transitive closure logic with counting and incomparable in\nexpressive power with symmetric transitive closure logic STC and transitive\nclosure logic (with or without counting). LREC is strictly contained in\nfixed-point logic with counting FPC. We also study an extension LREC= of LREC\nthat has nicer closure properties and is more expressive than both LREC and\nSTC, but is still contained in FPC and has a data complexity in LOGSPACE. Our\nmain results are that LREC captures LOGSPACE on the class of directed trees and\nthat LREC= captures LOGSPACE on the class of interval graphs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2012 21:37:58 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2013 01:56:53 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2013 09:51:53 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Grohe", "Martin", "", "RWTH Aachen University, Germany"], ["Gru\u00dfien", "Berit", "", "Humboldt-Universit\u00e4t zu Berlin"], ["Hernich", "Andr\u00e9", "", "Humboldt-Universit\u00e4t\n  zu Berlin"], ["Laubner", "Bastian", "", "Humboldt-Universit\u00e4t zu Berlin"]]}, {"id": "1212.6725", "submitter": "Koji Kobayashi", "authors": "Koji Kobayashi", "title": "Solvability of HornSAT and CNFSAT", "comments": "3 pages, English and Japanese (see Other formats - Source)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the solvability of HornSAT and CNFSAT.\n  Unsatisfiable HornCNF have partially ordered set that is made by causation of\neach clauses. In this partially ordered set, Truth value assignment that is\nfalse in each clauses become simply connected space. Therefore, if we reduce\nCNFSAT to HornSAT, we must make such partially ordered set in HornSAT. But\nCNFSAT have correlations of each clauses, the partially ordered set is not in\npolynomial size.\n  Therefore, we cannot reduce CNFSAT to HornSAT in polynomial size.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 14:11:39 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Kobayashi", "Koji", ""]]}, {"id": "1212.6745", "submitter": "Hector Zenil", "authors": "Hector Zenil, Fernando Soler-Toscano, Jean-Paul Delahaye and Nicolas\n  Gauvrit", "title": "Two-Dimensional Kolmogorov Complexity and Validation of the Coding\n  Theorem Method by Compressibility", "comments": "39 pages, 13 figures. Forthcoming in PeerJ Computer Science (this\n  version is as it was accepted with minor changes to the figures enumeration,\n  also a one-letter typo corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a measure based upon the fundamental theoretical concept in\nalgorithmic information theory that provides a natural approach to the problem\nof evaluating $n$-dimensional complexity by using an $n$-dimensional\ndeterministic Turing machine. The technique is interesting because it provides\na natural algorithmic process for symmetry breaking generating complex\n$n$-dimensional structures from perfectly symmetric and fully deterministic\ncomputational rules producing a distribution of patterns as described by\nalgorithmic probability. Algorithmic probability also elegantly connects the\nfrequency of occurrence of a pattern with its algorithmic complexity, hence\neffectively providing estimations to the complexity of the generated patterns.\nExperiments to validate estimations of algorithmic complexity based on these\nconcepts are presented, showing that the measure is stable in the face of some\nchanges in computational formalism and that results are in agreement with the\nresults obtained using lossless compression algorithms when both methods\noverlap in their range of applicability. We then use the output frequency of\nthe set of 2-dimensional Turing machines to classify the algorithmic complexity\nof the space-time evolutions of Elementary Cellular Automata.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 17:06:59 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 15:29:21 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2015 17:43:11 GMT"}, {"version": "v4", "created": "Wed, 26 Aug 2015 01:56:48 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Zenil", "Hector", ""], ["Soler-Toscano", "Fernando", ""], ["Delahaye", "Jean-Paul", ""], ["Gauvrit", "Nicolas", ""]]}, {"id": "1212.6781", "submitter": "Daniel Dadush", "authors": "Daniel Dadush, Gabor Kun", "title": "Lattice Sparsification and the Approximate Closest Vector Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic algorithm for solving the (1+eps)-approximate Closest\nVector Problem (CVP) on any n dimensional lattice and any norm in\n2^{O(n)}(1+1/eps)^n time and 2^n poly(n) space. Our algorithm builds on the\nlattice point enumeration techniques of Micciancio and Voulgaris (STOC 2010)\nand Dadush, Peikert and Vempala (FOCS 2011), and gives an elegant,\ndeterministic alternative to the \"AKS Sieve\" based algorithms for (1+eps)-CVP\n(Ajtai, Kumar, and Sivakumar; STOC 2001 and CCC 2002). Furthermore, assuming\nthe existence of a poly(n)-space and 2^{O(n)} time algorithm for exact CVP in\nthe l_2 norm, the space complexity of our algorithm can be reduced to\npolynomial.\n  Our main technical contribution is a method for \"sparsifying\" any input\nlattice while approximately maintaining its metric structure. To this end, we\nemploy the idea of random sublattice restrictions, which was first employed by\nKhot (FOCS 2003) for the purpose of proving hardness for Shortest Vector\nProblem (SVP) under l_p norms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 21:29:27 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Dadush", "Daniel", ""], ["Kun", "Gabor", ""]]}, {"id": "1212.6846", "submitter": "Sagar Kale", "authors": "Sagar Kale", "title": "Maximizing a Nonnegative, Monotone, Submodular Function Constrained to\n  Matchings", "comments": "Withdrawn because the main result is implied by a more general result\n  about p-independence-system (which generalize matchings) in the paper by\n  Calinescu, Chekuri, Pal, and Vondrak, Maximizing a Monotone Submodular\n  Function Subject to a Matroid Constraint, SIAM J. Comput., 2011, Vol 40, No\n  6, pp. 1740-1766", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions have many applications. Matchings have many\napplications. The bitext word alignment problem can be modeled as the problem\nof maximizing a nonnegative, monotone, submodular function constrained to\nmatchings in a complete bipartite graph where each vertex corresponds to a word\nin the two input sentences and each edge represents a potential word-to-word\ntranslation. We propose a more general problem of maximizing a nonnegative,\nmonotone, submodular function defined on the edge set of a complete graph\nconstrained to matchings; we call this problem the CSM-Matching problem.\nCSM-Matching also generalizes the maximum-weight matching problem, which has a\npolynomial-time algorithm; however, we show that it is NP-hard to approximate\nCSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to\nit. Our main result is a simple, greedy, 3-approximation algorithm for\nCSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,\nmonotone, submodular function over two matroids, i.e., CSM-2-Matroids.\nCSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show\nthat we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.\nWe extend this approach to similar problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 09:32:51 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 21:20:45 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Kale", "Sagar", ""]]}, {"id": "1212.6848", "submitter": "Gregory Gutin", "authors": "R. Crowston, G. Gutin, M. Jones, G. Muciaccia", "title": "Maximum Balanced Subgraph Problem Parameterized Above Lower Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider graphs without loops or parallel edges in which every edge is\nassigned + or -. Such a signed graph is balanced if its vertex set can be\npartitioned into parts $V_1$ and $V_2$ such that all edges between vertices in\nthe same part have sign + and all edges between vertices of different parts\nhave sign $-$ (one of the parts may be empty). It is well-known that every\nconnected signed graph with $n$ vertices and $m$ edges has a balanced subgraph\nwith at least $\\frac{m}{2} + \\frac{n-1}{4}$ edges and this bound is tight. We\nconsider the following parameterized problem: given a connected signed graph\n$G$ with $n$ vertices and $m$ edges, decide whether $G$ has a balanced subgraph\nwith at least $\\frac{m}{2} + \\frac{n-1}{4}+\\frac{k}{4}$ edges, where $k$ is the\nparameter.\n  We obtain an algorithm for the problem of runtime $8^k(kn)^{O(1)}$. We also\nprove that for each instance $(G,k)$ of the problem, in polynomial time, we can\neither solve $(G,k)$ or produce an equivalent instance $(G',k')$ such that\n$k'\\le k$ and $|V(G')|=O(k^3)$. Our first result generalizes a result of\nCrowston, Jones and Mnich (ICALP 2012) on the corresponding parameterization of\nMax Cut (when every edge of $G$ has sign $-$). Our second result generalizes\nand significantly improves the corresponding result of Crowston, Jones and\nMnich: they showed that $|V(G')|=O(k^5)$.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 10:01:10 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 13:40:17 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Crowston", "R.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Muciaccia", "G.", ""]]}, {"id": "1212.6925", "submitter": "Krzysztof Onak", "authors": "Venkatesan Guruswami and Krzysztof Onak", "title": "Superlinear lower bounds for multipass graph processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove $n^{1+\\Omega(1/p)}/p^{O(1)}$ lower bounds for the space complexity\nof $p$-pass streaming algorithms solving the following problems on $n$-vertex\ngraphs:\n  * testing if an undirected graph has a perfect matching (this implies lower\nbounds for computing a maximum matching or even just the maximum matching\nsize),\n  * testing if two specific vertices are at distance at most $2(p+1)$ in an\nundirected graph,\n  * testing if there is a directed path from $s$ to $t$ for two specific\nvertices $s$ and $t$ in a directed graph.\n  Prior to our result, it was known that these problems require $\\Omega(n^2)$\nspace in one pass, but no $n^{1+\\Omega(1)}$ lower bound was known for any $p\\ge\n2$.\n  These streaming results follow from a communication complexity lower bound\nfor a communication game in which the players hold two graphs on the same set\nof vertices. The task of the players is to find out whether the sets of\nvertices at distance exactly $p+1$ from a specific vertex intersect. The game\nrequires a significant amount of communication only if the players are forced\nto speak in a specific difficult order. This is reminiscent of lower bounds for\ncommunication problems such as indexing and pointer chasing. Among other\nthings, our line of attack requires proving an information cost lower bound for\na decision version of the classic pointer chasing problem and a direct sum type\ntheorem for the disjunction of several instances of this problem.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 16:57:22 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 02:29:10 GMT"}, {"version": "v3", "created": "Mon, 18 Aug 2014 17:15:18 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2016 22:20:14 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Guruswami", "Venkatesan", ""], ["Onak", "Krzysztof", ""]]}, {"id": "1212.6935", "submitter": "Giorgio Camerani", "authors": "Giorgio Camerani", "title": "The ODD EVEN DELTA problem is #P-hard", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G=(V,E) be a graph. Let k < |V| be an integer. Let O_k be the number of\nedge induced subgraphs of G having k vertices and an odd number of edges. Let\nE_k be the number of edge induced subgraphs of G having k vertices and an even\nnumber of edges. Let D_k = O_k - E_k. The ODD EVEN DELTA problem consists in\ncomputing D_k, given G and k. We show that such problem is #P-hard, even on\n3-regular bipartite planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 17:47:52 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Camerani", "Giorgio", ""]]}]