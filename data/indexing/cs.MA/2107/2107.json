[{"id": "2107.00032", "submitter": "Alex Raymond", "authors": "Alex Raymond, Matthew Malencia, Guilherme Paulino-Passos, and Amanda\n  Prorok", "title": "Agree to Disagree: Subjective Fairness in Privacy-Restricted\n  Decentralised Conflict Resolution", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is commonly seen as a property of the global outcome of a system and\nassumes centralisation and complete knowledge. However, in real decentralised\napplications, agents only have partial observation capabilities. Under limited\ninformation, agents rely on communication to divulge some of their private (and\nunobservable) information to others. When an agent deliberates to resolve\nconflicts, limited knowledge may cause its perspective of a correct outcome to\ndiffer from the actual outcome of the conflict resolution. This is subjective\nunfairness.\n  To enable decentralised, fairness-aware conflict resolution under privacy\nconstraints, we have two contributions: (1) a novel interaction approach and\n(2) a formalism of the relationship between privacy and fairness. Our proposed\ninteraction approach is an architecture for privacy-aware explainable conflict\nresolution where agents engage in a dialogue of hypotheses and facts. To\nmeasure the privacy-fairness relationship, we define subjective and objective\nfairness on both the local and global scope and formalise the impact of partial\nobservability due to privacy in these different notions of fairness.\n  We first study our proposed architecture and the privacy-fairness\nrelationship in the abstract, testing different argumentation strategies on a\nlarge number of randomised cultures. We empirically demonstrate the trade-off\nbetween privacy, objective fairness, and subjective fairness and show that\nbetter strategies can mitigate the effects of privacy in distributed systems.\nIn addition to this analysis across a broad set of randomised abstract\ncultures, we analyse a case study for a specific scenario: we instantiate our\narchitecture in a multi-agent simulation of prioritised rule-aware collision\navoidance with limited information disclosure.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:00:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Raymond", "Alex", ""], ["Malencia", "Matthew", ""], ["Paulino-Passos", "Guilherme", ""], ["Prorok", "Amanda", ""]]}, {"id": "2107.00165", "submitter": "Justin Luke", "authors": "Justin Luke, Mauro Salazar, Ram Rajagopal, Marco Pavone", "title": "Joint Optimization of Autonomous Electric Vehicle Fleet Operations and\n  Charging Station Siting", "comments": "9 pages, 7 figures. Corrected typos, revised text for clarification\n  purposes, results unchanged. A version of this submission, with minor\n  formatting changes, is to be published in the proceedings of the 24th IEEE\n  International Conference on Intelligent Transportation Systems (ITSC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charging infrastructure is the coupling link between power and transportation\nnetworks, thus determining charging station siting is necessary for planning of\npower and transportation systems. While previous works have either optimized\nfor charging station siting given historic travel behavior, or optimized fleet\nrouting and charging given an assumed placement of the stations, this paper\nintroduces a linear program that optimizes for station siting and macroscopic\nfleet operations in a joint fashion. Given an electricity retail rate and a set\nof travel demand requests, the optimization minimizes total cost for an\nautonomous EV fleet comprising of travel costs, station procurement costs,\nfleet procurement costs, and electricity costs, including demand charges.\nSpecifically, the optimization returns the number of charging plugs for each\ncharging rate (e.g., Level 2, DC fast charging) at each candidate location, as\nwell as the optimal routing and charging of the fleet. From a case-study of an\nelectric vehicle fleet operating in San Francisco, our results show that,\nalbeit with range limitations, small EVs with low procurement costs and high\nenergy efficiencies are the most cost-effective in terms of total ownership\ncosts. Furthermore, the optimal siting of charging stations is more spatially\ndistributed than the current siting of stations, consisting mainly of\nhigh-power Level 2 AC stations (16.8 kW) with a small share of DC fast charging\nstations and no standard 7.7kW Level 2 stations. Optimal siting reduces the\ntotal costs, empty vehicle travel, and peak charging load by up to 10%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 01:25:19 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 23:50:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Luke", "Justin", ""], ["Salazar", "Mauro", ""], ["Rajagopal", "Ram", ""], ["Pavone", "Marco", ""]]}, {"id": "2107.00246", "submitter": "Stepan Dergachev", "authors": "Stepan Dergachev, Konstantin Yakovlev", "title": "Distributed Multi-agent Navigation Based on Reciprocal Collision\n  Avoidance and Locally Confined Multi-agent Path Finding", "comments": "This is a preprint of the paper accepted to CASE'21. It contains 5\n  pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avoiding collisions is the core problem in multi-agent navigation. In\ndecentralized settings, when agents have limited communication and sensory\ncapabilities, collisions are typically avoided in a reactive fashion, relying\non local observations/communications. Prominent collision avoidance techniques,\ne.g. ORCA, are computationally efficient and scale well to a large number of\nagents. However, in numerous scenarios, involving navigation through the tight\npassages or confined spaces, deadlocks are likely to occur due to the egoistic\nbehaviour of the agents and as a result, the latter can not achieve their\ngoals. To this end, we suggest an application of the locally confined\nmulti-agent path finding (MAPF) solvers that coordinate sub-groups of the\nagents that appear to be in a deadlock (to detect the latter we suggest a\nsimple, yet efficient ad-hoc routine). We present a way to build a grid-based\nMAPF instance, typically required by modern MAPF solvers. We evaluate two of\nthem in our experiments, i.e. Push and Rotate and a bounded-suboptimal version\nof Conflict Based Search (ECBS), and show that their inclusion into the\nnavigation pipeline significantly increases the success rate, from 15% to 99%\nin certain cases.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:57:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dergachev", "Stepan", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "2107.00284", "submitter": "Kai Liu", "authors": "Kai Liu and Yuyang Zhao and Gang Wang and Bei Peng", "title": "SA-MATD3:Self-attention-based multi-agent continuous control method in\n  cooperative environments", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cooperative problems under continuous control have always been the focus of\nmulti-agent reinforcement learning. Existing algorithms suffer from the problem\nof uneven learning degree with the increase of the number of agents. In this\npaper, a new structure for a multi-agent actor critic is proposed, and the\nself-attention mechanism is applied in the critic network and the value\ndecomposition method used to solve the uneven problem. The proposed algorithm\nmakes full use of the samples in the replay memory buffer to learn the behavior\nof a class of agents. First, a new update method is proposed for policy\nnetworks that promotes learning efficiency. Second, the utilization of samples\nis improved, at the same time reflecting the ability of perspective-taking\namong groups. Finally, the \"deceptive signal\" in training is eliminated and the\nlearning degree among agents is more uniform than in the existing methods.\nMultiple experiments were conducted in two typical scenarios of a multi-agent\nparticle environment. Experimental results show that the proposed algorithm can\nperform better than the state-of-the-art ones, and that it exhibits higher\nlearning efficiency with an increasing number of agents.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:15:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Kai", ""], ["Zhao", "Yuyang", ""], ["Wang", "Gang", ""], ["Peng", "Bei", ""]]}, {"id": "2107.00431", "submitter": "Guilherme Ramos", "authors": "Guilherme Ramos and Daniel Silvestre and Carlos Silvestre", "title": "A Discrete-time Reputation-based Resilient Consensus Algorithm for\n  Synchronous or Asynchronous Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of a set of agents achieving resilient consensus in the\npresence of attacked agents. We present a discrete-time reputation-based\nconsensus algorithm for synchronous and asynchronous networks by developing a\nlocal strategy where, at each time, each agent assigns a reputation (between\nzero and one) to each neighbor. The reputation is then used to weigh the\nneighbors' values in the update of its state. Under mild assumptions, we show\nthat: (i) the proposed method converges exponentially to the consensus of the\nregular agents; (ii) if a regular agent identifies a neighbor as an attacked\nnode, then it is indeed an attacked node; (iii) if the consensus value of the\nnormal nodes differs from that of any of the attacked nodes' values, then the\nreputation that a regular agent assigns to the attacked neighbors goes to zero.\nFurther, we extend our method to achieve resilience in the scenarios where\nthere are noisy nodes, dynamic networks and stochastic node selection. Finally,\nwe illustrate our algorithm with several examples, and we delineate some\nattacking scenarios that can be dealt by the current proposal but not by the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:26:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ramos", "Guilherme", ""], ["Silvestre", "Daniel", ""], ["Silvestre", "Carlos", ""]]}, {"id": "2107.00769", "submitter": "Nathaniel Glaser", "authors": "Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira", "title": "Enhancing Multi-Robot Perception via Learned Data Association", "comments": "Accepted to ICRA 2020 Workshop on \"Emerging Learning and Algorithmic\n  Methods for Data Association in Robotics\"; associated spotlight talk\n  available at https://www.youtube.com/watch?v=-lEVvtsfz0I&t=16743s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the multi-robot collaborative perception problem,\nspecifically in the context of multi-view infilling for distributed semantic\nsegmentation. This setting entails several real-world challenges, especially\nthose relating to unregistered multi-agent image data. Solutions must\neffectively leverage multiple, non-static, and intermittently-overlapping RGB\nperspectives. To this end, we propose the Multi-Agent Infilling Network: an\nextensible neural architecture that can be deployed (in a distributed manner)\nto each agent in a robotic swarm. Specifically, each robot is in charge of\nlocally encoding and decoding visual information, and an extensible neural\nmechanism allows for an uncertainty-aware and context-based exchange of\nintermediate features. We demonstrate improved performance on a realistic\nmulti-robot AirSim dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 22:45:26 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Glaser", "Nathaniel", ""], ["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Kira", "Zsolt", ""]]}, {"id": "2107.00771", "submitter": "Nathaniel Glaser", "authors": "Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira", "title": "Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial\n  Handshaking", "comments": "Accepted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address bandwidth-limited and obstruction-prone\ncollaborative perception, specifically in the context of multi-agent semantic\nsegmentation. This setting presents several key challenges, including\nprocessing and exchanging unregistered robotic swarm imagery. To be successful,\nsolutions must effectively leverage multiple non-static and\nintermittently-overlapping RGB perspectives, while heeding bandwidth\nconstraints and overcoming unwanted foreground obstructions. As such, we\npropose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH)\nto process, compress, and propagate visual information across a robotic swarm.\nOur distributed communication module operates directly (and exclusively) on raw\nimage data, without additional input requirements such as pose, depth, or\nwarping data. We demonstrate superior performance of our model compared against\nseveral baselines in a photo-realistic multi-robot AirSim environment,\nespecially in the presence of image occlusions. Our method achieves an absolute\n11% IoU improvement over strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 22:56:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Glaser", "Nathaniel", ""], ["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Kira", "Zsolt", ""]]}, {"id": "2107.00913", "submitter": "Edward Elson Kosasih", "authors": "Edward Elson Kosasih, Alexandra Brintrup", "title": "Reinforcement Learning Provides a Flexible Approach for Realistic Supply\n  Chain Safety Stock Optimisation", "comments": "12 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although safety stock optimisation has been studied for more than 60 years,\nmost companies still use simplistic means to calculate necessary safety stock\nlevels, partly due to the mismatch between existing analytical methods'\nemphases on deriving provably optimal solutions and companies' preferences to\nsacrifice optimal results in favour of more realistic problem settings. A newly\nemerging method from the field of Artificial Intelligence (AI), namely\nReinforcement Learning (RL), offers promise in finding optimal solutions while\naccommodating more realistic problem features. Unlike analytical-based models,\nRL treats the problem as a black-box simulation environment mitigating against\nthe problem of oversimplifying reality. As such, assumptions on stock keeping\npolicy can be relaxed and a higher number of problem variables can be\naccommodated. While RL has been popular in other domains, its applications in\nsafety stock optimisation remain scarce. In this paper, we investigate three RL\nmethods, namely, Q-Learning, Temporal Difference Advantage Actor-Critic and\nMulti-agent Temporal Difference Advantage Actor-Critic for optimising safety\nstock in a linear chain of independent agents. We find that RL can\nsimultaneously optimise both safety stock level and order quantity parameters\nof an inventory policy, unlike classical safety stock optimisation models where\nonly safety stock level is optimised while order quantity is predetermined\nbased on simple rules. This allows RL to model more complex supply chain\nprocurement behaviour. However, RL takes longer time to arrive at solutions,\nnecessitating future research on identifying and improving trade-offs between\nthe use of AI and mathematical models are needed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:02:01 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kosasih", "Edward Elson", ""], ["Brintrup", "Alexandra", ""]]}, {"id": "2107.01292", "submitter": "Geoffrey Pettet", "authors": "Geoffrey Pettet, Ayan Mukhopadhyay, Mykel J. Kochenderfer, and\n  Abhishek Dubey", "title": "Hierarchical Planning for Dynamic Resource Allocation in Smart and\n  Connected Communities", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.13300", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation under uncertainty is a classical problem in city-scale\ncyber-physical systems. Consider emergency response as an example; urban\nplanners and first responders optimize the location of ambulances to minimize\nexpected response times to incidents such as road accidents. Typically, such\nproblems deal with sequential decision-making under uncertainty and can be\nmodeled as Markov (or semi-Markov) decision processes. The goal of the\ndecision-maker is to learn a mapping from states to actions that can maximize\nexpected rewards. While online, offline, and decentralized approaches have been\nproposed to tackle such problems, scalability remains a challenge for\nreal-world use-cases. We present a general approach to hierarchical planning\nthat leverages structure in city-level CPS problems for resource allocation. We\nuse emergency response as a case study and show how a large resource allocation\nproblem can be split into smaller problems. We then use Monte-Carlo planning\nfor solving the smaller problems and managing the interaction between them.\nFinally, we use data from Nashville, Tennessee, a major metropolitan area in\nthe United States, to validate our approach. Our experiments show that the\nproposed approach outperforms state-of-the-art approaches used in the field of\nemergency response.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 22:10:49 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pettet", "Geoffrey", ""], ["Mukhopadhyay", "Ayan", ""], ["Kochenderfer", "Mykel J.", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2107.01347", "submitter": "Paolo Fazzini", "authors": "Paolo Fazzini, Isaac Wheeler, Francesco Petracchini", "title": "Traffic Signal Control with Communicative Deep Reinforcement Learning\n  Agents: a Case Study", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we theoretically and experimentally analyze Multi-Agent\nAdvantage Actor-Critic (MA2C) and Independent Advantage Actor-Critic (IA2C),\ntwo recently proposed multi-agent reinforcement learning methods that can be\napplied to control traffic signals in urban areas. The two methods differ in\ntheir use of a reward calculated locally or globally and in the management of\nagents' communication. We analyze the methods theoretically with the framework\nprovided by non-Markov decision processes, which provides useful insights in\nthe analysis of the algorithms. Moreover, we analyze the efficacy and the\nrobustness of the methods experimentally by testing them in two traffic areas\nin the Bologna (Italy) area, simulated by SUMO, a software tool. The\nexperimental results indicate that MA2C achieves the best performance in the\nmajority of cases, outperforms the alternative method considered, and displays\nsufficient stability during the learning process.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 05:12:03 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Fazzini", "Paolo", ""], ["Wheeler", "Isaac", ""], ["Petracchini", "Francesco", ""]]}, {"id": "2107.01460", "submitter": "Arnu Pretorius", "authors": "Arnu Pretorius, Kale-ab Tessera, Andries P. Smit, Claude Formanek, St\n  John Grimbly, Kevin Eloff, Siphelele Danisa, Lawrence Francis, Jonathan\n  Shock, Herman Kamper, Willie Brink, Herman Engelbrecht, Alexandre Laterre,\n  Karim Beguir", "title": "Mava: a research framework for distributed multi-agent reinforcement\n  learning", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breakthrough advances in reinforcement learning (RL) research have led to a\nsurge in the development and application of RL. To support the field and its\nrapid growth, several frameworks have emerged that aim to help the community\nmore easily build effective and scalable agents. However, very few of these\nframeworks exclusively support multi-agent RL (MARL), an increasingly active\nfield in itself, concerned with decentralised decision-making problems. In this\nwork, we attempt to fill this gap by presenting Mava: a research framework\nspecifically designed for building scalable MARL systems. Mava provides useful\ncomponents, abstractions, utilities and tools for MARL and allows for simple\nscaling for multi-process system training and execution, while providing a high\nlevel of flexibility and composability. Mava is built on top of DeepMind's Acme\n\\citep{hoffman2020acme}, and therefore integrates with, and greatly benefits\nfrom, a wide range of already existing single-agent RL components made\navailable in Acme. Several MARL baseline systems have already been implemented\nin Mava. These implementations serve as examples showcasing Mava's reusable\nfeatures, such as interchangeable system architectures, communication and\nmixing modules. Furthermore, these implementations allow existing MARL\nalgorithms to be easily reproduced and extended. We provide experimental\nresults for these implementations on a wide range of multi-agent environments\nand highlight the benefits of distributed system training.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 16:23:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pretorius", "Arnu", ""], ["Tessera", "Kale-ab", ""], ["Smit", "Andries P.", ""], ["Formanek", "Claude", ""], ["Grimbly", "St John", ""], ["Eloff", "Kevin", ""], ["Danisa", "Siphelele", ""], ["Francis", "Lawrence", ""], ["Shock", "Jonathan", ""], ["Kamper", "Herman", ""], ["Brink", "Willie", ""], ["Engelbrecht", "Herman", ""], ["Laterre", "Alexandre", ""], ["Beguir", "Karim", ""]]}, {"id": "2107.01496", "submitter": "Ming Li", "authors": "Ming Li, Pradeep K.Murukannaiah, Catholijn M.Jonker", "title": "A Data-Driven Method for Recognizing Automated Negotiation Strategies", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding an opponent agent helps in negotiating with it. Existing works\non understanding opponents focus on preference modeling (or estimating the\nopponent's utility function). An important but largely unexplored direction is\nrecognizing an opponent's negotiation strategy, which captures the opponent's\ntactics, e.g., to be tough at the beginning but to concede toward the deadline.\nRecognizing complex, state-of-the-art, negotiation strategies is extremely\nchallenging, and simple heuristics may not be adequate for this purpose. We\npropose a novel data-driven approach for recognizing an opponent's s\nnegotiation strategy. Our approach includes a data generation method for an\nagent to generate domain-independent sequences by negotiating with a variety of\nopponents across domains, a feature engineering method for representing\nnegotiation data as time series with time-step features and overall features,\nand a hybrid (recurrent neural network-based) deep learning method for\nrecognizing an opponent's strategy from the time series of bids. We perform\nextensive experiments, spanning four problem scenarios, to demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 20:43:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Li", "Ming", ""], ["Murukannaiah", "Pradeep K.", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "2107.01856", "submitter": "Michael Schlechtinger", "authors": "Michael Schlechtinger, Damaris Kosack, Heiko Paulheim, Thomas Fetzer", "title": "Winning at Any Cost -- Infringing the Cartel Prohibition With\n  Reinforcement Learning", "comments": "accepted at the 19th International Conference on Practical\n  Applications of Agents and Multi-Agent Systems (PAAMS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pricing decisions are increasingly made by AI. Thanks to their ability to\ntrain with live market data while making decisions on the fly, deep\nreinforcement learning algorithms are especially effective in taking such\npricing decisions. In e-commerce scenarios, multiple reinforcement learning\nagents can set prices based on their competitor's prices. Therefore, research\nstates that agents might end up in a state of collusion in the long run. To\nfurther analyze this issue, we build a scenario that is based on a modified\nversion of a prisoner's dilemma where three agents play the game of rock paper\nscissors. Our results indicate that the action selection can be dissected into\nspecific stages, establishing the possibility to develop collusion prevention\nsystems that are able to recognize situations which might lead to a collusion\nbetween competitors. We furthermore provide evidence for a situation where\nagents are capable of performing a tacit cooperation strategy without being\nexplicitly trained to do so.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:21:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Schlechtinger", "Michael", ""], ["Kosack", "Damaris", ""], ["Paulheim", "Heiko", ""], ["Fetzer", "Thomas", ""]]}, {"id": "2107.02361", "submitter": "Paolo Fazzini", "authors": "Paolo Fazzini, Marco Torre, Valeria Rizza and Francesco Petracchini", "title": "Effects of Smart Traffic Signal Control on Air Quality", "comments": "23 pages, 21 figures. arXiv admin note: substantial text overlap with\n  arXiv:2107.01347", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adaptive traffic signal control (ATSC) in urban traffic networks poses a\nchallenging task due to the complicated dynamics arising in traffic systems. In\nrecent years, several approaches based on multi-agent deep reinforcement\nlearning (MARL) have been studied experimentally. These approaches propose\ndistributed techniques in which each signalized intersection is seen as an\nagent in a stochastic game whose purpose is to optimize the flow of vehicles in\nits vicinity. In this setting, the systems evolves towards an equilibrium among\nthe agents that shows beneficial for the whole traffic network. A recently\ndeveloped multi-agent variant of the well-established advantage actor-critic\n(A2C) algorithm, called MA2C (multi-agent A2C) exploits the promising idea of\nsome communication among the agents. In this view,the agents share their\nstrategies with other neighbor agents, thereby stabilizing the learning process\neven when the agents grow in number and variety. We experimented MA2C in two\ntraffic networks located in Bologna (Italy) and found that its action\ntranslates into a significant decrease of the amount of pollutants released\ninto the environment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:48:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Fazzini", "Paolo", ""], ["Torre", "Marco", ""], ["Rizza", "Valeria", ""], ["Petracchini", "Francesco", ""]]}, {"id": "2107.03924", "submitter": "Mahmoud Nasr Mr", "authors": "Mahmoud Nasr, MD. Milon Islam, Shady Shehata, Fakhri Karray and Yuri\n  Quintana", "title": "Smart Healthcare in the Age of AI: Recent Advances, Challenges, and\n  Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant increase in the number of individuals with chronic ailments\n(including the elderly and disabled) has dictated an urgent need for an\ninnovative model for healthcare systems. The evolved model will be more\npersonalized and less reliant on traditional brick-and-mortar healthcare\ninstitutions such as hospitals, nursing homes, and long-term healthcare\ncenters. The smart healthcare system is a topic of recently growing interest\nand has become increasingly required due to major developments in modern\ntechnologies, especially in artificial intelligence (AI) and machine learning\n(ML). This paper is aimed to discuss the current state-of-the-art smart\nhealthcare systems highlighting major areas like wearable and smartphone\ndevices for health monitoring, machine learning for disease diagnosis, and the\nassistive frameworks, including social robots developed for the ambient\nassisted living environment. Additionally, the paper demonstrates software\nintegration architectures that are very significant to create smart healthcare\nsystems, integrating seamlessly the benefit of data analytics and other tools\nof AI. The explained developed systems focus on several facets: the\ncontribution of each developed framework, the detailed working procedure, the\nperformance as outcomes, and the comparative merits and limitations. The\ncurrent research challenges with potential future directions are addressed to\nhighlight the drawbacks of existing systems and the possible methods to\nintroduce novel frameworks, respectively. This review aims at providing\ncomprehensive insights into the recent developments of smart healthcare systems\nto equip experts to contribute to the field.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:10:47 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Nasr", "Mahmoud", ""], ["Islam", "MD. Milon", ""], ["Shehata", "Shady", ""], ["Karray", "Fakhri", ""], ["Quintana", "Yuri", ""]]}, {"id": "2107.04050", "submitter": "Barna Pasztor", "authors": "Barna Pasztor, Ilija Bogunovic, Andreas Krause", "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning", "comments": "28 pages, 2 figures, Preprint, Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in multi-agent systems is highly challenging due to the inherent\ncomplexity introduced by agents' interactions. We tackle systems with a huge\npopulation of interacting agents (e.g., swarms) via Mean-Field Control (MFC).\nMFC considers an asymptotically infinite population of identical agents that\naim to collaboratively maximize the collective reward. Specifically, we\nconsider the case of unknown system dynamics where the goal is to\nsimultaneously optimize for the rewards and learn from experience. We propose\nan efficient model-based reinforcement learning algorithm\n$\\text{M}^3\\text{-UCRL}$ that runs in episodes and provably solves this\nproblem. $\\text{M}^3\\text{-UCRL}$ uses upper-confidence bounds to balance\nexploration and exploitation during policy learning. Our main theoretical\ncontributions are the first general regret bounds for model-based RL for MFC,\nobtained via a novel mean-field type analysis. $\\text{M}^3\\text{-UCRL}$ can be\ninstantiated with different models such as neural networks or Gaussian\nProcesses, and effectively combined with neural network policy learning. We\nempirically demonstrate the convergence of $\\text{M}^3\\text{-UCRL}$ on the\nswarm motion problem of controlling an infinite population of agents seeking to\nmaximize location-dependent reward and avoid congested areas.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 18:01:02 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Pasztor", "Barna", ""], ["Bogunovic", "Ilija", ""], ["Krause", "Andreas", ""]]}, {"id": "2107.04078", "submitter": "Alaa Abdulghafoor", "authors": "Alaa Z.Abdulghafoor and Efstathios Bakolas", "title": "Distributed Coverage Control of Multi-Agent Networks with Guaranteed\n  Collision Avoidance in Cluttered Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed control algorithm for a multi-agent network whose\nagents deploy over a cluttered region in accordance with a time-varying\ncoverage density function while avoiding collisions with all obstacles they\nencounter. Our algorithm is built on a two-level characterization of the\nnetwork. The first level treats the multi-agent network as a whole based on the\ndistribution of the locations of its agents over the spatial domain. In the\nsecond level, the network is described in terms of the individual positions of\nits agents. The aim of the multi-agent network is to attain a spatial\ndistribution that resembles that of a reference coverage density function\n(high-level problem) by means of local (microscopic) interactions of its agents\n(low-level problem). In addition, as the agents deploy, they must avoid\ncollisions with all the obstacles in the region at all times. Our approach\nutilizes a modified version of Voronoi tessellations which are comprised of\nwhat we refer to as Obstacle-Aware Voronoi Cells (OAVC) in order to enable\ncoverage control while ensuring obstacle avoidance. We consider two control\nproblems. The first problem which we refer to as the high-level coverage\ncontrol problem corresponds to an interpolation problem in the class of\nGaussian mixtures (no collision avoidance requirement), which we solve\nanalytically. The second problem which we refer to as the low-level coverage\ncontrol problem corresponds to a distributed control problem (collision\navoidance requirement is now enforced at all times) which is solved by\nutilizing Lloyd's algorithm together with the modified Voronoi tessellation\n(OAVC) and a time-varying coverage density function which corresponds to the\nsolution of the high-level coverage control problem. Finally, simulation\nresults for coverage in a cluttered environment are provided to demonstrate the\nefficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 19:28:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Abdulghafoor", "Alaa Z.", ""], ["Bakolas", "Efstathios", ""]]}, {"id": "2107.04145", "submitter": "Joao Victor De Carvalho Evangelista", "authors": "Joao V.C. Evangelista, Zeeshan Sattar, Georges Kaddoum, Bassant Selim,\n  Aydin Sarraf", "title": "Intelligent Link Adaptation for Grant-Free Access Cellular Networks: A\n  Distributed Deep Reinforcement Learning Approach", "comments": "14 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.IT eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the continuous growth of machine-type devices (MTDs), it is expected\nthat massive machine-type communication (mMTC) will be the dominant form of\ntraffic in future wireless networks. Applications based on this technology,\nhave fundamentally different traffic characteristics from human-to-human (H2H)\ncommunication, which involves a relatively small number of devices transmitting\nlarge packets consistently. Conversely, in mMTC applications, a very large\nnumber of MTDs transmit small packets sporadically. Therefore, conventional\ngrant-based access schemes commonly adopted for H2H service, are not suitable\nfor mMTC, as they incur in a large overhead associated with the channel request\nprocedure. We propose three grant-free distributed optimization architectures\nthat are able to significantly minimize the average power consumption of the\nnetwork. The problem of physical layer (PHY) and medium access control (MAC)\noptimization in grant-free random access transmission is is modeled as a\npartially observable stochastic game (POSG) aimed at minimizing the average\ntransmit power under a per-device delay constraint. The results show that the\nproposed architectures are able to achieve significantly less average latency\nthan a baseline, while spending less power. Moreover, the proposed\narchitectures are more robust than the baseline, as they present less variance\nin the performance for different system realizations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 23:17:15 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Evangelista", "Joao V. C.", ""], ["Sattar", "Zeeshan", ""], ["Kaddoum", "Georges", ""], ["Selim", "Bassant", ""], ["Sarraf", "Aydin", ""]]}, {"id": "2107.04267", "submitter": "Georg J\\\"ager", "authors": "Georg J\\\"ager and Daniel Reisinger", "title": "Can We Replicate Real Human Behaviour Using Artificial Neural Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based modelling is a powerful tool when simulating human systems, yet\nwhen human behaviour cannot be described by simple rules or maximising one's\nown profit, we quickly reach the limits of this methodology. Machine learning\nhas the potential to bridge this gap by providing a link between what people\nobserve and how they act in order to reach their goal. In this paper we use a\nframework for agent-based modelling that utilizes human values like fairness,\nconformity and altruism. Using this framework we simulate a public goods game\nand compare to experimental results. We can report good agreement between\nsimulation and experiment and furthermore find that the presented framework\noutperforms strict reinforcement learning. Both the framework and the utility\nfunction are generic enough that they can be used for arbitrary systems, which\nmakes this method a promising candidate for a foundation of a universal\nagent-based model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:21:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["J\u00e4ger", "Georg", ""], ["Reisinger", "Daniel", ""]]}, {"id": "2107.04487", "submitter": "Sampo Kuutti", "authors": "Sampo Kuutti, Saber Fallah, Richard Bowden", "title": "ARC: Adversarially Robust Control Policies for Autonomous Vehicles", "comments": "Accepted in IEEE Intelligent Transportation Systems Conference (ITSC)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated their capability to learn control\npolicies for a variety of tasks. However, these neural network-based policies\nhave been shown to be susceptible to exploitation by adversarial agents.\nTherefore, there is a need to develop techniques to learn control policies that\nare robust against adversaries. We introduce Adversarially Robust Control\n(ARC), which trains the protagonist policy and the adversarial policy\nend-to-end on the same loss. The aim of the protagonist is to maximise this\nloss, whilst the adversary is attempting to minimise it. We demonstrate the\nproposed ARC training in a highway driving scenario, where the protagonist\ncontrols the follower vehicle whilst the adversary controls the lead vehicle.\nBy training the protagonist against an ensemble of adversaries, it learns a\nsignificantly more robust control policy, which generalises to a variety of\nadversarial strategies. The approach is shown to reduce the amount of\ncollisions against new adversaries by up to 90.25%, compared to the original\npolicy. Moreover, by utilising an auxiliary distillation loss, we show that the\nfine-tuned control policy shows no drop in performance across its original\ntraining distribution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:22:29 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kuutti", "Sampo", ""], ["Fallah", "Saber", ""], ["Bowden", "Richard", ""]]}, {"id": "2107.04685", "submitter": "Yongjie Yang", "authors": "Yongjie Yang and Jianxin Wang", "title": "Parameterized Complexity of Multi-winner Determination: More Effort\n  Towards Fixed-Parameter Tractability", "comments": "27 pages, 2 figures, AAMAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study winner determination for three prevalent $k$-committee selection\nrules, namely minimax approval voting (MAV), proportional approval voting\n(PAV), and Chamberlin-Courant's approval voting (CCAV). It is known that winner\ndetermination for these rules is NP-hard. Parameterized complexity of the\nproblem has been studied with respect to some natural parameters recently.\nHowever, there are still numerous parameters that have not been considered. We\nrevisit the parameterized complexity of winner determination by considering\nseveral important single parameters, combined parameters, and structural\nparameters, aiming at detecting more interesting parameters leading to\nfixed-parameter tractability res\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 21:21:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yang", "Yongjie", ""], ["Wang", "Jianxin", ""]]}, {"id": "2107.04897", "submitter": "Kai Zhang", "authors": "Kai Zhang, Yupeng Yang, Chengtao Xu, Dahai Liu, Houbing Song", "title": "Learning-to-Dispatch: Reinforcement Learning Based Flight Planning under\n  Emergency", "comments": "Accepted paper in the IEEE Intelligent Transportation Systems\n  Conference - ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effectiveness of resource allocation under emergencies especially\nhurricane disasters is crucial. However, most researchers focus on emergency\nresource allocation in a ground transportation system. In this paper, we\npropose Learning-to-Dispatch (L2D), a reinforcement learning (RL) based air\nroute dispatching system, that aims to add additional flights for hurricane\nevacuation while minimizing the airspace's complexity and air traffic\ncontroller's workload. Given a bipartite graph with weights that are learned\nfrom the historical flight data using RL in consideration of short- and\nlong-term gains, we formulate the flight dispatch as an online maximum weight\nmatching problem. Different from the conventional order dispatch problem, there\nis no actual or estimated index that can evaluate how the additional evacuation\nflights influence the air traffic complexity. Then we propose a multivariate\nreward function in the learning phase and compare it with other univariate\nreward designs to show its superior performance. The experiments using the\nreal-world dataset for Hurricane Irma demonstrate the efficacy and efficiency\nof our proposed schema.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 19:21:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Kai", ""], ["Yang", "Yupeng", ""], ["Xu", "Chengtao", ""], ["Liu", "Dahai", ""], ["Song", "Houbing", ""]]}, {"id": "2107.04924", "submitter": "Behzad Khamidehi", "authors": "Behzad Khamidehi and Elvino S. Sousa", "title": "Distributed Deep Reinforcement Learning for Intelligent Traffic\n  Monitoring with a Team of Aerial Robots", "comments": "IEEE International Conference on Intelligent Transportation -\n  ITSC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the traffic monitoring problem in a road network using a\nteam of aerial robots. The problem is challenging due to two main reasons.\nFirst, the traffic events are stochastic, both temporally and spatially.\nSecond, the problem has a non-homogeneous structure as the traffic events\narrive at different locations of the road network at different rates.\nAccordingly, some locations require more visits by the robots compared to other\nlocations. To address these issues, we define an uncertainty metric for each\nlocation of the road network and formulate a path planning problem for the\naerial robots to minimize the network's average uncertainty. We express this\nproblem as a partially observable Markov decision process (POMDP) and propose a\ndistributed and scalable algorithm based on deep reinforcement learning to\nsolve it. We consider two different scenarios depending on the communication\nmode between the agents (aerial robots) and the traffic management center\n(TMC). The first scenario assumes that the agents continuously communicate with\nthe TMC to send/receive real-time information about the traffic events. Hence,\nthe agents have global and real-time knowledge of the environment. However, in\nthe second scenario, we consider a challenging setting where the observation of\nthe aerial robots is partial and limited to their sensing ranges. Moreover, in\ncontrast to the first scenario, the information exchange between the aerial\nrobots and the TMC is restricted to specific time instances. We evaluate the\nperformance of our proposed algorithm in both scenarios for a real road network\ntopology and demonstrate its functionality in a traffic monitoring system.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 22:41:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Khamidehi", "Behzad", ""], ["Sousa", "Elvino S.", ""]]}, {"id": "2107.04926", "submitter": "Talha Kavuncu", "authors": "Talha Kavuncu, Ayberk Yaraneri, Negar Mehr", "title": "Potential iLQR: A Potential-Minimizing Controller for Planning\n  Multi-Agent Interactive Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotic applications involve interactions between multiple agents where\nan agent's decisions affect the behavior of other agents. Such behaviors can be\ncaptured by the equilibria of differential games which provide an expressive\nframework for modeling the agents' mutual influence. However, finding the\nequilibria of differential games is in general challenging as it involves\nsolving a set of coupled optimal control problems. In this work, we propose to\nleverage the special structure of multi-agent interactions to generate\ninteractive trajectories by simply solving a single optimal control problem,\nnamely, the optimal control problem associated with minimizing the potential\nfunction of the differential game. Our key insight is that for a certain class\nof multi-agent interactions, the underlying differential game is indeed a\npotential differential game for which equilibria can be found by solving a\nsingle optimal control problem. We introduce such an optimal control problem\nand build on single-agent trajectory optimization methods to develop a\ncomputationally tractable and scalable algorithm for planning multi-agent\ninteractive trajectories. We will demonstrate the performance of our algorithm\nin simulation and show that our algorithm outperforms the state-of-the-art game\nsolvers. To further show the real-time capabilities of our algorithm, we will\ndemonstrate the application of our proposed algorithm in a set of experiments\ninvolving interactive trajectories for two quadcopters.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 23:06:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kavuncu", "Talha", ""], ["Yaraneri", "Ayberk", ""], ["Mehr", "Negar", ""]]}, {"id": "2107.05138", "submitter": "S. Rasoul Etesami", "authors": "S. Rasoul Etesami", "title": "Open-Loop Equilibrium Strategies for Dynamic Influence Maximization Game\n  Over Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA cs.SY eess.SY math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of budget allocation for competitive influence\nmaximization over social networks. In this problem, multiple competing parties\n(players) want to distribute their limited advertising resources over a set of\nsocial individuals to maximize their long-run cumulative payoffs. It is assumed\nthat the individuals are connected via a social network and update their\nopinions based on the classical DeGroot model. The players must decide the\nbudget distribution among the individuals at a finite number of campaign times\nto maximize their overall payoff given as a function of individuals' opinions.\nWe show that i) the optimal investment strategy for the case of a single-player\ncan be found in polynomial time by solving a concave program, and ii) the\nopen-loop equilibrium strategies for the multiplayer dynamic game can be\ncomputed efficiently by following natural regret minimization dynamics. Our\nresults extend the earlier work on the static version of the problem to a\ndynamic multistage game.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 22:31:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Etesami", "S. Rasoul", ""]]}, {"id": "2107.05173", "submitter": "Balsam Alkouz", "authors": "Balsam Alkouz, Athman Bouguettaya", "title": "Provider-centric Allocation of Drone Swarm Services", "comments": "10 pages, 11 figures. This is an accepted paper and it is going to\n  appear in the Proceedings of the 2021 IEEE International Conference on Web\n  Services (ICWS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for the allocation of drone swarms for delivery\nservices known as Swarm-based Drone-as-a-Service (SDaaS). The allocation\nframework ensures minimum cost (aka maximum profit) to drone swarm providers\nwhile meeting the time requirement of service consumers. The constraints in the\ndelivery environment (e.g., limited recharging pads) are taken into\nconsideration. We propose three algorithms to select the best allocation of\ndrone swarms given a set of requests from multiple consumers. We conduct a set\nof experiments to evaluate and compare the efficiency of these algorithms\nconsidering the provider's profit, feasibility, requests fulfilment, and drones\nutilization level.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 03:23:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Alkouz", "Balsam", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2107.06362", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer and Elchanan Mossel and Madhu Sudan", "title": "Information Spread with Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GT cs.MA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the process of information dispersal in a network with communication\nerrors and local error-correction. Specifically we consider a simple model\nwhere a single bit of information initially known to a single source is\ndispersed through the network, and communication errors lead to differences in\nthe agents' opinions on this information.\n  Naturally, such errors can very quickly make the communication completely\nunreliable, and in this work we study to what extent this unreliability can be\nmitigated by local error-correction, where nodes periodically correct their\nopinion based on the opinion of (some subset of) their neighbors. We analyze\nhow the error spreads in the \"early stages\" of information dispersal by\nmonitoring the average opinion, i.e., the fraction of agents that have the\ncorrect information among all nodes that hold an opinion at a given time. Our\nmain results show that even with significant effort in error-correction, tiny\namounts of noise can lead the average opinion to be nearly uncorrelated with\nthe truth in early stages. We also propose some local methods to help agents\ngauge when the information they have has stabilized.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:58:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Mossel", "Elchanan", ""], ["Sudan", "Madhu", ""]]}, {"id": "2107.06548", "submitter": "Alaa Awad Abdellatif", "authors": "Alaa Awad Abdellatif, Naram Mhaisen, Amr Mohamed, Aiman Erbad, Mohsen\n  Guizani, Zaher Dawy, Wassim Nasreddine", "title": "Communication-Efficient Hierarchical Federated Learning for IoT\n  Heterogeneous Systems with Imbalanced Data", "comments": "A version of this work has been submitted in Transactions on Network\n  Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) is a distributed learning methodology that allows\nmultiple nodes to cooperatively train a deep learning model, without the need\nto share their local data. It is a promising solution for telemonitoring\nsystems that demand intensive data collection, for detection, classification,\nand prediction of future events, from different locations while maintaining a\nstrict privacy constraint. Due to privacy concerns and critical communication\nbottlenecks, it can become impractical to send the FL updated models to a\ncentralized server. Thus, this paper studies the potential of hierarchical FL\nin IoT heterogeneous systems and propose an optimized solution for user\nassignment and resource allocation on multiple edge nodes. In particular, this\nwork focuses on a generic class of machine learning models that are trained\nusing gradient-descent-based schemes while considering the practical\nconstraints of non-uniformly distributed data across different users. We\nevaluate the proposed system using two real-world datasets, and we show that it\noutperforms state-of-the-art FL solutions. In particular, our numerical results\nhighlight the effectiveness of our approach and its ability to provide 4-6%\nincrease in the classification accuracy, with respect to hierarchical FL\nschemes that consider distance-based user assignment. Furthermore, the proposed\napproach could significantly accelerate FL training and reduce communication\noverhead by providing 75-85% reduction in the communication rounds between edge\nnodes and the centralized server, for the same model accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:32:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Abdellatif", "Alaa Awad", ""], ["Mhaisen", "Naram", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Guizani", "Mohsen", ""], ["Dawy", "Zaher", ""], ["Nasreddine", "Wassim", ""]]}, {"id": "2107.06617", "submitter": "Mikhail Prokopenko", "authors": "Sheryl L. Chang, Oliver M. Cliff, Cameron Zachreson, Mikhail\n  Prokopenko", "title": "Nowcasting transmission and suppression of the Delta variant of\n  SARS-CoV-2 in Australia", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of July 2021, there is a continuing outbreak of the B.1.617.2 (Delta)\nvariant of SARS-CoV-2 in Sydney, Australia. The outbreak is of major concern as\nthe Delta variant is estimated to have twice the reproductive number to\nprevious variants that circulated in Australia in 2020, which is worsened by\nlow levels of acquired immunity in the population. Using a re-calibrated\nagent-based model, we explored a feasible range of non-pharmaceutical\ninterventions, in terms of both mitigation (case isolation, home quarantine)\nand suppression (school closures, social distancing). Our nowcasting modelling\nindicated that the level of social distancing currently attained in Sydney is\ninadequate for the outbreak control. A counter-factual analysis suggested that\nif 80% of agents comply with social distancing, then at least a month is needed\nfor the new daily cases to reduce from their peak to below ten. A small\nreduction in social distancing compliance to 70% lengthens this period to 45\ndays.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 11:36:00 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:55:25 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chang", "Sheryl L.", ""], ["Cliff", "Oliver M.", ""], ["Zachreson", "Cameron", ""], ["Prokopenko", "Mikhail", ""]]}, {"id": "2107.06696", "submitter": "Frank Schweitzer", "authors": "Frank Schweitzer, Georges Andres", "title": "Social nucleation: Group formation as a phase transition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.other cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spontaneous formation and subsequent growth, dissolution, merger and\ncompetition of social groups bears similarities to physical phase transitions\nin metastable finite systems. We examine three different scenarios,\npercolation, spinodal decomposition and nucleation, to describe the formation\nof social groups of varying size and density. In our agent-based model, we use\na feedback between the opinions of agents and their ability to establish links.\nGroups can restrict further link formation, but agents can also leave if costs\nexceed the group benefits. We identify the critical parameters for\ncosts/benefits and social influence to obtain either one large group or the\nstable coexistence of several groups with different opinions. Analytic\ninvestigations allow to derive different critical densities that control the\nformation and coexistence of groups. Our novel approach sheds new light on the\nearly stage of network growth and the emergence of large connected components.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:37:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Schweitzer", "Frank", ""], ["Andres", "Georges", ""]]}, {"id": "2107.06857", "submitter": "Joel Leibo", "authors": "Joel Z. Leibo, Edgar Du\\'e\\~nez-Guzm\\'an, Alexander Sasha Vezhnevets,\n  John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie,\n  Igor Mordatch, Thore Graepel", "title": "Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting\n  Pot", "comments": "Accepted to ICML 2021 and presented as a long talk; 33 pages; 9\n  figures", "journal-ref": "In International Conference on Machine Learning 2021 (pp.\n  6187-6199). PMLR", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing evaluation suites for multi-agent reinforcement learning (MARL) do\nnot assess generalization to novel situations as their primary objective\n(unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a\nMARL evaluation suite that fills this gap, and uses reinforcement learning to\nreduce the human labor required to create novel test scenarios. This works\nbecause one agent's behavior constitutes (part of) another agent's environment.\nTo demonstrate scalability, we have created over 80 unique test scenarios\ncovering a broad range of research topics such as social dilemmas, reciprocity,\nresource sharing, and task partitioning. We apply these test scenarios to\nstandard MARL training algorithms, and demonstrate how Melting Pot reveals\nweaknesses not apparent from training performance alone.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:22:14 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Leibo", "Joel Z.", ""], ["Du\u00e9\u00f1ez-Guzm\u00e1n", "Edgar", ""], ["Vezhnevets", "Alexander Sasha", ""], ["Agapiou", "John P.", ""], ["Sunehag", "Peter", ""], ["Koster", "Raphael", ""], ["Matyas", "Jayd", ""], ["Beattie", "Charles", ""], ["Mordatch", "Igor", ""], ["Graepel", "Thore", ""]]}, {"id": "2107.06866", "submitter": "Federica Adobbati", "authors": "Federica Adobbati, Luca Bernardinello and Lucia Pomello", "title": "Asynchronous games on Petri nets and ATL", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define a game on distributed Petri nets, where several players interact\nwith each other, and with an environment. The players, or users, have perfect\nknowledge of the current state, and pursue a common goal. Such goal is\nexpressed by Alternating-time Temporal Logic (ATL). The users have a winning\nstrategy if they can cooperate to reach their goal, no matter how the\nenvironment behaves. We show that such a game can be translated into a game on\nconcurrent game structures (introduced in order to give a semantics to ATL). We\ncompare our game with the game on concurrent game structures and discuss the\ndifferences between the two approaches. Finally, we show that, when we consider\nmemoryless strategies and a fragment of ATL, we can construct a concurrent game\nstructure from the Petri net, such that an ATL formula is verified on the net\nif, and only if, it is verified on the game structure.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:35:03 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Adobbati", "Federica", ""], ["Bernardinello", "Luca", ""], ["Pomello", "Lucia", ""]]}, {"id": "2107.07769", "submitter": "Anton Kolonin Dr.", "authors": "Ali Raheman, Anton Kolonin, Ben Goertzel, Gergely Hegykozi, Ikram\n  Ansari", "title": "Architecture of Automated Crypto-Finance Agent", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the cognitive architecture of an autonomous agent for active\nportfolio management in decentralized finance, involving activities such as\nasset selection, portfolio balancing, liquidity provision, and trading. Partial\nimplementation of the architecture is provided and supplied with preliminary\nresults and conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 08:57:50 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:22:47 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 16:58:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Raheman", "Ali", ""], ["Kolonin", "Anton", ""], ["Goertzel", "Ben", ""], ["Hegykozi", "Gergely", ""], ["Ansari", "Ikram", ""]]}, {"id": "2107.08083", "submitter": "Pablo Hernandez-Leal", "authors": "Yue Gao and Kry Yik Chau Lui and Pablo Hernandez-Leal", "title": "Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets", "comments": "Reinforcement Learning for Real Life (RL4RealLife) Workshop at ICML\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trading markets represent a real-world financial application to deploy\nreinforcement learning agents, however, they carry hard fundamental challenges\nsuch as high variance and costly exploration. Moreover, markets are inherently\na multiagent domain composed of many actors taking actions and changing the\nenvironment. To tackle these type of scenarios agents need to exhibit certain\ncharacteristics such as risk-awareness, robustness to perturbations and low\nlearning variance. We take those as building blocks and propose a family of\nfour algorithms. First, we contribute with two algorithms that use risk-averse\nobjective functions and variance reduction techniques. Then, we augment the\nframework to multi-agent learning and assume an adversary which can take over\nand perturb the learning process. Our third and fourth algorithms perform well\nunder this setting and balance theoretical guarantees with practical use.\nAdditionally, we consider the multi-agent nature of the environment and our\nwork is the first one extending empirical game theory analysis for multi-agent\nlearning by considering risk-sensitive payoffs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 19:15:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gao", "Yue", ""], ["Lui", "Kry Yik Chau", ""], ["Hernandez-Leal", "Pablo", ""]]}, {"id": "2107.08114", "submitter": "Ekram Hossain", "authors": "Yuanchao Xu, Amal Feriani, and Ekram Hossain", "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading\n  Under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 20:49:30 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 17:00:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Yuanchao", ""], ["Feriani", "Amal", ""], ["Hossain", "Ekram", ""]]}, {"id": "2107.08295", "submitter": "Christian Schroeder de Witt", "authors": "Samuel Sokota, Christian Schroeder de Witt, Maximilian Igl, Luisa\n  Zintgraf, Philip Torr, Shimon Whiteson, Jakob Foerster", "title": "Implicit Communication as Minimum Entropy Coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many common-payoff games, achieving good performance requires players to\ndevelop protocols for communicating their private information implicitly --\ni.e., using actions that have non-communicative effects on the environment.\nMulti-agent reinforcement learning practitioners typically approach this\nproblem using independent learning methods in the hope that agents will learn\nimplicit communication as a byproduct of expected return maximization.\nUnfortunately, independent learning methods are incapable of doing this in many\nsettings. In this work, we isolate the implicit communication problem by\nidentifying a class of partially observable common-payoff games, which we call\nimplicit referential games, whose difficulty can be attributed to implicit\ncommunication. Next, we introduce a principled method based on minimum entropy\ncoupling that leverages the structure of implicit referential games, yielding a\nnew perspective on implicit communication. Lastly, we show that this method can\ndiscover performant implicit communication protocols in settings with very\nlarge spaces of messages.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 17:44:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Sokota", "Samuel", ""], ["de Witt", "Christian Schroeder", ""], ["Igl", "Maximilian", ""], ["Zintgraf", "Luisa", ""], ["Torr", "Philip", ""], ["Whiteson", "Shimon", ""], ["Foerster", "Jakob", ""]]}, {"id": "2107.08408", "submitter": "Ashutosh Modi", "authors": "Ishika Singh and Gargi Singh and Ashutosh Modi", "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based\n  Games", "comments": "55 Pages (8 Pages main content + 2 Pages references + 45 Pages\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires understanding and interaction\nusing natural language in a partially observable environment. In this paper, we\nimprove the semantic understanding of the agent by proposing a simple RL with\nLM framework where we use transformer-based language models with Deep RL\nmodels. We perform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1, to achieve a\nscore of 44.7, which is 1.6 higher than the state-of-the-art model. Our\nproposed approach also performs comparably to the state-of-the-art models on\nthe other set of text games.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 10:28:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Singh", "Ishika", ""], ["Singh", "Gargi", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2107.08834", "submitter": "Conrad Sanderson", "authors": "Xiaolong Zhu, Fernando Vanegas, Felipe Gonzalez, Conrad Sanderson", "title": "A Multi-UAV System for Exploration and Target Finding in Cluttered and\n  GPS-Denied Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue\nas well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,\nhave limited endurance. The range of UAV applications can be widened if teams\nof multiple UAVs are used. We propose a framework for a team of UAVs to\ncooperatively explore and find a target in complex GPS-denied environments with\nobstacles. The team of UAVs autonomously navigates, explores, detects, and\nfinds the target in a cluttered environment with a known map. Examples of such\nenvironments include indoor scenarios, urban or natural canyons, caves, and\ntunnels, where the GPS signal is limited or blocked. The framework is based on\na probabilistic decentralised Partially Observable Markov Decision Process\nwhich accounts for the uncertainties in sensing and the environment. The team\ncan cooperate efficiently, with each UAV sharing only limited processed\nobservations and their locations during the mission. The system is simulated\nusing the Robotic Operating System and Gazebo. Performance of the system with\nan increasing number of UAVs in several indoor scenarios with obstacles is\ntested. Results indicate that the proposed multi-UAV system has improvements in\nterms of time-cost, the proportion of search area surveyed, as well as\nsuccessful rates for search and rescue missions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 12:54:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhu", "Xiaolong", ""], ["Vanegas", "Fernando", ""], ["Gonzalez", "Felipe", ""], ["Sanderson", "Conrad", ""]]}, {"id": "2107.08959", "submitter": "Matthew Sun", "authors": "Eli Lucherini, Matthew Sun, Amy Winecoff, Arvind Narayanan", "title": "T-RECS: A Simulation Tool to Study the Societal Impact of Recommender\n  Systems", "comments": "17 pages, 5 figures; updated Figure 2(b) after fixing small bug in\n  replication code (see Github for more details)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation has emerged as a popular method to study the long-term societal\nconsequences of recommender systems. This approach allows researchers to\nspecify their theoretical model explicitly and observe the evolution of\nsystem-level outcomes over time. However, performing simulation-based studies\noften requires researchers to build their own simulation environments from the\nground up, which creates a high barrier to entry, introduces room for\nimplementation error, and makes it difficult to disentangle whether observed\noutcomes are due to the model or the implementation.\n  We introduce T-RECS, an open-sourced Python package designed for researchers\nto simulate recommendation systems and other types of sociotechnical systems in\nwhich an algorithm mediates the interactions between multiple stakeholders,\nsuch as users and content creators. To demonstrate the flexibility of T-RECS,\nwe perform a replication of two prior simulation-based research on\nsociotechnical systems. We additionally show how T-RECS can be used to generate\nnovel insights with minimal overhead. Our tool promotes reproducibility in this\narea of research, provides a unified language for simulating sociotechnical\nsystems, and removes the friction of implementing simulations from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:16:44 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 00:52:04 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lucherini", "Eli", ""], ["Sun", "Matthew", ""], ["Winecoff", "Amy", ""], ["Narayanan", "Arvind", ""]]}, {"id": "2107.09119", "submitter": "Muhammad Najib", "authors": "Julian Gutierrez, Lewis Hammond, Anthony W. Lin, Muhammad Najib,\n  Michael Wooldridge", "title": "Rational Verification for Probabilistic Systems", "comments": "18th International Conference on Principles of Knowledge\n  Representation and Reasoning (KR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Rational verification is the problem of determining which temporal logic\nproperties will hold in a multi-agent system, under the assumption that agents\nin the system act rationally, by choosing strategies that collectively form a\ngame-theoretic equilibrium. Previous work in this area has largely focussed on\ndeterministic systems. In this paper, we develop the theory and algorithms for\nrational verification in probabilistic systems. We focus on concurrent\nstochastic games (CSGs), which can be used to model uncertainty and randomness\nin complex multi-agent environments. We study the rational verification problem\nfor both non-cooperative games and cooperative games in the qualitative\nprobabilistic setting. In the former case, we consider LTL properties satisfied\nby the Nash equilibria of the game and in the latter case LTL properties\nsatisfied by the core. In both cases, we show that the problem is\n2EXPTIME-complete, thus not harder than the much simpler verification problem\nof model checking LTL properties of systems modelled as Markov decision\nprocesses (MDPs).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:24:16 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 09:52:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gutierrez", "Julian", ""], ["Hammond", "Lewis", ""], ["Lin", "Anthony W.", ""], ["Najib", "Muhammad", ""], ["Wooldridge", "Michael", ""]]}, {"id": "2107.09232", "submitter": "Kenta Hongo", "authors": "Keishu Utimula, Ken-taro Hayaschi, Kousuke Nakano, Kenta Hongo, Ryo\n  Maezono", "title": "Reinforcement learning autonomously identifying the source of errors for\n  agents in a group mission", "comments": "4 pages, 1 figure. References added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When agents are swarmed to carry out a mission, there is often a sudden\nfailure of some of the agents observed from the command base. It is generally\ndifficult to distinguish whether the failure is caused by actuators\n(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication\nbetween the command base and the concerning agent. By making a collision to the\nagent by another, we would be able to distinguish which hypothesis is likely:\nFor $h_a$, we expect to detect corresponding displacements while for $h_a$ we\ndo not. Such swarm strategies to grasp the situation are preferably to be\ngenerated autonomously by artificial intelligence (AI). Preferable actions\n($e.g.$, the collision) for the distinction would be those maximizing the\ndifference between the expected behaviors for each hypothesis, as a value\nfunction. Such actions exist, however, only very sparsely in the whole\npossibilities, for which the conventional search based on gradient methods does\nnot make sense. Instead, we have successfully applied the reinforcement\nlearning technique, achieving the maximization of such a sparse value function.\nThe machine learning actually concluded autonomously the colliding action to\ndistinguish the hypothesises. Getting recognized an agent with actuator error\nby the action, the agents behave as if other ones want to assist the\nmalfunctioning one to achieve a given mission.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 02:40:19 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 01:20:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Utimula", "Keishu", ""], ["Hayaschi", "Ken-taro", ""], ["Nakano", "Kousuke", ""], ["Hongo", "Kenta", ""], ["Maezono", "Ryo", ""]]}, {"id": "2107.09292", "submitter": "Haibin Shao", "authors": "Lulu Pan, Haibin Shao, Mehran Mesbahi, Dewei Li, Yugeng Xi", "title": "Cluster Consensus on Matrix-weighted Switching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the cluster consensus problem of multi-agent systems on\nmatrix-weighted switching networks. Necessary and/or sufficient conditions\nunder which cluster consensus can be achieved are obtained and quantitative\ncharacterization of the steady-state of the cluster consensus are provided as\nwell. Specifically, if the underlying network switches amongst finite number of\nnetworks, a necessary condition for cluster consensus of multi-agent system on\nswitching matrix-weighted networks is firstly presented, it is shown that the\nsteady-state of the system lies in the intersection of the null space of\nmatrix-valued Laplacians corresponding to all switching networks. Second, if\nthe underlying network switches amongst infinite number of networks, the\nmatrix-weighted integral network is employed to provide sufficient conditions\nfor cluster consensus and the quantitative characterization of the\ncorresponding steady-state of the multi-agent system, using null space analysis\nof matrix-valued Laplacian related of integral network associated with the\nswitching networks. In particular, conditions for the bipartite consensus under\nthe matrix-weighted switching networks are examined. Simulation results are\nfinally provided to demonstrate the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:20:23 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:09:29 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pan", "Lulu", ""], ["Shao", "Haibin", ""], ["Mesbahi", "Mehran", ""], ["Li", "Dewei", ""], ["Xi", "Yugeng", ""]]}, {"id": "2107.09598", "submitter": "Tim Franzmeyer", "authors": "Tim Franzmeyer, Mateusz Malinowski and Jo\\~ao F. Henriques", "title": "Learning Altruistic Behaviours in Reinforcement Learning without\n  External Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can artificial agents learn to assist others in achieving their goals without\nknowing what those goals are? Generic reinforcement learning agents could be\ntrained to behave altruistically towards others by rewarding them for\naltruistic behaviour, i.e., rewarding them for benefiting other agents in a\ngiven situation. Such an approach assumes that other agents' goals are known so\nthat the altruistic agent can cooperate in achieving those goals. However,\nexplicit knowledge of other agents' goals is often difficult to acquire. Even\nassuming such knowledge to be given, training of altruistic agents would\nrequire manually-tuned external rewards for each new environment. Thus, it is\nbeneficial to develop agents that do not depend on external supervision and can\nlearn altruistic behaviour in a task-agnostic manner. Assuming that other\nagents rationally pursue their goals, we hypothesize that giving them more\nchoices will allow them to pursue those goals better. Some concrete examples\ninclude opening a door for others or safeguarding them to pursue their\nobjectives without interference. We formalize this concept and propose an\naltruistic agent that learns to increase the choices another agent has by\nmaximizing the number of states that the other agent can reach in its future.\nWe evaluate our approach on three different multi-agent environments where\nanother agent's success depends on the altruistic agent's behaviour. Finally,\nwe show that our unsupervised agents can perform comparably to agents\nexplicitly trained to work cooperatively. In some cases, our agents can even\noutperform the supervised ones.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:19:39 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 06:43:03 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Franzmeyer", "Tim", ""], ["Malinowski", "Mateusz", ""], ["Henriques", "Jo\u00e3o F.", ""]]}, {"id": "2107.09707", "submitter": "David Lajeunesse", "authors": "David Lajeunesse and Hugo D. Scolnik", "title": "A Cooperative Optimal Mining Model for Bitcoin", "comments": "8 pages, 2 figures, Accepted to 2021 3rd Conference on Blockchain\n  Research & Applications for Innovative Networks and Services (BRAINS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze Bitcoin mining from the perspective of a game and propose an\noptimal mining model that maximizes profits of pools and miners. The model is a\ntwo-stage Stackelberg game in which each stage forms a sub-game. In stage I,\npools are the leaders who assign a computing power to be consumed by miners. In\nstage II, miners decide of their power consumption and distribution. They find\nthemselves in a social dilemma in which they must choose between mining in\nsolo, therefore prioritizing their individual preferences, and participating in\na pool for the collective interest. The model relies on a pool protocol based\non a simulated game in which the miners compete for the reward won by the pool.\nThe solutions for the stage I sub-game and the simulated protocol game are\nunique and stable Nash equilibriums while the stage II sub-game leads to a\nstable cooperative equilibrium only when miners choose their strategies\naccording to certain criteria. We conclude that the cooperative optimal mining\nmodel has the potential to favor Bitcoin decentralization and stability.\nMainly, the social dilemma faced by miners together with the balance of\nincentives ensure a certain distribution of the network computing power between\npools and solo miners, while equilibriums in the game solutions provide\nstability to the system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:20:20 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lajeunesse", "David", ""], ["Scolnik", "Hugo D.", ""]]}, {"id": "2107.09807", "submitter": "Amin Nikanjam", "authors": "Mahnoosh Mahdavimoghaddam, Amin Nikanjam, Monireh Abdoos", "title": "Multi-agent Reinforcement Learning Improvement in a Dynamic Environment\n  Using Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative multi-agent systems are being widely used in variety of areas.\nInteraction between agents would bring positive points, including reducing\ncosts of operating, high scalability, and facilitating parallel processing.\nThese systems pave the way for handling large-scale, unknown, and dynamic\nenvironments. However, learning in these environments has become a prominent\nchallenge in different applications. These challenges include the effect of\nsize of search space on learning time, inappropriate cooperation among agents,\nand the lack of proper coordination among agents' decisions. Moreover,\nreinforcement learning algorithms may suffer from long time of convergence in\nthese problems. In this paper, a communication framework using knowledge\ntransfer concepts is introduced to address such challenges in the herding\nproblem with large state space. To handle the problems of convergence,\nknowledge transfer has been utilized that can significantly increase the\nefficiency of reinforcement learning algorithms. Coordination between the\nagents is carried out through a head agent in each group of agents and a\ncoordinator agent respectively. The results demonstrate that this framework\ncould indeed enhance the speed of learning and reduce convergence time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 23:42:39 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 14:17:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mahdavimoghaddam", "Mahnoosh", ""], ["Nikanjam", "Amin", ""], ["Abdoos", "Monireh", ""]]}, {"id": "2107.09888", "submitter": "Venkata Sriram Siddhardh Nadendla", "authors": "Qizi Zhang and Venkata Sriram Siddhardh Nadendla and S. N.\n  Balakrishnan and Jerome Busemeyer", "title": "Strategic Mitigation of Agent Inattention in Drivers with Open-Quantum\n  Cognition Models", "comments": "12 pages, 4 figures, submitted to IEEE Transactions on Human-Machine\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.GT cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art driver-assist systems have failed to effectively mitigate\ndriver inattention and had minimal impacts on the ever-growing number of road\nmishaps (e.g. life loss, physical injuries due to accidents caused by various\nfactors that lead to driver inattention). This is because traditional\nhuman-machine interaction settings are modeled in classical and behavioral\ngame-theoretic domains which are technically appropriate to characterize\nstrategic interaction between either two utility maximizing agents, or human\ndecision makers. Therefore, in an attempt to improve the persuasive\neffectiveness of driver-assist systems, we develop a novel strategic and\npersonalized driver-assist system which adapts to the driver's mental state and\nchoice behavior. First, we propose a novel equilibrium notion in human-system\ninteraction games, where the system maximizes its expected utility and human\ndecisions can be characterized using any general decision model. Then we use\nthis novel equilibrium notion to investigate the strategic driver-vehicle\ninteraction game where the car presents a persuasive recommendation to steer\nthe driver towards safer driving decisions. We assume that the driver employs\nan open-quantum system cognition model, which captures complex aspects of human\ndecision making such as violations to classical law of total probability and\nincompatibility of certain mental representations of information. We present\nclosed-form expressions for players' final responses to each other's strategies\nso that we can numerically compute both pure and mixed equilibria. Numerical\nresults are presented to illustrate both kinds of equilibria.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 06:02:03 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Qizi", ""], ["Nadendla", "Venkata Sriram Siddhardh", ""], ["Balakrishnan", "S. N.", ""], ["Busemeyer", "Jerome", ""]]}, {"id": "2107.09918", "submitter": "Julian Bernhard", "authors": "Julian Bernhard, Patrick Hart, Amit Sahu, Christoph Sch\\\"oller,\n  Michell Guzman Cancimance", "title": "Risk-Based Safety Envelopes for Autonomous Vehicles Under Perception\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ensuring the safety of autonomous vehicles, given the uncertainty in sensing\nother road users, is an open problem. Moreover, separate safety specifications\nfor perception and planning components raise how to assess the overall system\nsafety. This work provides a probabilistic approach to calculate safety\nenvelopes under perception uncertainty. The probabilistic envelope definition\nis based on a risk threshold. It limits the cumulative probability that the\nactual safety envelope in a fully observable environment is larger than an\napplied envelope and is solved using iterative worst-case analysis of\nenvelopes. Our approach extends non-probabilistic envelopes - in this work, the\nResponsibility-Sensitive Safety (RSS) - to handle uncertainties. To evaluate\nour probabilistic envelope approach, we compare it in a simulated highway\nmerging scenario against several baseline safety architectures. Our evaluation\nshows that our model allows adjusting safety and performance based on a chosen\nrisk level and the amount of perception uncertainty. We conclude with an\noutline of how to formally argue safety under perception uncertainty using our\nformulation of envelope violation risk.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:36:50 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bernhard", "Julian", ""], ["Hart", "Patrick", ""], ["Sahu", "Amit", ""], ["Sch\u00f6ller", "Christoph", ""], ["Cancimance", "Michell Guzman", ""]]}, {"id": "2107.09973", "submitter": "Mirco Theile", "authors": "Mirco Theile, Jonathan Ponniah, Or Dantsker, Marco Caccamo", "title": "Multi-Agent Belief Sharing through Autonomous Hierarchical Multi-Level\n  Clustering", "comments": "Submitted to IEEE Transactions on Robotics, article extends on\n  https://doi.org/10.2514/6.2021-0656", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination in multi-agent systems is challenging for agile robots such as\nunmanned aerial vehicles (UAVs), where relative agent positions frequently\nchange due to unconstrained movement. The problem is exacerbated through the\nindividual take-off and landing of agents for battery recharging leading to a\nvarying number of active agents throughout the whole mission. This work\nproposes autonomous hierarchical multi-level clustering (MLC), which forms a\nclustering hierarchy utilizing decentralized methods. Through periodic cluster\nmaintenance executed by cluster-heads, stable multi-level clustering is\nachieved. The resulting hierarchy is used as a backbone to solve the\ncommunication problem for locally-interactive applications such as UAV tracking\nproblems. Using observation aggregation, compression, and dissemination, agents\nshare local observations throughout the hierarchy, giving every agent a total\nsystem belief with spatially dependent resolution and freshness. Extensive\nsimulations show that MLC yields a stable cluster hierarchy under different\nmotion patterns and that the proposed belief sharing is highly applicable in\nwildfire front monitoring scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:37:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Theile", "Mirco", ""], ["Ponniah", "Jonathan", ""], ["Dantsker", "Or", ""], ["Caccamo", "Marco", ""]]}, {"id": "2107.10121", "submitter": "Stanislav Zhydkov", "authors": "Omer Lev, Nicholas Mattei, Paolo Turrini, Stanislav Zhydkov", "title": "Peer Selection with Noisy Assessments", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the peer selection problem a group of agents must select a subset of\nthemselves as winners for, e.g., peer-reviewed grants or prizes. Here, we take\na Condorcet view of this aggregation problem, i.e., that there is a\nground-truth ordering over the agents and we wish to select the best set of\nagents, subject to the noisy assessments of the peers. Given this model, some\nagents may be unreliable, while others might be self-interested, attempting to\ninfluence the outcome in their favour. In this paper we extend PeerNomination,\nthe most accurate peer reviewing algorithm to date, into\nWeightedPeerNomination, which is able to handle noisy and inaccurate agents. To\ndo this, we explicitly formulate assessors' reliability weights in a way that\ndoes not violate strategyproofness, and use this information to reweight their\nscores. We show analytically that a weighting scheme can improve the overall\naccuracy of the selection significantly. Finally, we implement several\ninstances of reweighting methods and show empirically that our methods are\nrobust in the face of noisy assessments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 14:47:11 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lev", "Omer", ""], ["Mattei", "Nicholas", ""], ["Turrini", "Paolo", ""], ["Zhydkov", "Stanislav", ""]]}, {"id": "2107.11399", "submitter": "Juste Raimbault", "authors": "Thibaut Barbet, Amine Nacer-Weill, Changtao Yang, Juste Raimbault", "title": "An agent-based model for modal shift in public transport", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal shift in public transport as a consequence of a disruption on a line\nhas in some cases unforeseen consequences such as an increase in congestion in\nthe rest of the network. How information is provided to users and their\nbehavior plays a central role in such configurations. We introduce here a\nsimple and stylised agent-based model aimed at understanding the impact of\nbehavioural parameters on modal shift. The model is applied on a case study\nbased on a stated preference survey for a segment of Paris suburban train\nnetwork. We systematically explore the parameter space and show non-trivial\npatterns of congestion for some values of discrete choice parameters linked to\nperceived wait time and congestion. We also apply a genetic optimisation\nalgorithm to the model to search for optimal compromises between congestion in\ndifferent modes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:02:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Barbet", "Thibaut", ""], ["Nacer-Weill", "Amine", ""], ["Yang", "Changtao", ""], ["Raimbault", "Juste", ""]]}, {"id": "2107.11728", "submitter": "Fengjiao Li", "authors": "Fengjiao Li, Jia Liu, and Bo Ji", "title": "Federated Learning with Fair Worker Selection: A Multi-Round Submodular\n  Maximization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 05:17:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Fengjiao", ""], ["Liu", "Jia", ""], ["Ji", "Bo", ""]]}, {"id": "2107.12022", "submitter": "Haibin Shao", "authors": "Haibin Shao, Lulu Pan, Mehran Mesbahi, Yugeng Xi, Dewei Li", "title": "Distributed Neighbor Selection in Multi-agent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving consensus via nearest neighbor rules is an important prerequisite\nfor multi-agent networks to accomplish collective tasks. A common assumption in\nconsensus setup is that each agent interacts with all its neighbors during the\nprocess. This paper examines whether network functionality and performance can\nbe maintained-and even enhanced-when agents interact only with a subset of\ntheir respective (available) neighbors. As shown in the paper, the answer to\nthis inquiry is affirmative. In this direction, we show that by using the\nmonotonicity property of the Laplacian eigenvectors, a neighbor selection rule\nwith guaranteed performance enhancements, can be realized for consensus-type\nnetworks. For the purpose of distributed implementation, a quantitative\nconnection between Laplacian eigenvectors and the \"relative rate of change\" in\nthe state between neighboring agents is further established; this connection\nfacilitates a distributed algorithm for each agent to identify \"favorable\"\nneighbors to interact with. Multi-agent networks with and without external\ninfluence are examined, as well as extensions to signed networks. This paper\nunderscores the utility of Laplacian eigenvectors in the context of distributed\nneighbor selection, providing novel insights into distributed data-driven\ncontrol of multi-agent systems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:23:04 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shao", "Haibin", ""], ["Pan", "Lulu", ""], ["Mesbahi", "Mehran", ""], ["Xi", "Yugeng", ""], ["Li", "Dewei", ""]]}, {"id": "2107.12254", "submitter": "Amanda Prorok", "authors": "Amanda Prorok, Jan Blumenkamp, Qingbiao Li, Ryan Kortvelesy, Zhe Liu,\n  Ethan Stump", "title": "The Holy Grail of Multi-Robot Planning: Learning to Generate\n  Online-Scalable Solutions from Offline-Optimal Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many multi-robot planning problems are burdened by the curse of\ndimensionality, which compounds the difficulty of applying solutions to\nlarge-scale problem instances. The use of learning-based methods in multi-robot\nplanning holds great promise as it enables us to offload the online\ncomputational burden of expensive, yet optimal solvers, to an offline learning\nprocedure. Simply put, the idea is to train a policy to copy an optimal pattern\ngenerated by a small-scale system, and then transfer that policy to much larger\nsystems, in the hope that the learned strategy scales, while maintaining\nnear-optimal performance. Yet, a number of issues impede us from leveraging\nthis idea to its full potential. This blue-sky paper elaborates some of the key\nchallenges that remain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:59:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Prorok", "Amanda", ""], ["Blumenkamp", "Jan", ""], ["Li", "Qingbiao", ""], ["Kortvelesy", "Ryan", ""], ["Liu", "Zhe", ""], ["Stump", "Ethan", ""]]}, {"id": "2107.12450", "submitter": "Mostafa Safi", "authors": "Mostafa Safi and Seyed Mehran Dibaji", "title": "Resilient Distributed Averaging", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fully distributed averaging algorithm in the presence of\nadversarial Byzantine agents is proposed. The algorithm is based on a resilient\nretrieval procedure, where all non-Byzantine nodes send their own initial\nvalues and retrieve those of other agents. We establish that the convergence of\nthe proposed algorithm relies on strong robustness of the graph for locally\nbounded adversaries. A topology analysis in terms of time complexity and\nrelation between connectivity metrics is also presented. Simulation results are\nprovided to verify the effectiveness of the proposed algorithms under\nprescribed graph conditions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 19:39:41 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Safi", "Mostafa", ""], ["Dibaji", "Seyed Mehran", ""]]}, {"id": "2107.12596", "submitter": "Peihu Duan", "authors": "Peihu Duan and Lidong He and Zhisheng Duan and Ling Shi", "title": "Fully Distributed LQR-based Controller Design for Multi-input\n  Time-varying Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a cooperative Linear Quadratic Regulator (LQR) problem is\ninvestigated for multi-input systems, where each input is generated by an agent\nin a network. The input matrices are different and locally possessed by the\ncorresponding agents respectively, which can be regarded as different ways for\nagents to control the multi-input system. By embedding a fully distributed\ninformation fusion strategy, a novel cooperative LQR-based controller is\nproposed. Each agent only needs to communicate with its neighbors, rather than\nsharing information globally in a network. Moreover, only the joint\ncontrollability is required, which allows the multi-input system to be\nuncontrollable for every single agent or even all its neighbors. In particular,\nonly one-time information exchange is necessary at every control step, which\nsignificantly reduces the communication consumption. It is proved that the\nboundedness (convergence) of the controller gains is guaranteed for\ntime-varying (time-invariant) systems. Furthermore, the control performance of\nthe entire system is ensured. Generally, the proposed controller achieves a\nbetter trade-off between the control performance and the communication\noverhead, compared with the existing centralized/decentralized/consensus-based\nLQR controllers. Finally, the effectiveness of the theoretical results is\nillustrated by several comparative numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:53:50 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Duan", "Peihu", ""], ["He", "Lidong", ""], ["Duan", "Zhisheng", ""], ["Shi", "Ling", ""]]}, {"id": "2107.12808", "submitter": "Wojciech Czarnecki", "authors": "Open-Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros,\n  Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg,\n  Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong,\n  Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard,\n  Wojciech Marian Czarnecki", "title": "Open-Ended Learning Leads to Generally Capable Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:30:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ended Learning Team", "", ""], ["Stooke", "Adam", ""], ["Mahajan", "Anuj", ""], ["Barros", "Catarina", ""], ["Deck", "Charlie", ""], ["Bauer", "Jakob", ""], ["Sygnowski", "Jakub", ""], ["Trebacz", "Maja", ""], ["Jaderberg", "Max", ""], ["Mathieu", "Michael", ""], ["McAleese", "Nat", ""], ["Bradley-Schmieg", "Nathalie", ""], ["Wong", "Nathaniel", ""], ["Porcel", "Nicolas", ""], ["Raileanu", "Roberta", ""], ["Hughes-Fitt", "Steph", ""], ["Dalibard", "Valentin", ""], ["Czarnecki", "Wojciech Marian", ""]]}, {"id": "2107.12906", "submitter": "Edvin Wedin", "authors": "Edvin Wedin", "title": "A rigorous formulation of and partial results on Lorenz's \"consensus\n  strikes back\" phenomenon for the Hegselmann-Krause model", "comments": "37 pages, 2 figures. See the README file for instructions on how to\n  run the code in the ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.MA cs.SY eess.SY math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a 2006 paper, Jan Lorenz observed a curious behaviour in numerical\nsimulations of the Hegselmann-Krause model: Under some circumstances, making\nagents more closed-minded can produce a consensus from a dense configuration of\nopinions which otherwise leads to fragmentation. Suppose one considers initial\nopinions equally spaced on an interval of length $L$. As first observed by\nLorenz, simulations suggest that there are three intervals $[0, L_1)$, $(L_1,\nL_2)$ and $(L_2, L_3)$, with $L_1 \\approx 5.23$, $L_2 \\approx 5.67$ and $L_3\n\\approx 6.84$ such that, when the number of agents is sufficiently large,\nconsensus occurs in the first and third intervals, whereas for the second\ninterval the system fragments into three clusters. In this paper, we prove\nconsensus for $L \\leq 5.2$ and for $L$ sufficiently close to 6. These proofs\ninclude large computations and in principle the set of $L$ for which consensus\ncan be proven using our approach may be extended with the use of more computing\npower. We also prove that the set of $L$ for which consensus occurs is open.\nMoreover, we prove that, when consensus is assured for the equally spaced\nsystems, this in turn implies asymptotic almost sure consensus for the same\nvalues of $L$ when initial opinions are drawn independently and uniformly at\nrandom. We thus conjecture a pair of phase transitions, making precise the\nformulation of Lorenz's \"consensus strikes back\" hypothesis. Our approach makes\nuse of the continuous agent model introduced by Blondel, Hendrickx and\nTsitsiklis. Indeed, one contribution of the paper is to provide a presentation\nof the relationships between the three different models with equally spaced,\nuniformly random and continuous agents, respectively, which is more rigorous\nthan what can be found in the existing literature.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:50:55 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wedin", "Edvin", ""]]}, {"id": "2107.13252", "submitter": "Bang Xiang Yong", "authors": "Bang Xiang Yong and Alexandra Brintrup", "title": "Multi Agent System for Machine Learning Under Uncertainty in Cyber\n  Physical Manufacturing System", "comments": "International Workshop on Service Orientation in Holonic and\n  Multi-Agent Manufacturing", "journal-ref": null, "doi": "10.17863/CAM.51696", "report-no": null, "categories": "cs.MA cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in predictive machine learning has led to its application\nin various use cases in manufacturing. Most research focused on maximising\npredictive accuracy without addressing the uncertainty associated with it.\nWhile accuracy is important, focusing primarily on it poses an overfitting\ndanger, exposing manufacturers to risk, ultimately hindering the adoption of\nthese techniques. In this paper, we determine the sources of uncertainty in\nmachine learning and establish the success criteria of a machine learning\nsystem to function well under uncertainty in a cyber-physical manufacturing\nsystem (CPMS) scenario. Then, we propose a multi-agent system architecture\nwhich leverages probabilistic machine learning as a means of achieving such\ncriteria. We propose possible scenarios for which our proposed architecture is\nuseful and discuss future work. Experimentally, we implement Bayesian Neural\nNetworks for multi-tasks classification on a public dataset for the real-time\ncondition monitoring of a hydraulic system and demonstrate the usefulness of\nthe system by evaluating the probability of a prediction being accurate given\nits uncertainty. We deploy these models using our proposed agent-based\nframework and integrate web visualisation to demonstrate its real-time\nfeasibility.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 10:28:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yong", "Bang Xiang", ""], ["Brintrup", "Alexandra", ""]]}, {"id": "2107.13293", "submitter": "Alexandre Nicolas", "authors": "Alexandre Nicolas (CNRS, UCBL), Fadratul Hafinaz", "title": "Social groups in pedestrian crowds: Review of their influence on the\n  dynamics and their modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrians are often encountered walking in the company of some social\nrelations, rather than alone. The social groups thus formed, in variable\nproportions depending on the context, are not randomly organised but exhibit\ndistinct features, such as the well-known tendency of 3-member groups to be\narranged in a V-shape. The existence of group structures is thus likely to\nimpact the collective dynamics of the crowd, possibly in a critical way when\nemergency situations are considered. After turning a blind eye to these group\naspects for years, endeavours to model groups in crowd simulation software have\nthrived in the past decades. This fairly short review opens on a description of\ntheir empirical characteristics and their impact on the global flow. Then, it\naims to offer a pedagogical discussion of the main strategies to model such\ngroups, within different types of models, in order to provide guidance for\nprospective modellers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:36:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Nicolas", "Alexandre", "", "CNRS, UCBL"], ["Hafinaz", "Fadratul", ""]]}, {"id": "2107.13444", "submitter": "Wicak Ananduta", "authors": "Giuseppe Belgioioso, Wicak Ananduta, Sergio Grammatico, and Carlos\n  Ocampo-Martinez", "title": "Operationally-Safe Peer-to-Peer Energy Trading in Distribution Grids: A\n  Game-Theoretic Market-Clearing Mechanism", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In future distribution grids, prosumers (i.e., energy consumers with storage\nand/or production capabilities) will trade energy with each other and with the\nmain grid. To ensure an efficient and safe operation of energy trading, in this\npaper, we formulate a peer-to-peer energy market of prosumers as a generalized\naggregative game, in which a network operator is only responsible for the\noperational constraints of the system. We design a distributed market-clearing\nmechanism with convergence guarantee to an economically-efficient and\noperationally-safe configuration (i.e., a variational generalized Nash\nequilibrium). Numerical studies on the IEEE 37-bus testcase show the\nscalability of the proposed approach and suggest that active participation in\nthe market is beneficial for both prosumers and the network operator.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:50:53 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Belgioioso", "Giuseppe", ""], ["Ananduta", "Wicak", ""], ["Grammatico", "Sergio", ""], ["Ocampo-Martinez", "Carlos", ""]]}, {"id": "2107.14097", "submitter": "Noam Hazon", "authors": "Leora Schmerler, Noam Hazon", "title": "Strategic Voting in the Context of Negotiating Teams", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A negotiating team is a group of two or more agents who join together as a\nsingle negotiating party because they share a common goal related to the\nnegotiation. Since a negotiating team is composed of several stakeholders,\nrepresented as a single negotiating party, there is need for a voting rule for\nthe team to reach decisions. In this paper, we investigate the problem of\nstrategic voting in the context of negotiating teams. Specifically, we present\na polynomial-time algorithm that finds a manipulation for a single voter when\nusing a positional scoring rule. We show that the problem is still tractable\nwhen there is a coalition of manipulators that uses a x-approval rule. The\ncoalitional manipulation problem becomes computationally hard when using Borda,\nbut we provide a polynomial-time algorithm with the following guarantee: given\na manipulable instance with k manipulators, the algorithm finds a successful\nmanipulation with at most one additional manipulator. Our results hold for both\nconstructive and destructive manipulations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:25:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Schmerler", "Leora", ""], ["Hazon", "Noam", ""]]}]