[{"id": "2006.00176", "submitter": "Yen-Cheng Liu", "authors": "Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, Zsolt Kira", "title": "When2com: Multi-Agent Perception via Communication Graph Grouping", "comments": "Accepted to CVPR 2020; for the project page, see\n  https://ycliu93.github.io/projects/multi-agent-perception.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant advances have been made for single-agent perception, many\napplications require multiple sensing agents and cross-agent communication due\nto benefits such as coverage and robustness. It is therefore critical to\ndevelop frameworks which support multi-agent collaborative perception in a\ndistributed and bandwidth-efficient manner. In this paper, we address the\ncollaborative perception problem, where one agent is required to perform a\nperception task and can communicate and share information with other agents on\nthe same task. Specifically, we propose a communication framework by learning\nboth to construct communication groups and decide when to communicate. We\ndemonstrate the generalizability of our framework on two different perception\ntasks and show that it significantly reduces communication bandwidth while\nmaintaining superior performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 04:41:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:32:30 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Glaser", "Nathaniel", ""], ["Kira", "Zsolt", ""]]}, {"id": "2006.00584", "submitter": "Lav Varshney", "authors": "Ankur Mani, Lav R. Varshney, and Alex (Sandy) Pentland", "title": "Quantization Games on Social Networks and Language Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MA eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a strategic network quantizer design setting where agents must\nbalance fidelity in representing their local source distributions against their\nability to successfully communicate with other connected agents. We study the\nproblem as a network game and show existence of Nash equilibrium quantizers.\nFor any agent, under Nash equilibrium, the word representing a given partition\nregion is the conditional expectation of the mixture of local and social source\nprobability distributions within the region. Since having knowledge of the\noriginal source of information in the network may not be realistic, we show\nthat under certain conditions, the agents need not know the source origin and\nyet still settle on a Nash equilibrium using only the observed sources.\nFurther, the network may converge to equilibrium through a distributed version\nof the Lloyd-Max algorithm. In contrast to traditional results in the evolution\nof language, we find several vocabularies may coexist in the Nash equilibrium,\nwith each individual having exactly one of these vocabularies. The overlap\nbetween vocabularies is high for individuals that communicate frequently and\nhave similar local sources. Finally, we argue that error in translation along a\nchain of communication does not grow if and only if the chain consists of\nagents with shared vocabulary. Numerical results are given.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 18:48:22 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Mani", "Ankur", "", "Sandy"], ["Varshney", "Lav R.", "", "Sandy"], ["Alex", "", "", "Sandy"], ["Pentland", "", ""]]}, {"id": "2006.00587", "submitter": "Zhizhou Ren", "authors": "Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, Chongjie Zhang", "title": "Towards Understanding Linear Value Decomposition in Cooperative\n  Multi-Agent Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value decomposition is a popular and promising approach to scaling up\nmulti-agent reinforcement learning in cooperative settings. However, the\ntheoretical understanding of such methods is limited. In this paper, we\nintroduce a variant of the fitted Q-iteration framework for analyzing\nmulti-agent Q-learning with value decomposition. Based on this framework, we\nderive a closed-form solution to the empirical Bellman error minimization with\nlinear value decomposition. With this novel solution, we further reveal two\ninteresting insights: 1) linear value decomposition implicitly implements a\nclassical multi-agent credit assignment called counterfactual difference\nrewards; and 2) On-policy data distribution or richer Q function classes can\nimprove the training stability of multi-agent Q-learning. In the empirical\nstudy, our experiments demonstrate the realizability of our theoretical\nclosed-form formulation and implications in the didactic examples and a broad\nset of StarCraft II unit micromanagement tasks, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 19:14:03 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 15:11:41 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 15:24:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Jianhao", ""], ["Ren", "Zhizhou", ""], ["Han", "Beining", ""], ["Ye", "Jianing", ""], ["Zhang", "Chongjie", ""]]}, {"id": "2006.00680", "submitter": "Shouvik Roy", "authors": "Shouvik Roy, Usama Mehmood, Radu Grosu, Scott A. Smolka, Scott D.\n  Stoller, Ashish Tiwari", "title": "Learning Distributed Controllers for V-Formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a high-performing, fully distributed and symmetric neural\nV-formation controller can be synthesized from a Centralized MPC (Model\nPredictive Control) controller using Deep Learning. This result is significant\nas we also establish that under very reasonable conditions, it is impossible to\nachieve V-formation using a deterministic, distributed, and symmetric\ncontroller. The learning process we use for the neural V-formation controller\nis significantly enhanced by CEGkR, a Counterexample-Guided k-fold Retraining\ntechnique we introduce, which extends prior work in this direction in important\nways. Our experimental results show that our neural V-formation controller\ngeneralizes to a significantly larger number of agents than for which it was\ntrained (from 7 to 15), and exhibits substantial speedup over the MPC-based\ncontroller. We use a form of statistical model checking to compute confidence\nintervals for our neural V-formation controller's convergence rate and time to\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 02:56:19 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Roy", "Shouvik", ""], ["Mehmood", "Usama", ""], ["Grosu", "Radu", ""], ["Smolka", "Scott A.", ""], ["Stoller", "Scott D.", ""], ["Tiwari", "Ashish", ""]]}, {"id": "2006.00917", "submitter": "Peter Hillmann", "authors": "Tobias Uhlig and Peter Hillmann and Oliver Rose", "title": "Evaluation of the general applicability of Dragoon for the k-center\n  problem", "comments": null, "journal-ref": "Winter Simulation Conference 2016", "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CC cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-center problem is a fundamental problem we often face when considering\ncomplex service systems. Typical challenges include the placement of warehouses\nin logistics or positioning of servers for content delivery networks. We\npreviously have proposed Dragoon as an effective algorithm to approach the\nk-center problem. This paper evaluates Dragoon with a focus on potential worst\ncase behavior in comparison to other techniques. We use an evolutionary\nalgorithm to generate instances of the k-center problem that are especially\nchallenging for Dragoon. Ultimately, our experiments confirm the previous good\nresults of Dragoon, however, we also can reliably find scenarios where it is\nclearly outperformed by other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:54:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Uhlig", "Tobias", ""], ["Hillmann", "Peter", ""], ["Rose", "Oliver", ""]]}, {"id": "2006.01022", "submitter": "Muhammad Zuhair Qadir", "authors": "Muhammad Zuhair Qadir, Songhao Piao, Haiyang Jiang and Mohammed El\n  Habib Souidi", "title": "A novel approach for multi-agent cooperative pursuit to capture grouped\n  evaders", "comments": "published paper's draft version", "journal-ref": "Journal of Supercomputing, J Supercomput 76 (2020)", "doi": "10.1007/s11227-018-2591-3", "report-no": null, "categories": "cs.AI cs.GT cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach of mobile multi-agent pursuit based on application of\nself-organizing feature map (SOFM) and along with that reinforcement learning\nbased on agent group role membership function (AGRMF) model is proposed. This\nmethod promotes dynamic organization of the pursuers' groups and also makes\npursuers' group evader according to their desire based on SOFM and AGRMF\ntechniques. This helps to overcome the shortcomings of the pursuers that they\ncannot fully reorganize when the goal is too independent in process of AGRMF\nmodels operation. Besides, we also discuss a new reward function. After the\nformation of the group, reinforcement learning is applied to get the optimal\nsolution for each agent. The results of each step in capturing process will\nfinally affect the AGR membership function to speed up the convergence of the\ncompetitive neural network. The experiments result shows that this approach is\nmore effective for the mobile agents to capture evaders.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:39:58 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 10:25:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Qadir", "Muhammad Zuhair", ""], ["Piao", "Songhao", ""], ["Jiang", "Haiyang", ""], ["Souidi", "Mohammed El Habib", ""]]}, {"id": "2006.01029", "submitter": "Ehud Shapiro", "authors": "Ouri Poupko, Ehud Shapiro and Nimrod Talmon", "title": "Fault-Tolerant Distributed-Ledger Implementation of Digital Social\n  Contracts", "comments": "arXiv admin note: text overlap with arXiv:2005.06261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A companion paper defined the notion of digital social contracts, presented a\ndesign for a social-contracts programming language, and demonstrated its\npotential utility via example social contracts. The envisioned setup consists\nof people with genuine identifiers, which are unique and singular cryptographic\nkey pairs, that operate software agents thus identified on their mobile device.\nThe abstract model of digital social contracts consists of a transition system\nspecifying concurrent, non-deterministic asynchronous agents that operate on a\nshared ledger by performing digital speech acts, which are\ncryptographically-signed sequentially-indexed digital actions. Here, we address\nthe distributed-ledger implementation of digital social contracts in the\npresence of faulty agents: we present a design of a fault-tolerant\ndistributed-ledger transition system and show that it implements the abstract\nshared-ledger model of digital social contracts, and discuss its resilience to\nfaulty agents. The result is a novel ledger architecture that is distributed\nwith a blockchain-per-person (as opposed to centralized with one blockchain for\nall), partially-ordered (as opposed to totally-ordered), locally-replicated (as\nopposed to globally-replicated), asynchronous (as opposed to\nglobally-synchronized), peer-to-peer with each agent being both an actor and a\nvalidator (as opposed to having dedicated miners, validators, and clients),\nenvironmentally-friendly (as opposed to the environmentally-harmful\nProof-of-Work), self-sufficient (as opposed to the energy-hogging Proof-of-Work\nor capital-hogging Proof-of-Stake) and egalitarian (as opposed to the\nplutocratic Proof-of-Work and Proof-of-Stake).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:53:25 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 16:02:26 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 07:56:36 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 13:57:57 GMT"}, {"version": "v5", "created": "Sat, 19 Sep 2020 07:24:08 GMT"}, {"version": "v6", "created": "Thu, 19 Nov 2020 22:40:42 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Poupko", "Ouri", ""], ["Shapiro", "Ehud", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2006.01216", "submitter": "George Sidiropoulos", "authors": "George Sidiropoulos, Chairi Kiourt, Lefteris Moussiades", "title": "Crowd simulation for crisis management: the outcomes of the last decade", "comments": "Submitted to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few decades, crowd simulation for crisis management is highlighted\nas an important topic of interest for many scientific fields. As the continues\nevolution of computational resources increases, along with the capabilities of\nArtificial Intelligence, the demand for better and more realistic simulation\nhas become more attractive and popular to scientists. Along those years, there\nhave been published hundreds of research articles and have been created\nnumerous different systems that aim to simulate crowd behaviors, crisis cases\nand emergency evacuation scenarios. For better outcomes, recent research has\nfocused on the separation of the problem of crisis management, to multiple\nresearch sub-fields (categories), such as the navigation of the simulated\npedestrians, their psychology, the group dynamics etc. There have been extended\nresearch works suggesting new methods and techniques for those categories of\nproblems. In this paper, we propose three main research categories, each one\nconsist of several sub-categories, relying on crowd simulation for crisis\nmanagement aspects and we present the outcomes of the last decade, focusing\nmostly on works exploiting multi-agent technologies. We analyze a number of\ntechnologies, methodologies, techniques, tools and systems introduced\nthroughout the last years. A comparative review and discussion of the proposed\ncategories is presented towards the identification of the most efficient\naspects of the proposed categories. A general framework, towards the future\ncrowd simulation for crisis management is presented based on the most efficient\nto yield the most realistic outcomes of the last decades. The paper is\nconcluded with some highlights and open questions for future directions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 19:38:47 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 16:16:33 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sidiropoulos", "George", ""], ["Kiourt", "Chairi", ""], ["Moussiades", "Lefteris", ""]]}, {"id": "2006.01482", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Ying Wen, Liheng Chen, Jun Wang, Kun Shao, David Mguni,\n  Weinan Zhang", "title": "Multi-Agent Determinantal Q-Learning", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centralized training with decentralized execution has become an important\nparadigm in multi-agent learning. Though practical, current methods rely on\nrestrictive assumptions to decompose the centralized value function across\nagents for execution. In this paper, we eliminate this restriction by proposing\nmulti-agent determinantal Q-learning. Our method is established on Q-DPP, an\nextension of determinantal point process (DPP) with partition-matroid\nconstraint to multi-agent setting. Q-DPP promotes agents to acquire diverse\nbehavioral models; this allows a natural factorization of the joint Q-functions\nwith no need for \\emph{a priori} structural constraints on the value function\nor special network architectures. We demonstrate that Q-DPP generalizes major\nsolutions including VDN, QMIX, and QTRAN on decentralizable cooperative tasks.\nTo efficiently draw samples from Q-DPP, we adopt an existing\nsample-by-projection sampler with theoretical approximation guarantee. The\nsampler also benefits exploration by coordinating agents to cover orthogonal\ndirections in the state space during multi-agent training. We evaluate our\nalgorithm on various cooperative benchmarks; its effectiveness has been\ndemonstrated when compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:32:48 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:18:26 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 12:43:52 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 17:50:25 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yang", "Yaodong", ""], ["Wen", "Ying", ""], ["Chen", "Liheng", ""], ["Wang", "Jun", ""], ["Shao", "Kun", ""], ["Mguni", "David", ""], ["Zhang", "Weinan", ""]]}, {"id": "2006.01784", "submitter": "Vahid Yazdanpanah", "authors": "Vahid Yazdanpanah, Devrim Murat Yazan, W. Henk M. Zijm", "title": "Coordinating Multiagent Industrial Symbiosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal multiagent framework for coordinating a class of\ncollaborative industrial practices called Industrial Symbiotic Networks (ISNs)\nas cooperative games. The game-theoretic formulation of ISNs enables systematic\nreasoning about what we call the ISN implementation problem. Specifically, the\ncharacteristics of ISNs may lead to the inapplicability of standard fair and\nstable benefit allocation methods. Inspired by realistic ISN scenarios and\nfollowing the literature on normative multiagent systems, we consider\nregulations and normative socio-economic policies as coordination instruments\nthat in combination with ISN games resolve the situation. In this multiagent\nsystem, employing Marginal Contribution Nets (MC-Nets) as rule-based\ncooperative game representations foster the combination of regulations and ISN\ngames with no loss in expressiveness. We develop algorithmic methods for\ngenerating regulations that ensure the implementability of ISNs and as a policy\nsupport, present the policy requirements that guarantee the implementability of\nall the desired ISNs in a balanced-budget way.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:05:43 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Yazdanpanah", "Vahid", ""], ["Yazan", "Devrim Murat", ""], ["Zijm", "W. Henk M.", ""]]}, {"id": "2006.01866", "submitter": "Alexander Engelmann", "authors": "Alexander Engelmann, Yuning Jiang, Henrieke Benner, Ruchuan Ou, Boris\n  Houska, Timm Faulwasser", "title": "ALADIN-$\\alpha$ -- An open-source MATLAB toolbox for distributed\n  non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an open-source software for distributed and\ndecentralized non-convex optimization named ALADIN-$\\alpha$. ALADIN-$\\alpha$ is\na MATLAB implementation of the Augmented Lagrangian Alternating Direction\nInexact Newton (ALADIN) algorithm, which is tailored towards rapid prototyping\nfor non-convex distributed optimization. An improved version of the recently\nproposed bi-level variant of ALADIN is included enabling decentralized\nnon-convex optimization. A collection of application examples from different\napplications fields including chemical engineering, robotics, and power systems\nunderpins the application potential of ALADIN-$\\alpha$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:31:07 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Engelmann", "Alexander", ""], ["Jiang", "Yuning", ""], ["Benner", "Henrieke", ""], ["Ou", "Ruchuan", ""], ["Houska", "Boris", ""], ["Faulwasser", "Timm", ""]]}, {"id": "2006.01925", "submitter": "Kooktae Lee", "authors": "Kooktae Lee", "title": "Asynchronous Distributed Averaging: A Switched System Framework for\n  Average Error Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates an expected average error for distributed averaging\nproblems under asynchronous updates. The asynchronism in this context implies\nno existence of a global clock as well as random characteristics in\ncommunication uncertainty such as communication delays and packet drops.\nAlthough some previous works contributed to the design of average consensus\nprotocols to guarantee the convergence to an exact average, these methods may\nincrease computational burdens due to extra works. Sometimes it is thus\nbeneficial to make each agent exchange information asynchronously without\nmodifying the algorithm, which causes randomness in the average value as a\ntrade-off. In this study, an expected average error is analyzed based on the\nswitched system framework, to estimate an upper bound of the asynchronous\naverage compared to the exact one in the expectation sense. Numerical examples\nare provided to validate the proposed results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:17:59 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Lee", "Kooktae", ""]]}, {"id": "2006.02214", "submitter": "Mitra Baratchi", "authors": "Laurens Arp, Dyon van Vreumingen, Daniela Gawehns, Mitra Baratchi", "title": "Metaheuristic macro scale traffic flow optimisation from urban movement\n  data", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can urban movement data be exploited in order to improve the flow of\ntraffic within a city? Movement data provides valuable information about routes\nand specific roads that people are likely to drive on. This allows us to\npinpoint roads that occur in many routes and are thus sensitive to congestion.\nRedistributing some of the traffic to avoid unnecessary use of these roads\ncould be a key factor in improving traffic flow. Many proposed approaches to\ncombat congestion are either static or do not incorporate any movement data. In\nthis work, we present a method to redistribute traffic through the introduction\nof externally imposed variable costs to each road segment, assuming that all\ndrivers seek to drive the cheapest route. We use a metaheuristic optimisation\napproach to minimise total travel times by optimising a set of road-specific\nvariable cost parameters, which are used as input for an objective function\nbased on traffic flow theory. The optimisation scenario for the city centre of\nTokyo considered in this paper was defined using public spatial road network\ndata, and movement data acquired from Foursquare. Experimental results show\nthat our proposed scenario has the potential to achieve a 62.6\\% improvement of\ntotal travel time in Tokyo compared to that of a currently operational road\nnetwork configuration, with no imposed variable costs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 21:28:38 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Arp", "Laurens", ""], ["van Vreumingen", "Dyon", ""], ["Gawehns", "Daniela", ""], ["Baratchi", "Mitra", ""]]}, {"id": "2006.02732", "submitter": "Woojun Kim", "authors": "Woojun Kim, Whiyoung Jung, Myungsik Cho, Youngchul Sung", "title": "A Maximum Mutual Information Framework for Multi-Agent Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a maximum mutual information (MMI) framework for\nmulti-agent reinforcement learning (MARL) to enable multiple agents to learn\ncoordinated behaviors by regularizing the accumulated return with the mutual\ninformation between actions. By introducing a latent variable to induce nonzero\nmutual information between actions and applying a variational bound, we derive\na tractable lower bound on the considered MMI-regularized objective function.\nApplying policy iteration to maximize the derived lower bound, we propose a\npractical algorithm named variational maximum mutual information multi-agent\nactor-critic (VM3-AC), which follows centralized learning with decentralized\nexecution (CTDE). We evaluated VM3-AC for several games requiring coordination,\nand numerical results show that VM3-AC outperforms MADDPG and other MARL\nalgorithms in multi-agent tasks requiring coordination.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 09:43:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kim", "Woojun", ""], ["Jung", "Whiyoung", ""], ["Cho", "Myungsik", ""], ["Sung", "Youngchul", ""]]}, {"id": "2006.02736", "submitter": "Rafael C. Cardoso", "authors": "Rafael C. Cardoso and Angelo Ferrando and Fabio Papacchini", "title": "LFC: Combining Autonomous Agents and Automated Planning in the\n  Multi-Agent Programming Contest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2019 Multi-Agent Programming Contest introduced a new scenario, Agents\nAssemble, where two teams of agents move around a 2D grid and compete to\nassemble complex block structures. In this paper, we describe the strategies\nused by our team that led us to achieve first place in the contest. Our\nstrategies tackle some of the major challenges in the 2019 contest: how to\nexplore and build a map when agents only have access to local vision and no\nglobal coordinates; how to move around the map efficiently even though there\nare dynamic events that can change the cells in the grid; and how to assemble\nand submit complex block structures given that the opposing team may try to\nsabotage us. To implement our strategies, we use the multi-agent systems\ndevelopment platform JaCaMo to program our agents and the Fast Downward planner\nto plan the movement of the agent in the grid. We also provide a brief\ndiscussion of our matches in the contest and give our analysis of how our team\nperformed in each match.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 09:48:51 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Cardoso", "Rafael C.", ""], ["Ferrando", "Angelo", ""], ["Papacchini", "Fabio", ""]]}, {"id": "2006.02739", "submitter": "Tobias Ahlbrecht", "authors": "Tobias Ahlbrecht (1), J\\\"urgen Dix (1), Niklas Fiekas (1) and Tabajara\n  Krausburg (1 and 2) ((1) Department of Informatics, Clausthal University of\n  Technology, (2) School of Technology, Pontifical Catholic University of Rio\n  Grande do Sul)", "title": "The Multi-Agent Programming Contest: A r\\'esum\\'e", "comments": "Submitted to the proceedings of the Multi-Agent Programming Contest\n  2019, to appear in Springer Lect. Notes Computer Challenges Series\n  https://www.springer.com/series/16528", "journal-ref": null, "doi": "10.1007/978-3-030-59299-8_1", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multi-Agent Programming Contest, MAPC, is an annual event organized since\n2005 out of Clausthal University of Technology. Its aim is to investigate the\npotential of using decentralized, autonomously acting intelligent agents, by\nproviding a complex scenario to be solved in a competitive environment. For\nthis we need suitable benchmarks where agent-based systems can shine. We\npresent previous editions of the contest and also its current scenario and\nresults from its use in the 2019 MAPC with a special focus on its suitability.\nWe conclude with lessons learned over the years.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:00:51 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ahlbrecht", "Tobias", "", "1 and 2"], ["Dix", "J\u00fcrgen", "", "1 and 2"], ["Fiekas", "Niklas", "", "1 and 2"], ["Krausburg", "Tabajara", "", "1 and 2"]]}, {"id": "2006.02816", "submitter": "Michael Vezina", "authors": "Michael Vezina, Babak Esfandiari", "title": "The Requirement Gatherers' Approach to the 2019 Multi-Agent Programming\n  Contest Scenario", "comments": "43 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2019 Multi-Agent Programming Contest (MAPC) scenario poses many\nchallenges for agents participating in the contest. We discuss The Requirement\nGatherers' (TRG) approach to handling the various challenges we faced --\nincluding how we designed our system, how we went about debugging our agents,\nand the strategy we employed to each of our agents. We conclude the paper with\nremarks about the performance of our agents, and what we should have done\ndifferently.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 12:23:41 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Vezina", "Michael", ""], ["Esfandiari", "Babak", ""]]}, {"id": "2006.03175", "submitter": "Philip Gressman", "authors": "Philip T. Gressman and Jennifer R. Peck", "title": "Simulating COVID-19 in a University Environment", "comments": "30 pages, 9 figures; fixed minor typos and rephrased some unclear\n  points", "journal-ref": "Mathematical Biosciences Volume 328, October 2020, 108436", "doi": "10.1016/j.mbs.2020.108436", "report-no": null, "categories": "q-bio.PE cs.MA cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residential colleges and universities face unique challenges in providing\nin-person instruction during the COVID-19 pandemic. Administrators are\ncurrently faced with decisions about whether to open during the pandemic and\nwhat modifications of their normal operations might be necessary to protect\nstudents, faculty and staff. There is little information, however, on what\nmeasures are likely to be most effective and whether existing interventions\ncould contain the spread of an outbreak on campus. We develop a full-scale\nstochastic agent-based model to determine whether in-person instruction could\nsafely continue during the pandemic and evaluate the necessity of various\ninterventions. Simulation results indicate that large scale randomized testing,\ncontact-tracing, and quarantining are important components of a successful\nstrategy for containing campus outbreaks. High test specificity is critical for\nkeeping the size of the quarantine population manageable. Moving the largest\nclasses online is also crucial for controlling both the size of outbreaks and\nthe number of students in quarantine. Increased residential exposure can\nsignificantly impact the size of an outbreak, but it is likely more important\nto control non-residential social exposure among students. Finally, necessarily\nhigh quarantine rates even in controlled outbreaks imply significant\nabsenteeism, indicating a need to plan for remote instruction of quarantined\nstudents.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:04:03 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 16:46:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gressman", "Philip T.", ""], ["Peck", "Jennifer R.", ""]]}, {"id": "2006.03280", "submitter": "Fran\\c{c}ois Schwarzentruber", "authors": "Arthur Queffelec and Ocan Sankur and Fran\\c{c}ois Schwarzentruber", "title": "Conflict-Based Search for Connected Multi-Agent Path Finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the multi-agent path finding problem (MAPF) in which\nagents are required to remain connected to each other and to a designated base.\nThis problem has applications in search and rescue missions where the entire\nexecution must be monitored by a human operator. We re-visit the conflict-based\nsearch algorithm known for MAPF, and define a variant where conflicts arise\nfrom disconnections rather than collisions. We study optimizations, and give\nexperimental results in which we compare our algorithms with the literature.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:02:36 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Queffelec", "Arthur", ""], ["Sankur", "Ocan", ""], ["Schwarzentruber", "Fran\u00e7ois", ""]]}, {"id": "2006.03553", "submitter": "Lucas Cassano Dr.", "authors": "Lucas Cassano and Ali H. Sayed", "title": "Logical Team Q-learning: An approach towards factored policies in\n  cooperative MARL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenge of learning factored policies in cooperative MARL\nscenarios. In particular, we consider the situation in which a team of agents\ncollaborates to optimize a common cost. The goal is to obtain factored policies\nthat determine the individual behavior of each agent so that the resulting\njoint policy is optimal. The main contribution of this work is the introduction\nof Logical Team Q-learning (LTQL). LTQL does not rely on assumptions about the\nenvironment and hence is generally applicable to any collaborative MARL\nscenario. We derive LTQL as a stochastic approximation to a dynamic programming\nmethod we introduce in this work. We conclude the paper by providing\nexperiments (both in the tabular and deep settings) that illustrate the claims.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 17:02:36 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 19:05:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Cassano", "Lucas", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2006.03700", "submitter": "Maria Lombardi", "authors": "Maria Lombardi, William H. Warren, M. di Bernardo", "title": "Leadership emergence in walking groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mechanisms underlying the emergence of leadership in\nmulti-agent systems is still under investigation in many areas of research\nwhere group coordination is involved. While leadership has been mostly\ninvestigated in the case of animal groups, only a few works address the problem\nof leadership emergence in human ensembles, e.g. pedestrian walking, group\ndance. In this paper we study the emergence of leadership in the specific\nscenario of a small walking group. Our aim is to unveil the main mechanisms\nemerging in a human group when leader or follower roles are not designated a\npriori. Two groups of participants were asked to walk together and turn or\nchange speed at self-selected times. Data were analysed using time-dependent\ncross correlation to infer leader-follower interactions between each pair of\ngroup members. The results indicate that leadership emergence is due both to\ncontextual factors, such as an individual's position in the group, and to\npersonal factors, such as an individual's characteristic locomotor behaviour.\nOur approach can easily be extended to larger groups and other scenarios such\nas team sports and emergency evacuations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:31:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lombardi", "Maria", ""], ["Warren", "William H.", ""], ["di Bernardo", "M.", ""]]}, {"id": "2006.03916", "submitter": "Filippo Fabiani", "authors": "Filippo Fabiani, Mohammad Amin Tajeddini, Hamed Kebriaei, Sergio\n  Grammatico", "title": "Local Stackelberg equilibrium seeking in generalized aggregative games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-layer, semi-decentralized algorithm to compute a local\nsolution to the Stackelberg equilibrium problem in aggregative games with\ncoupling constraints. Specifically, we focus on a single-leader,\nmultiple-follower problem, and after equivalently recasting the Stackelberg\ngame as a mathematical program with complementarity constraints (MPCC), we\niteratively convexify a regularized version of the MPCC as inner problem, whose\nsolution generates a sequence of feasible descent directions for the original\nMPCC. Thus, by pursuing a descent direction at every outer iteration, we\nestablish convergence to a local Stackelberg equilibrium. Finally, the proposed\nalgorithm is tested on a numerical case study involving a hierarchical instance\nof the charging coordination of Plug-in Electric Vehicles (PEVs).\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 16:52:48 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fabiani", "Filippo", ""], ["Tajeddini", "Mohammad Amin", ""], ["Kebriaei", "Hamed", ""], ["Grammatico", "Sergio", ""]]}, {"id": "2006.03923", "submitter": "Ian Davies", "authors": "Ian Davies, Zheng Tian and Jun Wang", "title": "Learning to Model Opponent Learning", "comments": "Accepted to AAAI2020 as part of the Student Abstract Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Agent Reinforcement Learning (MARL) considers settings in which a set\nof coexisting agents interact with one another and their environment. The\nadaptation and learning of other agents induces non-stationarity in the\nenvironment dynamics. This poses a great challenge for value function-based\nalgorithms whose convergence usually relies on the assumption of a stationary\nenvironment. Policy search algorithms also struggle in multi-agent settings as\nthe partial observability resulting from an opponent's actions not being known\nintroduces high variance to policy training. Modelling an agent's opponent(s)\nis often pursued as a means of resolving the issues arising from the\ncoexistence of learning opponents. An opponent model provides an agent with\nsome ability to reason about other agents to aid its own decision making. Most\nprior works learn an opponent model by assuming the opponent is employing a\nstationary policy or switching between a set of stationary policies. Such an\napproach can reduce the variance of training signals for policy search\nalgorithms. However, in the multi-agent setting, agents have an incentive to\ncontinually adapt and learn. This means that the assumptions concerning\nopponent stationarity are unrealistic. In this work, we develop a novel\napproach to modelling an opponent's learning dynamics which we term Learning to\nModel Opponent Learning (LeMOL). We show our structured opponent model is more\naccurate and stable than naive behaviour cloning baselines. We further show\nthat opponent modelling can improve the performance of algorithmic agents in\nmulti-agent settings.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:19:04 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Davies", "Ian", ""], ["Tian", "Zheng", ""], ["Wang", "Jun", ""]]}, {"id": "2006.04021", "submitter": "Shuncheng He", "authors": "Shuncheng He, Jianzhun Shao, Xiangyang Ji", "title": "Skill Discovery of Coordination in Multi-agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised skill discovery drives intelligent agents to explore the unknown\nenvironment without task-specific reward signal, and the agents acquire various\nskills which may be useful when the agents adapt to new tasks. In this paper,\nwe propose \"Multi-agent Skill Discovery\"(MASD), a method for discovering skills\nfor coordination patterns of multiple agents. The proposed method aims to\nmaximize the mutual information between a latent code Z representing skills and\nthe combination of the states of all agents. Meanwhile it suppresses the\nempowerment of Z on the state of any single agent by adversarial training. In\nanother word, it sets an information bottleneck to avoid empowerment\ndegeneracy. First we show the emergence of various skills on the level of\ncoordination in a general particle multi-agent environment. Second, we reveal\nthat the \"bottleneck\" prevents skills from collapsing to a single agent and\nenhances the diversity of learned skills. Finally, we show the pretrained\npolicies have better performance on supervised RL tasks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 02:04:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["He", "Shuncheng", ""], ["Shao", "Jianzhun", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2006.04037", "submitter": "Hardik Meisheri", "authors": "Nazneen N Sultana, Hardik Meisheri, Vinita Baniwal, Somjit Nath,\n  Balaraman Ravindran, Harshad Khadilkar", "title": "Reinforcement Learning for Multi-Product Multi-Node Inventory Management\n  in Supply Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the application of reinforcement learning (RL) to\nmulti-product inventory management in supply chains. The problem description\nand solution are both adapted from a real-world business solution. The novelty\nof this problem with respect to supply chain literature is (i) we consider\nconcurrent inventory management of a large number (50 to 1000) of products with\nshared capacity, (ii) we consider a multi-node supply chain consisting of a\nwarehouse which supplies three stores, (iii) the warehouse, stores, and\ntransportation from warehouse to stores have finite capacities, (iv) warehouse\nand store replenishment happen at different time scales and with realistic time\nlags, and (v) demand for products at the stores is stochastic. We describe a\nnovel formulation in a multi-agent (hierarchical) reinforcement learning\nframework that can be used for parallelised decision-making, and use the\nadvantage actor critic (A2C) algorithm with quantised action spaces to solve\nthe problem. Experiments show that the proposed approach is able to handle a\nmulti-objective reward comprised of maximising product sales and minimising\nwastage of perishable products.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 04:02:59 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sultana", "Nazneen N", ""], ["Meisheri", "Hardik", ""], ["Baniwal", "Vinita", ""], ["Nath", "Somjit", ""], ["Ravindran", "Balaraman", ""], ["Khadilkar", "Harshad", ""]]}, {"id": "2006.04109", "submitter": "Yipeng Kang", "authors": "Yipeng Kang, Tonghan Wang, Gerard de Melo", "title": "Incorporating Pragmatic Reasoning Communication into Emergent Language", "comments": "9 pages. Accepted as a spotlight paper to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergentism and pragmatics are two research fields that study the dynamics of\nlinguistic communication along substantially different timescales and\nintelligence levels. From the perspective of multi-agent reinforcement\nlearning, they correspond to stochastic games with reinforcement training and\nstage games with opponent awareness. Given that their combination has been\nexplored in linguistics, we propose computational models that combine\nshort-term mutual reasoning-based pragmatics with long-term language\nemergentism. We explore this for agent communication referential games as well\nas in Starcraft II, assessing the relative merits of different kinds of mutual\nreasoning pragmatics models both empirically and theoretically. Our results\nshed light on their importance for making inroads towards getting more natural,\naccurate, robust, fine-grained, and succinct utterances.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 10:31:06 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:19:21 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Kang", "Yipeng", ""], ["Wang", "Tonghan", ""], ["de Melo", "Gerard", ""]]}, {"id": "2006.04222", "submitter": "Shariq Iqbal", "authors": "Shariq Iqbal, Christian A. Schroeder de Witt, Bei Peng, Wendelin\n  B\\\"ohmer, Shimon Whiteson, Fei Sha", "title": "Randomized Entity-wise Factorization for Multi-Agent Reinforcement\n  Learning", "comments": "ICML 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent settings in the real world often involve tasks with varying types\nand quantities of agents and non-agent entities; however, common patterns of\nbehavior often emerge among these agents/entities. Our method aims to leverage\nthese commonalities by asking the question: ``What is the expected utility of\neach agent when only considering a randomly selected sub-group of its observed\nentities?'' By posing this counterfactual question, we can recognize\nstate-action trajectories within sub-groups of entities that we may have\nencountered in another task and use what we learned in that task to inform our\nprediction in the current one. We then reconstruct a prediction of the full\nreturns as a combination of factors considering these disjoint groups of\nentities and train this ``randomly factorized\" value function as an auxiliary\nobjective for value-based multi-agent reinforcement learning. By doing so, our\nmodel can recognize and leverage similarities across tasks to improve learning\nefficiency in a multi-task setting. Our approach, Randomized Entity-wise\nFactorization for Imagined Learning (REFIL), outperforms all strong baselines\nby a significant margin in challenging multi-task StarCraft micromanagement\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 18:28:41 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 16:39:47 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 18:53:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Iqbal", "Shariq", ""], ["de Witt", "Christian A. Schroeder", ""], ["Peng", "Bei", ""], ["B\u00f6hmer", "Wendelin", ""], ["Whiteson", "Shimon", ""], ["Sha", "Fei", ""]]}, {"id": "2006.04635", "submitter": "Thomas William Anthony", "authors": "Thomas Anthony, Tom Eccles, Andrea Tacchetti, J\\'anos Kram\\'ar, Ian\n  Gemp, Thomas C. Hudson, Nicolas Porcel, Marc Lanctot, Julien P\\'erolat,\n  Richard Everett, Roman Werpachowski, Satinder Singh, Thore Graepel, and Yoram\n  Bachrach", "title": "Learning to Play No-Press Diplomacy with Best Response Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep reinforcement learning (RL) have led to considerable\nprogress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The\npurely adversarial nature of such games allows for conceptually simple and\nprincipled application of RL methods. However real-world settings are\nmany-agent, and agent interactions are complex mixtures of common-interest and\ncompetitive aspects. We consider Diplomacy, a 7-player board game designed to\naccentuate dilemmas resulting from many-agent interactions. It also features a\nlarge combinatorial action space and simultaneous moves, which are challenging\nfor RL algorithms. We propose a simple yet effective approximate best response\noperator, designed to handle large combinatorial action spaces and simultaneous\nmoves. We also introduce a family of policy iteration methods that approximate\nfictitious play. With these methods, we successfully apply RL to Diplomacy: we\nshow that our agents convincingly outperform the previous state-of-the-art, and\ngame theoretic equilibrium analysis shows that the new process yields\nconsistent improvements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:33:31 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 21:34:42 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 17:01:22 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Anthony", "Thomas", ""], ["Eccles", "Tom", ""], ["Tacchetti", "Andrea", ""], ["Kram\u00e1r", "J\u00e1nos", ""], ["Gemp", "Ian", ""], ["Hudson", "Thomas C.", ""], ["Porcel", "Nicolas", ""], ["Lanctot", "Marc", ""], ["P\u00e9rolat", "Julien", ""], ["Everett", "Richard", ""], ["Werpachowski", "Roman", ""], ["Singh", "Satinder", ""], ["Graepel", "Thore", ""], ["Bachrach", "Yoram", ""]]}, {"id": "2006.04803", "submitter": "Omar Abdul Wahab", "authors": "Omar Abdel Wahab, Jamal Bentahar, Robin Cohen, Hadi Otrok, Azzam\n  Mourad", "title": "A two-level solution to fight against dishonest opinions in\n  recommendation-based trust systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mechanism to deal with dishonest opinions in\nrecommendation-based trust models, at both the collection and processing\nlevels. We consider a scenario in which an agent requests recommendations from\nmultiple parties to build trust toward another agent. At the collection level,\nwe propose to allow agents to self-assess the accuracy of their recommendations\nand autonomously decide on whether they would participate in the recommendation\nprocess or not. At the processing level, we propose a recommendations\naggregation technique that is resilient to collusion attacks, followed by a\ncredibility update mechanism for the participating agents. The originality of\nour work stems from its consideration of dishonest opinions at both the\ncollection and processing levels, which allows for better and more persistent\nprotection against dishonest recommenders. Experiments conducted on the\nEpinions dataset show that our solution yields better performance in protecting\nthe recommendation process against Sybil attacks, in comparison with a\ncompeting model that derives the optimal network of advisors based on the\nagents' trust values.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 00:34:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wahab", "Omar Abdel", ""], ["Bentahar", "Jamal", ""], ["Cohen", "Robin", ""], ["Otrok", "Hadi", ""], ["Mourad", "Azzam", ""]]}, {"id": "2006.04969", "submitter": "Heiko Hamann", "authors": "Heiko Hamann and Andreagiovanni Reina", "title": "Scalability in Computing and Robotics", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TC.2021.3089044", "report-no": null, "categories": "cs.DC cs.MA cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient engineered systems require scalability. A scalable system has\nincreasing performance with increasing system size. In an ideal case, the\nincrease in performance (e.g., speedup) corresponds to the number of units that\nare added to the system. However, if multiple units work on the same task, then\ncoordination among these units is required. This coordination can introduce\noverheads with an impact on system performance. The coordination costs can lead\nto sublinear improvement or even diminishing performance with increasing system\nsize. However, there are also systems that implement efficient coordination and\nexploit collaboration of units to attain superlinear improvement. Modeling the\nscalability dynamics is key to understanding efficient systems. Known laws of\nscalability, such as Amdahl's law, Gustafson's law, and Gunther's Universal\nScalability Law, are minimalistic phenomenological models that explain a rich\nvariety of system behaviors through concise equations. While useful to gain\ngeneral insights, the phenomenological nature of these models may limit the\nunderstanding of the underlying dynamics, as they are detached from first\nprinciples that could explain coordination overheads among units. Through a\ndecentralized system approach, we propose a general model based on generic\ninteractions between units that is able to describe, as specific cases, any\ngeneral pattern of scalability included by previously reported laws. The\nproposed general model of scalability is built on first principles, or at least\non a microscopic description of interaction between units, and therefore has\nthe potential to contribute to a better understanding of system behavior and\nscalability. We show that this model can be applied to a diverse set of\nsystems, such as parallel supercomputers, robot swarms, or wireless sensor\nnetworks, creating a unified view on interdisciplinary design for scalability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 22:28:59 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:25:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hamann", "Heiko", ""], ["Reina", "Andreagiovanni", ""]]}, {"id": "2006.05048", "submitter": "Osonde Osoba Ph.D.", "authors": "Osonde A. Osoba, Raffaele Vardavas, Justin Grana, Rushil Zutshi, Amber\n  Jaycocks", "title": "Policy-focused Agent-based Modeling using RL Behavioral Models", "comments": "This is a more detailed version of a paper (\"Modeling Agent Behaviors\n  for Policy Analysis via Reinforcement Learning\") accepted to appear in IEEE\n  ICMLA 2020. This also corrects an error in Fig. 7 of the original arXiv\n  submission. Fig. 7 now specifies the right ABM architecture (\"flu\" instead of\n  \"tax\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help\nanalysts explore the emergent consequences of policy interventions in\nmulti-agent decision-making settings. But the validity of inferences drawn from\nABM explorations depends on the quality of the ABM agents' behavioral models.\nStandard specifications of agent behavioral models rely either on heuristic\ndecision-making rules or on regressions trained on past data. Both prior\nspecification modes have limitations. This paper examines the value of\nreinforcement learning (RL) models as adaptive, high-performing, and\nbehaviorally-valid models of agent decision-making in ABMs. We test the\nhypothesis that RL agents are effective as utility-maximizing agents in policy\nABMs. We also address the problem of adapting RL algorithms to handle\nmulti-agency in games by adapting and extending methods from recent literature.\nWe evaluate the performance of such RL-based ABM agents via experiments on two\npolicy-relevant ABMs: a minority game ABM, and an ABM of Influenza\nTransmission. We run some analytic experiments on our AI-equipped ABMs e.g.\nexplorations of the effects of behavioral heterogeneity in a population and the\nemergence of synchronization in a population. The experiments show that RL\nbehavioral models are effective at producing reward-seeking or\nreward-maximizing behaviors in ABM agents. Furthermore, RL behavioral models\ncan learn to outperform the default adaptive behavioral models in the two ABMs\nexamined.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 04:55:07 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 19:28:48 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 20:41:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Osoba", "Osonde A.", ""], ["Vardavas", "Raffaele", ""], ["Grana", "Justin", ""], ["Zutshi", "Rushil", ""], ["Jaycocks", "Amber", ""]]}, {"id": "2006.05251", "submitter": "Elisabetta Cornacchia", "authors": "Elisabetta Cornacchia, Neta Singer, Emmanuel Abbe", "title": "Polarization in Attraction-Repulsion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a model for opinion dynamics, where at each time step,\nrandomly selected agents see their opinions - modeled as scalars in [0,1] -\nevolve depending on a local interaction function. In the classical Bounded\nConfidence Model, agents opinions get attracted when they are close enough. The\nproposed model extends this by adding a repulsion component, which models the\neffect of opinions getting further pushed away when dissimilar enough. With\nthis repulsion component added, and under a repulsion-attraction cleavage\nassumption, it is shown that a new stable configuration emerges beyond the\nclassical consensus configuration, namely the polarization configuration. More\nspecifically, it is shown that total consensus and total polarization are the\nonly two possible limiting configurations. The paper further provides an\nanalysis of the infinite population regime in dimension 1 and higher, with a\nphase transition phenomenon conjectured and backed heuristically.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:33:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Cornacchia", "Elisabetta", ""], ["Singer", "Neta", ""], ["Abbe", "Emmanuel", ""]]}, {"id": "2006.05390", "submitter": "David Galindo", "authors": "Marcin Abram, David Galindo, Daniel Honerkamp, Jonathan Ward, Jin-Mann\n  Wong", "title": "Democratising blockchain: A minimal agency consensus model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel consensus protocol based on a hybrid approach, that\ncombines a directed acyclic graph (DAG) and a classical chain of blocks. This\narchitecture allows us to enforce collective block construction, minimising the\nmonopolistic power of the round-leader. In this way, we decrease the\npossibility for collusion among senders and miners, as well as miners\nthemselves, allowing the use of more incentive compatible and fair pricing\nstrategies. We investigate these possibilities alongside the ability to use the\nDAG structure to minimise the risk of transaction censoring. We conclude by\nproviding preliminary benchmarks of our protocol and by exploring further\nresearch directions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:39:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Abram", "Marcin", ""], ["Galindo", "David", ""], ["Honerkamp", "Daniel", ""], ["Ward", "Jonathan", ""], ["Wong", "Jin-Mann", ""]]}, {"id": "2006.05613", "submitter": "Cleber Amaral Mr.", "authors": "Ot\\'avio Arruda Matoso, Luis P. A. Lampert, Jomi Fred H\\\"ubner, Mateus\n  Concei\\c{c}\\~ao, S\\'ergio P. Bernardes, Cleber Jorge Amaral, Maicon R.\n  Zatelli, Marcelo L. de Lima", "title": "Agent Programming for Industrial Applications: Some Advantages and\n  Drawbacks", "comments": "12 pages, 6 figures, accepted to present on 14th Workshop-School on\n  Agents, Environments, and Applications (WESAAC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous agents are seen as a prominent technology to be applied in\nindustrial scenarios. Classical automation solutions are struggling with\nchallenges related to high dynamism, prompt actuation, heterogeneous entities,\nincluding humans, and decentralised decision-making. Besides promoting\nconcepts, languages, and tools to face such challenges, agents must also\nprovide high reliability. To assess how appropriate and mature are agents for\nindustrial applications, we have investigated its application in two scenarios\nof the gas and oil industry. This paper presents the development of systems and\nthe initial results highlighting the advantages and drawbacks of the agents\napproach when compared with the existing automation solutions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 02:15:33 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Matoso", "Ot\u00e1vio Arruda", ""], ["Lampert", "Luis P. A.", ""], ["H\u00fcbner", "Jomi Fred", ""], ["Concei\u00e7\u00e3o", "Mateus", ""], ["Bernardes", "S\u00e9rgio P.", ""], ["Amaral", "Cleber Jorge", ""], ["Zatelli", "Maicon R.", ""], ["de Lima", "Marcelo L.", ""]]}, {"id": "2006.05619", "submitter": "Cleber Amaral Mr.", "authors": "Cleber Jorge Amaral, Jomi Fred H\\\"ubner and Timotheus Kampik", "title": "Towards Jacamo-rest: A Resource-Oriented Abstraction for Managing\n  Multi-Agent Systems", "comments": "11 pages, 5 figures, Accepted to present on 14th Workshop-School on\n  Agents, Environments, and Applications (WESAAC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multi-Agent Oriented Programming (MAOP) paradigm provides abstractions to\nmodel and implements entities of agents, as well as of their organisations and\nenvironments. In recent years, researchers have started to explore the\nintegration of MAOP and the resource-oriented web architecture (REST). This\npaper further advances this line of research by presenting an ongoing work on\njacamo-rest, a resource-oriented web-based abstraction for the multi-agent\nprogramming platform JaCaMo. Jacamo-rest takes Multi-Agent System (MAS)\ninteroperability to a new level, enabling MAS to not only interact with\nservices or applications of the World Wide Web but also to be managed and\nupdated in their specifications by other applications. To add a developer\ninterface to JaCaMo that is suitable for the Web, we provide a novel conceptual\nperspective on the management of MAOP specification entities as web resources.\nWe tested jacamo-rest using it as a middleware of a programming interface\napplication that provides modern software engineering facilities such as\ncontinuous deployments and iterative software development for MAS.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 02:26:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Amaral", "Cleber Jorge", ""], ["H\u00fcbner", "Jomi Fred", ""], ["Kampik", "Timotheus", ""]]}, {"id": "2006.05678", "submitter": "Mateusz Iwo Dubaniowski", "authors": "Mateusz Iwo Dubaniowski, Hans R. Heinimann", "title": "A framework for modeling interdependencies among households, businesses,\n  and infrastructure systems; and their response to disruptions", "comments": "34 pages, 10 figures, 5 tables, Accepted in Reliability Engineering &\n  System Safety", "journal-ref": null, "doi": "10.1016/j.ress.2020.107063", "report-no": null, "categories": "eess.SY cs.MA cs.SI cs.SY econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban systems, composed of households, businesses, and infrastructures, are\ncontinuously evolving and expanding. This has several implications because the\nimpacts of disruptions, and the complexity and interdependence of systems, are\nrapidly increasing. Hence, we face a challenge in how to improve our\nunderstanding about the interdependencies among those entities, as well as\ntheir responses to disruptions. The aims of this study were to (1) create an\nagent that mimics the metabolism of a business or household that obtains\nsupplies from and provides output to infrastructure systems; (2) implement a\nnetwork of agents that exchange resources, as coordinated with a price\nmechanism; and (3) test the responses of this prototype model to disruptions.\nOur investigation resulted in the development of a business/household agent and\na dynamically self-organizing mechanism of network coordination under\ndisruption based on costs for production and transportation. Simulation\nexperiments confirmed the feasibility of this new model for analyzing responses\nto disruptions. Among the nine disruption scenarios considered, in line with\nour expectations, the one combining the failures of infrastructure links and\nproduction processes had the most negative impact. We also identified areas for\nfuture research that focus on network topologies, mechanisms for resource\nallocation, and disruption generation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 06:26:02 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Dubaniowski", "Mateusz Iwo", ""], ["Heinimann", "Hans R.", ""]]}, {"id": "2006.05799", "submitter": "Maolong Lv", "authors": "Maolong Lv, Wenwu Yu, Jinde Cao, Simone Baldi", "title": "A Separation-Based Methodology to Consensus Tracking of Switched\n  High-Order Nonlinear Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates a reduced-complexity adaptive methodology to consensus\ntracking for a team of uncertain high-order nonlinear systems with switched\n(possibly asynchronous) dynamics. It is well known that high-order nonlinear\nsystems are intrinsically challenging as feedback linearization and\nbackstepping methods successfully developed for low-order systems fail to work.\nAt the same time, even the adding-one power-integrator methodology, well\nexplored for the single-agent high-order case, presents some complexity issues\nand is unsuited for distributed control. At the core of the proposed\ndistributed methodology is a newly proposed definition for separable functions:\nthis definition allows the formulation of a separation-based lemma to handle\nthe high-order terms with reduced complexity in the control design. Complexity\nis reduced in a twofold sense: the control gain of each virtual control law\ndoes not have to be incorporated in the next virtual control law iteratively,\nthus leading to a simpler expression of the control laws; the order of the\nvirtual control gains increases only proportionally (rather than exponentially)\nwith the order of the systems, dramatically reducing high-gain issues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:38:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Lv", "Maolong", ""], ["Yu", "Wenwu", ""], ["Cao", "Jinde", ""], ["Baldi", "Simone", ""]]}, {"id": "2006.05842", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang and Zongqing Lu", "title": "The Emergence of Individuality in Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuality is essential in human society, which induces the division of\nlabor and thus improves the efficiency and productivity. Similarly, it should\nalso be the key to multi-agent cooperation. Inspired by that individuality is\nof being an individual separate from others, we propose a simple yet efficient\nmethod for the emergence of individuality (EOI) in multi-agent reinforcement\nlearning (MARL). EOI learns a probabilistic classifier that predicts a\nprobability distribution over agents given their observation and gives each\nagent an intrinsic reward of being correctly predicted by the classifier. The\nintrinsic reward encourages the agents to visit their own familiar\nobservations, and learning the classifier by such observations makes the\nintrinsic reward signals stronger and the agents more identifiable. To further\nenhance the intrinsic reward and promote the emergence of individuality, two\nregularizers are proposed to increase the discriminability of the classifier.\nWe implement EOI on top of popular MARL algorithms. Empirically, we show that\nEOI significantly outperforms existing methods in a variety of multi-agent\ncooperative scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:11:21 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Lu", "Zongqing", ""]]}, {"id": "2006.06051", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward\n  Hughes, Hongyuan Zha", "title": "Learning to Incentivize Other Learning Agents", "comments": "20 pages, 11 figures. To appear in 34th Conference on Neural\n  Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of developing powerful and general Reinforcement Learning (RL)\nagents has received increasing attention in recent years. Much of this effort\nhas focused on the single-agent setting, in which an agent maximizes a\npredefined extrinsic reward function. However, a long-term question inevitably\narises: how will such independent agents cooperate when they are continually\nlearning and acting in a shared multi-agent environment? Observing that humans\noften provide incentives to influence others' behavior, we propose to equip\neach RL agent in a multi-agent environment with the ability to give rewards\ndirectly to other agents, using a learned incentive function. Each agent learns\nits own incentive function by explicitly accounting for its impact on the\nlearning of recipients and, through them, the impact on its own extrinsic\nobjective. We demonstrate in experiments that such agents significantly\noutperform standard RL and opponent-shaping agents in challenging general-sum\nMarkov games, often by finding a near-optimal division of labor. Our work\npoints toward more opportunities and challenges along the path to ensure the\ncommon good in a multi-agent future.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:12:38 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:57:01 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Yang", "Jiachen", ""], ["Li", "Ang", ""], ["Farajtabar", "Mehrdad", ""], ["Sunehag", "Peter", ""], ["Hughes", "Edward", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2006.06455", "submitter": "Zongqing Lu", "authors": "Ziluo Ding, Tiejun Huang and Zongqing Lu", "title": "Learning Individually Inferred Communication for Multi-Agent Cooperation", "comments": "NeurIPS 2020, oral presentation. The code is available at\n  https://github.com/PKU-AI-Edge/I2C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication lays the foundation for human cooperation. It is also crucial\nfor multi-agent cooperation. However, existing work focuses on broadcast\ncommunication, which is not only impractical but also leads to information\nredundancy that could even impair the learning process. To tackle these\ndifficulties, we propose Individually Inferred Communication (I2C), a simple\nyet effective model to enable agents to learn a prior for agent-agent\ncommunication. The prior knowledge is learned via causal inference and realized\nby a feed-forward neural network that maps the agent's local observation to a\nbelief about who to communicate with. The influence of one agent on another is\ninferred via the joint action-value function in multi-agent reinforcement\nlearning and quantified to label the necessity of agent-agent communication.\nFurthermore, the agent policy is regularized to better exploit communicated\nmessages. Empirically, we show that I2C can not only reduce communication\noverhead but also improve the performance in a variety of multi-agent\ncooperative scenarios, comparing to existing methods. The code is available at\nhttps://github.com/PKU-AI-Edge/I2C.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:07:57 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 03:43:12 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ding", "Ziluo", ""], ["Huang", "Tiejun", ""], ["Lu", "Zongqing", ""]]}, {"id": "2006.06548", "submitter": "Jobst Heitzig", "authors": "Jobst Heitzig and Forest W. Simmons", "title": "Efficient democratic decisions via nondeterministic proportional\n  consensus", "comments": "10 pages main text plus 61 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.GT cs.MA q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are there voting methods which (i) give everyone, including minorities, an\nequal share of effective power even if voters act strategically, (ii) promote\nconsensus rather than polarization and inequality, and (iii) do not favour the\nstatus quo or rely too much on chance?\n  We show the answer is yes by describing two nondeterministic voting methods,\none based on automatic bargaining over lotteries, the other on conditional\ncommitments to approve compromise options. Our theoretical analysis and\nagent-based simulation experiments suggest that with these, majorities cannot\nconsistently suppress minorities as with deterministic methods, proponents of\nthe status quo cannot block decisions as in consensus-based approaches, the\nresulting aggregate welfare is comparable to existing methods, and average\nrandomness is lower than for other nondeterministic methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:13:51 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Heitzig", "Jobst", ""], ["Simmons", "Forest W.", ""]]}, {"id": "2006.06555", "submitter": "Yiheng Lin", "authors": "Yiheng Lin, Guannan Qu, Longbo Huang, Adam Wierman", "title": "Multi-Agent Reinforcement Learning in Time-varying Networked Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-agent reinforcement learning (MARL) in a time-varying network\nof agents. The objective is to find localized policies that maximize the\n(discounted) global reward. In general, scalability is a challenge in this\nsetting because the size of the global state/action space can be exponential in\nthe number of agents. Scalable algorithms are only known in cases where\ndependencies are static, fixed and local, e.g., between neighbors in a fixed,\ntime-invariant underlying graph. In this work, we propose a Scalable Actor\nCritic framework that applies in settings where the dependencies can be\nnon-local and time-varying, and provide a finite-time error bound that shows\nhow the convergence rate depends on the speed of information spread in the\nnetwork. Additionally, as a byproduct of our analysis, we obtain novel\nfinite-time convergence results for a general stochastic approximation scheme\nand for temporal difference learning with state aggregation, which apply beyond\nthe setting of RL in networked systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:08:16 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 00:49:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lin", "Yiheng", ""], ["Qu", "Guannan", ""], ["Huang", "Longbo", ""], ["Wierman", "Adam", ""]]}, {"id": "2006.06580", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi", "title": "Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior", "comments": "To the best of our knowledge, this is the first attempt to explore\n  the full spectrum of reinforcement learning agents (multi-armed bandits,\n  contextual bandits and reinforcement learning) in the sequential social\n  dilemma. This mental variants section supersedes and extends our work\n  arXiv:1706.02897 (MAB), arXiv:2005.04544 (CB) and arXiv:1906.11286 (RL) into\n  the multi-agent setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prisoner's Dilemma mainly treat the choice to cooperate or defect as an\natomic action. We propose to study online learning algorithm behavior in the\nIterated Prisoner's Dilemma (IPD) game, where we explored the full spectrum of\nreinforcement learning agents: multi-armed bandits, contextual bandits and\nreinforcement learning. We have evaluate them based on a tournament of iterated\nprisoner's dilemma where multiple agents can compete in a sequential fashion.\nThis allows us to analyze the dynamics of policies learned by multiple\nself-interested independent reward-driven agents, and also allows us study the\ncapacity of these algorithms to fit the human behaviors. Results suggest that\nconsidering the current situation to make decision is the worst in this kind of\nsocial dilemma game. Multiples discoveries on online learning behaviors and\nclinical validations are stated.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:58:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 14:17:09 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""]]}, {"id": "2006.06626", "submitter": "Guannan Qu", "authors": "Guannan Qu, Yiheng Lin, Adam Wierman, Na Li", "title": "Scalable Multi-Agent Reinforcement Learning for Networked Systems with\n  Average Reward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been recognized that multi-agent reinforcement learning (MARL)\nfaces significant scalability issues due to the fact that the size of the state\nand action spaces are exponentially large in the number of agents. In this\npaper, we identify a rich class of networked MARL problems where the model\nexhibits a local dependence structure that allows it to be solved in a scalable\nmanner. Specifically, we propose a Scalable Actor-Critic (SAC) method that can\nlearn a near optimal localized policy for optimizing the average reward with\ncomplexity scaling with the state-action space size of local neighborhoods, as\nopposed to the entire network. Our result centers around identifying and\nexploiting an exponential decay property that ensures the effect of agents on\neach other decays exponentially fast in their graph distance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:23:17 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Qu", "Guannan", ""], ["Lin", "Yiheng", ""], ["Wierman", "Adam", ""], ["Li", "Na", ""]]}, {"id": "2006.06775", "submitter": "Lukas Breitwieser", "authors": "Lukas Breitwieser, Ahmad Hesam, Jean de Montigny, Vasileios\n  Vavourakis, Alexandros Iosif, Jack Jennings, Marcus Kaiser, Marco Manca,\n  Alberto Di Meglio, Zaid Al-Ars, Fons Rademakers, Onur Mutlu, Roman Bauer", "title": "BioDynaMo: a general platform for scalable agent-based simulation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Agent-based modeling is an indispensable tool for studying\ncomplex biological systems. However, existing simulators do not always take\nfull advantage of modern hardware and often have a field-specific software\ndesign.\n  Results: We present a novel simulation platform called BioDynaMo that\nalleviates both of these problems. BioDynaMo features a general-purpose and\nhigh-performance simulation engine. We demonstrate that BioDynaMo can be used\nto simulate use cases in: neuroscience, oncology, and epidemiology. For each\nuse case we validate our findings with experimental data or an analytical\nsolution. Our performance results show that BioDynaMo performs up to three\norders of magnitude faster than the state-of-the-art baseline. This improvement\nmakes it feasible to simulate each use case with one billion agents on a single\nserver, showcasing the potential BioDynaMo has for computational biology\nresearch.\n  Availability: BioDynaMo is an open-source project under the Apache 2.0\nlicense and is available at www.biodynamo.org. Instructions to reproduce the\nresults are available in supplementary information.\n  Contact: lukas.breitwieser@inf.ethz.ch, a.s.hesam@tudelft.nl, omutlu@ethz.ch,\nr.bauer@surrey.ac.uk\n  Supplementary information: Available at\nhttps://doi.org/10.5281/zenodo.4501515\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:55:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 13:24:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Breitwieser", "Lukas", ""], ["Hesam", "Ahmad", ""], ["de Montigny", "Jean", ""], ["Vavourakis", "Vasileios", ""], ["Iosif", "Alexandros", ""], ["Jennings", "Jack", ""], ["Kaiser", "Marcus", ""], ["Manca", "Marco", ""], ["Di Meglio", "Alberto", ""], ["Al-Ars", "Zaid", ""], ["Rademakers", "Fons", ""], ["Mutlu", "Onur", ""], ["Bauer", "Roman", ""]]}, {"id": "2006.06844", "submitter": "J{\\o}rgen Villadsen", "authors": "Alexander Birch Jensen and J{\\o}rgen Villadsen", "title": "GOAL-DTU: Development of Distributed Intelligence for the Multi-Agent\n  Programming Contest", "comments": "28 pages, 45 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a brief description of the GOAL-DTU system for the agent contest,\nincluding the overall strategy and how the system is designed to apply this\nstrategy. Our agents are implemented using the GOAL programming language. We\nevaluate the performance of our agents for the contest, and finally also\ndiscuss how to improve the system based on analysis of its strengths and\nweaknesses.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:44:05 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Jensen", "Alexander Birch", ""], ["Villadsen", "J\u00f8rgen", ""]]}, {"id": "2006.06870", "submitter": "Justin Terry", "authors": "Justin K Terry, Nathaniel Grammel", "title": "Multi-Agent Informational Learning Processes", "comments": "We are withdrawing this paper as section 2.1.1 implicitly assumes\n  information gain at all points is homogenous. A researcher has provided us an\n  example showing that this assumption causes our model to make unexpected and\n  pathological predictions, and we are aware of now way to remove this\n  assumption from our work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new mathematical model of multi-agent reinforcement learning,\nthe Multi-Agent Informational Learning Processor \"MAILP\" model. The model is\nbased on the notion that agents have policies for a certain amount of\ninformation, models how this information iteratively evolves and propagates\nthrough many agents. This model is very general, and the only meaningful\nassumption made is that learning for individual agents progressively slows over\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:18:50 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 05:28:53 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 22:31:37 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 21:43:47 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Terry", "Justin K", ""], ["Grammel", "Nathaniel", ""]]}, {"id": "2006.07163", "submitter": "Wolfgang John", "authors": "Mina Sedaghat, Pontus Sk\\\"oldstr\\\"om, Daniel Turull, Vinay Yadhav,\n  Joacim Hal\\'en, Madhubala Ganesan, Amardeep Mehta and Wolfgang John", "title": "Nefele: Process Orchestration for the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization, either at OS- or hardware level, plays an important role in\ncloud computing. It enables easier automation and faster deployment in\ndistributed environments. While virtualized infrastructures provide a level of\nmanagement flexibility, they lack practical abstraction of the distributed\nresources. A developer in such an environment still needs to deal with all the\ncomplications of building a distributed software system. Different\norchestration systems are built to provide that abstraction; however, they do\nnot solve the inherent challenges of distributed systems, such as\nsynchronization issues or resilience to failures. This paper introduces Nefele,\na decentralized process orchestration system that automatically deploys and\nmanages individual processes, rather than containers/VMs, within a cluster.\nNefele is inspired by the Single System Image (SSI) vision of mitigating the\nintricacies of remote execution, yet it maintains the flexibility and\nperformance of virtualized infrastructures. Nefele offers a set of APIs for\nbuilding cloud-native applications that lets the developer easily build,\ndeploy, and scale applications in a cloud environment. We have implemented and\ndeployed Nefele on a cluster in our datacenter and evaluated its performance.\nOur evaluations show that Nefele can effectively deploy, scale, and monitor\nprocesses across a distributed environment, while it incorporates essential\nprimitives to build a distributed software system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:21:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:32:14 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Sedaghat", "Mina", ""], ["Sk\u00f6ldstr\u00f6m", "Pontus", ""], ["Turull", "Daniel", ""], ["Yadhav", "Vinay", ""], ["Hal\u00e9n", "Joacim", ""], ["Ganesan", "Madhubala", ""], ["Mehta", "Amardeep", ""], ["John", "Wolfgang", ""]]}, {"id": "2006.07169", "submitter": "Filippos Christianos", "authors": "Filippos Christianos, Lukas Sch\\\"afer, Stefano V. Albrecht", "title": "Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning", "comments": "Published in 34th Conference on Neural Information Processing Systems\n  (NeurIPS), see\n  https://proceedings.neurips.cc/paper/2020/hash/7967cc8e3ab559e68cc944c44b1cf3e8-Abstract.html\n  - This updated version of the paper is identical to the original paper\n  published at NeurIPS 2020 but includes minor clarifications following\n  recommendations in\n  http://agents.inf.ed.ac.uk/blog/multiagent-rl-inaccuracies/", "journal-ref": "Advances in Neural Information Processing System 33 (2020)\n  10707-10717", "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration in multi-agent reinforcement learning is a challenging problem,\nespecially in environments with sparse rewards. We propose a general method for\nefficient exploration by sharing experience amongst agents. Our proposed\nalgorithm, called Shared Experience Actor-Critic (SEAC), applies experience\nsharing in an actor-critic framework. We evaluate SEAC in a collection of\nsparse-reward multi-agent environments and find that it consistently\noutperforms two baselines and two state-of-the-art algorithms by learning in\nfewer steps and converging to higher returns. In some harder environments,\nexperience sharing makes the difference between learning to solve the task and\nnot learning at all.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:24:50 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 10:33:36 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 14:39:24 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 11:13:46 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Christianos", "Filippos", ""], ["Sch\u00e4fer", "Lukas", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2006.07200", "submitter": "Simon Vanneste", "authors": "Simon Vanneste, Astrid Vanneste, Siegfried Mercelis and Peter\n  Hellinckx", "title": "Learning to Communicate Using Counterfactual Reasoning", "comments": "Submitted to NeurIPS2020. Contains 10 pages with 9 figures and 4\n  appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach for multi-agent communication learning\ncalled multi-agent counterfactual communication (MACC) learning. Many\nreal-world problems are currently tackled using multi-agent techniques.\nHowever, in many of these tasks the agents do not observe the full state of the\nenvironment but only a limited observation. This absence of knowledge about the\nfull state makes completing the objectives significantly more complex or even\nimpossible. The key to this problem lies in sharing observation information\nbetween agents or learning how to communicate the essential data. In this paper\nwe present a novel multi-agent communication learning approach called MACC. It\naddresses the partial observability problem of the agents. MACC lets the agent\nlearn the action policy and the communication policy simultaneously. We focus\non decentralized Markov Decision Processes (Dec-MDP), where the agents have\njoint observability. This means that the full state of the environment can be\ndetermined using the observations of all agents. MACC uses counterfactual\nreasoning to train both the action and the communication policy. This allows\nthe agents to anticipate on how other agents will react to certain messages and\non how the environment will react to certain actions, allowing them to learn\nmore effective policies. MACC uses actor-critic with a centralized critic and\ndecentralized actors. The critic is used to calculate an advantage for both the\naction and communication policy. We demonstrate our method by applying it on\nthe Simple Reference Particle environment of OpenAI and a MNIST game. Our\nresults are compared with a communication and non-communication baseline. These\nexperiments demonstrate that MACC is able to train agents for each of these\nproblems with effective communication policies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:02:04 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Vanneste", "Simon", ""], ["Vanneste", "Astrid", ""], ["Mercelis", "Siegfried", ""], ["Hellinckx", "Peter", ""]]}, {"id": "2006.07228", "submitter": "Tao Sun", "authors": "Mohammad Rasouli, Tao Sun, Ram Rajagopal", "title": "FedGAN: Federated Generative Adversarial Networks for Distributed Data", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Federated Generative Adversarial Network (FedGAN) for training a\nGAN across distributed sources of non-independent-and-identically-distributed\ndata sources subject to communication and privacy constraints. Our algorithm\nuses local generators and discriminators which are periodically synced via an\nintermediary that averages and broadcasts the generator and discriminator\nparameters. We theoretically prove the convergence of FedGAN with both equal\nand two time-scale updates of generator and discriminator, under standard\nassumptions, using stochastic approximations and communication efficient\nstochastic gradient descents. We experiment FedGAN on toy examples (2D system,\nmixed Gaussian, and Swiss role), image datasets (MNIST, CIFAR-10, and CelebA),\nand time series datasets (household electricity consumption and electric\nvehicle charging sessions). We show FedGAN converges and has similar\nperformance to general distributed GAN, while reduces communication complexity.\nWe also show its robustness to reduced communications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:36:43 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 06:38:12 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rasouli", "Mohammad", ""], ["Sun", "Tao", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2006.07301", "submitter": "Neda Navidi", "authors": "Neda Navidi, Francoi Chabo, Saga Kurandwa, Iv Lutigma, Vincent Robt,\n  Gregry Szrftgr, Andea Schuh", "title": "Human and Multi-Agent collaboration in a human-MARL teaming framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning provides effective results with agents learning from\ntheir observations, received rewards, and internal interactions between agents.\nThis study proposes a new open-source MARL framework, called COGMENT, to\nefficiently leverage human and agent interactions as a source of learning. We\ndemonstrate these innovations by using a designed real-time environment with\nunmanned aerial vehicles driven by RL agents, collaborating with a human. The\nresults of this study show that the proposed collaborative paradigm and the\nopen-source framework leads to significant reductions in both human effort and\nexploration costs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:32:42 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 21:24:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Navidi", "Neda", ""], ["Chabo", "Francoi", ""], ["Kurandwa", "Saga", ""], ["Lutigma", "Iv", ""], ["Robt", "Vincent", ""], ["Szrftgr", "Gregry", ""], ["Schuh", "Andea", ""]]}, {"id": "2006.07443", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "Algorithm for Computing Approximate Nash Equilibrium in Continuous Games\n  with Application to Continuous Blotto", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA econ.TH math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful algorithms have been developed for computing Nash equilibrium in a\nvariety of finite game classes. However, solving continuous games -- in which\nthe pure strategy space is (potentially uncountably) infinite -- is far more\nchallenging. Nonetheless, many real-world domains have continuous action\nspaces, e.g., where actions refer to an amount of time, money, or other\nresource that is naturally modeled as being real-valued as opposed to integral.\nWe present a new algorithm for {approximating} Nash equilibrium strategies in\ncontinuous games. In addition to two-player zero-sum games, our algorithm also\napplies to multiplayer games and games with imperfect information. We\nexperiment with our algorithm on a continuous imperfect-information Blotto\ngame, in which two players distribute resources over multiple battlefields.\nBlotto games have frequently been used to model national security scenarios and\nhave also been applied to electoral competition and auction theory. Experiments\nshow that our algorithm is able to quickly compute close approximations of Nash\nequilibrium strategies for this game.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:53:18 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 06:33:21 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 00:34:42 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 09:15:14 GMT"}, {"version": "v5", "created": "Tue, 1 Jun 2021 06:46:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "2006.07869", "submitter": "Georgios Papoudakis", "authors": "Georgios Papoudakis, Filippos Christianos, Lukas Sch\\\"afer, Stefano V.\n  Albrecht", "title": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in\n  Cooperative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent deep reinforcement learning (MARL) suffers from a lack of\ncommonly-used evaluation tasks and criteria, making comparisons between\napproaches difficult. In this work, we consistently evaluate and compare three\ndifferent classes of MARL algorithms (independent learning, centralised\nmulti-agent policy gradient, value decomposition) in a diverse range of\ncooperative multi-agent learning tasks. Our experiments serve as a reference\nfor the expected performance of algorithms across different learning tasks, and\nwe provide insights regarding the effectiveness of different learning\napproaches. We open-source EPyMARL, which extends the PyMARL\ncodebase~\\citep{samvelyan19smac} to include additional algorithms and allow for\nflexible configuration of algorithm implementation details such as parameter\nsharing. Finally, we open-source two environments for multi-agent research\nwhich focus on coordination under sparse rewards.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 11:22:53 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 10:56:58 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 10:53:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Papoudakis", "Georgios", ""], ["Christianos", "Filippos", ""], ["Sch\u00e4fer", "Lukas", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2006.07969", "submitter": "Esmaeil Seraj", "authors": "Esmaeil Seraj and Matthew Gombolay", "title": "Coordinated Control of UAVs for Human-Centered Active Sensing of\n  Wildfires", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.RO cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fighting wildfires is a precarious task, imperiling the lives of engaging\nfirefighters and those who reside in the fire's path. Firefighters need online\nand dynamic observation of the firefront to anticipate a wildfire's unknown\ncharacteristics, such as size, scale, and propagation velocity, and to plan\naccordingly. In this paper, we propose a distributed control framework to\ncoordinate a team of unmanned aerial vehicles (UAVs) for a human-centered\nactive sensing of wildfires. We develop a dual-criterion objective function\nbased on Kalman uncertainty residual propagation and weighted multi-agent\nconsensus protocol, which enables the UAVs to actively infer the wildfire\ndynamics and parameters, track and monitor the fire transition, and safely\nmanage human firefighters on the ground using acquired information. We evaluate\nour approach relative to prior work, showing significant improvements by\nreducing the environment's cumulative uncertainty residual by more than $ 10^2\n$ and $ 10^5 $ times in firefront coverage performance to support human-robot\nteaming for firefighting. We also demonstrate our method on physical robots in\na mock firefighting exercise.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 18:26:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Seraj", "Esmaeil", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2006.08212", "submitter": "Raphael Berthier", "authors": "Rapha\\\"el Berthier (PSL, SIERRA), Francis Bach (SIERRA, PSL), Pierre\n  Gaillard (SIERRA, PSL, Thoth)", "title": "Tight Nonparametric Convergence Rates for Stochastic Gradient Descent\n  under the Noiseless Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of statistical supervised learning, the noiseless linear model\nassumes that there exists a deterministic linear relation $Y = \\langle\n\\theta_*, X \\rangle$ between the random output $Y$ and the random feature\nvector $\\Phi(U)$, a potentially non-linear transformation of the inputs $U$. We\nanalyze the convergence of single-pass, fixed step-size stochastic gradient\ndescent on the least-square risk under this model. The convergence of the\niterates to the optimum $\\theta_*$ and the decay of the generalization error\nfollow polynomial convergence rates with exponents that both depend on the\nregularities of the optimum $\\theta_*$ and of the feature vectors $\\Phi(u)$. We\ninterpret our result in the reproducing kernel Hilbert space framework. As a\nspecial case, we analyze an online algorithm for estimating a real function on\nthe unit interval from the noiseless observation of its value at randomly\nsampled points; the convergence depends on the Sobolev smoothness of the\nfunction and of a chosen kernel. Finally, we apply our analysis beyond the\nsupervised learning setting to obtain convergence rates for the averaging\nprocess (a.k.a. gossip algorithm) on a graph depending on its spectral\ndimension.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:25:50 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 08:38:32 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Berthier", "Rapha\u00ebl", "", "PSL, SIERRA"], ["Bach", "Francis", "", "SIERRA, PSL"], ["Gaillard", "Pierre", "", "SIERRA, PSL, Thoth"]]}, {"id": "2006.08333", "submitter": "Sasanka Sekhar Chanda", "authors": "Sasanka Sekhar Chanda and Sai Yayavaram", "title": "An Algorithm to find Superior Fitness on NK Landscapes under High\n  Complexity: Muddling Through", "comments": "6 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under high complexity - given by pervasive interdependence between\nconstituent elements of a decision in an NK landscape - our algorithm obtains\nfitness superior to that reported in extant research. We distribute the\ndecision elements comprising a decision into clusters. When a change in value\nof a decision element is considered, a forward move is made if the aggregate\nfitness of the cluster members residing alongside the decision element is\nhigher. The decision configuration with the highest fitness in the path is\nselected. Increasing the number of clusters obtains even higher fitness.\nFurther, implementing moves comprising of up to two changes in a cluster also\nobtains higher fitness. Our algorithm obtains superior outcomes by enabling\nmore extensive search, allowing inspection of more distant configurations. We\nname this algorithm the muddling through algorithm, in memory of Charles\nLindblom who spotted the efficacy of the process long before sophisticated\ncomputer simulations came into being.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 11:08:20 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 12:12:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Chanda", "Sasanka Sekhar", ""], ["Yayavaram", "Sai", ""]]}, {"id": "2006.08555", "submitter": "Stephen McAleer", "authors": "Stephen McAleer, John Lanier, Roy Fox, Pierre Baldi", "title": "Pipeline PSRO: A Scalable Approach for Finding Approximate Nash\n  Equilibria in Large Games", "comments": "SM and JL contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding approximate Nash equilibria in zero-sum imperfect-information games\nis challenging when the number of information states is large. Policy Space\nResponse Oracles (PSRO) is a deep reinforcement learning algorithm grounded in\ngame theory that is guaranteed to converge to an approximate Nash equilibrium.\nHowever, PSRO requires training a reinforcement learning policy at each\niteration, making it too slow for large games. We show through counterexamples\nand experiments that DCH and Rectified PSRO, two existing approaches to scaling\nup PSRO, fail to converge even in small games. We introduce Pipeline PSRO\n(P2SRO), the first scalable general method for finding approximate Nash\nequilibria in large zero-sum imperfect-information games. P2SRO is able to\nparallelize PSRO with convergence guarantees by maintaining a hierarchical\npipeline of reinforcement learning workers, each training against the policies\ngenerated by lower levels in the hierarchy. We show that unlike existing\nmethods, P2SRO converges to an approximate Nash equilibrium, and does so faster\nas the number of parallel workers increases, across a variety of imperfect\ninformation games. We also introduce an open-source environment for Barrage\nStratego, a variant of Stratego with an approximate game tree complexity of\n$10^{50}$. P2SRO is able to achieve state-of-the-art performance on Barrage\nStratego and beats all existing bots. Experiment code is available\nathttps://github.com/JBLanier/pipeline-psro.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:17:17 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 19:26:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["McAleer", "Stephen", ""], ["Lanier", "John", ""], ["Fox", "Roy", ""], ["Baldi", "Pierre", ""]]}, {"id": "2006.08742", "submitter": "Michael Curry", "authors": "Michael J. Curry, Ping-Yeh Chiang, Tom Goldstein, John Dickerson", "title": "Certifying Strategyproof Auction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal auctions maximize a seller's expected revenue subject to individual\nrationality and strategyproofness for the buyers. Myerson's seminal work in\n1981 settled the case of auctioning a single item; however, subsequent decades\nof work have yielded little progress moving beyond a single item, leaving the\ndesign of revenue-maximizing auctions as a central open problem in the field of\nmechanism design. A recent thread of work in \"differentiable economics\" has\nused tools from modern deep learning to instead learn good mechanisms. We focus\non the RegretNet architecture, which can represent auctions with arbitrary\nnumbers of items and participants; it is trained to be empirically\nstrategyproof, but the property is never exactly verified leaving potential\nloopholes for market participants to exploit. We propose ways to explicitly\nverify strategyproofness under a particular valuation profile using techniques\nfrom the neural network verification literature. Doing so requires making\nseveral modifications to the RegretNet architecture in order to represent it\nexactly in an integer program. We train our network and produce certificates in\nseveral settings, including settings for which the optimal strategyproof\nmechanism is not known.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:22:48 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Curry", "Michael J.", ""], ["Chiang", "Ping-Yeh", ""], ["Goldstein", "Tom", ""], ["Dickerson", "John", ""]]}, {"id": "2006.08845", "submitter": "Kyle Jordan Brown", "authors": "Kyle Brown, Oriana Peltzer, Martin A. Sehr, Mac Schwager, Mykel J.\n  Kochenderfer", "title": "Optimal Sequential Task Assignment and Path Finding for Multi-Agent\n  Robotic Assembly Planning", "comments": "Presented at International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": "International Conference on Robotics and Automation (ICRA) 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sequential task assignment and collision-free routing\nfor large teams of robots in applications with inter-task precedence\nconstraints (e.g., task $A$ and task $B$ must both be completed before task $C$\nmay begin). Such problems commonly occur in assembly planning for robotic\nmanufacturing applications, in which sub-assemblies must be completed before\nthey can be combined to form the final product. We propose a hierarchical\nalgorithm for computing makespan-optimal solutions to the problem. The\nalgorithm is evaluated on a set of randomly generated problem instances where\nrobots must transport objects between stations in a \"factory \"grid world\nenvironment. In addition, we demonstrate in high-fidelity simulation that the\noutput of our algorithm can be used to generate collision-free trajectories for\nnon-holonomic differential-drive robots.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:45:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Brown", "Kyle", ""], ["Peltzer", "Oriana", ""], ["Sehr", "Martin A.", ""], ["Schwager", "Mac", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2006.09066", "submitter": "Mohamed Sana", "authors": "Mohamed Sana, Antonio De Domenico, Wei Yu, Yves Lostanlen, and Emilio\n  Calvanese Strinati", "title": "Multi-Agent Reinforcement Learning for Adaptive User Association in\n  Dynamic mmWave Networks", "comments": "Part of this work has been presented in IEEE Globecom 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network densification and millimeter-wave technologies are key enablers to\nfulfill the capacity and data rate requirements of the fifth generation (5G) of\nmobile networks. In this context, designing low-complexity policies with local\nobservations, yet able to adapt the user association with respect to the global\nnetwork state and to the network dynamics is a challenge. In fact, the\nframeworks proposed in literature require continuous access to global network\ninformation and to recompute the association when the radio environment\nchanges. With the complexity associated to such an approach, these solutions\nare not well suited to dense 5G networks. In this paper, we address this issue\nby designing a scalable and flexible algorithm for user association based on\nmulti-agent reinforcement learning. In this approach, users act as independent\nagents that, based on their local observations only, learn to autonomously\ncoordinate their actions in order to optimize the network sum-rate. Since there\nis no direct information exchange among the agents, we also limit the signaling\noverhead. Simulation results show that the proposed algorithm is able to adapt\nto (fast) changes of radio environment, thus providing large sum-rate gain in\ncomparison to state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:51:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Sana", "Mohamed", ""], ["De Domenico", "Antonio", ""], ["Yu", "Wei", ""], ["Lostanlen", "Yves", ""], ["Strinati", "Emilio Calvanese", ""]]}, {"id": "2006.09447", "submitter": "Georgios Papoudakis", "authors": "Georgios Papoudakis, Filippos Christianos, Stefano V. Albrecht", "title": "Local Information Agent Modelling in Partially-Observable Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the behaviours of other agents is essential for understanding how\nagents interact and making effective decisions. Existing methods for agent\nmodelling commonly assume knowledge of the local observations and chosen\nactions of the modelled agents during execution. To eliminate this assumption,\nwe extract representations from the local information of the controlled agent\nusing encoder-decoder architectures. Using the observations and actions of the\nmodelled agents during training, our models learn to extract representations\nabout the modelled agents conditioned only on the local observations of the\ncontrolled agent. The representations are used to augment the controlled\nagent's decision policy which is trained via deep reinforcement learning; thus,\nduring execution, the policy does not require access to other agents'\ninformation. We provide a comprehensive evaluation and ablations studies in\ncooperative, competitive and mixed multi-agent environments, showing that our\nmethod achieves significantly higher returns than baseline methods which do not\nuse the learned representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:43:42 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:29:26 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 20:54:30 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Papoudakis", "Georgios", ""], ["Christianos", "Filippos", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2006.09718", "submitter": "V\\'aclav Uhl\\'i\\v{r}", "authors": "Vaclav Uhlir, Frantisek Zboril, Frantisek Vidensky", "title": "Multi-Agent Programming Contest 2019 FIT BUT Team solution", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During our participation in MAPC 2019, we have developed two multi-agent\nsystems that have been designed specifically for this competition. The first of\nthe systems is pro-active system that works with pre-specified scenarios and\ntasks agents with generated goals designed for individual agents according to\nassigned role. The second system is designed as more reactive and employs\nlayered architecture with highly dynamic behaviour, where agents select their\nown action based on their perception of usefulness of said action.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 08:37:51 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 20:23:04 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 11:57:09 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Uhlir", "Vaclav", ""], ["Zboril", "Frantisek", ""], ["Vidensky", "Frantisek", ""]]}, {"id": "2006.10232", "submitter": "Celia Ralha", "authors": "Denis J. S. de Albuquerque, Vanessa Tavares Nunes, Claudia Cappelli,\n  Celia Ghedini Ralha", "title": "Towards Auditability Requirements Specification Using an Agent-Based\n  Approach", "comments": "19 pages, 11 figures, 1 table, see\n  https://aircconline.com/abstract/ijsea/v11n3/11320ijsea02.html", "journal-ref": "International Journal of Software Engineering & Applications\n  (IJSEA), Vol.11, No.3, May 2020", "doi": "10.5121/ijsea.2020.11302", "report-no": null, "categories": "cs.MA cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency is an important factor in democratic societies composed of\ncharacteristics such as accessibility, usability, informativeness,\nunderstandability and auditability. In this research we focus on auditability\nsince it plays an important role for citizens that need to understand and audit\npublic information. Although auditability has been a subject of discussion when\ndesigning systems, there is a lack of systematization in its specification. We\npropose an approach to systematically add auditability requirements\nspecification during the goal-oriented agent-based Tropos methodology. We used\nthe Transparency Softgoal Interdependency Graph that captures the different\nfacets of transparency while considering their operationalization. An empirical\nevaluation was conducted through the design and implementation of LawDisTrA\nsystem that distributes lawsuits among judges in an appellate court.\nExperiments included the distribution of over 300,000 lawsuits at the Brazilian\nSuperior Labor Court. We theorize that the presented approach for auditability\nprovides adequate techniques to address the cross-organizational nature of\ntransparency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:52:31 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["de Albuquerque", "Denis J. S.", ""], ["Nunes", "Vanessa Tavares", ""], ["Cappelli", "Claudia", ""], ["Ralha", "Celia Ghedini", ""]]}, {"id": "2006.10241", "submitter": "Aritra Guha", "authors": "Aritra Guha, Rayleigh Lei, Jiacheng Zhu, XuanLong Nguyen and Ding Zhao", "title": "Robust Unsupervised Learning of Temporal Dynamic Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust representation learning of temporal dynamic interactions is an\nimportant problem in robotic learning in general and automated unsupervised\nlearning in particular. Temporal dynamic interactions can be described by\n(multiple) geometric trajectories in a suitable space over which unsupervised\nlearning techniques may be applied to extract useful features from raw and\nhigh-dimensional data measurements. Taking a geometric approach to robust\nrepresentation learning for temporal dynamic interactions, it is necessary to\ndevelop suitable metrics and a systematic methodology for comparison and for\nassessing the stability of an unsupervised learning method with respect to its\ntuning parameters. Such metrics must account for the (geometric) constraints in\nthe physical world as well as the uncertainty associated with the learned\npatterns. In this paper we introduce a model-free metric based on the\nProcrustes distance for robust representation learning of interactions, and an\noptimal transport based distance metric for comparing between distributions of\ninteraction primitives. These distance metrics can serve as an objective for\nassessing the stability of an interaction learning algorithm. They are also\nused for comparing the outcomes produced by different algorithms. Moreover,\nthey may also be adopted as an objective function to obtain clusters and\nrepresentative interaction primitives. These concepts and techniques will be\nintroduced, along with mathematical properties, while their usefulness will be\ndemonstrated in unsupervised learning of vehicle-to-vechicle interactions\nextracted from the Safety Pilot database, the world's largest database for\nconnected vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 02:39:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Guha", "Aritra", ""], ["Lei", "Rayleigh", ""], ["Zhu", "Jiacheng", ""], ["Nguyen", "XuanLong", ""], ["Zhao", "Ding", ""]]}, {"id": "2006.10412", "submitter": "Muhammad Arrasy Rahman", "authors": "Arrasy Rahman, Niklas H\\\"opner, Filippos Christianos, Stefano V.\n  Albrecht", "title": "Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning", "comments": "Published in the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad hoc teamwork is the challenging problem of designing an autonomous agent\nwhich can adapt quickly to collaborate with teammates without prior\ncoordination mechanisms, including joint training. Prior work in this area has\nfocused on closed teams in which the number of agents is fixed. In this work,\nwe consider open teams by allowing agents with different fixed policies to\nenter and leave the environment without prior notification. Our solution builds\non graph neural networks to learn agent models and joint-action value models\nunder varying team compositions. We contribute a novel action-value computation\nthat integrates the agent model and joint-action value model to produce\naction-value estimates. We empirically demonstrate that our approach\nsuccessfully models the effects other agents have on the learner, leading to\npolicies that robustly adapt to dynamic team compositions and significantly\noutperform several alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 10:39:41 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 10:21:23 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 15:58:03 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 16:23:52 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Rahman", "Arrasy", ""], ["H\u00f6pner", "Niklas", ""], ["Christianos", "Filippos", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2006.10422", "submitter": "Jesse Mulderij", "authors": "Jesse Mulderij (1) and Bob Huisman (2) and Denise T\\\"onissen (3) and\n  Koos van der Linden (1) and Mathijs de Weerdt (1) ((1) Delft University of\n  Technology, (2) Nederlandse Spoorwegen, (3) Vrije Universiteit Amsterdam)", "title": "Train Unit Shunting and Servicing: a Real-Life Application of\n  Multi-Agent Path Finding", "comments": "14 pages, 2 figures, to be published in the 4th International\n  Workshop on Multi-agent Path Finding (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In between transportation services, trains are parked and maintained at\nshunting yards. The conflict-free routing of trains to and on these yards and\nthe scheduling of service and maintenance tasks is known as the train unit\nshunting and service problem. Efficient use of the capacity of these yards is\nbecoming increasingly important, because of increasing numbers of trains\nwithout proportional extensions of the yards. Efficiently scheduling\nmaintenance activities is extremely challenging: currently only heuristics\nsucceed in finding solutions to the integrated problem at all. Bounds are\nneeded to determine the quality of these heuristics, and also to support\ninvestment decisions on increasing the yard capacity. For this, a complete\nalgorithm for a possibly relaxed problem model is required. We analyze the\npotential of extending the model for multi-agent path finding to be used for\nsuch a relaxation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 10:57:12 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mulderij", "Jesse", ""], ["Huisman", "Bob", ""], ["T\u00f6nissen", "Denise", ""], ["van der Linden", "Koos", ""], ["de Weerdt", "Mathijs", ""]]}, {"id": "2006.10611", "submitter": "Manish Prajapat", "authors": "Manish Prajapat, Kamyar Azizzadenesheli, Alexander Liniger, Yisong\n  Yue, Anima Anandkumar", "title": "Competitive Policy Optimization", "comments": "11 pages main paper, 6 pages references, and 31 pages appendix. 14\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core challenge in policy optimization in competitive Markov decision\nprocesses is the design of efficient optimization methods with desirable\nconvergence and stability properties. To tackle this, we propose competitive\npolicy optimization (CoPO), a novel policy gradient approach that exploits the\ngame-theoretic nature of competitive games to derive policy updates. Motivated\nby the competitive gradient optimization method, we derive a bilinear\napproximation of the game objective. In contrast, off-the-shelf policy gradient\nmethods utilize only linear approximations, and hence do not capture\ninteractions among the players. We instantiate CoPO in two ways:(i) competitive\npolicy gradient, and (ii) trust-region competitive policy optimization. We\ntheoretically study these methods, and empirically investigate their behavior\non a set of comprehensive, yet challenging, competitive games. We observe that\nthey provide stable optimization, convergence to sophisticated strategies, and\nhigher scores when played against baseline policy gradient methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:31:09 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Prajapat", "Manish", ""], ["Azizzadenesheli", "Kamyar", ""], ["Liniger", "Alexander", ""], ["Yue", "Yisong", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2006.10666", "submitter": "Mofeng Yang", "authors": "Yao Xiao, Mofeng Yang, Zheng Zhu, Hai Yang, Lei Zhang and Sepehr\n  Ghader", "title": "Modeling indoor-level non-pharmaceutical interventions during the\n  COVID-19 pandemic: a pedestrian dynamics-based microscopic simulation\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical modeling of epidemic spreading has been widely adopted to\nestimate the threats of epidemic diseases (i.e., the COVID-19 pandemic) as well\nas to evaluate epidemic control interventions. The indoor place is considered\nto be a significant epidemic spreading risk origin, but existing widely-used\nepidemic spreading models are usually limited for indoor places since the\ndynamic physical distance changes between people are ignored, and the empirical\nfeatures of the essential and non-essential travel are not differentiated. In\nthis paper, we introduce a pedestrian-based epidemic spreading model that is\ncapable of modeling indoor transmission risks of diseases during people's\nsocial activities. Taking advantage of the before-and-after mobility data from\nthe University of Maryland COVID-19 Impact Analysis Platform, it's found that\npeople tend to spend more time in grocery stores once their travel frequencies\nare restricted to a low level. In other words, an increase in dwell time could\nbalance the decrease in travel frequencies and satisfy people's demand. Based\non the pedestrian-based model and the empirical evidence, combined\nnon-pharmaceutical interventions from different operational levels are\nevaluated. Numerical simulations show that restrictions on people's travel\nfrequency and open-hours of indoor places may not be universally effective in\nreducing average infection risks for each pedestrian who visit the place. Entry\nlimitations can be a widely effective alternative, whereas the decision-maker\nneeds to balance the decrease in risky contacts and the increase in queue\nlength outside the place that may impede people from fulfilling their travel\nneeds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:47:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Xiao", "Yao", ""], ["Yang", "Mofeng", ""], ["Zhu", "Zheng", ""], ["Yang", "Hai", ""], ["Zhang", "Lei", ""], ["Ghader", "Sepehr", ""]]}, {"id": "2006.10800", "submitter": "Bei Peng", "authors": "Tabish Rashid, Gregory Farquhar, Bei Peng, Shimon Whiteson", "title": "Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep\n  Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QMIX is a popular $Q$-learning algorithm for cooperative MARL in the\ncentralised training and decentralised execution paradigm. In order to enable\neasy decentralisation, QMIX restricts the joint action $Q$-values it can\nrepresent to be a monotonic mixing of each agent's utilities. However, this\nrestriction prevents it from representing value functions in which an agent's\nordering over its actions can depend on other agents' actions. To analyse this\nrepresentational limitation, we first formalise the objective QMIX optimises,\nwhich allows us to view QMIX as an operator that first computes the\n$Q$-learning targets and then projects them into the space representable by\nQMIX. This projection returns a representable $Q$-value that minimises the\nunweighted squared error across all joint actions. We show in particular that\nthis projection can fail to recover the optimal policy even with access to\n$Q^*$, which primarily stems from the equal weighting placed on each joint\naction. We rectify this by introducing a weighting into the projection, in\norder to place more importance on the better joint actions. We propose two\nweighting schemes and prove that they recover the correct maximal action for\nany joint action $Q$-values, and therefore for $Q^*$ as well. Based on our\nanalysis and results in the tabular setting, we introduce two scalable versions\nof our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW)\nQMIX and demonstrate improved performance on both predator-prey and challenging\nmulti-agent StarCraft benchmark tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:34:50 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:58:45 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Rashid", "Tabish", ""], ["Farquhar", "Gregory", ""], ["Peng", "Bei", ""], ["Whiteson", "Shimon", ""]]}, {"id": "2006.10897", "submitter": "Oscar De Lima", "authors": "Oscar de Lima, Hansal Shah, Ting-Sheng Chu, Brian Fogelson", "title": "Efficient Ridesharing Dispatch Using Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of ride-sharing services, there is a huge increase in the\nnumber of people who rely on them for various needs. Most of the earlier\napproaches tackling this issue required handcrafted functions for estimating\ntravel times and passenger waiting times. Traditional Reinforcement Learning\n(RL) based methods attempting to solve the ridesharing problem are unable to\naccurately model the complex environment in which taxis operate. Prior\nMulti-Agent Deep RL based methods based on Independent DQN (IDQN) learn\ndecentralized value functions prone to instability due to the concurrent\nlearning and exploring of multiple agents. Our proposed method based on QMIX is\nable to achieve centralized training with decentralized execution. We show that\nour model performs better than the IDQN baseline on a fixed grid size and is\nable to generalize well to smaller or larger grid sizes. Also, our algorithm is\nable to outperform IDQN baseline in the scenario where we have a variable\nnumber of passengers and cars in each episode. Code for our paper is publicly\navailable at: https://github.com/UMich-ML-Group/RL-Ridesharing.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 23:37:53 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["de Lima", "Oscar", ""], ["Shah", "Hansal", ""], ["Chu", "Ting-Sheng", ""], ["Fogelson", "Brian", ""]]}, {"id": "2006.11097", "submitter": "Antonis Bikakis Dr.", "authors": "Antonis Bikakis, Patrice Caire", "title": "Contextual and Possibilistic Reasoning for Coalition Formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiagent systems, agents often have to rely on other agents to reach\ntheir goals, for example when they lack a needed resource or do not have the\ncapability to perform a required action. Agents therefore need to cooperate.\nThen, some of the questions raised are: Which agent(s) to cooperate with? What\nare the potential coalitions in which agents can achieve their goals? As the\nnumber of possibilities is potentially quite large, how to automate the\nprocess? And then, how to select the most appropriate coalition, taking into\naccount the uncertainty in the agents' abilities to carry out certain tasks? In\nthis article, we address the question of how to find and evaluate coalitions\namong agents in multiagent systems using MCS tools, while taking into\nconsideration the uncertainty around the agents' actions. Our methodology is\nthe following: We first compute the solution space for the formation of\ncoalitions using a contextual reasoning approach. Second, we model agents as\ncontexts in Multi-Context Systems (MCS), and dependence relations among agents\nseeking to achieve their goals, as bridge rules. Third, we systematically\ncompute all potential coalitions using algorithms for MCS equilibria, and given\na set of functional and non-functional requirements, we propose ways to select\nthe best solutions. Finally, in order to handle the uncertainty in the agents'\nactions, we extend our approach with features of possibilistic reasoning. We\nillustrate our approach with an example from robotics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 11:59:55 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 18:45:37 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Bikakis", "Antonis", ""], ["Caire", "Patrice", ""]]}, {"id": "2006.11156", "submitter": "Tarun Chitra", "authors": "Tarun Chitra, Alex Evans", "title": "Why Stake When You Can Borrow?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.MA q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smart contract platforms autonomously manage billions of dollars of\ncapital, quantifying the portfolio risk that investors engender in these\nsystems is increasingly important. Recent work illustrates that Proof of Stake\n(PoS) is vulnerable to financial attacks arising from on-chain lending and has\nworse capital efficiency than Proof of Work (PoW) \\cite{fanti_pos_econ}.\nNumerous methods for improving capital efficiency have been proposed that allow\nstakers to create fungible derivative claims on their staked assets. In this\npaper, we construct a unifying model for studying the security risks of these\nproposals. This model combines birth-death P\\'olya processes and risk models\nadapted from the credit derivatives literature to assess token inequality and\nreturn profiles. We find that there is a sharp transition between 'safe' and\n'unsafe' derivative usage. Surprisingly, we find that contrary to\n\\cite{fanti2019compounding} there exist conditions where derivatives can\n\\emph{reduce} concentration of wealth in these networks. This model also\napplies to Decentralized Finance (DeFi) protocols where staked assets are used\nas insurance. Our theoretical results are validated using agent-based\nsimulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:59:36 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Chitra", "Tarun", ""], ["Evans", "Alex", ""]]}, {"id": "2006.11438", "submitter": "Sheng Li", "authors": "Sheng Li, Jayesh K. Gupta, Peter Morales, Ross Allen, Mykel J.\n  Kochenderfer", "title": "Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) requires coordination to\nefficiently solve certain tasks. Fully centralized control is often infeasible\nin such domains due to the size of joint action spaces. Coordination graph\nbased formalization allows reasoning about the joint action based on the\nstructure of interactions. However, they often require domain expertise in\ntheir design. This paper introduces the deep implicit coordination graph (DICG)\narchitecture for such scenarios. DICG consists of a module for inferring the\ndynamic coordination graph structure which is then used by a graph neural\nnetwork based module to learn to implicitly reason about the joint actions or\nvalues. DICG allows learning the tradeoff between full centralization and\ndecentralization via standard actor-critic methods to significantly improve\ncoordination for domains with large number of agents. We apply DICG to both\ncentralized-training-centralized-execution and\ncentralized-training-decentralized-execution regimes. We demonstrate that DICG\nsolves the relative overgeneralization pathology in predatory-prey tasks as\nwell as outperforms various MARL baselines on the challenging StarCraft II\nMulti-agent Challenge (SMAC) and traffic junction environments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 23:41:49 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 23:29:50 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Sheng", ""], ["Gupta", "Jayesh K.", ""], ["Morales", "Peter", ""], ["Allen", "Ross", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2006.11671", "submitter": "Elad Schneidman", "authors": "Benjamin Brazowski and Elad Schneidman", "title": "Collective Learning by Ensembles of Altruistic Diversifying Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining the predictions of collections of neural networks often outperforms\nthe best single network. Such ensembles are typically trained independently,\nand their superior `wisdom of the crowd' originates from the differences\nbetween networks. Collective foraging and decision making in socially\ninteracting animal groups is often improved or even optimal thanks to local\ninformation sharing between conspecifics. We therefore present a model for\nco-learning by ensembles of interacting neural networks that aim to maximize\ntheir own performance but also their functional relations to other networks. We\nshow that ensembles of interacting networks outperform independent ones, and\nthat optimal ensemble performance is reached when the coupling between networks\nincreases diversity and degrades the performance of individual networks. Thus,\neven without a global goal for the ensemble, optimal collective behavior\nemerges from local interactions between networks. We show the scaling of\noptimal coupling strength with ensemble size, and that networks in these\nensembles specialize functionally and become more `confident' in their\nassessments. Moreover, optimal co-learning networks differ structurally,\nrelying on sparser activity, a wider range of synaptic weights, and higher\nfiring rates - compared to independently trained networks. Finally, we explore\ninteractions-based co-learning as a framework for expanding and boosting\nensembles.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 22:53:32 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Brazowski", "Benjamin", ""], ["Schneidman", "Elad", ""]]}, {"id": "2006.11683", "submitter": "Desik Rengarajan", "authors": "Kiyeob Lee, Desik Rengarajan, Dileep Kalathil, Srinivas Shakkottai", "title": "Reinforcement Learning for Mean Field Games with Strategic\n  Complementarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field Games (MFG) are the class of games with a very large number of\nagents and the standard equilibrium concept is a Mean Field Equilibrium (MFE).\nAlgorithms for learning MFE in dynamic MFGs are unknown in general. Our focus\nis on an important subclass that possess a monotonicity property called\nStrategic Complementarities (MFG-SC). We introduce a natural refinement to the\nequilibrium concept that we call Trembling-Hand-Perfect MFE (T-MFE), which\nallows agents to employ a measure of randomization while accounting for the\nimpact of such randomization on their payoffs. We propose a simple algorithm\nfor computing T-MFE under a known model. We also introduce a model-free and a\nmodel-based approach to learning T-MFE and provide sample complexities of both\nalgorithms. We also develop a fully online learning scheme that obviates the\nneed for a simulator. Finally, we empirically evaluate the performance of the\nproposed algorithms via examples motivated by real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 00:31:48 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 18:56:37 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 17:14:51 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lee", "Kiyeob", ""], ["Rengarajan", "Desik", ""], ["Kalathil", "Dileep", ""], ["Shakkottai", "Srinivas", ""]]}, {"id": "2006.11694", "submitter": "Cleber Amaral Mr.", "authors": "Cleber Jorge Amaral, Stephen Cranefield, Jomi Fred H\\\"ubner, Mario\n  Lucio Roloff", "title": "Integrating Industrial Artifacts and Agents Through Apache Camel", "comments": "6 pages, 5 figures, accept to present on 14th Simp\\'osio Brasileiro\n  de Automa\\c{c}\\~ao Inteligente (SBAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many challenges for building up the smart factory, among them to\ndeal with distributed data, high volume of information, and wide diversity of\ndevices and applications. In this sense, Cyber-Physical System (CPS) concept\nemerges to virtualize and integrate factory resources. Based on studies that\nuse Multi-Agent System as the core of a CPS, in this paper, we show that many\nresources of the factories can be modelled following the well-known Agents and\nArtifacts method of integrating agents and their environment. To enhance the\ninteroperability of this system, we use Apache Camel framework, a middleware to\ndefine routes allowing the integration with a wide range of endpoints using\ndifferent protocols. Finally, we present a Camel component for artifacts,\ndesigned in this research, illustrating its use.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 02:43:19 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Amaral", "Cleber Jorge", ""], ["Cranefield", "Stephen", ""], ["H\u00fcbner", "Jomi Fred", ""], ["Roloff", "Mario Lucio", ""]]}, {"id": "2006.11769", "submitter": "Daniel Santiago Cuervo G\\'omez", "authors": "Santiago Cuervo and Marco Alzate", "title": "Emergent cooperation through mutual information maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With artificial intelligence systems becoming ubiquitous in our society, its\ndesigners will soon have to start to consider its social dimension, as many of\nthese systems will have to interact among them to work efficiently. With this\nin mind, we propose a decentralized deep reinforcement learning algorithm for\nthe design of cooperative multi-agent systems. The algorithm is based on the\nhypothesis that highly correlated actions are a feature of cooperative systems,\nand hence, we propose the insertion of an auxiliary objective of maximization\nof the mutual information between the actions of agents in the learning\nproblem. Our system is applied to a social dilemma, a problem whose optimal\nsolution requires that agents cooperate to maximize a macroscopic performance\nfunction despite the divergent individual objectives of each agent. By\ncomparing the performance of the proposed system to a system without the\nauxiliary objective, we conclude that the maximization of mutual information\namong agents promotes the emergence of cooperation in social dilemmas.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 11:15:55 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Cuervo", "Santiago", ""], ["Alzate", "Marco", ""]]}, {"id": "2006.12032", "submitter": "Aniq Ur Rahman", "authors": "Aniq Ur Rahman, Gourab Ghatak, Antonio De Domenico", "title": "An Online Algorithm for Computation Offloading in Non-Stationary\n  Environments", "comments": "5 pages, 5 figures. Accepted at IEEE Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the latency minimization problem in a task-offloading scenario,\nwhere multiple servers are available to the user equipment for outsourcing\ncomputational tasks. To account for the temporally dynamic nature of the\nwireless links and the availability of the computing resources, we model the\nserver selection as a multi-armed bandit (MAB) problem. In the considered MAB\nframework, rewards are characterized in terms of the end-to-end latency. We\npropose a novel online learning algorithm based on the principle of optimism in\nthe face of uncertainty, which outperforms the state-of-the-art algorithms by\nup to ~1s. Our results highlight the significance of heavily discounting the\npast rewards in dynamic environments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 07:00:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Rahman", "Aniq Ur", ""], ["Ghatak", "Gourab", ""], ["De Domenico", "Antonio", ""]]}, {"id": "2006.12388", "submitter": "Ariah Klages-Mundt", "authors": "Ariah Klages-Mundt, Dominik Harz, Lewis Gudgeon, Jun-You Liu, Andreea\n  Minca", "title": "Stablecoins 2.0: Economic Foundations and Risk-based Models", "comments": null, "journal-ref": null, "doi": "10.1145/3419614.3423261", "report-no": null, "categories": "econ.GN cs.CR cs.MA q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stablecoins are one of the most widely capitalized type of cryptocurrency.\nHowever, their risks vary significantly according to their design and are often\npoorly understood. We seek to provide a sound foundation for stablecoin theory,\nwith a risk-based functional characterization of the economic structure of\nstablecoins. First, we match existing economic models to the disparate set of\ncustodial systems. Next, we characterize the unique risks that emerge in\nnon-custodial stablecoins and develop a model framework that unifies existing\nmodels from economics and computer science. We further discuss how this\nmodeling framework is applicable to a wide array of cryptoeconomic systems,\nincluding cross-chain protocols, collateralized lending, and decentralized\nexchanges. These unique risks yield unanswered research questions that will\nform the crux of research in decentralized finance going forward.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:17:58 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 17:30:15 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 01:27:48 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Klages-Mundt", "Ariah", ""], ["Harz", "Dominik", ""], ["Gudgeon", "Lewis", ""], ["Liu", "Jun-You", ""], ["Minca", "Andreea", ""]]}, {"id": "2006.12470", "submitter": "Byung-Cheol Min", "authors": "Shaocheng Luo, Jonghoek Kim, Byung-Cheol Min", "title": "Asymptotic Boundary Shrink Control with Multi-robot Systems", "comments": "Preprint version. Published in IEEE Transactions on Systems, Man, and\n  Cybernetics: Systems", "journal-ref": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, Early\n  Access, 2020", "doi": "10.1109/TSMC.2020.3003824", "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmful marine spills, such as algae blooms and oil spills, damage ecosystems\nand threaten public health tremendously. Hence, an effective spill coverage and\nremoval strategy will play a significant role in environmental protection. In\nrecent years, low-cost water surface robots have emerged as a solution, with\ntheir efficacy verified at small scale. However, practical limitations such as\nconnectivity, scalability, and sensing and operation ranges significantly\nimpair their large-scale use. To circumvent these limitations, we propose a\nnovel asymptotic boundary shrink control strategy that enables collective\ncoverage of a spill by autonomous robots featuring customized operation ranges.\nFor each robot, a novel controller is implemented that relies only on local\nvision sensors with limited vision range. Moreover, the distributedness of this\nstrategy allows any number of robots to be employed without inter-robot\ncollisions. Finally, features of this approach including the convergence of\nrobot motion during boundary shrink control, spill clearance rate, and the\ncapability to work under limited ranges of vision and wireless connectivity are\nvalidated through extensive experiments with simulation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:50:27 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 16:28:37 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Luo", "Shaocheng", ""], ["Kim", "Jonghoek", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "2006.12841", "submitter": "Haotian Liu", "authors": "Haotian Liu, Wenchuan Wu", "title": "Online Multi-agent Reinforcement Learning for Decentralized\n  Inverter-based Volt-VAR Control", "comments": "11 pages; submitted to IEEE Transaction on Smart Grid on 2020.6.4", "journal-ref": "DOI 10.1109/TSG.2021.3060027, IEEE", "doi": "10.1109/TNANO.2006.888527", "report-no": null, "categories": "eess.SY cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed Volt/Var control (VVC) methods have been widely studied for\nactive distribution networks(ADNs), which is based on perfect model and\nreal-time P2P communication. However, the model is always incomplete with\nsignificant parameter errors and such P2P communication system is hard to\nmaintain. In this paper, we propose an online multi-agent reinforcement\nlearning and decentralized control framework (OLDC) for VVC. In this framework,\nthe VVC problem is formulated as a constrained Markov game and we propose a\nnovel multi-agent constrained soft actor-critic (MACSAC) reinforcement learning\nalgorithm. MACSAC is used to train the control agents online, so the accurate\nADN model is no longer needed. Then, the trained agents can realize\ndecentralized optimal control using local measurements without real-time P2P\ncommunication. The OLDC with MACSAC has shown extraordinary flexibility,\nefficiency and robustness to various computing and communication conditions.\nNumerical simulations on IEEE test cases not only demonstrate that the proposed\nMACSAC outperforms the state-of-art learning algorithms, but also support the\nsuperiority of our OLDC framework in the online application.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:03:46 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:31:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Haotian", ""], ["Wu", "Wenchuan", ""]]}, {"id": "2006.13085", "submitter": "Nelson Vadori", "authors": "Nelson Vadori and Sumitra Ganesh and Prashant Reddy and Manuela Veloso", "title": "Calibration of Shared Equilibria in General Sum Partially Observable\n  Markov Games", "comments": "NeurIPS 2020, Thirty-fourth Conference on Neural Information\n  Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training multi-agent systems (MAS) to achieve realistic equilibria gives us a\nuseful tool to understand and model real-world systems. We consider a general\nsum partially observable Markov game where agents of different types share a\nsingle policy network, conditioned on agent-specific information. This paper\naims at i) formally understanding equilibria reached by such agents, and ii)\nmatching emergent phenomena of such equilibria to real-world targets. Parameter\nsharing with decentralized execution has been introduced as an efficient way to\ntrain multiple agents using a single policy network. However, the nature of\nresulting equilibria reached by such agents has not been yet studied: we\nintroduce the novel concept of Shared equilibrium as a symmetric pure Nash\nequilibrium of a certain Functional Form Game (FFG) and prove convergence to\nthe latter for a certain class of games using self-play. In addition, it is\nimportant that such equilibria satisfy certain constraints so that MAS are\ncalibrated to real world data for practical use: we solve this problem by\nintroducing a novel dual-Reinforcement Learning based approach that fits\nemergent behaviors of agents in a Shared equilibrium to externally-specified\ntargets, and apply our methods to a n-player market example. We do so by\ncalibrating parameters governing distributions of agent types rather than\nindividual agents, which allows both behavior differentiation among agents and\ncoherent scaling of the shared policy network to multiple agents.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:14:20 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 21:02:03 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 15:09:03 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 23:33:02 GMT"}, {"version": "v5", "created": "Fri, 23 Oct 2020 15:15:06 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Vadori", "Nelson", ""], ["Ganesh", "Sumitra", ""], ["Reddy", "Prashant", ""], ["Veloso", "Manuela", ""]]}, {"id": "2006.13109", "submitter": "Saurabh Deochake", "authors": "Saurabh Deochake and Debajyoti Mukhopadhyay", "title": "An Agent-based Cloud Service Negotiation in Hybrid Cloud Computing", "comments": "Fifth International Conference on ICT for Sustainable Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of evolution of cloud computing, large organizations have\nbeen scaling the on-premise IT infrastructure to the cloud. Although this being\na popular practice, it lacks comprehensive efforts to study the aspects of\nautomated negotiation of resources among cloud customers and providers. This\npaper proposes a full-fledged framework for the multi-party, multi-issue\nnegotiation system for cloud resources. It introduces a robust cloud\nmarketplace system to buy and sell cloud resources. The Belief-Desire-Intention\n(BDI) model-based cloud customer and provider agents concurrently negotiate on\nmultiple issues, pursuing a hybrid tactic of time and resource-based dynamic\ndeadline algorithms to generate offers and counter-offers. The cloud\nmarketplace-based system is further augmented with the assignment of behavior\nnorm score and reputation index to the agents to establish trust among them.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 05:23:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Deochake", "Saurabh", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "2006.13133", "submitter": "Zijia Zhong", "authors": "Zijia Zhong, Mark Nejad, Earl E. Lee", "title": "Autonomous and Semi-Autonomous Intersection Management: A Survey", "comments": "16 pages, 10 figures", "journal-ref": "IEEE Intelligent Transportation Systems Magazine, 2020", "doi": "10.1109/MITS.2020.3014074", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection is a major source of traffic delays and accidents within modern\ntransportation systems. Compared to signalized intersection management,\nautonomous intersection management (AIM) coordinates the intersection crossing\nat an individual vehicle level with additional flexibility. AIM can potentially\neliminate stopping in intersection crossing due to traffic lights while\nmaintaining a safe separation among conflicting movements. In this paper, the\nstate-of-the-art AIM research among various disciplines (e.g., traffic\nengineering, control engineering) is surveyed from the perspective of three\nhierarchical layers: corridor coordination layer, intersection management\nlayer, and vehicle control layer. The key aspects of AIM designs are discussed\nin details, including conflict detection schemes, priority rules, control\ncentralization, computation complexity, etc. The potential improvements for AIM\nevaluation with the emphasis of realistic scenarios are provided. This survey\nserves as a comprehensive review of AIM design and provides promising\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:28:08 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 02:03:55 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 14:41:19 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhong", "Zijia", ""], ["Nejad", "Mark", ""], ["Lee", "Earl E.", ""]]}, {"id": "2006.13140", "submitter": "Zohreh Kaheh", "authors": "Zohreh Kaheh, Reza Baradaran Kazemzadeh, Ellips Masehian, Ali\n  Husseinzadeh Kashan", "title": "Developing a Mathematical Negotiation Mechanism for a Distributed\n  Procurement Problem and a Hybrid Algorithm for its Solution", "comments": "the request of one of authors", "journal-ref": null, "doi": "10.24200/J65.2018.20035", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the players' bargaining power, designing a bi-level programming\nmodel is suitable to reflect the hierarchical nature of the decision-making\nprocess. In this paper, typical negotiation components perfectly match with the\nmathematical model and its solution procedure. For this purpose, a mathematical\nnegotiation mechanism is designed to minimize the negotiators' costs in a\ndistributed procurement problem at two echelons of an automotive supply chain.\nThe buyer's costs are procurement cost and shortage penalty in a one-period\ncontract. On the other hand, the suppliers intend to solve a multi-period,\nmulti-product production planning to minimize their costs. Such a mechanism\nprovides an alignment among suppliers' production planning and order\nallocation, also supports the partnership with the valued suppliers by taking\nsuppliers' capacities into account. Such a circumstance has been modeled via\nbi-level programming, in which the buyer acts as a leader, and the suppliers\nindividually appear as followers in the lower level. To solve this nonlinear\nbi-level programming model, a hybrid algorithm by combining the particle swarm\noptimization algorithm with a heuristic algorithm based on A search is\nproposed. In this algorithm, a heuristic algorithm based on A search is\nembedded to solve the mixed-integer nonlinear programming sub-problems for each\nsupplier according to the received variable values determined by PSO system\nparticles.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:05:16 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 16:01:37 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kaheh", "Zohreh", ""], ["Kazemzadeh", "Reza Baradaran", ""], ["Masehian", "Ellips", ""], ["Kashan", "Ali Husseinzadeh", ""]]}, {"id": "2006.13164", "submitter": "Xinshuo Weng", "authors": "Yongxin Wang and Kris Kitani and Xinshuo Weng", "title": "Joint Object Detection and Multi-Object Tracking with Graph Neural\n  Networks", "comments": "Published in International Conference on Robotics and Automation\n  (ICRA), 2021. Code is released here: https://github.com/yongxinw/GSDT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and data association are critical components in multi-object\ntracking (MOT) systems. Despite the fact that the two components are dependent\non each other, prior works often design detection and data association modules\nseparately which are trained with separate objectives. As a result, one cannot\nback-propagate the gradients and optimize the entire MOT system, which leads to\nsub-optimal performance. To address this issue, recent works simultaneously\noptimize detection and data association modules under a joint MOT framework,\nwhich has shown improved performance in both modules. In this work, we propose\na new instance of joint MOT approach based on Graph Neural Networks (GNNs). The\nkey idea is that GNNs can model relations between variable-sized objects in\nboth the spatial and temporal domains, which is essential for learning\ndiscriminative features for detection and data association. Through extensive\nexperiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of\nour GNN-based joint MOT approach and show state-of-the-art performance for both\ndetection and MOT tasks. Our code is available at:\nhttps://github.com/yongxinw/GSDT\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:07:00 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:03:24 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 13:32:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Yongxin", ""], ["Kitani", "Kris", ""], ["Weng", "Xinshuo", ""]]}, {"id": "2006.13368", "submitter": "Joseph Chow", "authors": "Ding Wang, Brian Yueshuai He, Jingqin Gao, Joseph Y. J. Chow, Kaan\n  Ozbay, Shri Iyer", "title": "Impact of COVID-19 behavioral inertia on reopening strategies for New\n  York City Transit", "comments": null, "journal-ref": "International Journal of Transportation Science & Technology 10(2)\n  197-211 (2021)", "doi": "10.1016/j.ijtst.2021.01.003", "report-no": null, "categories": "econ.GN cs.MA physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has affected travel behaviors and transportation system\noperations, and cities are grappling with what policies can be effective for a\nphased reopening shaped by social distancing. A baseline model was previously\ndeveloped and calibrated for pre-COVID conditions as MATSim-NYC. A new COVID\nmodel is calibrated that represents travel behavior during the COVID-19\npandemic by recalibrating the population agendas to include work-from-home and\nre-estimating the mode choice model for MATSim-NYC to fit observed traffic and\ntransit ridership data. Assuming the change in behavior exhibits inertia during\nreopening, we analyze the increase in car traffic due to the phased reopen plan\nguided by the state government of New York. Four reopening phases and two\nreopening scenarios (with and without transit capacity restrictions) are\nanalyzed. A Phase 4 reopening with 100% transit capacity may only see as much\nas 73% of pre-COVID ridership and an increase in the number of car trips by as\nmuch as 142% of pre-pandemic levels. Limiting transit capacity to 50% would\ndecrease transit ridership further from 73% to 64% while increasing car trips\nto as much as 143% of pre-pandemic levels. While the increase appears small,\nthe impact on consumer surplus is disproportionately large due to already\nincreased traffic congestion. Many of the trips also get shifted to other modes\nlike micromobility. The findings imply that a transit capacity restriction\npolicy during reopening needs to be accompanied by (1) support for\nmicromobility modes, particularly in non-Manhattan boroughs, and (2) congestion\nalleviation policies that focus on reducing traffic in Manhattan, such as\ncordon-based pricing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 22:46:22 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 05:14:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Ding", ""], ["He", "Brian Yueshuai", ""], ["Gao", "Jingqin", ""], ["Chow", "Joseph Y. J.", ""], ["Ozbay", "Kaan", ""], ["Iyer", "Shri", ""]]}, {"id": "2006.13534", "submitter": "Ehsan Asali", "authors": "Ehsan Asali, Farzin Negahbani, Shahriyar Bamaei, Zahra Abbasi", "title": "Namira Soccer 2D Simulation Team Description Paper 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we will discuss methods and ideas which are implemented on\nNamira 2D Soccer Simulation team in the recent year. Numerous scientific and\nprogramming activities were done in the process of code development, but we\nwill mention the most outstanding ones in details. A Kalman filtering method\nfor localization and two helpful software packages will be discussed here.\nNamira uses agent2d-3.1.1 as base code and librcsc-4.1.0 as library with some\ndeliberate changes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 07:40:44 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Asali", "Ehsan", ""], ["Negahbani", "Farzin", ""], ["Bamaei", "Shahriyar", ""], ["Abbasi", "Zahra", ""]]}, {"id": "2006.13659", "submitter": "Virginia Bordignon", "authors": "Virginia Bordignon, Vincenzo Matta and Ali H. Sayed", "title": "Social Learning with Partial Information Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of sharing partial information within social\nlearning strategies. In traditional social learning, agents solve a distributed\nmultiple hypothesis testing problem by performing two operations at each\ninstant: first, agents incorporate information from private observations to\nform their beliefs over a set of hypotheses; second, agents combine the\nentirety of their beliefs locally among neighbors. Within a sufficiently\ninformative environment and as long as the connectivity of the network allows\ninformation to diffuse across agents, these algorithms enable agents to learn\nthe true hypothesis. Instead of sharing the entirety of their beliefs, this\nwork considers the case in which agents will only share their beliefs regarding\none hypothesis of interest, with the purpose of evaluating its validity, and\ndraws conditions under which this policy does not affect truth learning. We\npropose two approaches for sharing partial information, depending on whether\nagents behave in a self-aware manner or not. The results show how different\nlearning regimes arise, depending on the approach employed and on the inherent\ncharacteristics of the inference problem. Furthermore, the analysis\ninterestingly points to the possibility of deceiving the network, as long as\nthe evaluated hypothesis of interest is close enough to the truth.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:22:27 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bordignon", "Virginia", ""], ["Matta", "Vincenzo", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2006.13763", "submitter": "Ahmad Beirami", "authors": "Sofia M Nikolakaki and Ogheneovo Dibie and Ahmad Beirami and Nicholas\n  Peterson and Navid Aghdaie and Kazi Zaman", "title": "Competitive Balance in Team Sports Games", "comments": "2020 IEEE Conference in Games (COG 2020), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competition is a primary driver of player satisfaction and engagement in\nmultiplayer online games. Traditional matchmaking systems aim at creating\nmatches involving teams of similar aggregated individual skill levels, such as\nElo score or TrueSkill. However, team dynamics cannot be solely captured using\nsuch linear predictors. Recently, it has been shown that nonlinear predictors\nthat target to learn probability of winning as a function of player and team\nfeatures significantly outperforms these linear skill-based methods. In this\npaper, we show that using final score difference provides yet a better\nprediction metric for competitive balance. We also show that a linear model\ntrained on a carefully selected set of team and individual features achieves\nalmost the performance of the more powerful neural network model while offering\ntwo orders of magnitude inference speed improvement. This shows significant\npromise for implementation in online matchmaking systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:19:07 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nikolakaki", "Sofia M", ""], ["Dibie", "Ogheneovo", ""], ["Beirami", "Ahmad", ""], ["Peterson", "Nicholas", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi", ""]]}, {"id": "2006.13912", "submitter": "Mathieu Lauri\\`ere", "authors": "Andrea Angiuli and Jean-Pierre Fouque and Mathieu Lauri\\`ere", "title": "Unified Reinforcement Q-Learning for Mean Field Game and Control\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Reinforcement Learning (RL) algorithm to solve infinite horizon\nasymptotic Mean Field Game (MFG) and Mean Field Control (MFC) problems. Our\napproach can be described as a unified two-timescale Mean Field Q-learning: The\n\\emph{same} algorithm can learn either the MFG or the MFC solution by simply\ntuning the ratio of two learning parameters. The algorithm is in discrete time\nand space where the agent not only provides an action to the environment but\nalso a distribution of the state in order to take into account the mean field\nfeature of the problem. Importantly, we assume that the agent can not observe\nthe population's distribution and needs to estimate it in a model-free manner.\nThe asymptotic MFG and MFC problems are also presented in continuous time and\nspace, and compared with classical (non-asymptotic or stationary) MFG and MFC\nproblems. They lead to explicit solutions in the linear-quadratic (LQ) case\nthat are used as benchmarks for the results of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:45:44 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:26:15 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 17:08:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Angiuli", "Andrea", ""], ["Fouque", "Jean-Pierre", ""], ["Lauri\u00e8re", "Mathieu", ""]]}, {"id": "2006.14186", "submitter": "Soubhik Deb", "authors": "Yifan Mao, Soubhik Deb, Shaileshh Bojja Venkatakrishnan, Sreeram\n  Kannan, Kannan Srinivasan", "title": "Perigee: Efficient Peer-to-Peer Network Design for Blockchains", "comments": "Accepted at ACM PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key performance metric in blockchains is the latency between when a\ntransaction is broadcast and when it is confirmed (the so-called, confirmation\nlatency). While improvements in consensus techniques can lead to lower\nconfirmation latency, a fundamental lower bound on confirmation latency is the\npropagation latency of messages through the underlying peer-to-peer (p2p)\nnetwork (inBitcoin, the propagation latency is several tens of seconds). The de\nfacto p2p protocol used by Bitcoin and other blockchains is based on random\nconnectivity: each node connects to a random subset of nodes. The induced p2p\nnetwork topology can be highly suboptimal since it neglects geographical\ndistance, differences in bandwidth, hash-power and computational abilities\nacross peers. We present Perigee, a decentralized algorithm that automatically\nlearns an efficient p2p topology tuned to the aforementioned network\nheterogeneities, purely based on peers' interactions with their neighbors.\nMotivated by the literature on the multi-armed bandit problem, Perigee\noptimally balances the tradeoff between retaining connections to known\nwell-connected neighbors, and exploring new connections to previously-unseen\nneighbors. Experimental evaluations show that Perigee reduces the latency to\nbroadcast by $33\\%$. Lastly Perigee is simple, computationally lightweight,\nadversary-resistant, and compatible with the selfish interests of peers, making\nit an attractive p2p protocol for blockchains.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 05:24:11 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Mao", "Yifan", ""], ["Deb", "Soubhik", ""], ["Venkatakrishnan", "Shaileshh Bojja", ""], ["Kannan", "Sreeram", ""], ["Srinivasan", "Kannan", ""]]}, {"id": "2006.14249", "submitter": "Frank Schweitzer", "authors": "Frank Schweitzer, Yan Zhang, Giona Casiraghi", "title": "Intervention scenarios to enhance knowledge transfer in a network of\n  firm", "comments": null, "journal-ref": "Front. Phys. (2020) 8:382", "doi": "10.3389/fphy.2020.00382", "report-no": null, "categories": "physics.soc-ph cs.MA cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a multi-agent model of firms in an R\\&D network. Each firm is\ncharacterized by its knowledge stock $x_{i}(t)$, which follows a non-linear\ndynamics. It can grow with the input from other firms, i.e., by knowledge\ntransfer, and decays otherwise. Maintaining interactions is costly. Firms can\nleave the network if their expected knowledge growth is not realized, which may\ncause other firms to also leave the network. The paper discusses two bottom-up\nintervention scenarios to prevent, reduce, or delay cascades of firms leaving.\nThe first one is based on the formalism of network controllability, in which\ndriver nodes are identified and subsequently incentivized, by reducing their\ncosts. The second one combines node interventions and network interventions. It\nproposes the controlled removal of a single firm and the random replacement of\nfirms leaving. This allows to generate small cascades, which prevents the\noccurrence of large cascades. We find that both approaches successfully\nmitigate cascades and thus improve the resilience of the R\\&D network.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:39:28 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Schweitzer", "Frank", ""], ["Zhang", "Yan", ""], ["Casiraghi", "Giona", ""]]}, {"id": "2006.14526", "submitter": "Nathan Brooks", "authors": "Nathan A. Brooks, Simon T. Powers and James M. Borg", "title": "A mechanism to promote social behaviour in household load balancing", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the peak energy consumption of households is essential for the\neffective use of renewable energy sources, in order to ensure that as much\nhousehold demand as possible can be met by renewable sources. This entails\nspreading out the use of high-powered appliances such as dishwashers and\nwashing machines throughout the day. Traditional approaches to this problem\nhave relied on differential pricing set by a centralised utility company. But\nthis mechanism has not been effective in promoting widespread shifting of\nappliance usage. Here we consider an alternative decentralised mechanism, where\nagents receive an initial allocation of time-slots to use their appliances and\ncan then exchange these with other agents. If agents are willing to be more\nflexible in the exchanges they accept, then overall satisfaction, in terms of\nthe percentage of agents time-slot preferences that are satisfied, will\nincrease. This requires a mechanism that can incentivise agents to be more\nflexible. Building on previous work, we show that a mechanism incorporating\nsocial capital - the tracking of favours given and received - can incentivise\nagents to act flexibly and give favours by accepting exchanges that do not\nimmediately benefit them. We demonstrate that a mechanism that tracks favours\nincreases the overall satisfaction of agents, and crucially allows social\nagents that give favours to outcompete selfish agents that do not under\npayoff-biased social learning. Thus, even completely self-interested agents are\nexpected to learn to produce socially beneficial outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:23:02 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Brooks", "Nathan A.", ""], ["Powers", "Simon T.", ""], ["Borg", "James M.", ""]]}, {"id": "2006.14947", "submitter": "Murat Cubuktepe", "authors": "Murat Cubuktepe, Zhe Xu, Ufuk Topcu", "title": "Distributed Policy Synthesis of Multi-Agent Systems With Graph Temporal\n  Logic Specifications", "comments": "Final version of IEEE Transactions on Control of Network Systems.\n  arXiv admin note: substantial text overlap with arXiv:2001.09066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributed synthesis of policies for multi-agent systems to\nperform \\emph{spatial-temporal} tasks. We formalize the synthesis problem as a\n\\emph{factored} Markov decision process subject to \\emph{graph temporal logic}\nspecifications. The transition function and task of each agent are functions of\nthe agent itself and its neighboring agents. In this work, we develop another\ndistributed synthesis method, which improves the scalability and runtime by two\norders of magnitude compared to our prior work. The synthesis method decomposes\nthe problem into a set of smaller problems, one for each agent by leveraging\nthe structure in the model, and the specifications. We show that the running\ntime of the method is linear in the number of agents. The size of the problem\nfor each agent is exponential only in the number of neighboring agents, which\nis typically much smaller than the number of agents. We demonstrate the\napplicability of the method in case studies on disease control, urban security,\nand search and rescue. The numerical examples show that the method scales to\nhundreds of agents with hundreds of states per agent and can also handle\nsignificantly larger state spaces than our prior work.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 00:56:28 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 02:41:05 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 16:00:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cubuktepe", "Murat", ""], ["Xu", "Zhe", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2006.15000", "submitter": "Vadim Malvone", "authors": "Francesco Belardinelli, Catalin Dima, Vadim Malvone, and Ferucio\n  Tiplea", "title": "A Hennessy-Milner Theorem for ATL with Imperfect Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a history-based variant of alternating bisimulation with\nimperfect information allows it to be related to a variant of Alternating-time\nTemporal Logic (ATL) with imperfect information by a full Hennessy-Milner\ntheorem. The variant of ATL we consider has a common knowledge semantics, which\nrequires that the uniform strategy available for a coalition to accomplish some\ngoal must be common knowledge inside the coalition, while other semantic\nvariants of ATL with imperfect information do not accommodate a Hennessy-Milner\ntheorem. We also show that the existence of a history-based alternating\nbisimulation between two finite Concurrent Game Structures with imperfect\ninformation (iCGS) is undecidable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 14:12:11 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Belardinelli", "Francesco", ""], ["Dima", "Catalin", ""], ["Malvone", "Vadim", ""], ["Tiplea", "Ferucio", ""]]}, {"id": "2006.15213", "submitter": "Sumanas Sarma", "authors": "Serge Plata, Sumanas Sarma, Melvin Lancelot, Kristine Bagrova, David\n  Romano-Critchley", "title": "Simulating human interactions in supermarkets to measure the risk of\n  COVID-19 contagion at scale", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the context of simulating a retail environment using agent based\nmodelling, a theoretical model is presented that describes the probability\ndistribution of customer \"collisions\" using a novel space transformation to the\nTorus $Tor^2$. A method for generating the distribution of customer paths based\non historical basket data is developed. Finally a calculation of the number of\nsimulations required for statistical significance is developed. An\nimplementation of this modelling approach to run simulations on multiple store\ngeometries at industrial scale is being developed with current progress\ndetailed in the technical appendix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 21:04:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Plata", "Serge", ""], ["Sarma", "Sumanas", ""], ["Lancelot", "Melvin", ""], ["Bagrova", "Kristine", ""], ["Romano-Critchley", "David", ""]]}, {"id": "2006.15807", "submitter": "Zahi Kakish", "authors": "Zahi M. Kakish, Karthik Elamvazhuthi, Spring Berman", "title": "Using Reinforcement Learning to Herd a Robotic Swarm to a Target\n  Distribution", "comments": "Paper was submitted to Conference on Robot Learning 2019 and IEEE\n  Robotics and Automation Letters 2020 Revised, updated, and submitted to\n  DARS/SWARMS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a reinforcement learning approach to designing a\ncontrol policy for a \"leader\" agent that herds a swarm of \"follower\" agents,\nvia repulsive interactions, as quickly as possible to a target probability\ndistribution over a strongly connected graph. The leader control policy is a\nfunction of the swarm distribution, which evolves over time according to a\nmean-field model in the form of an ordinary difference equation. The dependence\nof the policy on agent populations at each graph vertex, rather than on\nindividual agent activity, simplifies the observations required by the leader\nand enables the control strategy to scale with the number of agents. Two\nTemporal-Difference learning algorithms, SARSA and Q-Learning, are used to\ngenerate the leader control policy based on the follower agent distribution and\nthe leader's location on the graph. A simulation environment corresponding to a\ngrid graph with 4 vertices was used to train and validate the control policies\nfor follower agent populations ranging from 10 to 100. Finally, the control\npolicies trained on 100 simulated agents were used to successfully redistribute\na physical swarm of 10 small robots to a target distribution among 4 spatial\nregions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 04:55:59 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 20:52:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kakish", "Zahi M.", ""], ["Elamvazhuthi", "Karthik", ""], ["Berman", "Spring", ""]]}, {"id": "2006.16068", "submitter": "Shuyue Hu", "authors": "Shuyue Hu, Chin-Wing Leung, Ho-fung Leung, Harold Soh", "title": "The Evolutionary Dynamics of Independent Learning Agents in Population\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the evolutionary dynamics of reinforcement learning under\nmulti-agent settings has long remained an open problem. While previous works\nprimarily focus on 2-player games, we consider population games, which model\nthe strategic interactions of a large population comprising small and anonymous\nagents. This paper presents a formal relation between stochastic processes and\nthe dynamics of independent learning agents who reason based on the reward\nsignals. Using a master equation approach, we provide a novel unified framework\nfor characterising population dynamics via a single partial differential\nequation (Theorem 1). Through a case study involving Cross learning agents, we\nillustrate that Theorem 1 allows us to identify qualitatively different\nevolutionary dynamics, to analyse steady states, and to gain insights into the\nexpected behaviour of a population. In addition, we present extensive\nexperimental results validating that Theorem 1 holds for a variety of learning\nmethods and population games.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:22:23 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Hu", "Shuyue", ""], ["Leung", "Chin-Wing", ""], ["Leung", "Ho-fung", ""], ["Soh", "Harold", ""]]}, {"id": "2006.16399", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "An agent-based model of interdisciplinary interactions in science", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increased interdisciplinarity in science projects has been highlighted as\ncrucial to tackle complex real-world challenges, but also as beneficial for the\ndevelopment of disciplines themselves. This paper introduces a parcimonious\nagent-based model of interdisciplinary relationships in collective entreprises\nof knowledge discovery, to investigate the impact of scientist-level decisions\nand preferences on global interdisciplinarity patterns. Under the assumption of\nsimple rules for individual researcher project management, such as trade-offs\nbetween invested time overhead and knowledge benefit, model simulations show\nthat individual choices influence the distribution of compromise points between\nemergent level of disciplinary depth and interdisciplinarity in a non-linear\nway. Different structures for collaboration networks may also yield various\noutcomes in terms of global interdisciplinarity. We conclude that independently\nof the research field, the organization of research, and more particularly the\nlocal balancing between vertical and horizontal research, already influences\nthe final positioning of research results and the extent of the knowledge\nfront. This suggests direct applications to research policies with a bottom-up\nleverage on the interactions between disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:28:39 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Raimbault", "Juste", ""]]}, {"id": "2006.16472", "submitter": "Bilal Farooq", "authors": "Lama Alfaseeh and Bilal Farooq", "title": "Deep Learning Based Proactive Multi-Objective Eco-Routing Strategies for\n  Connected and Automated Vehicles", "comments": null, "journal-ref": "Frontiers in Future Transportation, 2020", "doi": "10.3389/ffutr.2020.594608", "report-no": null, "categories": "math.OC cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study exploits the advancements in information and communication\ntechnology (ICT), connected and automated vehicles (CAVs), and sensing, to\ndevelop proactive multi-objective eco-routing strategies. For a robust\napplication, several GHG costing approaches are examined. The predictive models\nfor the link level traffic and emission states are developed using long short\nterm memory deep network with exogenous predictors. It is found that proactive\nrouting strategies outperformed the myopic strategies, regardless of the\nrouting objective. Whether myopic or proactive, the multi-objective routing,\nwith travel time and GHG minimization as objectives, outperformed the single\nobjective routing strategies, causing a reduction in the average travel time\n(TT), average vehicle kilometre travelled (VKT), total GHG and total NOx by\n17%, 21%, 18%, and 20%, respectively. Finally, the additional TT and VKT\nexperienced by the vehicles in the network contributed adversely to the amount\nof GHG and NOx produced in the network.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:07:10 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 15:12:53 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Alfaseeh", "Lama", ""], ["Farooq", "Bilal", ""]]}]