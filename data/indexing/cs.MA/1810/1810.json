[{"id": "1810.00147", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Yuandong Tian", "title": "M$^3$RL: Mind-aware Multi-agent Management Reinforcement Learning", "comments": "ICLR 2019; 18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the prior work on multi-agent reinforcement learning (MARL) achieves\noptimal collaboration by directly controlling the agents to maximize a common\nreward. In this paper, we aim to address this from a different angle. In\nparticular, we consider scenarios where there are self-interested agents (i.e.,\nworker agents) which have their own minds (preferences, intentions, skills,\netc.) and can not be dictated to perform tasks they do not wish to do. For\nachieving optimal coordination among these agents, we train a super agent\n(i.e., the manager) to manage them by first inferring their minds based on both\ncurrent and past observations and then initiating contracts to assign suitable\ntasks to workers and promise to reward them with corresponding bonuses so that\nthey will agree to work together. The objective of the manager is maximizing\nthe overall productivity as well as minimizing payments made to the workers for\nad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent\nManagement Reinforcement Learning (M^3RL), which consists of agent modeling and\npolicy learning. We have evaluated our approach in two environments, Resource\nCollection and Crafting, to simulate multi-agent management problems with\nvarious task settings and multiple designs for the worker agents. The\nexperimental results have validated the effectiveness of our approach in\nmodeling worker agents' minds online, and in achieving optimal ad-hoc teaming\nwith good generalization and fast adaptation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 04:33:15 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 21:56:03 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 06:02:40 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Shu", "Tianmin", ""], ["Tian", "Yuandong", ""]]}, {"id": "1810.00182", "submitter": "Hector Garcia de Marina Dr.", "authors": "Zhiyong Sun, Hector Garcia de Marina, Brian D. O. Anderson, Changbin\n  Yu", "title": "Collaborative target-tracking control using multiple autonomous\n  fixed-wing UAVs with constant speeds", "comments": "33 pages (single column). To be published in the AIAA Journal of\n  Guidance, Dynamics, and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA cs.RO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a collaborative tracking control problem using a group\nof fixed-wing unmanned aerial vehicles (UAVs) with constant and non-identical\nspeeds. The dynamics of fixed-wing UAVs are modelled by unicycle-type equations\nwith nonholonomic constraints, assuming that UAVs fly at constant altitudes in\nthe nominal operation mode. The controller is designed such that all fixed-wing\nUAVs as a group can collaboratively track a desired target's position and\nvelocity. We first present conditions on the relative speeds of tracking UAVs\nand the target to ensure that the tracking objective can be achieved when UAVs\nare subject to constant speed constraints. We construct a reference velocity\nthat includes both the target's velocity and position as feedback, which is to\nbe tracked by the group centroid. In this way, all vehicles' headings are\ncontrolled such that the group centroid follows a reference trajectory that\nsuccessfully tracks the target's trajectory. A spacing controller is further\ndevised to ensure that all vehicles stay close to the group centroid\ntrajectory. Trade-offs in the controller design and performance limitations of\nthe target tracking control due to the constant-speed constraint are also\ndiscussed in detail. Experimental results with three fixed-wing UAVs tracking a\ntarget rotorcraft are provided.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 09:58:47 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 09:27:38 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 18:56:38 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Sun", "Zhiyong", ""], ["de Marina", "Hector Garcia", ""], ["Anderson", "Brian D. O.", ""], ["Yu", "Changbin", ""]]}, {"id": "1810.00510", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Caiming Xiong, Ying Nian Wu, Song-Chun Zhu", "title": "Interactive Agent Modeling by Learning to Probe", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of modeling the other agents, such as understanding their\nintentions and skills, is essential to an agent's interactions with other\nagents. Conventional agent modeling relies on passive observation from\ndemonstrations. In this work, we propose an interactive agent modeling scheme\nenabled by encouraging an agent to learn to probe. In particular, the probing\nagent (i.e. a learner) learns to interact with the environment and with a\ntarget agent (i.e., a demonstrator) to maximize the change in the observed\nbehaviors of that agent. Through probing, rich behaviors can be observed and\nare used for enhancing the agent modeling to learn a more accurate mind model\nof the target agent. Our framework consists of two learning processes: i)\nimitation learning for an approximated agent model and ii) pure\ncuriosity-driven reinforcement learning for an efficient probing policy to\ndiscover new behaviors that otherwise can not be observed. We have validated\nour approach in four different tasks. The experimental results suggest that the\nagent model learned by our approach i) generalizes better in novel scenarios\nthan the ones learned by passive observation, random probing, and other\ncuriosity-driven approaches do, and ii) can be used for enhancing performance\nin multiple applications including distilling optimal planning to a policy net,\ncollaboration, and competition. A video demo is available at\nhttps://www.dropbox.com/s/8mz6rd3349tso67/Probing_Demo.mov?dl=0\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 02:55:07 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Shu", "Tianmin", ""], ["Xiong", "Caiming", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1810.00596", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "Fault Tolerant Adaptive Parallel and Distributed Simulation through\n  Functional Replication", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07310", "journal-ref": "Simulation Modelling Practice and Theory, Elsevier, vol. 93 (May\n  2019)", "doi": "10.1016/j.simpat.2018.09.012", "report-no": null, "categories": "cs.DC cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents FT-GAIA, a software-based fault-tolerant parallel and\ndistributed simulation middleware. FT-GAIA has being designed to reliably\nhandle Parallel And Distributed Simulation (PADS) models, which are needed to\nproperly simulate and analyze complex systems arising in any kind of scientific\nor engineering field. PADS takes advantage of multiple execution units run in\nmulticore processors, cluster of workstations or HPC systems. However, large\ncomputing systems, such as HPC systems that include hundreds of thousands of\ncomputing nodes, have to handle frequent failures of some components. To cope\nwith this issue, FT-GAIA transparently replicates simulation entities and\ndistributes them on multiple execution nodes. This allows the simulation to\ntolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some\nprotection against Byzantine failures, since interaction messages among the\nsimulated entities are replicated as well, so that the receiving entity can\nidentify and discard corrupted messages. Results from an analytical model and\nfrom an experimental evaluation show that FT-GAIA provides a high degree of\nfault tolerance, at the cost of a moderate increase in the computational load\nof the execution units.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 09:51:46 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 08:56:49 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1810.01155", "submitter": "Arsham Mostaani", "authors": "Arsham Mostaani, Osvaldo Simeone, Symeon Chatzinotas, Bjorn Ottersten", "title": "Learning-Based Physical Layer Communications for Multi-agent\n  Collaboration", "comments": "Submitted to SPAWC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a collaborative task carried out by two autonomous agents that are\nable to communicate over a noisy channel. Each agent is only aware of its own\nstate, while the accomplishment of the task depends on the value of the joint\nstate of both agents. As an example, both agents must simultaneously reach a\ncertain location of the environment, while only being aware of their respective\npositions. Assuming the presence of feedback in the form of a common reward to\nthe agents, a conventional approach would apply separately: (i) an\noff-the-shelf coding and decoding scheme in order to enhance the reliability of\nthe communication of the state of one agent to the other; and (ii) a standard\nmulti-agent reinforcement learning strategy to learn how to act in the\nresulting environment. In this work, it is demonstrated that the performance of\nthe collaborative task can be improved if the agents learn jointly how to\ncommunicate and to act, even in the presence of a delay in the communication\nchannel.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 10:14:16 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 13:41:12 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 11:44:41 GMT"}, {"version": "v4", "created": "Thu, 28 Feb 2019 11:29:08 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Mostaani", "Arsham", ""], ["Simeone", "Osvaldo", ""], ["Chatzinotas", "Symeon", ""], ["Ottersten", "Bjorn", ""]]}, {"id": "1810.01719", "submitter": "Orfeas Stefanos Thyfronitis Litos", "authors": "Aggelos Kiayias and Benjamin Livshits and Andr\\'es Monteoliva Mosteiro\n  and Orfeas Stefanos Thyfronitis Litos", "title": "A Puff of Steem: Security Analysis of Decentralized Content Curation", "comments": "15 pages main text, 35 pages in total, 6 figures, 6 algorithms.\n  Contains mathematical analysis and computer simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Decentralized content curation is the process through which uploaded posts\nare ranked and filtered based exclusively on users' feedback. Platforms such as\nthe blockchain-based Steemit employ this type of curation while providing\nmonetary incentives to promote the visibility of high quality posts according\nto the perception of the participants. Despite the wide adoption of the\nplatform very little is known regarding its performance and resilience\ncharacteristics. In this work, we provide a formal model for decentralized\ncontent curation that identifies salient complexity and game-theoretic measures\nof performance and resilience to selfish participants. Armed with our model, we\nprovide a first analysis of Steemit identifying the conditions under which the\nsystem can be expected to correctly converge to curation while we demonstrate\nits susceptibility to selfish participant behaviour. We validate our\ntheoretical results with system simulations in various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 12:50:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 16:59:06 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Kiayias", "Aggelos", ""], ["Livshits", "Benjamin", ""], ["Mosteiro", "Andr\u00e9s Monteoliva", ""], ["Litos", "Orfeas Stefanos Thyfronitis", ""]]}, {"id": "1810.01784", "submitter": "James Usevitch", "authors": "James Usevitch and Dimitra Panagou", "title": "Determining r-Robustness of Digraphs Using Mixed Integer Linear\n  Programming", "comments": "Accepted to 2019 American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convergence guarantees of many resilient consensus algorithms are based on\nthe graph theoretic properties of $r$- and $(r,s)$-robustness. These algorithms\nguarantee consensus of normally behaving agents in the presence of a bounded\nnumber of arbitrarily misbehaving agents if the values of the integers $r$ and\n$s$ are sufficiently high. However, determining the largest integer $r$ for\nwhich an arbitrary digraph is $r$-robust is highly nontrivial. This paper\nintroduces a novel method for calculating this value using mixed integer linear\nprogramming. The method only requires knowledge of the graph Laplacian matrix,\nand can be formulated with affine objective and constraints, except for the\ninteger constraint. Integer programming methods such as branch-and-bound can\nallow both lower and upper bounds on $r$ to be iteratively tightened.\nSimulations suggest the proposed method demonstrates greater efficiency than\nprior algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 03:52:05 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 18:43:28 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Usevitch", "James", ""], ["Panagou", "Dimitra", ""]]}, {"id": "1810.02423", "submitter": "Pei Wang", "authors": "Pei Wang, Pushpi Paranamana, and Patrick Shafto", "title": "Generalizing the theory of cooperative inference", "comments": "Publish version for AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation information sharing is important to theories of human learning\nand has potential implications for machine learning. Prior work derived\nconditions for achieving optimal Cooperative Inference given strong, relatively\nrestrictive assumptions. We relax these assumptions by demonstrating\nconvergence for any discrete joint distribution, robustness through equivalence\nclasses and stability under perturbation, and effectiveness by deriving bounds\nfrom structural properties of the original joint distribution. We provide\ngeometric interpretations, connections to and implications for optimal\ntransport, and connections to importance sampling, and conclude by outlining\nopen questions and challenges to realizing the promise of Cooperative\nInference.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 21:04:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 21:02:30 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Wang", "Pei", ""], ["Paranamana", "Pushpi", ""], ["Shafto", "Patrick", ""]]}, {"id": "1810.02456", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c and Alex Olshevsky and C\\'esar A. Uribe", "title": "Graph-Theoretic Analysis of Belief System Dynamics under Logic\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CY cs.MA cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion formation cannot be modeled solely as an ideological deduction from a\nset of principles; rather, repeated social interactions and logic constraints\namong statements are consequential in the construct of belief systems. We\naddress three basic questions in the analysis of social opinion dynamics: (i)\nWill a belief system converge? (ii) How long does it take to converge? (iii)\nWhere does it converge? We provide graph-theoretic answers to these questions\nfor a model of opinion dynamics of a belief system with logic constraints. Our\nresults make plain the implicit dependence of the convergence properties of a\nbelief system on the underlying social network and on the set of logic\nconstraints that relate beliefs on different statements. Moreover, we provide\nan explicit analysis of a variety of commonly used large-scale network models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 23:30:26 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 20:38:54 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1810.02517", "submitter": "Philip Gun", "authors": "Philip Gun, Andrew Hill, Robin Vujanic", "title": "Multi-Vehicle Trajectory Optimisation On Road Networks", "comments": "Submitted to ICRA 2019 on 16/9/2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of planning time-optimal trajectories for\nmultiple cooperative agents along specified paths through a static road\nnetwork. Vehicle interactions at intersections create non-trivial decisions,\nwith complex flow-on effects for subsequent interactions. A globally optimal,\nminimum time trajectory is found for all vehicles using Mixed Integer Linear\nProgramming (MILP). Computational performance is improved by minimising binary\nvariables using iteratively applied targeted collision constraints, and\nefficient goal constraints. Simulation results in an open-pit mining scenario\ncompare the proposed method against a fast heuristic method and a reactive\napproach based on site practices. The heuristic is found to scale better with\nproblem size while the MILP is able to avoid local minima.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 05:19:27 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Gun", "Philip", ""], ["Hill", "Andrew", ""], ["Vujanic", "Robin", ""]]}, {"id": "1810.02536", "submitter": "Jurica Hizak", "authors": "Jurica Hi\\v{z}ak, Lovorka Gotal Dmitrovi\\'c, Mirko \\v{C}ubrilo", "title": "The role of memory in transition from direct to indirect reciprocity", "comments": "6 pages, 6 figures, the results were partly presented at Cambridge\n  Summit 2018 (World Summit on Advances in Science, Engineering and Technology\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested that direct reciprocity operates well within small\ngroups of people where it would be hard to get away with cheating one another\nbut no research has been done yet to show how exactly the mechanism of direct\nreciprocity fails to operate as the group size increases. Unlike previous\nmodels that have neglected the role of memory, our model takes into account the\nmemory capacity of the agents as well as the cost of having such memory. We\nhave shown that the optimal memory capacity for handling the exploiters grows\nwith the group size in a similar way as the relative size of the neocortex\ngrows with the group size of the primates as it was found by Robin Dunbar. The\ntime required for reaching the relative fitness of the defectors increases\nrapidly with the group size which points to the conclusion that there is an\nupper group size limit over which the mechanism of direct reciprocity is\ninsufficient to maintain the cooperation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 06:36:53 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Hi\u017eak", "Jurica", ""], ["Dmitrovi\u0107", "Lovorka Gotal", ""], ["\u010cubrilo", "Mirko", ""]]}, {"id": "1810.02650", "submitter": "Rocco Paolillo", "authors": "Rocco Paolillo and Wander Jager", "title": "Simulating acculturation dynamics between migrants and locals in\n  relation to network formation", "comments": "24 pages, plus supplemental material, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  International migration implies the coexistence of different ethnic and\ncultural groups in the receiving country. The refugee crisis of 2015 has\nresulted in critical levels of opinion polarization on the question of whether\nto welcome migrants, causing clashes in receiving countries. This scenario\nemphasizes the need to better understand the dynamics of mutual adaptation\nbetween locals and migrants, and the conditions that favor successful\nintegration. Agent-based simulations can help achieve this goal. In this work,\nwe introduce our model MigrAgent and our preliminary results. The model\nsynthesizes the dynamics of migration intake and post-migration adaptation. It\nexplores the different acculturation outcomes that can emerge from the mutual\nadaptation of a migrant population and a local population depending on their\ndegree of tolerance. With parameter sweeping, we detect how different\nacculturation strategies can coexist in a society and in different degrees\namong various subgroups. The results show higher polarization effects between a\nlocal population and a migrant population for fast intake conditions. When\nmigrant intake is slow, transitory conditions between acculturation outcomes\nemerge for subgroups, e.g., from assimilation to integration for liberal\nmigrants and from marginalization to separation for conservative migrants.\nRelative group sizes due to speed of intake cause counterintuitive scenarios,\nsuch as the separation of liberal locals. We qualitatively compare the\nprocesses of our model with the German portion sample of the survey Causes and\nConsequences of Socio-Cultural Integration Processes among New Immigrants in\nEurope (SCIP), finding preliminary confirmation of our assumptions and results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 12:44:49 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 11:32:51 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Paolillo", "Rocco", ""], ["Jager", "Wander", ""]]}, {"id": "1810.02912", "submitter": "Shariq Iqbal", "authors": "Shariq Iqbal, Fei Sha", "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning", "comments": "ICML 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning in multi-agent scenarios is important for real-world\napplications but presents challenges beyond those seen in single-agent\nsettings. We present an actor-critic algorithm that trains decentralized\npolicies in multi-agent settings, using centrally computed critics that share\nan attention mechanism which selects relevant information for each agent at\nevery timestep. This attention mechanism enables more effective and scalable\nlearning in complex multi-agent environments, when compared to recent\napproaches. Our approach is applicable not only to cooperative settings with\nshared rewards, but also individualized reward settings, including adversarial\nsettings, as well as settings that do not provide global states, and it makes\nno assumptions about the action spaces of the agents. As such, it is flexible\nenough to be applied to most multi-agent learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 23:45:14 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 23:28:13 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Iqbal", "Shariq", ""], ["Sha", "Fei", ""]]}, {"id": "1810.02930", "submitter": "Yue Zhao", "authors": "Hossein Khazaei and Yue Zhao", "title": "Indirect Mechanism Design for Efficient and Stable Renewable Energy\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanism design is studied for aggregating renewable power producers (RPPs)\nin a two-settlement power market. Employing an indirect mechanism design\nframework, a payoff allocation mechanism (PAM) is derived from the competitive\nequilibrium (CE) of a specially formulated market with transferrable payoff.\nGiven the designed mechanism, the strategic behaviors of the participating RPPs\nentail a non-cooperative game: It is proven that a unique pure Nash equilibrium\n(NE) exists among the RPPs, for which a closed-form expression is found.\nMoreover, it is proven that the designed mechanism achieves a number of key\ndesirable properties at the NE: these include efficiency (i.e., an ideal \"Price\nof Anarchy\" of one), stability (i.e., \"in the core\" from a coalitional game\ntheoretic perspective), and no collusion. In addition, it is shown that a set\nof desirable \"ex-post\" properties are also achieved by the designed mechanism.\nExtensive simulations are conducted and corroborate the theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 04:01:53 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Khazaei", "Hossein", ""], ["Zhao", "Yue", ""]]}, {"id": "1810.03284", "submitter": "Wei Su", "authors": "Wei Su, Ge Chen, Yongguang Yu, Xueqiao Wang", "title": "Noise-synchronizability of opinion dynamics", "comments": "We need to make a substantial revision about this paper. Will submit\n  it again two monthes later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA cs.SI math.OC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the analysis of noise-induced synchronization of opinion dynamics with\nbounded confidence (BC), a natural and fundamental question is what opinion\nstructures can be synchronized by noise. In the traditional Hegselmann-Krause\n(HK) model, each agent examines the opinion values of all the other ones and\nthen choose neighbors to update its own opinion according to the BC scheme. In\nreality, people are more likely to interchange opinions with only some\nindividuals, resulting in a predetermined local discourse relationship as\nintroduced by the DeGroot model. In this paper, we consider an opinion dynamics\nthat combines the schemes of BC and local discourse topology and investigate\nits synchronization induced by noise. The new model endows the heterogeneous HK\nmodel with a time-varying discourse topology. With the proposed definition of\nnoise-synchronizability, it is shown that the compound noisy model is almost\nsurely noise-synchronizable if and only if the time-varying discourse graph is\nuniformly jointly connected, taking the noise-induced synchronization of the\nclassical heterogeneous HK model as a special case. As a natural implication,\nthe result for the first time builds the equivalence between the connectivity\nof discourse graph and the beneficial effect of noise for opinion consensus.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:00:11 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 01:38:47 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Su", "Wei", ""], ["Chen", "Ge", ""], ["Yu", "Yongguang", ""], ["Wang", "Xueqiao", ""]]}, {"id": "1810.03298", "submitter": "Won-Yong Shin", "authors": "Huifa Lin, Won-Yong Shin, Bang Chul Jung", "title": "Multi-Stream Opportunistic Network Decoupling: Relay Selection and\n  Interference Management", "comments": "14 pages, 6 figures, 2 tables, to appear in IEEE Transactions on\n  Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.MA cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-stream transmission in the $K \\times N \\times K$ channel with\ninterfering relay nodes, consisting of $K$ multi-antenna source--destination\n(S--D) pairs and $N$ single-antenna half-duplex relay nodes between the S--D\npairs. We propose a new achievable scheme operating with partial effective\nchannel gain, termed multi-stream opportunistic network decoupling (MS-OND),\nwhich achieves the optimal degrees of freedom (DoF) under a certain relay\nscaling law. Our protocol is built upon the conventional OND that leads to\nvirtual full-duplex mode with one data stream transmission per S--D pair,\ngeneralizing the idea of OND to multi-stream scenarios by leveraging relay\nselection and interference management. Specifically, two subsets of relay nodes\nare opportunistically selected using alternate relaying in terms of producing\nor receiving the minimum total interference level. For interference management,\neach source node sends $S \\,(1 \\le S \\le M)$ data streams to selected relay\nnodes with random beamforming for the first hop, while each destination node\nreceives its desired $S$ streams from the selected relay nodes via\nopportunistic interference alignment for the second hop, where $M$ is the\nnumber of antennas at each source or destination node. Our analytical results\nare validated by numerical evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:58:57 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Lin", "Huifa", ""], ["Shin", "Won-Yong", ""], ["Jung", "Bang Chul", ""]]}, {"id": "1810.03679", "submitter": "Amit Prasad", "authors": "Amit Prasad and Ivana Dusparic", "title": "Multi-agent Deep Reinforcement Learning for Zero Energy Communities", "comments": "Accepted at ISGT Europe 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in renewable energy generation and introduction of the government\ntargets to improve energy efficiency gave rise to a concept of a Zero Energy\nBuilding (ZEB). A ZEB is a building whose net energy usage over a year is zero,\ni.e., its energy use is not larger than its overall renewables generation. A\ncollection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses\nthe problem of energy sharing in such a community. This is different from\npreviously addressed energy sharing between buildings as our focus is on the\nimprovement of community energy status, while traditionally research focused on\nreducing losses due to transmission and storage, or achieving economic gains.\nWe model this problem in a multi-agent environment and propose a Deep\nReinforcement Learning (DRL) based solution. Each building is represented by an\nintelligent agent that learns over time the appropriate behaviour to share\nenergy. We have evaluated the proposed solution in a multi-agent simulation\nbuilt using osBrain. Results indicate that with time agents learn to\ncollaborate and learn a policy comparable to the optimal policy, which in turn\nimproves the ZEC's energy status. Buildings with no renewables preferred to\nrequest energy from their neighbours rather than from the supply grid.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 19:58:46 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 15:10:27 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Prasad", "Amit", ""], ["Dusparic", "Ivana", ""]]}, {"id": "1810.04016", "submitter": "Amanda Prorok", "authors": "Amanda Prorok", "title": "Redundant Robot Assignment on Graphs with Uncertain Edge Costs", "comments": "Accepted for publication at DARS 2018 (Best Paper Award). arXiv admin\n  note: substantial text overlap with arXiv:1804.04986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a framework for the assignment of multiple robots to goal\nlocations, when robot travel times are uncertain. Our premise is that time is\nthe most valuable asset in the system. Hence, we make use of redundant robots\nto counter the effect of uncertainty and minimize the average waiting time at\ndestinations. We apply our framework to transport networks represented as\ngraphs, and consider uncertainty in the edge costs (i.e., travel time). Since\nsolving the redundant assignment problem is strongly NP-hard, we exploit\nstructural properties of our problem to propose a polynomial-time solution with\nprovable sub-optimality bounds. Our method uses distributive aggregate\nfunctions, which allow us to efficiently (i.e., incrementally) compute the\neffective cost of assigning redundant robots. Experimental results on random\ngraphs show that the deployment of redundant robots through our method reduces\nwaiting times at goal locations, when edge traversals are uncertain.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 14:58:54 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 17:11:08 GMT"}], "update_date": "2019-04-21", "authors_parsed": [["Prorok", "Amanda", ""]]}, {"id": "1810.04565", "submitter": "Themistoklis Melissourgos", "authors": "George Christodoulou, Themistoklis Melissourgos, Paul G. Spirakis", "title": "Strategic Contention Resolution in Multiple Channels", "comments": "The results of this work are included in the 11th International\n  Symposium on Algorithmic Game Theory (SAGT 2018) and the 16th Workshop on\n  Approximation and Online Algorithms (WAOA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of resolving contention in communication networks\nwith selfish users. In a \\textit{contention game} each of $n \\geq 2$ identical\nplayers has a single information packet that she wants to transmit using one of\n$k \\geq 1$ multiple-access channels. To do that, a player chooses a\nslotted-time protocol that prescribes the probabilities with which at a given\ntime-step she will attempt transmission at each channel. If more than one\nplayers try to transmit over the same channel (collision) then no transmission\nhappens on that channel. Each player tries to minimize her own expected\n\\textit{latency}, i.e. her expected time until successful transmission, by\nchoosing her protocol. The natural problem that arises in such a setting is,\ngiven $n$ and $k$, to provide the players with a common, anonymous protocol (if\nit exists) such that no one would unilaterally deviate from it (equilibrium\nprotocol).\n  All previous theoretical results on strategic contention resolution examine\nonly the case of a single channel and show that the equilibrium protocols\ndepend on the feedback that the communication system gives to the players. Here\nwe present multi-channel equilibrium protocols in two main feedback classes,\nnamely \\textit{acknowledgement-based} and \\textit{ternary}. In particular, we\nprovide equilibrium characterizations for more than one channels, and give\nspecific anonymous, equilibrium protocols with finite and infinite expected\nlatency. In the equilibrium protocols with infinite expected latency, all\nplayers transmit successfully in optimal time, i.e. $\\Theta(n/k)$, with\nprobability tending to 1 as $n/k \\to \\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 14:57:43 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Christodoulou", "George", ""], ["Melissourgos", "Themistoklis", ""], ["Spirakis", "Paul G.", ""]]}, {"id": "1810.04903", "submitter": "Fatma BenSaid", "authors": "Fatma BenSaid and Adel M. Alimi", "title": "MOANOFS: Multi-Objective Automated Negotiation based Online Feature\n  Selection System for Big Data Classification", "comments": "15 pages, 8 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Selection (FS) plays an important role in learning and classification\ntasks. The object of FS is to select the relevant and non-redundant features.\nConsidering the huge amount number of features in real-world applications, FS\nmethods using batch learning technique can't resolve big data problem\nespecially when data arrive sequentially. In this paper, we propose an online\nfeature selection system which resolves this problem. More specifically, we\ntreat the problem of online supervised feature selection for binary\nclassification as a decision-making problem. A philosophical vision to this\nproblem leads to a hybridization between two important domains: feature\nselection using online learning technique (OFS) and automated negotiation (AN).\nThe proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation\nbased Online Feature Selection) uses two levels of decision. In the first\nlevel, from n learners (or OFS methods), we decide which are the k trustful\nones (with high confidence or trust value). These elected k learners will\nparticipate in the second level. In this level, we integrate our proposed\nMultilateral Automated Negotiation based OFS (MANOFS) method to decide finally\nwhich is the best solution or which are relevant features. We show that MOANOFS\nsystem is applicable to different domains successfully and achieves high\naccuracy with several real-world applications.\n  Index Terms: Feature selection, online learning, multi-objective automated\nnegotiation, trust, classification, big data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:41:30 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 15:19:17 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["BenSaid", "Fatma", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1810.05587", "submitter": "Pablo Hernandez-Leal", "authors": "Pablo Hernandez-Leal, Bilal Kartal and Matthew E. Taylor", "title": "A Survey and Critique of Multiagent Deep Reinforcement Learning", "comments": "Under review since Oct 2018. Earlier versions of this work had the\n  title: \"Is multiagent deep reinforcement learning the answer or the question?\n  A brief survey\"", "journal-ref": null, "doi": "10.1007/s10458-019-09421-1", "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has achieved outstanding results in recent\nyears. This has led to a dramatic increase in the number of applications and\nmethods. Recent works have explored learning beyond single-agent scenarios and\nhave considered multiagent learning (MAL) scenarios. Initial results report\nsuccesses in complex multiagent domains, although there are several challenges\nto be addressed. The primary goal of this article is to provide a clear\noverview of current multiagent deep reinforcement learning (MDRL) literature.\nAdditionally, we complement the overview with a broader analysis: (i) we\nrevisit previous key components, originally presented in MAL and RL, and\nhighlight how they have been adapted to multiagent deep reinforcement learning\nsettings. (ii) We provide general guidelines to new practitioners in the area:\ndescribing lessons learned from MDRL works, pointing to recent benchmarks, and\noutlining open avenues of research. (iii) We take a more critical tone raising\npractical challenges of MDRL (e.g., implementation and computational demands).\nWe expect this article will help unify and motivate future research to take\nadvantage of the abundant literature that exists (e.g., RL and MAL) in a joint\neffort to promote fruitful research in the multiagent community.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 15:54:05 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 00:54:10 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 19:33:23 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hernandez-Leal", "Pablo", ""], ["Kartal", "Bilal", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "1810.05766", "submitter": "Jaime Fisac", "authors": "Jaime F. Fisac, Eli Bronstein, Elis Stefansson, Dorsa Sadigh, S.\n  Shankar Sastry, Anca D. Dragan", "title": "Hierarchical Game-Theoretic Planning for Autonomous Vehicles", "comments": "Submitted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The actions of an autonomous vehicle on the road affect and are affected by\nthose of other drivers, whether overtaking, negotiating a merge, or avoiding an\naccident. This mutual dependence, best captured by dynamic game theory, creates\na strong coupling between the vehicle's planning and its predictions of other\ndrivers' behavior, and constitutes an open problem with direct implications on\nthe safety and viability of autonomous driving technology. Unfortunately,\ndynamic games are too computationally demanding to meet the real-time\nconstraints of autonomous driving in its continuous state and action space. In\nthis paper, we introduce a novel game-theoretic trajectory planning algorithm\nfor autonomous driving, that enables real-time performance by hierarchically\ndecomposing the underlying dynamic game into a long-horizon \"strategic\" game\nwith simplified dynamics and full information structure, and a short-horizon\n\"tactical\" game with full dynamics and a simplified information structure. The\nvalue of the strategic game is used to guide the tactical planning, implicitly\nextending the planning horizon, pushing the local trajectory optimization\ncloser to global solutions, and, most importantly, quantitatively accounting\nfor the autonomous vehicle and the human driver's ability and incentives to\ninfluence each other. In addition, our approach admits non-deterministic models\nof human decision-making, rather than relying on perfectly rational\npredictions. Our results showcase richer, safer, and more effective autonomous\nbehavior in comparison to existing techniques.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 00:02:54 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Fisac", "Jaime F.", ""], ["Bronstein", "Eli", ""], ["Stefansson", "Elis", ""], ["Sadigh", "Dorsa", ""], ["Sastry", "S. Shankar", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1810.06213", "submitter": "Fran\\c{c}ois Schwarzentruber", "authors": "Tristan Charrier, Fran\\c{c}ois Schwarzentruber, Eva Soulier", "title": "Dynamic Connected Cooperative Coverage Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the so-called dynamic coverage problem by agents located in some\ntopological graph. The agents must visit all regions of interest but they also\nshould stay connected to the base via multi-hop. We prove that the algorithmic\ncomplexity of this planning problem is PSPACE-complete. Furthermore we prove\nthat the problem becomes NP-complete for bounded plans. We also prove the same\ncomplexities for the reachability problem of some positions. We also prove that\ncomplexities are maintained for a subclass of topological graphs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:00:34 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Charrier", "Tristan", ""], ["Schwarzentruber", "Fran\u00e7ois", ""], ["Soulier", "Eva", ""]]}, {"id": "1810.06443", "submitter": "Damien Pellier", "authors": "Bruno Bouzy and Marc M\\'etivier and Damien Pellier", "title": "Hedging Algorithms and Repeated Matrix Games", "comments": "12 pages, Workshop of the European Conference on Machine Learning on\n  Machine Learning and Data Mining in and around Games, 2011", "journal-ref": "Workshop of the European Conference on Machine Learning on Machine\n  Learning and Data Mining in and around Games, 2011", "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Playing repeated matrix games (RMG) while maximizing the cumulative returns\nis a basic method to evaluate multi-agent learning (MAL) algorithms. Previous\nwork has shown that $UCB$, $M3$, $S$ or $Exp3$ algorithms have good behaviours\non average in RMG. Besides, hedging algorithms have been shown to be effective\non prediction problems. An hedging algorithm is made up with a top-level\nalgorithm and a set of basic algorithms. To make its decision, an hedging\nalgorithm uses its top-level algorithm to choose a basic algorithm, and the\nchosen algorithm makes the decision. This paper experimentally shows that\nwell-selected hedging algorithms are better on average than all previous MAL\nalgorithms on the task of playing RMG against various players. $S$ is a very\ngood top-level algorithm, and $UCB$ and $M3$ are very good basic algorithms.\nFurthermore, two-level hedging algorithms are more effective than one-level\nhedging algorithms, and three levels are not better than two levels.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:05:21 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Bouzy", "Bruno", ""], ["M\u00e9tivier", "Marc", ""], ["Pellier", "Damien", ""]]}, {"id": "1810.06653", "submitter": "Shi Pu", "authors": "Shi Pu, Wei Shi, Jinming Xu, Angelia Nedi\\'c", "title": "Push-Pull Gradient Methods for Distributed Optimization in Networks", "comments": "Parts of the results appear in Proceedings of the 57th IEEE\n  Conference on Decision and Control (see arXiv:1803.07588)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on solving a distributed convex optimization problem\nin a network, where each agent has its own convex cost function and the goal is\nto minimize the sum of the agents' cost functions while obeying the network\nconnectivity structure. In order to minimize the sum of the cost functions, we\nconsider new distributed gradient-based methods where each node maintains two\nestimates, namely, an estimate of the optimal decision variable and an estimate\nof the gradient for the average of the agents' objective functions. From the\nviewpoint of an agent, the information about the gradients is pushed to the\nneighbors, while the information about the decision variable is pulled from the\nneighbors hence giving the name \"push-pull gradient methods\". The methods\nutilize two different graphs for the information exchange among agents, and as\nsuch, unify the algorithms with different types of distributed architecture,\nincluding decentralized (peer-to-peer), centralized (master-slave), and\nsemi-centralized (leader-follower) architecture. We show that the proposed\nalgorithms and their many variants converge linearly for strongly convex and\nsmooth objective functions over a network (possibly with unidirectional data\nlinks) in both synchronous and asynchronous random-gossip settings. In\nparticular, under the random-gossip setting, \"push-pull\" is the first class of\nalgorithms for distributed optimization over directed graphs. Moreover, we\nnumerically evaluate our proposed algorithms in both scenarios, and show that\nthey outperform other existing linearly convergent schemes, especially for\nill-conditioned problems and networks that are not well balanced.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 23:58:51 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 16:16:38 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 16:49:30 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 06:07:43 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pu", "Shi", ""], ["Shi", "Wei", ""], ["Xu", "Jinming", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1810.06913", "submitter": "Guillaume Cheze", "authors": "Guillaume Ch\\`eze (IMT)", "title": "How to share a cake with a secret agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we study a problem of fair division in the absence of full\ninformation. We give an algorithm which solves the following problem: n $\\ge$ 2\npersons want to cut a cake into n shares so that each person will get at least\n1/n of the cake for his or her own measure, furthermore the preferences of one\nperson are secret. How can we construct such shares? Our algorithm is a slight\nmodification of the Even-Paz algorithm and allows to give a connected part to\neach agent. Moreover, the number of cuts used during the algorithm is optimal:\nO (n log(n)) .\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 10:20:06 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Ch\u00e8ze", "Guillaume", "", "IMT"]]}, {"id": "1810.06918", "submitter": "Cedric Buron", "authors": "C\\'edric Buron (LIP6), Zahia Guessoum (LIP6, CRESTIC), Sylvain Ductor\n  (LIP6), Olivier Roussel", "title": "MoCaNA, un agent de n{\\'e}gociation automatique utilisant la recherche\n  arborescente de Monte-Carlo", "comments": "in French", "journal-ref": "Vingt-sixi{\\`e}mes Journ{\\'e}es Francophones sur les Syst{\\`e}mes\n  Multi-Agents, Oct 2018, M{\\'e}tabief, France. 0011", "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated negotiation is a rising topic in Artificial Intelligence research.\nMonte Carlo methods have got increasing interest, in particular since they have\nbeen used with success on games with high branching factor such as go.In this\npaper, we describe an Monte Carlo Negotiating Agent (MoCaNA) whose bidding\nstrategy relies on Monte Carlo Tree Search. We provide our agent with opponent\nmodeling tehcniques for bidding strtaegy and utility. MoCaNA can negotiate on\ncontinuous negotiating domains and in a context where no bound has been\nspecified. We confront MoCaNA and the finalists of ANAC 2014 and a RandomWalker\non different negotiation domains. Our agent ouperforms the RandomWalker in a\ndomain without bound and the majority of the ANAC finalists in a domain with a\nbound.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 10:28:21 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Buron", "C\u00e9dric", "", "LIP6"], ["Guessoum", "Zahia", "", "LIP6, CRESTIC"], ["Ductor", "Sylvain", "", "LIP6"], ["Roussel", "Olivier", ""]]}, {"id": "1810.07063", "submitter": "Alvaro Perez-Diaz", "authors": "Alvaro Perez-Diaz, Enrico Gerding, Frank McGroarty", "title": "Catching Cheats: Detecting Strategic Manipulation in Distributed\n  Optimisation of Electric Vehicle Aggregators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the rapid rise of electric vehicles (EVs) worldwide, and the ambitious\ntargets set for the near future, the management of large EV fleets must be seen\nas a priority. Specifically, we study a scenario where EV charging is managed\nthrough self-interested EV aggregators who compete in the day-ahead market in\norder to purchase the electricity needed to meet their clients' requirements.\nWith the aim of reducing electricity costs and lowering the impact on\nelectricity markets, a centralised bidding coordination framework has been\nproposed in the literature employing a coordinator. In order to improve privacy\nand limit the need for the coordinator, we propose a reformulation of the\ncoordination framework as a decentralised algorithm, employing the Alternating\nDirection Method of Multipliers (ADMM). However, given the self-interested\nnature of the aggregators, they can deviate from the algorithm in order to\nreduce their energy costs. Hence, we study the strategic manipulation of the\nADMM algorithm and, in doing so, describe and analyse different possible attack\nvectors and propose a mathematical framework to quantify and detect\nmanipulation. Importantly, this detection framework is not limited the\nconsidered EV scenario and can be applied to general ADMM algorithms. Finally,\nwe test the proposed decentralised coordination and manipulation detection\nalgorithms in realistic scenarios using real market and driver data from Spain.\nOur empirical results show that the decentralised algorithm's convergence to\nthe optimal solution can be effectively disrupted by manipulative attacks\nachieving convergence to a different non-optimal solution which benefits the\nattacker. With respect to the detection algorithm, results indicate that it\nachieves very high accuracies and significantly outperforms a naive benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:47:46 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 10:13:52 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Perez-Diaz", "Alvaro", ""], ["Gerding", "Enrico", ""], ["McGroarty", "Frank", ""]]}, {"id": "1810.07254", "submitter": "Amos Azaria", "authors": "Yitzhak Spielberg, Amos Azaria", "title": "The Concept of Criticality in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning methods carry a well known bias-variance trade-off in\nn-step algorithms for optimal control. Unfortunately, this has rarely been\naddressed in current research. This trade-off principle holds independent of\nthe choice of the algorithm, such as n-step SARSA, n-step Expected SARSA or\nn-step Tree backup. A small n results in a large bias, while a large n leads to\nlarge variance. The literature offers no straightforward recipe for the best\nchoice of this value. While currently all n-step algorithms use a fixed value\nof n over the state space we extend the framework of n-step updates by allowing\neach state to have its specific n.\n  We propose a solution to this problem within the context of human aided\nreinforcement learning. Our approach is based on the observation that a human\ncan learn more efficiently if she receives input regarding the criticality of a\ngiven state and thus the amount of attention she needs to invest into the\nlearning in that state. This observation is related to the idea that each state\nof the MDP has a certain measure of criticality which indicates how much the\nchoice of the action in that state influences the return. In our algorithm the\nRL agent utilizes the criticality measure, a function provided by a human\ntrainer, in order to locally choose the best stepnumber n for the update of the\nQ function.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 20:07:06 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Spielberg", "Yitzhak", ""], ["Azaria", "Amos", ""]]}, {"id": "1810.07910", "submitter": "Eduardo Castell\\'o Ferrer", "authors": "Antonio Luca Alfeo, Eduardo Castell\\'o Ferrer, Yago Lizarribar\n  Carrillo, Arnaud Grignard, Luis Alonso Pastor, Dylan T. Sleeper, Mario G. C.\n  A. Cimino, Bruno Lepri, Gigliola Vaglini, Kent Larson, Marco Dorigo, Alex\n  `Sandy' Pentland", "title": "Urban Swarms: A new approach for autonomous waste management", "comments": "Manuscript accepted for publication in IEEE ICRA 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8794020", "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern cities are growing ecosystems that face new challenges due to the\nincreasing population demands. One of the many problems they face nowadays is\nwaste management, which has become a pressing issue requiring new solutions.\nSwarm robotics systems have been attracting an increasing amount of attention\nin the past years and they are expected to become one of the main driving\nfactors for innovation in the field of robotics. The research presented in this\npaper explores the feasibility of a swarm robotics system in an urban\nenvironment. By using bio-inspired foraging methods such as multi-place\nforaging and stigmergy-based navigation, a swarm of robots is able to improve\nthe efficiency and autonomy of the urban waste management system in a realistic\nscenario. To achieve this, a diverse set of simulation experiments was\nconducted using real-world GIS data and implementing different garbage\ncollection scenarios driven by robot swarms. Results presented in this research\nshow that the proposed system outperforms current approaches. Moreover, results\nnot only show the efficiency of our solution, but also give insights about how\nto design and customize these systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 06:11:34 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 07:27:53 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Alfeo", "Antonio Luca", ""], ["Ferrer", "Eduardo Castell\u00f3", ""], ["Carrillo", "Yago Lizarribar", ""], ["Grignard", "Arnaud", ""], ["Pastor", "Luis Alonso", ""], ["Sleeper", "Dylan T.", ""], ["Cimino", "Mario G. C. A.", ""], ["Lepri", "Bruno", ""], ["Vaglini", "Gigliola", ""], ["Larson", "Kent", ""], ["Dorigo", "Marco", ""], ["Pentland", "Alex `Sandy'", ""]]}, {"id": "1810.08647", "submitter": "Natasha Jaques", "authors": "Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre,\n  Pedro A. Ortega, DJ Strouse, Joel Z. Leibo, Nando de Freitas", "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified mechanism for achieving coordination and communication\nin Multi-Agent Reinforcement Learning (MARL), through rewarding agents for\nhaving causal influence over other agents' actions. Causal influence is\nassessed using counterfactual reasoning. At each timestep, an agent simulates\nalternate actions that it could have taken, and computes their effect on the\nbehavior of other agents. Actions that lead to bigger changes in other agents'\nbehavior are considered influential and are rewarded. We show that this is\nequivalent to rewarding agents for having high mutual information between their\nactions. Empirical results demonstrate that influence leads to enhanced\ncoordination and communication in challenging social dilemma environments,\ndramatically increasing the learning curves of the deep RL agents, and leading\nto more meaningful learned communication protocols. The influence rewards for\nall agents can be computed in a decentralized way by enabling agents to learn a\nmodel of other agents using deep neural networks. In contrast, key previous\nworks on emergent communication in the MARL setting were unable to learn\ndiverse policies in a decentralized manner and had to resort to centralized\ntraining. Consequently, the influence reward opens up a window of new\nopportunities for research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 19:01:15 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 22:44:48 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 23:52:07 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 21:39:08 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Jaques", "Natasha", ""], ["Lazaridou", "Angeliki", ""], ["Hughes", "Edward", ""], ["Gulcehre", "Caglar", ""], ["Ortega", "Pedro A.", ""], ["Strouse", "DJ", ""], ["Leibo", "Joel Z.", ""], ["de Freitas", "Nando", ""]]}, {"id": "1810.08743", "submitter": "Christopher Jung", "authors": "Christopher Jung, Sampath Kannan, Neil Lutz", "title": "Quantifying the Burden of Exploration and the Unfairness of Free Riding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-armed bandit setting with a twist. Rather than having\njust one decision maker deciding which arm to pull in each round, we have $n$\ndifferent decision makers (agents). In the simple stochastic setting, we show\nthat a \"free-riding\" agent observing another \"self-reliant\" agent can achieve\njust $O(1)$ regret, as opposed to the regret lower bound of $\\Omega (\\log t)$\nwhen one decision maker is playing in isolation. This result holds whenever the\nself-reliant agent's strategy satisfies either one of two assumptions: (1) each\narm is pulled at least $\\gamma \\ln t$ times in expectation for a constant\n$\\gamma$ that we compute, or (2) the self-reliant agent achieves $o(t)$\nrealized regret with high probability. Both of these assumptions are satisfied\nby standard zero-regret algorithms. Under the second assumption, we further\nshow that the free rider only needs to observe the number of times each arm is\npulled by the self-reliant agent, and not the rewards realized.\n  In the linear contextual setting, each arm has a distribution over parameter\nvectors, each agent has a context vector, and the reward realized when an agent\npulls an arm is the inner product of that agent's context vector with a\nparameter vector sampled from the pulled arm's distribution. We show that the\nfree rider can achieve $O(1)$ regret in this setting whenever the free rider's\ncontext is a small (in $L_2$-norm) linear combination of other agents' contexts\nand all other agents pull each arm $\\Omega (\\log t)$ times with high\nprobability. Again, this condition on the self-reliant players is satisfied by\nstandard zero-regret algorithms like UCB. We also prove a number of lower\nbounds.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 03:08:52 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 22:22:35 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 01:16:56 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 17:29:49 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Jung", "Christopher", ""], ["Kannan", "Sampath", ""], ["Lutz", "Neil", ""]]}, {"id": "1810.08799", "submitter": "Piotr Skowron", "authors": "Piotr Skowron", "title": "Proportionality Degree of Multiwinner Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multiwinner elections with approval-based preferences. An instance\nof a multiwinner election consists of a set of alternatives, a population of\nvoters---each voter approves a subset of alternatives, and the desired\ncommittee size $k$; the goal is to select a committee (a~subset) of $k$\nalternatives according to the preferences of the voters. We investigate a\nnumber of election rules and ask whether the committees that they return\nrepresent the voters proportionally. In contrast to the classic literature, we\nemploy quantitative techniques that allow to measure the extent to which the\nconsidered rules are proportional. This allows us to arrange the rules in a\nclear hierarchy. For example, we find that Proportional Approval Voting (PAV)\nhas better proportionality guarantees than its sequential counterpart, and that\nPhragm\\'{e}n's Sequential Rule is worse than Sequential PAV. Yet, the loss of\nproportionality for the two sequential rules is moderate and in some contexts\ncan be outweighed by their other appealing properties. Finally, we measure the\ntradeoff between proportionality and utilitarian efficiency for a broad\nsubclass of committee election rules.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 12:55:10 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 14:05:07 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Skowron", "Piotr", ""]]}, {"id": "1810.08901", "submitter": "Bicheng Ying", "authors": "Bicheng Ying, Kun Yuan, Ali H. Sayed", "title": "Dynamic Average Diffusion with randomized Coordinate Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work derives and analyzes an online learning strategy for tracking the\naverage of time-varying distributed signals by relying on randomized\ncoordinate-descent updates. During each iteration, each agent selects or\nobserves a random entry of the observation vector, and different agents may\nselect different entries of their observations before engaging in a\nconsultation step. Careful coordination of the interactions among agents is\nnecessary to avoid bias and ensure convergence. We provide a convergence\nanalysis for the proposed methods, and illustrate the results by means of\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 05:26:05 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 15:57:22 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ying", "Bicheng", ""], ["Yuan", "Kun", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1810.09026", "submitter": "Marc Lanctot", "authors": "Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat,\n  Karl Tuyls, Remi Munos, Michael Bowling", "title": "Actor-Critic Policy Optimization in Partially Observable Multiagent\n  Environments", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of parameterized policies for reinforcement learning (RL) is an\nimportant and challenging problem in artificial intelligence. Among the most\ncommon approaches are algorithms based on gradient ascent of a score function\nrepresenting discounted return. In this paper, we examine the role of these\npolicy gradient and actor-critic algorithms in partially-observable multiagent\nenvironments. We show several candidate policy update rules and relate them to\na foundation of regret minimization and multiagent learning techniques for the\none-shot and tabular cases, leading to previously unknown convergence\nguarantees. We apply our method to model-free multiagent reinforcement learning\nin adversarial sequential decision problems (zero-sum imperfect information\ngames), using RL-style function approximation. We evaluate on commonly used\nbenchmark Poker domains, showing performance against fixed policies and\nempirical convergence to approximate Nash equilibria in self-play with rates\nsimilar to or better than a baseline model-free algorithm for zero sum games,\nwithout any domain-specific state space reductions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 21:01:49 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 04:41:14 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 23:16:33 GMT"}, {"version": "v4", "created": "Sat, 19 Oct 2019 15:23:13 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 04:32:01 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Srinivasan", "Sriram", ""], ["Lanctot", "Marc", ""], ["Zambaldi", "Vinicius", ""], ["Perolat", "Julien", ""], ["Tuyls", "Karl", ""], ["Munos", "Remi", ""], ["Bowling", "Michael", ""]]}, {"id": "1810.09109", "submitter": "Olivier Cardin", "authors": "Olivier Cardin (PSI, LS2N, IUT NANTES), William Derigent (CRAN),\n  Damien Trentesaux (LAMIH)", "title": "Evolution of holonic control architectures towards Industry 4.0: A short\n  overview", "comments": null, "journal-ref": "INCOM 2018, 2018, Bergamo, Italy. 51 (11), pp.1243 - 1248, 2018", "doi": "10.1016/j.ifacol.2018.08.420", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility claimed by the next generation production systems induces a\ndeep modification of the behavior and the core itself of the control systems.\nOverconnectivity and data management abilities targeted by Industry 4.0\nparadigm enable the emergence of more flexible and reactive control systems,\nbased on the cooperation of autonomous and connected entities in the decision\nmaking process. For the last 20 years, holonic paradigm has become the core\nparadigm of those evolutions, and evolved in itself. This contribution aims at\nemphasizing the conceptual evolutions in the application of holonic paradigm in\nthe control architectures of manufacturing systems and highlighting the current\nresearch trends in this field.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 06:49:57 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Cardin", "Olivier", "", "PSI, LS2N, IUT NANTES"], ["Derigent", "William", "", "CRAN"], ["Trentesaux", "Damien", "", "LAMIH"]]}, {"id": "1810.09202", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu", "title": "Graph Convolutional Reinforcement Learning", "comments": "ICLR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to cooperate is crucially important in multi-agent environments. The\nkey is to understand the mutual interplay between agents. However, multi-agent\nenvironments are highly dynamic, where agents keep moving and their neighbors\nchange quickly. This makes it hard to learn abstract representations of mutual\ninterplay between agents. To tackle these difficulties, we propose graph\nconvolutional reinforcement learning, where graph convolution adapts to the\ndynamics of the underlying graph of the multi-agent environment, and relation\nkernels capture the interplay between agents by their relation representations.\nLatent features produced by convolutional layers from gradually increased\nreceptive fields are exploited to learn cooperation, and cooperation is further\nimproved by temporal relation regularization for consistency. Empirically, we\nshow that our method substantially outperforms existing methods in a variety of\ncooperative scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:17:40 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 12:42:24 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 05:21:13 GMT"}, {"version": "v4", "created": "Tue, 4 Feb 2020 03:22:09 GMT"}, {"version": "v5", "created": "Tue, 11 Feb 2020 13:46:23 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Dun", "Chen", ""], ["Huang", "Tiejun", ""], ["Lu", "Zongqing", ""]]}, {"id": "1810.09206", "submitter": "Heechang Ryu", "authors": "Heechang Ryu, Hayong Shin, Jinkyoo Park", "title": "Multi-Agent Actor-Critic with Generative Cooperative Policy Network", "comments": "10 pages, total 9 figures including all sub-figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an efficient multi-agent reinforcement learning approach to derive\nequilibrium strategies for multi-agents who are participating in a Markov game.\nMainly, we are focused on obtaining decentralized policies for agents to\nmaximize the performance of a collaborative task by all the agents, which is\nsimilar to solving a decentralized Markov decision process. We propose to use\ntwo different policy networks: (1) decentralized greedy policy network used to\ngenerate greedy action during training and execution period and (2) generative\ncooperative policy network (GCPN) used to generate action samples to make other\nagents improve their objectives during training period. We show that the\nsamples generated by GCPN enable other agents to explore the policy space more\neffectively and favorably to reach a better policy in terms of achieving the\ncollaborative tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:24:11 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Ryu", "Heechang", ""], ["Shin", "Hayong", ""], ["Park", "Jinkyoo", ""]]}, {"id": "1810.10591", "submitter": "Davide Dell'Anna", "authors": "Davide Dell'Anna, Mehdi Dastani, Fabiano Dalpiaz", "title": "Reasoning about Norms Revision", "comments": "Appears in the preproceedings of the 29th Benelux Conference on\n  Artificial Intelligence (BNAIC 2017), Groningen, The Netherlands, pp. 281 to\n  290", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Norms with sanctions have been widely employed as a mechanism for controlling\nand coordinating the behavior of agents without limiting their autonomy. The\nnorms enforced in a multi-agent system can be revised in order to increase the\nlikelihood that desirable system properties are fulfilled or that system\nperformance is sufficiently high. In this paper, we provide a preliminary\nanalysis of some types of norm revision: relaxation and strengthening.\nFurthermore, with the help of some illustrative scenarios, we show the\nusefulness of norm revision for better satisfying the overall system\nobjectives.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 19:52:01 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Dell'Anna", "Davide", ""], ["Dastani", "Mehdi", ""], ["Dalpiaz", "Fabiano", ""]]}, {"id": "1810.10862", "submitter": "David Manheim", "authors": "David Manheim", "title": "Multiparty Dynamics and Failure Modes for Machine Learning and\n  Artificial Intelligence", "comments": "12 Pages, This version re-submitted to Big Data and Cognitive\n  Computing, Special Issue \"Artificial Superintelligence: Coordination &\n  Strategy\"", "journal-ref": "Big Data Cogn. Comput. 2019, 3, 21", "doi": "10.3390/bdcc3020021", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge for safety in machine learning and artificial\nintelligence systems is a~set of related failures involving specification\ngaming, reward hacking, fragility to distributional shifts, and Goodhart's or\nCampbell's law. This paper presents additional failure modes for interactions\nwithin multi-agent systems that are closely related. These multi-agent failure\nmodes are more complex, more problematic, and less well understood than the\nsingle-agent case, and are also already occurring, largely unnoticed. After\nmotivating the discussion with examples from poker-playing artificial\nintelligence (AI), the paper explains why these failure modes are in some\nsenses unavoidable. Following this, the paper categorizes failure modes,\nprovides definitions, and cites examples for each of the modes: accidental\nsteering, coordination failures, adversarial misalignment, input spoofing and\nfiltering, and goal co-option or direct hacking. The paper then discusses how\nextant literature on multi-agent AI fails to address these failure modes, and\nidentifies work which may be useful for the mitigation of these failure modes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 10:55:58 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 15:58:08 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 19:38:05 GMT"}, {"version": "v4", "created": "Sun, 14 Apr 2019 07:39:06 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Manheim", "David", ""]]}, {"id": "1810.11131", "submitter": "Lei Zhang", "authors": "Lei Zhang, Diego Lai, Andriy V. Miranskyy", "title": "The Impact of Position Errors on Crowd Simulation", "comments": "37 pages, 12 figures, accepted for publication in: Simulation\n  Modelling Practice and Theory, Elsevier", "journal-ref": null, "doi": "10.1016/j.simpat.2018.10.010", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large crowd events, there is always a potential possibility that a\nstampede accident will occur. The accident may cause injuries or even death.\nApproaches for controlling crowd flows and predicting dangerous congestion\nspots would be a boon to on-site authorities to manage the crowd and to prevent\nfatal accidents. One of the most popular approaches is real-time crowd\nsimulation based on position data from personal Global Positioning System (GPS)\ndevices. However, the accuracy of spatial data varies for different GPS\ndevices, and it is also affected by an environment in which an event takes\nplace. In this paper, we would like to assess the effect of position errors on\nstampede prediction. We propose an Automatic Real-time dEtection of Stampedes\n(ARES) method to predict stampedes for large events. We implement three\ndifferent stampede assessment methods in Menge framework and incorporate\nposition errors. Our analysis suggests that the probability of simulated\nstampede changes significantly with the increase of the magnitude of position\nerrors, which cannot be eliminated entirely with the help of classic\ntechniques, such as the Kalman filter. Thus, it is our position that novel\nstampede assessment methods should be developed, focusing on the detection of\nposition noise and the elimination of its effect.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:12:29 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhang", "Lei", ""], ["Lai", "Diego", ""], ["Miranskyy", "Andriy V.", ""]]}, {"id": "1810.11187", "submitter": "Abhishek Das", "authors": "Abhishek Das, Th\\'eophile Gervet, Joshua Romoff, Dhruv Batra, Devi\n  Parikh, Michael Rabbat, Joelle Pineau", "title": "TarMAC: Targeted Multi-Agent Communication", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a targeted communication architecture for multi-agent\nreinforcement learning, where agents learn both what messages to send and whom\nto address them to while performing cooperative tasks in partially-observable\nenvironments. This targeting behavior is learnt solely from downstream\ntask-specific reward without any communication supervision. We additionally\naugment this with a multi-round communication approach where agents coordinate\nvia multiple rounds of communication before taking actions in the environment.\nWe evaluate our approach on a diverse set of cooperative multi-agent tasks, of\nvarying difficulties, with varying number of agents, in a variety of\nenvironments ranging from 2D grid layouts of shapes and simulated traffic\njunctions to 3D indoor environments, and demonstrate the benefits of targeted\nand multi-round communication. Moreover, we show that the targeted\ncommunication strategies learned by agents are interpretable and intuitive.\nFinally, we show that our architecture can be easily extended to mixed and\ncompetitive environments, leading to improved performance and sample complexity\nover recent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 04:22:58 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 04:37:13 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Das", "Abhishek", ""], ["Gervet", "Th\u00e9ophile", ""], ["Romoff", "Joshua", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Rabbat", "Michael", ""], ["Pineau", "Joelle", ""]]}, {"id": "1810.11489", "submitter": "Alexander Siegenfeld", "authors": "Alexander Siegenfeld and Yaneer Bar-Yam", "title": "Negative Representation and Instability in Democratic Elections", "comments": null, "journal-ref": "Nature Physics 16, 186-190 (2020)", "doi": "10.1038/s41567-019-0739-6", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of understanding the collective behaviors of social systems can\nbenefit from methods and concepts from physics [1-6], not because humans are\nsimilar to electrons, but because certain large-scale behaviors can be\nunderstood without an understanding of the small-scale details [7], in much the\nsame way that sound waves can be understood without an understanding of atoms.\nDemocratic elections are one such behavior. Over the past few decades,\nphysicists have explored scaling patterns in voting and the dynamics of\npolitical opinion formation, e.g. [8-13]. Here, we define the concepts of\nnegative representation, in which a shift in electorate opinions produces a\nshift in the election outcome in the opposite direction, and electoral\ninstability, in which an arbitrarily small change in electorate opinions can\ndramatically swing the election outcome, and prove that unstable elections\nnecessarily contain negatively represented opinions. Furthermore, in the\npresence of low voter turnout, increasing polarization of the electorate can\ndrive elections through a transition from a stable to an unstable regime,\nanalogous to the phase transition by which some materials become ferromagnetic\nbelow their critical temperatures. Empirical data suggest that United States\npresidential elections underwent such a phase transition in the 1970s and have\nsince become increasingly unstable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:31:35 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 01:43:42 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Siegenfeld", "Alexander", ""], ["Bar-Yam", "Yaneer", ""]]}, {"id": "1810.11541", "submitter": "Huanfei Zheng", "authors": "Huanfei Zheng, Zhanrui Liao, Yue Wang", "title": "Human-Robot Trust Integrated Task Allocation and Symbolic Motion\n  planning for Heterogeneous Multi-robot Systems", "comments": null, "journal-ref": null, "doi": "10.1115/DSCC2018-9161", "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a human-robot trust integrated task allocation and motion\nplanning framework for multi-robot systems (MRS) in performing a set of tasks\nconcurrently. A set of task specifications in parallel are conjuncted with MRS\nto synthesize a task allocation automaton. Each transition of the task\nallocation automaton is associated with the total trust value of human in\ncorresponding robots. Here, the human-robot trust model is constructed with a\ndynamic Bayesian network (DBN) by considering individual robot performance,\nsafety coefficient, human cognitive workload and overall evaluation of task\nallocation. Hence, a task allocation path with maximum encoded human-robot\ntrust can be searched based on the current trust value of each robot in the\ntask allocation automaton. Symbolic motion planning (SMP) is implemented for\neach robot after they obtain the sequence of actions. The task allocation path\ncan be intermittently updated with this DBN based trust model. The overall\nstrategy is demonstrated by a simulation with 5 robots and 3 parallel subtask\nautomata.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 21:52:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zheng", "Huanfei", ""], ["Liao", "Zhanrui", ""], ["Wang", "Yue", ""]]}, {"id": "1810.11548", "submitter": "Chenyuan He", "authors": "Chenyuan He, Yan Wan, and Frank L. Lewis", "title": "On the Identifiability of the Influence Model for Stochastic\n  Spatiotemporal Spread Processes", "comments": "This temporary draft version of this paper has caused conflict of\n  interest and we request to withdraw this paper from arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence model is a discrete-time stochastic model that succinctly\ncaptures the interactions of a network of Markov chains. The model produces a\nreduced-order representation of the stochastic network, and can be used to\ndescribe and tractably analyze probabilistic spatiotemporal spread dynamics,\nand hence has found broad usage in network applications such as social\nnetworks, traffic management, and failure cascades in power systems. This paper\nprovides sufficient and necessary conditions for the identifiability of the\ninfluence model, and also develops estimators for the model structure through\nexploiting the model's special properties. In addition, we analyze conditions\nfor the identifiability of the partially observed influence model (POIM), for\nwhich not all of the sites can be measured.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 22:34:30 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 15:44:47 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["He", "Chenyuan", ""], ["Wan", "Yan", ""], ["Lewis", "Frank L.", ""]]}, {"id": "1810.11634", "submitter": "Jose Fontanari", "authors": "Sandro M. Reia, Andr\\'e C. Amado and Jos\\'e F. Fontanari", "title": "Agent-based models of collective intelligence", "comments": null, "journal-ref": "Phys. Life Rev. 31 (2019) 320-331", "doi": "10.1016/j.plrev.2018.10.004", "report-no": null, "categories": "cs.MA cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective or group intelligence is manifested in the fact that a team of\ncooperating agents can solve problems more efficiently than when those agents\nwork in isolation. Although cooperation is, in general, a successful problem\nsolving strategy, it is not clear whether it merely speeds up the time to find\nthe solution, or whether it alters qualitatively the statistical signature of\nthe search for the solution. Here we review and offer insights on two\nagent-based models of distributed cooperative problem-solving systems, whose\ntask is to solve a cryptarithmetic puzzle. The first model is the imitative\nlearning search in which the agents exchange information on the quality of\ntheir partial solutions to the puzzle and imitate the most successful agent in\nthe group. This scenario predicts a very poor performance in the case imitation\nis too frequent or the group is too large, a phenomenon akin to Groupthink of\nsocial psychology. The second model is the blackboard organization in which\nagents read and post hints on a public blackboard. This brainstorming scenario\nperforms the best when there is a stringent limit to the amount of information\nthat is exhibited on the board. Both cooperative scenarios produce a\nsubstantial speed up of the time to solve the puzzle as compared with the\nsituation where the agents work in isolation. The statistical signature of the\nsearch, however, is the same as that of the independent search.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 12:33:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Reia", "Sandro M.", ""], ["Amado", "Andr\u00e9 C.", ""], ["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "1810.11702", "submitter": "Christian Schroeder de Witt", "authors": "Christian A. Schroeder de Witt, Jakob N. Foerster, Gregory Farquhar,\n  Philip H. S. Torr, Wendelin Boehmer, Shimon Whiteson", "title": "Multi-Agent Common Knowledge Reinforcement Learning", "comments": "Advances in Neural Information Processing Systems, 9924-9935", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative multi-agent reinforcement learning often requires decentralised\npolicies, which severely limit the agents' ability to coordinate their\nbehaviour. In this paper, we show that common knowledge between agents allows\nfor complex decentralised coordination. Common knowledge arises naturally in a\nlarge number of decentralised cooperative multi-agent tasks, for example, when\nagents can reconstruct parts of each others' observations. Since agents an\nindependently agree on their common knowledge, they can execute complex\ncoordinated policies that condition on this knowledge in a fully decentralised\nfashion. We propose multi-agent common knowledge reinforcement learning\n(MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical\npolicy tree. Higher levels in the hierarchy coordinate groups of agents by\nconditioning on their common knowledge, or delegate to lower levels with\nsmaller subgroups but potentially richer common knowledge. The entire policy\ntree can be executed in a fully decentralised fashion. As the lowest policy\ntree level consists of independent policies for each agent, MACKRL reduces to\nindependently learnt decentralised policies as a special case. We demonstrate\nthat our method can exploit common knowledge for superior performance on\ncomplex decentralised coordination tasks, including a stochastic matrix game\nand challenging problems in StarCraft II unit micromanagement.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 20:45:19 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 14:53:34 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 13:05:30 GMT"}, {"version": "v4", "created": "Sun, 23 Jun 2019 16:45:17 GMT"}, {"version": "v5", "created": "Tue, 1 Oct 2019 11:13:59 GMT"}, {"version": "v6", "created": "Sun, 10 Nov 2019 13:35:42 GMT"}, {"version": "v7", "created": "Tue, 3 Dec 2019 11:03:40 GMT"}, {"version": "v8", "created": "Sat, 11 Jan 2020 22:42:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["de Witt", "Christian A. Schroeder", ""], ["Foerster", "Jakob N.", ""], ["Farquhar", "Gregory", ""], ["Torr", "Philip H. S.", ""], ["Boehmer", "Wendelin", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1810.12457", "submitter": "Milind Rao", "authors": "Milind Rao, Stefano Rini, Andrea Goldsmith", "title": "Distributed Convex Optimization With Limited Communications", "comments": "Extended version of submission to IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a distributed convex optimization algorithm, termed\n\\emph{distributed coordinate dual averaging} (DCDA) algorithm, is proposed. The\nDCDA algorithm addresses the scenario of a large distributed optimization\nproblem with limited communication among nodes in the network. Currently known\ndistributed subgradient methods, such as the distributed dual averaging or the\ndistributed alternating direction method of multipliers algorithms, assume that\nnodes can exchange messages of large cardinality. Such network communication\ncapabilities are not valid in many scenarios of practical relevance. In the\nDCDA algorithm, on the other hand, communication of each coordinate of the\noptimization variable is restricted over time. For the proposed algorithm, we\nbound the rate of convergence under different communication protocols and\nnetwork architectures. We also consider the extensions to the case of imperfect\ngradient knowledge and the case in which transmitted messages are corrupted by\nadditive noise or are quantized. Relevant numerical simulations are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 23:42:37 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Rao", "Milind", ""], ["Rini", "Stefano", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1810.13084", "submitter": "Nicolas Loizou", "authors": "Nicolas Loizou, Michael Rabbat, Peter Richt\\'arik", "title": "Provably Accelerated Randomized Gossip Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present novel provably accelerated gossip algorithms for\nsolving the average consensus problem. The proposed protocols are inspired from\nthe recently developed accelerated variants of the randomized Kaczmarz method -\na popular method for solving linear systems. In each gossip iteration all nodes\nof the network update their values but only a pair of them exchange their\nprivate information. Numerical experiments on popular wireless sensor networks\nshowing the benefits of our protocols are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 02:59:02 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Loizou", "Nicolas", ""], ["Rabbat", "Michael", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1810.13189", "submitter": "Yacine Ouzrout", "authors": "Samia Chehbi-Gamoura (LIESP), Yacine Ouzrout (LIESP), Abdelaziz Bouras\n  (LIESP)", "title": "Multi-Layers Supply chain modelling based on Multi-Agent Approach", "comments": null, "journal-ref": "Ed. L.M. Cam. Emerging Solutions for Future Manufacturing\n  systems., Springer/ Kluwer, pp.307-314, 2004", "doi": null, "report-no": "Lyon 2", "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategic multi layers model based on multi agents\napproach for supply chain system. It introduces a formulation and a solution\nmethodology for the problem of supply chain design and modeling. In this paper\nwe describe and analyze the relationships among main entities of a supply\nchain, such as suppliers, producers, and distribution centers, in the aim to\ndesign the agents and define their behavior. We also study, how these\nrelationships can be formulated in a multi layer model. Finally, a generic\nmulti agent model is illustrated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:57:07 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Chehbi-Gamoura", "Samia", "", "LIESP"], ["Ouzrout", "Yacine", "", "LIESP"], ["Bouras", "Abdelaziz", "", "LIESP"]]}, {"id": "1810.13191", "submitter": "Yacine Ouzrout", "authors": "Laurent Buzon (LIESP), Abdelaziz Bouras (LIESP), Yacine Ouzrout\n  (LIESP)", "title": "Infrastructure for the representation and electronic exchange of design\n  knowledge", "comments": null, "journal-ref": "IJEBM, 2003, 1 (1), pp.1-8", "doi": null, "report-no": "Lyon 2", "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the concept of knowledge and its exchange using Semantic\nWeb technologies. It points out that knowledge is more than information because\nit embodies the meaning, that is to say semantic and context. These\ncharacteristics will influence our approach to represent and to treat the\nknowledge. In order to be adopted, the developed system needs to be simple and\nto use standards. The goal of the paper is to find standards to model knowledge\nand exchange it with an other person. Therefore, we propose to model knowledge\nusing UML models to show a graphical representation and to exchange it with XML\nto ensure the portability at low cost. We introduce the concept of ontology for\norganizing knowledge and for facilitating the knowledge exchange. Proposals\nhave been tested by implementing an application on the design knowledge of a\npen.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:07:47 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Buzon", "Laurent", "", "LIESP"], ["Bouras", "Abdelaziz", "", "LIESP"], ["Ouzrout", "Yacine", "", "LIESP"]]}, {"id": "1810.13193", "submitter": "Yacine Ouzrout", "authors": "Napaporn Reeveerakul (LIESP), Ridha Derrouiche (LIESP), Nopasit\n  Chakpitak (CAMT), Yacine Ouzrout (LIESP), Napat Harnpornchai, Abdelaziz\n  Bouras (LIESP)", "title": "A Decision Support Framework for Manufacturing Improvement and\n  Relocation Prevention in Thailand: Supply Chain Perspective", "comments": null, "journal-ref": "International Conference on Industrial Engineering and Systems\n  Management, May 2009, Montreal, Canada. pp.9, 2009", "doi": null, "report-no": "Lyon 2", "categories": "cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low economic growth and competition among neighbouring countries has\ncaused Foreign Direct nvestments (FDIs) to relocate their businesses. In order\nto prevent further business relocation, this aper proposes an integrated\nframework based on the supply chain to help analyse decision making for plant\nsituations and enhance manufacturing perrformance. The context of this\nperspective is pplied to manufacturers located in the industrial state region\nof Lumphun province, Thailand. Data collection and review of literature was\nused to identify the factors that influence industrial investment. The SCOR\nmodel was used to define the parameters, which here then used in Arena\nsimulation. The simulation needs to describe factors affected on industrial\nerformance. From this result, an integrated analysis model was built and the\nimportance of supply chain collaboration was identified. A Multi Agent System\n(MAS) was suggested to enhance of the effective nteraction between supply\nchain' agents. It is in order to mitigate risks among them.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:11:59 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Reeveerakul", "Napaporn", "", "LIESP"], ["Derrouiche", "Ridha", "", "LIESP"], ["Chakpitak", "Nopasit", "", "CAMT"], ["Ouzrout", "Yacine", "", "LIESP"], ["Harnpornchai", "Napat", "", "LIESP"], ["Bouras", "Abdelaziz", "", "LIESP"]]}, {"id": "1810.13195", "submitter": "Yacine Ouzrout", "authors": "Thtiya Manakitsirisuthi, Yacine Ouzrout (LIESP), Abdelaziz Bouras\n  (LIESP)", "title": "A multi-agent system for managing the product lifecycle sustainability", "comments": null, "journal-ref": "International Conference on Software, Knowledge and Application,\n  Oct 2009, Fez, Morocco. pp.8, 2009", "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The international competitive market causes the increasing of shorten product\nlife cycle and product development process with the improvement in term of\ntime, cost and quality while increasing the waste generation. Product life\ncycle sustainability can reduce waste, conserve resources, use recycling\nmaterials, design product for easy disassembly and avoid using hazardous\nmaterial. This paper proposes a knowledge management architecture, based on a\nmulti-agent system, which focuses on the \"sustainability\" in order to manage\nknowledge in each stage of the product lifecycle, and particularly in the\nrecovery process. The aim of this research work is to make the link between a\ndecision-making system based on the agent's knowledge about the sustainability\n(environmental norms, rules...) and a PLM (Product Lifecycle Management)\nsystem. The software Agents will help the decision makers in each stage of the\nlifecycle and make them take into account the environmental impact of their\ndecisions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 10:17:33 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Manakitsirisuthi", "Thtiya", "", "LIESP"], ["Ouzrout", "Yacine", "", "LIESP"], ["Bouras", "Abdelaziz", "", "LIESP"]]}]