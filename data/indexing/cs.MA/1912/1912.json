[{"id": "1912.00011", "submitter": "Jaelle Scheuerman", "authors": "Jaelle Scheuerman, Jason L. Harman, Nicholas Mattei, K. Brent Venable", "title": "Heuristic Strategies in Uncertain Approval Voting Environments", "comments": "arXiv admin note: text overlap with arXiv:1905.12104", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many collective decision making situations, agents vote to choose an\nalternative that best represents the preferences of the group. Agents may\nmanipulate the vote to achieve a better outcome by voting in a way that does\nnot reflect their true preferences. In real world voting scenarios, people\noften do not have complete information about other voter preferences and it can\nbe computationally complex to identify a strategy that will maximize their\nexpected utility. In such situations, it is often assumed that voters will vote\ntruthfully rather than expending the effort to strategize. However, being\ntruthful is just one possible heuristic that may be used. In this paper, we\nexamine the effectiveness of heuristics in single winner and multi-winner\napproval voting scenarios with missing votes. In particular, we look at\nheuristics where a voter ignores information about other voting profiles and\nmakes their decisions based solely on how much they like each candidate. In a\nbehavioral experiment, we show that people vote truthfully in some situations\nand prioritize high utility candidates in others. We examine when these\nbehaviors maximize expected utility and show how the structure of the voting\nenvironment affects both how well each heuristic performs and how humans employ\nthese heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:38:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Scheuerman", "Jaelle", ""], ["Harman", "Jason L.", ""], ["Mattei", "Nicholas", ""], ["Venable", "K. Brent", ""]]}, {"id": "1912.00225", "submitter": "Michael Curry", "authors": "Michael J. Curry, John P. Dickerson, Karthik Abinav Sankararaman,\n  Aravind Srinivasan, Yuhao Wan, Pan Xu", "title": "Mix and Match: Markov Chains & Mixing Times for Matching in Rideshare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rideshare platforms such as Uber and Lyft dynamically dispatch drivers to\nmatch riders' requests. We model the dispatching process in rideshare as a\nMarkov chain that takes into account the geographic mobility of both drivers\nand riders over time. Prior work explores dispatch policies in the limit of\nsuch Markov chains; we characterize when this limit assumption is valid, under\na variety of natural dispatch policies. We give explicit bounds on convergence\nin general, and exact (including constants) convergence rates for special\ncases. Then, on simulated and real transit data, we show that our bounds\ncharacterize convergence rates -- even when the necessary theoretical\nassumptions are relaxed. Additionally these policies compare well against a\nstandard reinforcement learning algorithm which optimizes for profit without\nany convergence properties.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 16:26:33 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Curry", "Michael J.", ""], ["Dickerson", "John P.", ""], ["Sankararaman", "Karthik Abinav", ""], ["Srinivasan", "Aravind", ""], ["Wan", "Yuhao", ""], ["Xu", "Pan", ""]]}, {"id": "1912.00253", "submitter": "Hang Ma", "authors": "Ngai Meng Kou, Cheng Peng, Hang Ma, T. K. Satish Kumar, Sven Koenig", "title": "Idle Time Optimization for Target Assignment and Path Finding in\n  Sortation Centers", "comments": "AAAI 2020, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the one-shot and lifelong versions of the Target\nAssignment and Path Finding problem in automated sortation centers, where each\nagent needs to constantly assign itself a sorting station, move to its assigned\nstation without colliding with obstacles or other agents, wait in the queue of\nthat station to obtain a parcel for delivery, and then deliver the parcel to a\nsorting bin. The throughput of such centers is largely determined by the total\nidle time of all stations since their queues can frequently become empty. To\naddress this problem, we first formalize and study the one-shot version that\nassigns stations to a set of agents and finds collision-free paths for the\nagents to their assigned stations. We present efficient algorithms for this\ntask based on a novel min-cost max-flow formulation that minimizes the total\nidle time of all stations in a fixed time window. We then demonstrate how our\nalgorithms for solving the one-shot problem can be applied to solving the\nlifelong problem as well. Experimentally, we believe to be the first\nresearchers to consider real-world automated sortation centers using an\nindustrial simulator with realistic data and a kinodynamic model of real\nrobots. On this simulator, we showcase the benefits of our algorithms by\ndemonstrating their efficiency and effectiveness for up to 350 agents.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 19:16:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kou", "Ngai Meng", ""], ["Peng", "Cheng", ""], ["Ma", "Hang", ""], ["Kumar", "T. K. Satish", ""], ["Koenig", "Sven", ""]]}, {"id": "1912.00303", "submitter": "Hong Xu", "authors": "Han Zhang and Hong Xu", "title": "MANELA: A Multi-Agent Algorithm for Learning Network Embeddings", "comments": "11 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Playing an essential role in data mining, machine learning has a long history\nof being applied to networks on multifarious tasks and has played an essential\nrole in data mining. However, the discrete and sparse natures of networks often\nrender it difficult to apply machine learning directly to networks. To\ncircumvent this difficulty, one major school of thought to approach networks\nusing machine learning is via network embeddings. On the one hand, this network\nembeddings have achieved huge success on aggregated network data in recent\nyears. On the other hand, learning network embeddings on distributively stored\nnetworks still remained understudied: To the best of our knowledge, all\nexisting algorithms for learning network embeddings have hitherto been\nexclusively centralized and thus cannot be applied to these networks. To\naccommodate distributively stored networks, in this paper, we proposed a\nmulti-agent model. Under this model, we developed the multi-agent network\nembedding learning algorithm (MANELA) for learning network embeddings. We\ndemonstrate MANELA's advantages over other existing centralized network\nembedding learning algorithms both theoretically and experimentally. Finally,\nwe further our understanding in MANELA via visualization and exploration of its\nrelationship to DeepWalk.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 02:23:34 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Hong", ""]]}, {"id": "1912.00437", "submitter": "Pavel Chebotarev", "authors": "Natalia Basimova and Pavel Chebotarev", "title": "Clustering as a means of leader selection in consensus networks", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the leader-follower approach, one or more agents are selected as leaders\nwho do not change their states or have autonomous dynamics and can influence\nother agents, while the other agents, called followers, perform a simple\nprotocol based on the states of their neighbors. This approach provides a\nnatural link between control theory and networked agents with their input data.\nDespite the fact that the leader-follower approach is widely used, the\nfundamental question still remains: how to choose leaders from a set of agent.\nThis question is called the problem of choosing leaders. There is still no\nselection algorithm that is both optimal under a natural criterion and fast. In\nthis paper, for agents that obey a linear consensus protocol, we propose to\nchoose leaders using graph nodes' clustering algorithms and show that this\nmethod is the most accurate among the fast existing algorithms of choosing\nleaders.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 16:08:24 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Basimova", "Natalia", ""], ["Chebotarev", "Pavel", ""]]}, {"id": "1912.00498", "submitter": "Donghwan Lee", "authors": "Donghwan Lee, Niao He, Parameswaran Kamalaruban, Volkan Cevher", "title": "Optimization for Reinforcement Learning: From Single Agent to\n  Cooperative Agents", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2020.2976000", "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews recent advances in multi-agent reinforcement learning\nalgorithms for large-scale control systems and communication networks, which\nlearn to communicate and cooperate. We provide an overview of this emerging\nfield, with an emphasis on the decentralized setting under different\ncoordination protocols. We highlight the evolution of reinforcement learning\nalgorithms from single-agent to multi-agent systems, from a distributed\noptimization perspective, and conclude with future directions and challenges,\nin the hope to catalyze the growing synergy among distributed optimization,\nsignal processing, and reinforcement learning communities.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 20:39:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lee", "Donghwan", ""], ["He", "Niao", ""], ["Kamalaruban", "Parameswaran", ""], ["Cevher", "Volkan", ""]]}, {"id": "1912.00733", "submitter": "Yue Zhao", "authors": "Hossein Khazaei, X. Andy Sun, and Yue Zhao", "title": "On the Equilibria and Efficiency of Electricity Markets with Renewable\n  Power Producers and Congestion Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing renewable penetration in power systems, a prominent challenge\nin efficient and reliable power system operation is handling the uncertainties\ninherent in the renewable generation. In this paper, we propose a simple\ntwo-settlement market mechanism in which renewable power producers (RPPs)\nparticipate, so that a) the independent system operator (ISO) does not need to\nconsider the uncertainties of the renewables in its economic dispatch, and yet\nb) the market equilibrium is shown to approach social efficiency as if the ISO\nsolves a stochastic optimization taking into account all the uncertainties. In\nshowing this result, a key innovation is a new approach of efficiently\ncomputing the Nash equilibrium (NE) among the strategic RPPs in\ncongestion-constrained power networks. In particular, the proposed approach\ndecouples finding an NE into searching over congestion patterns and computing\nan NE candidate assuming a congestion pattern. As such, the computational\ncomplexity of finding an NE grows only cubically with the number of RPPs in the\nmarket. We demonstrate our results in the IEEE 14-bus system and show that the\nNE approaches social efficiency as the number of RPPs grows.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:20:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Khazaei", "Hossein", ""], ["Sun", "X. Andy", ""], ["Zhao", "Yue", ""]]}, {"id": "1912.00758", "submitter": "Carlos Gershenson", "authors": "Carlos Gershenson", "title": "Guiding the Self-organization of Cyber-Physical Systems", "comments": "16 pages, 2 figures", "journal-ref": "Frontiers in Robotics and AI, 7:41, 2020", "doi": "10.3389/frobt.2020.00041", "report-no": null, "categories": "cs.MA nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organization offers a promising approach for designing adaptive systems.\nGiven the inherent complexity of most cyber-physical systems, adaptivity is\ndesired, as predictability is limited. Here I summarize different concepts and\napproaches that can facilitate self-organization in cyber-physical systems, and\nthus be exploited for design. Then I mention real-world examples of systems\nwhere self-organization has managed to provide solutions that outperform\nclassical approaches, in particular related to urban mobility. Finally, I\nidentify when a centralized, distributed, or self-organizing control is more\nappropriate.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 01:52:35 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Gershenson", "Carlos", ""]]}, {"id": "1912.00949", "submitter": "Feng Wu", "authors": "Yixiang Wang and Feng Wu", "title": "Multi-Agent Deep Reinforcement Learning with Adaptive Policies", "comments": "arXiv admin note: text overlap with arXiv:1706.02275 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to address one aspect of the non-stationarity\nproblem in multi-agent reinforcement learning (RL), where the other agents may\nalter their policies due to environment changes during execution. This violates\nthe Markov assumption that governs most single-agent RL methods and is one of\nthe key challenges in multi-agent RL. To tackle this, we propose to train\nmultiple policies for each agent and postpone the selection of the best policy\nat execution time. Specifically, we model the environment non-stationarity with\na finite set of scenarios and train policies fitting each scenario. In addition\nto multiple policies, each agent also learns a policy predictor to determine\nwhich policy is the best with its local information. By doing so, each agent is\nable to adapt its policy when the environment changes and consequentially the\nother agents alter their policies during execution. We empirically evaluated\nour method on a variety of common benchmark problems proposed for multi-agent\ndeep RL in the literature. Our experimental results show that the agents\ntrained by our algorithm have better adaptiveness in changing environments and\noutperform the state-of-the-art methods in all the tested environments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 07:23:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Yixiang", ""], ["Wu", "Feng", ""]]}, {"id": "1912.01513", "submitter": "Jan Feyereisl", "authors": "Marek Rosa and Olga Afanasjeva and Simon Andersson and Joseph Davidson\n  and Nicholas Guttenberg and Petr Hlubu\\v{c}ek and Martin Poliak and Jaroslav\n  V\\'itku and Jan Feyereisl", "title": "BADGER: Learning to (Learn [Learning Algorithms] through Multi-Agent\n  Communication)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel memory-based multi-agent meta-learning\narchitecture and learning procedure that allows for learning of a shared\ncommunication policy that enables the emergence of rapid adaptation to new and\nunseen environments by learning to learn learning algorithms through\ncommunication. Behavior, adaptation and learning to adapt emerges from the\ninteractions of homogeneous experts inside a single agent. The proposed\narchitecture should allow for generalization beyond the level seen in existing\nmethods, in part due to the use of a single policy shared by all experts within\nthe agent as well as the inherent modularity of 'Badger'.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:36:42 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Rosa", "Marek", ""], ["Afanasjeva", "Olga", ""], ["Andersson", "Simon", ""], ["Davidson", "Joseph", ""], ["Guttenberg", "Nicholas", ""], ["Hlubu\u010dek", "Petr", ""], ["Poliak", "Martin", ""], ["V\u00edtku", "Jaroslav", ""], ["Feyereisl", "Jan", ""]]}, {"id": "1912.01629", "submitter": "Tarik A. Rashid", "authors": "Danial A. Muhammed, Soran A.M. Saeed, Tarik A. Rashid", "title": "A Simulation Model for Pedestrian Crowd Evacuation Based on Various AI\n  Techniques", "comments": "10 pages", "journal-ref": "Revue d'Intelligence Artificielle, 2019", "doi": "10.18280/ria.330404", "report-no": null, "categories": "cs.MA cs.AI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper attempts to design an intelligent simulation model for pedestrian\ncrowd evacuation. For this purpose, the cellular automata(CA) was fully\nintegrated with fuzzy logic, the kth nearest neighbors (KNN), and some\nstatistical equations. In this model, each pedestrian was assigned a specific\nspeed, according to his/her physical, biological and emotional features. The\nemergency behavior and evacuation efficiency of each pedestrian were evaluated\nby coupling his or her speed with various elements, such as environment,\npedestrian distribution and familiarity with the exits. These elements all have\ngreat impacts on the evacuation process. Several experiments were carried out\nto verify the performance of the model in different emergency scenarios. The\nresults show that the proposed model can predict the evacuation time and\nemergency behavior in various types of building interiors and pedestrian\ndistributions. The research provides a good reference to the design of building\nevacuation systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 19:03:57 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Muhammed", "Danial A.", ""], ["Saeed", "Soran A. M.", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "1912.01665", "submitter": "Gangshan Jing", "authors": "Gangshan Jing, Changhuang Wan and Ran Dai", "title": "Angle-Based Sensor Network Localization", "comments": "This is a supplementary paper containing all the theoretical proofs\n  omitted in the paper \"Angle-based sensor network localization\", which will\n  appear in IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies angle-based sensor network localization (ASNL) in a plane,\nwhich is to determine locations of all sensors in a sensor network, given\nlocations of partial sensors (called anchors) and angle measurements obtained\nin the local coordinate frame of each sensor. Firstly it is shown that a\nframework with a non-degenerate bilateration ordering must be angle fixable,\nimplying that it can be uniquely determined by angles between edges up to\ntranslations, rotations, reflections and uniform scaling. Then ASNL is proved\nto have a unique solution if and only if the grounded framework is angle\nfixable and anchors are not all collinear. Subsequently, ASNL is solved in\ncentralized and distributed settings, respectively. The centralized ASNL is\nformulated as a rank-constrained semi-definite program (SDP) in either a\nnoise-free or a noisy scenario, with a decomposition approach proposed to deal\nwith large-scale ASNL. The distributed protocol for ASNL is designed based on\ninter-sensor communications. Graphical conditions for equivalence of the\nformulated rank-constrained SDP and a linear SDP, decomposition of the SDP, as\nwell as the effectiveness of the distributed protocol, are proposed,\nrespectively. Finally, simulation examples demonstrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:06:04 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 15:25:38 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 16:21:49 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 03:03:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jing", "Gangshan", ""], ["Wan", "Changhuang", ""], ["Dai", "Ran", ""]]}, {"id": "1912.01711", "submitter": "Jorge Pe\\~na Queralta", "authors": "Jorge Pe\\~na Queralta and Tomi Westerlund", "title": "Blockchain-Powered Collaboration in Heterogeneous Swarms of Robots", "comments": "2019 Symposium on Blockchain for Robotics and AI Systems, 16 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in the collaboration within heterogeneous\nmulti-robot systems is the optimization of the amount and type of data to be\nshared between robots with different sensing capabilities and computational\nresources. In this paper, we present a novel approach to managing collaboration\nterms in heterogeneous multi-robot systems with blockchain technology.\nLeveraging the extensive research of consensus algorithms in the blockchain\ndomain, we exploit key technologies in this field to be integrated for\nconsensus in robotic systems. We propose the utilization of proof of work\nsystems to have an online estimation of the available computational resources\nat different robots. Furthermore, we define smart contracts that integrate\ninformation about the environment from different robots in order to evaluate\nand rank the quality and accuracy of each of the robots' sensor data. This\nmeans that the key parameters involved in heterogeneous robotic collaboration\nare integrated within the Blockchain and estimated at all robots equally\nwithout explicitly sharing information about the robots' hardware or sensors.\nTrustability is based on the verification of data samples that are submitted to\nthe blockchain within each data exchange transaction and validated by other\nrobots operating in the same environment. Initial results are reported which\nshow the viability of the concepts presented in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:28:06 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 12:19:49 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 18:00:07 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Queralta", "Jorge Pe\u00f1a", ""], ["Westerlund", "Tomi", ""]]}, {"id": "1912.01741", "submitter": "Marco Sim\\~oes", "authors": "Marco A. C. Sim\\~oes, Robson Marinho da Silva and Tatiane Nogueira", "title": "A Dataset Schema for Cooperative Learning from Demonstration in\n  Multi-robots Systems", "comments": "This is a pre-print of an article published in the Journal of\n  Intelligent & Robotic Systems. The final authenticated version will be\n  available online at: https://doi. org/10.1007/s10846-019-01123-w", "journal-ref": null, "doi": "10.1007/s10846-019-01123-w", "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Agent Systems (MASs) have been used to solve complex problems that\ndemand intelligent agents working together to reach the desired goals. These\nAgents should effectively synchronize their individual behaviors so that they\ncan act as a team in a coordinated manner to achieve the common goal of the\nwhole system. One of the main issues in MASs is the agents' coordination, being\ncommon domain experts observing MASs execution disapprove agents' decisions.\nEven if the MAS was designed using the best methods and tools for agents'\ncoordination, this difference of decisions between experts and MAS is\nconfirmed. Therefore, this paper proposes a new dataset schema to support\nlearning the coordinated behavior in MASs from demonstration. The results of\nthe proposed solution are validated in a Multi-Robot System (MRS) organizing a\ncollection of new cooperative plans recommendations from the demonstration by\ndomain experts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 23:42:24 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Sim\u00f5es", "Marco A. C.", ""], ["da Silva", "Robson Marinho", ""], ["Nogueira", "Tatiane", ""]]}, {"id": "1912.02000", "submitter": "Martina Vanelli", "authors": "Martina Vanelli, Laura Arditti, Giacomo Como, Fabio Fagnani", "title": "On games with coordinating and anti-coordinating agents", "comments": "8 pages, 6 figures", "journal-ref": "21st IFAC World Congress, Berlin (2020), pp. 10975-10980", "doi": null, "report-no": null, "categories": "cs.GT cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies Nash equilibria for games where a mixture of coordinating\nand anti-coordinating agents, with possibly heterogeneous thresholds, coexist\nand interact through an all-to-all network. Whilst games with only coordinating\nor only anti-coordinating agents are potential, also in the presence of\nheterogeneities, this does not hold when both type of agents are simultaneously\npresent. This makes their analysis more difficult and existence of Nash\nequilibria not guaranteed. Our main result is a checkable condition on the\nthreshold distributions that characterizes the existence of Nash equilibria in\nsuch mixed games. When this condition is satisfied an explicit algorithm allows\nto determine the complete set of such equilibria. Moreover, for the special\ncase when only one type of agents is present (either coordinating or\nanti-coordinating), our results allow an explicit computation of the\ncardinality of Nash equilibria.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:11:31 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:18:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Vanelli", "Martina", ""], ["Arditti", "Laura", ""], ["Como", "Giacomo", ""], ["Fagnani", "Fabio", ""]]}, {"id": "1912.02318", "submitter": "Adam Lerer", "authors": "Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown", "title": "Improving Policies via Search in Cooperative Partially Observable Games", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent superhuman results in games have largely been achieved in a variety of\nzero-sum settings, such as Go and Poker, in which agents need to compete\nagainst others. However, just like humans, real-world AI systems have to\ncoordinate and communicate with other agents in cooperative partially\nobservable environments as well. These settings commonly require participants\nto both interpret the actions of others and to act in a way that is informative\nwhen being interpreted. Those abilities are typically summarized as theory f\nmind and are seen as crucial for social interactions. In this paper we propose\ntwo different search techniques that can be applied to improve an arbitrary\nagreed-upon policy in a cooperative partially observable game. The first one,\nsingle-agent search, effectively converts the problem into a single agent\nsetting by making all but one of the agents play according to the agreed-upon\npolicy. In contrast, in multi-agent search all agents carry out the same\ncommon-knowledge search procedure whenever doing so is computationally\nfeasible, and fall back to playing according to the agreed-upon policy\notherwise. We prove that these search procedures are theoretically guaranteed\nto at least maintain the original performance of the agreed-upon policy (up to\na bounded approximation error). In the benchmark challenge problem of Hanabi,\nour search technique greatly improves the performance of every agent we tested\nand when applied to a policy trained using RL achieves a new state-of-the-art\nscore of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:14:34 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lerer", "Adam", ""], ["Hu", "Hengyuan", ""], ["Foerster", "Jakob", ""], ["Brown", "Noam", ""]]}, {"id": "1912.02580", "submitter": "Francesco Farina", "authors": "Francesco Farina", "title": "Collective Learning", "comments": "update references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of collective learning (CL) which\nexploits the notion of collective intelligence in the field of distributed\nsemi-supervised learning. The proposed framework draws inspiration from the\nlearning behavior of human beings, who alternate phases involving\ncollaboration, confrontation and exchange of views with other consisting of\nstudying and learning on their own. On this regard, CL comprises two main\nphases: a self-training phase in which learning is performed on local private\n(labeled) data only and a collective training phase in which proxy-labels are\nassigned to shared (unlabeled) data by means of a consensus-based algorithm. In\nthe considered framework, heterogeneous systems can be connected over the same\nnetwork, each with different computational capabilities and resources and\neveryone in the network may take advantage of the cooperation and will\neventually reach higher performance with respect to those it can reach on its\nown. An extensive experimental campaign on an image classification problem\nemphasizes the properties of CL by analyzing the performance achieved by the\ncooperating agents.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:08:34 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:39:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Farina", "Francesco", ""]]}, {"id": "1912.02775", "submitter": "John Cartlidge", "authors": "Henry Hanifan and John Cartlidge", "title": "Fools Rush In: Competitive Effects of Reaction Time in Automated Trading", "comments": "12 pages, 9 figures. Author's accepted manuscript. Published in\n  ICAART 2020: Proceedings of the 12th International Conference on Agents and\n  Artificial Intelligence, pages 82-93. Valletta, Malta, Feb. 2020. V2 edits:\n  source code links moved from reference list to footnotes", "journal-ref": "In Proceedings of the 12th International Conference on Agents and\n  Artificial Intelligence - Volume 1: ICAART, pages 82-93 (2020)", "doi": "10.5220/0008973700820093", "report-no": null, "categories": "q-fin.TR cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the competitive effects of reaction time of automated trading\nstrategies in simulated financial markets containing a single exchange with\npublic limit order book and continuous double auction matching. A large body of\nresearch conducted over several decades has been devoted to trading agent\ndesign and simulation, but the majority of this work focuses on pricing\nstrategy and does not consider the time taken for these strategies to compute.\nIn real-world financial markets, speed is known to heavily influence the design\nof automated trading algorithms, with the generally accepted wisdom that faster\nis better. Here, we introduce increasingly realistic models of trading speed\nand profile the computation times of a suite of eminent trading algorithms from\nthe literature. Results demonstrate that: (a) trading performance is impacted\nby speed, but faster is not always better; (b) the Adaptive-Aggressive (AA)\nalgorithm, until recently considered the most dominant trading strategy in the\nliterature, is outperformed by the simplistic Shaver (SHVR) strategy - shave\none tick off the current best bid or ask - when relative computation times are\naccurately simulated.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:14:05 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 16:50:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hanifan", "Henry", ""], ["Cartlidge", "John", ""]]}, {"id": "1912.02945", "submitter": "Thanh-Trung Trinh", "authors": "Thanh-Trung Trinh, Dinh-Minh Vu, Masaomi Kimura", "title": "A pedestrian path-planning model in accordance with obstacle's danger\n  with reinforcement learning", "comments": null, "journal-ref": null, "doi": "10.1145/3388176.3388187", "report-no": null, "categories": "cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most microscopic pedestrian navigation models use the concept of \"forces\"\napplied to the pedestrian agents to replicate the navigation environment. While\nthe approach could provide believable results in regular situations, it does\nnot always resemble natural pedestrian navigation behaviour in many typical\nsettings. In our research, we proposed a novel approach using reinforcement\nlearning for simulation of pedestrian agent path planning and collision\navoidance problem. The primary focus of this approach is using human perception\nof the environment and danger awareness of interferences. The implementation of\nour model has shown that the path planned by the agent shares many similarities\nwith a human pedestrian in several aspects such as following common walking\nconventions and human behaviours.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:40:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Trinh", "Thanh-Trung", ""], ["Vu", "Dinh-Minh", ""], ["Kimura", "Masaomi", ""]]}, {"id": "1912.03347", "submitter": "Jose Fontanari", "authors": "Sandro M. Reia, Larissa F. Aquino and Jos\\'e F. Fontanari", "title": "The surprising little effectiveness of cooperative algorithms in\n  parallel problem solving", "comments": null, "journal-ref": "Eur. Phys. J. B (2020) 93: 140", "doi": "10.1140/epjb/e2020-10199-9", "report-no": null, "categories": "cs.MA cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and cultural inspired optimization algorithms are nowadays part of\nthe basic toolkit of a great many research domains. By mimicking processes in\nnature and animal societies, these general-purpose search algorithms promise to\ndeliver optimal or near-optimal solutions using hardly any information on the\noptimization problems they are set to tackle. Here we study the performances of\na cultural-inspired algorithm -- the imitative learning search -- as well as of\nasexual and sexual variants of evolutionary algorithms in finding the global\nmaxima of NK-fitness landscapes. The main performance measure is the total\nnumber of agent updates required by the algorithms to find those global maxima\nand the baseline performance, which establishes the effectiveness of the\ncooperative algorithms, is set by the blind search in which the agents explore\nthe problem space (binary strings) by flipping bits at random. We find that\neven for smooth landscapes that exhibit a single maximum, the evolutionary\nalgorithms do not perform much better than the blind search due to the\nstochastic effects of the genetic roulette. The imitative learning is immune to\nthis effect thanks to the deterministic choice of the fittest string in the\npopulation, which is used as a model for imitation. The tradeoff is that for\nrugged landscapes the imitative learning search is more prone to be trapped in\nlocal maxima than the evolutionary algorithms. In fact, in the case of rugged\nlandscapes with a mild density of local maxima, the blind search either beats\nor matches the cooperative algorithms regardless of whether the task is to find\nthe global maximum or to find the fittest state within a given runtime.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:22:42 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 16:01:51 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:37:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Reia", "Sandro M.", ""], ["Aquino", "Larissa F.", ""], ["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "1912.03460", "submitter": "Bolin Gao", "authors": "Bolin Gao, Lacra Pavel", "title": "Continuous-time Discounted Mirror-Descent Dynamics in Monotone Concave\n  Games", "comments": "8 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": "IEEE Transactions on Automatic Control, vol 66 (11), 2021", "doi": "10.1109/TAC.2020.3045094", "report-no": null, "categories": "math.OC cs.GT cs.LG cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider concave continuous-kernel games characterized by\nmonotonicity properties and propose discounted mirror descent-type dynamics. We\nintroduce two classes of dynamics whereby the associated mirror map is\nconstructed based on a strongly convex or a Legendre regularizer. Depending on\nthe properties of the regularizer we show that these new dynamics can converge\nasymptotically in concave games with monotone (negative) pseudo-gradient.\nFurthermore, we show that when the regularizer enjoys strong convexity, the\nresulting dynamics can converge even in games with hypo-monotone (negative)\npseudo-gradient, which corresponds to a shortage of monotonicity.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 08:04:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gao", "Bolin", ""], ["Pavel", "Lacra", ""]]}, {"id": "1912.03470", "submitter": "Mohammadreza Doostmohammadian", "authors": "Mohammadreza Doostmohammadian and Usman A. Khan", "title": "Minimal Sufficient Conditions for Structural\n  Observability/Controllability of Composite Networks via Kronecker Product", "comments": "Accepted for publication in IEEE TSIPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider composite networks formed from the Kronecker\nproduct of smaller networks. We find the observability and controllability\nproperties of the product network from those of its constituent smaller\nnetworks. The overall network is modeled as a Linear-Structure-Invariant (LSI)\ndynamical system where the underlying matrices have a fixed zero/non-zero\nstructure but the non-zero elements are potentially time-varying. This approach\nallows to model the system parameters as free variables whose values may only\nbe known within a certain tolerance. We particularly look for minimal\nsufficient conditions on the observability and controllability of the composite\nnetwork, which have a direct application in distributed estimation and in the\ndesign of networked control systems. The methodology in this paper is based on\nthe structured systems analysis and graph theory, and therefore, the results\nare generic, i.e., they apply to almost all non-zero choices of free\nparameters. We show the controllability/observability results for composite\nproduct networks resulting from full structural-rank systems and self-damped\nnetworks. We provide an illustrative example of estimation based on Kalman\nfiltering over a composite network to verify our results.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 09:13:46 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Doostmohammadian", "Mohammadreza", ""], ["Khan", "Usman A.", ""]]}, {"id": "1912.03558", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Igor Borovikov, Hongyuan Zha", "title": "Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill\n  Discovery", "comments": "Published at International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human players in professional team sports achieve high level coordination by\ndynamically choosing complementary skills and executing primitive actions to\nperform these skills. As a step toward creating intelligent agents with this\ncapability for fully cooperative multi-agent settings, we propose a two-level\nhierarchical multi-agent reinforcement learning (MARL) algorithm with\nunsupervised skill discovery. Agents learn useful and distinct skills at the\nlow level via independent Q-learning, while they learn to select complementary\nlatent skill variables at the high level via centralized multi-agent training\nwith an extrinsic team reward. The set of low-level skills emerges from an\nintrinsic reward that solely promotes the decodability of latent skill\nvariables from the trajectory of a low-level skill, without the need for\nhand-crafted rewards for each skill. For scalable decentralized execution, each\nagent independently chooses latent skill variables and primitive actions based\non local observations. Our overall method enables the use of general\ncooperative MARL algorithms for training high level policies and single-agent\nRL for training low level skills. Experiments on a stochastic high dimensional\nteam game show the emergence of useful skills and cooperative team play. The\ninterpretability of the learned skills show the promise of the proposed method\nfor achieving human-AI cooperation in team sports games.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 20:41:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:30:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 03:00:47 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yang", "Jiachen", ""], ["Borovikov", "Igor", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1912.03821", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Zhuoran Yang, Tamer Ba\\c{s}ar", "title": "Decentralized Multi-Agent Reinforcement Learning with Networked Agents:\n  Recent Advances", "comments": "This is a invited submission to a Special Issue of the Journal of\n  Frontiers of Information Technology & Electronic Engineering (FITEE). Most of\n  the contents are based on the Sec. 4 in our recent overview arXiv:1911.10635,\n  with focus on the setting of decentralized MARL with networked agents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) has long been a significant and\neverlasting research topic in both machine learning and control. With the\nrecent development of (single-agent) deep RL, there is a resurgence of\ninterests in developing new MARL algorithms, especially those that are backed\nby theoretical analysis. In this paper, we review some recent advances a\nsub-area of this topic: decentralized MARL with networked agents. Specifically,\nmultiple agents perform sequential decision-making in a common environment,\nwithout the coordination of any central controller. Instead, the agents are\nallowed to exchange information with their neighbors over a communication\nnetwork. Such a setting finds broad applications in the control and operation\nof robots, unmanned vehicles, mobile sensor networks, and smart grid. This\nreview is built upon several our research endeavors in this direction, together\nwith some progresses made by other researchers along the line. We hope this\nreview to inspire the devotion of more research efforts to this exciting yet\nchallenging area.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 02:33:57 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Yang", "Zhuoran", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1912.03851", "submitter": "Ujwal Padam Tewari", "authors": "Ujwal Padam Tewari, Vishal Bidawatka, Varsha Raveendran, Vinay\n  Sudhakaran, Shreedhar Kodate Shreeshail, Jayanth Prakash Kulkarni", "title": "Intelligent Coordination among Multiple Traffic Intersections Using\n  Multi-Agent Reinforcement Learning", "comments": "Accepted in the NeurIPS 2019 Deep RL Workshop :\n  https://sites.google.com/view/deep-rl-workshop-neurips-2019/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Asynchronous Advantage Actor Critic (A3C) for implementing an AI agent\nin the controllers that optimize flow of traffic across a single intersection\nand then extend it to multiple intersections by considering a multi-agent\nsetting. We explore three different methodologies to address the multi-agent\nproblem - (1) use of asynchronous property of A3C to control multiple\nintersections using a single agent (2) utilise self/competitive play among\nindependent agents across multiple intersections and (3) ingest a global reward\nfunction among agents to introduce cooperative behavior between intersections.\nWe observe that (1) & (2) leads to a reduction in traffic congestion.\nAdditionally the use of (3) with (1) & (2) led to a further reduction in\ncongestion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 04:54:31 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 12:53:18 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 15:58:24 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 14:21:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tewari", "Ujwal Padam", ""], ["Bidawatka", "Vishal", ""], ["Raveendran", "Varsha", ""], ["Sudhakaran", "Vinay", ""], ["Shreeshail", "Shreedhar Kodate", ""], ["Kulkarni", "Jayanth Prakash", ""]]}, {"id": "1912.03960", "submitter": "Ankit Sharma", "authors": "Ankit Sharma, Garima Gupta, Ranjitha Prasad, Arnab Chatterjee,\n  Lovekesh Vig, Gautam Shroff", "title": "MetaCI: Meta-Learning for Causal Inference in a Heterogeneous Population", "comments": "10 pages, 4 figures, Accepted in CausalML Workshop - NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing inference on data obtained through observational studies is\nbecoming extremely relevant due to the widespread availability of data in\nfields such as healthcare, education, retail, etc. Furthermore, this data is\naccrued from multiple homogeneous subgroups of a heterogeneous population, and\nhence, generalizing the inference mechanism over such data is essential. We\npropose the MetaCI framework with the goal of answering counterfactual\nquestions in the context of causal inference (CI), where the factual\nobservations are obtained from several homogeneous subgroups. While the CI\nnetwork is designed to generalize from factual to counterfactual distribution\nin order to tackle covariate shift, MetaCI employs the meta-learning paradigm\nto tackle the shift in data distributions between training and test phase due\nto the presence of heterogeneity in the population, and due to drifts in the\ntarget distribution, also known as concept shift. We benchmark the performance\nof the MetaCI algorithm using the mean absolute percentage error over the\naverage treatment effect as the metric, and demonstrate that meta\ninitialization has significant gains compared to randomly initialized networks,\nand other methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:01:09 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 05:40:02 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 05:06:05 GMT"}, {"version": "v4", "created": "Fri, 1 May 2020 05:15:55 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2020 11:02:08 GMT"}, {"version": "v6", "created": "Wed, 17 Feb 2021 15:19:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Sharma", "Ankit", ""], ["Gupta", "Garima", ""], ["Prasad", "Ranjitha", ""], ["Chatterjee", "Arnab", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "1912.04442", "submitter": "Hossein Moradian", "authors": "Hossein Moradian and Solmaz S. Kia", "title": "A Study on Accelerating Average Consensus Algorithms Using Delayed\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study accelerating a Laplacian-based dynamic average\nconsensus algorithm by splitting the conventional delay-free disagreement\nfeedback into weighted summation of a current and an outdated term. We\ndetermine for what weighted sum there exists a range of time delay that results\nin the higher rate of convergence for the algorithm. For such weights, using\nthe Lambert W function, we obtain the rate increasing range of the time delay,\nthe maximum reachable rate and comment on the value of the corresponding\nmaximizer delay. We also study the effect of use of outdated feedback on the\ncontrol effort of the agents and show that only for some specific affine\ncombination of the immediate and outdated feedback the control effort of the\nagents does not go beyond that of the delay-free algorithm. Additionally, we\ndemonstrate that using outdated feedback does not increase the steady state\ntracking error of the average consensus algorithm. Lastly, we determine the\noptimum combination of the current and the outdated feedback weights to achieve\nthe maximum increase in the rate of convergence without increasing the control\neffort of the agents. We demonstrate our results through a numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 01:26:03 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Moradian", "Hossein", ""], ["Kia", "Solmaz S.", ""]]}, {"id": "1912.04451", "submitter": "Alexander Shmakov", "authors": "Alexander Shmakov, John Lanier, Stephen McAleer, Rohan Achar, Cristina\n  Lopes, and Pierre Baldi", "title": "ColosseumRL: A Framework for Multiagent Reinforcement Learning in\n  $N$-Player Games", "comments": "Accepted for the 2020 AAAI Spring Symposium, Challenges and\n  Opportunities for Multi-Agent Reinforcement Learning. Source code available\n  at https://github.com/colosseumrl/colosseumrl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of recent success in multiagent reinforcement learning has been in\ntwo-player zero-sum games. In these games, algorithms such as fictitious\nself-play and minimax tree search can converge to an approximate Nash\nequilibrium. While playing a Nash equilibrium strategy in a two-player zero-sum\ngame is optimal, in an $n$-player general sum game, it becomes a much less\ninformative solution concept. Despite the lack of a satisfying solution\nconcept, $n$-player games form the vast majority of real-world multiagent\nsituations. In this paper we present a new framework for research in\nreinforcement learning in $n$-player games. We hope that by analyzing behavior\nlearned by agents in these environments the community can better understand\nthis important research area and move toward meaningful solution concepts and\nresearch directions. The implementation and additional information about this\nframework can be found at https://colosseumrl.igb.uci.edu/.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 02:03:39 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Shmakov", "Alexander", ""], ["Lanier", "John", ""], ["McAleer", "Stephen", ""], ["Achar", "Rohan", ""], ["Lopes", "Cristina", ""], ["Baldi", "Pierre", ""]]}, {"id": "1912.04531", "submitter": "Prashant Khanduri", "authors": "Prashant Khanduri, Saikiran Bulusu, Pranay Sharma, and Pramod K.\n  Varshney", "title": "Byzantine Resilient Non-Convex SVRG with Distributed Batch Gradient\n  Computations", "comments": "Optimization for Machine Learning, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the distributed stochastic optimization problem of\nminimizing a non-convex function $f(x) = \\mathbb{E}_{\\xi \\sim \\mathcal{D}} f(x;\n\\xi)$ in an adversarial setting, where the individual functions $f(x; \\xi)$ can\nalso be potentially non-convex. We assume that at most $\\alpha$-fraction of a\ntotal of $K$ nodes can be Byzantines. We propose a robust stochastic\nvariance-reduced gradient (SVRG) like algorithm for the problem, where the\nbatch gradients are computed at the worker nodes (WNs) and the stochastic\ngradients are computed at the server node (SN). For the non-convex optimization\nproblem, we show that we need $\\tilde{O}\\left( \\frac{1}{\\epsilon^{5/3} K^{2/3}}\n+ \\frac{\\alpha^{4/3}}{\\epsilon^{5/3}} \\right)$ gradient computations on average\nat each node (SN and WNs) to reach an $\\epsilon$-stationary point. The proposed\nalgorithm guarantees convergence via the design of a novel Byzantine filtering\nrule which is independent of the problem dimension. Importantly, we capture the\neffect of the fraction of Byzantine nodes $\\alpha$ present in the network on\nthe convergence performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:42:52 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Khanduri", "Prashant", ""], ["Bulusu", "Saikiran", ""], ["Sharma", "Pranay", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1912.04800", "submitter": "Lars Ankile", "authors": "Lars Lien Ankile, Kjartan Krange, Yuto Yagi", "title": "Approximate Strategy-Proofness in Large, Two-Sided Matching Markets", "comments": "9 pages, 6 figures, 1 algorithm, empirical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An approximation of strategy-proofness in large, two-sided matching markets\nis highly evident. Through simulations, one can observe that the percentage of\nagents with useful deviations decreases as the market size grows. Furthermore,\nthere seems to be a strong connection between the length of preference order\nlists, the correlation of agent preferences, and the approximation of strategy\nproofness. Interestingly, approximate strategy proofness is reached easier with\nshorter length of preference orders and higher preference correlation. These\nfindings justify the use of the deferred acceptance algorithm in large\ntwo-sided matching markets despite it not being strategy-proof.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:34:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Ankile", "Lars Lien", ""], ["Krange", "Kjartan", ""], ["Yagi", "Yuto", ""]]}, {"id": "1912.04941", "submitter": "Svitlana Vyetrenko", "authors": "Svitlana Vyetrenko, David Byrd, Nick Petosa, Mahmoud Mahfouz, Danial\n  Dervovic, Manuela Veloso, Tucker Hybinette Balch", "title": "Get Real: Realism Metrics for Robust Limit Order Book Market Simulations", "comments": null, "journal-ref": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy", "doi": null, "report-no": null, "categories": "q-fin.TR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (especially reinforcement learning) methods for trading are\nincreasingly reliant on simulation for agent training and testing. Furthermore,\nsimulation is important for validation of hand-coded trading strategies and for\ntesting hypotheses about market structure. A challenge, however, concerns the\nrobustness of policies validated in simulation because the simulations lack\nfidelity. In fact, researchers have shown that many market simulation\napproaches fail to reproduce statistics and stylized facts seen in real\nmarkets. As a step towards addressing this we surveyed the literature to\ncollect a set of reference metrics and applied them to real market data and\nsimulation output. Our paper provides a comprehensive catalog of these metrics\nincluding mathematical formulations where appropriate. Our results show that\nthere are still significant discrepancies between simulated markets and real\nones. However, this work serves as a benchmark against which we can measure\nfuture improvement.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:28:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Vyetrenko", "Svitlana", ""], ["Byrd", "David", ""], ["Petosa", "Nick", ""], ["Mahfouz", "Mahmoud", ""], ["Dervovic", "Danial", ""], ["Veloso", "Manuela", ""], ["Balch", "Tucker Hybinette", ""]]}, {"id": "1912.05304", "submitter": "Hangyu Mao", "authors": "Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, Yan Ni", "title": "Learning Agent Communication under Limited Bandwidth by Message Pruning", "comments": "accepted as a regular paper with poster presentation @ AAAI20. arXiv\n  admin note: text overlap with arXiv:1903.05561", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is a crucial factor for the big multi-agent world to stay\norganized and productive. Recently, Deep Reinforcement Learning (DRL) has been\napplied to learn the communication strategy and the control policy for multiple\nagents. However, the practical \\emph{\\textbf{limited bandwidth}} in multi-agent\ncommunication has been largely ignored by the existing DRL methods.\nSpecifically, many methods keep sending messages incessantly, which consumes\ntoo much bandwidth. As a result, they are inapplicable to multi-agent systems\nwith limited bandwidth. To handle this problem, we propose a gating mechanism\nto adaptively prune less beneficial messages. We evaluate the gating mechanism\non several tasks. Experiments demonstrate that it can prune a lot of messages\nwith little impact on performance. In fact, the performance may be greatly\nimproved by pruning redundant messages. Moreover, the proposed gating mechanism\nis applicable to several previous methods, equipping them the ability to\naddress bandwidth restricted settings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:41:36 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mao", "Hangyu", ""], ["Zhang", "Zhengchao", ""], ["Xiao", "Zhen", ""], ["Gong", "Zhibo", ""], ["Ni", "Yan", ""]]}, {"id": "1912.05362", "submitter": "Sylvain Cherrier", "authors": "Hantanirina Felixie, Jean Razafindramintsa, Sylvain Cherrier (LIGM),\n  Thomas Mahatody, Laurent George (LIGM), Victor Manantsoa", "title": "Jason-RS, a Collaboration between Agents and an IoT Platform", "comments": null, "journal-ref": "International Workshop on Networking for Smart Living, Dec 2019,\n  Paris, France", "doi": null, "report-no": null, "categories": "cs.MA cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we start from the observation that REST services are the most\nused as tools of interoperability and orchestration in the Internet of Things\n(IoT). But REST does not make it possible to inject artificial intelligence\ninto connected objects, ie it cannot allow autonomy and decision-making by the\nobjects themselves. To define an intelligence to a connected object, one can\nuse a Beleive Desire Intention agent (BDI an intelligent agent that adopts\nhuman behavior) such as Jason Agentspeak. But Jason AgentSpeak does not\nguarantee orchestration or choreography between connected objects. There are\nplatforms for service orchestration and choreography in IoT, still the\ninterconnection with artificial intelligence needs to be built. In this\narticle, we propose a new approach called Jason-RS. It is a result of pairing\nJason BDI agent with the web service technologies to exploit the agent capacity\nas a service, Jason-RS turn in Java SE and it does not need any middleware. The\narchitecture that we propose allows to create the link between Artificial\nIntelligence and Services choreography to reduce human intervention in the\nservice choreography. In order to validate the proposed approach, we have\ninterconnected the Iot BeC 3 platform and the REST agent (Jason-RS). The\ndecision-making faculty offered by Jason-RS is derived from the information\nsent by the objects according to the different methods of REST (GET, POST, PUT,\nand DELETE) that Jason-RS offers. As a result, the objects feed the inter-agent\ncollaborations and decision-making inside the agent. Finally, we show that\nJason-RS allows the Web of Objects to power complex systems such as an\nartificial intelligence responsible for processing data. This performance is\npromising.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:43:22 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Felixie", "Hantanirina", "", "LIGM"], ["Razafindramintsa", "Jean", "", "LIGM"], ["Cherrier", "Sylvain", "", "LIGM"], ["Mahatody", "Thomas", "", "LIGM"], ["George", "Laurent", "", "LIGM"], ["Manantsoa", "Victor", ""]]}, {"id": "1912.05434", "submitter": "Gregory Chance", "authors": "Greg Chance, Abanoub Ghobrial, Severin Lemaignan, Tony Pipe, Kerstin\n  Eder", "title": "An Agency-Directed Approach to Test Generation for Simulation-based\n  Autonomous Vehicle Verification", "comments": "18 pages, 8 figures", "journal-ref": "2020 IEEE International Conference On Artificial Intelligence\n  Testing (AITest)", "doi": "10.1109/AITEST49225.2020.00012", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based verification is beneficial for assessing otherwise dangerous\nor costly on-road testing of autonomous vehicles (AV). This paper addresses the\nchallenge of efficiently generating effective tests for simulation-based AV\nverification using software testing agents. The multi-agent system (MAS)\nprogramming paradigm offers rational agency, causality and strategic planning\nbetween multiple agents. We exploit these aspects for test generation, focusing\nin particular on the generation of tests that trigger the precondition of an\nassertion. On the example of a key assertion we show that, by encoding a\nvariety of different behaviours respondent to the agent's perceptions of the\ntest environment, the agency-directed approach generates twice as many\neffective tests than pseudo-random test generation, while being both efficient\nand robust. Moreover, agents can be encoded to behave naturally without\ncompromising the effectiveness of test generation. Our results suggest that\ngenerating tests using agency-directed testing significantly improves upon\nrandom and simultaneously provides more realistic driving scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:41:29 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 11:20:28 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 10:46:48 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Chance", "Greg", ""], ["Ghobrial", "Abanoub", ""], ["Lemaignan", "Severin", ""], ["Pipe", "Tony", ""], ["Eder", "Kerstin", ""]]}, {"id": "1912.05676", "submitter": "Tom Eccles", "authors": "Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, Thore\n  Graepel", "title": "Biases for Emergent Communication in Multi-agent Reinforcement Learning", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of emergent communication, in which language arises\nbecause speakers and listeners must communicate information in order to solve\ntasks. In temporally extended reinforcement learning domains, it has proved\nhard to learn such communication without centralized training of agents, due in\npart to a difficult joint exploration problem. We introduce inductive biases\nfor positive signalling and positive listening, which ease this problem. In a\nsimple one-step environment, we demonstrate how these biases ease the learning\nproblem. We also apply our methods to a more extended environment, showing that\nagents with these inductive biases achieve better performance, and analyse the\nresulting communication protocols.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:39:51 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Eccles", "Tom", ""], ["Bachrach", "Yoram", ""], ["Lever", "Guy", ""], ["Lazaridou", "Angeliki", ""], ["Graepel", "Thore", ""]]}, {"id": "1912.05748", "submitter": "Mehdi Dadvar", "authors": "Mehdi Dadvar, Saeed Moazami, Harley R. Myler, and Hassan Zargarzadeh", "title": "Multi-Agent Task Allocation in Complementary Teams: A Hunter and\n  Gatherer Approach", "comments": "15 pages, 12 figures", "journal-ref": "Complexity, vol. 2020, Article ID 1752571, 15 pages, 2020", "doi": "10.1155/2020/1752571", "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a dynamic task allocation problem, where tasks are unknowingly\ndistributed over an environment. This paper considers each task comprised of\ntwo sequential subtasks: detection and completion, where each subtask can only\nbe carried out by a certain type of agent. We address this problem using a\nnovel nature-inspired approach called \"hunter and gatherer\". The proposed\nmethod employs two complementary teams of agents: one agile in detecting\n(hunters) and another skillful in completing (gatherers) the tasks. To minimize\nthe collective cost of task accomplishments in a distributed manner, a\ngame-theoretic solution is introduced to couple agents from complementary\nteams. We utilize market-based negotiation models to develop incentive-based\ndecision-making algorithms relying on innovative notions of \"certainty and\nuncertainty profit margins\". The simulation results demonstrate that employing\ntwo complementary teams of hunters and gatherers can effectually improve the\nnumber of tasks completed by agents compared to conventional methods, while the\ncollective cost of accomplishments is minimized. In addition, the stability and\nefficacy of the proposed solutions are studied using Nash equilibrium analysis\nand statistical analysis respectively. It is also numerically shown that the\nproposed solutions function fairly, i.e. for each type of agent, the overall\nworkload is distributed equally.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 03:13:29 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 22:15:20 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dadvar", "Mehdi", ""], ["Moazami", "Saeed", ""], ["Myler", "Harley R.", ""], ["Zargarzadeh", "Hassan", ""]]}, {"id": "1912.06019", "submitter": "Yang Tang Prof.", "authors": "Kaile Chen, Wangli He, Yang Tang, and Wenle Zhang", "title": "Leader Selection in Multi-Agent Networks with Switching Topologies via\n  Submodular Optimization", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In leader-follower multi-agent networks with switching topologies, choosing a\nsubset of agents as leaders is a critical step to achieve desired performances.\nIn this paper, we concentrate on the problem of selecting a minimum-size set of\nleaders that ensure the tracking of a reference signal in a highorder linear\nmulti-agent network with a set of given topology dependent dwell time (TDDT).\nFirst, we derive a sufficient condition that guarantees the states of all\nagents converging to an expected state trajectory. Second, by exploiting\nsubmodular optimization method, we formulate the problem of identifying a\nminimal leader set which satisfies the proposed sufficient condition. Third, we\npresent an algorithm with the provable optimality bound to solve the formulated\nproblem. Finally, several numerical examples are provided to verify the\neffectiveness of the designed selection scheme.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:14:17 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chen", "Kaile", ""], ["He", "Wangli", ""], ["Tang", "Yang", ""], ["Zhang", "Wenle", ""]]}, {"id": "1912.06036", "submitter": "Pranay Sharma", "authors": "Pranay Sharma, Swatantra Kafle, Prashant Khanduri, Saikiran Bulusu,\n  Ketan Rajawat, and Pramod K. Varshney", "title": "Parallel Restarted SPIDER -- Communication Efficient Distributed\n  Nonconvex Optimization with Optimal Computation Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed algorithm for stochastic smooth,\nnon-convex optimization. We assume a worker-server architecture where $N$\nnodes, each having $n$ (potentially infinite) number of samples, collaborate\nwith the help of a central server to perform the optimization task. The global\nobjective is to minimize the average of local cost functions available at\nindividual nodes. The proposed approach is a non-trivial extension of the\npopular parallel-restarted SGD algorithm, incorporating the optimal\nvariance-reduction based SPIDER gradient estimator into it. We prove\nconvergence of our algorithm to a first-order stationary solution. The proposed\napproach achieves the best known communication complexity $O(\\epsilon^{-1})$\nalong with the optimal computation complexity. For finite-sum problems (finite\n$n$), we achieve the optimal computation (IFO) complexity\n$O(\\sqrt{Nn}\\epsilon^{-1})$. For online problems ($n$ unknown or infinite), we\nachieve the optimal IFO complexity $O(\\epsilon^{-3/2})$. In both the cases, we\nmaintain the linear speedup achieved by existing methods. This is a massive\nimprovement over the $O(\\epsilon^{-2})$ IFO complexity of the existing\napproaches. Additionally, our algorithm is general enough to allow\nnon-identical distributions of data across workers, as in the recently proposed\nfederated learning paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:36:22 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 06:03:32 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sharma", "Pranay", ""], ["Kafle", "Swatantra", ""], ["Khanduri", "Prashant", ""], ["Bulusu", "Saikiran", ""], ["Rajawat", "Ketan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1912.06085", "submitter": "Francesco De Lellis", "authors": "Francesco De Lellis, Fabrizia Auletta, Giovanni Russo, Piero De Lellis\n  and Mario di Bernardo", "title": "Control-Tutored Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a control-tutored reinforcement learning (CTRL) algorithm. The\nidea is to enhance tabular learning algorithms so as to improve the exploration\nof the state-space, and substantially reduce learning times by leveraging some\nlimited knowledge of the plant encoded into a tutoring model-based control\nstrategy. We illustrate the benefits of our novel approach and its\neffectiveness by using the problem of controlling one or more agents to herd\nand contain within a goal region a set of target free-roving agents in the\nplane.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:14:15 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["De Lellis", "Francesco", ""], ["Auletta", "Fabrizia", ""], ["Russo", "Giovanni", ""], ["De Lellis", "Piero", ""], ["di Bernardo", "Mario", ""]]}, {"id": "1912.06095", "submitter": "Qingbiao Li", "authors": "Qingbiao Li, Fernando Gama, Alejandro Ribeiro, Amanda Prorok", "title": "Graph Neural Networks for Decentralized Multi-Robot Path Planning", "comments": "This paper has been accepted in the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2020. For the simulation demo, see\n  this https URL \"https://youtu.be/AGDk2RozpMQ\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective communication is key to successful, decentralized, multi-robot path\nplanning. Yet, it is far from obvious what information is crucial to the task\nat hand, and how and when it must be shared among robots. To side-step these\nissues and move beyond hand-crafted heuristics, we propose a combined model\nthat automatically synthesizes local communication and decision-making policies\nfor robots navigating in constrained workspaces. Our architecture is composed\nof a convolutional neural network (CNN) that extracts adequate features from\nlocal observations, and a graph neural network (GNN) that communicates these\nfeatures among robots. We train the model to imitate an expert algorithm, and\nuse the resulting model online in decentralized planning involving only local\ncommunication and local observations. We evaluate our method in simulations {by\nnavigating teams of robots to their destinations in 2D} cluttered workspaces.\nWe measure the success rates and sum of costs over the planned paths. The\nresults show a performance close to that of our expert algorithm, demonstrating\nthe validity of our approach. In particular, we show our model's capability to\ngeneralize to previously unseen cases (involving larger environments and larger\nrobot teams).\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 17:48:14 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:04:20 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Qingbiao", ""], ["Gama", "Fernando", ""], ["Ribeiro", "Alejandro", ""], ["Prorok", "Amanda", ""]]}, {"id": "1912.06285", "submitter": "Zhihong Liu", "authors": "Zhihong Liu, Xiangke Wang, Lincheng Shen, Shulong Zhao, Yirui Cong,\n  Jie Li, Dong Yin, Shengde Jia, Xiaojia Xiang", "title": "Mission Oriented Miniature Fixed-wing UAV Swarms: A Multi-layered and\n  Distributed Architecture", "comments": null, "journal-ref": null, "doi": "10.1109/TSMC.2020.3033935", "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UAV swarms have triggered wide concern due to their potential application\nvalues in recent years. While there are studies proposed in terms of the\narchitecture design for UAV swarms, two main challenges still exist: (1)\nScalability, supporting a large scale of vehicles; (2) Versatility, integrating\ndiversified missions. To this end, a multi-layered and distributed architecture\nfor mission oriented miniature fixed-wing UAV swarms is presented in this\npaper. The proposed architecture is built on the concept of modularity. It\ndivides the overall system to five layers: low-level control, high-level\ncontrol, coordination, communication and human interaction layers, and many\nmodules that can be viewed as black boxes with interfaces of inputs and\noutputs. In this way, not only the complexity of developing a large system can\nbe reduced, but also the versatility of supporting diversified missions can be\nensured. Furthermore, the proposed architecture is fully distributed that each\nUAV performs the decision-making procedure autonomously so as to achieve better\nscalability. Moreover, different kinds of aerial platforms can be feasibly\nextended by using the control allocation matrices and the integrated hardware\nbox. A prototype swarm system based on the proposed architecture is built and\nthe proposed architecture is evaluated through field experiments with a scale\nof 21 fixed-wing UAVs. Particularly, to the best of our knowledge, this paper\nis the first work which successfully demonstrates formation flight, target\nrecognition and tracking missions within an integrated architecture for\nfixed-wing UAV swarms through field experiments.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:31:05 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liu", "Zhihong", ""], ["Wang", "Xiangke", ""], ["Shen", "Lincheng", ""], ["Zhao", "Shulong", ""], ["Cong", "Yirui", ""], ["Li", "Jie", ""], ["Yin", "Dong", ""], ["Jia", "Shengde", ""], ["Xiang", "Xiaojia", ""]]}, {"id": "1912.06511", "submitter": "Sergei Plotnikov", "authors": "Sergei A. Plotnikov and Alexander L. Fradkov", "title": "Desynchronization in Oscillatory Networks Based on Yakubovich\n  Oscillatority", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The desynchronization problems in oscillatory networks is considered. A new\ndesynchronization notion is introduced and desynchronization conditions are\nprovided. The desynchronization notion is formulated in terms of Yakubovich\noscillatority of the auxiliary synchronization error system. As an example, the\nnetwork of diffusively coupled FitzHugh-Nagumo systems with undirected graph is\nconsidered. The simple inequality guaranteeing network desynchronization is\nderived. The simulation results confirm the validity of the obtained analytical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:59:22 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Plotnikov", "Sergei A.", ""], ["Fradkov", "Alexander L.", ""]]}, {"id": "1912.06513", "submitter": "Charlotte Roman", "authors": "Charlotte Roman and Paolo Turrini", "title": "Reducing selfish routing inefficiencies using traffic lights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic congestion games abstract away from the costs of junctions in\ntransport networks, yet, in urban environments, these often impact journey\ntimes significantly. In this paper we equip congestion games with traffic\nlights, modelled as junction-based waiting cycles, therefore enabling more\nrealistic route planning strategies. Using the SUMO simulator, we show that our\nmodelling choices coincide with realistic routing behaviours, in particular,\nthat drivers' decisions about route choices are based on the proportion of red\nlight time for their direction of travel. Drawing upon the experimental\nresults, we show that the effects of the notorious Braess' paradox can be\navoided in theory and significantly reduced in practice, by allocating the\nappropriate traffic light cycles along a transport network.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 14:18:13 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Roman", "Charlotte", ""], ["Turrini", "Paolo", ""]]}, {"id": "1912.06860", "submitter": "George Vouros VOUROS GEORGE", "authors": "Theocharis Kravaris, Christos Spatharis, Alevizos Bastas, George A.\n  Vouros, Konstantinos Blekas, Gennady Andrienko, Natalia Andrienko, Jose\n  Manuel Cordero Garcia", "title": "Resolving Congestions in the Air Traffic Management Domain via\n  Multiagent Reinforcement Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, we report on the efficiency and effectiveness of multiagent\nreinforcement learning methods (MARL) for the computation of flight delays to\nresolve congestion problems in the Air Traffic Management (ATM) domain.\nSpecifically, we aim to resolve cases where demand of airspace use exceeds\ncapacity (demand-capacity problems), via imposing ground delays to flights at\nthe pre-tactical stage of operations (i.e. few days to few hours before\noperation). Casting this into the multiagent domain, agents, representing\nflights, need to decide on own delays w.r.t. own preferences, having no\ninformation about others' payoffs, preferences and constraints, while they plan\nto execute their trajectories jointly with others, adhering to operational\nconstraints. Specifically, we formalize the problem as a multiagent Markov\nDecision Process (MA-MDP) and we show that it can be considered as a Markov\ngame in which interacting agents need to reach an equilibrium: What makes the\nproblem more interesting is the dynamic setting in which agents operate, which\nis also due to the unforeseen, emergent effects of their decisions in the whole\nsystem. We propose collaborative multiagent reinforcement learning methods to\nresolve demand-capacity imbalances: Extensive experimental study on real-world\ncases, shows the potential of the proposed approaches in resolving problems,\nwhile advanced visualizations provide detailed views towards understanding the\nquality of solutions provided.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 15:06:35 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kravaris", "Theocharis", ""], ["Spatharis", "Christos", ""], ["Bastas", "Alevizos", ""], ["Vouros", "George A.", ""], ["Blekas", "Konstantinos", ""], ["Andrienko", "Gennady", ""], ["Andrienko", "Natalia", ""], ["Garcia", "Jose Manuel Cordero", ""]]}, {"id": "1912.06880", "submitter": "Wenhang Bao", "authors": "Wenhang Bao, Xiao-yang Liu", "title": "Spatial Influence-aware Reinforcement Learning for Intelligent\n  Transportation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent transportation systems (ITSs) are envisioned to be crucial for\nsmart cities, which aims at improving traffic flow to improve the life quality\nof urban residents and reducing congestion to improve the efficiency of\ncommuting. However, several challenges need to be resolved before such systems\ncan be deployed, for example, conventional solutions for Markov decision\nprocess (MDP) and single-agent Reinforcement Learning (RL) algorithms suffer\nfrom poor scalability, and multi-agent systems suffer from poor communication\nand coordination. In this paper, we explore the potential of mutual information\nsharing, or in other words, spatial influence based communication, to optimize\ntraffic light control policy. First, we mathematically analyze the\ntransportation system. We conclude that the transportation system does not have\nstationary Nash Equilibrium, thereby reinforcement learning algorithms offer\nsuitable solutions. Secondly, we describe how to build a multi-agent Deep\nDeterministic Policy Gradient (DDPG) system with spatial influence and social\ngroup utility incorporated. Then we utilize the grid topology road network to\nempirically demonstrate the scalability of the new system. We demonstrate three\ntypes of directed communications to show the effect of directions of social\ninfluence on the entire network utility and individual utility. Lastly, we\ndefine \"selfish index\" and analyze the effect of it on total group utility.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:57:56 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bao", "Wenhang", ""], ["Liu", "Xiao-yang", ""]]}, {"id": "1912.06909", "submitter": "Hadi Hosseini", "authors": "Yuki Tamura and Hadi Hosseini", "title": "The Crawler: Two Equivalence Results for Object (Re)allocation Problems\n  when Preferences Are Single-peaked", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For object reallocation problems, if preferences are strict but otherwise\nunrestricted, the Top Trading Cycle rule (TTC) is the leading rule: It is the\nonly rule satisfying efficiency, the endowment lower bound, and\nstrategy-proofness; moreover, TTC coincides with the core. However, on the\nsubdomain of single-peaked preferences, Bade (2019a) defines a new rule, the\n\"crawler\", which also satisfies the first three properties. Our first theorem\nstates that the crawler and a naturally defined \"dual\" rule are actually the\nsame. Next, for object allocation problems, we define a probabilistic version\nof the crawler by choosing an endowment profile at random according to a\nuniform distribution, and applying the original definition. Our second theorem\nstates that this rule is the same as the \"random priority rule\" which, as\nproved by Knuth (1996) and Abdulkadiroglu and S\\\"onmez (1998), is equivalent to\nthe \"core from random endowments\".\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 19:27:12 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Tamura", "Yuki", ""], ["Hosseini", "Hadi", ""]]}, {"id": "1912.07521", "submitter": "Mehdi Dadvar", "authors": "Mehdi Dadvar, Saeed Moazami, Harley R. Myler, and Hassan Zargarzadeh", "title": "Exploration and Coordination of Complementary Multi-Robot Teams in a\n  Hunter and Gatherer Scenario", "comments": "19 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1912.05748", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hunter and gatherer approach copes with the problem of dynamic\nmulti-robot task allocation, where tasks are unknowingly distributed over an\nenvironment. This approach employs two complementary teams of agents: one agile\nin exploring (hunters) and another dexterous in completing (gatherers) the\ntasks. Although this approach has been studied from the task planning point of\nview in our previous works, the multi-robot exploration and coordination\naspects of the problem remain uninvestigated. This paper proposes a multi-robot\nexploration algorithm for hunters based on innovative notions of \"expected\ninformation gain\" to minimize the collective cost of task accomplishments in a\ndistributed manner. Besides, we present a coordination solution between hunters\nand gatherers by integrating the novel notion of profit margins into the\nconcept of expected information gain. Statistical analysis of extensive\nsimulation results confirms the efficacy of the proposed algorithms compared in\ndifferent environments with varying levels of obstacles complexities. We also\ndemonstrate that the lack of effective coordination between hunters and\ngatherers significantly hurts the total effectiveness of the planning,\nespecially in environments containing dense obstacles and confined corridors.\nFinally, it is statistically proven that the overall workload is distributed\nequally for each type of agent which ensures that the proposed solution is not\nbiased to a particular agent and all agents behave analogously under similar\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 03:48:40 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 13:01:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Dadvar", "Mehdi", ""], ["Moazami", "Saeed", ""], ["Myler", "Harley R.", ""], ["Zargarzadeh", "Hassan", ""]]}, {"id": "1912.08066", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Marija Sakota, Aris Filos-Ratsikas, Boi Faltings", "title": "Putting Ridesharing to the Test: Efficient and Scalable Solutions and\n  the Power of Dynamic Vehicle Relocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a systematic evaluation of a diverse set of algorithms for the\nridesharing problem which is, to the best of our knowledge, one of the largest\nand most comprehensive to date. In particular, we evaluate 12 different\nalgorithms over 12 metrics related to global efficiency, complexity, passenger,\ndriver, and platform incentives. Our evaluation setting is specifically\ndesigned to resemble reality as closely as possible. We achieve this by (a)\nusing actual data from the NYC's yellow taxi trip records, both for modeling\ncustomer requests, and taxis (b) following closely the pricing model employed\nby ridesharing platforms and (c) running our simulations to the scale of the\nactual problem faced by the ridesharing platforms.\n  Our results provide a clear-cut recommendation to ridesharing platforms on\nwhich solutions can be employed in practice and demonstrate the large potential\nfor efficiency gains. Moreover, we show that simple, lightweight relocation\nschemes -- which can be used as independent components to any ridesharing\nalgorithm -- can significantly improve Quality of Service metrics by up to 50%.\nAs a highlight of our findings, we identify a scalable, on-device heuristic\nthat offers an efficient, end-to-end solution for the Dynamic Ridesharing and\nFleet Relocation problem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:10:03 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:34:19 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Sakota", "Marija", ""], ["Filos-Ratsikas", "Aris", ""], ["Faltings", "Boi", ""]]}, {"id": "1912.08243", "submitter": "Milad Siami", "authors": "Milad Siami, Amir Ajorlou, and Ali Jadbabaie", "title": "Competitive Contagion with Sparse Seeding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a strategic model of marketing and product diffusion in\nsocial networks. We consider two firms offering substitutable products which\ncan improve their market share by seeding the key individuals in the market.\nConsumers update their consumption level for each of the two products as the\nbest response to the consumption of their neighbors in the previous period.\nThis results in linear update dynamics for the product consumption. Each\nconsumer receives externality from the consumption of each neighbor where the\nstrength of the externality is higher for consumption of the products of the\nsame firm. We represent the above setting as a duopoly game between the firms\nand introduce a novel framework that allows for sparse seeding to emerge as an\nequilibrium strategy. We then study the effect of the network structure on the\noptimal seeding strategies and the extent to which the strategies can be\nsparsified. In particular, we derive conditions under which near Nash\nequilibrium strategies can asymptotically lead to sparse seeding in large\npopulations. The results are illustrated using a core-periphery network.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:25:56 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Siami", "Milad", ""], ["Ajorlou", "Amir", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1912.08446", "submitter": "Leonit Zeynalvand", "authors": "Leonit Zeynalvand, Tie Luo, Jie Zhang", "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment", "comments": "To be published in the Proceedings of AAAI, Feb 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust and reputation management (TRM) plays an increasingly important role in\nlarge-scale online environments such as multi-agent systems (MAS) and the\nInternet of Things (IoT). One main objective of TRM is to achieve accurate\ntrust assessment of entities such as agents or IoT service providers. However,\nthis encounters an accuracy-privacy dilemma as we identify in this paper, and\nwe propose a framework called Context-aware Bernoulli Neural Network based\nReputation Assessment (COBRA) to address this challenge. COBRA encapsulates\nagent interactions or transactions, which are prone to privacy leak, in machine\nlearning models, and aggregates multiple such models using a Bernoulli neural\nnetwork to predict a trust score for an agent. COBRA preserves agent privacy\nand retains interaction contexts via the machine learning models, and achieves\nmore accurate trust prediction than a fully-connected neural network\nalternative. COBRA is also robust to security attacks by agents who inject fake\nmachine learning models; notably, it is resistant to the 51-percent attack. The\nperformance of COBRA is validated by our experiments using a real dataset, and\nby our simulations, where we also show that COBRA outperforms other\nstate-of-the-art TRM systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:23:34 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 04:40:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zeynalvand", "Leonit", ""], ["Luo", "Tie", ""], ["Zhang", "Jie", ""]]}, {"id": "1912.08465", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Augusto Santos, Ali H. Sayed", "title": "Graph Learning Under Partial Observability", "comments": "to appear in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization, inference and learning tasks can be accomplished\nefficiently by means of decentralized processing algorithms where the network\ntopology (i.e., the graph) plays a critical role in enabling the interactions\namong neighboring nodes. There is a large body of literature examining the\neffect of the graph structure on the performance of decentralized processing\nstrategies. In this article, we examine the inverse problem and consider the\nreverse question: How much information does observing the behavior at the nodes\nof a graph convey about the underlying topology? For large-scale networks, the\ndifficulty in addressing such inverse problems is compounded by the fact that\nusually only a limited fraction of the nodes can be probed, giving rise to a\nsecond important question: Despite the presence of unobserved nodes, can\npartial observations still be sufficient to discover the graph linking the\nprobed nodes? The article surveys recent advances on this challenging learning\nproblem and related questions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:10:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:35:58 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 15:03:52 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Matta", "Vincenzo", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1912.08929", "submitter": "S\\^inziana-Maria Sebe", "authors": "S\\^inziana-Maria Sebe and J\\\"org P. M\\\"uller", "title": "PFaRA: a Platoon Forming and Routing Algorithm for Same-Day Deliveries", "comments": "Submitted to \"Communications in Computer and Information Science\"\n  published by Springer", "journal-ref": "Communications in Computer and Information Science, vol 1217\n  (2021) pages: 297--320", "doi": "10.1007/978-3-030-68028-2_14", "report-no": null, "categories": "cs.MA cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Platoons, vehicles that travel very close together acting as one, promise to\nimprove road usage on freeways and city roads alike. We study platoon formation\nin the context of same-day delivery in urban environments. Multiple\nself-interested logistic service providers (LSP) carry out same-day deliveries\nby deploying autonomous electric vehicles that are capable of forming and\ntraveling in platoons. The novel aspect that we consider in our research is\nheterogeneity of platoons in the sense that vehicles are equipped with\ndifferent capabilities and constraints, and belong to different providers. Our\naim is to examine how these platoons can form and their potential properties\nand benefits. We present a platoon forming and routing algorithm, called PFaRA,\nthat finds longest common routes for multiple vehicles, while also respecting\nvehicle preferences and constraints. PFaRA consists of two parts, a speed\nclustering step and a linear optimisation step. To test the approach, a\nsimulation was used, working with realistic urban network data and background\ntraffic models. Our results showed that the performance of our approach is\ncomparable to a simple route-matching one, but it leads to better utility\nvalues for vehicles and by extension the LSPs. We show that the grouping\nprovided is viable and provides benefits to all vehicles participating in the\nplatoon.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 14:15:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sebe", "S\u00eenziana-Maria", ""], ["M\u00fcller", "J\u00f6rg P.", ""]]}, {"id": "1912.08946", "submitter": "Francisco C. Santos", "authors": "Luis Moniz Pereira and Francisco C. Santos", "title": "Counterfactual thinking in cooperation dynamics", "comments": "18 pages", "journal-ref": "in: Model-based reason-ing in science and technology - Inferential\n  models for Logic, Language, Cognition and Computation, Collection on Studies\n  in Applied Philosophy, Epistemology and Rational Ethics (SAPERE), Springer,\n  2019", "doi": "10.1007/978-3-030-32722-4_5", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Thinking is a human cognitive ability studied in a wide\nvariety of domains. It captures the process of reasoning about a past event\nthat did not occur, namely what would have happened had this event occurred,\nor, otherwise, to reason about an event that did occur but what would ensue had\nit not. Given the wide cognitive empowerment of counterfactual reasoning in the\nhuman individual, the question arises of how the presence of individuals with\nthis capability may improve cooperation in populations of self-regarding\nindividuals. Here we propose a mathematical model, grounded on Evolutionary\nGame Theory, to examine the population dynamics emerging from the interplay\nbetween counterfactual thinking and social learning (i.e., individuals that\nlearn from the actions and success of others) whenever the individuals in the\npopulation face a collective dilemma. Our results suggest that counterfactual\nreasoning fosters coordination in collective action problems occurring in large\npopulations, and has a limited impact on cooperation dilemmas in which\ncoordination is not required. Moreover, we show that a small prevalence of\nindividuals resorting to counterfactual thinking is enough to nudge an entire\npopulation towards highly cooperative standards.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:38:34 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Pereira", "Luis Moniz", ""], ["Santos", "Francisco C.", ""]]}, {"id": "1912.09864", "submitter": "Grzegorz Lisowski", "authors": "Dmitry Chistikov, Grzegorz Lisowski, Mike Paterson, Paolo Turrini", "title": "Convergence of Opinion Diffusion is PSPACE-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse opinion diffusion in social networks, where a finite set of\nindividuals is connected in a directed graph and each simultaneously changes\ntheir opinion to that of the majority of their influencers. We study the\nalgorithmic properties of the fixed-point behaviour of such networks, showing\nthat the problem of establishing whether individuals converge to stable\nopinions is PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:05:46 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 05:17:14 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chistikov", "Dmitry", ""], ["Lisowski", "Grzegorz", ""], ["Paterson", "Mike", ""], ["Turrini", "Paolo", ""]]}, {"id": "1912.10944", "submitter": "Kun Shao", "authors": "Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, Dongbin Zhao", "title": "A Survey of Deep Reinforcement Learning in Video Games", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) has made great achievements since proposed.\nGenerally, DRL agents receive high-dimensional inputs at each step, and make\nactions according to deep-neural-network-based policies. This learning\nmechanism updates the policy to maximize the return with an end-to-end method.\nIn this paper, we survey the progress of DRL methods, including value-based,\npolicy gradient, and model-based algorithms, and compare their main techniques\nand properties. Besides, DRL plays an important role in game artificial\nintelligence (AI). We also take a review of the achievements of DRL in various\nvideo games, including classical Arcade games, first-person perspective games\nand multi-agent real-time strategy games, from 2D to 3D, and from single-agent\nto multi-agent. A large number of video game AIs with DRL have achieved\nsuper-human performance, while there are still some challenges in this domain.\nTherefore, we also discuss some key points when applying DRL methods to this\nfield, including exploration-exploitation, sample efficiency, generalization\nand transfer, multi-agent learning, imperfect information, and delayed spare\nrewards, as well as some research directions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 16:04:40 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 14:47:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Shao", "Kun", ""], ["Tang", "Zhentao", ""], ["Zhu", "Yuanheng", ""], ["Li", "Nannan", ""], ["Zhao", "Dongbin", ""]]}, {"id": "1912.11495", "submitter": "Huile Xu", "authors": "Huile Xu, Yi Zhang, Christos G. Cassandras, Li Li, Shuo Feng", "title": "A Bi-Level Cooperative Driving Strategy Allowing Lane Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the cooperative driving of connected and automated\nvehicles (CAVs) at conflict areas (e.g., non-signalized intersections and\nramping regions). Due to safety concerns, most existing studies prohibit lane\nchange since this may cause lateral collisions when coordination is not\nappropriately performed. However, in many traffic scenarios (e.g., work zones),\nvehicles must change lanes. To solve this problem, we categorize the potential\ncollision into two kinds and thus establish a bi-level planning problem. The\nright-of-way of vehicles for the critical conflict zone is considered in the\nupper-level, and the right-of-way of vehicles during lane changes is then\nresolved in the lower-level. The solutions of the upper-level problem are\nrepresented in tree space, and a near-optimal solution is searched for by\ncombining Monte Carlo Tree Search (MCTS) with some heuristic rules within a\nvery short planning time. The proposed strategy is suitable for not only the\nshortest delay objective but also other objectives (e.g., energy-saving and\npassenger comfort). Numerical examples show that the proposed strategy leads to\ngood traffic performance in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 19:12:21 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Xu", "Huile", ""], ["Zhang", "Yi", ""], ["Cassandras", "Christos G.", ""], ["Li", "Li", ""], ["Feng", "Shuo", ""]]}, {"id": "1912.12147", "submitter": "Eduardo Arnold", "authors": "Eduardo Arnold, Mehrdad Dianati, Robert de Temple, Saber Fallah", "title": "Cooperative Perception for 3D Object Detection in Driving Scenarios\n  using Infrastructure Sensors", "comments": "13 pages, 4 tables, 7 figures. Published in IEEE Transactions on\n  Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.3028424", "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is a common function within the perception system of an\nautonomous vehicle and outputs a list of 3D bounding boxes around objects of\ninterest. Various 3D object detection methods have relied on fusion of\ndifferent sensor modalities to overcome limitations of individual sensors.\nHowever, occlusion, limited field-of-view and low-point density of the sensor\ndata cannot be reliably and cost-effectively addressed by multi-modal sensing\nfrom a single point of view. Alternatively, cooperative perception incorporates\ninformation from spatially diverse sensors distributed around the environment\nas a way to mitigate these limitations. This article proposes two schemes for\ncooperative 3D object detection using single modality sensors. The early fusion\nscheme combines point clouds from multiple spatially diverse sensing points of\nview before detection. In contrast, the late fusion scheme fuses the\nindependently detected bounding boxes from multiple spatially diverse sensors.\nWe evaluate the performance of both schemes, and their hybrid combination,\nusing a synthetic cooperative dataset created in two complex driving scenarios,\na T-junction and a roundabout. The evaluation shows that the early fusion\napproach outperforms late fusion by a significant margin at the cost of higher\ncommunication bandwidth. The results demonstrate that cooperative perception\ncan recall more than 95% of the objects as opposed to 30% for single-point\nsensing in the most challenging scenario. To provide practical insights into\nthe deployment of such system, we report how the number of sensors and their\nconfiguration impact the detection performance of the system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:19:27 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:59:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Arnold", "Eduardo", ""], ["Dianati", "Mehrdad", ""], ["de Temple", "Robert", ""], ["Fallah", "Saber", ""]]}, {"id": "1912.12671", "submitter": "Mart\\'i S\\'anchez-Fibla", "authors": "Marco Jerome Gasparrini, Ricard Sol\\'e, Mart\\'i S\\'anchez-Fibla", "title": "Individual specialization in multi-task environments with multiagent\n  reinforcement learners", "comments": "5 pages, 2 figures, paper appeared in CCIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing interest in Multi-Agent Reinforcement Learning (MARL) as\nthe first steps towards building general intelligent agents that learn to make\nlow and high-level decisions in non-stationary complex environments in the\npresence of other agents. Previous results point us towards increased\nconditions for coordination, efficiency/fairness, and common-pool resource\nsharing. We further study coordination in multi-task environments where several\nrewarding tasks can be performed and thus agents don't necessarily need to\nperform well in all tasks, but under certain conditions may specialize. An\nobservation derived from the study is that epsilon greedy exploration of\nvalue-based reinforcement learning methods is not adequate for multi-agent\nindependent learners because the epsilon parameter that controls the\nprobability of selecting a random action synchronizes the agents artificially\nand forces them to have deterministic policies at the same time. By using\npolicy-based methods with independent entropy regularised exploration updates,\nwe achieved a better and smoother convergence. Another result that needs to be\nfurther investigated is that with an increased number of agents specialization\ntends to be more probable.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 15:20:24 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gasparrini", "Marco Jerome", ""], ["Sol\u00e9", "Ricard", ""], ["S\u00e1nchez-Fibla", "Mart\u00ed", ""]]}, {"id": "1912.13107", "submitter": "Jennifer Hobbs", "authors": "Jennifer Hobbs, Matthew Holbrook, Nathan Frank, Long Sha, Patrick\n  Lucey", "title": "Improved Structural Discovery and Representation Learning of Multi-Agent\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to all machine learning algorithms is data representation. For\nmulti-agent systems, selecting a representation which adequately captures the\ninteractions among agents is challenging due to the latent group structure\nwhich tends to vary depending on context. However, in multi-agent systems with\nstrong group structure, we can simultaneously learn this structure and map a\nset of agents to a consistently ordered representation for further learning. In\nthis paper, we present a dynamic alignment method which provides a robust\nordering of structured multi-agent data enabling representation learning to\noccur in a fraction of the time of previous methods. We demonstrate the value\nof this approach using a large amount of soccer tracking data from a\nprofessional league.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 22:49:55 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Hobbs", "Jennifer", ""], ["Holbrook", "Matthew", ""], ["Frank", "Nathan", ""], ["Sha", "Long", ""], ["Lucey", "Patrick", ""]]}, {"id": "1912.13122", "submitter": "Andres Garcia-Camino", "authors": "Andr\\'es Garc\\'ia-Camino", "title": "Declarative Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report-no: 01", "categories": "cs.AI cs.LG cs.LO cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulation of Multi-Agent Systems (MAS) and Declarative Electronic\nInstitutions (DEIs) was a multidisciplinary research topic of the past decade\ninvolving (Physical and Software) Agents and Law since the beginning, but\nrecently evolved towards News-claimed Robot Lawyer since 2016. One of these\nfirst proposals of restricting the behaviour of Software Agentswas Electronic\nInstitutions.However, with the recent reformulation of Artificial Neural\nNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal\nissues regarding the use of DL has raised concerns in the Artificial\nIntelligence (AI) Community. Now that the Regulation of MAS is almost correctly\naddressed, we propose the Regulation of Artificial Neural Networks as\nAgent-based Training of a special type of regulated Artificial Neural Network\nthat we call Institutional Neural Network (INN).The main purpose of this paper\nis to bring attention to Artificial Teaching (AT) and to give a tentative\nanswer showing a proof-of-concept implementation of Regulated Deep Learning\n(RDL). This paper introduces the former concept and provide sI, a language\npreviously used to model declaratively and extend Electronic Institutions, as a\nmeans to regulate the execution of Artificial Neural Networks and their\ninteractions with Artificial Teachers (ATs)\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 00:10:50 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 22:36:52 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 17:19:26 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Garc\u00eda-Camino", "Andr\u00e9s", ""]]}]