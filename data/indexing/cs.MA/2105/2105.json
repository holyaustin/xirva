[{"id": "2105.00090", "submitter": "Rafhael Cunha Mr.", "authors": "Rafhael R. Cunha, Jomi Fred H\\\"ubner, Maiquel de Brito", "title": "Coupling purposes with status-functions in artificial institutions", "comments": "International Workshop on Coordination, Organizations, Institutions,\n  Norms and Ethics for Governance of Multi-Agent Systems (COINE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-agent systems, the agents may have goals that depend on a social,\nshared interpretation about the facts occurring in the system. These are the\nso-called social goals. Artificial institutions provide such a social\ninterpretation by assigning statuses to the concrete elements that compose the\nsystem. These statuses are supposed to enable the assignee element to perform\nfunctions that are not exclusively inherent to their design features. However,\nthe enabled functions are not explicit in the existing models of artificial\ninstitutions. As a consequence, (i) agents may have difficulties to reasoning\nabout how to achieve their own social goals with the help of artificial\ninstitutions and (ii) these institutions are not well instrumented to receive\nincoming agents, in the case of open systems. Considering those problems, this\npaper proposes a model to express the functions -- or the purposes --\nassociated with the status-functions helping the agents to reason about their\nsocial goals and the institution. We evaluate the model by using it in some\nscenarios, showing how the agents can use purposes to reason about the\nsatisfaction of their social goals in institutional contexts and how the\ninstitution can be flexible enough to support new agents operating in the\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:10:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cunha", "Rafhael R.", ""], ["H\u00fcbner", "Jomi Fred", ""], ["de Brito", "Maiquel", ""]]}, {"id": "2105.00124", "submitter": "Maha Riad", "authors": "Maha Riad and Fatemeh Golpayegani", "title": "Run-time Norms Synthesis in Multi-Objective Multi-Agent Systems", "comments": "15 pages, 5 figures, COINE, AAMAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Norms represent behavioural aspects that are encouraged by a social group of\nagents or the majority of agents in a system. Normative systems enable\ncoordinating synthesised norms of heterogeneous agents in complex multi-agent\nsystems autonomously. In real applications, agents have multiple objectives\nthat may contradict each other or contradict the synthesised norms. Therefore,\nagents need a mechanism to understand the impact of a suggested norm on their\nobjectives and decide whether or not to adopt it. To address these challenges,\na utility based norm synthesis (UNS) model is proposed which allows the agents\nto coordinate their behaviour while achieving their conflicting objectives. UNS\nproposes a utility-based case-based reasoning technique, using case-based\nreasoning for run-time norm synthesising in a centralised approach, and a\nutility function derived from the objectives of the system and its operating\nagents to decide whether or not to adopt a norm. The model is evaluated using a\ntwo intersecting roads scenario and the results show its efficacy to optimise\nmultiple objectives while adopting synthesised norms.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:04:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Riad", "Maha", ""], ["Golpayegani", "Fatemeh", ""]]}, {"id": "2105.00200", "submitter": "Nicoletta Fornara Mrs", "authors": "Nicoletta Fornara, Soheil Roshankish, Marco Colombetti", "title": "A Framework for Automatic Monitoring of Norms that regulate Time\n  Constrained Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of proposing a model of norms and a\nframework for automatically computing their violation or fulfilment. The\nproposed T-NORM model can be used to express abstract norms able to regulate\nclasses of actions that should or should not be performed in a temporal\ninterval. We show how the model can be used to formalize obligations and\nprohibitions and for inhibiting them by introducing permissions and exemptions.\nThe basic building blocks for norm specification consists of rules with\nsuitably nested components. The activation condition, the regulated actions,\nand the temporal constrains of norms are specified using the W3C Web Ontology\nLanguage (OWL 2). Thanks to this choice, it is possible to use OWL reasoning\nfor computing the effects that the logical implication between actions has on\nnorms fulfilment or violation. The operational semantics of the T-NORM model is\nspecified by providing an unambiguous procedure for translating every norm and\nevery exception into production rules.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 09:29:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fornara", "Nicoletta", ""], ["Roshankish", "Soheil", ""], ["Colombetti", "Marco", ""]]}, {"id": "2105.00216", "submitter": "Davide Grossi", "authors": "Davide Grossi", "title": "Lecture Notes on Voting Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA econ.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  These lecture notes have been developed for the course Computational Social\nChoice of the Artificial Intelligence MSc programme at the University of\nGroningen. They cover mathematical and algorithmic aspects of voting theory.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:19:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Grossi", "Davide", ""]]}, {"id": "2105.00376", "submitter": "Lijun Sun Mr", "authors": "Jiawei Wang and Lijun Sun", "title": "Reducing Bus Bunching with Asynchronous Multi-Agent Reinforcement\n  Learning", "comments": "IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bus system is a critical component of sustainable urban transportation.\nHowever, due to the significant uncertainties in passenger demand and traffic\nconditions, bus operation is unstable in nature and bus bunching has become a\ncommon phenomenon that undermines the reliability and efficiency of bus\nservices. Despite recent advances in multi-agent reinforcement learning (MARL)\non traffic control, little research has focused on bus fleet control due to the\ntricky asynchronous characteristic -- control actions only happen when a bus\narrives at a bus stop and thus agents do not act simultaneously. In this study,\nwe formulate route-level bus fleet control as an asynchronous multi-agent\nreinforcement learning (ASMR) problem and extend the classical actor-critic\narchitecture to handle the asynchronous issue. Specifically, we design a novel\ncritic network to effectively approximate the marginal contribution for other\nagents, in which graph attention neural network is used to conduct inductive\nlearning for policy evaluation. The critic structure also helps the ego agent\noptimize its policy more efficiently. We evaluate the proposed framework on\nreal-world bus services and actual passenger demand derived from smart card\ndata. Our results show that the proposed model outperforms both traditional\nheadway-based control methods and existing MARL methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:08:07 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 13:06:07 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Jiawei", ""], ["Sun", "Lijun", ""]]}, {"id": "2105.00451", "submitter": "Luca Capezzuto", "authors": "Luca Capezzuto, Danesh Tarapore, Sarvapali D. Ramchurn", "title": "Multi-Agent Routing and Scheduling Through Coalition Formation", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In task allocation for real-time domains, such as disaster response, a\nlimited number of agents is deployed across a large area to carry out numerous\ntasks, each with its prerequisites, profit, time window and workload. To\nmaximize profits while minimizing time penalties, agents need to cooperate by\nforming, disbanding and reforming coalitions. In this paper, we name this\nproblem Multi-Agent Routing and Scheduling through Coalition formation (MARSC)\nand show that it generalizes the important Team Orienteering Problem with Time\nWindows. We propose a binary integer program and an anytime and scalable\nheuristic to solve it. Using public London Fire Brigade records, we create a\ndataset with 347588 tasks and a test framework that simulates the mobilization\nof firefighters. In problems with up to 150 agents and 3000 tasks, our\nheuristic finds solutions up to 3.25 times better than the Earliest Deadline\nFirst approach commonly used in real-time systems. Our results constitute the\nfirst large-scale benchmark for the MARSC problem.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 11:53:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Capezzuto", "Luca", ""], ["Tarapore", "Danesh", ""], ["Ramchurn", "Sarvapali D.", ""]]}, {"id": "2105.00499", "submitter": "Mahdi Rezaei", "authors": "Saeed Tafazzol, Erfan Fathi, Mahdi Rezaei, Ehsan Asali", "title": "Curious Exploration and Return-based Memory Restoration for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reward engineering and designing an incentive reward function are non-trivial\ntasks to train agents in complex environments. Furthermore, an inaccurate\nreward function may lead to a biased behaviour which is far from an efficient\nand optimised behaviour. In this paper, we focus on training a single agent to\nscore goals with binary success/failure reward function in Half Field Offense\ndomain. As the major advantage of this research, the agent has no presumption\nabout the environment which means it only follows the original formulation of\nreinforcement learning agents. The main challenge of using such a reward\nfunction is the high sparsity of positive reward signals. To address this\nproblem, we use a simple prediction-based exploration strategy (called Curious\nExploration) along with a Return-based Memory Restoration (RMR) technique which\ntends to remember more valuable memories. The proposed method can be utilized\nto train agents in environments with fairly complex state and action spaces.\nOur experimental results show that many recent solutions including our baseline\nmethod fail to learn and perform in complex soccer domain. However, the\nproposed method can converge easily to the nearly optimal behaviour. The video\npresenting the performance of our trained agent is available at\nhttp://bit.ly/HFO_Binary_Reward.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:01:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Tafazzol", "Saeed", ""], ["Fathi", "Erfan", ""], ["Rezaei", "Mahdi", ""], ["Asali", "Ehsan", ""]]}, {"id": "2105.00505", "submitter": "Sixie Yu", "authors": "Sixie Yu, David Kempe, Yevgeniy Vorobeychik", "title": "Altruism Design in Networked Public Goods Games", "comments": "To appear in IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many collective decision-making settings feature a strategic tension between\nagents acting out of individual self-interest and promoting a common good.\nThese include wearing face masks during a pandemic, voting, and vaccination.\nNetworked public goods games capture this tension, with networks encoding\nstrategic interdependence among agents. Conventional models of public goods\ngames posit solely individual self-interest as a motivation, even though\naltruistic motivations have long been known to play a significant role in\nagents' decisions. We introduce a novel extension of public goods games to\naccount for altruistic motivations by adding a term in the utility function\nthat incorporates the perceived benefits an agent obtains from the welfare of\nothers, mediated by an altruism graph. Most importantly, we view altruism not\nas immutable, but rather as a lever for promoting the common good. Our central\nalgorithmic question then revolves around the computational complexity of\nmodifying the altruism network to achieve desired public goods game investment\nprofiles. We first show that the problem can be solved using linear programming\nwhen a principal can fractionally modify the altruism network. While the\nproblem becomes in general intractable if the principal's actions are\nall-or-nothing, we exhibit several tractable special cases.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:35:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yu", "Sixie", ""], ["Kempe", "David", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2105.00691", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann, Philipp Ebel, Matthias Soellner, Jan Marco\n  Leimeister", "title": "Hybrid Intelligence", "comments": null, "journal-ref": null, "doi": "10.1007/s12599-019-00595-2", "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research has a long history of discussing what is superior in predicting\ncertain outcomes: statistical methods or the human brain. This debate has\nrepeatedly been sparked off by the remarkable technological advances in the\nfield of artificial intelligence (AI), such as solving tasks like object and\nspeech recognition, achieving significant improvements in accuracy through\ndeep-learning algorithms (Goodfellow et al. 2016), or combining various methods\nof computational intelligence, such as fuzzy logic, genetic algorithms, and\ncase-based reasoning (Medsker 2012). One of the implicit promises that underlie\nthese advancements is that machines will 1 day be capable of performing complex\ntasks or may even supersede humans in performing these tasks. This triggers new\nheated debates of when machines will ultimately replace humans (McAfee and\nBrynjolfsson 2017). While previous research has proved that AI performs well in\nsome clearly defined tasks such as playing chess, playing Go or identifying\nobjects on images, it is doubted that the development of an artificial general\nintelligence (AGI) which is able to solve multiple tasks at the same time can\nbe achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the\nuse of AI to solve complex business problems in organizational contexts occurs\nscarcely, and applications for AI that solve complex problems remain mainly in\nlaboratory settings instead of being implemented in practice. Since the road to\nAGI is still a long one, we argue that the most likely paradigm for the\ndivision of labor between humans and machines in the next decades is Hybrid\nIntelligence. This concept aims at using the complementary strengths of human\nintelligence and AI, so that they can perform better than each of the two could\nseparately (e.g., Kamar 2016).\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:56:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dellermann", "Dominik", ""], ["Ebel", "Philipp", ""], ["Soellner", "Matthias", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.00767", "submitter": "Xiong Wang", "authors": "Xiong Wang, Riheng Jia", "title": "Mean Field Equilibrium in Multi-Armed Bandit Game with Continuous Reward", "comments": "IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field game facilitates analyzing multi-armed bandit (MAB) for a large\nnumber of agents by approximating their interactions with an average effect.\nExisting mean field models for multi-agent MAB mostly assume a binary reward\nfunction, which leads to tractable analysis but is usually not applicable in\npractical scenarios. In this paper, we study the mean field bandit game with a\ncontinuous reward function. Specifically, we focus on deriving the existence\nand uniqueness of mean field equilibrium (MFE), thereby guaranteeing the\nasymptotic stability of the multi-agent system. To accommodate the continuous\nreward function, we encode the learned reward into an agent state, which is in\nturn mapped to its stochastic arm playing policy and updated using realized\nobservations. We show that the state evolution is upper semi-continuous, based\non which the existence of MFE is obtained. As the Markov analysis is mainly for\nthe case of discrete state, we transform the stochastic continuous state\nevolution into a deterministic ordinary differential equation (ODE). On this\nbasis, we can characterize a contraction mapping for the ODE to ensure a unique\nMFE for the bandit game. Extensive evaluations validate our MFE\ncharacterization, and exhibit tight empirical regret of the MAB problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 11:50:06 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 12:37:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Xiong", ""], ["Jia", "Riheng", ""]]}, {"id": "2105.00872", "submitter": "Shuo Wan", "authors": "Shuo Wan, Jiaxun Lu, Pingyi Fan, Yunfeng Shao, Chenghui Peng and\n  Khaled B. letaief", "title": "Convergence Analysis and System Design for Federated Learning over\n  Wireless Networks", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as an important and promising\nlearning scheme in IoT, enabling devices to jointly learn a model without\nsharing their raw data sets. However, as the training data in FL is not\ncollected and stored centrally, FL training requires frequent model exchange,\nwhich is largely affected by the wireless communication network. Therein,\nlimited bandwidth and random package loss restrict interactions in training.\nMeanwhile, the insufficient message synchronization among distributed clients\ncould also affect FL convergence. In this paper, we analyze the convergence\nrate of FL training considering the joint impact of communication network and\ntraining settings. Further by considering the training costs in terms of time\nand power, the optimal scheduling problems for communication networks are\nformulated. The developed theoretical results can be used to assist the system\nparameter selections and explain the principle of how the wireless\ncommunication system could influence the distributed training process and\nnetwork scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:33:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wan", "Shuo", ""], ["Lu", "Jiaxun", ""], ["Fan", "Pingyi", ""], ["Shao", "Yunfeng", ""], ["Peng", "Chenghui", ""], ["letaief", "Khaled B.", ""]]}, {"id": "2105.00931", "submitter": "Unnat Jain", "authors": "Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik, Aniruddha Kembhavi, Luca\n  Weihs, Alexander Schwing", "title": "GridToPix: Training Embodied Agents with Minimal Supervision", "comments": "Project page: https://unnat.github.io/gridtopix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep reinforcement learning (RL) promises freedom from hand-labeled\ndata, great successes, especially for Embodied AI, require significant work to\ncreate supervision via carefully shaped rewards. Indeed, without shaped\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\ndegrade significantly across Embodied AI problems from single-agent\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\nAs training from shaped rewards doesn't scale to more realistic tasks, the\ncommunity needs to improve the success of training with terminal rewards. For\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\nthat generically mirror Embodied AI environments, i.e., they are independent of\nthe task; 2) distill the learned policy into agents that reside in complex\nvisual worlds. Despite learning from only terminal rewards with identical\nmodels and RL algorithms, GridToPix significantly improves results across\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\nMoving (success improves from 1% to 25%) to football gameplay (game score\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\nshaped reward training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:59:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jain", "Unnat", ""], ["Liu", "Iou-Jen", ""], ["Lazebnik", "Svetlana", ""], ["Kembhavi", "Aniruddha", ""], ["Weihs", "Luca", ""], ["Schwing", "Alexander", ""]]}, {"id": "2105.01129", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu, Robin Cohen, Olga Vechtomova", "title": "Towards A Multi-agent System for Online Hate Speech Detection", "comments": "Accepted to the 2nd International Workshop on Autonomous Agents for\n  Social Good (AASG), AAMAS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper envisions a multi-agent system for detecting the presence of hate\nspeech in online social media platforms such as Twitter and Facebook. We\nintroduce a novel framework employing deep learning techniques to coordinate\nthe channels of textual and im-age processing. Our experimental results aim to\ndemonstrate the effectiveness of our methods for classifying online content,\ntraining the proposed neural network model to effectively detect hateful\ninstances in the input. We conclude with a discussion of how our system may be\nof use to provide recommendations to users who are managing online social\nnetworks, showcasing the immense potential of intelligent multi-agent systems\ntowards delivering social good.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 19:06:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sahu", "Gaurav", ""], ["Cohen", "Robin", ""], ["Vechtomova", "Olga", ""]]}, {"id": "2105.01225", "submitter": "Murat Cubuktepe", "authors": "Murat Cubuktepe, Franti\\v{s}ek Blahoudek, and Ufuk Topcu", "title": "Polynomial-Time Algorithms for Multi-Agent Minimal-Capacity Planning", "comments": "Submitted to TCNS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the resource capacity of autonomous agents\ncooperating to achieve a shared task. More specifically, we consider high-level\nplanning for a team of homogeneous agents that operate under resource\nconstraints in stochastic environments and share a common goal: given a set of\ntarget locations, ensure that each location will be visited infinitely often by\nsome agent almost surely. We formalize the dynamics of agents by consumption\nMarkov decision processes. In a consumption Markov decision process, the agent\nhas a resource of limited capacity. Each action of the agent may consume some\namount of the resource. To avoid exhaustion, the agent can replenish its\nresource to full capacity in designated reload states. The resource capacity\nrestricts the capabilities of the agent. The objective is to assign target\nlocations to agents, and each agent is only responsible for visiting the\nassigned subset of target locations repeatedly. Moreover, the assignment must\nensure that the agents can carry out their tasks with minimal resource\ncapacity. We reduce the problem of finding target assignments for a team of\nagents with the lowest possible capacity to an equivalent graph-theoretical\nproblem. We develop an algorithm that solves this graph problem in time that is\n\\emph{polynomial} in the number of agents, target locations, and size of the\nconsumption Markov decision process. We demonstrate the applicability and\nscalability of the algorithm in a scenario where hundreds of unmanned\nunderwater vehicles monitor hundreds of locations in environments with\nstochastic ocean currents.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 00:30:02 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Cubuktepe", "Murat", ""], ["Blahoudek", "Franti\u0161ek", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2105.01372", "submitter": "Mattia Bianchi", "authors": "Mattia Bianchi, Wicak Ananduta, Sergio Grammatico", "title": "The distributed dual ascent algorithm is robust to asynchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed dual ascent is an established algorithm to solve strongly\nconvex multi-agent optimization problems with separable cost functions, in the\npresence of coupling constraints. In this paper, we study its asynchronous\ncounterpart. Specifically, we assume that each agent only relies on the\noutdated information received from some neighbors. Differently from the\nexisting randomized and dual block-coordinate schemes, we show convergence\nunder heterogeneous delays, communication and update frequencies. Consequently,\nour asynchronous dual ascent algorithm can be implemented without requiring any\ncoordination between the agents.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:01:49 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bianchi", "Mattia", ""], ["Ananduta", "Wicak", ""], ["Grammatico", "Sergio", ""]]}, {"id": "2105.01820", "submitter": "Qi Dai", "authors": "Qi Dai, Di Shen, Jinhong Wang, Suzhou Huang and Dimitar Filev", "title": "Calibration of Human Driving Behavior and Preference Using Naturalistic\n  Traffic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding human driving behaviors quantitatively is critical even in the\nera when connected and autonomous vehicles and smart infrastructure are\nbecoming ever more prevalent. This is particularly so as that mixed traffic\nsettings, where autonomous vehicles and human driven vehicles co-exist, are\nexpected to persist for quite some time. Towards this end it is necessary that\nwe have a comprehensive modeling framework for decision-making within which\nhuman driving preferences can be inferred statistically from observed driving\nbehaviors in realistic and naturalistic traffic settings. Leveraging a recently\nproposed computational framework for smart vehicles in a smart world using\nmulti-agent based simulation and optimization, we first recapitulate how the\nforward problem of driving decision-making is modeled as a state space model.\nWe then show how the model can be inverted to estimate driver preferences from\nnaturalistic traffic data using the standard Kalman filter technique. We\nexplicitly illustrate our approach using the vehicle trajectory data from\nSugiyama experiment that was originally meant to demonstrate how stop-and-go\nshockwave can arise spontaneously without bottlenecks. Not only the estimated\nstate filter can fit the observed data well for each individual vehicle, the\ninferred utility functions can also re-produce quantitatively similar pattern\nof the observed collective behaviors. One distinct advantage of our approach is\nthe drastically reduced computational burden. This is possible because our\nforward model treats driving decision process, which is intrinsically dynamic\nwith multi-agent interactions, as a sequence of independent static optimization\nproblems contingent on the state with a finite look ahead anticipation.\nConsequently we can practically sidestep solving an interacting dynamic\ninversion problem that would have been much more computationally demanding.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 01:20:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dai", "Qi", ""], ["Shen", "Di", ""], ["Wang", "Jinhong", ""], ["Huang", "Suzhou", ""], ["Filev", "Dimitar", ""]]}, {"id": "2105.01889", "submitter": "Tianhao Wu", "authors": "Tianhao Wu, Mingzhi Jiang, Yinhui Han, Zheng Yuan, Lin Zhang", "title": "Density-Aware Federated Imitation Learning for Connected and Automated\n  Vehicles with Unsignalized Intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent Transportation System (ITS) has become one of the essential\ncomponents in Industry 4.0. As one of the critical indicators of ITS,\nefficiency has attracted wide attention from researchers. However, the next\ngeneration of urban traffic carried by multiple transport service providers may\nprohibit the raw data interaction among multiple regions for privacy reasons,\neasily ignored in the existing research. This paper puts forward a federated\nlearning-based vehicle control framework to solve the above problem, including\ninteractors, trainers, and an aggregator. In addition, the density-aware model\naggregation method is utilized in this framework to improve vehicle control.\nWhat is more, to promote the performance of the end-to-end learning algorithm\nin the safety aspect, this paper proposes an imitation learning algorithm,\nwhich can obtain collision avoidance capabilities from a set of collision\navoidance rules. Furthermore, a loss-aware experience selection strategy is\nalso explored, reducing the communication overhead between the interactors and\nthe trainers via extra computing. Finally, the experiment results demonstrate\nthat the proposed imitation learning algorithm obtains the ability to avoid\ncollisions and reduces discomfort by 55.71%. Besides, density-aware model\naggregation can further reduce discomfort by 41.37%, and the experience\nselection scheme can reduce the communication overhead by 12.80% while ensuring\nmodel convergence.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:41:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wu", "Tianhao", ""], ["Jiang", "Mingzhi", ""], ["Han", "Yinhui", ""], ["Yuan", "Zheng", ""], ["Zhang", "Lin", ""]]}, {"id": "2105.02371", "submitter": "Arvin Tashakori", "authors": "Arvin Tashakori", "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in\n  wireless sensor network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report aims to survey multi-agent Q-Learning algorithms, analyze\ndifferent game theory frameworks used, address each framework's applications,\nand report challenges and future directions. The target application for this\nstudy is resource management in the wireless sensor network.\n  In the first section, the author provided an introduction regarding the\napplications of wireless sensor networks. After that, the author presented a\nsummary of the Q-Learning algorithm, a well-known classic solution for\nmodel-free reinforcement learning problems.\n  In the third section, the author extended the Q-Learning algorithm for\nmulti-agent scenarios and discussed its challenges.\n  In the fourth section, the author surveyed sets of game-theoretic frameworks\nthat researchers used to address this problem for resource allocation and task\nscheduling in the wireless sensor networks. Lastly, the author mentioned some\ninteresting open challenges in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:43:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tashakori", "Arvin", ""]]}, {"id": "2105.02931", "submitter": "Vijay Gupta", "authors": "Nayara Aguiar, Parv Venkitasubramaniam and Vijay Gupta", "title": "Data-Driven Contract Design for Multi-Agent Systems with Collusion\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications such as participatory sensing and crowd sensing,\nself-interested agents exert costly effort towards achieving an objective for\nthe system operator. We study such a setup where a principal incentivizes\nmultiple agents of different types who can collude with each other to derive\nrent. The principal cannot observe the efforts exerted directly, but only the\noutcome of the task, which is a noisy function of the effort. The type of each\nagent influences the effort cost and task output. For a duopoly in which agents\nare coupled in their payments, we show that if the principal and the agents\ninteract finitely many times, the agents can derive rent by colluding even if\nthe principal knows the types of the agents. However, if the principal and the\nagents interact infinitely often, the principal can disincentivize agent\ncollusion through a suitable data-driven contract.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:00:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Aguiar", "Nayara", ""], ["Venkitasubramaniam", "Parv", ""], ["Gupta", "Vijay", ""]]}, {"id": "2105.03052", "submitter": "Tao Zhang", "authors": "Tao Zhang and Quanyan Zhu", "title": "Informational Design of Dynamic Multi-Agent System", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.07152", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a novel information design problem and studies how the\ncraft of payoff-relevant environmental signals solely can influence the\nbehaviors of intelligent agents. The agents' strategic interactions are\ncaptured by a Markov game, in which each agent first selects one external\nsignal from multiple signal sources as additional payoff-relevant information\nand then takes an action. There is a rational information designer (principal)\nwho possesses one signal source and aims to influence the equilibrium behaviors\nof the agents by designing the information structure of her signals sent to the\nagents. We propose a direct information design approach that incentivizes each\nagent to select the signal sent by the principal, such that the design process\navoids the predictions of the agents' strategic selection behaviors. We then\nintroduce the design protocol given a goal of the designer which we refer to as\nobedient implementability (OIL) and characterize the OIL in a class of obedient\nsequential Markov perfect equilibria (O-SMPE). A design regime is proposed\nbased on an approach which we refer to as the fixed-point alignment that\nincentivizes the agents to choose the signal sent by the principal, guarantees\nthat the agents' policy profile of taking actions is the policy component of an\nO-SMPE and the principal's goal is achieved. We then formulate the principal's\noptimal goal selection problem in terms of information design and characterize\nthe optimization problem by minimizing the fixed-point misalignments. The\nproposed approach can be applied to elicit desired behaviors of multi-agent\nsystems in competing as well as cooperating settings and be extended to\nheterogeneous stochastic games in the complete- and the incomplete-information\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 03:46:14 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 01:08:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Tao", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2105.03166", "submitter": "Sriashalya Srivathsan", "authors": "Sriashalya Srivathsan, Stephen Cranefield, Jeremy Pitt", "title": "A Bayesian model of information cascades", "comments": "13 pages, 37 figures, Paper accepted for presentation A Bayesian\n  model of information cascade in International Workshop on Coordination,\n  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent\n  Systems (COINE), co-located with AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An information cascade is a circumstance where agents make decisions in a\nsequential fashion by following other agents. Bikhchandani et al., predict that\nonce a cascade starts it continues, even if it is wrong, until agents receive\nan external input such as public information. In an information cascade, even\nif an agent has its own personal choice, it is always overridden by observation\nof previous agents' actions. This could mean agents end up in a situation where\nthey may act without valuing their own information. As information cascades can\nhave serious social consequences, it is important to have a good understanding\nof what causes them. We present a detailed Bayesian model of the information\ngained by agents when observing the choices of other agents and their own\nprivate information. Compared to prior work, we remove the high impact of the\nfirst observed agent's action by incorporating a prior probability distribution\nover the information of unobserved agents and investigate an alternative model\nof choice to that considered in prior work: weighted random choice. Our results\nshow that, in contrast to Bikhchandani's results, cascades will not necessarily\noccur and adding prior agents' information will delay the effects of cascades.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:18:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Srivathsan", "Sriashalya", ""], ["Cranefield", "Stephen", ""], ["Pitt", "Jeremy", ""]]}, {"id": "2105.03363", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Xihuai Wang, Jian Shen, Ming Zhou", "title": "Model-based Multi-agent Policy Optimization with Adaptive Opponent-wise\n  Rollouts", "comments": "Paper accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the model-based methods in multi-agent reinforcement\nlearning (MARL). We specify the dynamics sample complexity and the opponent\nsample complexity in MARL, and conduct a theoretic analysis of return\ndiscrepancy upper bound. To reduce the upper bound with the intention of low\nsample complexity during the whole learning process, we propose a novel\ndecentralized model-based MARL method, named Adaptive Opponent-wise Rollout\nPolicy Optimization (AORPO). In AORPO, each agent builds its multi-agent\nenvironment model, consisting of a dynamics model and multiple opponent models,\nand trains its policy with the adaptive opponent-wise rollout. We further prove\nthe theoretic convergence of AORPO under reasonable assumptions. Empirical\nexperiments on competitive and cooperative tasks demonstrate that AORPO can\nachieve improved sample efficiency with comparable asymptotic performance over\nthe compared MARL methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:20:22 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 14:48:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Weinan", ""], ["Wang", "Xihuai", ""], ["Shen", "Jian", ""], ["Zhou", "Ming", ""]]}, {"id": "2105.03371", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Darko Anicic, Thomas Runkler", "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT", "comments": "Accepted by The 15th ACM International Conference on Distributed and\n  Event-based Systems (DEBS) 2021", "journal-ref": null, "doi": "10.1145/3465480.3466928", "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:58:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Haoyu", ""], ["Anicic", "Darko", ""], ["Runkler", "Thomas", ""]]}, {"id": "2105.03546", "submitter": "Austin Nguyen", "authors": "Austin Anhkhoi Nguyen", "title": "Scalable, Decentralized Multi-Agent Reinforcement Learning Methods\n  Inspired by Stigmergy and Ant Colonies", "comments": "50 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bolstering multi-agent learning algorithms to tackle complex coordination and\ncontrol tasks has been a long-standing challenge of on-going research. Numerous\nmethods have been proposed to help reduce the effects of non-stationarity and\nunscalability. In this work, we investigate a novel approach to decentralized\nmulti-agent learning and planning that attempts to address these two\nchallenges. In particular, this method is inspired by the cohesion,\ncoordination, and behavior of ant colonies. As a result, these algorithms are\ndesigned to be naturally scalable to systems with numerous agents. While no\noptimality is guaranteed, the method is intended to work well in practice and\nscale better in efficacy with the number of agents present than others. The\napproach combines single-agent RL and an ant-colony-inspired decentralized,\nstigmergic algorithm for multi-agent path planning and environment\nmodification. Specifically, we apply this algorithm in a setting where agents\nmust navigate to a goal location, learning to push rectangular boxes into holes\nto yield new traversable pathways. It is shown that while the approach yields\npromising success in this particular environment, it may not be as easily\ngeneralized to others. The algorithm designed is notably scalable to numerous\nagents but is limited in its performance due to its relatively simplistic,\nrule-based approach. Furthermore, the composability of RL-trained policies is\ncalled into question, where, while policies are successful in their training\nenvironments, applying trained policies to a larger-scale, multi-agent\nframework results in unpredictable behavior.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 01:04:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nguyen", "Austin Anhkhoi", ""]]}, {"id": "2105.03552", "submitter": "Abira Sengupta", "authors": "Abira Sengupta, Stephen Cranefield, Jeremy Pitt", "title": "Solving social dilemmas by reasoning about expectations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued that one role of social constructs, such as institutions,\ntrust and norms, is to coordinate the expectations of autonomous entities in\norder to resolve collective action situations (such as collective risk\ndilemmas) through the coordination of behaviour. While much work has addressed\nthe formal representation of these social constructs, in this paper we focus\nspecifically on the formal representation of, and associated reasoning with,\nthe expectations themselves. In particular, we investigate how explicit\nreasoning about expectations can be used to encode both traditional game theory\nsolution concepts and social mechanisms for the social dilemma situation. We\nuse the Collective Action Simulation Platform (CASP) to model a collective risk\ndilemma based on a flood plain scenario and show how using expectations in the\nreasoning mechanisms of the agents making decisions supports the choice of\ncooperative behaviour.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 01:49:00 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sengupta", "Abira", ""], ["Cranefield", "Stephen", ""], ["Pitt", "Jeremy", ""]]}, {"id": "2105.03941", "submitter": "Lorenzo Minto", "authors": "Lorenzo Minto, Moritz Haller, Hamed Haddadi, Benjamin Livshits", "title": "Stronger Privacy for Federated Collaborative Filtering with Implicit\n  Feedback", "comments": "Accepted for publication at RecSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are commonly trained on centrally collected user\ninteraction data like views or clicks. This practice however raises serious\nprivacy concerns regarding the recommender's collection and handling of\npotentially sensitive data. Several privacy-aware recommender systems have been\nproposed in recent literature, but comparatively little attention has been\ngiven to systems at the intersection of implicit feedback and privacy. To\naddress this shortcoming, we propose a practical federated recommender system\nfor implicit data under user-level local differential privacy (LDP). The\nprivacy-utility trade-off is controlled by parameters $\\epsilon$ and $k$,\nregulating the per-update privacy budget and the number of $\\epsilon$-LDP\ngradient updates sent by each user respectively. To further protect the user's\nprivacy, we introduce a proxy network to reduce the fingerprinting surface by\nanonymizing and shuffling the reports before forwarding them to the\nrecommender. We empirically demonstrate the effectiveness of our framework on\nthe MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k\nusers with 5k items. Even on the full dataset, we show that it is possible to\nachieve reasonable utility with HR@10>0.5 without compromising user privacy.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 13:41:45 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 10:05:37 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 09:39:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Minto", "Lorenzo", ""], ["Haller", "Moritz", ""], ["Haddadi", "Hamed", ""], ["Livshits", "Benjamin", ""]]}, {"id": "2105.04027", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Florian Wiedemair, Boi Faltings", "title": "Improving Multi-agent Coordination by Learning to Estimate Contention", "comments": "Accepted to the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-agent learning algorithm, ALMA-Learning, for efficient and\nfair allocations in large-scale systems. We circumvent the traditional pitfalls\nof multi-agent learning (e.g., the moving target problem, the curse of\ndimensionality, or the need for mutually consistent actions) by relying on the\nALMA heuristic as a coordination mechanism for each stage game. ALMA-Learning\nis decentralized, observes only own action/reward pairs, requires no\ninter-agent communication, and achieves near-optimal (<5% loss) and fair\ncoordination in a variety of synthetic scenarios and a real-world meeting\nscheduling problem. The lightweight nature and fast learning constitute\nALMA-Learning ideal for on-device deployment.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 21:30:48 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 17:53:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Wiedemair", "Florian", ""], ["Faltings", "Boi", ""]]}, {"id": "2105.04196", "submitter": "Mohammad Parvini", "authors": "Mohammad Parvini, Mohammad Reza Javan, Nader Mokari, Bijan Abbasi, and\n  Eduard A. Jorswieck", "title": "AoI-Aware Resource Allocation for Platoon-Based C-V2X Networks via\n  Multi-Agent Multi-Task Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of age of information (AoI) aware radio\nresource management for a platooning system. Multiple autonomous platoons\nexploit the cellular wireless vehicle-to-everything (C-V2X) communication\ntechnology to disseminate the cooperative awareness messages (CAMs) to their\nfollowers while ensuring timely delivery of safety-critical messages to the\nRoad-Side Unit (RSU). Due to the challenges of dynamic channel conditions,\ncentralized resource management schemes that require global information are\ninefficient and lead to large signaling overheads. Hence, we exploit a\ndistributed resource allocation framework based on multi-agent reinforcement\nlearning (MARL), where each platoon leader (PL) acts as an agent and interacts\nwith the environment to learn its optimal policy. Existing MARL algorithms\nconsider a holistic reward function for the group's collective success, which\noften ends up with unsatisfactory results and cannot guarantee an optimal\npolicy for each agent. Consequently, motivated by the existing literature in\nRL, we propose a novel MARL framework that trains two critics with the\nfollowing goals: A global critic which estimates the global expected reward and\nmotivates the agents toward a cooperating behavior and an exclusive local\ncritic for each agent that estimates the local individual reward. Furthermore,\nbased on the tasks each agent has to accomplish, the individual reward of each\nagent is decomposed into multiple sub-reward functions where task-wise value\nfunctions are learned separately. Numerical results indicate our proposed\nalgorithm's effectiveness compared with the conventional RL methods applied in\nthis area.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:39:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Parvini", "Mohammad", ""], ["Javan", "Mohammad Reza", ""], ["Mokari", "Nader", ""], ["Abbasi", "Bijan", ""], ["Jorswieck", "Eduard A.", ""]]}, {"id": "2105.04230", "submitter": "Adrian Redder", "authors": "Adrian Redder, Arunselvan Ramaswamy, Holger Karl", "title": "Practical sufficient conditions for convergence of distributed\n  optimisation algorithms over communication networks with interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information exchange over networks can be affected by various forms of delay.\nThis causes challenges for using the network by a multi-agent system to solve a\ndistributed optimisation problem. Distributed optimisation schemes, however,\ntypically do not assume network models that are representative for real-world\ncommunication networks, since communication links are most of the time\nabstracted as lossless. Our objective is therefore to formulate a\nrepresentative network model and provide practically verifiable network\nconditions that ensure convergence of distributed algorithms in the presence of\ninterference and possibly unbounded delay. Our network is modelled by a\nsequence of directed-graphs, where to each network link we associate a process\nfor the instantaneous signal-to-interference-plus-noise ratio. We then\nformulate practical conditions that can be verified locally and show that the\nage of information (AoI) associated with data communicated over the network is\nin $\\mathcal{O}(\\sqrt{n})$. Under these conditions we show that a penalty-based\ngradient descent algorithm can be used to solve a rich class of stochastic,\nconstrained, distributed optimisation problems. The strength of our result lies\nin the bridge between practical verifiable network conditions and an abstract\noptimisation theory. We illustrate numerically that our algorithm converges in\nan extreme scenario where the average AoI diverges.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:45:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Redder", "Adrian", ""], ["Ramaswamy", "Arunselvan", ""], ["Karl", "Holger", ""]]}, {"id": "2105.04514", "submitter": "Stephan Leitner", "authors": "Stephan Leitner", "title": "On the Role of Incentives in Evolutionary Approaches to Organizational\n  Design", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.MA nlin.AO q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a model of a stylized organization that is comprised of\nseveral departments that autonomously allocate tasks. To do so, the departments\neither take short-sighted decisions that immediately maximize their utility or\ntake long-sighted decisions that aim at minimizing the interdependencies\nbetween tasks. The organization guides the departments' behavior by either an\nindividualistic, a balanced, or an altruistic linear incentive scheme. Even if\ntasks are perfectly decomposable, altruistic incentive schemes are preferred\nover individualistic incentive schemes since they substantially increase the\norganization's performance. Interestingly, if altruistic incentive schemes are\neffective, short-sighted decisions appear favorable since they do not only\nincrease performance in the short run but also result in significantly higher\nperformances in the long run.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:06:50 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 05:52:18 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Leitner", "Stephan", ""]]}, {"id": "2105.04598", "submitter": "Suman Banerjee", "authors": "Suman Banerjee, Bithika Pal, Maheswar Singhamahapatra", "title": "A Social Distancing-Based Facility Location Approach for Combating\n  COVID-19", "comments": "16 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we introduce and study the problem of facility location along\nwith the notion of \\emph{`social distancing'}. The input to the problem is the\nroad network of a city where the nodes are the residential zones, edges are the\nroad segments connecting the zones along with their respective distance. We\nalso have the information about the population at each zone, different types of\nfacilities to be opened and in which number, and their respective demands in\neach zone. The goal of the problem is to locate the facilities such that the\npeople can be served and at the same time the total social distancing is\nmaximized. We formally call this problem as the \\textsc{Social Distancing-Based\nFacility Location Problem}. We mathematically quantify social distancing for a\ngiven allocation of facilities and proposed an optimization model. As the\nproblem is \\textsf{NP-Hard}, we propose a simulation-based and heuristic\napproach for solving this problem. A detailed analysis of both methods has been\ndone. We perform an extensive set of experiments with synthetic datasets. From\nthe results, we observe that the proposed heuristic approach leads to a better\nallocation compared to the simulation-based approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:27:07 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""], ["Singhamahapatra", "Maheswar", ""]]}, {"id": "2105.04666", "submitter": "David Kohan Marzag\\~ao", "authors": "David Kohan Marzag\\~ao, Luciana Basualdo Bonatto, Tiago Madeira,\n  Marcelo Matheus Gauy, Peter McBurney", "title": "The Influence of Memory in Multi-Agent Consensus", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-agent consensus problems can often be seen as a sequence of autonomous\nand independent local choices between a finite set of decision options, with\neach local choice undertaken simultaneously, and with a shared goal of\nachieving a global consensus state. Being able to estimate probabilities for\nthe different outcomes and to predict how long it takes for a consensus to be\nformed, if ever, are core issues for such protocols.\n  Little attention has been given to protocols in which agents can remember\npast or outdated states. In this paper, we propose a framework to study what we\ncall \\emph{memory consensus protocol}. We show that the employment of memory\nallows such processes to always converge, as well as, in some scenarios, such\nas cycles, converge faster. We provide a theoretical analysis of the\nprobability of each option eventually winning such processes based on the\ninitial opinions expressed by agents. Further, we perform experiments to\ninvestigate network topologies in which agents benefit from memory on the\nexpected time needed for consensus.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:59:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Marzag\u00e3o", "David Kohan", ""], ["Bonatto", "Luciana Basualdo", ""], ["Madeira", "Tiago", ""], ["Gauy", "Marcelo Matheus", ""], ["McBurney", "Peter", ""]]}, {"id": "2105.04738", "submitter": "Zhenyuan Yuan", "authors": "Zhenyuan Yuan, Minghui Zhu", "title": "Resource-aware Distributed Gaussian Process Regression for Real-time\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem where a group of agents aim to collaboratively learn a\ncommon latent function through streaming data. We propose a Resource-aware\nGaussian process regression algorithm that is cognizant of agents' limited\ncapabilities in communication, computation and memory. We quantify the\nimprovement that limited inter-agent communication brings to the transient and\nsteady-state performance in predictive variance and predictive mean. A set of\nsimulations is conducted to evaluate the developed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 01:13:22 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 23:08:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yuan", "Zhenyuan", ""], ["Zhu", "Minghui", ""]]}, {"id": "2105.04764", "submitter": "Vincent Hill", "authors": "Vincent W. Hill, Ryan W. Thomas, and Jordan D. Larson", "title": "Autonomous Situational Awareness for Robotic Swarms in High-Risk\n  Environments", "comments": "arXiv admin note: substantial text overlap with arXiv:2104.08904", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a technique for the autonomous mission planning of\nrobotic swarms in high risk environments where agent disablement is likely.\nGiven a swarm operating in a known area, a central command system generates\nmeasurements from the swarm. If those measurements indicate changes to the\nmission situation such as target movement or agent loss, the swarm planning is\nupdated to reflect the new situation and guidance updates are broadcast to the\nswarm. The primary algorithms featured in this work are A* pathfinding and the\nGeneralized Labeled Multi-Bernoulli multi-object tracking method.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:03:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hill", "Vincent W.", ""], ["Thomas", "Ryan W.", ""], ["Larson", "Jordan D.", ""]]}, {"id": "2105.04851", "submitter": "Shi Pu", "authors": "Kun Huang and Shi Pu", "title": "Improving the Transient Times for Distributed Stochastic Gradient\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed optimization problem where $n$ agents each\npossessing a local cost function, collaboratively minimize the average of the\n$n$ cost functions over a connected network. Assuming stochastic gradient\ninformation is available, we study a distributed stochastic gradient algorithm,\ncalled exact diffusion with adaptive stepsizes (EDAS) adapted from the Exact\nDiffusion method and NIDS and perform a non-asymptotic convergence analysis. We\nnot only show that EDAS asymptotically achieves the same network independent\nconvergence rate as centralized stochastic gradient descent (SGD) for\nminimizing strongly convex and smooth objective functions, but also\ncharacterize the transient time needed for the algorithm to approach the\nasymptotic convergence rate, which behaves as\n$K_T=\\mathcal{O}\\left(\\frac{n}{1-\\lambda_2}\\right)$, where $1-\\lambda_2$ stands\nfor the spectral gap of the mixing matrix. To the best of our knowledge, EDAS\nachieves the shortest transient time when the average of the $n$ cost functions\nis strongly convex and each cost function is smooth. Numerical simulations\nfurther corroborate and strengthen the obtained theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:09:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Huang", "Kun", ""], ["Pu", "Shi", ""]]}, {"id": "2105.04888", "submitter": "Xiaolong Wei", "authors": "Xiaolong Wei, LiFang Yang, Xianglin Huang, Gang Cao, Tao Zhulin,\n  Zhengyang Du, Jing An", "title": "Hierarchical RNNs-Based Transformers MADDPG for Mixed\n  Cooperative-Competitive Environments", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, attention mechanism has been widely applied to the fields of deep\nlearning models. Structural models that based on attention mechanism can not\nonly record the relationships between features position, but also can measure\nthe importance of different features based on their weights. By establishing\ndynamically weighted parameters for choosing relevant and irrelevant features,\nthe key information can be strengthened, and the irrelevant information can be\nweakened. Therefore, the efficiency of deep learning algorithms can be\nsignificantly elevated and improved. Although transformers have been performed\nvery well in many fields including reinforcement learning, there are still many\nproblems and applications can be solved and made with transformers within this\narea. MARL (known as Multi-Agent Reinforcement Learning) can be recognized as a\nset of independent agents trying to adapt and learn through their way to reach\nthe goal. In order to emphasize the relationship between each MDP decision in a\ncertain time period, we applied the hierarchical coding method and validated\nthe effectiveness of this method. This paper proposed a hierarchical\ntransformers MADDPG based on RNN which we call it Hierarchical RNNs-Based\nTransformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on\nRNNs that encodes multiple step sizes in each time sequence, and it also\nconsists of an upper sequence level encoder based on transformer for learning\nthe correlations between multiple sequences so that we can capture the causal\nrelationship between sub-time sequences and make HRTMADDPG more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:22:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wei", "Xiaolong", ""], ["Yang", "LiFang", ""], ["Huang", "Xianglin", ""], ["Cao", "Gang", ""], ["Zhulin", "Tao", ""], ["Du", "Zhengyang", ""], ["An", "Jing", ""]]}, {"id": "2105.05094", "submitter": "Damiano Brunori", "authors": "Damiano Brunori, Stefania Colonnese, Francesca Cuomo and Luca Iocchi", "title": "A Reinforcement Learning Environment for Multi-Service UAV-enabled\n  Wireless Systems", "comments": null, "journal-ref": "2021 IEEE International Conference on Pervasive Computing and\n  Communications Workshops and other Affiliated Events (PerCom Workshops)", "doi": "10.1109/PerComWorkshops51409.2021.9431048", "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a multi-purpose environment for autonomous UAVs offering different\ncommunication services in a variety of application contexts (e.g., wireless\nmobile connectivity services, edge computing, data gathering). We develop the\nenvironment, based on OpenAI Gym framework, in order to simulate different\ncharacteristics of real operational environments and we adopt the Reinforcement\nLearning to generate policies that maximize some desired performance.The\nquality of the resulting policies are compared with a simple baseline to\nevaluate the system and derive guidelines to adopt this technique in different\nuse cases. The main contribution of this paper is a flexible and extensible\nOpenAI Gym environment, which allows to generate, evaluate, and compare\npolicies for autonomous multi-drone systems in multi-service applications. This\nenvironment allows for comparative evaluation and benchmarking of different\napproaches in a variety of application contexts.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:45:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Brunori", "Damiano", ""], ["Colonnese", "Stefania", ""], ["Cuomo", "Francesca", ""], ["Iocchi", "Luca", ""]]}, {"id": "2105.05145", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson", "title": "Visual Perspective Taking for Opponent Behavior Modeling", "comments": "ICRA 2021. Website: http://www.cs.columbia.edu/~bchen/vpttob/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to engage in complex social interaction, humans learn at a young age\nto infer what others see and cannot see from a different point-of-view, and\nlearn to predict others' plans and behaviors. These abilities have been mostly\nlacking in robots, sometimes making them appear awkward and socially inept.\nHere we propose an end-to-end long-term visual prediction framework for robots\nto begin to acquire both these critical cognitive skills, known as Visual\nPerspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our\napproach in the context of visual hide-and-seek - a game that represents a\ncognitive milestone in human development. Unlike traditional visual predictive\nmodel that generates new frames from immediate past frames, our agent can\ndirectly predict to multiple future timestamps (25s), extrapolating by 175%\nbeyond the training horizon. We suggest that visual behavior modeling and\nperspective taking skills will play a critical role in the ability of physical\nrobots to fully integrate into real-world multi-agent activities. Our website\nis at http://www.cs.columbia.edu/~bchen/vpttob/.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:02:32 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Kwiatkowski", "Robert", ""], ["Song", "Shuran", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.05170", "submitter": "Sasanka Sekhar Chanda", "authors": "Sasanka Sekhar Chanda", "title": "Mandating Code Disclosure is Unnecessary -- Strict Model Verification\n  Does Not Require Accessing Original Computer Code", "comments": "12 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mandating public availability of computer code underlying computational\nsimulation modeling research ends up doing a disservice to the cause of model\nverification when inconsistencies between the specifications in the publication\ntext and specifications in the computer code go unchallenged. Conversely, a\nmodel is verified when an independent researcher undertakes the set of mental\nprocessing tasks necessary to convert natural language specifications in a\npublication text into computer code instructions that produce numerical or\ngraphical outputs identical to the outputs found in the original publication.\nThe effort towards obtaining convergence with the numerical or graphical\noutputs directs intensive consideration of the publication text. The original\ncomputer code has little role to play in determining the verification status -\nverified/ failed verification. An insight is obtained that skillful deployment\nof human intelligence is feasible when effort-directing feedback processes are\nin place to appropriately go around the human frailty of giving up in the\nabsence of actionable feedback. This principle can be put to use to develop\nbetter organizational configurations in business, government and society.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:25:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chanda", "Sasanka Sekhar", ""]]}, {"id": "2105.05377", "submitter": "Mustafa O. Karabag", "authors": "Mustafa O. Karabag, Melkior Ornik, Ufuk Topcu", "title": "Identity Concealment Games: How I Learned to Stop Revealing and Love the\n  Coincidences", "comments": "19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an adversarial environment, a hostile player performing a task may behave\nlike a non-hostile one in order not to reveal its identity to an opponent. To\nmodel such a scenario, we define identity concealment games: zero-sum\nstochastic reachability games with a zero-sum objective of identity\nconcealment. To measure the identity concealment of the player, we introduce\nthe notion of an average player. The average player's policy represents the\nexpected behavior of a non-hostile player. We show that there exists an\nequilibrium policy pair for every identity concealment game and give the\noptimality equations to synthesize an equilibrium policy pair. If the player's\nopponent follows a non-equilibrium policy, the player can hide its identity\nbetter. For this reason, we study how the hostile player may learn the\nopponent's policy. Since learning via exploration policies would quickly reveal\nthe hostile player's identity to the opponent, we consider the problem of\nlearning a near-optimal policy for the hostile player using the game runs\ncollected under the average player's policy. Consequently, we propose an\nalgorithm that provably learns a near-optimal policy and give an upper bound on\nthe number of sample runs to be collected.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:41:58 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Karabag", "Mustafa O.", ""], ["Ornik", "Melkior", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2105.05687", "submitter": "Wicak Ananduta", "authors": "Wicak Ananduta and Sergio Grammatico", "title": "Bregman algorithms for a class of Mixed-Integer Generalized Nash\n  Equilibrium Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing a mixed-strategy generalized Nash\nequilibrium (MS-GNE) for a class of games where each agent has both continuous\nand integer decision variables. Specifically, we propose a novel Bregman\nforward-reflected-backward splitting and design distributed algorithms that\nexploit the problem structure. Technically, we prove convergence to a\nvariational MS-GNE under monotonicity and Lipschitz continuity assumptions,\nwhich are typical of continuous GNE problems. Finally, we show the performance\nof our algorithms via numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 14:23:44 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ananduta", "Wicak", ""], ["Grammatico", "Sergio", ""]]}, {"id": "2105.05725", "submitter": "Manuel Sorge", "authors": "Jiehua Chen, Adrian Chmurovic, Fabian Jogl, and Manuel Sorge", "title": "On (Coalitional) Exchange-Stable Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study (coalitional) exchange stability, which Alcalde [Economic Design,\n1995] introduced as an alternative solution concept for matching markets\ninvolving property rights, such as assigning persons to two-bed rooms. Here, a\nmatching of a given Stable Marriage or Stable Roommates instance is called\ncoalitional exchange-stable if it does not admit any exchange-blocking\ncoalition, that is, a subset S of agents in which everyone prefers the partner\nof some other agent in S. The matching is exchange-stable if it does not admit\nany exchange-blocking pair, that is, an exchange-blocking coalition of size\ntwo.\n  We investigate the computational and parameterized complexity of the\nCoalitional Exchange-Stable Marriage (resp. Coalitional Exchange Roommates)\nproblem, which is to decide whether a Stable Marriage (resp. Stable Roommates)\ninstance admits a coalitional exchange-stable matching. Our findings resolve an\nopen question and confirm the conjecture of Cechl\\'arov\\'a and Manlove\n[Discrete Applied Mathematics, 2005] that Coalitional Exchange-Stable Marriage\nis NP-hard even for complete preferences without ties. We also study\nbounded-length preference lists and a local-search variant of deciding whether\na given matching can reach an exchange-stable one after at most k swaps, where\na swap is defined as exchanging the partners of the two agents in an\nexchange-blocking pair.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:17:35 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:12:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Jiehua", ""], ["Chmurovic", "Adrian", ""], ["Jogl", "Fabian", ""], ["Sorge", "Manuel", ""]]}, {"id": "2105.06228", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Yunpeng Bai, Dapeng Li, Bin Zhang, Guoliang Fan", "title": "SIDE: I Infer the State I Want to Learn", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the solutions to the Dec-POMDP problem, the value decomposition\nmethod has achieved good results recently. However, most value decomposition\nmethods require the global state during training, but this is not feasible in\nsome scenarios where the global state cannot be obtained. Therefore, we propose\na novel value decomposition framework, named State Inference for value\nDEcomposition (SIDE), which eliminates the need to know the true state by\nsimultaneously seeking solutions to the two problems of optimal control and\nstate inference. SIDE can be extended to any value decomposition method, as\nwell as other types of multi-agent algorithms in the case of Dec-POMDP. Based\non the performance results of different algorithms in Starcraft II\nmicromanagement tasks, we verified that SIDE can construct the current state\nthat contributes to the reinforcement learning process based on past local\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 12:26:02 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xu", "Zhiwei", ""], ["Bai", "Yunpeng", ""], ["Li", "Dapeng", ""], ["Zhang", "Bin", ""], ["Fan", "Guoliang", ""]]}, {"id": "2105.06593", "submitter": "Woodrow Wang", "authors": "Woodrow Z. Wang, Mark Beliaev, Erdem B{\\i}y{\\i}k, Daniel A. Lazar,\n  Ramtin Pedarsani, Dorsa Sadigh", "title": "Emergent Prosociality in Multi-Agent Games Through Gifting", "comments": "9 pages, 6 figures, IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination is often critical to forming prosocial behaviors -- behaviors\nthat increase the overall sum of rewards received by all agents in a\nmulti-agent game. However, state of the art reinforcement learning algorithms\noften suffer from converging to socially less desirable equilibria when\nmultiple equilibria exist. Previous works address this challenge with explicit\nreward shaping, which requires the strong assumption that agents can be forced\nto be prosocial. We propose using a less restrictive peer-rewarding mechanism,\ngifting, that guides the agents toward more socially desirable equilibria while\nallowing agents to remain selfish and decentralized. Gifting allows each agent\nto give some of their reward to other agents. We employ a theoretical framework\nthat captures the benefit of gifting in converging to the prosocial equilibrium\nby characterizing the equilibria's basins of attraction in a dynamical system.\nWith gifting, we demonstrate increased convergence of high risk, general-sum\ncoordination games to the prosocial equilibrium both via numerical analysis and\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:28:30 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Woodrow Z.", ""], ["Beliaev", "Mark", ""], ["B\u0131y\u0131k", "Erdem", ""], ["Lazar", "Daniel A.", ""], ["Pedarsani", "Ramtin", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2105.06718", "submitter": "Jan Eskil Snellman Ph.D.", "authors": "Jan E. Snellman and Rafael A. Barrio and Kimmo K. Kaski and Maarit J.\n  K\\\"apyl\\\"a", "title": "Modeling the interplay between epidemics and regional socio-economics", "comments": "13 pages, 6 figures. Submitted to Scientific Reports utilizing In\n  Review service of Research Square", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this study we present a dynamical agent-based model to investigate the\ninterplay between the socio-economy of and SEIRS-type epidemic spreading over a\ngeographical area, divided to smaller area districts and further to smallest\narea cells. The model treats the populations of cells and authorities of\ndistricts as agents, such that the former can reduce their economic activity\nand the latter can recommend economic activity reduction both with the overall\ngoal to slow down the epidemic spreading. The agents make decisions with the\naim of attaining as high social positions as possible relative to other agents.\nThey evaluate their social positions based on the local and regional infection\nrates, compliance to the authorities' regulations, regional drops in economic\nactivity, and the efforts they make to mitigate the spread of epidemic. We find\nthat the willingness of populations to comply with authorities' recommendations\nhas the most drastic effect to the spreading of epidemic: periodic waves spread\nalmost unimpeded in non-compliant populations, while in compliant ones the\nspread is minimal with a chaotic spreading pattern and significantly lower\ninfection rates. Health and economic concerns of agents turned out to have\nlesser roles, the former increasing their efforts and the latter decreasing\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:55:18 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Snellman", "Jan E.", ""], ["Barrio", "Rafael A.", ""], ["Kaski", "Kimmo K.", ""], ["K\u00e4pyl\u00e4", "Maarit J.", ""]]}, {"id": "2105.06763", "submitter": "Matteo Capucci", "authors": "Matteo Capucci, Neil Ghani, J\\'er\\'emy Ledent, Fredrik Nordvall\n  Forsberg", "title": "Translating Extensive Form Games to Open Games with Agency", "comments": "13 pages, submitted to ACT2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA math.CT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We show open games cover extensive form games with both perfect and imperfect\ninformation. Doing so forces us to address two current weaknesses in open\ngames: the lack of a notion of player and their agency within open games, and\nthe lack of choice operators. Using the former we construct the latter, and\nthese choice operators subsume previous proposed operators for open games,\nthereby making progress towards a core, canonical and ergonomic calculus of\ngame operators. Collectively these innovations increase the level of\ncompositionality of open games, and demonstrate their expressiveness.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 11:15:25 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Capucci", "Matteo", ""], ["Ghani", "Neil", ""], ["Ledent", "J\u00e9r\u00e9my", ""], ["Forsberg", "Fredrik Nordvall", ""]]}, {"id": "2105.07132", "submitter": "Keisuke Okumura", "authors": "Keisuke Okumura, Fran\\c{c}ois Bonnet, Yasumasa Tamura, Xavier D\\'efago", "title": "Offline Time-Independent Multi-Agent Path Planning", "comments": "32 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a novel planning problem for multiple agents moving on\ngraphs that we call offline time-independent multi-agent path planning\n(OTIMAPP). The motivation is to overcome time uncertainties in multi-agent\nscenarios where we cannot expect agents to act perfectly following timed plans,\ne.g., executions with mobile robots. For this purpose, OTIMAPP abandons all\ntiming assumptions; it is offline planning that assumes event-driven executions\nwithout or less run-time effort. The problem is finding plans to be terminated\ncorrectly in any action orders of agents, i.e., guaranteeing that all agents\neventually reach their destinations. We address a bunch of questions for this\nproblem: required conditions for feasible solutions, computational complexity,\ncomparison with well-known other multi-agent problems, construction of solvers,\neffective relaxation of a solution concept, and how to implement the plans by\nactual robots. Throughout the paper, we establish the foundation of OTIMAPP and\ndemonstrate its utility. A video is available at\nhttps://kei18.github.io/otimapp.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:05:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Okumura", "Keisuke", ""], ["Bonnet", "Fran\u00e7ois", ""], ["Tamura", "Yasumasa", ""], ["D\u00e9fago", "Xavier", ""]]}, {"id": "2105.07183", "submitter": "Anton V. Proskurnikov", "authors": "Anton V. Proskurnikov and Giuseppe Carlo Calafiore", "title": "Delay Robustness of Consensus Algorithms: Beyond The Uniform\n  Connectivity (Extended Version)", "comments": "a shortened version is submitted to IEEE TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY math.DS nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Consensus of autonomous agents is a benchmark problem in multi-agent control.\nIn this paper, we consider continuous-time averaging consensus policies (or\nLaplacian flows) and their discrete-time counterparts over time-varying graphs\nin presence of unknown but bounded communication delays. It is known that\nconsensus is established (no matter how large the delays are) if the graph is\nperiodically, or uniformly quasi-strongly connected (UQSC). The UQSC condition\nis often believed to be the weakest sufficient condition under which consensus\ncan be proved. We show that the UQSC condition can actually be substantially\nrelaxed and replaced by a condition that we call aperiodic quasi-strong\nconnectivity (AQSC), which, in some sense, proves to be very close to the\nnecessary condition of integral connectivity. Furthermore, in some special\nsituations such as undirected or type-symmetric graph, we find a necessary and\nsufficient condition for consensus in presence of bounded delay; the relevant\nresults have been previously proved only in the undelayed case. The consensus\ncriteria established in this paper generalize a number of results known in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 09:34:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Proskurnikov", "Anton V.", ""], ["Calafiore", "Giuseppe Carlo", ""]]}, {"id": "2105.07405", "submitter": "Feng Huang", "authors": "Feng Huang, Ming Cao, and Long Wang", "title": "Optimal control of robust team stochastic games", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.GT cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic dynamic environments, team stochastic games have emerged as a\nversatile paradigm for studying sequential decision-making problems of fully\ncooperative multi-agent systems. However, the optimality of the derived\npolicies is usually sensitive to the model parameters, which are typically\nunknown and required to be estimated from noisy data in practice. To mitigate\nthe sensitivity of the optimal policy to these uncertain parameters, in this\npaper, we propose a model of \"robust\" team stochastic games, where players\nutilize a robust optimization approach to make decisions. This model extends\nteam stochastic games to the scenario of incomplete information and meanwhile\nprovides an alternative solution concept of robust team optimality. To seek\nsuch a solution, we develop a learning algorithm in the form of a Gauss-Seidel\nmodified policy iteration and prove its convergence. This algorithm, compared\nwith robust dynamic programming, not only possesses a faster convergence rate,\nbut also allows for using approximation calculations to alleviate the curse of\ndimensionality. Moreover, some numerical simulations are presented to\ndemonstrate the effectiveness of the algorithm by generalizing the game model\nof social dilemmas to sequential robust scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:42:09 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Huang", "Feng", ""], ["Cao", "Ming", ""], ["Wang", "Long", ""]]}, {"id": "2105.07443", "submitter": "Qin Yang", "authors": "Qin Yang and Ramviyas Parasuraman", "title": "How Can Robots Trust Each Other? A Relative Needs Entropy Based Trust\n  Assessment Models", "comments": "This paper already submitted to the SMC 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation in multi-agent and multi-robot systems can help agents build\nvarious formations, shapes, and patterns presenting corresponding functions and\npurposes adapting to different situations. Relationship between agents such as\ntheir spatial proximity and functional similarities could play a crucial role\nin cooperation between agents. Trust level between agents is an essential\nfactor in evaluating their relationships' reliability and stability, much as\npeople do. This paper proposes a new model called Relative Needs Entropy (RNE)\nto assess trust between robotic agents. RNE measures the distance of needs\ndistribution between individual agents or groups of agents. To exemplify its\nutility, we implement and demonstrate our trust model through experiments\nsimulating a heterogeneous multi-robot grouping task in a persistent urban\nsearch and rescue mission consisting of tasks at two levels of difficulty. The\nresults suggest that RNE trust-Based grouping of robots can achieve better\nperformance and adaptability for diverse task execution compared to the\nstate-of-the-art energy-based or distance-based grouping models.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 14:33:11 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Qin", ""], ["Parasuraman", "Ramviyas", ""]]}, {"id": "2105.07501", "submitter": "Mohammad Sayad Haghighi", "authors": "Ghader Ebrahimpour, Mohammad Sayad Haghighi", "title": "Analysis of Bitcoin Vulnerability to Bribery Attacks Launched Through\n  Large Transactions", "comments": "This work is under review for formal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin uses blockchain technology to maintain transactions order and\nprovides probabilistic guarantee to prevent double-spending, assuming that an\nattacker's computational power does not exceed %50 of the network power. In\nthis paper, we design a novel bribery attack and show that this guarantee can\nbe hugely undermined. Miners are assumed to be rational in this setup and they\nare given incentives that are dynamically calculated. In this attack, the\nadversary misuses the Bitcoin protocol to bribe miners and maximize their\ngained advantage. We will reformulate the bribery attack to propose a general\nmathematical foundation upon which we build multiple strategies. We show that,\nunlike Whale Attack, these strategies are practical. If the rationality\nassumption holds, this shows how vulnerable blockchain-based systems like\nBitcoin are. We suggest a soft fork on Bitcoin to fix this issue at the end.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 19:35:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ebrahimpour", "Ghader", ""], ["Haghighi", "Mohammad Sayad", ""]]}, {"id": "2105.07648", "submitter": "Jieting Luo", "authors": "Jieting Luo, Beishui Liao, John-Jules Meyer", "title": "A Formal Framework for Reasoning about Agents' Independence in\n  Self-organizing Multi-agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organization is a process where a stable pattern is formed by the\ncooperative behavior between parts of an initially disordered system without\nexternal control or influence. It has been introduced to multi-agent systems as\nan internal control process or mechanism to solve difficult problems\nspontaneously. However, because a self-organizing multi-agent system has\nautonomous agents and local interactions between them, it is difficult to\npredict the behavior of the system from the behavior of the local agents we\ndesign. This paper proposes a logic-based framework of self-organizing\nmulti-agent systems, where agents interact with each other by following their\nprescribed local rules. The dependence relation between coalitions of agents\nregarding their contributions to the global behavior of the system is reasoned\nabout from the structural and semantic perspectives. We show that the\ncomputational complexity of verifying such a self-organizing multi-agent system\nis in exponential time. We then combine our framework with graph theory to\ndecompose a system into different coalitions located in different layers, which\nallows us to verify agents' full contributions more efficiently. The resulting\ninformation about agents' full contributions allows us to understand the\ncomplex link between local agent behavior and system level behavior in a\nself-organizing multi-agent system. Finally, we show how we can use our\nframework to model a constraint satisfaction problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 07:32:43 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 11:50:42 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 06:20:35 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Luo", "Jieting", ""], ["Liao", "Beishui", ""], ["Meyer", "John-Jules", ""]]}, {"id": "2105.07933", "submitter": "Sarah Perrin", "authors": "Sarah Perrin, Mathieu Lauri\\`ere, Julien P\\'erolat, Matthieu Geist,\n  Romuald \\'Elie, Olivier Pietquin", "title": "Mean Field Games Flock! The Reinforcement Learning Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a method enabling a large number of agents to learn how to flock,\nwhich is a natural behavior observed in large populations of animals. This\nproblem has drawn a lot of interest but requires many structural assumptions\nand is tractable only in small dimensions. We phrase this problem as a Mean\nField Game (MFG), where each individual chooses its acceleration depending on\nthe population behavior. Combining Deep Reinforcement Learning (RL) and\nNormalizing Flows (NF), we obtain a tractable solution requiring only very weak\nassumptions. Our algorithm finds a Nash Equilibrium and the agents adapt their\nvelocity to match the neighboring flock's average one. We use Fictitious Play\nand alternate: (1) computing an approximate best response with Deep RL, and (2)\nestimating the next population distribution with NF. We show numerically that\nour algorithm learn multi-group or high-dimensional flocking with obstacles.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:17:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Perrin", "Sarah", ""], ["Lauri\u00e8re", "Mathieu", ""], ["P\u00e9rolat", "Julien", ""], ["Geist", "Matthieu", ""], ["\u00c9lie", "Romuald", ""], ["Pietquin", "Olivier", ""]]}, {"id": "2105.07946", "submitter": "Federico Mason", "authors": "Federico Mason, Gianfranco Nencioni, Andrea Zanella", "title": "Using Distributed Reinforcement Learning for Resource Orchestration in a\n  Network Slicing Scenario", "comments": "14 pages, 11 figures, 4 tables. This paper is under review at IEEE\n  Transaction on Networking. Copyright IEEE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network Slicing (NS) paradigm enables the partition of physical and\nvirtual resources among multiple logical networks, possibly managed by\ndifferent tenants. In such a scenario, network resources need to be dynamically\nallocated according to the slices' requirements. In this paper, we attack the\nabove problem by exploiting a Deep Reinforcement Learning approach. Our\nframework is based on a distributed architecture, where multiple agents\ncooperate towards a common goal. The agents' training is carried out following\nthe Advantage Actor Critic algorithm, which allows to handle continuous action\nspaces. By means of extensive simulations, we show that our approach yields\nbetter performance than both a static allocation of system resources and an\nefficient empirical strategy. At the same time, the proposed system ensures\nhigh adaptability to different scenarios without the need for additional\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:34:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mason", "Federico", ""], ["Nencioni", "Gianfranco", ""], ["Zanella", "Andrea", ""]]}, {"id": "2105.08110", "submitter": "Guangzhao Cheng", "authors": "Guangzhao Cheng and Siliang Tang", "title": "To be a fast adaptive learner: using game history to defeat opponents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world games, such as traders repeatedly bargaining with\ncustomers, it is very hard for a single AI trader to make good deals with\nvarious customers in a few turns, since customers may adopt different\nstrategies even the strategies they choose are quite simple. In this paper, we\nmodel this problem as fast adaptive learning in the finitely repeated games. We\nbelieve that past game history plays a vital role in such a learning procedure,\nand therefore we propose a novel framework (named, F3) to fuse the past and\ncurrent game history with an Opponent Action Estimator (OAE) module that uses\npast game history to estimate the opponent's future behaviors. The experiments\nshow that the agent trained by F3 can quickly defeat opponents who adopt\nunknown new strategies. The F3 trained agent obtains more rewards in a fixed\nnumber of turns than the agents that are trained by deep reinforcement\nlearning. Further studies show that the OAE module in F3 contains\nmeta-knowledge that can even be transferred across different games.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:40:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cheng", "Guangzhao", ""], ["Tang", "Siliang", ""]]}, {"id": "2105.08158", "submitter": "Tao Li", "authors": "Tao Li, Guanze Peng, Quanyan Zhu and Tamer Basar", "title": "The Confluence of Networks, Games and Learning", "comments": "The manuscript has been submitted to IEEE control system magazine\n  under review, as part of the special issue \"Distributed Nash Equilibrium\n  Seeking over Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed significant advances in technologies and services\nin modern network applications, including smart grid management, wireless\ncommunication, cybersecurity as well as multi-agent autonomous systems.\nConsidering the heterogeneous nature of networked entities, emerging network\napplications call for game-theoretic models and learning-based approaches in\norder to create distributed network intelligence that responds to uncertainties\nand disruptions in a dynamic or an adversarial environment. This paper\narticulates the confluence of networks, games and learning, which establishes a\ntheoretical underpinning for understanding multi-agent decision-making over\nnetworks. We provide an selective overview of game-theoretic learning\nalgorithms within the framework of stochastic approximation theory, and\nassociated applications in some representative contexts of modern network\nsystems, such as the next generation wireless communication networks, the smart\ngrid and distributed machine learning. In addition to existing research works\non game-theoretic learning over networks, we highlight several new angles and\nresearch endeavors on learning in games that are related to recent developments\nin artificial intelligence. Some of the new angles extrapolate from our own\nresearch interests. The overall objective of the paper is to provide the reader\na clear picture of the strengths and challenges of adopting game-theoretic\nlearning methods within the context of network systems, and further to identify\nfruitful future research directions on both theoretical and applied studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 20:54:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Tao", ""], ["Peng", "Guanze", ""], ["Zhu", "Quanyan", ""], ["Basar", "Tamer", ""]]}, {"id": "2105.08187", "submitter": "Rafa{\\l} Muszy\\'nski", "authors": "Rafal Muszynski, Katja Hofmann, Jun Wang", "title": "Learning to Win, Lose and Cooperate through Reward Signal Evolution", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solving a reinforcement learning problem typically involves correctly\nprespecifying the reward signal from which the algorithm learns. Here, we\napproach the problem of reward signal design by using an evolutionary approach\nto perform a search on the space of all possible reward signals. We introduce a\ngeneral framework for optimizing $N$ goals given $n$ reward signals. Through\nexperiments we demonstrate that such an approach allows agents to learn\nhigh-level goals - such as winning, losing and cooperating - from scratch\nwithout prespecified reward signals in the game of Pong. Some of the solutions\nfound by the algorithm are surprising, in the sense that they would probably\nnot have been chosen by a person trying to hand-code a given behaviour through\na specific reward signal. Furthermore, it seems that the proposed approach may\nalso benefit from higher stability of the training performance when compared\nwith the typical score-based reward signals.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:50:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Muszynski", "Rafal", ""], ["Hofmann", "Katja", ""], ["Wang", "Jun", ""]]}, {"id": "2105.08268", "submitter": "Yan  Li", "authors": "Yan Li, Lingxiao Wang, Jiachen Yang, Ethan Wang, Zhaoran Wang, Tuo\n  Zhao, Hongyuan Zha", "title": "Permutation Invariant Policy Optimization for Mean-Field Multi-Agent\n  Reinforcement Learning: A Principled Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) becomes more challenging in the\npresence of more agents, as the capacity of the joint state and action spaces\ngrows exponentially in the number of agents. To address such a challenge of\nscale, we identify a class of cooperative MARL problems with permutation\ninvariance, and formulate it as a mean-field Markov decision processes (MDP).\nTo exploit the permutation invariance therein, we propose the mean-field\nproximal policy optimization (MF-PPO) algorithm, at the core of which is a\npermutation-invariant actor-critic neural architecture. We prove that MF-PPO\nattains the globally optimal policy at a sublinear rate of convergence.\nMoreover, its sample complexity is independent of the number of agents. We\nvalidate the theoretical advantages of MF-PPO with numerical experiments in the\nmulti-agent particle environment (MPE). In particular, we show that the\ninductive bias introduced by the permutation-invariant neural architecture\nenables MF-PPO to outperform existing competitors with a smaller number of\nmodel parameters, which is the key to its generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:35:41 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Yan", ""], ["Wang", "Lingxiao", ""], ["Yang", "Jiachen", ""], ["Wang", "Ethan", ""], ["Wang", "Zhaoran", ""], ["Zhao", "Tuo", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2105.08310", "submitter": "Dave Cliff", "authors": "Dave Cliff", "title": "BBE: Simulating the Microstructural Dynamics of an In-Play Betting\n  Exchange via Agent-Based Modelling", "comments": "47 pages, 9 figures, 120 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CE q-fin.CP q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I describe the rationale for, and design of, an agent-based simulation model\nof a contemporary online sports-betting exchange: such exchanges, closely\nrelated to the exchange mechanisms at the heart of major financial markets,\nhave revolutionized the gambling industry in the past 20 years, but gathering\nsufficiently large quantities of rich and temporally high-resolution data from\nreal exchanges - i.e., the sort of data that is needed in large quantities for\nDeep Learning - is often very expensive, and sometimes simply impossible; this\ncreates a need for a plausibly realistic synthetic data generator, which is\nwhat this simulation now provides. The simulator, named the \"Bristol Betting\nExchange\" (BBE), is intended as a common platform, a data-source and\nexperimental test-bed, for researchers studying the application of AI and\nmachine learning (ML) techniques to issues arising in betting exchanges; and,\nas far as I have been able to determine, BBE is the first of its kind: a free\nopen-source agent-based simulation model consisting not only of a\nsports-betting exchange, but also a minimal simulation model of racetrack\nsporting events (e.g., horse-races or car-races) about which bets may be made,\nand a population of simulated bettors who each form their own private\nevaluation of odds and place bets on the exchange before and - crucially -\nduring the race itself (i.e., so-called \"in-play\" betting) and whose betting\nopinions change second-by-second as each race event unfolds. BBE is offered as\na proof-of-concept system that enables the generation of large high-resolution\ndata-sets for automated discovery or improvement of profitable strategies for\nbetting on sporting events via the application of AI/ML and advanced data\nanalytics techniques. This paper offers an extensive survey of relevant\nliterature and explains the motivation and design of BBE, and presents brief\nillustrative results.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:52:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cliff", "Dave", ""]]}, {"id": "2105.08540", "submitter": "Zack Fitzsimmons", "authors": "Zack Fitzsimmons and Edith Hemaspaandra", "title": "Kemeny Consensus Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational study of election problems generally focuses on questions\nrelated to the winner or set of winners of an election. But social preference\nfunctions such as Kemeny rule output a full ranking of the candidates (a\nconsensus). We study the complexity of consensus-related questions, with a\nparticular focus on Kemeny and its qualitative version Slater. The simplest of\nthese questions is the problem of determining whether a ranking is a consensus,\nand we show that this problem is coNP-complete. We also study the natural\nquestion of the complexity of manipulative actions that have a specific\nconsensus as a goal. Though determining whether a ranking is a Kemeny consensus\nis hard, the optimal action for manipulators is to simply vote their desired\nconsensus. We provide evidence that this simplicity is caused by the\ncombination of election system (Kemeny), manipulative action (manipulation),\nand manipulative goal (consensus). In the process we provide the first\ncompleteness results at the second level of the polynomial hierarchy for\nelectoral manipulation and for optimal solution recognition.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:15:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fitzsimmons", "Zack", ""], ["Hemaspaandra", "Edith", ""]]}, {"id": "2105.08601", "submitter": "Lifeng Zhou", "authors": "Lifeng Zhou, Vishnu D. Sharma, Qingbiao Li, Amanda Prorok, Alejandro\n  Ribeiro, Vijay Kumar", "title": "Graph Neural Networks for Decentralized Multi-Robot Submodular Action\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a learning-based approach for decentralized\nsubmodular maximization. We focus on applications where robots are required to\njointly select actions, e.g., motion primitives, to maximize team submodular\nobjectives with local communications only. Such applications are essential for\nlarge-scale multi-robot coordination such as multi-robot motion planning for\narea coverage, environment exploration, and target tracking. But the current\ndecentralized submodular maximization algorithms either require assumptions on\nthe inter-robot communication or lose some suboptimal guarantees. In this work,\nwe propose a general-purpose learning architecture towards submodular\nmaximization at scale, with decentralized communications. Particularly, our\nlearning architecture leverages a graph neural network (GNN) to capture local\ninteractions of the robots and learns decentralized decision-making for the\nrobots. We train the learning model by imitating an expert solution and\nimplement the resulting model for decentralized action selection involving\nlocal observations and communications only. We demonstrate the performance of\nour GNN-based learning approach in a scenario of active target coverage with\nlarge networks of robots. The simulation results show our approach nearly\nmatches the coverage performance of the expert algorithm, and yet runs several\norders faster with more than 30 robots. The results also exhibit our approach's\ngeneralization capability in previously unseen scenarios, e.g., larger\nenvironments and larger networks of robots.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:32:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhou", "Lifeng", ""], ["Sharma", "Vishnu D.", ""], ["Li", "Qingbiao", ""], ["Prorok", "Amanda", ""], ["Ribeiro", "Alejandro", ""], ["Kumar", "Vijay", ""]]}, {"id": "2105.09386", "submitter": "Debmalya Mandal", "authors": "Hadi Hosseini, Debmalya Mandal, Nisarg Shah, and Kevin Shi", "title": "Surprisingly Popular Voting Recovers Rankings, Surprisingly!", "comments": "Forthcoming at IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The wisdom of the crowd has long become the de facto approach for eliciting\ninformation from individuals or experts in order to predict the ground truth.\nHowever, classical democratic approaches for aggregating individual\n\\emph{votes} only work when the opinion of the majority of the crowd is\nrelatively accurate. A clever recent approach, \\emph{surprisingly popular\nvoting}, elicits additional information from the individuals, namely their\n\\emph{prediction} of other individuals' votes, and provably recovers the ground\ntruth even when experts are in minority. This approach works well when the goal\nis to pick the correct option from a small list, but when the goal is to\nrecover a true ranking of the alternatives, a direct application of the\napproach requires eliciting too much information. We explore practical\ntechniques for extending the surprisingly popular algorithm to ranked voting by\npartial votes and predictions and designing robust aggregation rules. We\nexperimentally demonstrate that even a little prediction information helps\nsurprisingly popular voting outperform classical approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:31:23 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hosseini", "Hadi", ""], ["Mandal", "Debmalya", ""], ["Shah", "Nisarg", ""], ["Shi", "Kevin", ""]]}, {"id": "2105.09764", "submitter": "Tuomas Takko M.Sc.", "authors": "Tuomas Takko, Kunal Bhattacharya, Daniel Monsivais and Kimmo Kaski", "title": "Human-agent coordination in a group formation game", "comments": "Chosen to be published in Scientific Reports on 24.05.2021 with DOI:\n  10.1038/s41598-021-90123-8", "journal-ref": null, "doi": "10.1038/s41598-021-90123-8", "report-no": null, "categories": "physics.soc-ph cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coordination and cooperation between humans and autonomous agents in\ncooperative games raises interesting questions of human decision making and\nbehaviour changes. Here we report our findings from a group formation game in a\nsmall-world network of different mixes of human and agent players, aiming to\nachieve connected clusters of the same colour by swapping places with\nneighbouring players using non-overlapping information. In the experiments the\nhuman players are incentivized by rewarding to prioritize their own cluster\nwhile the model of agents' decision making is derived from our previous\nexperiment of purely cooperative game between human players. The experiments\nwere performed by grouping the players in three different setups to investigate\nthe overall effect of having cooperative autonomous agents within teams. We\nobserve that the change in the behavior of human subjects adjusts to playing\nwith autonomous agents by being less risk averse, while keeping the overall\nperformance efficient by splitting the behaviour into selfish and cooperative\nin the two actions performed during the rounds of the game. Moreover, results\nfrom two hybrid human-agent setups suggest that the group composition affects\nthe evolution of clusters. Our findings indicate that in purely or lesser\ncooperative settings, providing more control to humans could help in maximizing\nthe overall performance of hybrid systems.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:11:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Takko", "Tuomas", ""], ["Bhattacharya", "Kunal", ""], ["Monsivais", "Daniel", ""], ["Kaski", "Kimmo", ""]]}, {"id": "2105.09976", "submitter": "Gaia Belardinelli", "authors": "Gaia Belardinelli and Rasmus K. Rendsvig", "title": "Epistemic Planning with Attention as a Bounded Resource", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA econ.TH math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Where information grows abundant, attention becomes a scarce resource. As a\nresult, agents must plan wisely how to allocate their attention in order to\nachieve epistemic efficiency. Here, we present a framework for multi-agent\nepistemic planning with attention, based on Dynamic Epistemic Logic (DEL, a\npowerful formalism for epistemic planning). We identify the framework as a\nfragment of standard DEL, and consider its plan existence problem. While in the\ngeneral case undecidable, we show that when attention is required for learning,\nall instances of the problem are decidable.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 18:14:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Belardinelli", "Gaia", ""], ["Rendsvig", "Rasmus K.", ""]]}, {"id": "2105.10339", "submitter": "Olalekan Lekings Abdulrasheed Ogunjimi Dr", "authors": "J. A. Sarumi, E. C. Onwubiko, O. L. A. Ogunjimi", "title": "Infection in a Confined Space using an Agent-Based Model", "comments": "17 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examined a simulated confined space modelled as a hospital waiting\narea, where people who could have underlying conditions congregate and mix with\npotentially infectious individuals. It further investigated the impact of the\nvolume of the waiting area, the number of people in the room, the placement of\nthem as well as their weight. The simulation is an agent-based model (ABM).\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 00:55:34 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sarumi", "J. A.", ""], ["Onwubiko", "E. C.", ""], ["Ogunjimi", "O. L. A.", ""]]}, {"id": "2105.10423", "submitter": "Yongzhao Wang", "authors": "Yongzhao Wang, Qiurui Ma, Michael P. Wellman", "title": "Evaluating Strategy Exploration in Empirical Game-Theoretic Analysis", "comments": "23 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In empirical game-theoretic analysis (EGTA), game models are extended\niteratively through a process of generating new strategies based on learning\nfrom experience with prior strategies. The strategy exploration problem in EGTA\nis how to direct this process so to construct effective models with minimal\niteration. A variety of approaches have been proposed in the literature,\nincluding methods based on classic techniques and novel concepts. Comparing the\nperformance of these alternatives can be surprisingly subtle, depending\nsensitively on criteria adopted and measures employed. We investigate some of\nthe methodological considerations in evaluating strategy exploration, defining\nkey distinctions and identifying a few general principles based on examples and\nexperimental observations. In particular, we emphasize the fact that empirical\ngames create a space of strategies that should be evaluated as a whole. Based\non this fact, we suggest that the minimum regret constrained profile (MRCP)\nprovides a particularly robust basis for evaluating a space of strategies, and\npropose a local search method for MRCP that outperforms previous approaches.\nHowever, the computation of MRCP is not always feasible especially in large\ngames. In this scenario, we highlight consistency considerations for comparing\nacross different approaches. Surprisingly, we find that recent works violate\nthese considerations that are necessary for evaluation, which may result in\nmisleading conclusions on the performance of different approaches. For proper\nevaluation, we propose a new evaluation scheme and demonstrate that our scheme\ncan reveal the true learning performance of different approaches compared to\nprevious evaluation methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:47:49 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wang", "Yongzhao", ""], ["Ma", "Qiurui", ""], ["Wellman", "Michael P.", ""]]}, {"id": "2105.10605", "submitter": "Jayson Boubin", "authors": "Jayson Boubin, Codi Burley, Peida Han, Bowen Li, Barry Porter,\n  Christopher Stewart", "title": "Programming and Deployment of Autonomous Swarms using Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous systems (AS) carry out complex missions by continuously observing\nthe state of their surroundings and taking actions toward a goal. Swarms of AS\nworking together can complete missions faster and more effectively than single\nAS alone. To build swarms today, developers handcraft their own software for\nstoring, aggregating, and learning from observations. We present the Fleet\nComputer, a platform for developing and managing swarms. The Fleet Computer\nprovides a programming paradigm that simplifies multi-agent reinforcement\nlearning (MARL) -- an emerging class of algorithms that coordinate swarms of\nagents. Using just two programmer-provided functions Map() and Eval(), the\nFleet Computer compiles and deploys swarms and continuously updates the\nreinforcement learning models that govern actions. To conserve compute\nresources, the Fleet Computer gives priority scheduling to models that\ncontribute to effective actions, drawing a novel link between online learning\nand resource management. We developed swarms for unmanned aerial vehicles (UAV)\nin agriculture and for video analytics on urban traffic. Compared to individual\nAS, our swarms achieved speedup of 4.4X using 4 UAV and 62X using 130 video\ncameras. Compared to a competing approach for building swarms that is widely\nused in practice, our swarms were 3X more effective, using 3.9X less energy.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:22:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Boubin", "Jayson", ""], ["Burley", "Codi", ""], ["Han", "Peida", ""], ["Li", "Bowen", ""], ["Porter", "Barry", ""], ["Stewart", "Christopher", ""]]}, {"id": "2105.10626", "submitter": "Yuhao Huang", "authors": "Xin Yang, Yuhao Huang, Ruobing Huang, Haoran Dou, Rui Li, Jikuan Qian,\n  Xiaoqiong Huang, Wenlong Shi, Chaoyu Chen, Yuanji Zhang, Haixia Wang, Yi\n  Xiong, Dong Ni", "title": "Searching Collaborative Agents for Multi-plane Localization in 3D\n  Ultrasound", "comments": "Accepted by Medical Image Analysis (10 figures, 8 tabels)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D ultrasound (US) has become prevalent due to its rich spatial and\ndiagnostic information not contained in 2D US. Moreover, 3D US can contain\nmultiple standard planes (SPs) in one shot. Thus, automatically localizing SPs\nin 3D US has the potential to improve user-independence and\nscanning-efficiency. However, manual SP localization in 3D US is challenging\nbecause of the low image quality, huge search space and large anatomical\nvariability. In this work, we propose a novel multi-agent reinforcement\nlearning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our\ncontribution is four-fold. First, our proposed method is general and it can\naccurately localize multiple SPs in different challenging US datasets. Second,\nwe equip the MARL system with a recurrent neural network (RNN) based\ncollaborative module, which can strengthen the communication among agents and\nlearn the spatial relationship among planes effectively. Third, we explore to\nadopt the neural architecture search (NAS) to automatically design the network\narchitecture of both the agents and the collaborative module. Last, we believe\nwe are the first to realize automatic SP localization in pelvic US volumes, and\nnote that our approach can handle both normal and abnormal uterus cases.\nExtensively validated on two challenging datasets of the uterus and fetal\nbrain, our proposed method achieves the average localization accuracy of 7.03\ndegrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our\nlight-weight MARL model has higher accuracy than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:48:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Xin", ""], ["Huang", "Yuhao", ""], ["Huang", "Ruobing", ""], ["Dou", "Haoran", ""], ["Li", "Rui", ""], ["Qian", "Jikuan", ""], ["Huang", "Xiaoqiong", ""], ["Shi", "Wenlong", ""], ["Chen", "Chaoyu", ""], ["Zhang", "Yuanji", ""], ["Wang", "Haixia", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2105.10639", "submitter": "Mohammadreza Doostmohammadian", "authors": "Mohammadreza Doostmohammadian, Themistoklis Charalambous, Miadreza\n  Shafie-khah, Nader Meskin, Usman A. Khan", "title": "Simultaneous Distributed Estimation and Attack Detection/Isolation in\n  Social Networks: Structural Observability, Kronecker-Product Network, and\n  Chi-Square Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SI cs.SY math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers distributed estimation of linear systems when the state\nobservations are corrupted with Gaussian noise of unbounded support and under\npossible random adversarial attacks. We consider sensors equipped with single\ntime-scale estimators and local chi-square ($\\chi^2$) detectors to\nsimultaneously opserve the states, share information, fuse the\nnoise/attack-corrupted data locally, and detect possible anomalies in their own\nobservations. While this scheme is applicable to a wide variety of systems\nassociated with full-rank (invertible) matrices, we discuss it within the\ncontext of distributed inference in social networks. The proposed technique\noutperforms existing results in the sense that: (i) we consider Gaussian noise\nwith no simplifying upper-bound assumption on the support; (ii) all existing\n$\\chi^2$-based techniques are centralized while our proposed technique is\ndistributed, where the sensors \\textit{locally} detect attacks, with no central\ncoordinator, using specific probabilistic thresholds; and (iii) no\nlocal-observability assumption at a sensor is made, which makes our method\nfeasible for large-scale social networks. Moreover, we consider a Linear Matrix\nInequalities (LMI) approach to design block-diagonal gain (estimator) matrices\nunder appropriate constraints for isolating the attacks.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 04:59:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Doostmohammadian", "Mohammadreza", ""], ["Charalambous", "Themistoklis", ""], ["Shafie-khah", "Miadreza", ""], ["Meskin", "Nader", ""], ["Khan", "Usman A.", ""]]}, {"id": "2105.10716", "submitter": "Won Joon Yun", "authors": "Won Joon Yun, Byungju Lim, Soyi Jung, Young-Chai Ko, Jihong Park,\n  Joongheon Kim, Mehdi Bennis", "title": "Attention-based Reinforcement Learning for Real-Time UAV Semantic\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we study the problem of air-to-ground ultra-reliable and\nlow-latency communication (URLLC) for a moving ground user. This is done by\ncontrolling multiple unmanned aerial vehicles (UAVs) in real time while\navoiding inter-UAV collisions. To this end, we propose a novel multi-agent deep\nreinforcement learning (MADRL) framework, coined a graph attention exchange\nnetwork (GAXNet). In GAXNet, each UAV constructs an attention graph locally\nmeasuring the level of attention to its neighboring UAVs, while exchanging the\nattention weights with other UAVs so as to reduce the attention mismatch\nbetween them. Simulation results corroborates that GAXNet achieves up to 4.5x\nhigher rewards during training. At execution, without incurring inter-UAV\ncollisions, GAXNet achieves 6.5x lower latency with the target 0.0000001 error\nrate, compared to a state-of-the-art baseline framework.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:43:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yun", "Won Joon", ""], ["Lim", "Byungju", ""], ["Jung", "Soyi", ""], ["Ko", "Young-Chai", ""], ["Park", "Jihong", ""], ["Kim", "Joongheon", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2105.10907", "submitter": "Unnikrishnan Menon", "authors": "Unnikrishnan Rajendran Menon and Anirudh Rajiv Menon", "title": "An Efficient Application of Neuroevolution for Competitive Multiagent\n  Learning", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": "Transactions on Machine Learning and Artificial Intelligence,\n  9(3), 1-13 (2021)", "doi": "10.14738/tmlai.93.10149", "report-no": "TMLAI-10149", "categories": "cs.AI cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiagent systems provide an ideal environment for the evaluation and\nanalysis of real-world problems using reinforcement learning algorithms. Most\ntraditional approaches to multiagent learning are affected by long training\nperiods as well as high computational complexity. NEAT (NeuroEvolution of\nAugmenting Topologies) is a popular evolutionary strategy used to obtain the\nbest performing neural network architecture often used to tackle optimization\nproblems in the field of artificial intelligence. This paper utilizes the NEAT\nalgorithm to achieve competitive multiagent learning on a modified pong game\nenvironment in an efficient manner. The competing agents abide by different\nrules while having similar observation space parameters. The proposed algorithm\nutilizes this property of the environment to define a singular\nneuroevolutionary procedure that obtains the optimal policy for all the agents.\nThe compiled results indicate that the proposed implementation achieves ideal\nbehaviour in a very short training period when compared to existing multiagent\nreinforcement learning models.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 10:34:48 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Menon", "Unnikrishnan Rajendran", ""], ["Menon", "Anirudh Rajiv", ""]]}, {"id": "2105.10993", "submitter": "Nir Greshler", "authors": "Nir Greshler, Ofir Gordon, Oren Salzman, and Nahum Shimkin", "title": "Cooperative Multi-Agent Path Finding: Beyond Path Planning and Collision\n  Avoidance", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Cooperative Multi-Agent Path Finding (Co-MAPF) problem, an\nextension to the classical MAPF problem, where cooperative behavior is\nincorporated. In this setting, a group of autonomous agents operate in a shared\nenvironment and have to complete cooperative tasks while avoiding collisions\nwith the other agents in the group. This extension naturally models many\nreal-world applications, where groups of agents are required to collaborate in\norder to complete a given task. To this end, we formalize the Co-MAPF problem\nand introduce Cooperative Conflict-Based Search (Co-CBS), a CBS-based algorithm\nfor solving the problem optimally for a wide set of Co-MAPF problems. Co-CBS\nuses a cooperation-planning module integrated into CBS such that cooperation\nplanning is decoupled from path planning. Finally, we present empirical results\non several MAPF benchmarks demonstrating our algorithm's properties.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 18:25:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Greshler", "Nir", ""], ["Gordon", "Ofir", ""], ["Salzman", "Oren", ""], ["Shimkin", "Nahum", ""]]}, {"id": "2105.11611", "submitter": "Zijian Gao", "authors": "Zijian Gao, Kele Xu, Bo Ding, Huaimin Wang, Yiying Li, Hongda Jia", "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep reinforcement learning (RL) algorithms have made great\nprogress in multi-agent domain. However, due to characteristics of RL, training\nfor complex tasks would be resource-intensive and time-consuming. To meet this\nchallenge, mutual learning strategy between homogeneous agents is essential,\nwhich is under-explored in previous studies, because most existing methods do\nnot consider to use the knowledge of agent models. In this paper, we present an\nadaptation method of the majority of multi-agent reinforcement learning (MARL)\nalgorithms called KnowSR which takes advantage of the differences in learning\nbetween agents. We employ the idea of knowledge distillation (KD) to share\nknowledge among agents to shorten the training phase. To empirically\ndemonstrate the robustness and effectiveness of KnowSR, we performed extensive\nexperiments on state-of-the-art MARL algorithms in collaborative and\ncompetitive scenarios. The results demonstrate that KnowSR outperforms recently\nreported methodologies, emphasizing the importance of the proposed knowledge\nsharing for MARL.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:19:41 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gao", "Zijian", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Li", "Yiying", ""], ["Jia", "Hongda", ""]]}, {"id": "2105.11760", "submitter": "Igor Balaz", "authors": "Igor Balaz, Tara Petric, Namid Stillman", "title": "Towards open-ended evolutionary simulator for developing novel tumour\n  drug delivery systems", "comments": "Accepted extended abstract for the ALIFE2021 conference (July 19-23,\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tumours behave as moving targets that can evade chemotherapeutic treatments\nby rapidly acquiring resistance via various mechanisms. In Balaz et al. (2021,\nBiosystems; 199:104290) we initiated the development of the agent-based\nopen-ended evolutionary simulator of novel drug delivery systems (DDS). It is\nan agent-based simulator where evolvable agents can change their perception of\nthe environment and thus adapt to tumour mutations. Here we mapped the\nparameters of evolvable agent properties to the realistic biochemical\nboundaries and test their efficacy by simulating their behaviour at the cell\nscale using the stochastic simulator, STEPS. We show that the shape of the\nparameter space evolved in our simulator is comparable to those obtained by the\nrational design.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:54:17 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Balaz", "Igor", ""], ["Petric", "Tara", ""], ["Stillman", "Namid", ""]]}, {"id": "2105.11923", "submitter": "Stanis{\\l}aw Szufa", "authors": "Stanis{\\l}aw Szufa, Piotr Faliszewski", "title": "The Complexity of Subelection Isomorphism Problems", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study extensions of the Election Isomorphism problem, focused on the\nexistence of isomorphic subelections. Specifically, we propose the Subelection\nIsomorphism and the Maximum Common Subelection problems and study their\ncomputational complexity, mostly showing intractability. Additionally, we show\nexperiments indicating that our problems provide meaningful insights into the\nnature of a number of statistical models of elections.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:27:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Szufa", "Stanis\u0142aw", ""], ["Faliszewski", "Piotr", ""]]}, {"id": "2105.11999", "submitter": "Arjun Balasingam", "authors": "Arjun Balasingam, Karthik Gopalakrishnan, Radhika Mittal, Venkat Arun,\n  Ahmed Saeed, Mohammad Alizadeh, Hamsa Balakrishnan, Hari Balakrishnan", "title": "Throughput-Fairness Tradeoffs in Mobility Platforms", "comments": "Technical report for paper to appear at ACM MobiSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MA cs.NI cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the problem of allocating tasks from different customers\nto vehicles in mobility platforms, which are used for applications like food\nand package delivery, ridesharing, and mobile sensing. A mobility platform\nshould allocate tasks to vehicles and schedule them in order to optimize both\nthroughput and fairness across customers. However, existing approaches to\nscheduling tasks in mobility platforms ignore fairness.\n  We introduce Mobius, a system that uses guided optimization to achieve both\nhigh throughput and fairness across customers. Mobius supports spatiotemporally\ndiverse and dynamic customer demands. It provides a principled method to\nnavigate inherent tradeoffs between fairness and throughput caused by shared\nmobility. Our evaluation demonstrates these properties, along with the\nversatility and scalability of Mobius, using traces gathered from ridesharing\nand aerial sensing applications. Our ridesharing case study shows that Mobius\ncan schedule more than 16,000 tasks across 40 customers and 200 vehicles in an\nonline manner.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:04:04 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Balasingam", "Arjun", ""], ["Gopalakrishnan", "Karthik", ""], ["Mittal", "Radhika", ""], ["Arun", "Venkat", ""], ["Saeed", "Ahmed", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hamsa", ""], ["Balakrishnan", "Hari", ""]]}, {"id": "2105.12196", "submitter": "Siqi Liu", "authors": "Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S. M. Ali Eslami, Daniel\n  Hennes, Wojciech M. Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas\n  Abdolmaleki, Noah Y. Siegel, Leonard Hasenclever, Luke Marris, Saran\n  Tunyasuvunakool, H. Francis Song, Markus Wulfmeier, Paul Muller, Tuomas\n  Haarnoja, Brendan D. Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess", "title": "From Motor Control to Team Play in Simulated Humanoid Football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent behaviour in the physical world exhibits structure at multiple\nspatial and temporal scales. Although movements are ultimately executed at the\nlevel of instantaneous muscle tensions or joint torques, they must be selected\nto serve goals defined on much longer timescales, and in terms of relations\nthat extend far beyond the body itself, ultimately involving coordination with\nother agents. Recent research in artificial intelligence has shown the promise\nof learning-based approaches to the respective problems of complex movement,\nlonger-term planning and multi-agent coordination. However, there is limited\nresearch aimed at their integration. We study this problem by training teams of\nphysically simulated humanoid avatars to play football in a realistic virtual\nenvironment. We develop a method that combines imitation learning, single- and\nmulti-agent reinforcement learning and population-based training, and makes use\nof transferable representations of behaviour for decision making at different\nlevels of abstraction. In a sequence of stages, players first learn to control\na fully articulated body to perform realistic, human-like movements such as\nrunning and turning; they then acquire mid-level football skills such as\ndribbling and shooting; finally, they develop awareness of others and play as a\nteam, bridging the gap between low-level motor control at a timescale of\nmilliseconds, and coordinated goal-directed behaviour as a team at the\ntimescale of tens of seconds. We investigate the emergence of behaviours at\ndifferent levels of abstraction, as well as the representations that underlie\nthese behaviours using several analysis techniques, including statistics from\nreal-world sports analytics. Our work constitutes a complete demonstration of\nintegrated decision-making at multiple scales in a physically embodied\nmulti-agent setting. See project video at https://youtu.be/KHMwq9pv7mg.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:17:10 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Siqi", ""], ["Lever", "Guy", ""], ["Wang", "Zhe", ""], ["Merel", "Josh", ""], ["Eslami", "S. M. Ali", ""], ["Hennes", "Daniel", ""], ["Czarnecki", "Wojciech M.", ""], ["Tassa", "Yuval", ""], ["Omidshafiei", "Shayegan", ""], ["Abdolmaleki", "Abbas", ""], ["Siegel", "Noah Y.", ""], ["Hasenclever", "Leonard", ""], ["Marris", "Luke", ""], ["Tunyasuvunakool", "Saran", ""], ["Song", "H. Francis", ""], ["Wulfmeier", "Markus", ""], ["Muller", "Paul", ""], ["Haarnoja", "Tuomas", ""], ["Tracey", "Brendan D.", ""], ["Tuyls", "Karl", ""], ["Graepel", "Thore", ""], ["Heess", "Nicolas", ""]]}, {"id": "2105.12500", "submitter": "Noam Hazon", "authors": "David Zar, Noam Hazon, Amos Azaria", "title": "Explaining Ridesharing: Selection of Explanations for Increasing User\n  Satisfaction", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transportation services play a crucial part in the development of modern\nsmart cities. In particular, on-demand ridesharing services, which group\ntogether passengers with similar itineraries, are already operating in several\nmetropolitan areas. These services can be of significant social and\nenvironmental benefit, by reducing travel costs, road congestion and CO2\nemissions.\n  Unfortunately, despite their advantages, not many people opt to use these\nridesharing services. We believe that increasing the user satisfaction from the\nservice will cause more people to utilize it, which, in turn, will improve the\nquality of the service, such as the waiting time, cost, travel time, and\nservice availability. One possible way for increasing user satisfaction is by\nproviding appropriate explanations comparing the alternative modes of\ntransportation, such as a private taxi ride and public transportation. For\nexample, a passenger may be more satisfied from a shared-ride if she is told\nthat a private taxi ride would have cost her 50% more. Therefore, the problem\nis to develop an agent that provides explanations that will increase the user\nsatisfaction.\n  We model our environment as a signaling game and show that a rational agent,\nwhich follows the perfect Bayesian equilibrium, must reveal all of the\ninformation regarding the possible alternatives to the passenger. In addition,\nwe develop a machine learning based agent that, when given a shared-ride along\nwith its possible alternatives, selects the explanations that are most likely\nto increase user satisfaction. Using feedback from humans we show that our\nmachine learning based agent outperforms the rational agent and an agent that\nrandomly chooses explanations, in terms of user satisfaction.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 12:03:09 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zar", "David", ""], ["Hazon", "Noam", ""], ["Azaria", "Amos", ""]]}, {"id": "2105.12903", "submitter": "MIngchao Liang", "authors": "Mingchao Liang, Florian Meyer", "title": "Neural Enhanced Belief Propagation for Cooperative Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-aware networks will introduce innovative services and applications\nfor modern convenience, applied ocean sciences, and public safety. In this\npaper, we establish a hybrid method for model-based and data-driven inference.\nWe consider a cooperative localization (CL) scenario where the mobile agents in\na wireless network aim to localize themselves by performing pairwise\nobservations with other agents and by exchanging location information. A\ntraditional method for distributed CL in large agent networks is belief\npropagation (BP) which is completely model-based and is known to suffer from\nproviding inconsistent (overconfident) estimates. The proposed approach\naddresses these limitations by complementing BP with learned information\nprovided by a graph neural network (GNN). We demonstrate numerically that our\nmethod can improve estimation accuracy and avoid overconfident beliefs, while\nits computational complexity remains comparable to BP. Notably, more consistent\nbeliefs are obtained by not explicitly addressing overconfidence in the loss\nfunction used for training of the GNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:42:54 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liang", "Mingchao", ""], ["Meyer", "Florian", ""]]}, {"id": "2105.13284", "submitter": "David Biagioni", "authors": "Erotokritos Skordilis, Yi Hou, Charles Tripp, Matthew Moniot, Peter\n  Graf, David Biagioni", "title": "A Modular and Transferable Reinforcement Learning Framework for the\n  Fleet Rebalancing Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobility on demand (MoD) systems show great promise in realizing flexible and\nefficient urban transportation. However, significant technical challenges arise\nfrom operational decision making associated with MoD vehicle dispatch and fleet\nrebalancing. For this reason, operators tend to employ simplified algorithms\nthat have been demonstrated to work well in a particular setting. To help\nbridge the gap between novel and existing methods, we propose a modular\nframework for fleet rebalancing based on model-free reinforcement learning (RL)\nthat can leverage an existing dispatch method to minimize system cost. In\nparticular, by treating dispatch as part of the environment dynamics, a\ncentralized agent can learn to intermittently direct the dispatcher to\nreposition free vehicles and mitigate against fleet imbalance. We formulate RL\nstate and action spaces as distributions over a grid partitioning of the\noperating area, making the framework scalable and avoiding the complexities\nassociated with multiagent RL. Numerical experiments, using real-world trip and\nnetwork data, demonstrate that this approach has several distinct advantages\nover baseline methods including: improved system cost; high degree of\nadaptability to the selected dispatch method; and the ability to perform\nscale-invariant transfer learning between problem instances with similar\nvehicle and request distributions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:32:28 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Skordilis", "Erotokritos", ""], ["Hou", "Yi", ""], ["Tripp", "Charles", ""], ["Moniot", "Matthew", ""], ["Graf", "Peter", ""], ["Biagioni", "David", ""]]}, {"id": "2105.13348", "submitter": "Yu-Guan Hsieh", "authors": "Yu-Guan Hsieh, Franck Iutzeler, J\\'er\\^ome Malick, Panayotis\n  Mertikopoulos", "title": "Optimization in Open Networks via Dual Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In networks of autonomous agents (e.g., fleets of vehicles, scattered\nsensors), the problem of minimizing the sum of the agents' local functions has\nreceived a lot of interest. We tackle here this distributed optimization\nproblem in the case of open networks when agents can join and leave the network\nat any time. Leveraging recent online optimization techniques, we propose and\nanalyze the convergence of a decentralized asynchronous optimization method for\nopen networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:52:48 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hsieh", "Yu-Guan", ""], ["Iutzeler", "Franck", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "2105.13907", "submitter": "Zijian Hu", "authors": "Zijian Hu, Chengxiang Zhuge and Wei Ma", "title": "Towards a Very Large Scale Traffic Simulator for Multi-Agent\n  Reinforcement Learning Testbeds", "comments": "IJCAI 2021 Reinforcement Learning for Intelligent Transportation\n  Systems (RL4ITS) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart traffic control and management become an emerging application for Deep\nReinforcement Learning (DRL) to solve traffic congestion problems in urban\nnetworks. Different traffic control and management policies can be tested on\nthe traffic simulation. Current DRL-based studies are mainly supported by the\nmicroscopic simulation software (e.g., SUMO), while it is not suitable for\ncity-wide control due to the computational burden and gridlock effect. To the\nbest of our knowledge, there is a lack of studies on the large-scale traffic\nsimulator for DRL testbeds, which could further hinder the development of DRL.\nIn view of this, we propose a meso-macro traffic simulator for very large-scale\nDRL scenarios. The proposed simulator integrates mesoscopic and macroscopic\ntraffic simulation models to improve efficiency and eliminate gridlocks. The\nmesoscopic link model simulates flow dynamics on roads, and the macroscopic\nBathtub model depicts vehicle movement in regions. Moreover, both types of\nmodels can be hybridized to accommodate various DRL tasks. This creates portals\nfor mixed transportation applications under different contexts. The result\nshows that the developed simulator only takes 46 seconds to finish a 24-hour\nsimulation in a very large city with 2.2 million vehicles, which is much faster\nthan SUMO. Additionally, we develop a graphic interface for users to visualize\nthe simulation results in a web explorer. In the future, the developed\nmeso-macro traffic simulator could serve as a new environment for very\nlarge-scale DRL problems.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:19:43 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hu", "Zijian", ""], ["Zhuge", "Chengxiang", ""], ["Ma", "Wei", ""]]}, {"id": "2105.14004", "submitter": "Zhiyong Sun", "authors": "Zhiyong Sun, Anders Rantzer, Zhongkui Li, Anders Robertsson", "title": "Distributed adaptive stabilization", "comments": "16 Pages and 7 figures", "journal-ref": "Automatica: Volume 129, 109616 (1-13), July 2021", "doi": "10.1016/j.automatica.2021.109616", "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY math.OC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider distributed adaptive stabilization for uncertain\nmultivariable linear systems with a time-varying diagonal matrix gain. We show\nthat uncertain multivariable linear systems are stabilizable by diagonal matrix\nhigh gains if the system matrix is an H-matrix with positive diagonal entries.\nBased on matrix measure and stability theory for diagonally dominant systems,\nwe consider two classes of uncertain linear systems, and derive a threshold\ncondition to ensure their exponential stability by a monotonically increasing\ndiagonal gain matrix. When each individual gain function in the matrix gain is\nupdated by state-dependent functions using only local state information, the\nboundedness and convergence of both system states and adaptive matrix gains are\nguaranteed. We apply the adaptive distributed stabilization approach to\nadaptive synchronization control for large-scale complex networks consisting of\nnonlinear node dynamics and time-varying coupling weights. A unified framework\nfor adaptive synchronization is proposed that includes several general design\napproaches for adaptive coupling weights to guarantee network synchronization.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:28:29 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Zhiyong", ""], ["Rantzer", "Anders", ""], ["Li", "Zhongkui", ""], ["Robertsson", "Anders", ""]]}, {"id": "2105.14329", "submitter": "Gerrit Gro{\\ss}mann", "authors": "Gerrit Gro{\\ss}mann, Julian Zimmerlin, Michael Backenk\\\"ohler, Verena\n  Wolf", "title": "GINA: Neural Relational Inference From Independent Snapshots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.MA physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamical systems in which local interactions among agents give rise to\ncomplex emerging phenomena are ubiquitous in nature and society. This work\nexplores the problem of inferring the unknown interaction structure\n(represented as a graph) of such a system from measurements of its constituent\nagents or individual components (represented as nodes). We consider a setting\nwhere the underlying dynamical model is unknown and where different\nmeasurements (i.e., snapshots) may be independent (e.g., may stem from\ndifferent experiments). We propose GINA (Graph Inference Network Architecture),\na graph neural network (GNN) to simultaneously learn the latent interaction\ngraph and, conditioned on the interaction graph, the prediction of a node's\nobservable state based on adjacent vertices. GINA is based on the hypothesis\nthat the ground truth interaction graph -- among all other potential graphs --\nallows to predict the state of a node, given the states of its neighbors, with\nthe highest accuracy. We test this hypothesis and demonstrate GINA's\neffectiveness on a wide range of interaction graphs and dynamical processes.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:42:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gro\u00dfmann", "Gerrit", ""], ["Zimmerlin", "Julian", ""], ["Backenk\u00f6hler", "Michael", ""], ["Wolf", "Verena", ""]]}, {"id": "2105.14586", "submitter": "Hardhik Mohanty", "authors": "Gourab Ghatak, Hardhik Mohanty, Aniq Ur Rahman", "title": "Kolmogorov-Smirnov Test-Based Actively-Adaptive Thompson Sampling for\n  Non-Stationary Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-stationary multi-armed bandit (MAB) framework and propose\na Kolmogorov-Smirnov (KS) test based Thompson Sampling (TS) algorithm named\nTS-KS, that actively detects change points and resets the TS parameters once a\nchange is detected. In particular, for the two-armed bandit case, we derive\nbounds on the number of samples of the reward distribution to detect the change\nonce it occurs. Consequently, we show that the proposed algorithm has\nsub-linear regret. Contrary to existing works, our algorithm is able to detect\na change when the underlying reward distribution changes even though the mean\nreward remains the same. Finally, to test the efficacy of the proposed\nalgorithm, we employ it in two case-studies: i) task-offloading scenario in\nwireless edge-computing, and ii) portfolio optimization. Our results show that\nthe proposed TS-KS algorithm outperforms not only the static TS algorithm but\nalso it performs better than other bandit algorithms designed for\nnon-stationary environments. Moreover, the performance of TS-KS is at par with\nthe state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:28:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ghatak", "Gourab", ""], ["Mohanty", "Hardhik", ""], ["Rahman", "Aniq Ur", ""]]}, {"id": "2105.14707", "submitter": "Felipe S. Abrah\\~ao", "authors": "Felipe S. Abrah\\~ao, Hector Zenil", "title": "Emergence and algorithmic information dynamics of systems and observers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.FL cs.MA cs.SY eess.SY math.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that perturbation analysis in software space can\nproduce candidate computable generative models and uncover possible causal\nproperties from the finite description of an object or system quantifying the\nalgorithmic contribution of each of its elements relative to the whole. One of\nthe challenges for defining emergence is that one observer's prior knowledge\nmay cause a phenomenon to present itself to such observer as emergent while for\nanother as reducible. When attempting to quantify emergence, we demonstrate\nthat the methods of Algorithmic Information Dynamics can deal with the richness\nof such observer-object dependencies both in theory and practice. By\nformalising the act of observing as mutual algorithmic perturbation, the\nemergence of algorithmic information is rendered invariant, minimal, and robust\nin the face of information cost and distortion, while still observer-dependent.\nWe demonstrate that the unbounded increase of emergent algorithmic information\nimplies asymptotically observer-independent emergence, which eventually\novercomes any formal theory that an observer might devise to finitely\ncharacterise a phenomenon. We discuss observer-dependent emergence and\nasymptotically observer-independent emergence solving some previous suggestions\nindicating a hard distinction between strong and weak emergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:59:59 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 22:55:04 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Abrah\u00e3o", "Felipe S.", ""], ["Zenil", "Hector", ""]]}, {"id": "2105.14716", "submitter": "Haizheng Zhang", "authors": "Haizheng Zhang, Ravi Seshadri, A. Arun Prakash, Constantinos Antoniou,\n  Francisco C. Pereira, Moshe Ben-Akiva", "title": "Improving the Accuracy and Efficiency of Online Calibration for\n  Simulation-based Dynamic Traffic Assignment", "comments": "26 pages, 15 figures", "journal-ref": "Transportation Research Part C: Emerging Technologies Volume 128,\n  July 2021, 103195", "doi": "10.1016/j.trc.2021.103195", "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Simulation-based Dynamic Traffic Assignment models have important\napplications in real-time traffic management and control. The efficacy of these\nsystems rests on the ability to generate accurate estimates and predictions of\ntraffic states, which necessitates online calibration. A widely used solution\napproach for online calibration is the Extended Kalman Filter (EKF), which --\nalthough appealing in its flexibility to incorporate any class of parameters\nand measurements -- poses several challenges with regard to calibration\naccuracy and scalability, especially in congested situations for large-scale\nnetworks. This paper addresses these issues in turn so as to improve the\naccuracy and efficiency of EKF-based online calibration approaches for large\nand congested networks. First, the concept of state augmentation is revisited\nto handle violations of the Markovian assumption typically implicit in online\napplications of the EKF. Second, a method based on graph-coloring is proposed\nto operationalize the partitioned finite-difference approach that enhances\nscalability of the gradient computations.\n  Several synthetic experiments and a real world case study demonstrate that\napplication of the proposed approaches yields improvements in terms of both\nprediction accuracy and computational performance. The work has applications in\nreal-world deployments of simulation-based dynamic traffic assignment systems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 06:05:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Haizheng", ""], ["Seshadri", "Ravi", ""], ["Prakash", "A. Arun", ""], ["Antoniou", "Constantinos", ""], ["Pereira", "Francisco C.", ""], ["Ben-Akiva", "Moshe", ""]]}, {"id": "2105.15013", "submitter": "Jianhong Wang", "authors": "Jianhong Wang, Jinxin Wang, Yuan Zhang, Yunjie Gu, Tae-Kyun Kim", "title": "SHAQ: Incorporating Shapley Value Theory into Q-Learning for Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value factorisation proves to be a very useful technique in multi-agent\nreinforcement learning (MARL), but the underlying mechanism is not yet fully\nunderstood. This paper explores a theoretic basis for value factorisation. We\ngeneralise the Shapley value in the coalitional game theory to a Markov convex\ngame (MCG) and use it to guide value factorisation in MARL. We show that the\ngeneralised Shapley value possesses several features such as (1) accurate\nestimation of the maximum global value, (2) fairness in the factorisation of\nthe global value, and (3) being sensitive to dummy agents. The proposed theory\nyields a new learning algorithm called Sharpley Q-learning (SHAQ), which\ninherits the important merits of ordinary Q-learning but extends it to MARL. In\ncomparison with prior-arts, SHAQ has a much weaker assumption (MCG) that is\nmore compatible with real-world problems, but has superior explainability and\nperformance in many cases. We demonstrated SHAQ and verified the theoretic\nclaims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:50:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Jianhong", ""], ["Wang", "Jinxin", ""], ["Zhang", "Yuan", ""], ["Gu", "Yunjie", ""], ["Kim", "Tae-Kyun", ""]]}]