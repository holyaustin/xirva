[{"id": "2103.00600", "submitter": "John Cartlidge", "authors": "Henry Hanifan, Ben Watson, John Cartlidge, Dave Cliff", "title": "Time Matters: Exploring the Effects of Urgency and Reaction Speed in\n  Automated Traders", "comments": "22 pages. To be published in A. P. Rocha et al. (Eds.), ICAART 2020,\n  LNAI 12613, 2021. arXiv admin note: substantial text overlap with\n  arXiv:1912.02775", "journal-ref": null, "doi": "10.1007/978-3-030-71158-0_7", "report-no": null, "categories": "cs.MA cs.CE q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider issues of time in automated trading strategies in simulated\nfinancial markets containing a single exchange with public limit order book and\ncontinuous double auction matching. In particular, we explore two effects: (i)\nreaction speed - the time taken for trading strategies to calculate a response\nto market events; and (ii) trading urgency - the sensitivity of trading\nstrategies to approaching deadlines. Much of the literature on trading agents\nfocuses on optimising pricing strategies only and ignores the effects of time,\nwhile real-world markets continue to experience a race to zero latency, as\nautomated trading systems compete to quickly access information and act in the\nmarket ahead of others. We demonstrate that modelling reaction speed can\nsignificantly alter previously published results, with simple strategies such\nas SHVR outperforming more complex adaptive algorithms such as AA. We also show\nthat adding a pace parameter to ZIP traders (ZIP-Pace, or ZIPP) can create a\nsense of urgency that significantly improves profitability.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 19:38:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hanifan", "Henry", ""], ["Watson", "Ben", ""], ["Cartlidge", "John", ""], ["Cliff", "Dave", ""]]}, {"id": "2103.01636", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Elena Mocanu, Tiago Pinto, Selima Curci,\n  Phuong H. Nguyen, Madeleine Gibescu, Damien Ernst, Zita A. Vale", "title": "Sparse Training Theory for Scalable and Efficient Agents", "comments": null, "journal-ref": "20th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task for artificial intelligence is learning. Deep Neural\nNetworks have proven to cope perfectly with all learning paradigms, i.e.\nsupervised, unsupervised, and reinforcement learning. Nevertheless, traditional\ndeep learning approaches make use of cloud computing facilities and do not\nscale well to autonomous agents with low computational resources. Even in the\ncloud, they suffer from computational and memory limitations, and they cannot\nbe used to model adequately large physical worlds for agents which assume\nnetworks with billions of neurons. These issues are addressed in the last few\nyears by the emerging topic of sparse training, which trains sparse networks\nfrom scratch. This paper discusses sparse training state-of-the-art, its\nchallenges and limitations while introducing a couple of new theoretical\nresearch directions which has the potential of alleviating sparse training\nlimitations to push deep learning scalability well beyond its current\nboundaries. Nevertheless, the theoretical advancements impact in complex\nmulti-agents settings is discussed from a real-world perspective, using the\nsmart grid case study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:48:29 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""], ["Pinto", "Tiago", ""], ["Curci", "Selima", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""], ["Ernst", "Damien", ""], ["Vale", "Zita A.", ""]]}, {"id": "2103.01778", "submitter": "Roee Shraga PhD", "authors": "Gregory Goren, Roee Shraga, Alexander Tuisov", "title": "OSOUM Framework for Trading Data Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades, data have become a cornerstone component in many\nbusiness decisions, and copious resources are being poured into production and\nacquisition of the high-quality data. This emerging market possesses unique\nfeatures, and thus came under the spotlight for the stakeholders and\nresearchers alike. In this work, we aspire to provide the community with a set\nof tools for making business decisions, as well as analysis of markets behaving\naccording to certain rules. We supply, to the best of our knowledge, the first\nopen source simulation platform, termed Open SOUrce Market Simulator (OSOUM) to\nanalyze trading markets and specifically data markets. We also describe and\nimplement a specific data market model, consisting of two types of agents:\nsellers who own various datasets available for acquisition, and buyers\nsearching for relevant and beneficial datasets for purchase. The current\nsimulation treats data as an infinite supply product. Yet, other market\nsettings may be easily implemented using OSOUM. Although commercial frameworks,\nintended for handling data markets, already exist, we provide a free and\nextensive end-to-end research tool for simulating possible behavior for both\nbuyers and sellers participating in (data) markets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 09:20:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Goren", "Gregory", ""], ["Shraga", "Roee", ""], ["Tuisov", "Alexander", ""]]}, {"id": "2103.01955", "submitter": "Akash Velu", "authors": "Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, Yi Wu", "title": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximal Policy Optimization (PPO) is a popular on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due the belief that on-policy\nmethods are significantly less sample efficient than their off-policy\ncounterparts in multi-agent problems. In this work, we investigate Multi-Agent\nPPO (MAPPO), a variant of PPO which is specialized for multi-agent settings.\nUsing a 1-GPU desktop, we show that MAPPO achieves surprisingly strong\nperformance in three popular multi-agent testbeds: the particle-world\nenvironments, the Starcraft multi-agent challenge, and the Hanabi challenge,\nwith minimal hyperparameter tuning and without any domain-specific algorithmic\nmodifications or architectures. In the majority of environments, we find that\ncompared to off-policy baselines, MAPPO achieves strong results while\nexhibiting comparable sample efficiency. Finally, through ablation studies, we\npresent the implementation and algorithmic factors which are most influential\nto MAPPO's practical performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:59:56 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 23:45:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yu", "Chao", ""], ["Velu", "Akash", ""], ["Vinitsky", "Eugene", ""], ["Wang", "Yu", ""], ["Bayen", "Alexandre", ""], ["Wu", "Yi", ""]]}, {"id": "2103.01991", "submitter": "Izzeddin Gur", "authors": "Izzeddin Gur, Natasha Jaques, Kevin Malta, Manoj Tiwari, Honglak Lee,\n  Aleksandra Faust", "title": "Adversarial Environment Generation for Learning to Navigate the Web", "comments": "Presented at Deep RL Workshop, NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to autonomously navigate the web is a difficult sequential decision\nmaking task. The state and action spaces are large and combinatorial in nature,\nand websites are dynamic environments consisting of several pages. One of the\nbottlenecks of training web navigation agents is providing a learnable\ncurriculum of training environments that can cover the large variety of\nreal-world websites. Therefore, we propose using Adversarial Environment\nGeneration (AEG) to generate challenging web environments in which to train\nreinforcement learning (RL) agents. We provide a new benchmarking environment,\ngMiniWoB, which enables an RL adversary to use compositional primitives to\nlearn to generate arbitrarily complex websites. To train the adversary, we\npropose a new technique for maximizing regret using the difference in the\nscores obtained by a pair of navigator agents. Our results show that our\napproach significantly outperforms prior methods for minimax regret AEG. The\nregret objective trains the adversary to design a curriculum of environments\nthat are \"just-the-right-challenge\" for the navigator agents; our results show\nthat over time, the adversary learns to generate increasingly complex web\nnavigation tasks. The navigator agents trained with our technique learn to\ncomplete challenging, high-dimensional web navigation tasks, such as form\nfilling, booking a flight etc. We show that the navigator agent trained with\nour proposed Flexible b-PAIRED technique significantly outperforms competitive\nautomatic curriculum generation baselines -- including a state-of-the-art RL\nweb navigation approach -- on a set of challenging unseen test environments,\nand achieves more than 80% success rate on some tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:19:30 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gur", "Izzeddin", ""], ["Jaques", "Natasha", ""], ["Malta", "Kevin", ""], ["Tiwari", "Manoj", ""], ["Lee", "Honglak", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2103.02056", "submitter": "Jiahua Xu", "authors": "Alevtina Dubovitskaya, Damien Ackerer, Jiahua Xu", "title": "A Game-Theoretic Analysis of Cross-Ledger Swaps with Packetized Payments", "comments": null, "journal-ref": "Workshop Proceedings of Financial Cryptography and Data Security\n  (2021)", "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a game-theoretic framework to study the outcomes of packetized\npayments, a cross-ledger transaction protocol, with strategic and possibly\nmalicious agents. We derive the transaction failure rate and demonstrate that\nwithout disciplinary mechanisms, packetized payments are likely to be\nincomplete. Our analysis suggests that collateral deposits can prevent\nmalicious agents from taking advantage of the protocol. We further infer that\nthe deposit amount should depend on the underlying asset price volatility or\nthat it should be dynamically adjusted as the price changes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:12:20 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 19:32:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dubovitskaya", "Alevtina", ""], ["Ackerer", "Damien", ""], ["Xu", "Jiahua", ""]]}, {"id": "2103.02150", "submitter": "Varun Bhatt", "authors": "Varun Bhatt, Michael Buro", "title": "Inference-Based Deterministic Messaging For Multi-Agent Communication", "comments": "13 pages, 10 figures. Accepted at accepted at the 35th AAAI\n  Conference on Artificial Intelligence, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is essential for coordination among humans and animals.\nTherefore, with the introduction of intelligent agents into the world,\nagent-to-agent and agent-to-human communication becomes necessary. In this\npaper, we first study learning in matrix-based signaling games to empirically\nshow that decentralized methods can converge to a suboptimal policy. We then\npropose a modification to the messaging policy, in which the sender\ndeterministically chooses the best message that helps the receiver to infer the\nsender's observation. Using this modification, we see, empirically, that the\nagents converge to the optimal policy in nearly all the runs. We then apply\nthis method to a partially observable gridworld environment which requires\ncooperation between two agents and show that, with appropriate approximation\nmethods, the proposed sender modification can enhance existing decentralized\ntraining methods for more complex domains as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:09:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bhatt", "Varun", ""], ["Buro", "Michael", ""]]}, {"id": "2103.02480", "submitter": "Jawad Yasin", "authors": "Jawad N. Yasin, Huma Mahboob, Mohammad-Hashem Haghbayan, Muhammad\n  Mehboob Yasin, Juha Plosila", "title": "Cellular Formation Maintenance and Collision Avoidance Using\n  Centroid-Based Point Set Registration in a Swarm of Drones", "comments": "Accepted to the Intelligent Systems Conference (IntelliSys) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work focuses on low-energy collision avoidance and formation maintenance\nin autonomous swarms of drones. Here, the two main problems are: 1) how to\navoid collisions by temporarily breaking the formation, i.e., collision\navoidance reformation, and 2) how do such reformation while minimizing the\ndeviation resulting in minimization of the overall time and energy consumption\nof the drones. To address the first question, we use cellular automata based\ntechnique to find an efficient formation that avoids the obstacle while\nminimizing the time and energy. Concerning the second question, a near-optimal\nreformation of the swarm after successful collision avoidance is achieved by\napplying a temperature function reduction technique, originally used in the\npoint set registration process. The goal of the reformation process is to\nremove the disturbance while minimizing the overall time it takes for the swarm\nto reach the destination and consequently reducing the energy consumption\nrequired by this operation. To measure the degree of formation disturbance due\nto collision avoidance, deviation of the centroid of the swarm formation is\nused, inspired by the concept of the center of mass in classical mechanics.\nExperimental results show the efficiency of the proposed technique, in terms of\nperformance and energy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:47:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yasin", "Jawad N.", ""], ["Mahboob", "Huma", ""], ["Haghbayan", "Mohammad-Hashem", ""], ["Yasin", "Muhammad Mehboob", ""], ["Plosila", "Juha", ""]]}, {"id": "2103.02733", "submitter": "Vasileios Tzoumas", "authors": "Brent Schlotfeldt, Vasileios Tzoumas, George J. Pappas", "title": "Resilient Active Information Acquisition with Teams of Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging applications of collaborative autonomy, such as Multi-Target\nTracking, Unknown Map Exploration, and Persistent Surveillance, require robots\nplan paths to navigate an environment while maximizing the information\ncollected via on-board sensors. In this paper, we consider such information\nacquisition tasks but in adversarial environments, where attacks may\ntemporarily disable the robots' sensors. We propose the first receding horizon\nalgorithm, aiming for robust and adaptive multi-robot planning against any\nnumber of attacks, which we call Resilient Active Information acquisitioN\n(RAIN). RAIN calls, in an online fashion, a Robust Trajectory Planning (RTP)\nsubroutine which plans attack-robust control inputs over a look-ahead planning\nhorizon. We quantify RTP's performance by bounding its suboptimality. We base\nour theoretical analysis on notions of curvature introduced in combinatorial\noptimization. We evaluate RAIN in three information acquisition scenarios:\nMulti-Target Tracking, Occupancy Grid Mapping, and Persistent Surveillance. The\nscenarios are simulated in C++ and a Unity-based simulator. In all simulations,\nRAIN runs in real-time, and exhibits superior performance against a\nstate-of-the-art baseline information acquisition algorithm, even in the\npresence of a high number of attacks. We also demonstrate RAIN's robustness and\neffectiveness against varying models of attacks (worst-case and random), as\nwell as, varying replanning rates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:48:25 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 20:53:15 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Schlotfeldt", "Brent", ""], ["Tzoumas", "Vasileios", ""], ["Pappas", "George J.", ""]]}, {"id": "2103.03020", "submitter": "Manuel Guimar\\~aes", "authors": "Samuel Mascarenhas, Manuel Guimar\\~aes, Pedro A. Santos, Jo\\~ao Dias,\n  Rui Prada, Ana Paiva", "title": "FAtiMA Toolkit -- Toward an effective and accessible tool for the\n  development of intelligent virtual agents and social robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  More than a decade has passed since the development of FearNot!, an\napplication designed to help children deal with bullying through role-playing\nwith virtual characters. It was also the application that led to the creation\nof FAtiMA, an affective agent architecture for creating autonomous characters\nthat can evoke empathic responses. In this paper, we describe FAtiMA Toolkit, a\ncollection of open-source tools that is designed to help researchers, game\ndevelopers and roboticists incorporate a computational model of emotion and\ndecision-making in their work. The toolkit was developed with the goal of\nmaking FAtiMA more accessible, easier to incorporate into different projects\nand more flexible in its capabilities for human-agent interaction, based upon\nthe experience gathered over the years across different virtual environments\nand human-robot interaction scenarios. As a result, this work makes several\ndifferent contributions to the field of Agent-Based Architectures. More\nprecisely, FAtiMA Toolkit's library based design allows developers to easily\nintegrate it with other frameworks, its meta-cognitive model affords different\ninternal reasoners and affective components and its explicit dialogue structure\ngives control to the author even within highly complex scenarios. To\ndemonstrate the use of FAtiMA Toolkit, several different use cases where the\ntoolkit was successfully applied are described and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:30:59 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mascarenhas", "Samuel", ""], ["Guimar\u00e3es", "Manuel", ""], ["Santos", "Pedro A.", ""], ["Dias", "Jo\u00e3o", ""], ["Prada", "Rui", ""], ["Paiva", "Ana", ""]]}, {"id": "2103.03022", "submitter": "Guiying Huang", "authors": "Victoria Huang, Gang Chen, Qiang Fu", "title": "Multi-Agent Deep Reinforcement Learning for Request Dispatching in\n  Distributed-Controller Software-Defined Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distributed controller architectures have been quickly gaining\npopularity in Software-Defined Networking (SDN). However, the use of\ndistributed controllers introduces a new and important Request Dispatching (RD)\nproblem with the goal for every SDN switch to properly dispatch their requests\namong all controllers so as to optimize network performance. This goal can be\nfulfilled by designing an RD policy to guide distribution of requests at each\nswitch. In this paper, we propose a Multi-Agent Deep Reinforcement Learning\n(MA-DRL) approach to automatically design RD policies with high adaptability\nand performance. This is achieved through a new problem formulation in the form\nof a Multi-Agent Markov Decision Process (MA-MDP), a new adaptive RD policy\ndesign and a new MA-DRL algorithm called MA-PPO. Extensive simulation studies\nshow that our MA-DRL technique can effectively train RD policies to\nsignificantly outperform man-made policies, model-based policies, as well as RD\npolicies learned via single-agent DRL algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 09:49:27 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Huang", "Victoria", ""], ["Chen", "Gang", ""], ["Fu", "Qiang", ""]]}, {"id": "2103.03216", "submitter": "Hadi Nekoei", "authors": "Hadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville, Sarath Chandar", "title": "Continuous Coordination As a Realistic Scenario for Lifelong Learning", "comments": "19 pages with supplementary materials. Added results for Lifelong RL\n  methods and some future work. Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current deep reinforcement learning (RL) algorithms are still highly\ntask-specific and lack the ability to generalize to new environments. Lifelong\nlearning (LLL), however, aims at solving multiple tasks sequentially by\nefficiently transferring and using knowledge between tasks. Despite a surge of\ninterest in lifelong RL in recent years, the lack of a realistic testbed makes\nrobust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the\nother hand, can be seen as a natural scenario for lifelong RL due to its\ninherent non-stationarity, since the agents' policies change over time. In this\nwork, we introduce a multi-agent lifelong learning testbed that supports both\nzero-shot and few-shot settings. Our setup is based on Hanabi -- a\npartially-observable, fully cooperative multi-agent game that has been shown to\nbe challenging for zero-shot coordination. Its large strategy space makes it a\ndesirable environment for lifelong RL tasks. We evaluate several recent MARL\nmethods, and benchmark state-of-the-art LLL algorithms in limited memory and\ncomputation regimes to shed light on their strengths and weaknesses. This\ncontinual learning paradigm also provides us with a pragmatic way of going\nbeyond centralized training which is the most commonly used training protocol\nin MARL. We empirically show that the agents trained in our setup are able to\ncoordinate well with unseen agents, without any additional assumptions made by\nprevious works. The code and all pre-trained models are available at\nhttps://github.com/chandar-lab/Lifelong-Hanabi.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:44:03 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 17:56:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nekoei", "Hadi", ""], ["Badrinaaraayanan", "Akilesh", ""], ["Courville", "Aaron", ""], ["Chandar", "Sarath", ""]]}, {"id": "2103.03234", "submitter": "Furkan Gursoy", "authors": "Furkan G\\\"ursoy and Bertan Badur", "title": "An Agent-Based Modelling Approach to Brain Drain", "comments": "Changes: minor language improvements, copyright notice", "journal-ref": null, "doi": "10.1109/TCSS.2021.3066074", "report-no": null, "categories": "physics.soc-ph cs.MA econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of brain drain, that is the emigration of highly skilled\npeople, has many undesirable effects, particularly for developing countries. In\nthis study, an agent-based model is developed to understand the dynamics of\nsuch emigration. We hypothesise that skilled people's emigration decisions are\nbased on several factors including the overall economic and social difference\nbetween the home and host countries, people's ability and capacity to obtain\ngood jobs and start a life abroad, and the barriers of moving abroad.\nFurthermore, the social network of individuals also plays a significant role.\nThe model is validated using qualitative and quantitative pattern matching with\nreal-world observations. Sensitivity and uncertainty analyses are performed in\naddition to several scenario analyses. Linear and random forest response\nsurface models are created to provide quick predictions on the number of\nemigrants as well as to understand the effect sizes of individual parameters.\nOverall, the study provides an abstract model where brain drain dynamics can be\nexplored. Findings from the simulation outputs show that future socioeconomic\nstate of the country is more important than the current state, lack of barriers\nresults in a large number of emigrants, and network effects ensue compounding\neffects on emigration. Upon further development and customisation, future\nversions can assist in the decision-making of social policymakers regarding\nbrain drain.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:40 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 18:39:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["G\u00fcrsoy", "Furkan", ""], ["Badur", "Bertan", ""]]}, {"id": "2103.03247", "submitter": "Mateusz Iwo Dubaniowski", "authors": "Mateusz Iwo Dubaniowski, Hans Rudolf Heinimann", "title": "Time granularity impact on propagation of disruptions in a\n  system-of-systems simulation of infrastructure and business networks", "comments": "26 pages, 11 figures, 2 tables, Submitted to International Journal of\n  Environmental Research and Public Health: Special Issue on Cascading Disaster\n  Modelling and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  System-of-systems (SoS) approach is often used for simulating disruptions to\nbusiness and infrastructure system networks allowing for integration of several\nmodels into one simulation. However, the integration is frequently challenging\nas each system is designed individually with different characteristics, such as\ntime granularity. Understanding the impact of time granularity on propagation\nof disruptions between businesses and infrastructure systems and finding the\nappropriate granularity for the SoS simulation remain as major challenges. To\ntackle these, we explore how time granularity, recovery time, and disruption\nsize affect the propagation of disruptions between constituent systems of an\nSoS simulation. To address this issue, we developed a High Level Architecture\n(HLA) simulation of 3 networks and performed a series of simulation\nexperiments. Our results revealed that time granularity and especially recovery\ntime have huge impact on propagation of disruptions. Consequently, we developed\na model for selecting an appropriate time granularity for an SoS simulation\nbased on expected recovery time. Our simulation experiments show that time\ngranularity should be less than 1.13 of expected recovery time. We identified\nsome areas for future research centered around extending the experimental\nfactors space.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:39:37 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Dubaniowski", "Mateusz Iwo", ""], ["Heinimann", "Hans Rudolf", ""]]}, {"id": "2103.03359", "submitter": "Amol Kelkar", "authors": "Amol Kelkar", "title": "Cognitive Homeostatic Agents", "comments": "Accepted at AAMAS2021 Blue Sky Ideas Track", "journal-ref": "In Proc. of the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021, IFAAMAS, 5 pages", "doi": "10.5555/3461017.3461021", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human brain has been used as an inspiration for building autonomous agents,\nbut it is not obvious what level of computational description of the brain one\nshould use. This has led to overly opinionated symbolic approaches and overly\nunstructured connectionist approaches. We propose that using homeostasis as the\ncomputational description provides a good compromise. Similar to how\nphysiological homeostasis is the regulation of certain homeostatic variables,\ncognition can be interpreted as the regulation of certain 'cognitive\nhomeostatic variables'. We present an outline of a Cognitive Homeostatic Agent,\nbuilt as a hierarchy of physiological and cognitive homeostatic subsystems and\ndescribe structures and processes to guide future exploration. We expect this\nto be a fruitful line of investigation towards building sophisticated\nartificial agents that can act flexibly in complex environments, and produce\nbehaviors indicating planning, thinking and feelings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:29:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kelkar", "Amol", ""]]}, {"id": "2103.03388", "submitter": "Richard Cheng", "authors": "Richard Cheng, Richard M. Murray, Joel W. Burdick", "title": "Limits of Probabilistic Safety Guarantees when Considering Human\n  Uncertainty", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When autonomous robots interact with humans, such as during autonomous\ndriving, explicit safety guarantees are crucial in order to avoid potentially\nlife-threatening accidents. Many data-driven methods have explored learning\nprobabilistic bounds over human agents' trajectories (i.e. confidence tubes\nthat contain trajectories with probability $\\delta$), which can then be used to\nguarantee safety with probability $1-\\delta$. However, almost all existing\nworks consider $\\delta \\geq 0.001$. The purpose of this paper is to argue that\n(1) in safety-critical applications, it is necessary to provide safety\nguarantees with $\\delta < 10^{-8}$, and (2) current learning-based methods are\nill-equipped to compute accurate confidence bounds at such low $\\delta$. Using\nhuman driving data (from the highD dataset), as well as synthetically generated\ndata, we show that current uncertainty models use inaccurate distributional\nassumptions to describe human behavior and/or require infeasible amounts of\ndata to accurately learn confidence bounds for $\\delta \\leq 10^{-8}$. These two\nissues result in unreliable confidence bounds, which can have dangerous\nimplications if deployed on safety-critical systems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:00:56 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 00:13:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Cheng", "Richard", ""], ["Murray", "Richard M.", ""], ["Burdick", "Joel W.", ""]]}, {"id": "2103.03450", "submitter": "Jiayu Chen", "authors": "Jiayu Chen, Abhishek K. Umrawal, Tian Lan, and Vaneet Aggarwal", "title": "DeepFreight: A Model-free Deep-reinforcement-learning-based Algorithm\n  for Multi-transfer Freight Delivery", "comments": "This paper is presented in part at the 31st International Conference\n  on Automated Planning and Scheduling (ICAPS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the freight delivery demands and shipping costs increasing rapidly,\nintelligent control of fleets to enable efficient and cost-conscious solutions\nbecomes an important problem. In this paper, we propose DeepFreight, a\nmodel-free deep-reinforcement-learning-based algorithm for multi-transfer\nfreight delivery, which includes two closely-collaborative components:\ntruck-dispatch and package-matching. Specifically, a deep multi-agent\nreinforcement learning framework called QMIX is leveraged to learn a dispatch\npolicy, with which we can obtain the multi-step joint dispatch decisions for\nthe fleet with respect to the delivery requests. Then an efficient\nmulti-transfer matching algorithm is executed to assign the delivery requests\nto the trucks. Also, DeepFreight is integrated with a Mixed-Integer Linear\nProgramming optimizer for further optimization. The evaluation results show\nthat the proposed system is highly scalable and ensures a 100% delivery success\nwhile maintaining low delivery time and fuel consumption.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:06:48 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Chen", "Jiayu", ""], ["Umrawal", "Abhishek K.", ""], ["Lan", "Tian", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2103.03652", "submitter": "Ehud Shapiro", "authors": "Ben Abramowitz, Edith Elkind, Davide Grossi, Ehud Shapiro, and Nimrod\n  Talmon", "title": "Democratic Forking: Choosing Sides with Social Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Any community in which membership is optional may eventually break apart, or\nfork. For example, forks may occur in political parties, business partnerships,\nsocial groups, cryptocurrencies, and federated governing bodies. Forking is\ntypically the product of informal social processes or the organized action of\nan aggrieved minority, and it is not always amicable. Forks usually come at a\ncost, and can be seen as consequences of collective decisions that destabilize\nthe community. Here, we provide a social choice setting in which agents can\nreport preferences not only over a set of alternatives, but also over the\npossible forks that may occur in the face of disagreement. We study this social\nchoice setting, concentrating on stability issues and concerns of strategic\nagent behavior.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:25:11 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Abramowitz", "Ben", ""], ["Elkind", "Edith", ""], ["Grossi", "Davide", ""], ["Shapiro", "Ehud", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2103.03662", "submitter": "Dani\\\"el Willemsen Willemsen", "authors": "Dani\\\"el Willemsen, Mario Coppola and Guido C.H.E. de Croon", "title": "MAMBPO: Sample-efficient multi-robot reinforcement learning using\n  learned world models", "comments": "Submitted to 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot systems can benefit from reinforcement learning (RL) algorithms\nthat learn behaviours in a small number of trials, a property known as sample\nefficiency. This research thus investigates the use of learned world models to\nimprove sample efficiency. We present a novel multi-agent model-based RL\nalgorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the\nCentralized Learning for Decentralized Execution (CLDE) framework. CLDE\nalgorithms allow a group of agents to act in a fully decentralized manner after\ntraining. This is a desirable property for many systems comprising of multiple\nrobots. MAMBPO uses a learned world model to improve sample efficiency compared\nto model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two\nsimulated multi-robot tasks, where MAMBPO achieves a similar performance to\nMASAC, but requires far fewer samples to do so. Through this, we take an\nimportant step towards making real-life learning for multi-robot systems\npossible.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:37:23 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Willemsen", "Dani\u00ebl", ""], ["Coppola", "Mario", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "2103.03786", "submitter": "Shuai Wang", "authors": "Zijian Zhang, Shuai Wang, Yuncong Hong, Liangkai Zhou, and Qi Hao", "title": "Distributed Dynamic Map Fusion via Federated Learning for Intelligent\n  Networked Vehicles", "comments": "12 pages, 5 figures, to appear in 2021 IEEE International Conference\n  on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology of dynamic map fusion among networked vehicles has been\ndeveloped to enlarge sensing ranges and improve sensing accuracies for\nindividual vehicles. This paper proposes a federated learning (FL) based\ndynamic map fusion framework to achieve high map quality despite unknown\nnumbers of objects in fields of view (FoVs), various sensing and model\nuncertainties, and missing data labels for online learning. The novelty of this\nwork is threefold: (1) developing a three-stage fusion scheme to predict the\nnumber of objects effectively and to fuse multiple local maps with fidelity\nscores; (2) developing an FL algorithm which fine-tunes feature models (i.e.,\nrepresentation learning networks for feature extraction) distributively by\naggregating model parameters; (3) developing a knowledge distillation method to\ngenerate FL training labels when data labels are unavailable. The proposed\nframework is implemented in the Car Learning to Act (CARLA) simulation\nplatform. Extensive experimental results are provided to verify the superior\nperformance and robustness of the developed map fusion and FL schemes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:28:46 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Zijian", ""], ["Wang", "Shuai", ""], ["Hong", "Yuncong", ""], ["Zhou", "Liangkai", ""], ["Hao", "Qi", ""]]}, {"id": "2103.04015", "submitter": "Behzad Khamidehi", "authors": "Behzad Khamidehi, Majid Raeis, Elvino S. Sousa", "title": "Dynamic Resource Management for Providing QoS in Drone Delivery Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones have been considered as an alternative means of package delivery to\nreduce the delivery cost and time. Due to the battery limitations, the drones\nare best suited for last-mile delivery, i.e., the delivery from the package\ndistribution centers (PDCs) to the customers. Since a typical delivery system\nconsists of multiple PDCs, each having random and time-varying demands, the\ndynamic drone-to-PDC allocation would be of great importance in meeting the\ndemand in an efficient manner. In this paper, we study the dynamic UAV\nassignment problem for a drone delivery system with the goal of providing\nmeasurable Quality of Service (QoS) guarantees. We adopt a queueing theoretic\napproach to model the customer-service nature of the problem. Furthermore, we\ntake a deep reinforcement learning approach to obtain a dynamic policy for the\nre-allocation of the UAVs. This policy guarantees a probabilistic upper-bound\non the queue length of the packages waiting in each PDC, which is beneficial\nfrom both the service provider's and the customers' viewpoints. We evaluate the\nperformance of our proposed algorithm by considering three broad arrival\nclasses, including Bernoulli, Time-Varying Bernoulli, and Markov-Modulated\nBernoulli arrivals. Our results show that the proposed method outperforms the\nbaselines, particularly in scenarios with Time-Varying and Markov-Modulated\nBernoulli arrivals, which are more representative of real-world demand\npatterns. Moreover, our algorithm satisfies the QoS constraints in all the\nstudied scenarios while minimizing the average number of UAVs in use.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:11:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Khamidehi", "Behzad", ""], ["Raeis", "Majid", ""], ["Sousa", "Elvino S.", ""]]}, {"id": "2103.04555", "submitter": "Zhiwei Qin", "authors": "Yan Jiao, Xiaocheng Tang, Zhiwei Qin, Shuaiji Li, Fan Zhang, Hongtu\n  Zhu and Jieping Ye", "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": "Transportation Research: Part C, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:34:05 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 00:14:19 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 06:32:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jiao", "Yan", ""], ["Tang", "Xiaocheng", ""], ["Qin", "Zhiwei", ""], ["Li", "Shuaiji", ""], ["Zhang", "Fan", ""], ["Zhu", "Hongtu", ""], ["Ye", "Jieping", ""]]}, {"id": "2103.04931", "submitter": "Bartosz Sawicki", "authors": "Maciej \\'Swiechowski, Konrad Godlewski, Bartosz Sawicki, Jacek\n  Ma\\'ndziuk", "title": "Monte Carlo Tree Search: A Review of Recent Modifications and\n  Applications", "comments": "84 pages, submitted to AI Review journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo Tree Search (MCTS) is a powerful approach to designing\ngame-playing bots or solving sequential decision problems. The method relies on\nintelligent tree search that balances exploration and exploitation. MCTS\nperforms random sampling in the form of simulations and stores statistics of\nactions to make more educated choices in each subsequent iteration. The method\nhas become a state-of-the-art technique for combinatorial games, however, in\nmore complex games (e.g. those with high branching factor or real-time ones),\nas well as in various practical domains (e.g. transportation, scheduling or\nsecurity) an efficient MCTS application often requires its problem-dependent\nmodification or integration with other techniques. Such domain-specific\nmodifications and hybrid approaches are the main focus of this survey. The last\nmajor MCTS survey has been published in 2012. Contributions that appeared since\nits release are of particular interest for this review.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:44:15 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 13:04:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["\u015awiechowski", "Maciej", ""], ["Godlewski", "Konrad", ""], ["Sawicki", "Bartosz", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2103.04972", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey and Alex Pentland", "title": "Provably Efficient Cooperative Multi-Agent Reinforcement Learning with\n  Function Approximation", "comments": "53 pages including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning in cooperative multi-agent settings has recently\nadvanced significantly in its scope, with applications in cooperative\nestimation for advertising, dynamic treatment regimes, distributed control, and\nfederated learning. In this paper, we discuss the problem of cooperative\nmulti-agent RL with function approximation, where a group of agents\ncommunicates with each other to jointly solve an episodic MDP. We demonstrate\nthat via careful message-passing and cooperative value iteration, it is\npossible to achieve near-optimal no-regret learning even with a fixed constant\ncommunication budget. Next, we demonstrate that even in heterogeneous\ncooperative settings, it is possible to achieve Pareto-optimal no-regret\nlearning with limited communication. Our work generalizes several ideas from\nthe multi-agent contextual and multi-armed bandit literature to MDPs and\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:51:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Pentland", "Alex", ""]]}, {"id": "2103.04982", "submitter": "Kevin McKee", "authors": "Kevin R. McKee, Edward Hughes, Tina O. Zhu, Martin J. Chadwick,\n  Raphael Koster, Antonio Garcia Castaneda, Charlie Beattie, Thore Graepel,\n  Matt Botvinick, Joel Z. Leibo", "title": "Deep reinforcement learning models the emergent dynamics of human\n  cooperation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective action demands that individuals efficiently coordinate how much,\nwhere, and when to cooperate. Laboratory experiments have extensively explored\nthe first part of this process, demonstrating that a variety of\nsocial-cognitive mechanisms influence how much individuals choose to invest in\ngroup efforts. However, experimental research has been unable to shed light on\nhow social cognitive mechanisms contribute to the where and when of collective\naction. We leverage multi-agent deep reinforcement learning to model how a\nsocial-cognitive mechanism--specifically, the intrinsic motivation to achieve a\ngood reputation--steers group behavior toward specific spatial and temporal\nstrategies for collective action in a social dilemma. We also collect\nbehavioral data from groups of human participants challenged with the same\ndilemma. The model accurately predicts spatial and temporal patterns of group\nbehavior: in this public goods dilemma, the intrinsic motivation for reputation\ncatalyzes the development of a non-territorial, turn-taking strategy to\ncoordinate collective action.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:58:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["McKee", "Kevin R.", ""], ["Hughes", "Edward", ""], ["Zhu", "Tina O.", ""], ["Chadwick", "Martin J.", ""], ["Koster", "Raphael", ""], ["Castaneda", "Antonio Garcia", ""], ["Beattie", "Charlie", ""], ["Graepel", "Thore", ""], ["Botvinick", "Matt", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "2103.05091", "submitter": "Ekaterina Tolstaya", "authors": "Ekaterina Tolstaya, Landon Butler, Daniel Mox, James Paulos, Vijay\n  Kumar, Alejandro Ribeiro", "title": "Learning Connectivity for Data Distribution in Robot Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many algorithms for control of multi-robot teams operate under the assumption\nthat low-latency, global state information necessary to coordinate agent\nactions can readily be disseminated among the team. However, in harsh\nenvironments with no existing communication infrastructure, robots must form\nad-hoc networks, forcing the team to operate in a distributed fashion. To\novercome this challenge, we propose a task-agnostic, decentralized, low-latency\nmethod for data distribution in ad-hoc networks using Graph Neural Networks\n(GNN). Our approach enables multi-agent algorithms based on global state\ninformation to function by ensuring it is available at each robot. To do this,\nagents glean information about the topology of the network from packet\ntransmissions and feed it to a GNN running locally which instructs the agent\nwhen and where to transmit the latest state information. We train the\ndistributed GNN communication policies via reinforcement learning using the\naverage Age of Information as the reward function and show that it improves\ntraining stability compared to task-specific reward functions. Our approach\nperforms favorably compared to industry-standard methods for data distribution\nsuch as random flooding and round robin. We also show that the trained policies\ngeneralize to larger teams of both static and mobile agents.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:48:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Tolstaya", "Ekaterina", ""], ["Butler", "Landon", ""], ["Mox", "Daniel", ""], ["Paulos", "James", ""], ["Kumar", "Vijay", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "2103.05174", "submitter": "Pavan Samtani", "authors": "Pavan Samtani, Francisco Leiva, Javier Ruiz-del-Solar", "title": "Learning to Play Soccer From Scratch: Sample-Efficient Emergent\n  Coordination through Curriculum-Learning and Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a scheme that allows learning complex multi-agent\nbehaviors in a sample efficient manner, applied to 2v2 soccer. The problem is\nformulated as a Markov game, and solved using deep reinforcement learning. We\npropose a basic multi-agent extension of TD3 for learning the policy of each\nplayer, in a decentralized manner. To ease learning, the task of 2v2 soccer is\ndivided in three stages: 1v0, 1v1 and 2v2. The process of learning in\nmulti-agent stages (1v1 and 2v2) uses agents trained on a previous stage as\nfixed opponents. In addition, we propose using experience sharing, a method\nthat shares experience from a fixed opponent, trained in a previous stage, for\ntraining the agent currently learning, and a form of frame-skipping, to raise\nperformance significantly. Our results show that high quality soccer play can\nbe obtained with our approach in just under 40M interactions. A summarized\nvideo of the resulting game play can be found in https://youtu.be/f25l1j1U9RM.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:57:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Samtani", "Pavan", ""], ["Leiva", "Francisco", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "2103.05293", "submitter": "Tianhao Zhang", "authors": "Tianhao Zhang and Yueheng Li and Shuai Li and Qiwei Ye and Chen Wang\n  and Guangming Xie", "title": "Decentralized Circle Formation Control for Fish-like Robots in the\n  Real-world via Reinforcement Learning", "comments": "to be published in ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the circle formation control problem is addressed for a group\nof cooperative underactuated fish-like robots involving unknown nonlinear\ndynamics and disturbances. Based on the reinforcement learning and cognitive\nconsistency theory, we propose a decentralized controller without the knowledge\nof the dynamics of the fish-like robots. The proposed controller can be\ntransferred from simulation to reality. It is only trained in our established\nsimulation environment, and the trained controller can be deployed to real\nrobots without any manual tuning. Simulation results confirm that the proposed\nmodel-free robust formation control method is scalable with respect to the\ngroup size of the robots and outperforms other representative RL algorithms.\nSeveral experiments in the real world verify the effectiveness of our RL-based\napproach for circle formation control.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:38:28 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhang", "Tianhao", ""], ["Li", "Yueheng", ""], ["Li", "Shuai", ""], ["Ye", "Qiwei", ""], ["Wang", "Chen", ""], ["Xie", "Guangming", ""]]}, {"id": "2103.05576", "submitter": "Chang Yu", "authors": "Chang Yu, Xiaoqing Lu, Jingang Lai and Li Chai", "title": "Distributed Frequency Restoration and SoC Balancing Control for AC\n  Microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an improved distributed finite-time control algorithm for\nmultiagent-based ac microgrids with battery energy storage systems (BESSs)\nutilizing a low-width communication network. The proposed control algorithm can\nsimultaneously coordinate BESSs to eliminate any deviation from the nominal\nfrequency as well as solving the state of charge (SoC) balancing problem. The\nstability of the proposed control algorithm is established using the Lyapunov\nmethod and homogeneous approximation theory, which guarantees an accelerated\nconvergence within a settling time that does not dependent on initial\nconditions. Based on this, to significantly reduce the communication burdens,\nan event-triggered communication mechanism is designed which can also avoid\nZeno behavior. Then sufficient conditions on the event-triggered boundary are\nderived to guarantee the stability and reliability of the whole system.\nPractical local constraints are imposed to implement the control protocol, and\nthe theoretical results are applied to a test system consisting of five DGs and\nfive BESSs, which verifies the effectiveness of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:33:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yu", "Chang", ""], ["Lu", "Xiaoqing", ""], ["Lai", "Jingang", ""], ["Chai", "Li", ""]]}, {"id": "2103.05737", "submitter": "Corban Rivera", "authors": "Edward W. Staley, Corban G.Rivera, Ashley J. Llorens", "title": "The AI Arena: A Framework for Distributed Multi-Agent Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in reinforcement learning (RL) have resulted in recent breakthroughs\nin the application of artificial intelligence (AI) across many different\ndomains. An emerging landscape of development environments is making powerful\nRL techniques more accessible for a growing community of researchers. However,\nmost existing frameworks do not directly address the problem of learning in\ncomplex operating environments, such as dense urban settings or defense-related\nscenarios, that incorporate distributed, heterogeneous teams of agents. To help\nenable AI research for this important class of applications, we introduce the\nAI Arena: a scalable framework with flexible abstractions for distributed\nmulti-agent reinforcement learning. The AI Arena extends the OpenAI Gym\ninterface to allow greater flexibility in learning control policies across\nmultiple agents with heterogeneous learning strategies and localized views of\nthe environment. To illustrate the utility of our framework, we present\nexperimental results that demonstrate performance gains due to a distributed\nmulti-agent learning approach over commonly-used RL techniques in several\ndifferent learning environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:16:19 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Staley", "Edward W.", ""], ["Rivera", "Corban G.", ""], ["Llorens", "Ashley J.", ""]]}, {"id": "2103.06113", "submitter": "Cillian Brewitt", "authors": "Cillian Brewitt, Balint Gyevnar, Stefano V. Albrecht", "title": "GRIT: Verifiable Goal Recognition for Autonomous Driving using Decision\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is useful for autonomous vehicles to have the ability to infer the goals\nof other vehicles (goal recognition), in order to safely interact with other\nvehicles and predict their future trajectories. Goal recognition methods must\nbe fast to run in real time and make accurate inferences. As autonomous driving\nis safety-critical, it is important to have methods which are human\ninterpretable and for which safety can be formally verified. Existing goal\nrecognition methods for autonomous vehicles fail to satisfy all four objectives\nof being fast, accurate, interpretable and verifiable. We propose Goal\nRecognition with Interpretable Trees (GRIT), a goal recognition system for\nautonomous vehicles which achieves these objectives. GRIT makes use of decision\ntrees trained on vehicle trajectory data. Evaluation on two vehicle trajectory\ndatasets demonstrates the inference speed and accuracy of GRIT compared to an\nablation and two deep learning baselines. We show that the learned trees are\nhuman interpretable and demonstrate how properties of GRIT can be formally\nverified using an SMT solver.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:06:11 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Brewitt", "Cillian", ""], ["Gyevnar", "Balint", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2103.06359", "submitter": "Ankur Deka", "authors": "Ankur Deka, Wenhao Luo, Huao Li, Michael Lewis, Katia Sycara", "title": "Hiding Leader's Identity in Leader-Follower Navigation through\n  Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leader-follower navigation is a popular class of multi-robot algorithms where\na leader robot leads the follower robots in a team. The leader has specialized\ncapabilities or mission critical information (e.g. goal location) that the\nfollowers lack which makes the leader crucial for the mission's success.\nHowever, this also makes the leader a vulnerability - an external adversary who\nwishes to sabotage the robot team's mission can simply harm the leader and the\nwhole robot team's mission would be compromised. Since robot motion generated\nby traditional leader-follower navigation algorithms can reveal the identity of\nthe leader, we propose a defense mechanism of hiding the leader's identity by\nensuring the leader moves in a way that behaviorally camouflages it with the\nfollowers, making it difficult for an adversary to identify the leader. To\nachieve this, we combine Multi-Agent Reinforcement Learning, Graph Neural\nNetworks and adversarial training. Our approach enables the multi-robot team to\noptimize the primary task performance with leader motion similar to follower\nmotion, behaviorally camouflaging it with the followers. Our algorithm\noutperforms existing work that tries to hide the leader's identity in a\nmulti-robot team by tuning traditional leader-follower control parameters with\nClassical Genetic Algorithms. We also evaluated human performance in inferring\nthe leader's identity and found that humans had lower accuracy when the robot\nteam used our proposed navigation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:07:07 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Deka", "Ankur", ""], ["Luo", "Wenhao", ""], ["Li", "Huao", ""], ["Lewis", "Michael", ""], ["Sycara", "Katia", ""]]}, {"id": "2103.06426", "submitter": "Stephen McAleer", "authors": "Stephen McAleer, John Lanier, Pierre Baldi, Roy Fox", "title": "XDO: A Double Oracle Algorithm for Extensive-Form Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy Space Response Oracles (PSRO) is a deep reinforcement learning\nalgorithm for two-player zero-sum games that has empirically found approximate\nNash equilibria in large games. Although PSRO is guaranteed to converge to a\nNash equilibrium, it may take an exponential number of iterations as the number\nof infostates grows. We propose Extensive-Form Double Oracle (XDO), an\nextensive-form double oracle algorithm that is guaranteed to converge to an\napproximate Nash equilibrium linearly in the number of infostates. Unlike PSRO,\nwhich mixes best responses at the root of the game, XDO mixes best responses at\nevery infostate. We also introduce Neural XDO (NXDO), where the best response\nis learned through deep RL. In tabular experiments on Leduc poker, we find that\nXDO achieves an approximate Nash equilibrium in a number of iterations 1-2\norders of magnitude smaller than PSRO. In experiments on a modified Leduc poker\ngame, we show that tabular XDO achieves over 11x lower exploitability than CFR\nand over 82x lower exploitability than PSRO and XFP in the same amount of time.\nWe also show that NXDO beats PSRO and is competitive with NFSP on a large\nno-limit poker game.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:05:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["McAleer", "Stephen", ""], ["Lanier", "John", ""], ["Baldi", "Pierre", ""], ["Fox", "Roy", ""]]}, {"id": "2103.06492", "submitter": "Joshua Daymude", "authors": "Robert Axelrod and Joshua J. Daymude and Stephanie Forrest", "title": "Preventing Extreme Polarization of Political Attitudes", "comments": "23 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme polarization can undermine democracy by making compromise impossible\nand transforming politics into a zero-sum game. Ideological polarization - the\nextent to which political views are widely dispersed - is already strong among\nelites, but less so among the general public (McCarty, 2019, p. 50-68). Strong\nmutual distrust and hostility between Democrats and Republicans in the U.S.,\ncombined with the elites' already strong ideological polarization, could lead\nto increasing ideological polarization among the public. The paper addresses\ntwo questions: (1) Is there a level of ideological polarization above which\npolarization feeds upon itself to become a runaway process? (2) If so, what\npolicy interventions could prevent such dangerous positive feedback loops? To\nexplore these questions, we present an agent-based model of ideological\npolarization that differentiates between the tendency for two actors to\ninteract (exposure) and how they respond when interactions occur, positing that\ninteraction between similar actors reduces their difference while interaction\nbetween dissimilar actors increases their difference. Our analysis explores the\neffects on polarization of different levels of tolerance to other views,\nresponsiveness to other views, exposure to dissimilar actors, multiple\nideological dimensions, economic self-interest, and external shocks. The\nresults suggest strategies for preventing, or at least slowing, the development\nof extreme polarization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:41:04 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 20:14:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Axelrod", "Robert", ""], ["Daymude", "Joshua J.", ""], ["Forrest", "Stephanie", ""]]}, {"id": "2103.06518", "submitter": "Hergys Rexha", "authors": "Hergys Rexha, Sebastien Lafond", "title": "Data Collection and Utilization Framework for Edge AI Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As data being produced by IoT applications continues to explode, there is a\ngrowing need to bring computing power closer to the source of the data to meet\nthe response time, power dissipation and cost goals of performance-critical\napplications in various domains like the Industrial Internet of Things (IIoT),\nAutomated Driving, Medical Imaging or Surveillance among others. This paper\nproposes a data collection and utilization framework that allows runtime\nplatform and application data to be sent to an edge and cloud system via data\ncollection agents running close to the platform. Agents are connected to a\ncloud system able to train AI models to improve overall energy efficiency of an\nAI application executed on an edge platform. In the implementation part, we\nshow the benefits of FPGA-based platform for the task of object detection.\nFurthermore, we show that it is feasible to collect relevant data from an FPGA\nplatform, transmit the data to a cloud system for processing and receiving\nfeedback actions to execute an edge AI application energy efficiently. As\nfuture work, we foresee the possibility to train, deploy and continuously\nimprove a base model able to efficiently adapt the execution of edge\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:07:29 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 07:30:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rexha", "Hergys", ""], ["Lafond", "Sebastien", ""]]}, {"id": "2103.06574", "submitter": "Mohammad Shaqfeh Ph.D.", "authors": "Mohammad Shaqfeh, Salah Hessien, Erchin Serpedin", "title": "Utility of Traffic Information in Dynamic Routing: Is Sharing\n  Information Always Useful?", "comments": "6 pages, 6 figures, published in 2020 IEEE 3rd Connected and\n  Automated Vehicles Symposium (CAVS)", "journal-ref": null, "doi": "10.1109/CAVS51000.2020.9334640", "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-time traffic information can be utilized to enhance the efficiency of\ntransportation networks by dynamically updating routing plans to mitigate\ntraffic jams. However, an interesting question is whether the network\ncoordinator should broadcast real-time traffic information to all network users\nor communicate it selectively to some of them. Which option enhances the\nnetwork efficiency more? In this work, we demonstrate using simulation\nexperiments that sharing real-time traffic information with all network users\nis sub-optimal, and it is actually better to share the information with a\nmajority subset of the total population in order to improve the overall network\nperformance. This result is valid under the assumption that each network user\ndecides its route to destination locally.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:02:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Shaqfeh", "Mohammad", ""], ["Hessien", "Salah", ""], ["Serpedin", "Erchin", ""]]}, {"id": "2103.07370", "submitter": "Andrew Clark", "authors": "Andrew G. Clark, Neil Walkinshaw, Robert M. Hierons", "title": "Test case generation for agent-based models: A systematic literature\n  review", "comments": null, "journal-ref": null, "doi": "10.1016/j.infsof.2021.106567", "report-no": null, "categories": "cs.SE cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Agent-based models play an important role in simulating complex emergent\nphenomena and supporting critical decisions. In this context, a software fault\nmay result in poorly informed decisions that lead to disastrous consequences.\nThe ability to rigorously test these models is therefore essential. In this\nsystematic literature review, we answer five research questions related to the\nkey aspects of test case generation in agent-based models: What are the\ninformation artifacts used to generate tests? How are these tests generated?\nHow is a verdict assigned to a generated test? How is the adequacy of a\ngenerated test suite measured? What level of abstraction of an agent-based\nmodel is targeted by a generated test? Our results show that whilst the\nmajority of techniques are effective for testing functional requirements at the\nagent and integration levels of abstraction, there are comparatively few\ntechniques capable of testing society-level behaviour. Additionally, we\nidentify a need for more thorough evaluation using realistic case studies that\nfeature challenging properties associated with a typical agent-based model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:07:12 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 09:56:13 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Clark", "Andrew G.", ""], ["Walkinshaw", "Neil", ""], ["Hierons", "Robert M.", ""]]}, {"id": "2103.07597", "submitter": "Amirali Salehi-Abari", "authors": "Sarina Sajadi Ghaemmaghami and Amirali Salehi-Abari", "title": "DeepGroup: Representation Learning for Group Recommendation with\n  Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group recommender systems facilitate group decision making for a set of\nindividuals (e.g., a group of friends, a team, a corporation, etc.). Many of\nthese systems, however, either assume that (i) user preferences can be elicited\n(or inferred) and then aggregated into group preferences or (ii) group\npreferences are partially observed/elicited. We focus on making recommendations\nfor a new group of users whose preferences are unknown, but we are given the\ndecisions/choices of other groups. By formulating this problem as group\nrecommendation from group implicit feedback, we focus on two of its practical\ninstances: group decision prediction and reverse social choice. Given a set of\ngroups and their observed decisions, group decision prediction intends to\npredict the decision of a new group of users, whereas reverse social choice\naims to infer the preferences of those users involved in observed group\ndecisions. These two problems are of interest to not only group recommendation,\nbut also to personal privacy when the users intend to conceal their personal\npreferences but have participated in group decisions. To tackle these two\nproblems, we propose and study DeepGroup -- a deep learning approach for group\nrecommendation with group implicit data. We empirically assess the predictive\npower of DeepGroup on various real-world datasets, group conditions (e.g.,\nhomophily or heterophily), and group decision (or voting) rules. Our extensive\nexperiments not only demonstrate the efficacy of DeepGroup, but also shed light\non the privacy-leakage concerns of some decision making processes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:05:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ghaemmaghami", "Sarina Sajadi", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2103.07635", "submitter": "Luyang Hou PhD", "authors": "Luyang Hou, Chun Wang and Jun Yan", "title": "Electric Vehicle Charging Scheduling in Green Logistics: Challenges,\n  Approaches and Opportunities", "comments": "Book chapter, 27 pages", "journal-ref": "Awasthi, A., 2019. Sustainable city logistics planning: methods\n  and applications (Vol. 1). Nova", "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Replacing a fossil fuel-powered car with an electric model can halve\ngreenhouse gas emissions over the course of the vehicle's lifetime and reduce\nthe noise pollution in urban areas. In green logistics, a well-scheduled\ncharging ensures an efficient operation of transportation and power systems\nand, at the same time, provides economical and satisfactory charging services\nfor drivers. This paper presents a taxonomy of current electric vehicle\ncharging scheduling problems in green logistics by analyzing its unique\nfeatures with some typical use cases, such as space assignment, routing and\nenergy management; discusses the challenges, i.e., the information availability\nand stakeholders' strategic behaviors that arise in stochastic and\ndecentralized environments; and classifies the existing approaches, as\ncentralized, distributed and decentralized ones, that apply to these\nchallenges. Moreover, we discuss research opportunities in applying\nmarket-based mechanisms, which shall be coordinated with stochastic\noptimization and machine learning, to the decentralized, dynamic and\ndata-driven charging scheduling problems for the management of the future green\nlogistics.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:47:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hou", "Luyang", ""], ["Wang", "Chun", ""], ["Yan", "Jun", ""]]}, {"id": "2103.07714", "submitter": "Daniel Jarne Ornia", "authors": "Daniel Jarne Ornia, Pedro J Zufiria, Manuel Mazo Jr", "title": "Mean Field Behaviour of Collaborative Multi-Agent Foragers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative multi-agent robotic systems where agents coordinate by\nmodifying a shared environment often result in undesired dynamical couplings\nthat complicate the analysis and experiments when solving a specific problem or\ntask. Simultaneously, biologically-inspired robotics rely on simplifying agents\nand increasing their number to obtain more efficient solutions to such\nproblems, drawing similarities with natural processes. In this work we focus on\nthe problem of a biologically-inspired multi-agent system solving collaborative\nforaging. We show how mean field techniques can be used to re-formulate such a\nstochastic multi-agent problem into a deterministic autonomous system. This\nde-couples agent dynamics, enabling the computation of limit behaviours and the\nanalysis of optimality guarantees. Furthermore, we analyse how having finite\nnumber of agents affects the performance when compared to the mean field limit\nand we discuss the implications of such limit approximations in this\nmulti-agent system, which have impact on more general collaborative stochastic\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 13:12:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ornia", "Daniel Jarne", ""], ["Zufiria", "Pedro J", ""], ["Mazo", "Manuel", "Jr"]]}, {"id": "2103.07927", "submitter": "Yaodong Yang Mr.", "authors": "Nicolas Perez Nieves, Yaodong Yang, Oliver Slumbers, David Henry\n  Mguni, Ying Wen, Jun Wang", "title": "Modelling Behavioural Diversity for Learning in Open-Ended Games", "comments": "corresponds to <yaodong.yang@cs.ucl.ac.uk>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoting behavioural diversity is critical for solving games with\nnon-transitive dynamics where strategic cycles exist, and there is no\nconsistent winner (e.g., Rock-Paper-Scissors). Yet, there is a lack of rigorous\ntreatment for defining diversity and constructing diversity-aware learning\ndynamics. In this work, we offer a geometric interpretation of behavioural\ndiversity in games and introduce a novel diversity metric based on\ndeterminantal point processes (DPP). By incorporating the diversity metric into\nbest-response dynamics, we develop diverse fictitious play and diverse\npolicy-space response oracle for solving normal-form games and open-ended\ngames. We prove the uniqueness of the diverse best response and the convergence\nof our algorithms on two-player games. Importantly, we show that maximising the\nDPP-based diversity metric guarantees to enlarge the gamescape -- convex\npolytopes spanned by agents' mixtures of strategies. To validate our\ndiversity-aware solvers, we test on tens of games that show strong\nnon-transitivity. Results suggest that our methods achieve at least the same,\nand in most games, lower exploitability than PSRO solvers by finding effective\nand diverse strategies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 13:42:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 10:03:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nieves", "Nicolas Perez", ""], ["Yang", "Yaodong", ""], ["Slumbers", "Oliver", ""], ["Mguni", "David Henry", ""], ["Wen", "Ying", ""], ["Wang", "Jun", ""]]}, {"id": "2103.08067", "submitter": "Kalesha Bullard", "authors": "Kalesha Bullard, Douwe Kiela, Franziska Meier, Joelle Pineau, Jakob\n  Foerster", "title": "Quasi-Equivalence Discovery for Zero-Shot Emergent Communication", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective communication is an important skill for enabling information\nexchange in multi-agent settings and emergent communication is now a vibrant\nfield of research, with common settings involving discrete cheap-talk channels.\nSince, by definition, these settings involve arbitrary encoding of information,\ntypically they do not allow for the learned protocols to generalize beyond\ntraining partners. In contrast, in this work, we present a novel problem\nsetting and the Quasi-Equivalence Discovery (QED) algorithm that allows for\nzero-shot coordination (ZSC), i.e., discovering protocols that can generalize\nto independently trained agents. Real world problem settings often contain\ncostly communication channels, e.g., robots have to physically move their\nlimbs, and a non-uniform distribution over intents. We show that these two\nfactors lead to unique optimal ZSC policies in referential games, where agents\nuse the energy cost of the messages to communicate intent. Other-Play was\nrecently introduced for learning optimal ZSC policies, but requires prior\naccess to the symmetries of the problem. Instead, QED can iteratively discovers\nthe symmetries in this setting and converges to the optimal ZSC policy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:42:37 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:52:57 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Bullard", "Kalesha", ""], ["Kiela", "Douwe", ""], ["Meier", "Franziska", ""], ["Pineau", "Joelle", ""], ["Foerster", "Jakob", ""]]}, {"id": "2103.08529", "submitter": "Stefanos Leonardos Mr.", "authors": "Yun Kuen Cheung, Stefanos Leonardos, Georgios Piliouras", "title": "Learning in Markets: Greed Leads to Chaos but Following the Price is\n  Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA econ.TH math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study learning dynamics in distributed production economies such as\nblockchain mining, peer-to-peer file sharing and crowdsourcing. These economies\ncan be modelled as multi-product Cournot competitions or all-pay auctions\n(Tullock contests) when individual firms have market power, or as Fisher\nmarkets with quasi-linear utilities when every firm has negligible influence on\nmarket outcomes. In the former case, we provide a formal proof that Gradient\nAscent (GA) can be Li-Yorke chaotic for a step size as small as $\\Theta(1/n)$,\nwhere $n$ is the number of firms. In stark contrast, for the Fisher market\ncase, we derive a Proportional Response (PR) protocol that converges to market\nequilibrium. The positive results on the convergence of the PR dynamics are\nobtained in full generality, in the sense that they hold for Fisher markets\nwith \\emph{any} quasi-linear utility functions. Conversely, the chaos results\nfor the GA dynamics are established even in the simplest possible setting of\ntwo firms and one good, and they hold for a wide range of price functions with\ndifferent demand elasticities. Our findings suggest that by considering\nmulti-agent interactions from a market rather than a game-theoretic\nperspective, we can formally derive natural learning protocols which are stable\nand converge to effective outcomes rather than being chaotic.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:48:30 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:33:36 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cheung", "Yun Kuen", ""], ["Leonardos", "Stefanos", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2103.08968", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang and Florian Meyer", "title": "Graph-Based Multiobject Tracking with Embedded Particle Flow", "comments": "6 pages, 2 figures. To be published in Proc. IEEE RadarConf-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Seamless situational awareness provided by modern radar systems relies on\neffective methods for multiobject tracking (MOT). This paper presents a\ngraph-based Bayesian method for nonlinear and high-dimensional MOT problems\nthat embeds particle flow. To perform operations on the graph effectively,\nparticles are migrated towards regions of high likelihood based on the solution\nof a partial differential equation. This makes it possible to obtain good\nobject detection and tracking performance with a relatively small number of\nparticles even if object states are high dimensional and sensor measurements\nare very informative. Simulation results demonstrate reduced computational\ncomplexity and memory requirements as well as favorable detection and\nestimation accuracy in a challenging 3-D MOT scenario.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:46:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Wenyu", ""], ["Meyer", "Florian", ""]]}, {"id": "2103.09184", "submitter": "Hubert P. H. Shum", "authors": "John Hartley, Hubert P. H. Shum, Edmond S. L. Ho, He Wang, Subramanian\n  Ramamoorthy", "title": "Formation Control for UAVs Using a Flux Guided Approach", "comments": "8 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While multiple studies have proposed methods for the formation control of\nunmanned aerial vehicles (UAV), the trajectories generated are generally\nunsuitable for tracking targets where the optimum coverage of the target by the\nformation is required at all times. We propose a path planning approach called\nthe Flux Guided (FG) method, which generates collision-free trajectories while\nmaximising the coverage of one or more targets. We show that by reformulating\nan existing least-squares flux minimisation problem as a constrained\noptimisation problem, the paths obtained are $1.5 \\times$ shorter and track\ndirectly toward the target. Also, we demonstrate that the scale of the\nformation can be controlled during flight, and that this feature can be used to\ntrack multiple scattered targets. The method is highly scalable since the\nplanning algorithm is only required for a sub-set of UAVs on the open boundary\nof the formation's surface. Finally, through simulating a 3d dynamic particle\nsystem that tracks the desired trajectories using a PID controller, we show\nthat the resulting trajectories after time-optimal parameterisation are\nsuitable for robotic controls.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:30:56 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Hartley", "John", ""], ["Shum", "Hubert P. H.", ""], ["Ho", "Edmond S. L.", ""], ["Wang", "He", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "2103.09284", "submitter": "Yaodong Yang Mr.", "authors": "David Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li,\n  Ying Wen, Joel Jennings, Jun Wang", "title": "Learning in Nonzero-Sum Stochastic Games with Potentials", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) has become effective in tackling\ndiscrete cooperative game scenarios. However, MARL has yet to penetrate\nsettings beyond those modelled by team and zero-sum games, confining it to a\nsmall subset of multi-agent systems. In this paper, we introduce a new\ngeneration of MARL learners that can handle nonzero-sum payoff structures and\ncontinuous settings. In particular, we study the MARL problem in a class of\ngames known as stochastic potential games (SPGs) with continuous state-action\nspaces. Unlike cooperative games, in which all agents share a common reward,\nSPGs are capable of modelling real-world scenarios where agents seek to fulfil\ntheir individual goals. We prove theoretically our learning method, SPot-AC,\nenables independent agents to learn Nash equilibrium strategies in polynomial\ntime. We demonstrate our framework tackles previously unsolvable tasks such as\nCoordination Navigation and large selfish routing games and that it outperforms\nthe state of the art MARL baselines such as MADDPG and COMIX in such scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 19:02:01 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 15:45:52 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 23:25:15 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 10:55:47 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Mguni", "David", ""], ["Wu", "Yutong", ""], ["Du", "Yali", ""], ["Yang", "Yaodong", ""], ["Wang", "Ziyi", ""], ["Li", "Minne", ""], ["Wen", "Ying", ""], ["Jennings", "Joel", ""], ["Wang", "Jun", ""]]}, {"id": "2103.09520", "submitter": "Roi Yehoshua", "authors": "Roi Yehoshua, Juan Heredia-Juesas, Yushu Wu, Christopher Amato, Jose\n  Martinez-Lorenzo", "title": "Decentralized Reinforcement Learning for Multi-Target Search and\n  Detection by a Team of Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targets search and detection encompasses a variety of decision problems such\nas coverage, surveillance, search, observing and pursuit-evasion along with\nothers. In this paper we develop a multi-agent deep reinforcement learning\n(MADRL) method to coordinate a group of aerial vehicles (drones) for the\npurpose of locating a set of static targets in an unknown area. To that end, we\nhave designed a realistic drone simulator that replicates the dynamics and\nperturbations of a real experiment, including statistical inferences taken from\nexperimental data for its modeling. Our reinforcement learning method, which\nutilized this simulator for training, was able to find near-optimal policies\nfor the drones. In contrast to other state-of-the-art MADRL methods, our method\nis fully decentralized during both learning and execution, can handle\nhigh-dimensional and continuous observation spaces, and does not require tuning\nof additional hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:04:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Yehoshua", "Roi", ""], ["Heredia-Juesas", "Juan", ""], ["Wu", "Yushu", ""], ["Amato", "Christopher", ""], ["Martinez-Lorenzo", "Jose", ""]]}, {"id": "2103.09636", "submitter": "Luidnel Maignan", "authors": "Alexandre Fernandez (LACL), Luidnel Maignan (LACL), Antoine Spicher\n  (LACL)", "title": "Accretive Computation of Global Transformations of Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA math.CT nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of global transformations aims at describing synchronous\nrewriting systems on a given data structure. In this work we focus on the data\nstructure of graphs. Global transformations of graphs are defined and a local\ncriterion is given for a rule system to extend to a graph global\ntransformation. Finally we present an algorithm, with its correction, which\ncomputes online the global transformation of a finite graph in an accretive\nmanner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:24:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Fernandez", "Alexandre", "", "LACL"], ["Maignan", "Luidnel", "", "LACL"], ["Spicher", "Antoine", "", "LACL"]]}, {"id": "2103.09845", "submitter": "Sarper Ayd{\\i}n", "authors": "Sarper Ayd{\\i}n, Sina Arefizadeh and Ceyhun Eksin", "title": "Decentralized Fictitious Play in Near-Potential Games with Time-Varying\n  Communication Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the convergence properties of decentralized fictitious play (DFP)\nfor the class of near-potential games where the incentives of agents are nearly\naligned with a potential function. In DFP, agents share information only with\ntheir current neighbors in a sequence of time-varying networks, keep estimates\nof other agents' empirical frequencies, and take actions to maximize their\nexpected utility functions computed with respect to the estimated empirical\nfrequencies. We show that empirical frequencies of actions converge to a set of\nstrategies with potential function values that are larger than the potential\nfunction values obtained by approximate Nash equilibria of the closest\npotential game. This result establishes that DFP has identical convergence\nguarantees in near-potential games as the standard fictitious play in which\nagents observe the past actions of all the other agents.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:12:42 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ayd\u0131n", "Sarper", ""], ["Arefizadeh", "Sina", ""], ["Eksin", "Ceyhun", ""]]}, {"id": "2103.09856", "submitter": "Raz Saremi", "authors": "Razieh Saremi, Ye Yang, Gregg Vesonder, Guenther Ruhe, He Zhang", "title": "CrowdSim: A Hybrid Simulation Model for Failure Prediction in\n  Crowdsourced Software Development", "comments": "13 pages paper, 5 pages Appendix, 9 Figure, 6 Tables, 1 algorithm\n  This work has been submitted to the IEEE Transactions on Systems, Man, and\n  Cybernetics: Systems for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical crowdsourcing software development(CSD) marketplace consists of a\nlist of software tasks as service demands and a pool of freelancer developers\nas service suppliers. Highly dynamic and competitive CSD market places may\nresult in task failure due to unforeseen risks, such as increased competition\nover shared worker supply, or uncertainty associated with workers' experience\nand skills, and so on. To improve CSD effectiveness, it is essential to better\nunderstand and plan with respect to dynamic worker characteristics and risks\nassociated with CSD processes. In this paper, we present a hybrid simulation\nmodel, CrowdSim, to forecast crowdsourcing task failure risk in competitive CSD\nplatforms. CrowdSim is composed of three layered components: the macro-level\nreflects the overall crowdsourcing platform based on system dynamics,the\nmeso-level represents the task life cycle based on discrete event simulation,\nand the micro-level models the crowd workers' decision-making processes based\non agent-based simulation. CrowdSim is evaluated through three CSD decision\nscenarios to demonstrate its effectiveness, using a real-world historical\ndataset and the results demonstrate CrowdSim's potential in empowering\ncrowdsourcing managers to explore crowdsourcing outcomes with respect to\ndifferent task scheduling options.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:41:53 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Saremi", "Razieh", ""], ["Yang", "Ye", ""], ["Vesonder", "Gregg", ""], ["Ruhe", "Guenther", ""], ["Zhang", "He", ""]]}, {"id": "2103.10280", "submitter": "Christoph Welzel", "authors": "Javier Esparza, Mikhail Raskin, Christoph Welzel", "title": "Computing Parameterized Invariants of Parameterized Petri Nets", "comments": "Extended version; accepted at Petri nets'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental advantage of Petri net models is the possibility to\nautomatically compute useful system invariants from the syntax of the net.\nClassical techniques used for this are place invariants, P-components, siphons\nor traps. Recently, Bozga et al. have presented a novel technique for the\n\\emph{parameterized} verification of safety properties of systems with a ring\nor array architecture. They show that the statement \\enquote{for every instance\nof the parameterized Petri net, all markings satisfying the linear invariants\nassociated to all the P-components, siphons and traps of the instance are safe}\ncan be encoded in \\acs{WS1S} and checked using tools like MONA. However, while\nthe technique certifies that this infinite set of linear invariants extracted\nfrom P-components, siphons or traps are strong enough to prove safety, it does\nnot return an explanation of this fact understandable by humans. We present a\nCEGAR loop that constructs a \\emph{finite} set of \\emph{parameterized}\nP-components, siphons or traps, whose infinitely many instances are strong\nenough to prove safety. For this we design parameterization procedures for\ndifferent architectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:22:42 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:40:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Esparza", "Javier", ""], ["Raskin", "Mikhail", ""], ["Welzel", "Christoph", ""]]}, {"id": "2103.10316", "submitter": "Madhu Vadali", "authors": "Rohith G and Madhu Vadali", "title": "A Quasi-centralized Collision-free Path Planning Approach for\n  Multi-Robot Systems", "comments": "6 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a novel quasi-centralized approach for collision-free\npath planning of multi-robot systems (MRS) in obstacle-ridden environments. A\nnew formation potential fields (FPF) concept is proposed around a virtual\nagent, located at the center of the formation which ensures self-organization\nand maintenance of the formation. The path of the virtual agent is centrally\nplanned and the robots at the minima of the FPF are forced to move along with\nthe virtual agent. In the neighborhood of obstacles, individual robots\nselfishly avoid collisions, thus marginally deviating from the formation. The\nproposed quasi-centralized approach introduces formation flexibility into the\nMRS, which enables MRS to effectively navigate in an obstacle-ridden workspace.\nMethodical analysis of the proposed approach and guidelines for selecting the\nFPF are presented. Results using a candidate FPF are shown that ensure a\npentagonal formation effectively squeezes through a narrow passage avoiding any\ncollisions with the walls.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:19:50 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["G", "Rohith", ""], ["Vadali", "Madhu", ""]]}, {"id": "2103.10752", "submitter": "Takehiro Tottori", "authors": "Takehiro Tottori and Tetsuya J. Kobayashi", "title": "Forward and Backward Bellman equations improve the efficiency of EM\n  algorithm for DEC-POMDP", "comments": null, "journal-ref": "Entropy 2021, 23, 551", "doi": "10.3390/e23050551", "report-no": null, "categories": "cs.LG cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized partially observable Markov decision process (DEC-POMDP) models\nsequential decision making problems by a team of agents. Since the planning of\nDEC-POMDP can be interpreted as the maximum likelihood estimation for the\nlatent variable model, DEC-POMDP can be solved by the EM algorithm. However, in\nEM for DEC-POMDP, the forward--backward algorithm needs to be calculated up to\nthe infinite horizon, which impairs the computational efficiency. In this\npaper, we propose the Bellman EM algorithm (BEM) and the modified Bellman EM\nalgorithm (MBEM) by introducing the forward and backward Bellman equations into\nEM. BEM can be more efficient than EM because BEM calculates the forward and\nbackward Bellman equations instead of the forward--backward algorithm up to the\ninfinite horizon. However, BEM cannot always be more efficient than EM when the\nsize of problems is large because BEM calculates an inverse matrix. We\ncircumvent this shortcoming in MBEM by calculating the forward and backward\nBellman equations without the inverse matrix. Our numerical experiments\ndemonstrate that the convergence of MBEM is faster than that of EM.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:35:58 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:33:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tottori", "Takehiro", ""], ["Kobayashi", "Tetsuya J.", ""]]}, {"id": "2103.11067", "submitter": "Federico Rossi", "authors": "Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, Marco Pavone", "title": "Multi-Agent Algorithms for Collective Behavior: A structural and\n  application-focused atlas", "comments": "Under review for journal publication. Revised and extended version of\n  arXiv:1803.05464", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to provide a survey and application-focused atlas\nof collective behavior coordination algorithms for multi-agent systems.\n  We survey the general family of collective behavior algorithms for\nmulti-agent systems and classify them according to their underlying\nmathematical structure. In doing so, we aim to capture fundamental mathematical\nproperties of algorithms (e.g., scalability with respect to the number of\nagents and bandwidth use) and to show how the same algorithm or family of\nalgorithms can be used for multiple tasks and applications.\n  Collectively, this paper provides an application-focused atlas of algorithms\nfor collective behavior of multi-agent systems, with three objectives:\n  1. to act as a tutorial guide to practitioners in the selection of\ncoordination algorithms for a given application;\n  2. to highlight how mathematically similar algorithms can be used for a\nvariety of tasks, ranging from low-level control to high-level coordination;\n  3. to explore the state-of-the-art in the field of control of multi-agent\nsystems and identify areas for future research.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 00:37:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rossi", "Federico", ""], ["Bandyopadhyay", "Saptarshi", ""], ["Wolf", "Michael T.", ""], ["Pavone", "Marco", ""]]}, {"id": "2103.11210", "submitter": "Maria H\\\"anel Dr.", "authors": "Maria L. H\\\"anel and Carola-B. Sch\\\"onlieb", "title": "Efficient Global Optimization of Non-differentiable, Symmetric\n  Objectives for Multi Camera Placement", "comments": "Submitted to be reviewed, 10 pages, 6 figures, 2 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel iterative method for optimally placing and orienting\nmultiple cameras in a 3D scene. Sample applications include improving the\naccuracy of 3D reconstruction, maximizing the covered area for surveillance, or\nimproving the coverage in multi-viewpoint pedestrian tracking. Our algorithm is\nbased on a block-coordinate ascent combined with a surrogate function and an\nexclusion area technique. This allows to flexibly handle difficult objective\nfunctions that are often expensive and quantized or non-differentiable. The\nsolver is globally convergent and easily parallelizable. We show how to\naccelerate the optimization by exploiting special properties of the objective\nfunction, such as symmetry. Additionally, we discuss the trade-off between\nnon-optimal stationary points and the cost reduction when optimizing the\nviewpoints consecutively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:01:15 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["H\u00e4nel", "Maria L.", ""], ["Sch\u00f6nlieb", "Carola-B.", ""]]}, {"id": "2103.11300", "submitter": "Zhenyu Shi", "authors": "Zhenyu Shi, Wei Wei, Xiangnan Feng, Ruizhi Zhang and Zhiming Zheng", "title": "Effects of Dynamic-Win-Stay-Lose-Learn model with voluntary\n  participation in social dilemma", "comments": "13 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Win-Stay-Lose-Learn rule has attracted wide attention as an\neffective strategy updating rule, and voluntary participation is proposed by\nintroducing a third strategy in Prisoner's dilemma game. Some researches show\nthat combining Win-Stay-Lose-Learn rule with voluntary participation could\npromote cooperation more significantly under moderate temptation values,\nhowever, cooperators' survival under high aspiration levels and high temptation\nvalues is still a challenging problem. In this paper, inspired by Achievement\nMotivation Theory, a Dynamic-Win-Stay-Lose-Learn rule with voluntary\nparticipation is investigated, where a dynamic aspiration process is introduced\nto describe the co-evolution of individuals' strategies and aspirations. It is\nfound that cooperation is extremely promoted and defection is almost extinct in\nour model, even when the initial aspiration levels and temptation values are\nhigh. The combination of dynamic aspiration and voluntary participation plays\nan active role since loners could survive under high initial aspiration levels\nand they will expand stably because of their fixed payoffs. The robustness of\nour model is also discussed and some adverse structures are found which should\nbe alerted in the evolutionary process. Our work provides a more realistic\nmodel and shows that cooperators may prevail defectors in an unfavorable\ninitial environment.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 04:30:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Shi", "Zhenyu", ""], ["Wei", "Wei", ""], ["Feng", "Xiangnan", ""], ["Zhang", "Ruizhi", ""], ["Zheng", "Zhiming", ""]]}, {"id": "2103.11341", "submitter": "Dave Cliff", "authors": "Dave Cliff", "title": "Parameterised-Response Zero-Intelligence Traders", "comments": "39 pages, 18 figures, 67 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.CE cs.GT cs.MA econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce PRZI (Parameterised-Response Zero Intelligence), a new form of\nzero-intelligence trader intended for use in simulation studies of auction\nmarkets. Like Gode & Sunder's classic Zero-Intelligence Constrained (ZIC)\ntrader, PRZI generates quote-prices from a random distribution over some\nspecified domain of discretely-valued allowable quote-prices. Unlike ZIC, which\nuses a uniform distribution to generate prices, the probability distribution in\na PRZI trader is parameterised in such a way that its probability mass function\n(PMF) is determined by a real-valued control variable s in the range [-1.0,\n+1.0] that determines the strategy for that trader. When s is zero, a PRZI\ntrader behaves identically to the ZIC strategy, with a flat/rectangular PMF;\nbut when s is close to plus or minus one the PRZI trader's PMF becomes\nasymptotically maximally skewed to one extreme or the other of the price-range,\nthereby enabling the PRZI trader to act in the same way as the \"Shaver\"\nstrategy (SHVR) or the \"Giveaway\" strategy (GVWY), both of which have recently\nbeen demonstrated to be surprisingly dominant over more sophisticated, and\nsupposedly more profitable, trader-strategies that incorporate adaptive\nmechanisms and machine learning. Depending on the value of s, a PRZI trader\nwill behave either as a ZIC, or as a SHVR, or as a GVWY, or as some hybrid\nstrategy part-way between two of these three previously-reported strategies.\nThe novel smoothly-varying strategy in PRZI has value in giving trader-agents\nplausibly useful \"market impact\" responses to imbalances in an auction-market's\nlimit-order-book, and also allows for the study of co-adaptive dynamics in\ncontinuous strategy-spaces rather than the discrete spaces that have\ntraditionally been studied in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:43:39 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 19:09:48 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 13:53:31 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 21:43:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Cliff", "Dave", ""]]}, {"id": "2103.11517", "submitter": "Prashank Kadam", "authors": "Prashank Kadam, Ruiyang Xu, Karl Lieberherr", "title": "Dual Monte Carlo Tree Search", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AlphaZero, using a combination of Deep Neural Networks and Monte Carlo Tree\nSearch (MCTS), has successfully trained reinforcement learning agents in a\ntabula-rasa way. The neural MCTS algorithm has been successful in finding\nnear-optimal strategies for games through self-play. However, the AlphaZero\nalgorithm has a significant drawback; it takes a long time to converge and\nrequires high computational power due to complex neural networks for solving\ngames like Chess, Go, Shogi, etc. Owing to this, it is very difficult to pursue\nneural MCTS research without cutting-edge hardware, which is a roadblock for\nmany aspiring neural MCTS researchers. In this paper, we propose a new neural\nMCTS algorithm, called Dual MCTS, which helps overcome these drawbacks. Dual\nMCTS uses two different search trees, a single deep neural network, and a new\nupdate technique for the search trees using a combination of the PUCB, a\nsliding-window, and the epsilon-greedy algorithm. This technique is applicable\nto any MCTS based algorithm to reduce the number of updates to the tree. We\nshow that Dual MCTS performs better than one of the most widely used neural\nMCTS algorithms, AlphaZero, for various symmetric and asymmetric games.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:34:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kadam", "Prashank", ""], ["Xu", "Ruiyang", ""], ["Lieberherr", "Karl", ""]]}, {"id": "2103.11572", "submitter": "Siavash Alemzadeh", "authors": "Siavash Alemzadeh, Shahriar Talebi, Mehran Mesbahi", "title": "D3PI: Data-Driven Distributed Policy Iteration for Homogeneous\n  Interconnected Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control of large-scale networked systems often necessitates the availability\nof complex models for the interactions amongst the agents. While building\naccurate models of these interactions could become prohibitive in many\napplications, data-driven control methods can circumvent model complexities by\ndirectly synthesizing a controller from the observed data. In this paper, we\npropose the Data-Driven Distributed Policy Iteration (D3PI) algorithm to design\na feedback mechanism for a potentially large system that enjoys an underlying\ngraph structure characterizing communications among the agents. Rather than\nhaving access to system parameters, our algorithm requires temporary\n\"auxiliary\" links to boost information exchange of a small portion of the graph\nduring the learning phase. Therein, the costs are partitioned for learning and\nnon-learning agents in order to ensure consistent control of the entire\nnetwork. After the termination of the learning process, a distributed policy is\nproposed for the entire networked system by leveraging estimated components\nobtained in the learning phase. We provide extensive stability and convergence\nguarantees of the proposed distributed controller throughout the learning phase\nby exploiting the structure of the system parameters that occur due to the\ngraph topology and existence of the temporary links. The practicality of our\nmethod is then illustrated with a simulation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:47:47 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Alemzadeh", "Siavash", ""], ["Talebi", "Shahriar", ""], ["Mesbahi", "Mehran", ""]]}, {"id": "2103.11574", "submitter": "Aseem Borkar", "authors": "Aseem Vivek Borkar and Girish Chowdhary", "title": "Multi-agent Aerial Monitoring of Moving Convoys using Elliptical Orbits", "comments": "This is the extended version of the paper with same title published\n  in International Conference on Robotics and Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel scheme for surveillance of a dynamic ground convoy moving\nalong a non-linear trajectory, by aerial agents that maintain a uniformly\nspaced formation on a time-varying elliptical orbit encompassing the convoy.\nElliptical orbits are used as they are more economical than circular orbits for\ncircumnavigating the group of targets in the moving convoy. The proposed scheme\nincludes an algorithm for computing feasible elliptical orbits, a vector\nguidance law for agent motion along the desired orbit, and a cooperative\nstrategy to control the speeds of the aerial agents in order to quickly achieve\nand maintain the desired formation. It achieves mission objectives while\naccounting for linear and angular speed constraints on the aerial agents. The\nscheme is validated through simulations and actual experiments with a convoy of\nground robots and a team of quadrotors as the aerial agents, in a motion\ncapture environment.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 04:00:07 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Borkar", "Aseem Vivek", ""], ["Chowdhary", "Girish", ""]]}, {"id": "2103.11883", "submitter": "Ling Pan", "authors": "Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, Shimon Whiteson", "title": "Regularized Softmax Deep Multi-Agent $Q$-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling overestimation in $Q$-learning is an important problem that has been\nextensively studied in single-agent reinforcement learning, but has received\ncomparatively little attention in the multi-agent setting. In this work, we\nempirically demonstrate that QMIX, a popular $Q$-learning algorithm for\ncooperative multi-agent reinforcement learning (MARL), suffers from a more\nsevere overestimation in practice than previously acknowledged, and is not\nmitigated by existing approaches. We rectify this with a novel\nregularization-based update scheme that penalizes large joint action-values\nthat deviate from a baseline and demonstrate its effectiveness in stabilizing\nlearning. Furthermore, we propose to employ a softmax operator, which we\nefficiently approximate in a novel way in the multi-agent setting, to further\nreduce the potential overestimation bias. Our approach, Regularized Softmax\n(RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any\n$Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX,\nRES avoids severe overestimation and significantly improves performance,\nyielding state-of-the-art results in a variety of cooperative multi-agent\ntasks, including the challenging StarCraft II micromanagement benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:18:39 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 03:33:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pan", "Ling", ""], ["Rashid", "Tabish", ""], ["Peng", "Bei", ""], ["Huang", "Longbo", ""], ["Whiteson", "Shimon", ""]]}, {"id": "2103.12070", "submitter": "Christoph Killing", "authors": "Christoph Killing, Adam Villaflor, John M. Dolan", "title": "Learning to Robustly Negotiate Bi-Directional Lane Usage in\n  High-Conflict Driving Scenarios", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, autonomous driving has made substantial progress in addressing the\nmost common traffic scenarios like intersection navigation and lane changing.\nHowever, most of these successes have been limited to scenarios with\nwell-defined traffic rules and require minimal negotiation with other vehicles.\nIn this paper, we introduce a previously unconsidered, yet everyday,\nhigh-conflict driving scenario requiring negotiations between agents of equal\nrights and priorities. There exists no centralized control structure and we do\nnot allow communications. Therefore, it is unknown if other drivers are willing\nto cooperate, and if so to what extent. We train policies to robustly negotiate\nwith opposing vehicles of an unobservable degree of cooperativeness using\nmulti-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft\nActor-Critic (DASAC), a maximum-entropy off-policy MARL algorithm allowing for\ncentralized training with decentralized execution. We show that using DASAC we\nare able to successfully negotiate and traverse the scenario considered over\n99% of the time. Our agents are robust to an unknown timing of opponent\ndecisions, an unobservable degree of cooperativeness of the opposing vehicle,\nand previously unencountered policies. Furthermore, they learn to exhibit\nhuman-like behaviors such as defensive driving, anticipating solution options\nand interpreting the behavior of other agents.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:46:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Killing", "Christoph", ""], ["Villaflor", "Adam", ""], ["Dolan", "John M.", ""]]}, {"id": "2103.12184", "submitter": "Jan Prosi", "authors": "Jan Prosi, Sina Khajehabdollahi, Emmanouil Giannakakis, Georg Martius\n  and Anna Levina", "title": "The dynamical regime and its importance for evolvability, task\n  performance and generalization", "comments": "8 Pages, 7 Figures, Artificial Life Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.MA nlin.AO q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has long been hypothesized that operating close to the critical state is\nbeneficial for natural and artificial systems. We test this hypothesis by\nevolving foraging agents controlled by neural networks that can change the\nsystem's dynamical regime throughout evolution. Surprisingly, we find that all\npopulations, regardless of their initial regime, evolve to be subcritical in\nsimple tasks and even strongly subcritical populations can reach comparable\nperformance. We hypothesize that the moderately subcritical regime combines the\nbenefits of generalizability and adaptability brought by closeness to\ncriticality with the stability of the dynamics characteristic for subcritical\nsystems. By a resilience analysis, we find that initially critical agents\nmaintain their fitness level even under environmental changes and degrade\nslowly with increasing perturbation strength. On the other hand, subcritical\nagents originally evolved to the same fitness, were often rendered utterly\ninadequate and degraded faster. We conclude that although the subcritical\nregime is preferable for a simple task, the optimal deviation from criticality\ndepends on the task difficulty: for harder tasks, agents evolve closer to\ncriticality. Furthermore, subcritical populations cannot find the path to\ndecrease their distance to criticality. In summary, our study suggests that\ninitializing models near criticality is important to find an optimal and\nflexible solution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 21:22:52 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prosi", "Jan", ""], ["Khajehabdollahi", "Sina", ""], ["Giannakakis", "Emmanouil", ""], ["Martius", "Georg", ""], ["Levina", "Anna", ""]]}, {"id": "2103.12192", "submitter": "Shufan Yang", "authors": "Changgang Zheng, Shufan Yang, Juan Parra-Ullauri, Antonio\n  Garcia-Dominguez, and Nelly Bencomo", "title": "Reward-Reinforced Reinforcement Learning for Multi-agent Systems", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reinforcement learning algorithms in multi-agent systems deliver highly\nresilient and adaptable solutions for common problems in\ntelecommunications,aerospace, and industrial robotics. However, achieving an\noptimal global goal remains a persistent obstacle for collaborative multi-agent\nsystems, where learning affects the behaviour of more than one agent. A number\nof nonlinear function approximation methods have been proposed for solving the\nBellman equation, which describe a recursive format of an optimal policy.\nHowever, how to leverage the value distribution based on reinforcement\nlearning, and how to improve the efficiency and efficacy of such systems remain\na challenge. In this work, we developed a reward-reinforced generative\nadversarial network to represent the distribution of the value function,\nreplacing the approximation of Bellman updates. We demonstrated our method is\nresilient and outperforms other conventional reinforcement learning methods.\nThis method is also applied to a practical case study: maximising the number of\nuser connections to autonomous airborne base stations in a mobile communication\nnetwork. Our method maximises the data likelihood using a cost function under\nwhich agents have optimal learned behaviours. This reward-reinforced generative\nadversarial network can be used as ageneric framework for multi-agent learning\nat the system level\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 21:50:09 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 21:35:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zheng", "Changgang", ""], ["Yang", "Shufan", ""], ["Parra-Ullauri", "Juan", ""], ["Garcia-Dominguez", "Antonio", ""], ["Bencomo", "Nelly", ""]]}, {"id": "2103.12476", "submitter": "Philipp Andelfinger", "authors": "Philipp Andelfinger", "title": "Differentiable Agent-Based Simulation for Gradient-Guided\n  Simulation-Based Optimization", "comments": "Accepted at the 2021 ACM SIGSIM Conference Conference on Principles\n  of Advanced Discrete Simulation (PADS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based optimization using agent-based models is typically carried\nout under the assumption that the gradient describing the sensitivity of the\nsimulation output to the input cannot be evaluated directly. To still apply\ngradient-based optimization methods, which efficiently steer the optimization\ntowards a local optimum, gradient estimation methods can be employed. However,\nmany simulation runs are needed to obtain accurate estimates if the input\ndimension is large. Automatic differentiation (AD) is a family of techniques to\ncompute gradients of general programs directly. Here, we explore the use of AD\nin the context of time-driven agent-based simulations. By substituting common\ndiscrete model elements such as conditional branching with smooth\napproximations, we obtain gradient information across discontinuities in the\nmodel logic. On the example of microscopic traffic models and an epidemics\nmodel, we study the fidelity and overhead of the differentiable models, as well\nas the convergence speed and solution quality achieved by gradient-based\noptimization compared to gradient-free methods. In traffic signal timing\noptimization problems with high input dimension, the gradient-based methods\nexhibit substantially superior performance. Finally, we demonstrate that the\napproach enables gradient-based training of neural network-controlled\nsimulation entities embedded in the model logic.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:58:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Andelfinger", "Philipp", ""]]}, {"id": "2103.12519", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno (MICS), Lorenzo Zino", "title": "A multi-layer network model to assess school opening policies during the\n  COVID-19 vaccination campaign", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-layer network model for the spread of COVID-19 that\naccounts for interactions within the family, between schoolmates, and casual\ncontacts in the population. We utilize the proposed model-calibrated on\nepidemiological and demographic data-to investigate current questions\nconcerning the implementation of non-pharmaceutical interventions (NPIs) during\nthe vaccination campaign. Specifically, we consider scenarios in which the most\nfragile population has already received the vaccine, and we focus our analysis\non the role of schools as drivers of the contagions and on the implementation\nof targeted intervention policies oriented to children and their families. We\nperform our analysis by means of a campaign of Monte Carlo simulations. Our\nfindings suggest that, in a phase with NPIs enacted but in-person education,\nchildren play a key role in the spreading of COVID-19. Interestingly, we show\nthat children's testing might be an important tool to flatten the epidemic\ncurve, in particular when combined with enacting temporary online education for\nclasses in which infected students are detected. Finally, we test a vaccination\nstrategy that prioritizes the members of large families and we demonstrate its\ngood performance. We believe that our modeling framework and our findings could\nbe of help for public health authorities for planning their current and future\ninterventions, as well as to increase preparedness for future epidemic\noutbreaks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:53:44 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bongiorno", "Christian", "", "MICS"], ["Zino", "Lorenzo", ""]]}, {"id": "2103.12553", "submitter": "Hao Xiong", "authors": "Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and Hao Xiong", "title": "Safe Multi-Agent Reinforcement Learning through Decentralized Multiple\n  Control Barrier Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Agent Reinforcement Learning (MARL) algorithms show amazing performance\nin simulation in recent years, but placing MARL in real-world applications may\nsuffer safety problems. MARL with centralized shields was proposed and verified\nin safety games recently. However, centralized shielding approaches can be\ninfeasible in several real-world multi-agent applications that involve\nnon-cooperative agents or communication delay. Thus, we propose to combine MARL\nwith decentralized Control Barrier Function (CBF) shields based on available\nlocal information. We establish a safe MARL framework with decentralized\nmultiple CBFs and develop Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG) to Multi-Agent Deep Deterministic Policy Gradient with decentralized\nmultiple Control Barrier Functions (MADDPG-CBF). Based on a collision-avoidance\nproblem that includes not only cooperative agents but obstacles, we demonstrate\nthe construction of multiple CBFs with safety guarantees in theory. Experiments\nare conducted and experiment results verify that the proposed safe MARL\nframework can guarantee the safety of agents included in MARL.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:54:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cai", "Zhiyuan", ""], ["Cao", "Huanhui", ""], ["Lu", "Wenjie", ""], ["Zhang", "Lin", ""], ["Xiong", "Hao", ""]]}, {"id": "2103.12585", "submitter": "Filippo Fabiani", "authors": "Filippo Fabiani", "title": "Pursuing robust decisions in uncertain traffic equilibrium problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the robustness of agents' traffic equilibria in randomized\nrouting games characterized by an uncertain network demand with a possibly\nunknown probability distribution. Specifically, we extend the so-called hose\nmodel by considering a traffic equilibrium model where the uncertain network\ndemand configuration belongs to a polyhedral set, whose shape is itself\na-priori unknown. By exploiting available data, we apply the scenario approach\ntheory to establish distribution-free feasibility guarantees for agents'\ntraffic equilibria of the uncertain routing game without the need to know an\nexplicit characterization of such set. A numerical example on a traffic network\ntestbed corroborates the proposed theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:38:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Fabiani", "Filippo", ""]]}, {"id": "2103.12710", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz,\n  Thomas Funkhouser", "title": "Spatial Intention Maps for Multi-Agent Mobile Manipulation", "comments": "To appear at IEEE International Conference on Robotics and Automation\n  (ICRA), 2021. Project page: https://spatial-intention-maps.cs.princeton.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to communicate intention enables decentralized multi-agent robots\nto collaborate while performing physical tasks. In this work, we present\nspatial intention maps, a new intention representation for multi-agent\nvision-based deep reinforcement learning that improves coordination between\ndecentralized mobile manipulators. In this representation, each agent's\nintention is provided to other agents, and rendered into an overhead 2D map\naligned with visual observations. This synergizes with the recently proposed\nspatial action maps framework, in which state and action representations are\nspatially aligned, providing inductive biases that encourage emergent\ncooperative behaviors requiring spatial coordination, such as passing objects\nto each other or avoiding collisions. Experiments across a variety of\nmulti-agent environments, including heterogeneous robot teams with different\nabilities (lifting, pushing, or throwing), show that incorporating spatial\nintention maps improves performance for different mobile manipulation tasks\nwhile significantly enhancing cooperative behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:31:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wu", "Jimmy", ""], ["Sun", "Xingyuan", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2103.12820", "submitter": "John Meluso", "authors": "John Meluso, Jesse Austin-Breneman, James P. Bagrow, Laurent\n  H\\'ebert-Dufresne", "title": "A Review & Framework for Modeling Complex Engineered System Development\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI nlin.AO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developing complex engineered systems (CES) poses significant challenges for\nengineers, managers, designers, and businesspeople alike due to the inherent\ncomplexity of the systems and contexts involved. Furthermore, experts have\nexpressed great interest in filling the gap in theory about how CES develop.\nThis article begins to address that gap in two ways. First, it reviews the\nnumerous definitions of CES along with existing theory and methods on CES\ndevelopment processes. Then, it proposes the ComplEx System Integrated\nUtilities Model (CESIUM), a novel framework for exploring how numerous system\nand development process characteristics may affect the performance of CES.\nCESIUM creates simulated representations of a system architecture, the\ncorresponding engineering organization, and the new product development process\nthrough which the organization designs the system. It does so by representing\nthe system as a network of interdependent artifacts designed by agents. Agents\niteratively design their artifacts through optimization and share information\nwith other agents, thereby advancing the CES toward a solution. This paper\ndescribes the model, conducts a sensitivity analysis, provides validation, and\nsuggests directions for future study.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:12:51 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 01:52:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Meluso", "John", ""], ["Austin-Breneman", "Jesse", ""], ["Bagrow", "James P.", ""], ["H\u00e9bert-Dufresne", "Laurent", ""]]}, {"id": "2103.12833", "submitter": "Vincent Leon", "authors": "Vincent Leon, S. Rasoul Etesami", "title": "Bandit Learning for Dynamic Colonel Blotto Game with a Budget Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamic Colonel Blotto game (CBG) in which one of the players\nis the learner and has limited troops (budget) to allocate over a finite time\nhorizon. At each stage, the learner strategically determines the budget and its\ndistribution to allocate among the battlefields based on past observations. The\nother player is the adversary, who chooses its budget allocation strategies\nrandomly from some fixed but unknown distribution. The learner's objective is\nto minimize the regret, which is defined as the difference between the optimal\npayoff in terms of the best dynamic policy and the realized payoff by following\na learning algorithm. The dynamic CBG is analyzed under the framework of\ncombinatorial bandit and bandit with knapsacks. We first convert the dynamic\nCBG with the budget constraint to a path planning problem on a graph. We then\ndevise an efficient dynamic policy for the learner that uses a combinatorial\nbandit algorithm Edge on the path planning graph as a subroutine for another\nalgorithm LagrangeBwK. A high-probability regret bound is derived, and it is\nshown that under the proposed policy, the learner's regret in the\nbudget-constrained dynamic CBG matches (up to a logarithmic factor) that of the\nrepeated CBG without budget constraints.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:52:56 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Leon", "Vincent", ""], ["Etesami", "S. Rasoul", ""]]}, {"id": "2103.12842", "submitter": "Justin Lane", "authors": "Justin E. Lane, Kevin McCaffree, F. LeRon Shults", "title": "Is radicalization reinforced by social media censorship?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Radicalized beliefs, such as those tied to QAnon, Russiagate, and other\npolitical conspiracy theories, can lead some individuals and groups to engage\nin violent behavior, as evidenced in recent months. Understanding the\nmechanisms by which such beliefs are accepted, spread, and intensified is\ncritical for any attempt to mitigate radicalization and avoid increased\npolitical polarization. This article presents and agent-based model of a social\nmedia network that enables investigation of the effects of censorship on the\namount of dissenting information to which agents become exposed and the\ncertainty of their radicalized views. The model explores two forms of\ncensorship: 1) decentralized censorship-in which individuals can choose to\nbreak an online social network tie (unfriend or unfollow) with another\nindividual who transmits conflicting beliefs and 2) centralized censorship-in\nwhich a single authority can ban an individual from the social media network\nfor spreading a certain type of belief. This model suggests that both forms of\ncensorship increase certainty in radicalized views by decreasing the amount of\ndissent to which an agent is exposed, but centralized \"banning\" of individuals\nhas the strongest effect on radicalization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:07:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lane", "Justin E.", ""], ["McCaffree", "Kevin", ""], ["Shults", "F. LeRon", ""]]}, {"id": "2103.12968", "submitter": "Jun Ma", "authors": "Xiaoxue Zhang, Jun Ma, Zilong Cheng, Sunan Huang, Tong Heng Lee", "title": "Receding Horizon Motion Planning for Multi-Agent Systems: A Velocity\n  Obstacle Based Probabilistic Method", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel and innovative methodology for feasible motion\nplanning in the multi-agent system is developed. On the basis of velocity\nobstacles characteristics, the chance constraints are formulated in the\nreceding horizon control (RHC) problem, and geometric information of collision\ncones is used to generate the feasible regions of velocities for the host\nagent. By this approach, the motion planning is conducted at the velocity level\ninstead of the position level. Thus, it guarantees a safer collision-free\ntrajectory for the multi-agent system, especially for the systems with\nhigh-speed moving agents. Moreover, a probability threshold of potential\ncollisions can be satisfied during the motion planning process. In order to\nvalidate the effectiveness of the methodology, different scenarios for multiple\nagents are investigated, and the simulation results clearly show that the\nproposed approach can effectively avoid potential collisions with a collision\nprobability less than a specific threshold.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:46:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhang", "Xiaoxue", ""], ["Ma", "Jun", ""], ["Cheng", "Zilong", ""], ["Huang", "Sunan", ""], ["Lee", "Tong Heng", ""]]}, {"id": "2103.13026", "submitter": "Xing Xu", "authors": "Xing Xu and Rongpeng Li and Zhifeng Zhao and Honggang Zhang", "title": "The Gradient Convergence Bound of Federated Multi-Agent Reinforcement\n  Learning with Efficient Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a distributed version of deep reinforcement learning\n(DRL) for multi-agent decision-making process in the paradigm of federated\nlearning. Since the deep neural network models in federated learning are\ntrained locally and aggregated iteratively through a central server, frequent\ninformation exchange incurs a large amount of communication overheads. Besides,\ndue to the heterogeneity of agents, Markov state transition trajectories from\ndifferent agents are usually unsynchronized within the same time interval,\nwhich will further influence the convergence bound of the aggregated deep\nneural network models. Therefore, it is of vital importance to reasonably\nevaluate the effectiveness of different optimization methods. Accordingly, this\npaper proposes a utility function to consider the balance between reducing\ncommunication overheads and improving convergence performance. Meanwhile, this\npaper develops two new optimization methods on top of variation-aware periodic\naveraging methods: 1) the decay-based method which gradually decreases the\nweight of the model's local gradients within the progress of local updating,\nand 2) the consensus-based method which introduces the consensus algorithm into\nfederated learning for the exchange of the model's local gradients. This paper\nalso provides novel convergence guarantees for both developed methods and\ndemonstrates their effectiveness and efficiency through theoretical analysis\nand numerical simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:21:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xu", "Xing", ""], ["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Zhang", "Honggang", ""]]}, {"id": "2103.13147", "submitter": "Ziyi Chen", "authors": "Ziyi Chen, Yi Zhou, Rongrong Chen", "title": "Multi-Agent Off-Policy TD Learning: Finite-Time Analysis with\n  Near-Optimal Sample Complexity and Communication Complexity", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite-time convergence of off-policy TD learning has been\ncomprehensively studied recently. However, such a type of convergence has not\nbeen well established for off-policy TD learning in the multi-agent setting,\nwhich covers broader applications and is fundamentally more challenging. This\nwork develops two decentralized TD with correction (TDC) algorithms for\nmulti-agent off-policy TD learning under Markovian sampling. In particular, our\nalgorithms preserve full privacy of the actions, policies and rewards of the\nagents, and adopt mini-batch sampling to reduce the sampling variance and\ncommunication frequency. Under Markovian sampling and linear function\napproximation, we proved that the finite-time sample complexity of both\nalgorithms for achieving an $\\epsilon$-accurate solution is in the order of\n$\\mathcal{O}(\\epsilon^{-1}\\ln \\epsilon^{-1})$, matching the near-optimal sample\ncomplexity of centralized TD(0) and TDC. Importantly, the communication\ncomplexity of our algorithms is in the order of $\\mathcal{O}(\\ln\n\\epsilon^{-1})$, which is significantly lower than the communication complexity\n$\\mathcal{O}(\\epsilon^{-1}\\ln \\epsilon^{-1})$ of the existing decentralized\nTD(0). Experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:48:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Ziyi", ""], ["Zhou", "Yi", ""], ["Chen", "Rongrong", ""]]}, {"id": "2103.13381", "submitter": "Mingming Shi", "authors": "Mingming Shi and Julien M. Hendrickx", "title": "Are energy savings the only reason for the emergence of bird echelon\n  formation?", "comments": "8 pages, 12 figures, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.GT cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the conditions under which the emergence of frequently observed\nechelon formation can be explained solely by the maximization of energy\nsavings. We consider a two-dimensional multi-agent echelon formation, where\neach agent receives a benefit that depends on its position relative to the\nothers, and adjusts its position to increase this benefit. We analyze the\nselfish case where each agent maximizes its own benefit, leading to a\nNash-equilibrium problem, and the collaborative case in which agents maximize\nthe global benefit of the group. We provide conditions on the benefit function\nunder which the frequently observed echelon formations cannot be Nash\nequilbriums or group optimums.\n  We then show that these conditions are satisfied by the conventionally used\nfixed-wing wake benefit model. This implies that energy saving alone is not\nsufficient to explain the emergence of the migratory formations observed, based\non the fixed-wing model. Hence, either non-aerodynamic aspects or a more\naccurate model of bird dynamics should be considered to construct such\nformations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:54:28 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Shi", "Mingming", ""], ["Hendrickx", "Julien M.", ""]]}, {"id": "2103.13446", "submitter": "Ryan Kortvelesy", "authors": "Ryan Kortvelesy and Amanda Prorok", "title": "ModGNN: Expert Policy Approximation in Multi-Agent Systems with a\n  Modular Graph Neural Network Architecture", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in the multi-agent domain has shown the promise of Graph Neural\nNetworks (GNNs) to learn complex coordination strategies. However, most current\napproaches use minor variants of a Graph Convolutional Network (GCN), which\napplies a convolution to the communication graph formed by the multi-agent\nsystem. In this paper, we investigate whether the performance and\ngeneralization of GCNs can be improved upon. We introduce ModGNN, a\ndecentralized framework which serves as a generalization of GCNs, providing\nmore flexibility. To test our hypothesis, we evaluate an implementation of\nModGNN against several baselines in the multi-agent flocking problem. We\nperform an ablation analysis to show that the most important component of our\nframework is one that does not exist in a GCN. By varying the number of agents,\nwe also demonstrate that an application-agnostic implementation of ModGNN\npossesses an improved ability to generalize to new environments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:48:12 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:38:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kortvelesy", "Ryan", ""], ["Prorok", "Amanda", ""]]}, {"id": "2103.13475", "submitter": "Brandon Collins", "authors": "Brandon C. Collins, Lisa Hines, Gia Barboza, and Philip N. Brown", "title": "Robust Stochastic Stability with Applications to Social Distancing in a\n  Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of learning in games has extensively studied situations where\nagents respond dynamically to each other in light of a fixed utility function.\nHowever, in many settings of interest, agent utility functions themselves vary\nas a result of past agent choices. The ongoing COVID-19 pandemic has\nhighlighted the need to formulate and analyze such models which feature\ngame-environment feedback. For instance, a highly prevalent virus may\nincentivize individuals to wear masks, but extensive adoption of mask-wearing\nreduces virus prevalence which in turn reduces individual incentives for\nmask-wearing. What is the interplay between epidemic severity and the behaviors\nof a victim population? For initial answers, we develop a general framework\nusing probabilistic coupling methods that can be used to derive the\nstochastically stable states of log-linear learning in certain games which\nfeature such game-environment feedback. We then apply this framework to a\nsimple dynamic game-theoretic model of social precautions in an epidemic and\ngive conditions under which maximally-cautious social behavior in this model is\nstochastically stable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 20:36:36 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Collins", "Brandon C.", ""], ["Hines", "Lisa", ""], ["Barboza", "Gia", ""], ["Brown", "Philip N.", ""]]}, {"id": "2103.13748", "submitter": "Yiwei Liao", "authors": "Yiwei Liao, Zhuorui Li, Kun Huang, and Shi Pu", "title": "Compressed Gradient Tracking Methods for Decentralized Optimization with\n  Linear Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication compression techniques are of growing interests for solving the\ndecentralized optimization problem under limited communication, where the\nglobal objective is to minimize the average of local cost functions over a\nmulti-agent network using only local computation and peer-to-peer\ncommunication. In this paper, we first propose a novel compressed gradient\ntracking algorithm (C-GT) that combines gradient tracking technique with\ncommunication compression. In particular, C-GT is compatible with a general\nclass of compression operators that unifies both unbiased and biased\ncompressors. We show that C-GT inherits the advantages of gradient\ntracking-based algorithms and achieves linear convergence rate for strongly\nconvex and smooth objective functions. In the second part of this paper, we\npropose an error feedback based compressed gradient tracking algorithm\n(EF-C-GT) to further improve the algorithm efficiency for biased compression\noperators. Numerical examples complement the theoretical findings and\ndemonstrate the efficiency and flexibility of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:00:49 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 11:30:45 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 06:27:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Liao", "Yiwei", ""], ["Li", "Zhuorui", ""], ["Huang", "Kun", ""], ["Pu", "Shi", ""]]}, {"id": "2103.14023", "submitter": "Ye Yuan", "authors": "Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani", "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent\n  Forecasting", "comments": "Project page: https://www.ye-yuan.com/agentformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately; e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method significantly improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yuan", "Ye", ""], ["Weng", "Xinshuo", ""], ["Ou", "Yanglan", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.14078", "submitter": "Cyrille Berger", "authors": "Cyrille Berger and Patrick Doherty and Piotr Rudol and Mariusz Wzorek", "title": "Hastily Formed Knowledge Networks and Distributed Situation Awareness\n  for Collaborative Robotics", "comments": "68 pages 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the context of collaborative robotics, distributed situation awareness is\nessential for supporting collective intelligence in teams of robots and human\nagents where it can be used for both individual and collective decision\nsupport. This is particularly important in applications pertaining to emergency\nrescue and crisis management. During operational missions, data and knowledge\nis gathered incrementally and in different ways by heterogeneous robots and\nhumans. We describe this as the creation of \\emph{Hastily Formed Knowledge\nNetworks} (HFKNs). The focus of this paper is the specification and prototyping\nof a general distributed system architecture that supports the creation of\nHFKNs by teams of robots and humans. The information collected ranges from\nlow-level sensor data to high-level semantic knowledge, the latter represented\nin part as RDF Graphs. The framework includes a synchronization protocol and\nassociated algorithms that allow for the automatic distribution and sharing of\ndata and knowledge between agents. This is done through the distributed\nsynchronization of RDF Graphs shared between agents. High-level semantic\nqueries specified in SPARQL can be used by robots and humans alike to acquire\nboth knowledge and data content from team members. The system is empirically\nvalidated and complexity results of the proposed algorithms are provided.\nAdditionally, a field robotics case study is described, where a 3D mapping\nmission has been executed using several UAVs in a collaborative emergency\nrescue scenario while using the full HFKN Framework.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:53:18 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Berger", "Cyrille", ""], ["Doherty", "Patrick", ""], ["Rudol", "Piotr", ""], ["Wzorek", "Mariusz", ""]]}, {"id": "2103.14123", "submitter": "Juan Heredia-Juesas", "authors": "Jose Martinez-Lorenzo, Jeff Hudack, Yutao Jing, Michael Shaham, Zixuan\n  Liang, Abdullah Al Bashit, Yushu Wu, Weite Zhang, Matthew Skopin, Juan\n  Heredia-Juesas, Yuntao Ma, Tristan Sweeney, Nicolas Ares, Ari Fox", "title": "Preliminary Experimental Results of Context-Aware Teams of Multiple\n  Autonomous Agents Operating under Constrained Communications", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents and experimentally test the framework used by our\ncontext-aware, distributed team of small Unmanned Aerial Systems (SUAS) capable\nof operating in real-time, in an autonomous fashion, and under constrained\ncommunications. Our framework relies on three layered approach: (1) Operational\nlayer, where fast temporal and narrow spatial decisions are made; (2) Tactical\nLayer, where temporal and spatial decisions are made for a team of agents; and\n(3) Strategical Layer, where slow temporal and wide spatial decisions are made\nfor the team of agents. These three layers are coordinated by an ad-hoc,\nsoftware-defined communications network, which ensures sparse, but timely\ndelivery of messages amongst groups and teams of agents at each layer even\nunder constrained communications. Experimental results are presented for a team\nof 10 small unmanned aerial systems tasked with searching and monitoring a\nperson in an open area. At the operational layer, our use case presents an\nagent autonomously performing searching, detection, localization,\nclassification, identification, tracking, and following of the person, while\navoiding malicious collisions. At the tactical layer, our experimental use case\npresents the cooperative interaction of a group of multiple agents that enable\nthe monitoring of the targeted person over a wider spatial and temporal\nregions. At the strategic layer, our use case involves the detection of complex\nbehaviors-i.e. the person being followed enters a car and runs away, or the\nperson being followed exits the car and runs away-that requires strategic\nresponses to successfully accomplish the mission.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:29:58 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Martinez-Lorenzo", "Jose", ""], ["Hudack", "Jeff", ""], ["Jing", "Yutao", ""], ["Shaham", "Michael", ""], ["Liang", "Zixuan", ""], ["Bashit", "Abdullah Al", ""], ["Wu", "Yushu", ""], ["Zhang", "Weite", ""], ["Skopin", "Matthew", ""], ["Heredia-Juesas", "Juan", ""], ["Ma", "Yuntao", ""], ["Sweeney", "Tristan", ""], ["Ares", "Nicolas", ""], ["Fox", "Ari", ""]]}, {"id": "2103.14396", "submitter": "Sebastien Colla", "authors": "Sebastien Colla, Julien M. Hendrickx", "title": "Automated Worst-Case Performance Analysis of Decentralized Gradient\n  Descent", "comments": "7 pages, 4 figures, submitted to Conference on Decision and Control\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a methodology to automatically compute worst-case performance\nbounds for a class of decentralized optimization algorithms that optimize the\naverage of local functions distributed across a network. We extend to\ndecentralized optimization the recently proposed PEP approach, which allows\ncomputing the exact worst-case performance and worst-case instance of\ncentralized algorithms by solving an SDP. We obtain an exact formulation when\nthe network matrix is given and a relaxation for classes of network matrices\ncharacterized by their spectral range. We apply our methodology to the\ndistributed (sub)gradient method, obtain a nearly tight worst-case performance\nbound that significantly improves over the literature, and gain insights into\nthe worst communication networks for a given spectral range.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:57:07 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:23:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Colla", "Sebastien", ""], ["Hendrickx", "Julien M.", ""]]}, {"id": "2103.14709", "submitter": "Souma Chowdhury", "authors": "Leighton Collins, Payam Ghassemi, Ehsan T. Esfahani, David Doermann,\n  Karthik Dantu, Souma Chowdhury", "title": "Scalable Coverage Path Planning of Multi-Robot Teams for Monitoring\n  Non-Convex Areas", "comments": "Accepted for publication in the proceedings of the 2021 IEEE\n  International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel multi-robot coverage path planning (CPP)\nalgorithm - aka SCoPP - that provides a time-efficient solution, with workload\nbalanced plans for each robot in a multi-robot system, based on their initial\nstates. This algorithm accounts for discontinuities (e.g., no-fly zones) in a\nspecified area of interest, and provides an optimized ordered list of\nway-points per robot using a discrete, computationally efficient, nearest\nneighbor path planning algorithm. This algorithm involves five main stages,\nwhich include the transformation of the user's input as a set of vertices in\ngeographical coordinates, discretization, load-balanced partitioning,\nauctioning of conflict cells in a discretized space, and a path planning\nprocedure. To evaluate the effectiveness of the primary algorithm, a\nmulti-unmanned aerial vehicle (UAV) post-flood assessment application is\nconsidered, and the performance of the algorithm is tested on three test maps\nof varying sizes. Additionally, our method is compared with a state-of-the-art\nmethod created by Guasella et al. Further analyses on scalability and\ncomputational time of SCoPP are conducted. The results show that SCoPP is\nsuperior in terms of mission completion time; its computing time is found to be\nunder 2 mins for a large map covered by a 150-robot team, thereby demonstrating\nits computationally scalability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:45:45 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Collins", "Leighton", ""], ["Ghassemi", "Payam", ""], ["Esfahani", "Ehsan T.", ""], ["Doermann", "David", ""], ["Dantu", "Karthik", ""], ["Chowdhury", "Souma", ""]]}, {"id": "2103.14764", "submitter": "Anastasia Bizyaeva", "authors": "Anastasia Bizyaeva, Timothy Sorochkin, Alessio Franci, Naomi Ehrich\n  Leonard", "title": "Control of Agreement and Disagreement Cascades with Distributed Inputs", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a group of autonomous communicating agents, the ability to distinguish a\nmeaningful input from disturbance, and come to collective agreement or\ndisagreement in response to that input, is paramount for carrying out\ncoordinated objectives. In this work we study how a cascade of opinion\nformation spreads through a group of networked decision-makers in response to a\ndistributed input signal. Using a nonlinear opinion dynamics model with dynamic\nfeedback modulation of an attention parameter, we show how the triggering of an\nopinion cascade and the collective decision itself depend on both the\ndistributed input and the node agreement and disagreement centrality,\ndetermined by the spectral properties of the network graph. We further show how\nthe attention dynamics introduce an implicit threshold that distinguishes\nbetween distributed inputs that trigger cascades and ones that are rejected as\ndisturbance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 23:19:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bizyaeva", "Anastasia", ""], ["Sorochkin", "Timothy", ""], ["Franci", "Alessio", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "2103.14805", "submitter": "Stewart Jamieson", "authors": "Stewart Jamieson, Kaveh Fathian, Kasra Khosoussi, Jonathan P. How,\n  Yogesh Girdhar", "title": "Multi-Robot Distributed Semantic Mapping in Unfamiliar Environments\n  through Online Matching of Learned Representations", "comments": "7 pages, 6 figures, 1 table; accepted for presentation in IEEE Int.\n  Conf. on Robotics and Automation, ICRA '21, Xi'an, China, June 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solution to multi-robot distributed semantic mapping of novel\nand unfamiliar environments. Most state-of-the-art semantic mapping systems are\nbased on supervised learning algorithms that cannot classify novel observations\nonline. While unsupervised learning algorithms can invent labels for novel\nobservations, approaches to detect when multiple robots have independently\ndeveloped their own labels for the same new class are prone to erroneous or\ninconsistent matches. These issues worsen as the number of robots in the system\nincreases and prevent fusing the local maps produced by each robot into a\nconsistent global map, which is crucial for cooperative planning and joint\nmission summarization. Our proposed solution overcomes these obstacles by\nhaving each robot learn an unsupervised semantic scene model online and use a\nmultiway matching algorithm to identify consistent sets of matches between\nlearned semantic labels belonging to different robots. Compared to the state of\nthe art, the proposed solution produces 20-60% higher quality global maps that\ndo not degrade even as many more local maps are fused.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 04:22:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jamieson", "Stewart", ""], ["Fathian", "Kaveh", ""], ["Khosoussi", "Kasra", ""], ["How", "Jonathan P.", ""], ["Girdhar", "Yogesh", ""]]}, {"id": "2103.14847", "submitter": "Aviram Imber", "authors": "Aviram Imber, Jonas Israel, Markus Brill, Benny Kimelfeld", "title": "Committee Voting with Incomplete Approvals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate approval-based committee voting with incomplete information\nabout voters' approval preferences. We consider several models of\nincompleteness where each voter partitions the set of candidates into approved,\ndisapproved, and unknown candidates, possibly with ordinal preference\nconstraints among candidates in the latter category. For a number of classic\napproval-based committee voting rules including Chamberlin--Courant and\nProportional Approval Voting, we study the complexity of some fundamental\ncomputational problems such as determining whether a given committee is a\npossible or necessary winning committee and whether it possibly or necessarily\nsatisfies representation axioms.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:11:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Imber", "Aviram", ""], ["Israel", "Jonas", ""], ["Brill", "Markus", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2103.14891", "submitter": "Zijian Gao", "authors": "Zijian Gao, Kele Xu, Bo Ding, Huaimin Wang, Yiying Li, Hongda Jia", "title": "KnowRU: Knowledge Reusing via Knowledge Distillation in Multi-agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep Reinforcement Learning (RL) algorithms have achieved\ndramatically progress in the multi-agent area. However, training the\nincreasingly complex tasks would be time-consuming and resources-exhausting. To\nalleviate this problem, efficient leveraging the historical experience is\nessential, which is under-explored in previous studies as most of the exiting\nmethods may fail to achieve this goal in a continuously variational system due\nto their complicated design and environmental dynamics. In this paper, we\npropose a method, named \"KnowRU\" for knowledge reusing which can be easily\ndeployed in the majority of the multi-agent reinforcement learning algorithms\nwithout complicated hand-coded design. We employ the knowledge distillation\nparadigm to transfer the knowledge among agents with the goal to accelerate the\ntraining phase for new tasks, while improving the asymptotic performance of\nagents. To empirically demonstrate the robustness and effectiveness of KnowRU,\nwe perform extensive experiments on state-of-the-art multi-agent reinforcement\nlearning (MARL) algorithms on collaborative and competitive scenarios. The\nresults show that KnowRU can outperform the recently reported methods, which\nemphasizes the importance of the proposed knowledge reusing for MARL.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:38:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Zijian", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Li", "Yiying", ""], ["Jia", "Hongda", ""]]}, {"id": "2103.14979", "submitter": "Konstantinos Ntemos", "authors": "Konstantinos Ntemos, George Pikramenos, and Nicholas Kalouptsidis", "title": "Dynamic Information Sharing and Punishment Strategies", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of information sharing among rational\nself-interested agents as a dynamic game of asymmetric information. We assume\nthat the agents imperfectly observe a Markov chain and they are called to\ndecide whether they will share their noisy observations or not at each time\ninstant. We utilize the notion of conditional mutual information to evaluate\nthe information being shared among the agents. The challenges that arise due to\nthe inter-dependence of agents' information structure and decision-making are\nexhibited. For the finite horizon game we prove that agents do not have\nincentive to share information. In contrast, we show that cooperation can be\nsustained in the infinite horizon case by devising appropriate punishment\nstrategies which are defined over the agents' beliefs on the system state. We\nshow that these strategies are closed under the best-response mapping and that\ncooperation can be the optimal choice in some subsets of the state belief\nsimplex. We characterize these equilibrium regions, prove uniqueness of a\nmaximal equilibrium region and devise an algorithm for its approximate\ncomputation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:09:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ntemos", "Konstantinos", ""], ["Pikramenos", "George", ""], ["Kalouptsidis", "Nicholas", ""]]}, {"id": "2103.15090", "submitter": "Antonios Liapis", "authors": "Konstantinos Sfikas and Antonios Liapis", "title": "Playing Against the Board: Rolling Horizon Evolutionary Algorithms\n  Against Pandemic", "comments": "Accepted to IEEE Transactions on Games, 11 pages, 7 figures. arXiv\n  admin note: text overlap with arXiv:2103.11388", "journal-ref": null, "doi": "10.1109/TG.2021.3069766", "report-no": null, "categories": "cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive board games have provided a rich and diverse testbed for\nartificial intelligence. This paper contends that collaborative board games\npose a different challenge to artificial intelligence as it must balance\nshort-term risk mitigation with long-term winning strategies. Collaborative\nboard games task all players to coordinate their different powers or pool their\nresources to overcome an escalating challenge posed by the board and a\nstochastic ruleset. This paper focuses on the exemplary collaborative board\ngame Pandemic and presents a rolling horizon evolutionary algorithm designed\nspecifically for this game. The complex way in which the Pandemic game state\nchanges in a stochastic but predictable way required a number of specially\ndesigned forward models, macro-action representations for decision-making, and\nrepair functions for the genetic operations of the evolutionary algorithm.\nVariants of the algorithm which explore optimistic versus pessimistic game\nstate evaluations, different mutation rates and event horizons are compared\nagainst a baseline hierarchical policy agent. Results show that an evolutionary\napproach via short-horizon rollouts can better account for the future dangers\nthat the board may introduce, and guard against them. Results highlight the\ntypes of challenges that collaborative board games pose to artificial\nintelligence, especially for handling multi-player collaboration interactions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:22:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sfikas", "Konstantinos", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.15230", "submitter": "Xiwei Liu", "authors": "Xiwei Liu", "title": "Synchronization and Control for Multi-Weighted and Directed Complex\n  Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of complex networks with multi-weights has been a hot topic\nrecently. For a network with a single weight, previous studies have shown that\nthey can promote synchronization. But for complex networks with multi-weights,\nthere are no rigorous analysis to show that synchronization can be reached\nfaster. In this paper, the complex network is allowed to be directed, which\nwill make the synchronization analysis difficult for multiple couplings. In\nvirtue of the normalized left eigenvectors (NLEVec) corresponding to the zero\neigenvalue of coupling matrices, we prove that if the Chebyshev distance\nbetween NLEVec is less than some value, which is defined as the allowable\ndeviation bound, then the synchronization and control will be realized with\nsufficiently large coupling strengths, i.e., all coupling matrices do\naccelerate synchronization. Moreover, adaptive rules are also designed for the\ncoupling strength.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:50:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Xiwei", ""]]}, {"id": "2103.15559", "submitter": "Antonio Bucchiarone Dr.", "authors": "Antonio Bucchiarone, Antonio Cicchetti, Nelly Bencomo, Enrica Loria,\n  Annapaola Marconi", "title": "Gamified and Self-Adaptive Applications for the Common Good: Research\n  Challenges Ahead", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivational digital systems offer capabilities to engage and motivate\nend-users to foster behavioral changes towards a common goal. In general these\nsystems use gamification principles in non-games contexts. Over the years,\ngamification has gained consensus among researchers and practitioners as a tool\nto motivate people to perform activities with the ultimate goal of promoting\nbehavioural change, or engaging the users to perform activities that can offer\nrelevant benefits but which can be seen as unrewarding and even tedious.\n  There exists a plethora of heterogeneous application scenarios towards\nreaching the common good that can benefit from gamification. However, an open\nproblem is how to effectively combine multiple motivational campaigns to\nmaximise the degree of participation without exposing the system to\ncounterproductive behaviours.\n  We conceive motivational digital systems as multi-agent systems:\nself-adaptation is a feature of the overall system, while individual agents may\nself-adapt in order to leverage other agents' resources, functionalities and\ncapabilities to perform tasks more efficiently and effectively. Consequently,\nmultiple campaigns can be run and adapted to reach common good. At the same\ntime, agents are grouped into micro-communities in which agents contribute with\ntheir own social capital and leverage others' capabilities to balance their\nweaknesses.\n  In this paper we propose our vision on how the principles at the base of the\nautonomous and multi-agent systems can be exploited to design multi-challenge\nmotivational systems to engage smart communities towards common goals. We\npresent an initial version of a general framework based on the MAPE-K loop and\na set of research challenges that characterise our research roadmap for the\nimplementation of our vision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:56:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bucchiarone", "Antonio", ""], ["Cicchetti", "Antonio", ""], ["Bencomo", "Nelly", ""], ["Loria", "Enrica", ""], ["Marconi", "Annapaola", ""]]}, {"id": "2103.15561", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Ashkan Soleymani, Amin Abyaneh, Samir Bhatt, Bernhard\n  Sch\\\"olkopf, Stefan Bauer", "title": "Pyfectious: An individual-level simulator to discover optimal\n  containment polices for epidemic diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.AI cs.LG cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulating the spread of infectious diseases in human communities is critical\nfor predicting the trajectory of an epidemic and verifying various policies to\ncontrol the devastating impacts of the outbreak. Many existing simulators are\nbased on compartment models that divide people into a few subsets and simulate\nthe dynamics among those subsets using hypothesized differential equations.\nHowever, these models lack the requisite granularity to study the effect of\nintelligent policies that influence every individual in a particular way. In\nthis work, we introduce a simulator software capable of modeling a population\nstructure and controlling the disease's propagation at an individualistic\nlevel. In order to estimate the confidence of the conclusions drawn from the\nsimulator, we employ a comprehensive probabilistic approach where the entire\npopulation is constructed as a hierarchical random variable. This approach\nmakes the inferred conclusions more robust against sampling artifacts and gives\nconfidence bounds for decisions based on the simulation results. To showcase\npotential applications, the simulator parameters are set based on the formal\nstatistics of the COVID-19 pandemic, and the outcome of a wide range of control\nmeasures is investigated. Furthermore, the simulator is used as the environment\nof a reinforcement learning problem to find the optimal policies to control the\npandemic. The obtained experimental results indicate the simulator's\nadaptability and capacity in making sound predictions and a successful policy\nderivation example based on real-world data. As an exemplary application, our\nresults show that the proposed policy discovery method can lead to control\nmeasures that produce significantly fewer infected individuals in the\npopulation and protect the health system against saturation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:54:46 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 00:13:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mehrjou", "Arash", ""], ["Soleymani", "Ashkan", ""], ["Abyaneh", "Amin", ""], ["Bhatt", "Samir", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bauer", "Stefan", ""]]}, {"id": "2103.15664", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski and Ali H. Sayed", "title": "Competing Adaptive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive networks have the capability to pursue solutions of global\nstochastic optimization problems by relying only on local interactions within\nneighborhoods. The diffusion of information through repeated interactions\nallows for globally optimal behavior, without the need for central\ncoordination. Most existing strategies are developed for cooperative learning\nsettings, where the objective of the network is common to all agents. We\nconsider in this work a team setting, where a subset of the agents form a team\nwith a common goal while competing with the remainder of the network. We\ndevelop an algorithm for decentralized competition among teams of adaptive\nagents, analyze its dynamics and present an application in the decentralized\ntraining of generative adversarial neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:42:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2103.15894", "submitter": "Dinuka Sahabandu", "authors": "Dinuka Sahabandu, Luyao Niu, Andrew Clark, Radha Poovendran", "title": "Scalable Planning in Multi-Agent MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-agent Markov Decision Processes (MMDPs) arise in a variety of\napplications including target tracking, control of multi-robot swarms, and\nmultiplayer games. A key challenge in MMDPs occurs when the state and action\nspaces grow exponentially in the number of agents, making computation of an\noptimal policy computationally intractable for medium- to large-scale problems.\nOne property that has been exploited to mitigate this complexity is transition\nindependence, in which each agent's transition probabilities are independent of\nthe states and actions of other agents. Transition independence enables\nfactorization of the MMDP and computation of local agent policies but does not\nhold for arbitrary MMDPs. In this paper, we propose an approximate transition\ndependence property, called $\\delta$-transition dependence and develop a metric\nfor quantifying how far an MMDP deviates from transition independence. Our\ndefinition of $\\delta$-transition dependence recovers transition independence\nas a special case when $\\delta$ is zero. We develop a polynomial time algorithm\nin the number of agents that achieves a provable bound on the global optimum\nwhen the reward functions are monotone increasing and submodular in the agent\nactions. We evaluate our approach on two case studies, namely, multi-robot\ncontrol and multi-agent patrolling example.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:04:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sahabandu", "Dinuka", ""], ["Niu", "Luyao", ""], ["Clark", "Andrew", ""], ["Poovendran", "Radha", ""]]}, {"id": "2103.15901", "submitter": "Amir Leshem", "authors": "Tomer Boyarski and Amir Leshem and Vikram Krishnamurthy", "title": "Distributed learning in congested environments with partial information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can non-communicating agents learn to share congested resources\nefficiently? This is a challenging task when the agents can access the same\nresource simultaneously (in contrast to multi-agent multi-armed bandit\nproblems) and the resource valuations differ among agents. We present a fully\ndistributed algorithm for learning to share in congested environments and prove\nthat the agents' regret with respect to the optimal allocation is\npoly-logarithmic in the time horizon. Performance in the non-asymptotic regime\nis illustrated in numerical simulations. The distributed algorithm has\napplications in cloud computing and spectrum sharing. Keywords: Distributed\nlearning, congestion games, poly-logarithmic regret.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:22:32 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 20:49:28 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Boyarski", "Tomer", ""], ["Leshem", "Amir", ""], ["Krishnamurthy", "Vikram", ""]]}, {"id": "2103.16033", "submitter": "Salma Elmalaki", "authors": "Salma Elmalaki (University of California, Irvine)", "title": "FaiR-IoT: Fairness-aware Human-in-the-Loop Reinforcement Learning for\n  Harnessing Human Variability in Personalized IoT", "comments": "14 pages", "journal-ref": null, "doi": "10.1145/3450268.3453525", "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rapid growth in wearable technologies, monitoring complex human\ncontext becomes feasible, paving the way to develop human-in-the-loop IoT\nsystems that naturally evolve to adapt to the human and environment state\nautonomously. Nevertheless, a central challenge in designing such personalized\nIoT applications arises from human variability. Such variability stems from the\nfact that different humans exhibit different behaviors when interacting with\nIoT applications (intra-human variability), the same human may change the\nbehavior over time when interacting with the same IoT application (inter-human\nvariability), and human behavior may be affected by the behaviors of other\npeople in the same environment (multi-human variability). To that end, we\npropose FaiR-IoT, a general reinforcement learning-based framework for adaptive\nand fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three\nlevels of reinforcement learning agents interact to continuously learn human\npreferences and maximize the system's performance and fairness while taking\ninto account the intra-, inter-, and multi-human variability. We validate the\nproposed framework on two applications, namely (i) Human-in-the-Loop Automotive\nAdvanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House.\nResults obtained on these two applications validate the generality of FaiR-IoT\nand its ability to provide a personalized experience while enhancing the\nsystem's performance by 40%-60% compared to non-personalized systems and\nenhancing the fairness of the multi-human systems by 1.5 orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:30:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Elmalaki", "Salma", "", "University of California, Irvine"]]}, {"id": "2103.16376", "submitter": "Blake Buchanan", "authors": "Blake Buchanan, Matthew Travers, Howie Choset, Scott Kelly", "title": "Stability and Control of Chaplygin Beanies Coupled to a Platform through\n  Nonholonomic Constraints", "comments": null, "journal-ref": null, "doi": "10.1115/DSCC2020-3315", "report-no": "DSCC2020-3315, V002T30A006", "categories": "nlin.CD cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many multi-agent systems in nature are comprised of agents that interact\nwith, and respond to, the dynamics of their environment. In this paper, we\napproach the study of such agent-environment interactions through the study of\npassively compliant vehicles coupled to their environment via simple\nnonholonomic constraints. We first consider a single passively compliant\nChaplygin beanie atop a platform having translational compliance, introduce the\nreduced equations for the system using the notion of nonholonomic momentum, and\nprovide proof for its stability under arbitrary deformations of the elastic\nelement modeling its compliance. We then direct our focus to results concerning\nthe frequency response and control of passive Chaplygin beanies under actuation\nof the platform, discuss rich dynamical features arising from periodic\nactuation, and develop rules by which control can be exerted to collect and\ndisperse multiple passive vehicles. We then discuss how the latter of these\nresults clarifies the extent to which stable behavior can be excited in the\nsystem through exogenous control.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:17:37 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 17:22:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Buchanan", "Blake", ""], ["Travers", "Matthew", ""], ["Choset", "Howie", ""], ["Kelly", "Scott", ""]]}, {"id": "2103.16496", "submitter": "Timothee Brochier", "authors": "Timoth\\'ee Brochier (IRD, SU, ESP Dakar, UMMISCO), Alassane Bah (ESP\n  Dakar, UMMISCO)", "title": "FisherMob : a bioeconomic model of fishers' migrations", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sea fishing is a highly mobile activity, favoured by the vastness of the\noceans, the absence of physical boundaries and the abstraction of legislative\nboundaries. Understanding and anticipating this mobility is a major challenge\nfor fisheries management issues, both at the national and international levels.\n''FisherMob'' is a free Gama tool designed to study the effect of economic and\nbiological factors on the dynamics of connected fisheries. It incorporate the\nmost important processes involved in fisheries dynamics: fish abundance\nvariability, price of the fishing effort and ex-vessel fish market price that\nwhich depends on the ratio between offer and demand. The tool uses as input a\nscheme of a coastal area with delimited fishing sites, fish biological\nparameters and fisheries parameters. It runs with a userfriendly graphic\ninterface and generates output files that can be post-processed easily using\ngraphic and statistical software.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:34:16 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Brochier", "Timoth\u00e9e", "", "IRD, SU, ESP Dakar, UMMISCO"], ["Bah", "Alassane", "", "ESP\n  Dakar, UMMISCO"]]}, {"id": "2103.16688", "submitter": "Keith Paarporn", "authors": "Keith Paarporn, Rahul Chandan, Mahnoosh Alizadeh, Jason R. Marden", "title": "The Division of Assets in Multiagent Systems: A Case Study in Team\n  Blotto Games", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.GT cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-agent systems are designed to concurrently accomplish a diverse set of\ntasks at unprecedented scale. Here, the central problems faced by a system\noperator are to decide (i) how to divide available resources amongst the agents\nassigned to tasks and (ii) how to coordinate the behavior of the agents to\noptimize the efficiency of the resulting collective behavior. The focus of this\npaper is on problem (i), where we seek to characterize the impact of the\ndivision of resources on the best-case efficiency of the resulting collective\nbehavior. Specifically, we focus on a team Colonel Blotto game where there are\ntwo sub-colonels competing against a common adversary in a two battlefield\nenvironment. Here, each sub-colonel is assigned a given resource budget and is\nrequired to allocate these resources independent of the other sub-colonel.\nHowever, their success is dependent on the allocation strategy of both\nsub-colonels. The central focus of this manuscript is on how to divide a common\npool of resources among the two sub-colonels to optimize the resulting\nbest-case efficiency guarantees. Intuitively, one would imagine that the more\nbalanced the division of resources, the worse the performance, as such\ndivisions restrict the sub-colonels' ability to employ joint randomized\nstrategies that tend to be necessary for optimizing performance guarantees.\nHowever, the main result of this paper demonstrates that this intuition is\nactually incorrect. A more balanced division of resources can offer better\nperformance guarantees than a more centralized division. Hence, this paper\ndemonstrates that the resource division problem is highly non-trivial in such\nenmeshed environments and worthy of significant future research efforts.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:20:18 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Paarporn", "Keith", ""], ["Chandan", "Rahul", ""], ["Alizadeh", "Mahnoosh", ""], ["Marden", "Jason R.", ""]]}, {"id": "2103.16813", "submitter": "Xiaojie Chen", "authors": "Fang Yan, Xiaojie Chen, Zhipeng Qiu, and Attila Szolnoki", "title": "Cooperator driven oscillation in a time-delayed feedback-evolving game", "comments": null, "journal-ref": "New Journal of Physics 23 (2021) 053017", "doi": "10.1088/1367-2630/abf205", "report-no": null, "categories": "physics.soc-ph cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering feedback of collective actions of cooperation on common resources\nhas vital importance to reach sustainability. But such efforts may have not\nimmediate consequence on the state of environment and it is unclear how they\ninfluence the strategic and environmental dynamics with feedbacks. To address\nthis issue, we construct a feedback-evolving game model in which we consider\nthe growth capacity of resources and the punishment efficiency on defectors who\ndo not provide returns to the environment. Importantly, we further assume a\ndelay in adopting the contribution of cooperative individuals to environmental\nchange in our model. We find that when this contribution amount from\ncooperators' endowment is fixed, the time delay has no particular consequence\non the coevolutionary dynamics. However, when the return is proportional to\ntheir endowment, then the time delay can induce periodic oscillatory dynamics\nof cooperation level and environment. Our work reveals the potential effects of\ntime delay of cooperative actions on the coevolutionary dynamics in strategic\ninteractions with environmental feedback.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:11:20 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yan", "Fang", ""], ["Chen", "Xiaojie", ""], ["Qiu", "Zhipeng", ""], ["Szolnoki", "Attila", ""]]}, {"id": "2103.16977", "submitter": "Ed Hill", "authors": "Edward Hill, Marco Bardoscia and Arthur Turrell", "title": "Solving Heterogeneous General Equilibrium Economic Models with Deep\n  Reinforcement Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG cs.MA q-fin.EC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General equilibrium macroeconomic models are a core tool used by policymakers\nto understand a nation's economy. They represent the economy as a collection of\nforward-looking actors whose behaviours combine, possibly with stochastic\neffects, to determine global variables (such as prices) in a dynamic\nequilibrium. However, standard semi-analytical techniques for solving these\nmodels make it difficult to include the important effects of heterogeneous\neconomic actors. The COVID-19 pandemic has further highlighted the importance\nof heterogeneity, for example in age and sector of employment, in macroeconomic\noutcomes and the need for models that can more easily incorporate it. We use\ntechniques from reinforcement learning to solve such models incorporating\nheterogeneous agents in a way that is simple, extensible, and computationally\nefficient. We demonstrate the method's accuracy and stability on a toy problem\nfor which there is a known analytical solution, its versatility by solving a\ngeneral equilibrium problem that includes global stochasticity, and its\nflexibility by solving a combined macroeconomic and epidemiological model to\nexplore the economic and health implications of a pandemic. The latter\nsuccessfully captures plausible economic behaviours induced by differential\nhealth risks by age.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 10:55:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hill", "Edward", ""], ["Bardoscia", "Marco", ""], ["Turrell", "Arthur", ""]]}, {"id": "2103.16985", "submitter": "Mohamed Sana", "authors": "Mohamed Sana, Mattia Merluzzi, Nicola di Pietro, Emilio Calvanese\n  Strinati", "title": "Energy Efficient Edge Computing: When Lyapunov Meets Distributed\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of energy-efficient computation offloading\nenabled by edge computing. In the considered scenario, multiple users\nsimultaneously compete for limited radio and edge computing resources to get\noffloaded tasks processed under a delay constraint, with the possibility of\nexploiting low power sleep modes at all network nodes. The radio resource\nallocation takes into account inter- and intra-cell interference, and the duty\ncycles of the radio and computing equipment have to be jointly optimized to\nminimize the overall energy consumption. To address this issue, we formulate\nthe underlying problem as a dynamic long-term optimization. Then, based on\nLyapunov stochastic optimization tools, we decouple the formulated problem into\na CPU scheduling problem and a radio resource allocation problem to be solved\nin a per-slot basis. Whereas the first one can be optimally and efficiently\nsolved using a fast iterative algorithm, the second one is solved using\ndistributed multi-agent reinforcement learning due to its non-convexity and\nNP-hardness. The resulting framework achieves up to 96.5% performance of the\noptimal strategy based on exhaustive search, while drastically reducing\ncomplexity. The proposed solution also allows to increase the network's energy\nefficiency compared to a benchmark heuristic approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:02:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sana", "Mohamed", ""], ["Merluzzi", "Mattia", ""], ["di Pietro", "Nicola", ""], ["Strinati", "Emilio Calvanese", ""]]}, {"id": "2103.17241", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "A multiscale model of urban morphogenesis", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics and processes of urban morphogenesis are a central issue\nregarding a long-term sustainability of urban systems. They however imply\nstakeholders and parameters at multiple temporal and spatial scales\nsimultaneously, leading to intricate interactions between dimensions and\nscales. We introduce in this paper from a theoretical viewpoint a simple\nagent-based model of urban morphogenesis at the scale of an urban area, with\nthe feature of integrating the microscopic and the mesoscopic scales. At the\nlocal level, developer agents drive urban development conditioned by local\nproperties but also an infrastructure network at a smaller scale. The network\nevolves more slowly following global properties. Indicators of sustainability\nincluding modal shares and urban density suggest an application of the model to\nmulti-objective optimisation. We finally discuss possible implementation,\nextensions and applications of the model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:39:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Raimbault", "Juste", ""]]}]