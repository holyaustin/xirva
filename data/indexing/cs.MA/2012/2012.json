[{"id": "2012.00508", "submitter": "Rupert Mitchell", "authors": "Rupert Mitchell, Jan Blumenkamp and Amanda Prorok", "title": "Gaussian Process Based Message Filtering for Robust Multi-Agent\n  Cooperation in the Presence of Adversarial Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of providing robustness to adversarial\ncommunication in multi-agent systems. Specifically, we propose a solution\ntowards robust cooperation, which enables the multi-agent system to maintain\nhigh performance in the presence of anonymous non-cooperative agents that\ncommunicate faulty, misleading or manipulative information. In pursuit of this\ngoal, we propose a communication architecture based on Graph Neural Networks\n(GNNs), which is amenable to a novel Gaussian Process (GP)-based probabilistic\nmodel characterizing the mutual information between the simultaneous\ncommunications of different agents due to their physical proximity and relative\nposition. This model allows agents to locally compute approximate posterior\nprobabilities, or confidences, that any given one of their communication\npartners is being truthful. These confidences can be used as weights in a\nmessage filtering scheme, thereby suppressing the influence of suspicious\ncommunication on the receiving agent's decisions. In order to assess the\nefficacy of our method, we introduce a taxonomy of non-cooperative agents,\nwhich distinguishes them by the amount of information available to them. We\ndemonstrate in two distinct experiments that our method performs well across\nthis taxonomy, outperforming alternative methods. For all but the best informed\nadversaries, our filtering method is able to reduce the impact that\nnon-cooperative agents cause, reducing it to the point of negligibility, and\nwith negligible cost to performance in the absence of adversaries.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:21:58 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mitchell", "Rupert", ""], ["Blumenkamp", "Jan", ""], ["Prorok", "Amanda", ""]]}, {"id": "2012.01101", "submitter": "Kim Phuc Tran", "authors": "Zhenglei He, Kim Phuc Tran (GEMTEX), Sebastien Thomassey, Xianyi Zeng,\n  Jie Xu, Changhai Yi", "title": "Multi-Objective Optimization of the Textile Manufacturing Process Using\n  Deep-Q-Network Based Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective optimization of the textile manufacturing process is an\nincreasing challenge because of the growing complexity involved in the\ndevelopment of the textile industry. The use of intelligent techniques has been\noften discussed in this domain, although a significant improvement from certain\nsuccessful applications has been reported, the traditional methods failed to\nwork with high-as well as human intervention. Upon which, this paper proposed a\nmulti-agent reinforcement learning (MARL) framework to transform the\noptimization process into a stochastic game and introduced the deep Q-networks\nalgorithm to train the multiple agents. A utilitarian selection mechanism was\nemployed in the stochastic game, which (-greedy policy) in each state to avoid\nthe interruption of multiple equilibria and achieve the correlated equilibrium\noptimal solutions of the optimizing process. The case study result reflects\nthat the proposed MARL system is possible to achieve the optimal solutions for\nthe textile ozonation process and it performs better than the traditional\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:37:44 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["He", "Zhenglei", "", "GEMTEX"], ["Tran", "Kim Phuc", "", "GEMTEX"], ["Thomassey", "Sebastien", ""], ["Zeng", "Xianyi", ""], ["Xu", "Jie", ""], ["Yi", "Changhai", ""]]}, {"id": "2012.01245", "submitter": "Fabian Schilling", "authors": "Fabian Schilling, Fabrizio Schiano, Dario Floreano", "title": "Vision-based Drone Flocking in Outdoor Environments", "comments": "8 pages, 8 figures, accepted for publication in the IEEE Robotics and\n  Automation Letters (RA-L) on February 2, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized deployment of drone swarms usually relies on inter-agent\ncommunication or visual markers that are mounted on the vehicles to simplify\ntheir mutual detection. This letter proposes a vision-based detection and\ntracking algorithm that enables groups of drones to navigate without\ncommunication or visual markers. We employ a convolutional neural network to\ndetect and localize nearby agents onboard the quadcopters in real-time. Rather\nthan manually labeling a dataset, we automatically annotate images to train the\nneural network using background subtraction by systematically flying a\nquadcopter in front of a static camera. We use a multi-agent state tracker to\nestimate the relative positions and velocities of nearby agents, which are\nsubsequently fed to a flocking algorithm for high-level control. The drones are\nequipped with multiple cameras to provide omnidirectional visual inputs. The\ncamera setup ensures the safety of the flock by avoiding blind spots regardless\nof the agent configuration. We evaluate the approach with a group of three real\nquadcopters that are controlled using the proposed vision-based flocking\nalgorithm. The results show that the drones can safely navigate in an outdoor\nenvironment despite substantial background clutter and difficult lighting\nconditions. The source code, image dataset, and trained detection model are\navailable at https://github.com/lis-epfl/vswarm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:44:40 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:13:38 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Schilling", "Fabian", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "2012.01369", "submitter": "Md. Musfiqur Rahman", "authors": "Md. Musfiqur Rahman, Mashrur Rashik, Md. Mamun-or-Rashid and Md.\n  Mosaddek Khan", "title": "Improving Solution Quality of Bounded Max-Sum Algorithm to Solve DCOPs\n  involving Hard and Soft Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bounded Max-Sum (BMS) is a message-passing algorithm that provides\napproximation solution to a specific form of de-centralized coordination\nproblems, namely Distributed Constrained Optimization Problems (DCOPs). In\nparticular, BMS algorithm is able to solve problems of this type having large\nsearch space at the expense of low computational cost. Notably, the traditional\nDCOP formulation does not consider those constraints that must be\nsatisfied(also known as hard constraints), rather it concentrates only on soft\nconstraints. Hence, although the presence of both types of constraints are\nobserved in a number of real-world applications, the BMS algorithm does not\nactively capitalize on the hard constraints. To address this issue, we tailor\nBMS in such a way that can deal with DCOPs having both type constraints. In so\ndoing, our approach improves the solution quality of the algorithm. The\nempirical results exhibit a marked improvement in the quality of the solutions\nof large DCOPs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:10:14 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Rahman", "Md. Musfiqur", ""], ["Rashik", "Mashrur", ""], ["Mamun-or-Rashid", "Md.", ""], ["Khan", "Md. Mosaddek", ""]]}, {"id": "2012.01474", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Elsa Rizk, Ali H. Sayed", "title": "Second-Order Guarantees in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a useful framework for centralized learning from\ndistributed data under practical considerations of heterogeneity, asynchrony,\nand privacy. Federated architectures are frequently deployed in deep learning\nsettings, which generally give rise to non-convex optimization problems.\nNevertheless, most existing analysis are either limited to convex loss\nfunctions, or only establish first-order stationarity, despite the fact that\nsaddle-points, which are first-order stationary, are known to pose bottlenecks\nin deep learning. We draw on recent results on the second-order optimality of\nstochastic gradient algorithms in centralized and decentralized settings, and\nestablish second-order guarantees for a class of federated learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:30:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Vlaski", "Stefan", ""], ["Rizk", "Elsa", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2012.01657", "submitter": "EPTCS", "authors": "Okan \\\"Ozkan (Carl von Ossietzky University of Oldenburg, Germany)", "title": "Modeling Adverse Conditions in the Framework of Graph Transformation\n  Systems", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 35-54", "doi": "10.4204/EPTCS.330.3", "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of adverse conditions addresses systems interacting with an\nadversary environment and finds use also in the development of new\ntechnologies. We present an approach for modeling adverse conditions by graph\ntransformation systems. In contrast to other approaches for\ngraph-transformational interacting systems, the presented main constructs are\ngraph transformation systems. We introduce joint graph transformation systems\nwhich involve a system, an interfering environment, and an automaton modeling\ntheir interaction. For joint graph transformation systems, we introduce notions\nof (partial) correctness under adverse conditions, which contain the\ncorrectness of the system and a recovery condition. As main result, we show\nthat two instances of correctness, namely k-step correctness (recovery in at\nmost k steps after an environment intervention) and last-minute correctness\n(recovery until next environment intervention) are expressible in LTL (linear\ntemporal logic), and that a weaker notion of k-step correctness is expressible\nin CTL (computation tree logic).\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:27:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["\u00d6zkan", "Okan", "", "Carl von Ossietzky University of Oldenburg, Germany"]]}, {"id": "2012.01928", "submitter": "Samet Uzun", "authors": "Samet Uzun, Nazim Kemal Ure", "title": "A Probabilistic Guidance Approach to Swarm-to-Swarm Engagement Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a probabilistic guidance approach for the\nswarm-to-swarm engagement problem. The idea is based on driving the controlled\nswarm towards an adversary swarm, where the adversary swarm aims to converge to\na stationary distribution that corresponds to a defended base location. The\nprobabilistic approach is based on designing a Markov chain for the\ndistribution of the swarm to converge a stationary distribution. This approach\nis decentralized, so each agent can propagate its position independently of\nother agents. Our main contribution is the formulation of the swarm-to-swarm\nengagement as an optimization problem where the population of each swarm decays\nwith each engagement and determining a desired distribution for the controlled\nswarm to converge time-varying distribution and eliminate agents of the\nadversary swarm until adversary swarm enters the defended base location. We\ndemonstrate the validity of proposed approach on several swarm engagement\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 12:52:27 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Uzun", "Samet", ""], ["Ure", "Nazim Kemal", ""]]}, {"id": "2012.01934", "submitter": "Zohreh Raziei", "authors": "Zohreh Raziei, Mohsen Moghaddam", "title": "Adaptable Automation with Modular Deep Reinforcement Learning and Policy\n  Transfer", "comments": "32 pages, 13 Figures, Presented at 2020 INFORMS Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep Reinforcement Learning (RL) have created\nunprecedented opportunities for intelligent automation, where a machine can\nautonomously learn an optimal policy for performing a given task. However,\ncurrent deep RL algorithms predominantly specialize in a narrow range of tasks,\nare sample inefficient, and lack sufficient stability, which in turn hinder\ntheir industrial adoption. This article tackles this limitation by developing\nand testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the\nnotions of task modularization and transfer learning. The goal of the proposed\nHASAC is to enhance the adaptability of an agent to new tasks by transferring\nthe learned policies of former tasks to the new task via a \"hyper-actor\". The\nHASAC framework is tested on a new virtual robotic manipulation benchmark,\nMeta-World. Numerical experiments show superior performance by HASAC over\nstate-of-the-art deep RL algorithms in terms of reward value, success rate, and\ntask completion time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 03:09:05 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Raziei", "Zohreh", ""], ["Moghaddam", "Mohsen", ""]]}, {"id": "2012.02096", "submitter": "Michael Dennis", "authors": "Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen,\n  Stuart Russell, Andrew Critch, Sergey Levine", "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of reinforcement learning (RL) problems - including robustness,\ntransfer learning, unsupervised RL, and emergent complexity - require\nspecifying a distribution of tasks or environments in which a policy will be\ntrained. However, creating a useful distribution of environments is error\nprone, and takes a significant amount of developer time and effort. We propose\nUnsupervised Environment Design (UED) as an alternative paradigm, where\ndevelopers provide environments with unknown parameters, and these parameters\nare used to automatically produce a distribution over valid, solvable\nenvironments. Existing approaches to automatically generating environments\nsuffer from common failure modes: domain randomization cannot generate\nstructure or adapt the difficulty of the environment to the agent's learning\nprogress, and minimax adversarial training leads to worst-case environments\nthat are often unsolvable. To generate structured, solvable environments for\nour protagonist agent, we introduce a second, antagonist agent that is allied\nwith the environment-generating adversary. The adversary is motivated to\ngenerate environments which maximize regret, defined as the difference between\nthe protagonist and antagonist agent's return. We call our technique\nProtagonist Antagonist Induced Regret Environment Design (PAIRED). Our\nexperiments demonstrate that PAIRED produces a natural curriculum of\nincreasingly complex environments, and PAIRED agents achieve higher zero-shot\ntransfer performance when tested in highly novel environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:37:01 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 03:01:31 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Dennis", "Michael", ""], ["Jaques", "Natasha", ""], ["Vinitsky", "Eugene", ""], ["Bayen", "Alexandre", ""], ["Russell", "Stuart", ""], ["Critch", "Andrew", ""], ["Levine", "Sergey", ""]]}, {"id": "2012.02178", "submitter": "George Atia", "authors": "George K. Atia, Andre Beckus, Ismail Alkhouri, Alvaro Velasquez", "title": "Verifiable Planning in Expected Reward Multichain MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The planning domain has experienced increased interest in the formal\nsynthesis of decision-making policies. This formal synthesis typically entails\nfinding a policy which satisfies formal specifications in the form of some\nwell-defined logic, such as Linear Temporal Logic (LTL) or Computation Tree\nLogic (CTL), among others. While such logics are very powerful and expressive\nin their capacity to capture desirable agent behavior, their value is limited\nwhen deriving decision-making policies which satisfy certain types of\nasymptotic behavior. In particular, we are interested in specifying constraints\non the steady-state behavior of an agent, which captures the proportion of time\nan agent spends in each state as it interacts for an indefinite period of time\nwith its environment. This is sometimes called the average or expected behavior\nof the agent. In this paper, we explore the steady-state planning problem of\nderiving a decision-making policy for an agent such that constraints on its\nsteady-state behavior are satisfied. A linear programming solution for the\ngeneral case of multichain Markov Decision Processes (MDPs) is proposed and we\nprove that optimal solutions to the proposed programs yield stationary policies\nwith rigorous guarantees of behavior.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:54:24 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Atia", "George K.", ""], ["Beckus", "Andre", ""], ["Alkhouri", "Ismail", ""], ["Velasquez", "Alvaro", ""]]}, {"id": "2012.02303", "submitter": "Samet Uzun", "authors": "Samet Uzun, Nazim Kemal Ure", "title": "Decentralized State-Dependent Markov Chain Synthesis for Swarm Guidance", "comments": "arXiv admin note: text overlap with arXiv:2012.01928", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a decentralized state-dependent Markov chain synthesis\nmethod for probabilistic swarm guidance of a large number of autonomous agents\nto a desired steady-state distribution. The probabilistic swarm guidance\napproach is based on using a Markov chain that determines the transition\nprobabilities of agents to transition from one state to another while\nsatisfying prescribed transition constraints and converging to a desired\nsteady-state distribution. Our main contribution is to develop a decentralized\napproach to the Markov chain synthesis that updates the underlying column\nstochastic Markov matrix as a function of the state, i.e., the current swarm\nprobability distribution. Having a decentralized synthesis method eliminates\nthe need to have complex communication architecture. Furthermore, the proposed\nmethod aims to cause a minimal number of state transitions to minimize resource\nusage while guaranteeing convergence to the desired distribution. It is also\nshown that the convergence rate is faster when compared with previously\nproposed methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:10:54 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Uzun", "Samet", ""], ["Ure", "Nazim Kemal", ""]]}, {"id": "2012.02340", "submitter": "Aniket Shirsat", "authors": "Aniket Shirsat, Spring Berman", "title": "Decentralized Multi-target Tracking with Multiple Quadrotors using a PHD\n  Filter", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider a scenario in which a group of quadrotors is tasked at tracking\nmultiple stationary targets in an unknown, bounded environment. The quadrotors\nsearch for targets along a spatial grid overlaid on the environment while\nperforming a random walk on this grid modeled by a discrete-time discrete-state\n(DTDS) Markov chain. The quadrotors can transmit their estimates of the target\nlocations to other quadrotors that occupy their current location on the grid;\nthus, their communication network is time-varying and not necessarily\nconnected. We model the search procedure as a renewal-reward process on the\nunderlying DTDS Markov chain. To accommodate changes in the set of targets\nobserved by each quadrotor as it explores the environment, along with\nuncertainties in the quadrotors' measurements of the targets, we formulate the\ntracking problem in terms of Random Finite Sets (RFS). The quadrotors use\nRFS-based Probability Hypothesis Density (PHD) filters to estimate the number\nof targets and their locations. We present a theoretical estimation framework,\nbased on the Gaussian Mixture formulation of the PHD filter, and preliminary\nsimulation results toward extending existing approaches for RFS-based\nmulti-target tracking to a decentralized multi-robot strategy for multi-target\ntracking. We validate this approach with simulations of multi-target tracking\nscenarios with different densities of robots and targets, and we evaluate the\naverage time required for the robots in each scenario to reach agreement on a\ncommon set of targets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 00:07:02 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Shirsat", "Aniket", ""], ["Berman", "Spring", ""]]}, {"id": "2012.02703", "submitter": "Santiago Quintero", "authors": "M\\'ario S. Alvim (1), Bernardo Amorim (1), Sophia Knight (2), Santiago\n  Quintero (3), Frank Valencia (4) ((1) Department of Computer Science,\n  Universidade Federal de Minas Gerais, (2) Department of Computer Science,\n  University of Minnesotta Duluth, (3) LIX, \\'Ecole Polytechnique de Paris, (4)\n  CNRS-LIX, \\'Ecole Polytechnique de Paris)", "title": "Polarization and Belief Convergence of Agents in Strongly-Connected\n  Influence Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a model for polarization in multi-agent systems based on Esteban\nand Ray's classic measure of polarization from economics. Agents evolve by\nupdating their beliefs (opinions) based on the beliefs of others and an\nunderlying influence graph. We show that polarization eventually disappears\n(converges to zero) if the influence graph is strongly-connected. If the\ninfluence graph is a circulation we determine the unique belief value all\nagents converge to. For clique influence graphs we determine the time after\nwhich agents will reach a given difference of opinion. Our results imply that\nif polarization does not disappear then either there is a disconnected subgroup\nof agents or some agent influences others more than she is influenced. Finally,\nwe show that polarization does not necessarily vanish in weakly-connected\ngraphs, and illustrate the model with a series of case studies and simulations\ngiving some insights about polarization.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:21:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Alvim", "M\u00e1rio S.", ""], ["Amorim", "Bernardo", ""], ["Knight", "Sophia", ""], ["Quintero", "Santiago", ""], ["Valencia", "Frank", ""]]}, {"id": "2012.02811", "submitter": "Jaelle Scheuerman", "authors": "Jaelle Scheuerman, Jason Harman, Nicholas Mattei, K. Brent Venable", "title": "Modeling Voters in Multi-Winner Approval Voting", "comments": "9 pages, 4 figures. To be published in the Proceedings of the\n  Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world situations, collective decisions are made using voting\nand, in scenarios such as committee or board elections, employing voting rules\nthat return multiple winners. In multi-winner approval voting (AV), an agent\nsubmits a ballot consisting of approvals for as many candidates as they wish,\nand winners are chosen by tallying up the votes and choosing the top-$k$\ncandidates receiving the most approvals. In many scenarios, an agent may\nmanipulate the ballot they submit in order to achieve a better outcome by\nvoting in a way that does not reflect their true preferences. In complex and\nuncertain situations, agents may use heuristics instead of incurring the\nadditional effort required to compute the manipulation which most favors them.\nIn this paper, we examine voting behavior in single-winner and multi-winner\napproval voting scenarios with varying degrees of uncertainty using behavioral\ndata obtained from Mechanical Turk. We find that people generally manipulate\ntheir vote to obtain a better outcome, but often do not identify the optimal\nmanipulation. There are a number of predictive models of agent behavior in the\nCOMSOC and psychology literature that are based on cognitively plausible\nheuristic strategies. We show that the existing approaches do not adequately\nmodel real-world data. We propose a novel model that takes into account the\nsize of the winning set and human cognitive constraints, and demonstrate that\nthis model is more effective at capturing real-world behaviors in multi-winner\napproval voting scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:24:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Scheuerman", "Jaelle", ""], ["Harman", "Jason", ""], ["Mattei", "Nicholas", ""], ["Venable", "K. Brent", ""]]}, {"id": "2012.03083", "submitter": "Stefanos Leonardos Mr.", "authors": "Stefanos Leonardos and Georgios Piliouras", "title": "Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory\n  Meets Game Theory", "comments": "Appears in the 35th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploration-exploitation is a powerful and practical tool in multi-agent\nlearning (MAL), however, its effects are far from understood. To make progress\nin this direction, we study a smooth analogue of Q-learning. We start by\nshowing that our learning model has strong theoretical justification as an\noptimal model for studying exploration-exploitation. Specifically, we prove\nthat smooth Q-learning has bounded regret in arbitrary games for a cost model\nthat explicitly captures the balance between game and exploration costs and\nthat it always converges to the set of quantal-response equilibria (QRE), the\nstandard solution concept for games under bounded rationality, in weighted\npotential games with heterogeneous learning agents. In our main task, we then\nturn to measure the effect of exploration in collective system performance. We\ncharacterize the geometry of the QRE surface in low-dimensional MAL systems and\nlink our findings with catastrophe (bifurcation) theory. In particular, as the\nexploration hyperparameter evolves over-time, the system undergoes phase\ntransitions where the number and stability of equilibria can change radically\ngiven an infinitesimal change to the exploration parameter. Based on this, we\nprovide a formal theoretical treatment of how tuning the exploration parameter\ncan provably lead to equilibrium selection with both positive as well as\nnegative (and potentially unbounded) effects to system performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 17:37:22 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 14:33:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Leonardos", "Stefanos", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2012.03158", "submitter": "Ye Hu", "authors": "Ye Hu, Mingzhe Chen, Walid Saad, H. Vincent Poor, and Shuguang Cui", "title": "Distributed Multi-agent Meta Learning for Trajectory Design in Wireless\n  Drone Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of the trajectory design for a group of\nenergy-constrained drones operating in dynamic wireless network environments is\nstudied. In the considered model, a team of drone base stations (DBSs) is\ndispatched to cooperatively serve clusters of ground users that have dynamic\nand unpredictable uplink access demands. In this scenario, the DBSs must\ncooperatively navigate in the considered area to maximize coverage of the\ndynamic requests of the ground users. This trajectory design problem is posed\nas an optimization framework whose goal is to find optimal trajectories that\nmaximize the fraction of users served by all DBSs. To find an optimal solution\nfor this non-convex optimization problem under unpredictable environments, a\nvalue decomposition based reinforcement learning (VDRL) solution coupled with a\nmeta-training mechanism is proposed. This algorithm allows the DBSs to\ndynamically learn their trajectories while generalizing their learning to\nunseen environments. Analytical results show that, the proposed VD-RL algorithm\nis guaranteed to converge to a local optimal solution of the non-convex\noptimization problem. Simulation results show that, even without meta-training,\nthe proposed VD-RL algorithm can achieve a 53.2% improvement of the service\ncoverage and a 30.6% improvement in terms of the convergence speed, compared to\nbaseline multi-agent algorithms. Meanwhile, the use of meta-learning improves\nthe convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must\ndeal with a previously unseen task.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 01:30:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hu", "Ye", ""], ["Chen", "Mingzhe", ""], ["Saad", "Walid", ""], ["Poor", "H. Vincent", ""], ["Cui", "Shuguang", ""]]}, {"id": "2012.03243", "submitter": "Lifeng Wang", "authors": "Lifeng Wang, Yu Duan, Yun Lai, Shizhuo Mu, Xiang Li", "title": "V2I-Based Platooning Design with Delay Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the vehicle platooning system based on\nvehicle-to-infrastructure (V2I) communication, where all the vehicles in the\nplatoon upload their driving state information to the roadside unit (RSU), and\nRSU makes the platoon control decisions with the assistance of edge computing.\nBy addressing the delay concern, a platoon control approach is proposed to\nachieve plant stability and string stability. The effects of the time headway,\ncommunication and edge computing delays on the stability are quantified. The\nvelocity and size of the stable platoon are calculated, which show the impacts\nof the radio parameters such as massive MIMO antennas and frequency band on the\nplatoon configuration. The handover performance between RSUs in the V2I-based\nplatooning system is quantified by considering the effects of the RSU's\ncoverage and platoon size, which demonstrates that the velocity of a stable\nplatoon should be appropriately chosen, in order to meet the V2I's\nQuality-of-Service and handover constraints.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 11:44:42 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Lifeng", ""], ["Duan", "Yu", ""], ["Lai", "Yun", ""], ["Mu", "Shizhuo", ""], ["Li", "Xiang", ""]]}, {"id": "2012.03766", "submitter": "Xiaowei Wu", "authors": "Xiaowei Wu, Bo Li and Jiarui Gan", "title": "Budget-feasible Maximum Nash Social Welfare Allocation is Almost\n  Envy-free", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Nash social welfare (NSW) is a well-known social welfare measurement that\nbalances individual utilities and the overall efficiency. In the context of\nfair allocation of indivisible goods, it has been shown by Caragiannis et al.\n(EC 2016 and TEAC 2019) that an allocation maximizing the NSW is envy-free up\nto one good (EF1). In this paper, we are interested in the fairness of the NSW\nin a budget-feasible allocation problem, in which each item has a cost that\nwill be incurred to the agent it is allocated to, and each agent has a budget\nconstraint on the total cost of items she receives. We show that a\nbudget-feasible allocation that maximizes the NSW achieves a 1/4-approximation\nof EF1 and the approximation ratio is tight. The approximation ratio improves\ngracefully when the items have small costs compared with the agents' budgets;\nit converges to 1/2 when the budget-cost ratio approaches infinity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:07:40 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wu", "Xiaowei", ""], ["Li", "Bo", ""], ["Gan", "Jiarui", ""]]}, {"id": "2012.04107", "submitter": "Graham Todd", "authors": "Graham Todd, Shane Steinert-Threlkeld, Christopher Potts", "title": "Learning Compositional Negation in Populations of Roth-Erev and Neural\n  Agents", "comments": "7 pages, 2 figures, 1-page technical appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Agent-based models and signalling games are useful tools with which to study\nthe emergence of linguistic communication in a tractable setting. These\ntechniques have been used to study the compositional property of natural\nlanguages, but have been limited in how closely they model real communicators.\nIn this work, we present a novel variant of the classic signalling game that\nexplores the learnability of simple compositional rules concerning negation.\nThe approach builds on the work of Steinert-Threlkeld (2016) by allowing agents\nto determine the identity of the \"function word\" representing negation while\nsimultaneously learning to assign meanings to atomic symbols. We extend the\nanalysis with the introduction of a population of concurrently communicating\nagents, and explore how the complications brought about by a larger population\nsize affect the type and stability of the signalling systems learned. We also\nrelax assumptions of the parametric form of the learning agents and examine how\nneural network-based agents optimized through reinforcement learning behave\nunder various task settings. We find that basic compositional properties are\nrobustly learnable across a wide range of model relaxations and agent\ninstantiations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:20:15 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Todd", "Graham", ""], ["Steinert-Threlkeld", "Shane", ""], ["Potts", "Christopher", ""]]}, {"id": "2012.04144", "submitter": "John Harwell", "authors": "John Harwell, Maria Gini", "title": "Improved Swarm Engineering: Aligning Intuition and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When designing swarm-robotic systems, systematic comparison of swarm control\nalgorithms is necessary to determine which can scale up to handle the target\nproblem size and operating conditions. Qualitative predictions of performance\nbased on algorithm descriptions are often incorrect, and can lead to costly\ndesign processes for swarm-robotic systems. We propose a set of quantitative\nmeasures for swarm scalability, emergence, flexibility, and robustness which\nenable swarm control algorithms analysis and comparison, swarm performance of a\ngiven control algorithm, collectively enabling quicker and more confident\ndesign decisions. We demonstrate the utility of our proposed measurements as\nmodeling and design tools for real-world scenarios by analyzing two common\nproblems, indoor warehouse object transport and search and rescue, and present\nexperimental results obtained in simulation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:08:08 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Harwell", "John", ""], ["Gini", "Maria", ""]]}, {"id": "2012.04174", "submitter": "Ninad Jadhav", "authors": "Ninad Jadhav, Weiying Wang, Diana Zhang, Oussama Khatib, Swarun Kumar\n  and Stephanie Gil", "title": "WSR: A WiFi Sensor for Collaborative Robotics", "comments": "28 pages, 25 figures, *co-primary authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive a new capability for robots to measure relative\ndirection, or Angle-of-Arrival (AOA), to other robots operating in\nnon-line-of-sight and unmapped environments with occlusions, without requiring\nexternal infrastructure. We do so by capturing all of the paths that a WiFi\nsignal traverses as it travels from a transmitting to a receiving robot, which\nwe term an AOA profile. The key intuition is to \"emulate antenna arrays in the\nair\" as the robots move in 3D space, a method akin to Synthetic Aperture Radar\n(SAR). The main contributions include development of i) a framework to\naccommodate arbitrary 3D trajectories, as well as continuous mobility all\nrobots, while computing AOA profiles and ii) an accompanying analysis that\nprovides a lower bound on variance of AOA estimation as a function of robot\ntrajectory geometry based on the Cramer Rao Bound. This is a critical\ndistinction with previous work on SAR that restricts robot mobility to\nprescribed motion patterns, does not generalize to 3D space, and/or requires\ntransmitting robots to be static during data acquisition periods. Our method\nresults in more accurate AOA profiles and thus better AOA estimation, and\nformally characterizes this observation as the informativeness of the\ntrajectory; a computable quantity for which we derive a closed form. All\ntheoretical developments are substantiated by extensive simulation and hardware\nexperiments. We also show that our formulation can be used with an\noff-the-shelf trajectory estimation sensor. Finally, we demonstrate the\nperformance of our system on a multi-robot dynamic rendezvous task.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:31:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 21:56:08 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jadhav", "Ninad", ""], ["Wang", "Weiying", ""], ["Zhang", "Diana", ""], ["Khatib", "Oussama", ""], ["Kumar", "Swarun", ""], ["Gil", "Stephanie", ""]]}, {"id": "2012.04213", "submitter": "Amir-Salar Esteki", "authors": "Amir-Salar Esteki and Solmaz S. Kia", "title": "Deterministic Privacy Preservation in Static Average Consensus Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of privacy preservation in the static\naverage consensus problem. This problem normally is solved by proposing privacy\npreservation augmentations for the popular first order Laplacian-based\nalgorithm. These mechanisms however come with computational overhead, may need\ncoordination among the agents to choose their parameters and also alter the\ntransient response of the algorithm. In this paper we show that an alternative\niterative algorithm that is proposed in the literature in the context of\ndynamic average consensus problem has intrinsic privacy preservation and can be\nused as a privacy preserving algorithm that yields the same performance\nbehavior as the well-known Laplacian consensus algorithm but without the\noverheads that come with the existing privacy preservation methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:59:29 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Esteki", "Amir-Salar", ""], ["Kia", "Solmaz S.", ""]]}, {"id": "2012.04582", "submitter": "Dmitry Shalymov S", "authors": "Dmitry S. Shalymov, Oleg N. Granichin, Zeev Volkovich and\n  Gerhard-Wilhelm Weber", "title": "Multi-agent control of airplane wing stability under the flexural\n  torsion flutter", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for prevention of the increasing\noscillation of an aircraft wing under the flexural torsion flutter. The paper\nintroduces the novel multi-agent method for control of an aircraft wing,\nassuming that the wing surface consists of controlled 'feathers' (agents).\nTheoretical evaluation of the approach demonstrates its high ability to prevent\nflexural-torsional vibrations of an aircraft. Our model expands the\npossibilities for damping the wing oscillations, which potentially allows an\nincrease in aircraft speed without misgiving of flutter. The study shows that\nthe main limitation is the time, during which the system is able to damp\nvibrations to a safe level and keep them. The relevance of this indicator is\nimportant because of the rather fast process of increasing wing oscillations\nduring flutter. In this paper, we suggest a new method for controlling an\naircraft wing, with the use of which it becomes theoretically possible to\nincrease the maximum flight speed of an aircraft without flutter occurrence. A\nmathematical model of the bending-torsional vibrations of an airplane wing with\ncontrolled feathers on its surface is presented. Based on the Speed-Gradient\nmethod a new control laws are synthesized.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:32:10 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Shalymov", "Dmitry S.", ""], ["Granichin", "Oleg N.", ""], ["Volkovich", "Zeev", ""], ["Weber", "Gerhard-Wilhelm", ""]]}, {"id": "2012.04861", "submitter": "Pedro Enrique Iturria Rivera Mr.", "authors": "Pedro Enrique Iturria Rivera, Shahram Mollahasani, Melike\n  Erol-Kantarci", "title": "Multi Agent Team Learning in Disaggregated Virtualized Open Radio Access\n  Networks (O-RAN)", "comments": "7 pages, 3 figures, 1 table, submitted to IEEE Wireless\n  Communications Magazine on Feb, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Starting from the Cloud Radio Access Network (C-RAN), continuing with the\nvirtual Radio Access Network (vRAN) and most recently with Open RAN (O-RAN)\ninitiative, Radio Access Network (RAN) architectures have significantly evolved\nin the past decade. In the last few years, the wireless industry has witnessed\na strong trend towards disaggregated, virtualized and open RANs, with numerous\ntests and deployments world wide. One unique aspect that motivates this paper\nis the availability of new opportunities that arise from using machine learning\nto optimize the RAN in closed-loop, i.e. without human intervention, where the\ncomplexity of disaggregation and virtualization makes well-known Self-Organized\nNetworking (SON) solutions inadequate. In our view, Multi-Agent Systems (MASs)\nwith team learning, can play an essential role in the control and coordination\nof controllers of O-RAN, i.e. near-real-time and non-real-time RAN Intelligent\nController (RIC). In this article, we first present the state-of-the-art\nresearch in multi-agent systems and team learning, then we provide an overview\nof the landscape in RAN disaggregation and virtualization, as well as O-RAN\nwhich emphasizes the open interfaces introduced by the O-RAN Alliance. We\npresent a case study for agent placement and the AI feedback required in O-RAN,\nand finally, we identify challenges and open issues to provide a roadmap for\nresearchers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:47:07 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 03:41:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Rivera", "Pedro Enrique Iturria", ""], ["Mollahasani", "Shahram", ""], ["Erol-Kantarci", "Melike", ""]]}, {"id": "2012.04980", "submitter": "Michael Amir", "authors": "Michael Amir, Noa Agmon, Alfred M. Bruckstein", "title": "A Discrete Model of Collective Marching on Rings", "comments": "To appear in DARS2021 (\"The 15th International Symposium on\n  Distributed Autonomous Robotics\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DM math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the collective motion of autonomous mobile agents on a ringlike\nenvironment. The agents' dynamics is inspired by known laboratory experiments\non the dynamics of locust swarms. In these experiments, locusts placed at\narbitrary locations and initial orientations on a ring-shaped arena are\nobserved to eventually all march in the same direction. In this work we ask\nwhether, and how fast, a similar phenomenon occurs in a stochastic swarm of\nsimple agents whose goal is to maintain the same direction of motion for as\nlong as possible. The agents are randomly initiated as marching either\nclockwise or counterclockwise on a wide ring-shaped region, which we model as\n$k$ \"narrow\" concentric tracks on a cylinder. Collisions cause agents to change\ntheir direction of motion. To avoid this, agents may decide to switch tracks so\nas to merge with platoons of agents marching in their direction.\n  We prove that such agents must eventually converge to a local consensus about\ntheir direction of motion, meaning that all agents on each narrow track must\neventually march in the same direction. We give asymptotic bounds for the\nexpected amount of time it takes for such convergence or \"stabilization\" to\noccur, which depends on the number of agents, the length of the tracks, and the\nnumber of tracks. We show that when agents also have a small probability of\n\"erratic\", random track-jumping behaviour, a global consensus on the direction\nof motion across all tracks will eventually be reached. Finally, we verify our\ntheoretical findings in numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:16:36 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 02:51:47 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 07:49:46 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 20:55:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Amir", "Michael", ""], ["Agmon", "Noa", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "2012.05105", "submitter": "Xuejie Wang", "authors": "Honghao Gao and Xuejie Wang and Xiaojin Ma and Wei Wei and Shahid\n  Mumtaz", "title": "Com-DDPG: A Multiagent Reinforcement Learning-based Offloading Strategy\n  for Mobile Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of mobile services has impacted a variety of\ncomputation-intensive and time-sensitive applications, such as recommendation\nsystems and daily payment methods. However, computing task competition\ninvolving limited resources increases the task processing latency and energy\nconsumption of mobile devices, as well as time constraints. Mobile edge\ncomputing (MEC) has been widely used to address these problems. However, there\nare limitations to existing methods used during computation offloading. On the\none hand, they focus on independent tasks rather than dependent tasks. The\nchallenges of task dependency in the real world, especially task segmentation\nand integration, remain to be addressed. On the other hand, the multiuser\nscenarios related to resource allocation and the mutex access problem must be\nconsidered. In this paper, we propose a novel offloading approach, Com-DDPG,\nfor MEC using multiagent reinforcement learning to enhance the offloading\nperformance. First, we discuss the task dependency model, task priority model,\nenergy consumption model, and average latency from the perspective of server\nclusters and multidependence on mobile tasks. Our method based on these models\nis introduced to formalize communication behavior among multiple agents; then,\nreinforcement learning is executed as an offloading strategy to obtain the\nresults. Because of the incomplete state information, long short-term memory\n(LSTM) is employed as a decision-making tool to assess the internal state.\nMoreover, to optimize and support effective action, we consider using a\nbidirectional recurrent neural network (BRNN) to learn and enhance features\nobtained from agents' communication. Finally, we simulate experiments on the\nAlibaba cluster dataset. The results show that our method is better than other\nbaselines in terms of energy consumption, load status and latency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:22:47 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Gao", "Honghao", ""], ["Wang", "Xuejie", ""], ["Ma", "Xiaojin", ""], ["Wei", "Wei", ""], ["Mumtaz", "Shahid", ""]]}, {"id": "2012.05213", "submitter": "Krzysztof Sornat", "authors": "Pallavi Jain, Krzysztof Sornat, Nimrod Talmon, Meirav Zehavi", "title": "Participatory Budgeting with Project Groups", "comments": "23 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the standard approval-based model of\nparticipatory budgeting (PB), in which voters are providing approval ballots\nover a set of predefined projects and -- in addition to a global budget limit,\nthere are several groupings of the projects, each group with its own budget\nlimit. We study the computational complexity of identifying project bundles\nthat maximize voter satisfaction while respecting all budget limits. We show\nthat the problem is generally intractable and describe efficient exact\nalgorithms for several special cases, including instances with only few groups\nand instances where the group structure is close to be hierarchical, as well as\nefficient approximation algorithms. Our results could allow, e.g.,\nmunicipalities to hold richer PB processes that are thematically and\ngeographically inclusive.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:23:04 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Jain", "Pallavi", ""], ["Sornat", "Krzysztof", ""], ["Talmon", "Nimrod", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2012.05672", "submitter": "Timothy Lillicrap", "authors": "Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico\n  Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc,\n  Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden\n  Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson,\n  So\\v{n}a Mokr\\'a, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant\n  Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, Rui Zhu", "title": "Imitating Interactive Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common vision from science fiction is that robots will one day inhabit our\nphysical spaces, sense the world as we do, assist our physical labours, and\ncommunicate with us through natural language. Here we study how to design\nartificial agents that can interact naturally with humans using the\nsimplification of a virtual environment. This setting nevertheless integrates a\nnumber of the central challenges of artificial intelligence (AI) research:\ncomplex visual perception and goal-directed physical control, grounded language\ncomprehension and production, and multi-agent social interaction. To build\nagents that can robustly interact with humans, we would ideally train them\nwhile they interact with humans. However, this is presently impractical.\nTherefore, we approximate the role of the human with another learned agent, and\nuse ideas from inverse reinforcement learning to reduce the disparities between\nhuman-human and agent-agent interactive behaviour. Rigorously evaluating our\nagents poses a great challenge, so we develop a variety of behavioural tests,\nincluding evaluation by humans who watch videos of agents or interact directly\nwith them. These evaluations convincingly demonstrate that interactive training\nand auxiliary losses improve agent behaviour beyond what is achieved by\nsupervised learning of actions alone. Further, we demonstrate that agent\ncapabilities generalise beyond literal experiences in the dataset. Finally, we\ntrain evaluation models whose ratings of agents agree well with human\njudgement, thus permitting the evaluation of new agent models without\nadditional effort. Taken together, our results in this virtual environment\nprovide evidence that large-scale human behavioural imitation is a promising\ntool to create intelligent, interactive agents, and the challenge of reliably\nevaluating such agents is possible to surmount.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:55:47 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 03:25:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Abramson", "Josh", ""], ["Ahuja", "Arun", ""], ["Barr", "Iain", ""], ["Brussee", "Arthur", ""], ["Carnevale", "Federico", ""], ["Cassin", "Mary", ""], ["Chhaparia", "Rachita", ""], ["Clark", "Stephen", ""], ["Damoc", "Bogdan", ""], ["Dudzik", "Andrew", ""], ["Georgiev", "Petko", ""], ["Guy", "Aurelia", ""], ["Harley", "Tim", ""], ["Hill", "Felix", ""], ["Hung", "Alden", ""], ["Kenton", "Zachary", ""], ["Landon", "Jessica", ""], ["Lillicrap", "Timothy", ""], ["Mathewson", "Kory", ""], ["Mokr\u00e1", "So\u0148a", ""], ["Muldal", "Alistair", ""], ["Santoro", "Adam", ""], ["Savinov", "Nikolay", ""], ["Varma", "Vikrant", ""], ["Wayne", "Greg", ""], ["Williams", "Duncan", ""], ["Wong", "Nathaniel", ""], ["Yan", "Chen", ""], ["Zhu", "Rui", ""]]}, {"id": "2012.05894", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Kris Kitani", "title": "AutoSelect: Automatic and Dynamic Detection Selection for 3D\n  Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking is an important component in robotic perception\nsystems such as self-driving vehicles. Recent work follows a\ntracking-by-detection pipeline, which aims to match past tracklets with\ndetections in the current frame. To avoid matching with false positive\ndetections, prior work filters out detections with low confidence scores via a\nthreshold. However, finding a proper threshold is non-trivial, which requires\nextensive manual search via ablation study. Also, this threshold is sensitive\nto many factors such as target object category so we need to re-search the\nthreshold if these factors change. To ease this process, we propose to\nautomatically select high-quality detections and remove the efforts needed for\nmanual threshold search. Also, prior work often uses a single threshold per\ndata sequence, which is sub-optimal in particular frames or for certain\nobjects. Instead, we dynamically search threshold per frame or per object to\nfurther boost performance. Through experiments on KITTI and nuScenes, our\nmethod can filter out $45.7\\%$ false positives while maintaining the recall,\nachieving new S.O.T.A. performance and removing the need for manually threshold\ntuning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:55:51 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "2012.06539", "submitter": "Stanis{\\l}aw Szufa", "authors": "Dariusz Stolicki, Stanis{\\l}aw Szufa, Nimrod Talmon", "title": "Pabulib: A Participatory Budgeting Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe the PArticipatory BUdgeting LIBrary website (in short, Pabulib),\nwhich can be accessed via http://pabulib.org/, and which is a library of\nparticipatory budgeting data. In particular, we describe the file format (.pb)\nthat is used for instances of participatory budgeting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:23:28 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Stolicki", "Dariusz", ""], ["Szufa", "Stanis\u0142aw", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2012.06652", "submitter": "Stefano Guarino", "authors": "Stefano Guarino, Enrico Mastrostefano, Massimo Bernaschi, Alessandro\n  Celestini, Marco Cianfriglia, Davide Torre, Lena Zastrow", "title": "Inferring urban social networks from publicly available data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence of social networks and the definition of suitable generative\nmodels for synthetic yet realistic social graphs are widely studied problems in\nthe literature. By not being tied to any real data, random graph models cannot\ncapture all the subtleties of real networks and are inadequate for many\npractical contexts -- including areas of research, such as computational\nepidemiology, which are recently high on the agenda. At the same time, the\nso-called contact networks describe interactions, rather than relationships,\nand are strongly dependent on the application and on the size and quality of\nthe sample data used to infer them. To fill the gap between these two\napproaches, we present a data-driven model for urban social networks,\nimplemented and released as open source software. Given a territory of\ninterest, and only based on widely available aggregated demographic and\nsocial-mixing data, we construct an age-stratified and geo-referenced synthetic\npopulation whose individuals are connected by \"strong ties\" of two types:\nintra-household (e.g., kinship) or friendship. While household links are\nentirely data-driven, we propose a parametric probabilistic model for\nfriendship, based on the assumption that distances and age differences play a\nrole, and that not all individuals are equally sociable. The demographic and\ngeographic factors governing the structure of the obtained network, under\ndifferent configurations, are thoroughly studied through extensive simulations\nfocused on three Italian cities of different size.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:47:00 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 19:59:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Guarino", "Stefano", ""], ["Mastrostefano", "Enrico", ""], ["Bernaschi", "Massimo", ""], ["Celestini", "Alessandro", ""], ["Cianfriglia", "Marco", ""], ["Torre", "Davide", ""], ["Zastrow", "Lena", ""]]}, {"id": "2012.07348", "submitter": "Lydia T. Liu", "authors": "Lydia T. Liu, Feng Ruan, Horia Mania, Michael I. Jordan", "title": "Bandit Learning in Decentralized Matching Markets", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two-sided matching markets in which one side of the market (the\nplayers) does not have a priori knowledge about its preferences for the other\nside (the arms) and is required to learn its preferences from experience. Also,\nwe assume the players have no direct means of communication. This model extends\nthe standard stochastic multi-armed bandit framework to a decentralized\nmultiple player setting with competition. We introduce a new algorithm for this\nsetting that, over a time horizon $T$, attains $\\mathcal{O}(\\log(T))$ stable\nregret when preferences of the arms over players are shared, and\n$\\mathcal{O}(\\log(T)^2)$ regret when there are no assumptions on the\npreferences on either side. Moreover, in the setting where a single player may\ndeviate, we show that the algorithm is incentive compatible whenever the arms'\npreferences are shared, but not necessarily so when preferences are fully\ngeneral.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:58:07 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 00:29:03 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 03:55:05 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 19:56:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Liu", "Lydia T.", ""], ["Ruan", "Feng", ""], ["Mania", "Horia", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2012.07617", "submitter": "Douglas Meneghetti", "authors": "Douglas De Rizzo Meneghetti, Reinaldo Augusto da Costa Bianchi", "title": "Specializing Inter-Agent Communication in Heterogeneous Multi-Agent\n  Reinforcement Learning using Agent Class Information", "comments": "Presented at the AAAI-21 Workshop on Artificial Intelligence in Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in agent communication with graph neural\nnetworks, this work proposes the representation of multi-agent communication\ncapabilities as a directed labeled heterogeneous agent graph, in which node\nlabels denote agent classes and edge labels, the communication type between two\nclasses of agents. We also introduce a neural network architecture that\nspecializes communication in fully cooperative heterogeneous multi-agent tasks\nby learning individual transformations to the exchanged messages between each\npair of agent classes. By also employing encoding and action selection modules\nwith parameter sharing for environments with heterogeneous agents, we\ndemonstrate comparable or superior performance in environments where a larger\nnumber of agent classes operates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:09:57 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:19:56 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Meneghetti", "Douglas De Rizzo", ""], ["Bianchi", "Reinaldo Augusto da Costa", ""]]}, {"id": "2012.07623", "submitter": "Daniel Biedermann", "authors": "Daniel H. Biedermann, Jan Clever, Andre Borrmann", "title": "A generic and density-sensitive method for multi-scale pedestrian\n  dynamics", "comments": null, "journal-ref": "Automation in Construction 122 (2021) 103489", "doi": "10.1016/j.autcon.2020.103489", "report-no": null, "categories": "cs.MA physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Microscopic approaches to the simulation of pedestrian dynamics rely on\nmodelling the behaviour of individual agents and their mutual interactions.\nRegarding the spatial resolution, microscopic simulators are either based on\ncontinuous (SpaceCont) or discrete (SpaceDisc) approaches. To combine the\nadvantages of both approaches, we propose to integrate SpaceCont and SpaceDisc\ninto a hybrid simulation model. Such a hybrid approach allows simulating\ncritical regions with a continuous spatial resolution and uncritical ones with\ndiscrete spatial resolution while enabling consistent information exchange\nbetween the two simulation models. We introduce a generic approach that\nprovides consistent solutions for the challenges resulting from coupling\ndiverging time steps and spatial resolutions. Furthermore, we present a dynamic\nand density-sensitive approach to detect dense areas during the simulation run.\nIf a critical region is detected, the simulation model used in this area is\ndynamically switched to a space-continuous one. The correctness of the hybrid\nmodel is evaluated by comparison with a established simulator. Its superior\ncomputational efficiency is shown by runtime comparison with a standard\nmicroscopic simulation.on with the simulation results of other,\nwell-established simulation models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 11:37:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Biedermann", "Daniel H.", ""], ["Clever", "Jan", ""], ["Borrmann", "Andre", ""]]}, {"id": "2012.07949", "submitter": "Fabian Ritz", "authors": "Fabian Ritz, Thomy Phan, Robert M\\\"uller, Thomas Gabor, Andreas\n  Sedlmeier, Marc Zeller, Jan Wieghardt, Reiner Schmid, Horst Sauer, Cornel\n  Klein, Claudia Linnhoff-Popien", "title": "SAT-MARL: Specification Aware Training in Multi-Agent Reinforcement\n  Learning", "comments": "9 pages, 5 figures; accepted as a full paper at ICAART 2021\n  (http://www.icaart.org/)", "journal-ref": "Proceedings of the 13th International Conference on Agents and\n  Artificial Intelligence - Volume 1: ICAART, 28-37, 2021", "doi": "10.5220/0010189500280037", "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A characteristic of reinforcement learning is the ability to develop\nunforeseen strategies when solving problems. While such strategies sometimes\nyield superior performance, they may also result in undesired or even dangerous\nbehavior. In industrial scenarios, a system's behavior also needs to be\npredictable and lie within defined ranges. To enable the agents to learn (how)\nto align with a given specification, this paper proposes to explicitly transfer\nfunctional and non-functional requirements into shaped rewards. Experiments are\ncarried out on the smart factory, a multi-agent environment modeling an\nindustrial lot-size-one production facility, with up to eight agents and\ndifferent multi-agent reinforcement learning algorithms. Results indicate that\ncompliance with functional and non-functional constraints can be achieved by\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:33:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ritz", "Fabian", ""], ["Phan", "Thomy", ""], ["M\u00fcller", "Robert", ""], ["Gabor", "Thomas", ""], ["Sedlmeier", "Andreas", ""], ["Zeller", "Marc", ""], ["Wieghardt", "Jan", ""], ["Schmid", "Reiner", ""], ["Sauer", "Horst", ""], ["Klein", "Cornel", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2012.08181", "submitter": "Mohammadreza Doostmohammadian", "authors": "Mohammadreza Doostmohammadian, Alireza Aghasi, Themistoklis\n  Charalambous", "title": "Fast-Convergent Dynamics for Distributed Resource Allocation Over Sparse\n  Time-Varying Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.MA cs.SI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, distributed dynamics are deployed to solve resource allocation\nover time-varying multi-agent networks. The state of each agent represents the\namount of resources used/produced at that agent while the total amount of\nresources is fixed. The idea is to optimally allocate the resources among the\ngroup of agents by reducing the total cost functions subject to fixed amount of\ntotal resources. The information of each agent is restricted to its own state\nand cost function and those of its immediate neighbors. This is motivated by\ndistributed applications such as in mobile edge-computing, economic dispatch\nover smart grids, and multi-agent coverage control. The non-Lipschitz dynamics\nproposed in this work shows fast convergence as compared to the linear and some\nnonlinear solutions in the literature. Further, the multi-agent network\nconnectivity is more relaxed in this paper. To be more specific, the proposed\ndynamics even reaches optimal solution over time-varying disconnected\nundirected networks as far as the union of these networks over some bounded\nnon-overlapping time-intervals includes a spanning-tree. The proposed\nconvergence analysis can be applied for similar 1st-order resource allocation\nnonlinear dynamics. We provide simulations to verify our results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:57:54 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 09:08:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Doostmohammadian", "Mohammadreza", ""], ["Aghasi", "Alireza", ""], ["Charalambous", "Themistoklis", ""]]}, {"id": "2012.08255", "submitter": "Tessa van der Heiden", "authors": "T. van der Heiden, C. Salge, E. Gavves, H. van Hoof", "title": "Robust Multi-Agent Reinforcement Learning with Social Empowerment for\n  Coordination and Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of robust multi-agent reinforcement learning (MARL)\nfor cooperative communication and coordination tasks. MARL agents, mainly those\ntrained in a centralized way, can be brittle because they can adopt policies\nthat act under the expectation that other agents will act a certain way rather\nthan react to their actions. Our objective is to bias the learning process\ntowards finding strategies that remain reactive towards others' behavior.\nSocial empowerment measures the potential influence between agents' actions. We\npropose it as an additional reward term, so agents better adapt to other\nagents' actions. We show that the proposed method results in obtaining higher\nrewards faster and a higher success rate in three cooperative communication and\ncoordination tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:43:17 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["van der Heiden", "T.", ""], ["Salge", "C.", ""], ["Gavves", "E.", ""], ["van Hoof", "H.", ""]]}, {"id": "2012.08382", "submitter": "Tanner Fiez", "authors": "Stratis Skoulakis, Tanner Fiez, Ryann Sim, Georgios Piliouras, Lillian\n  Ratliff", "title": "Evolutionary Game Theory Squared: Evolving Agents in Endogenously\n  Evolving Zero-Sum Games", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant paradigm in evolutionary game theory and more generally\nonline learning in games is based on a clear distinction between a population\nof dynamic agents that interact given a fixed, static game. In this paper, we\nmove away from the artificial divide between dynamic agents and static games,\nto introduce and analyze a large class of competitive settings where both the\nagents and the games they play evolve strategically over time. We focus on\narguably the most archetypal game-theoretic setting -- zero-sum games (as well\nas network generalizations) -- and the most studied evolutionary learning\ndynamic -- replicator, the continuous-time analogue of multiplicative weights.\nPopulations of agents compete against each other in a zero-sum competition that\nitself evolves adversarially to the current population mixture. Remarkably,\ndespite the chaotic coevolution of agents and games, we prove that the system\nexhibits a number of regularities. First, the system has conservation laws of\nan information-theoretic flavor that couple the behavior of all agents and\ngames. Secondly, the system is Poincar\\'{e} recurrent, with effectively all\npossible initializations of agents and games lying on recurrent orbits that\ncome arbitrarily close to their initial conditions infinitely often. Thirdly,\nthe time-average agent behavior and utility converge to the Nash equilibrium\nvalues of the time-average game. Finally, we provide a polynomial time\nalgorithm to efficiently predict this time-average behavior for any such\ncoevolving network game.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:54:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Skoulakis", "Stratis", ""], ["Fiez", "Tanner", ""], ["Sim", "Ryann", ""], ["Piliouras", "Georgios", ""], ["Ratliff", "Lillian", ""]]}, {"id": "2012.08610", "submitter": "Pedro Cisneros-Velarde", "authors": "Pedro Cisneros-Velarde, Francesco Bullo", "title": "Distributed Wasserstein Barycenters via Displacement Interpolation", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a multi-agent system whereby each agent has an initial probability\nmeasure. In this paper, we propose a distributed algorithm based upon\nstochastic, asynchronous and pairwise exchange of information and displacement\ninterpolation in the Wasserstein space. We characterize the evolution of this\nalgorithm and prove it computes the Wasserstein barycenter of the initial\nmeasures under various conditions. One version of the algorithm computes a\nstandard Wasserstein barycenter, i.e., a barycenter based upon equal weights;\nand the other version computes a randomized Wasserstein barycenter, i.e., a\nbarycenter based upon random weights for the initial measures. Finally, we\nspecialize our results to Gaussian distributions and draw a connection with the\nmodeling of opinion dynamics in mathematical sociology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:50:28 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Cisneros-Velarde", "Pedro", ""], ["Bullo", "Francesco", ""]]}, {"id": "2012.08630", "submitter": "Kevin McKee", "authors": "Allan Dafoe and Edward Hughes and Yoram Bachrach and Tantum Collins\n  and Kevin R. McKee and Joel Z. Leibo and Kate Larson and Thore Graepel", "title": "Open Problems in Cooperative AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems of cooperation--in which agents seek ways to jointly improve their\nwelfare--are ubiquitous and important. They can be found at scales ranging from\nour daily routines--such as driving on highways, scheduling meetings, and\nworking collaboratively--to our global challenges--such as peace, commerce, and\npandemic preparedness. Arguably, the success of the human species is rooted in\nour ability to cooperate. Since machines powered by artificial intelligence are\nplaying an ever greater role in our lives, it will be important to equip them\nwith the capabilities necessary to cooperate and to foster cooperation.\n  We see an opportunity for the field of artificial intelligence to explicitly\nfocus effort on this class of problems, which we term Cooperative AI. The\nobjective of this research would be to study the many aspects of the problems\nof cooperation and to innovate in AI to contribute to solving these problems.\nCentral goals include building machine agents with the capabilities needed for\ncooperation, building tools to foster cooperation in populations of (machine\nand/or human) agents, and otherwise conducting AI research for insight relevant\nto problems of cooperation. This research integrates ongoing work on\nmulti-agent systems, game theory and social choice, human-machine interaction\nand alignment, natural-language processing, and the construction of social\ntools and platforms. However, Cooperative AI is not the union of these existing\nareas, but rather an independent bet about the productivity of specific kinds\nof conversations that involve these and other areas. We see opportunity to more\nexplicitly focus on the problem of cooperation, to construct unified theory and\nvocabulary, and to build bridges with adjacent communities working on\ncooperation, including in the natural, social, and behavioural sciences.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:39:50 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dafoe", "Allan", ""], ["Hughes", "Edward", ""], ["Bachrach", "Yoram", ""], ["Collins", "Tantum", ""], ["McKee", "Kevin R.", ""], ["Leibo", "Joel Z.", ""], ["Larson", "Kate", ""], ["Graepel", "Thore", ""]]}, {"id": "2012.08660", "submitter": "Sen Lin", "authors": "Sen Lin, Mehmet Dedeoglu and Junshan Zhang", "title": "Accelerating Distributed Online Meta-Learning via Multi-Agent\n  Collaboration under Limited Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online meta-learning is emerging as an enabling technique for achieving edge\nintelligence in the IoT ecosystem. Nevertheless, to learn a good meta-model for\nwithin-task fast adaptation, a single agent alone has to learn over many tasks,\nand this is the so-called 'cold-start' problem. Observing that in a multi-agent\nnetwork the learning tasks across different agents often share some model\nsimilarity, we ask the following fundamental question: \"Is it possible to\naccelerate the online meta-learning across agents via limited communication and\nif yes how much benefit can be achieved? \" To answer this question, we propose\na multi-agent online meta-learning framework and cast it as an equivalent\ntwo-level nested online convex optimization (OCO) problem. By characterizing\nthe upper bound of the agent-task-averaged regret, we show that the performance\nof multi-agent online meta-learning depends heavily on how much an agent can\nbenefit from the distributed network-level OCO for meta-model updates via\nlimited communication, which however is not well understood. To tackle this\nchallenge, we devise a distributed online gradient descent algorithm with\ngradient tracking where each agent tracks the global gradient using only one\ncommunication step with its neighbors per iteration, and it results in an\naverage regret $O(\\sqrt{T/N})$ per agent, indicating that a factor of\n$\\sqrt{1/N}$ speedup over the optimal single-agent regret $O(\\sqrt{T})$ after\n$T$ iterations, where $N$ is the number of agents. Building on this sharp\nperformance speedup, we next develop a multi-agent online meta-learning\nalgorithm and show that it can achieve the optimal task-average regret at a\nfaster rate of $O(1/\\sqrt{NT})$ via limited communication, compared to\nsingle-agent online meta-learning. Extensive experiments corroborate the\ntheoretic results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:08:36 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 19:26:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Lin", "Sen", ""], ["Dedeoglu", "Mehmet", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.08858", "submitter": "Shuji Shinohara Shinohara", "authors": "Shuji Shinohara, Nobuhito Manome, Yoshihiro Nakajima, Yukio Pegio\n  Gunji, Toru Moriyama, Hiroshi Okamoto, Shunji Mitsuyoshi, Ung-il Chung", "title": "L\\'evy walks derived from a Bayesian decision-making model in\n  non-stationary environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L\\'evy walks are found in the migratory behaviour patterns of various\norganisms, and the reason for this phenomenon has been much discussed. We use\nsimulations to demonstrate that learning causes the changes in confidence level\nduring decision-making in non-stationary environments, and results in\nL\\'evy-walk-like patterns. One inference algorithm involving confidence is\nBayesian inference. We propose an algorithm that introduces the effects of\nlearning and forgetting into Bayesian inference, and simulate an imitation game\nin which two decision-making agents incorporating the algorithm estimate each\nother's internal models from their opponent's observational data. For\nforgetting without learning, agent confidence levels remained low due to a lack\nof information on the counterpart and Brownian walks occurred for a wide range\nof forgetting rates. Conversely, when learning was introduced, high confidence\nlevels occasionally occurred even at high forgetting rates, and Brownian walks\nuniversally became L\\'evy walks through a mixture of high- and low-confidence\nstates.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:59:22 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Shinohara", "Shuji", ""], ["Manome", "Nobuhito", ""], ["Nakajima", "Yoshihiro", ""], ["Gunji", "Yukio Pegio", ""], ["Moriyama", "Toru", ""], ["Okamoto", "Hiroshi", ""], ["Mitsuyoshi", "Shunji", ""], ["Chung", "Ung-il", ""]]}, {"id": "2012.09052", "submitter": "Jingkai Chen", "authors": "Jingkai Chen, Jiaoyang Li, Chuchu Fan, Brian Williams", "title": "Scalable and Safe Multi-Agent Motion Planning with Nonlinear Dynamics\n  and Bounded Disturbances", "comments": "Accepted at AAAI2021. 9 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable and effective multi-agent safe motion planner that\nenables a group of agents to move to their desired locations while avoiding\ncollisions with obstacles and other agents, with the presence of rich\nobstacles, high-dimensional, nonlinear, nonholonomic dynamics, actuation\nlimits, and disturbances. We address this problem by finding a piecewise linear\npath for each agent such that the actual trajectories following these paths are\nguaranteed to satisfy the reach-and-avoid requirement. We show that the spatial\ntracking error of the actual trajectories of the controlled agents can be\npre-computed for any qualified path that considers the minimum duration of each\npath segment due to actuation limits. Using these bounds, we find a\ncollision-free path for each agent by solving Mixed Integer-Linear Programs and\ncoordinate agents by using the priority-based search. We demonstrate our method\nby benchmarking in 2D and 3D scenarios with ground vehicles and quadrotors,\nrespectively, and show improvements over the solving time and the solution\nquality compared to two state-of-the-art multi-agent motion planners.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:18:02 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Chen", "Jingkai", ""], ["Li", "Jiaoyang", ""], ["Fan", "Chuchu", ""], ["Williams", "Brian", ""]]}, {"id": "2012.09134", "submitter": "Hongda Qiu", "authors": "Hongda Qiu", "title": "Multi-agent navigation based on deep reinforcement learning and\n  traditional pathfinding algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new framework for multi-agent collision avoidance problem. The\nframework combined traditional pathfinding algorithm and reinforcement\nlearning. In our approach, the agents learn whether to be navigated or to take\nsimple actions to avoid their partners via a deep neural network trained by\nreinforcement learning at each time step. This framework makes it possible for\nagents to arrive terminal points in abstract new scenarios. In our experiments,\nwe use Unity3D and Tensorflow to build the model and environment for our\nscenarios. We analyze the results and modify the parameters to approach a\nwell-behaved strategy for our agents. Our strategy could be attached in\ndifferent environments under different cases, especially when the scale is\nlarge.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 08:56:58 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Qiu", "Hongda", ""]]}, {"id": "2012.09135", "submitter": "Tarik A. Rashid", "authors": "Danial A. Muhammed, Tarik A. Rashid, Abeer Alsadoon, Nebojsa Bacanin,\n  Polla Fattah, Mokhtar Mohammadi and Indradip Banerjee", "title": "An Improved Simulation Model for Pedestrian Crowd Evacuation", "comments": "15 pages, accepted in Mathematics, MDPI, 2020", "journal-ref": null, "doi": "10.3390/math8122171", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper works on one of the most recent pedestrian crowd evacuation\nmodels, i.e., \"a simulation model for pedestrian crowd evacuation based on\nvarious AI techniques\", developed in late 2019. This study adds a new feature\nto the developed model by proposing a new method and integrating it with the\nmodel. This method enables the developed model to find a more appropriate\nevacuation area design, among others regarding safety due to selecting the best\nexit door location among many suggested locations. This method is completely\ndependent on the selected model's output, i.e., the evacuation time for each\nindividual within the evacuation process. The new method finds an average of\nthe evacuees' evacuation times of each exit door location; then, based on the\naverage evacuation time, it decides which exit door location would be the best\nexit door to be used for evacuation by the evacuees. To validate the method,\nvarious designs for the evacuation area with various written scenarios were\nused. The results showed that the model with this new method could predict a\nproper exit door location among many suggested locations. Lastly, from the\nresults of this research using the integration of this newly proposed method, a\nnew capability for the selected model in terms of safety allowed the right\ndecision in selecting the finest design for the evacuation area among other\ndesigns.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:25:03 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Muhammed", "Danial A.", ""], ["Rashid", "Tarik A.", ""], ["Alsadoon", "Abeer", ""], ["Bacanin", "Nebojsa", ""], ["Fattah", "Polla", ""], ["Mohammadi", "Mokhtar", ""], ["Banerjee", "Indradip", ""]]}, {"id": "2012.09136", "submitter": "Griffin Adams", "authors": "Griffin Adams, Sarguna Janani Padmanabhan, Shivang Shekhar", "title": "Resolving Implicit Coordination in Multi-Agent Deep Reinforcement\n  Learning with Deep Q-Networks & Game Theory", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address two major challenges of implicit coordination in multi-agent deep\nreinforcement learning: non-stationarity and exponential growth of state-action\nspace, by combining Deep-Q Networks for policy learning with Nash equilibrium\nfor action selection. Q-values proxy as payoffs in Nash settings, and mutual\nbest responses define joint action selection. Coordination is implicit because\nmultiple/no Nash equilibria are resolved deterministically. We demonstrate that\nknowledge of game type leads to an assumption of mirrored best responses and\nfaster convergence than Nash-Q. Specifically, the Friend-or-Foe algorithm\ndemonstrates signs of convergence to a Set Controller which jointly chooses\nactions for two agents. This encouraging given the highly unstable nature of\ndecentralized coordination over joint actions. Inspired by the dueling network\narchitecture, which decouples the Q-function into state and advantage streams,\nas well as residual networks, we learn both a single and joint agent\nrepresentation, and merge them via element-wise addition. This simplifies\ncoordination by recasting it is as learning a residual function. We also draw\nhigh level comparative insights on key MADRL and game theoretic variables:\ncompetitive vs. cooperative, asynchronous vs. parallel learning, greedy versus\nsocially optimal Nash equilibria tie breaking, and strategies for the no Nash\nequilibrium case. We evaluate on 3 custom environments written in Python using\nOpenAI Gym: a Predator Prey environment, an alternating Warehouse environment,\nand a Synchronization environment. Each environment requires successively more\ncoordination to achieve positive rewards.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:30:47 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Adams", "Griffin", ""], ["Padmanabhan", "Sarguna Janani", ""], ["Shekhar", "Shivang", ""]]}, {"id": "2012.09147", "submitter": "Andrew Estornell", "authors": "Andrew Estornell, Sanmay Das, Yevgeniy Vorobeychik", "title": "Incentivizing Truthfulness Through Audits in Strategic Classification", "comments": "To be published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many societal resource allocation domains, machine learning methods are\nincreasingly used to either score or rank agents in order to decide which ones\nshould receive either resources (e.g., homeless services) or scrutiny (e.g.,\nchild welfare investigations) from social services agencies. An agency's\nscoring function typically operates on a feature vector that contains a\ncombination of self-reported features and information available to the agency\nabout individuals or households.This can create incentives for agents to\nmisrepresent their self-reported features in order to receive resources or\navoid scrutiny, but agencies may be able to selectively audit agents to verify\nthe veracity of their reports.\n  We study the problem of optimal auditing of agents in such settings. When\ndecisions are made using a threshold on an agent's score, the optimal audit\npolicy has a surprisingly simple structure, uniformly auditing all agents who\ncould benefit from lying. While this policy can, in general, be hard to compute\nbecause of the difficulty of identifying the set of agents who could benefit\nfrom lying given a complete set of reported types, we also present necessary\nand sufficient conditions under which it is tractable. We show that the scarce\nresource setting is more difficult, and exhibit an approximately optimal audit\npolicy in this case. In addition, we show that in either setting verifying\nwhether it is possible to incentivize exact truthfulness is hard even to\napproximate. However, we also exhibit sufficient conditions for solving this\nproblem optimally, and for obtaining good approximations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:35:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Estornell", "Andrew", ""], ["Das", "Sanmay", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2012.09331", "submitter": "Brian Reily", "authors": "Brian Reily, Hao Zhang", "title": "Team Assignment for Heterogeneous Multi-Robot Sensor Coverage through\n  Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor coverage is the critical multi-robot problem of maximizing the\ndetection of events in an environment through the deployment of multiple\nrobots. Large multi-robot systems are often composed of simple robots that are\ntypically not equipped with a complete set of sensors, so teams with\ncomprehensive sensing abilities are required to properly cover an area. Robots\nalso exhibit multiple forms of relationships (e.g., communication connections\nor spatial distribution) that need to be considered when assigning robot teams\nfor sensor coverage. To address this problem, in this paper we introduce a\nnovel formulation of sensor coverage by multi-robot systems with heterogeneous\nrelationships as a graph representation learning problem. We propose a\nprincipled approach based on the mathematical framework of regularized\noptimization to learn a unified representation of the multi-robot system from\nthe graphs describing the heterogeneous relationships and to identify the\nlearned representation's underlying structure in order to assign the robots to\nteams. To evaluate the proposed approach, we conduct extensive experiments on\nsimulated multi-robot systems and a physical multi-robot system as a case\nstudy, demonstrating that our approach is able to effectively assign teams for\nheterogeneous multi-robot sensor coverage.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:12:58 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:51:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Reily", "Brian", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.09334", "submitter": "Brian Reily", "authors": "Brian Reily, Terran Mott, Hao Zhang", "title": "Adaptation to Team Composition Changes for Heterogeneous Multi-Robot\n  Sensor Coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-robot sensor coverage, which deals with\ndeploying a multi-robot team in an environment and optimizing the sensing\nquality of the overall environment. As real-world environments involve a\nvariety of sensory information, and individual robots are limited in their\navailable number of sensors, successful multi-robot sensor coverage requires\nthe deployment of robots in such a way that each individual team member's\nsensing quality is maximized. Additionally, because individual robots have\nvarying complements of sensors and both robots and sensors can fail, robots\nmust be able to adapt and adjust how they value each sensing capability in\norder to obtain the most complete view of the environment, even through changes\nin team composition. We introduce a novel formulation for sensor coverage by\nmulti-robot teams with heterogeneous sensing capabilities that maximizes each\nrobot's sensing quality, balancing the varying sensing capabilities of\nindividual robots based on the overall team composition. We propose a solution\nbased on regularized optimization that uses sparsity-inducing terms to ensure a\nrobot team focuses on all possible event types, and which we show is proven to\nconverge to the optimal solution. Through extensive simulation, we show that\nour approach is able to effectively deploy a multi-robot team to maximize the\nsensing quality of an environment, responding to failures in the multi-robot\nteam more robustly than non-adaptive approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:22:18 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:44:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Reily", "Brian", ""], ["Mott", "Terran", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.09342", "submitter": "Nethra Viswanathan", "authors": "Nethra Viswanathan", "title": "Adaptive Multi-Agent E-Learning Recommender Systems", "comments": "4 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Educational recommender systems have become a necessity in the recent years\ndue to overload of available educational resource which makes it difficult for\nan individual to manually hunt for the required resource on the internet.\nE-learning recommender systems simplify the tedious task of gathering the right\nweb pages and web documents from the scattered world wide web repositories\naccording to every users' requirements thus increasing the demand and hence the\ncuriosity to study them. Retrieval of a handful of recommendations from a very\nhuge collection of web pages using different recommendation techniques becomes\na productive and time efficient process when the system functions with a set of\ncooperative agents. The system is also required to keep up with the changing\nuser interests and web resources in the dynamic web environment, and hence\nadaptivity is an important factor in determining the efficiency of recommender\nsystems. The paper provides an overview of such adaptive multi-agent e-learning\nrecommender systems and the concepts employed to implement them. It precisely\nprovides all the information required by a researcher who wants to study the\nstate-of-the-art work on such systems thus enabling him to decide on the\nimplementation concepts for his own system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 01:02:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Viswanathan", "Nethra", ""]]}, {"id": "2012.09421", "submitter": "Matthieu Zimmer", "authors": "Matthieu Zimmer, Claire Glanois, Umer Siddique, Paul Weng", "title": "Learning Fair Policies in Decentralized Cooperative Multi-Agent\n  Reinforcement Learning", "comments": "International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning fair policies in (deep) cooperative\nmulti-agent reinforcement learning (MARL). We formalize it in a principled way\nas the problem of optimizing a welfare function that explicitly encodes two\nimportant aspects of fairness: efficiency and equity. As a solution method, we\npropose a novel neural network architecture, which is composed of two\nsub-networks specifically designed for taking into account the two aspects of\nfairness. In experiments, we demonstrate the importance of the two sub-networks\nfor fair optimization. Our overall approach is general as it can accommodate\nany (sub)differentiable welfare function. Therefore, it is compatible with\nvarious notions of fairness that have been proposed in the literature (e.g.,\nlexicographic maximin, generalized Gini social welfare function, proportional\nfairness). Our solution method is generic and can be implemented in various\nMARL settings: centralized training and decentralized execution, or fully\ndecentralized. Finally, we experimentally validate our approach in various\ndomains and show that it can perform much better than previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 07:17:36 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 05:32:23 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 06:40:20 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 07:20:24 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zimmer", "Matthieu", ""], ["Glanois", "Claire", ""], ["Siddique", "Umer", ""], ["Weng", "Paul", ""]]}, {"id": "2012.09759", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate", "title": "Game-theoretic Models of Moral and Other-Regarding Agents", "comments": null, "journal-ref": "Proceedings of TARK 2021, the 18th Conference of Theoretical\n  Aspects of Rationality and Knowledge", "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate Kantian equilibria in finite normal form games, a class of\nnon-Nashian, morally motivated courses of action that was recently proposed in\nthe economics literature. We highlight a number of problems with such\nequilibria, including computational intractability, a high price of\nmiscoordination, and expensive/problematic extension to general normal form\ngames. We point out that such a proper generalization will likely involve the\nconcept of program equilibrium. Finally we propose some general, intuitive,\ncomputationally tractable, other-regarding equilibria related to Kantian\nequilibria, as well as a class of courses of action that interpolates between\npurely self-regarding and Kantian behavior.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:16:50 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 20:40:50 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Istrate", "Gabriel", ""]]}, {"id": "2012.10146", "submitter": "Conor McMenamin", "authors": "Conor McMenamin and Vanesa Daza and Matteo Pontecorvi", "title": "Achieving State Machine Replication without Honest Players", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing standards for player characterisation in tokenised state machine\nreplication protocols depend on honest players who will always follow the\nprotocol, regardless of possible token increases for deviating. Given the\never-increasing market capitalisation of these tokenised protocols, honesty is\nbecoming more expensive and more unrealistic. As such, this out-dated player\ncharacterisation must be removed to provide true guarantees of safety and\nliveness in a major stride towards universal trust in state machine replication\nprotocols and a new scale of adoption. As all current state machine replication\nprotocols are built on these legacy standards, it is imperative that a new\nplayer model is identified and utilised to reflect the true nature of players\nin tokenised protocols, now and into the future.\n  To this effect, we propose the ByRa player model for state machine\nreplication protocols. In the ByRa model, players either attempt to maximise\ntheir tokenised rewards, or behave adversarially. This merges the fields of\ngame theory and distributed systems, an intersection in which tokenised state\nmachine replication protocols exist, but on which little formalisation has been\ncarried out. In the ByRa model, we identify the properties of strong incentive\ncompatibility in expectation and fairness that all protocols must satisfy in\norder to achieve state machine replication. We then provide Tenderstake, a\nprotocol which provably satisfies these properties, and by doing so, achieves\nstate machine replication in the ByRa model.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:13:35 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:35:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["McMenamin", "Conor", ""], ["Daza", "Vanesa", ""], ["Pontecorvi", "Matteo", ""]]}, {"id": "2012.10153", "submitter": "Usama Mehmood", "authors": "Usama Mehmood, Scott D. Stoller, Radu Grosu, Shouvik Roy, Amol Damare,\n  Scott A. Smolka", "title": "A Distributed Simplex Architecture for Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Distributed Simplex Architecture (DSA), a new runtime assurance\ntechnique that provides safety guarantees for multi-agent systems (MASs). DSA\nis inspired by the Simplex control architecture of Sha et al., but with some\nsignificant differences. The traditional Simplex approach is limited to\nsingle-agent systems or a MAS with a centralized control scheme. DSA addresses\nthis limitation by extending the scope of Simplex to include MASs under\ndistributed control. In DSA, each agent has a local instance of traditional\nSimplex such that the preservation of safety in the local instances implies\nsafety for the entire MAS. We provide a proof of safety for DSA, and present\nexperimental results for several case studies, including flocking with\ncollision avoidance, safe navigation of ground rovers through way-points, and\nthe safe operation of a microgrid.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:31:50 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Mehmood", "Usama", ""], ["Stoller", "Scott D.", ""], ["Grosu", "Radu", ""], ["Roy", "Shouvik", ""], ["Damare", "Amol", ""], ["Smolka", "Scott A.", ""]]}, {"id": "2012.10389", "submitter": "Harshavardhan Kamarthi", "authors": "Aravind Venugopal, Elizabeth Bondi, Harshavardhan Kamarthi, Keval\n  Dholakia, Balaraman Ravindran, Milind Tambe", "title": "Reinforcement Learning for Unified Allocation and Patrolling in\n  Signaling Games with Uncertainty", "comments": "Accepted at AAMAS 2021", "journal-ref": null, "doi": "10.5555/3463952", "report-no": "page 1353-1361", "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Green Security Games (GSGs) have been successfully used in the protection of\nvaluable resources such as fisheries, forests and wildlife. While real-world\ndeployment involves both resource allocation and subsequent coordinated\npatrolling with communication and real-time, uncertain information, previous\ngame models do not fully address both of these stages simultaneously.\nFurthermore, adopting existing solution strategies is difficult since they do\nnot scale well for larger, more complex variants of the game models.\n  We therefore first propose a novel GSG model that combines defender\nallocation, patrolling, real-time drone notification to human patrollers, and\ndrones sending warning signals to attackers. The model further incorporates\nuncertainty for real-time decision-making within a team of drones and human\npatrollers. Second, we present CombSGPO, a novel and scalable algorithm based\non reinforcement learning, to compute a defender strategy for this game model.\nCombSGPO performs policy search over a multi-dimensional, discrete action space\nto compute an allocation strategy that is best suited to a best-response\npatrolling strategy for the defender, learnt by training a multi-agent Deep\nQ-Network. We show via experiments that CombSGPO converges to better strategies\nand is more scalable than comparable approaches. Third, we provide a detailed\nanalysis of the coordination and signaling behavior learnt by CombSGPO, showing\ngroup formation between defender resources and patrolling formations based on\nsignaling and notifications between resources. Importantly, we find that\nstrategic signaling emerges in the final learnt strategy. Finally, we perform\nexperiments to evaluate these strategies under different levels of uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:53:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Venugopal", "Aravind", ""], ["Bondi", "Elizabeth", ""], ["Kamarthi", "Harshavardhan", ""], ["Dholakia", "Keval", ""], ["Ravindran", "Balaraman", ""], ["Tambe", "Milind", ""]]}, {"id": "2012.10480", "submitter": "Guangyi Liu", "authors": "Guangyi Liu, Arash Amini, Martin Tak\\'a\\v{c}, H\\'ector Mu\\~noz-Avila,\n  and Nader Motee", "title": "Distributed Map Classification using Local Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of classifying a map using a team of communicating\nrobots. It is assumed that all robots have localized visual sensing\ncapabilities and can exchange their information with neighboring robots. Using\na graph decomposition technique, we proposed an offline learning structure that\nmakes every robot capable of communicating with and fusing information from its\nneighbors to plan its next move towards the most informative parts of the\nenvironment for map classification purposes. The main idea is to decompose a\ngiven undirected graph into a union of directed star graphs and train robots\nw.r.t a bounded number of star graphs. This will significantly reduce the\ncomputational cost of offline training and makes learning scalable (independent\nof the number of robots). Our approach is particularly useful for fast map\nclassification in large environments using a large number of communicating\nrobots. We validate the usefulness of our proposed methodology through\nextensive simulations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 19:35:10 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 18:00:45 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Liu", "Guangyi", ""], ["Amini", "Arash", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Mu\u00f1oz-Avila", "H\u00e9ctor", ""], ["Motee", "Nader", ""]]}, {"id": "2012.11047", "submitter": "Renming Liu", "authors": "Renming Liu, Yu Jiang, Carlos Lima Azevedo (DTU Management, Technical\n  University of Denmark, Denmark)", "title": "Bayesian Optimization of Area-based Road Pricing", "comments": "6 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a Bayesian Optimization framework for area- and\ndistance-based time-of-day pricing (TODP) for urban networks. The road pricing\noptimization problem can reach high level of complexity depending on the\npricing scheme considered, its associated detailed network properties and the\naffected heterogeneous demand features. We consider heterogeneous travellers\nwith individual-specific trip attributes and departure-time choice parameters\ntogether with a Macroscopic Fundamental Diagram (MFD) model for the urban\nnetwork. Its mathematical formulation is presented and an agent-based\nsimulation framework is constructed as evaluation function for the TODP\noptimization problem. The latter becomes highly nonlinear and relying on an\nexpensive-to-evaluate objective function. We then present and test a Bayesian\nOptimization approach to compute different time-of-day pricing schemes by\nmaximizing social welfare. Our proposed method learns the relationship between\nthe prices and welfare within a few iterations and is able to find good\nsolutions even in scenarios with high dimensionality in the decision variables\nspace, setting a path for complexity reduction in more realistic road pricing\noptimization problems. Furthermore and as expected, the simulation results show\nthat TODP improves the social welfare against the no-pricing case.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 23:13:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Liu", "Renming", "", "DTU Management, Technical\n  University of Denmark, Denmark"], ["Jiang", "Yu", "", "DTU Management, Technical\n  University of Denmark, Denmark"], ["Azevedo", "Carlos Lima", "", "DTU Management, Technical\n  University of Denmark, Denmark"]]}, {"id": "2012.11258", "submitter": "Jacopo Castellini", "authors": "Jacopo Castellini, Sam Devlin, Frans A. Oliehoek, Rahul Savani", "title": "Difference Rewards Policy Gradients", "comments": "This work as been accepted as an Extended Abstract in Proc. of the\n  20th International Conference on Autonomous Agents and Multiagent Systems\n  (AAMAS 2021), U. Endriss, A. Now\\'e, F. Dignum, A. Lomuscio (eds.), May 3-7\n  2021, Online", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy gradient methods have become one of the most popular classes of\nalgorithms for multi-agent reinforcement learning. A key challenge, however,\nthat is not addressed by many of these methods is multi-agent credit\nassignment: assessing an agent's contribution to the overall performance, which\nis crucial for learning good policies. We propose a novel algorithm called\nDr.Reinforce that explicitly tackles this by combining difference rewards with\npolicy gradients to allow for learning decentralized policies when the reward\nfunction is known. By differencing the reward function directly, Dr.Reinforce\navoids difficulties associated with learning the Q-function as done by\nCounterfactual Multiagent Policy Gradients (COMA), a state-of-the-art\ndifference rewards method. For applications where the reward function is\nunknown, we show the effectiveness of a version of Dr.Reinforce that learns an\nadditional reward network that is used to estimate the difference rewards.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 11:23:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Castellini", "Jacopo", ""], ["Devlin", "Sam", ""], ["Oliehoek", "Frans A.", ""], ["Savani", "Rahul", ""]]}, {"id": "2012.11444", "submitter": "David Mark Bossens", "authors": "David M. Bossens and Danesh Tarapore", "title": "Rapidly adapting robot swarms with Swarm Map-based Bayesian Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid performance recovery from unforeseen environmental perturbations\nremains a grand challenge in swarm robotics. To solve this challenge, we\ninvestigate a behaviour adaptation approach, where one searches an archive of\ncontrollers for potential recovery solutions. To apply behaviour adaptation in\nswarm robotic systems, we propose two algorithms: (i) Swarm Map-based\nOptimisation (SMBO), which selects and evaluates one controller at a time, for\na homogeneous swarm, in a centralised fashion; and (ii) Swarm Map-based\nOptimisation Decentralised (SMBO-Dec), which performs an asynchronous\nbatch-based Bayesian optimisation to simultaneously explore different\ncontrollers for groups of robots in the swarm. We set up foraging experiments\nwith a variety of disturbances: injected faults to proximity sensors, ground\nsensors, and the actuators of individual robots, with 100 unique combinations\nfor each type. We also investigate disturbances in the operating environment of\nthe swarm, where the swarm has to adapt to drastic changes in the number of\nresources available in the environment, and to one of the robots behaving\ndisruptively towards the rest of the swarm, with 30 unique conditions for each\nsuch perturbation. The viability of SMBO and SMBO-Dec is demonstrated,\ncomparing favourably to variants of random search and gradient descent, and\nvarious ablations, and improving performance up to 80% compared to the\nperformance at the time of fault injection within at most 30 evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:54:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bossens", "David M.", ""], ["Tarapore", "Danesh", ""]]}, {"id": "2012.11527", "submitter": "Marion G\\\"odel", "authors": "Marion G\\\"odel and Luca Spataro and Gerta K\\\"oster", "title": "Can we learn where people come from? Retracing of origins in merging\n  situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One crucial information for a pedestrian crowd simulation is the number of\nagents moving from an origin to a certain target. While this setup has a large\nimpact on the simulation, it is in most setups challenging to find the number\nof agents that should be spawned at a source in the simulation. Often, number\nare chosen based on surveys and experience of modelers and event organizers.\nThese approaches are important and useful but reach their limits when we want\nto perform real-time predictions. In this case, a static information about the\ninflow is not sufficient. Instead, we need a dynamic information that can be\nretrieved each time the prediction is started. Nowadays, sensor data such as\nvideo footage or GPS tracks of a crowd are often available. If we can estimate\nthe number of pedestrians who stem from a certain origin from this sensor data,\nwe can dynamically initialize the simulation. In this study, we use density\nheatmaps that can be derived from sensor data as input for a random forest\nregressor to predict the origin distributions. We study three different\ndatasets: A simulated dataset, experimental data, and a hybrid approach with\nboth experimental and simulated data. In the hybrid setup, the model is trained\nwith simulated data and then tested on experimental data. The results\ndemonstrate that the random forest model is able to predict the origin\ndistribution based on a single density heatmap for all three configurations.\nThis is especially promising for applying the approach on real data since there\nis often only a limited amount of data available.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:42:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["G\u00f6del", "Marion", ""], ["Spataro", "Luca", ""], ["K\u00f6ster", "Gerta", ""]]}, {"id": "2012.11579", "submitter": "Yu-Guan Hsieh", "authors": "Yu-Guan Hsieh, Franck Iutzeler, J\\'er\\^ome Malick, Panayotis\n  Mertikopoulos", "title": "Multi-Agent Online Optimization with Delays: Asynchronicity, Adaptivity,\n  and Optimism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has been successfully applied to many problems in which data\nare revealed over time. In this paper, we provide a general framework for\nstudying multi-agent online learning problems in the presence of delays and\nasynchronicities. Specifically, we propose and analyze a class of adaptive dual\naveraging schemes in which agents only need to accumulate gradient feedback\nreceived from the whole system, without requiring any between-agent\ncoordination. In the single-agent case, the adaptivity of the proposed method\nallows us to extend a range of existing results to problems with potentially\nunbounded delays between playing an action and receiving the corresponding\nfeedback. In the multi-agent case, the situation is significantly more\ncomplicated because agents may not have access to a global clock to use as a\nreference point; to overcome this, we focus on the information that is\navailable for producing each prediction rather than the actual delay associated\nwith each feedback. This allows us to derive adaptive learning strategies with\noptimal regret bounds, at both the agent and network levels. Finally, we also\nanalyze an \"optimistic\" variant of the proposed algorithm which is capable of\nexploiting the predictability of problems with a slower variation and leads to\nimproved regret bounds.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:55:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hsieh", "Yu-Guan", ""], ["Iutzeler", "Franck", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "2012.11783", "submitter": "Wei Cui", "authors": "Wei Cui and Wei Yu", "title": "Scalable Deep Reinforcement Learning for Routing and Spectrum Access in\n  Physical Layer", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel and scalable reinforcement learning approach for\nsimultaneous routing and spectrum access in wireless ad-hoc networks. In most\nprevious works on reinforcement learning for network optimization, routing and\nspectrum access are tackled as separate tasks; further, the wireless links in\nthe network are assumed to be fixed, and a different agent is trained for each\ntransmission node -- this limits scalability and generalizability. In this\npaper, we account for the inherent signal-to-interference-plus-noise ratio\n(SINR) in the physical layer and propose a more scalable approach in which a\nsingle agent is associated with each flow. Specifically, a single agent makes\nall routing and spectrum access decisions as it moves along the frontier nodes\nof each flow. The agent is trained according to the physical layer\ncharacteristics of the environment using the future bottleneck SINR as a novel\nreward definition. This allows a highly effective routing strategy based on the\ngeographic locations of the nodes in the wireless ad-hoc network. The proposed\ndeep reinforcement learning strategy is capable of accounting for the mutual\ninterference between the links. It learns to avoid interference by\nintelligently allocating spectrum slots and making routing decisions for the\nentire network in a scalable manner.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:47:20 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cui", "Wei", ""], ["Yu", "Wei", ""]]}, {"id": "2012.11903", "submitter": "Rijk Mercuur", "authors": "Rijk Mercuur, Virginia Dignum, Catholijn M. Jonker", "title": "Modelling Human Routines: Conceptualising Social Practice Theory for\n  Agent-Based Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our routines play an important role in a wide range of social challenges such\nas climate change, disease outbreaks and coordinating staff and patients in a\nhospital. To use agent-based simulations (ABS) to understand the role of\nroutines in social challenges we need an agent framework that integrates\nroutines. This paper provides the domain-independent Social Practice Agent\n(SoPrA) framework that satisfies requirements from the literature to simulate\nour routines. By choosing the appropriate concepts from the literature on agent\ntheory, social psychology and social practice theory we ensure SoPrA correctly\ndepicts current evidence on routines. By creating a consistent, modular and\nparsimonious framework suitable for multiple domains we enhance the usability\nof SoPrA. SoPrA provides ABS researchers with a conceptual, formal and\ncomputational framework to simulate routines and gain new insights into social\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:06:47 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mercuur", "Rijk", ""], ["Dignum", "Virginia", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "2012.12062", "submitter": "Pascal Leroy", "authors": "Pascal Leroy, Damien Ernst, Pierre Geurts, Gilles Louppe, Jonathan\n  Pisane, Matthia Sabatelli", "title": "QVMix and QVMix-Max: Extending the Deep Quality-Value Family of\n  Algorithms to Cooperative Multi-Agent Reinforcement Learning", "comments": "To be published in AAAI-21 Workshop on Reinforcement Learning in\n  Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces four new algorithms that can be used for tackling\nmulti-agent reinforcement learning (MARL) problems occurring in cooperative\nsettings. All algorithms are based on the Deep Quality-Value (DQV) family of\nalgorithms, a set of techniques that have proven to be successful when dealing\nwith single-agent reinforcement learning problems (SARL). The key idea of DQV\nalgorithms is to jointly learn an approximation of the state-value function\n$V$, alongside an approximation of the state-action value function $Q$. We\nfollow this principle and generalise these algorithms by introducing two fully\ndecentralised MARL algorithms (IQV and IQV-Max) and two algorithms that are\nbased on the centralised training with decentralised execution training\nparadigm (QVMix and QVMix-Max). We compare our algorithms with state-of-the-art\nMARL techniques on the popular StarCraft Multi-Agent Challenge (SMAC)\nenvironment. We show competitive results when QVMix and QVMix-Max are compared\nto well-known MARL techniques such as QMIX and MAVEN and show that QVMix can\neven outperform them on some of the tested environments, being the algorithm\nwhich performs best overall. We hypothesise that this is due to the fact that\nQVMix suffers less from the overestimation bias of the $Q$ function.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:53:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Leroy", "Pascal", ""], ["Ernst", "Damien", ""], ["Geurts", "Pierre", ""], ["Louppe", "Gilles", ""], ["Pisane", "Jonathan", ""], ["Sabatelli", "Matthia", ""]]}, {"id": "2012.12383", "submitter": "Hang Wang", "authors": "Hang Wang, Sen Lin, Hamid Jafarkhani, Junshan Zhang", "title": "Distributed Q-Learning with State Tracking for Multi-agent Networked\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed Q-learning for Linear Quadratic Regulator\n(LQR) in a multi-agent network. The existing results often assume that agents\ncan observe the global system state, which may be infeasible in large-scale\nsystems due to privacy concerns or communication constraints. In this work, we\nconsider a setting with unknown system models and no centralized coordinator.\nWe devise a state tracking (ST) based Q-learning algorithm to design optimal\ncontrollers for agents. Specifically, we assume that agents maintain local\nestimates of the global state based on their local information and\ncommunications with neighbors. At each step, every agent updates its local\nglobal state estimation, based on which it solves an approximate Q-factor\nlocally through policy iteration. Assuming decaying injected excitation noise\nduring the policy evaluation, we prove that the local estimation converges to\nthe true global state, and establish the convergence of the proposed\ndistributed ST-based Q-learning algorithm. The experimental studies corroborate\nour theoretical results by showing that our proposed method achieves comparable\nperformance with the centralized case.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:03:49 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wang", "Hang", ""], ["Lin", "Sen", ""], ["Jafarkhani", "Hamid", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.12586", "submitter": "Oscar Guerrero-Rosado", "authors": "Oscar Guerrero-Rosado and Paul Verschure", "title": "Distributed Adaptive Control: An ideal Cognitive Architecture candidate\n  for managing a robotic recycling plant", "comments": "12 pages, 2 figures, Living Machines conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.NE cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade, society has experienced notable growth in a variety of\ntechnological areas. However, the Fourth Industrial Revolution has not been\nembraced yet. Industry 4.0 imposes several challenges which include the\nnecessity of new architectural models to tackle the uncertainty that open\nenvironments represent to cyber-physical systems (CPS). Waste Electrical and\nElectronic Equipment (WEEE) recycling plants stand for one of such open\nenvironments. Here, CPSs must work harmoniously in a changing environment,\ninteracting with similar and not so similar CPSs, and adaptively collaborating\nwith human workers. In this paper, we support the Distributed Adaptive Control\n(DAC) theory as a suitable Cognitive Architecture for managing a recycling\nplant. Specifically, a recursive implementation of DAC (between both\nsingle-agent and large-scale levels) is proposed to meet the expected demands\nof the European Project HR-Recycler. Additionally, with the aim of having a\nrealistic benchmark for future implementations of the recursive DAC, a\nmicro-recycling plant prototype is presented.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:33:22 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Guerrero-Rosado", "Oscar", ""], ["Verschure", "Paul", ""]]}, {"id": "2012.12839", "submitter": "Alok Talekar", "authors": "Alok Talekar, Sharad Shriram, Nidhin Vaidhiyan, Gaurav Aggarwal,\n  Jiangzhuo Chen, Srini Venkatramanan, Lijing Wang, Aniruddha Adiga, Adam\n  Sadilek, Ashish Tendulkar, Madhav Marathe, Rajesh Sundaresan and Milind Tambe", "title": "Cohorting to isolate asymptomatic spreaders: An agent-based simulation\n  study on the Mumbai Suburban Railway", "comments": "Will be presented at AAMAS 2021. Minor edits to styling (per conf\n  guidelines) and acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Mumbai Suburban Railways, \\emph{locals}, are a key transit infrastructure\nof the city and is crucial for resuming normal economic activity. To reduce\ndisease transmission, policymakers can enforce reduced crowding and mandate\nwearing of masks. \\emph{Cohorting} -- forming groups of travelers that always\ntravel together, is an additional policy to reduce disease transmission on\n\\textit{locals} without severe restrictions. Cohorting allows us to: ($i$) form\ntraveler bubbles, thereby decreasing the number of distinct interactions over\ntime; ($ii$) potentially quarantine an entire cohort if a single case is\ndetected, making contact tracing more efficient, and ($iii$) target cohorts for\ntesting and early detection of symptomatic as well as asymptomatic cases.\nStudying impact of cohorts using compartmental models is challenging because of\nthe ensuing representational complexity. Agent-based models provide a natural\nway to represent cohorts along with the representation of the cohort members\nwith the larger social network. This paper describes a novel multi-scale\nagent-based model to study the impact of cohorting strategies on COVID-19\ndynamics in Mumbai. We achieve this by modeling the Mumbai urban region using a\ndetailed agent-based model comprising of 12.4 million agents. Individual\ncohorts and their inter-cohort interactions as they travel on locals are\nmodeled using local mean field approximations. The resulting multi-scale model\nin conjunction with a detailed disease transmission and intervention simulator\nis used to assess various cohorting strategies. The results provide a\nquantitative trade-off between cohort size and its impact on disease dynamics\nand well being. The results show that cohorts can provide significant benefit\nin terms of reduced transmission without significantly impacting ridership and\nor economic \\& social activity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:06:22 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 08:20:31 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Talekar", "Alok", ""], ["Shriram", "Sharad", ""], ["Vaidhiyan", "Nidhin", ""], ["Aggarwal", "Gaurav", ""], ["Chen", "Jiangzhuo", ""], ["Venkatramanan", "Srini", ""], ["Wang", "Lijing", ""], ["Adiga", "Aniruddha", ""], ["Sadilek", "Adam", ""], ["Tendulkar", "Ashish", ""], ["Marathe", "Madhav", ""], ["Sundaresan", "Rajesh", ""], ["Tambe", "Milind", ""]]}, {"id": "2012.12982", "submitter": "Gaia Belardinelli", "authors": "Gaia Belardinelli, Rasmus K. Rendsvig", "title": "Awareness Logic: A Kripke-based Rendition of the Heifetz-Meier-Schipper\n  Model", "comments": "18 pages, 2 figures, proceedings of DaLi conference 2020", "journal-ref": "Martins M.A., Sedl\\'ar I. (eds) Dynamic Logic. New Trends and\n  Applications. DaLi 2020. Lecture Notes in Computer Science, vol 12569, pp\n  33-50. Springer, Cham", "doi": "10.1007/978-3-030-65840-3_3", "report-no": null, "categories": "cs.AI cs.LO cs.MA econ.TH math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heifetz, Meier and Schipper (HMS) present a lattice model of awareness. The\nHMS model is syntax-free, which precludes the simple option to rely on formal\nlanguage to induce lattices, and represents uncertainty and unawareness with\none entangled construct, making it difficult to assess the properties of\neither. Here, we present a model based on a lattice of Kripke models, induced\nby atom subset inclusion, in which uncertainty and unawareness are separate. We\nshow the models to be equivalent by defining transformations between them which\npreserve formula satisfaction, and obtain completeness through our and HMS'\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:24:06 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Belardinelli", "Gaia", ""], ["Rendsvig", "Rasmus K.", ""]]}, {"id": "2012.12990", "submitter": "Hoa Van Nguyen", "authors": "Hoa Van Nguyen, Hamid Rezatofighi, Ba-Ngu Vo, and Damith C. Ranasinghe", "title": "Distributed Multi-object Tracking under Limited Field of View Sensors", "comments": "13 pages, 10 figures. Submitted to the IEEE Transactions on Signal\n  Processing (TSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the challenging problem of tracking multiple objects using a\ndistributed network of sensors. In the pragmatic settings of a limited field of\nview (FoV) sensors, computing and communication resources of nodes, we develop\na novel distributed multi-target algorithm that fuses local multi-object states\ninstead of local multi-object densities. This algorithm uses a novel label\nconsensus approach that reduces label inconsistency, caused by movements of\nobjects from one node's limited FoV to another. To accomplish this, we\nformalise the concept of label consistency and determine a sufficient condition\nto achieve it. The proposed algorithm is i) fast and requires significantly\nless processing time than fusion methods using multi-object filtering\ndensities, and ii) achieves better tracking accuracy by considering tracking\nerrors measured by the Optimal Sub-Pattern Assignment (OSPA) metric over\nseveral scans rather than a single scan. Numerical experiments demonstrate the\nreal-time capability of our proposed solution, in computational efficiency and\naccuracy compared to state-of-the-art solutions in challenging scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 21:34:28 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Van Nguyen", "Hoa", ""], ["Rezatofighi", "Hamid", ""], ["Vo", "Ba-Ngu", ""], ["Ranasinghe", "Damith C.", ""]]}, {"id": "2012.13727", "submitter": "Thomas Dag\\`es", "authors": "Thomas Dag\\`es, Alfred M. Bruckstein", "title": "Doubly Stochastic Pairwise Interactions for Agreement and Alignment", "comments": "20 pages plus supplemental material; added funding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random pairwise encounters often occur in large populations, or groups of\nmobile agents, and various types of local interactions that happen at\nencounters account for emergent global phenomena. In particular, in the fields\nof swarm robotics, sociobiology, and social dynamics, several types of local\npairwise interactions were proposed and analysed leading to spatial gathering\nor clustering and agreement in teams of robotic agents coordinated motion, in\nanimal herds, or in human societies. We here propose a very simple stochastic\ninteraction at encounters that leads to agreement or geometric alignment in\nswarms of simple agents, and analyse the process of converging to consensus.\nConsider a group of agents whose \"states\" evolve in time by pairwise\ninteractions: the state of an agent is either a real value (a randomly\ninitialised position within an interval) or a vector that is either\nunconstrained (e.g. the location of the agent in the plane) or constrained to\nhave unit length (e.g. the direction of the agent's motion). The interactions\nare doubly stochastic, in the sense that, at discrete time steps, pairs of\nagents are randomly selected and their new states are independently and\nuniformly set at random in (local) domains or intervals defined by the states\nof the interacting pair. We show that such processes lead, in finite expected\ntime (measured by the number of interactions that occurred) to agreement in\ncase of unconstrained states and alignment when the states are unit vectors.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 12:45:18 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 14:06:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Dag\u00e8s", "Thomas", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "2012.13884", "submitter": "Xiaowei Wu", "authors": "Haris Aziz, Bo Li and Xiaowei Wu", "title": "Approximate and Strategyproof Maximin Share Allocation of Chores with\n  Ordinal Preferences", "comments": "arXiv admin note: text overlap with arXiv:1905.08925", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We initiate the work on maximin share (MMS) fair allocation of m indivisible\nchores to n agents using only their ordinal preferences, from both algorithmic\nand mechanism design perspectives. The previous best-known approximation is\n2-1/n by Aziz et al. [IJCAI 2017]. We improve this result by giving a simple\ndeterministic 5/3-approximation algorithm that determines an allocation\nsequence of agents, according to which items are allocated one by one. By a\ntighter analysis, we show that for n=2,3, our algorithm achieves better\napproximation ratios, and is actually optimal. We also consider the setting\nwith strategic agents, where agents may misreport their preferences to\nmanipulate the outcome. We first provide a O(\\log (m/n))-approximation\nconsecutive picking algorithm, and then improve the approximation ratio to\nO(\\sqrt{\\log n}) by a randomized algorithm. Our results uncover some\ninteresting contrasts between the approximation ratios achieved for chores\nversus goods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 07:30:16 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Aziz", "Haris", ""], ["Li", "Bo", ""], ["Wu", "Xiaowei", ""]]}, {"id": "2012.14137", "submitter": "Zheqi Zhu", "authors": "Zheqi Zhu, Shuo Wan, Pingyi Fan, Khaled B. Letaief", "title": "Federated Multi-Agent Actor-Critic Learning for Age Sensitive Mobile\n  Edge Computing", "comments": null, "journal-ref": null, "doi": "10.1109/JIOT.2021.3078514", "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging technique, mobile edge computing (MEC) introduces a new\nprocessing scheme for various distributed communication-computing systems such\nas industrial Internet of Things (IoT), vehicular communication, smart city,\netc. In this work, we mainly focus on the timeliness of the MEC systems where\nthe freshness of the data and computation tasks is significant. Firstly, we\nformulate a kind of age-sensitive MEC models and define the average age of\ninformation (AoI) minimization problems of interests. Then, a novel policy\nbased multi-agent deep reinforcement learning (RL) framework, called\nheterogeneous multi-agent actor critic (H-MAAC), is proposed as a paradigm for\njoint collaboration in the investigated MEC systems, where edge devices and\ncenter controller learn the interactive strategies through their own\nobservations. To improves the system performance, we develop the corresponding\nonline algorithm by introducing an edge federated learning mode into the\nmulti-agent cooperation whose advantages on learning convergence can be\nguaranteed theoretically. To the best of our knowledge, it's the first joint\nMEC collaboration algorithm that combines the edge federated mode with the\nmulti-agent actor-critic reinforcement learning. Furthermore, we evaluate the\nproposed approach and compare it with classical RL based methods. As a result,\nthe proposed framework not only outperforms the baseline on average system age,\nbut also promotes the stability of training process. Besides, the simulation\nresults provide some innovative perspectives for the system design under the\nedge federated collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:19:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 13:43:32 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 10:02:12 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhu", "Zheqi", ""], ["Wan", "Shuo", ""], ["Fan", "Pingyi", ""], ["Letaief", "Khaled B.", ""]]}, {"id": "2012.14195", "submitter": "Sebastian Enqvist", "authors": "Sebastian Enqvist and Valentin Goranko", "title": "The temporal logic of coalitional goal assignments in concurrent\n  multi-player games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a natural extension of the Alternating time temporal\nlogic ATL, called Temporal Logic of Coalitional Goal Assignments (TLCGA). It\nfeatures just one, but quite expressive, coalitional strategic operator, viz.\nthe coalitional goal assignment operator, which is based on a mapping assigning\nto each set of players in the game its coalitional goal, formalised by a path\nformula of the language of TLCGA, i.e. a formula prefixed with a temporal\noperator X;U, or G, representing a temporalised objective for the respective\ncoalition, describing the property of the plays on which that objective is\nsatisfied. We establish fixpoint characterizations of the temporal goal\nassignments in a mu-calculus extension of TLCGA, discuss its expressiveness and\nillustrate it with some examples, prove bisimulation invariance and\nHennessy-Milner property for it with respect to a suitably defined notion of\nbisimulation, construct a sound and complete axiomatic system for TLCGA, and\nobtain its decidability via finite model property.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:20:20 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Enqvist", "Sebastian", ""], ["Goranko", "Valentin", ""]]}, {"id": "2012.14329", "submitter": "Ramviyas Parasuraman", "authors": "Sanjay Sarma O V and Ramviyas Parasuraman and Ramana Pidaparti", "title": "Impact of Heterogeneity in Multi-Robot Systems on Collective Behaviors\n  Studied Using a Search and Rescue Problem", "comments": "Accepted for Publication at the 2020 IEEE International Symposium on\n  Safety, Security, and Rescue Robotics (SSRR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many species in nature demonstrate symbiotic relationships leading to\nemergent behaviors through cooperation, which are sometimes beyond the scope of\nthe partnerships within the same species. These symbiotic relationships are\nclassified as mutualism, commensalism, and parasitism based on the benefit\nlevels involved. While these partnerships are ubiquitous in nature, it is\nimperative to understand the benefits of collective behaviors in designing\nheterogeneous multi-robot systems (HMRS). In this paper, we investigate the\nimpact of heterogeneity on the performance of HMRS applied to a search and\nrescue problem. The groups consisting of searchers and rescuers, varied in the\nindividual robot behaviors with multiple degrees of functionality overlap and\ngroup compositions, demonstrating various levels of heterogeneity. We propose a\nnew technique to measure heterogeneity in the agents through the use of\nBehavior Trees and use it to obtain heterogeneity informatics from our Monte\nCarlo simulations. The results show a positive correlation between the group's\nheterogeneity measure and the rescue efficiency demonstrating benefits in most\nof the scenarios. However, we also see cases where heterogeneity may hamper the\ngroup's abilities pointing to the need for determining the optimal\nheterogeneity in group required to maximally benefit from HMRS in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:31:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["O", "Sanjay Sarma", "V"], ["Parasuraman", "Ramviyas", ""], ["Pidaparti", "Ramana", ""]]}, {"id": "2012.14581", "submitter": "Mehdi Mashayekhi", "authors": "Mehdi Mashayekhi and Nirav Ajmeri and George F. List and Munindar P.\n  Singh", "title": "Prosocial Norm Emergence in Multiagent Systems", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent systems provide a basis of developing systems of autonomous\nentities and thus find application in a variety of domains. We consider a\nsetting where not only the member agents are adaptive but also the multiagent\nsystem itself is adaptive. Specifically, the social structure of a multiagent\nsystem can be reflected in the social norms among its members. It is well\nrecognized that the norms that arise in society are not always beneficial to\nits members. We focus on prosocial norms, which help achieve positive outcomes\nfor society and often provide guidance to agents to act in a manner that takes\ninto account the welfare of others.\n  Specifically, we propose Cha, a framework for the emergence of prosocial\nnorms. Unlike previous norm emergence approaches, Cha supports continual change\nto a system (agents may enter and leave), and dynamism (norms may change when\nthe environment changes). Importantly, Cha agents incorporate prosocial\ndecision making based on inequity aversion theory, reflecting an intuition of\nguilt from being antisocial. In this manner, Cha brings together two important\nthemes in prosociality: decision making by individuals and fairness of\nsystem-level outcomes. We demonstrate via simulation that Cha can improve\naggregate societal gains and fairness of outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 02:59:55 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Mashayekhi", "Mehdi", ""], ["Ajmeri", "Nirav", ""], ["List", "George F.", ""], ["Singh", "Munindar P.", ""]]}, {"id": "2012.14736", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Pierre Fraigniaud, and Petr A. Golovach", "title": "Present-Biased Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the behavior of present-biased agents, that is, agents\nwho erroneously anticipate the costs of future actions compared to their real\ncosts. Specifically, the paper extends the original framework proposed by\nAkerlof (1991) for studying various aspects of human behavior related to\ntime-inconsistent planning, including procrastination, and abandonment, as well\nas the elegant graph-theoretic model encapsulating this framework recently\nproposed by Kleinberg and Oren (2014). The benefit of this extension is\ntwofold. First, it enables to perform fine grained analysis of the behavior of\npresent-biased agents depending on the optimisation task they have to perform.\nIn particular, we study covering tasks vs. hitting tasks, and show that the\nratio between the cost of the solutions computed by present-biased agents and\nthe cost of the optimal solutions may differ significantly depending on the\nproblem constraints. Second, our extension enables to study not only\nunderestimation of future costs, coupled with minimization problems, but also\nall combinations of minimization/maximization, and\nunderestimation/overestimation. We study the four scenarios, and we establish\nupper bounds on the cost ratio for three of them (the cost ratio for the\noriginal scenario was known to be unbounded), providing a complete global\npicture of the behavior of present-biased agents, as far as optimisation tasks\nare concerned.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 12:40:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Fraigniaud", "Pierre", ""], ["Golovach", "Petr A.", ""]]}, {"id": "2012.14851", "submitter": "Dilian Gurov", "authors": "Dilian Gurov, Valentin Goranko and Edvin Lundberg", "title": "Knowledge-Based Strategies for Multi-Agent Teams Playing Against Nature", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study teams of agents that play against Nature towards achieving a common\nobjective. The agents are assumed to have imperfect information due to partial\nobservability, and have no communication during the play of the game. We\npropose a natural notion of higher-order knowledge of agents. Based on this\nnotion, we define a class of knowledge-based strategies, and consider the\nproblem of synthesis of strategies of this class. We introduce a multi-agent\nextension, MKBSC, of the well-known Knowledge-Based Subset Construction applied\nto such games. Its iterative applications turn out to compute higher-order\nknowledge of the agents. We show how the MKBSC can be used for the design of\nknowledge-based strategy profiles and investigate the transfer of existence of\nsuch strategies between the original game and in the iterated applications of\nthe MKBSC, under some natural assumptions. We also relate and compare the\n\"intensional\" view on knowledge-based strategies based on explicit knowledge\nrepresentation and update, with the \"extensional\" view on finite memory\nstrategies based on finite transducers and show that, in a certain sense, these\nare equivalent.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:59:12 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:11:24 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gurov", "Dilian", ""], ["Goranko", "Valentin", ""], ["Lundberg", "Edvin", ""]]}, {"id": "2012.15321", "submitter": "Yubo Qin", "authors": "Yubo Qin, Ivan Rodero, Anthony Simonet, Charles Meertens, Daniel\n  Reiner, James Riley, Manish Parashar", "title": "Leveraging User Access Patterns and Advanced Cyberinfrastructure to\n  Accelerate Data Delivery from Shared-use Scientific Observatories", "comments": "10 pages, 13 figures, 5 tables, Future Generation Computer Systems\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing number and increasing availability of shared-use instruments\nand observatories, observational data is becoming an essential part of\napplication workflows and contributor to scientific discoveries in a range of\ndisciplines. However, the corresponding growth in the number of users accessing\nthese facilities coupled with the expansion in the scale and variety of the\ndata, is making it challenging for these facilities to ensure their data can be\naccessed, integrated, and analyzed in a timely manner, and is resulting\nsignificant demands on their cyberinfrastructure (CI).\n  In this paper, we present the design of a push-based data delivery framework\nthat leverages emerging in-network capabilities, along with data pre-fetching\ntechniques based on a hybrid data management model. Specifically, we analyze\ndata access traces for two large-scale observatories, Ocean Observatories\nInitiative (OOI) and Geodetic Facility for the Advancement of Geoscience\n(GAGE), to identify typical user access patterns and to develop a model that\ncan be used for data pre-fetching. Furthermore, we evaluate our data\npre-fetching model and the proposed framework using a simulation of the Virtual\nData Collaboratory (VDC) platform that provides in-network data staging and\nprocessing capabilities. The results demonstrate that the ability of the\nframework to significantly improve data delivery performance and reduce network\ntraffic at the observatories' facilities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:52:00 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Qin", "Yubo", ""], ["Rodero", "Ivan", ""], ["Simonet", "Anthony", ""], ["Meertens", "Charles", ""], ["Reiner", "Daniel", ""], ["Riley", "James", ""], ["Parashar", "Manish", ""]]}, {"id": "2012.15377", "submitter": "Vaneet Aggarwal", "authors": "Arnob Ghosh and Vaneet Aggarwal", "title": "Model Free Reinforcement Learning Algorithm for Stationary Mean field\n  Equilibrium for Multiple Types of Agents", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-agent Markov strategic interaction over an infinite\nhorizon where agents can be of multiple types. We model the strategic\ninteraction as a mean-field game in the asymptotic limit when the number of\nagents of each type becomes infinite. Each agent has a private state; the state\nevolves depending on the distribution of the state of the agents of different\ntypes and the action of the agent. Each agent wants to maximize the discounted\nsum of rewards over the infinite horizon which depends on the state of the\nagent and the distribution of the state of the leaders and followers. We seek\nto characterize and compute a stationary multi-type Mean field equilibrium\n(MMFE) in the above game. We characterize the conditions under which a\nstationary MMFE exists. Finally, we propose Reinforcement learning (RL) based\nalgorithm using policy gradient approach to find the stationary MMFE when the\nagents are unaware of the dynamics. We, numerically, evaluate how such kind of\ninteraction can model the cyber attacks among defenders and adversaries, and\nshow how RL based algorithm can converge to an equilibrium.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:12:46 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ghosh", "Arnob", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2012.15545", "submitter": "Md Ferdous Pervej", "authors": "Md Ferdous Pervej and Shih-Chun Lin", "title": "Vehicular Network Slicing for Reliable Access and Deadline-Constrained\n  Data Offloading: A Multi-Agent On-Device Learning Approach", "comments": "Submitted for possible journal publication, 15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient data offloading plays a pivotal role in computational-intensive\nplatforms as data rate over wireless channels is fundamentally limited. On top\nof that, high mobility adds an extra burden in vehicular edge networks (VENs),\nbolstering the desire for efficient user-centric solutions. Therefore, unlike\nthe legacy inflexible network-centric approach, this paper exploits a\nsoftware-defined flexible, open, and programmable networking platform for an\nefficient user-centric, fast, reliable, and deadline-constrained offloading\nsolution in VENs. In the proposed model, each active vehicle user (VU) is\nserved from multiple low-powered access points (APs) by creating a noble\nvirtual cell (VC). A joint node association, power allocation, and distributed\nresource allocation problem is formulated. As centralized learning is not\npractical in many real-world problems, following the distributed nature of\nautonomous VUs, each VU is considered an edge learning agent. To that end,\nconsidering practical location-aware node associations, a joint radio and power\nresource allocation non-cooperative stochastic game is formulated. Leveraging\nreinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is\nproposed where the edge learners aim to learn the Nash equilibrium (NE)\nstrategies to solve the game efficiently. Besides, real-world map data, with a\npractical microscopic mobility model, are used for the simulation. Results\nsuggest that the proposed user-centric approach can deliver remarkable\nperformances in VENs. Moreover, the proposed MARL solution delivers\nnear-optimal performances with approximately 3% collision probabilities in case\nof distributed random access in the uplink.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:15:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Pervej", "Md Ferdous", ""], ["Lin", "Shih-Chun", ""]]}, {"id": "2012.15791", "submitter": "Sriram Ganapathi Subramanian", "authors": "Sriram Ganapathi Subramanian, Matthew E. Taylor, Mark Crowley, Pascal\n  Poupart", "title": "Partially Observable Mean Field Reinforcement Learning", "comments": "Paper to be published in International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS) - 2021. New version has some typos\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Traditional multi-agent reinforcement learning algorithms are not scalable to\nenvironments with more than a few agents, since these algorithms are\nexponential in the number of agents. Recent research has introduced successful\nmethods to scale multi-agent reinforcement learning algorithms to many agent\nscenarios using mean field theory. Previous work in this field assumes that an\nagent has access to exact cumulative metrics regarding the mean field behaviour\nof the system, which it can then use to take its actions. In this paper, we\nrelax this assumption and maintain a distribution to model the uncertainty\nregarding the mean field of the system. We consider two different settings for\nthis problem. In the first setting, only agents in a fixed neighbourhood are\nvisible, while in the second setting, the visibility of agents is determined at\nrandom based on distances. For each of these settings, we introduce a\nQ-learning based algorithm that can learn effectively. We prove that this\nQ-learning estimate stays very close to the Nash Q-value (under a common set of\nassumptions) for the first setting. We also empirically show our algorithms\noutperform multiple baselines in three different games in the MAgents\nframework, which supports large environments with many agents learning\nsimultaneously to achieve possibly distinct goals.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:12:31 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 21:16:25 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 16:05:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Subramanian", "Sriram Ganapathi", ""], ["Taylor", "Matthew E.", ""], ["Crowley", "Mark", ""], ["Poupart", "Pascal", ""]]}]