[{"id": "2102.00053", "submitter": "Aleksander Czechowski", "authors": "Aleksander Czechowski and Georgios Piliouras", "title": "Poincar\\'{e}-Bendixson Limit Sets in Multi-Agent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge of evolutionary game theory and multi-agent learning is to\ncharacterize the limit behaviour of game dynamics. Whereas convergence is often\na property of learning algorithms in games satisfying a particular reward\nstructure (e.g. zero-sum), it is well known, that for general payoffs even\nbasic learning models, such as the replicator dynamics, are not guaranteed to\nconverge. Worse yet, chaotic behavior is possible even in rather simple games,\nsuch as variants of Rock-Paper-Scissors games (Sato et al., 2002). Although\nchaotic behavior in learning dynamics can be precluded by the celebrated\nPoincar\\'e-Bendixson theorem, it is only applicable to low-dimensional\nsettings. Are there other characteristics of a game, which can force regularity\nin the limit sets of learning?\n  In this paper, we show that behaviors consistent with the\nPoincar\\'e-Bendixson theorem (limit cycles, but no chaotic attractor) follows\npurely based on the topological structure of the interaction graph, even for\nhigh-dimensional settings with arbitrary number of players and arbitrary payoff\nmatrices. We prove our result for a wide class of follow-the-regularized leader\n(FoReL) dynamics, which generalize replicator dynamics, for games where each\nplayer has two strategies at disposal, and for interaction graphs where payoffs\nof each agent are only affected by one other agent (i.e. interaction graphs of\nindegree one). Since chaos has been observed in a game with only two players\nand three strategies, this class of non-chaotic games is in a sense maximal.\nMoreover, we provide simple conditions under which such behavior translates to\nsocial welfare guarantees, implying that FoReL learning achieves time average\nsocial welfare which is at least as good as that of a Nash equilibrium; and\nconnecting the topology of the dynamics to the Price of Anarchy analysis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 20:32:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Czechowski", "Aleksander", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2102.00109", "submitter": "Jasmine Sekhon", "authors": "Jasmine Sekhon, Cody Fleming", "title": "SCAN: A Spatial Context Attentive Network for Joint Multi-Agent Intent\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe navigation of autonomous agents in human centric environments requires\nthe ability to understand and predict motion of neighboring pedestrians.\nHowever, predicting pedestrian intent is a complex problem. Pedestrian motion\nis governed by complex social navigation norms, is dependent on neighbors'\ntrajectories, and is multimodal in nature. In this work, we propose SCAN, a\nSpatial Context Attentive Network that can jointly predict socially-acceptable\nmultiple future trajectories for all pedestrians in a scene. SCAN encodes the\ninfluence of spatially close neighbors using a novel spatial attention\nmechanism in a manner that relies on fewer assumptions, is parameter efficient,\nand is more interpretable compared to state-of-the-art spatial attention\napproaches. Through experiments on several datasets we demonstrate that our\napproach can also quantitatively outperform state of the art trajectory\nprediction methods in terms of accuracy of predicted intent.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 23:35:00 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:54:39 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sekhon", "Jasmine", ""], ["Fleming", "Cody", ""]]}, {"id": "2102.00582", "submitter": "Lewis Hammond", "authors": "Lewis Hammond and Alessandro Abate and Julian Gutierrez and Michael\n  Wooldridge", "title": "Multi-Agent Reinforcement Learning with Temporal Logic Specifications", "comments": "Accepted to the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning to satisfy temporal logic\nspecifications with a group of agents in an unknown environment, which may\nexhibit probabilistic behaviour. From a learning perspective these\nspecifications provide a rich formal language with which to capture tasks or\nobjectives, while from a logic and automated verification perspective the\nintroduction of learning capabilities allows for practical applications in\nlarge, stochastic, unknown environments. The existing work in this area is,\nhowever, limited. Of the frameworks that consider full linear temporal logic or\nhave correctness guarantees, all methods thus far consider only the case of a\nsingle temporal logic specification and a single agent. In order to overcome\nthis limitation, we develop the first multi-agent reinforcement learning\ntechnique for temporal logic specifications, which is also novel in its ability\nto handle multiple specifications. We provide correctness and convergence\nguarantees for our main algorithm - ALMANAC (Automaton/Logic Multi-Agent\nNatural Actor-Critic) - even when using function approximation. Alongside our\ntheoretical results, we further demonstrate the applicability of our technique\nvia a set of preliminary experiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 01:13:03 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 18:57:49 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hammond", "Lewis", ""], ["Abate", "Alessandro", ""], ["Gutierrez", "Julian", ""], ["Wooldridge", "Michael", ""]]}, {"id": "2102.00824", "submitter": "Nikunj Gupta", "authors": "Nikunj Gupta, G Srinivasaraghavan, Swarup Kumar Mohalik, Matthew E.\n  Taylor", "title": "HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via\n  Learned Messaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cooperative multi-agent reinforcement learning (MARL) has achieved\nsignificant results, most notably by leveraging the representation learning\nabilities of deep neural networks. However, large centralized approaches\nquickly become infeasible as the number of agents scale, and fully\ndecentralized approaches can miss important opportunities for information\nsharing and coordination. Furthermore, not all agents are equal - in some\ncases, individual agents may not even have the ability to send communication to\nother agents or explicitly model other agents. This paper considers the case\nwhere there is a single, powerful, central agent that can observe the entire\nobservation space, and there are multiple, low powered, local agents that can\nonly receive local observations and cannot communicate with each other. The job\nof the central agent is to learn what message to send to different local\nagents, based on the global observations, not by centrally solving the entire\nproblem and sending action commands, but by determining what additional\ninformation an individual agent should receive so that it can make a better\ndecision. After explaining our MARL algorithm, hammer, and where it would be\nmost applicable, we implement it in the cooperative navigation and multi-agent\nwalker domains. Empirical results show that 1) learned communication does\nindeed improve system performance, 2) results generalize to multiple numbers of\nagents, and 3) results generalize to different reward structures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:00:12 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Nikunj", ""], ["Srinivasaraghavan", "G", ""], ["Mohalik", "Swarup Kumar", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "2102.00844", "submitter": "Dibakar Das", "authors": "Dibakar Das", "title": "Agent Based Virus Model using NetLogo: Infection Propagation,\n  Precaution, Recovery, Multi-site Mobility and (Un)Lockdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel virus propagation model using NetLogo. The model\nallows agents to move across multiple sites using different routes. Routes can\nbe configured, enabled for mobility and (un)locked down independently.\nSimilarly, locations can also be (un)locked down independently. Agents can get\ninfected, propagate their infections to others, can take precautions against\ninfection and also subsequently recover from infection. This model contains\ncertain features that are not present in existing models. The model may be used\nfor educational and research purposes, and the code is made available as open\nsource. This model may also provide a broader framework for more detailed\nsimulations. The results presented are only to demonstrate the model\nfunctionalities and do not serve any other purpose.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:20:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Das", "Dibakar", ""]]}, {"id": "2102.00847", "submitter": "Carter Blum", "authors": "Carter Blum, Hao Liu, Hui Xiong", "title": "CoordiQ : Coordinated Q-learning for Electric Vehicle Charging\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electric vehicles have been rapidly increasing in usage, but stations to\ncharge them have not always kept up with demand, so efficient routing of\nvehicles to stations is critical to operating at maximum efficiency. Deciding\nwhich stations to recommend drivers to is a complex problem with a multitude of\npossible recommendations, volatile usage patterns and temporally extended\nconsequences of recommendations. Reinforcement learning offers a powerful\nparadigm for solving sequential decision-making problems, but traditional\nmethods may struggle with sample efficiency due to the high number of possible\nactions. By developing a model that allows complex representations of actions,\nwe improve outcomes for users of our system by over 30% when compared to\nexisting baselines in a simulation. If implemented widely, these better\nrecommendations can globally save over 4 million person-hours of waiting and\ndriving each year.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:25:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Blum", "Carter", ""], ["Liu", "Hao", ""], ["Xiong", "Hui", ""]]}, {"id": "2102.01004", "submitter": "Ruben Glatt", "authors": "William A. Dawson, Ruben Glatt, Edward Rusu, Braden C. Soper, Ryan A.\n  Goldhahn", "title": "Hybrid Information-driven Multi-agent Reinforcement Learning", "comments": "Published at Workshop on Challenges and Opportunities for Multi-Agent\n  Reinforcement Learning (COMARL AAAI 2021). This work was performed under the\n  auspices of the U.S. Department of Energy by Lawrence Livermore National\n  Laboratory under contract DE-AC52-07NA27344. Lawrence Livermore National\n  Security, LLC through the support of LDRD 20-SI-005. LLNL-CONF-816423", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Information theoretic sensor management approaches are an ideal solution to\nstate estimation problems when considering the optimal control of multi-agent\nsystems, however they are too computationally intensive for large state spaces,\nespecially when considering the limited computational resources typical of\nlarge-scale distributed multi-agent systems. Reinforcement learning (RL) is a\npromising alternative which can find approximate solutions to distributed\noptimal control problems that take into account the resource constraints\ninherent in many systems of distributed agents. However, the RL training can be\nprohibitively inefficient, especially in low-information environments where\nagents receive little to no feedback in large portions of the state space. We\npropose a hybrid information-driven multi-agent reinforcement learning (MARL)\napproach that utilizes information theoretic models as heuristics to help the\nagents navigate large sparse state spaces, coupled with information based\nrewards in an RL framework to learn higher-level policies. This paper presents\nour ongoing work towards this objective. Our preliminary findings show that\nsuch an approach can result in a system of agents that are approximately three\norders of magnitude more efficient at exploring a sparse state space than naive\nbaseline metrics. While the work is still in its early stages, it provides a\npromising direction for future research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 17:28:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dawson", "William A.", ""], ["Glatt", "Ruben", ""], ["Rusu", "Edward", ""], ["Soper", "Braden C.", ""], ["Goldhahn", "Ryan A.", ""]]}, {"id": "2102.01434", "submitter": "Borja Gonzalez Leon", "authors": "Pierre El Mqirmi, Francesco Belardinelli and Borja G. Le\\'on", "title": "An Abstraction-based Method to Check Multi-Agent Deep\n  Reinforcement-Learning Behaviors", "comments": "Extended version of AAMAS publication under the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (RL) often struggles to ensure the safe\nbehaviours of the learning agents, and therefore it is generally not adapted to\nsafety-critical applications. To address this issue, we present a methodology\nthat combines formal verification with (deep) RL algorithms to guarantee the\nsatisfaction of formally-specified safety constraints both in training and\ntesting. The approach we propose expresses the constraints to verify in\nProbabilistic Computation Tree Logic (PCTL) and builds an abstract\nrepresentation of the system to reduce the complexity of the verification step.\nThis abstract model allows for model checking techniques to identify a set of\nabstract policies that meet the safety constraints expressed in PCTL. Then, the\nagents' behaviours are restricted according to these safe abstract policies. We\nprovide formal guarantees that by using this method, the actions of the agents\nalways meet the safety constraints, and provide a procedure to generate an\nabstract model automatically. We empirically evaluate and show the\neffectiveness of our method in a multi-agent environment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:12:30 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 00:52:59 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mqirmi", "Pierre El", ""], ["Belardinelli", "Francesco", ""], ["Le\u00f3n", "Borja G.", ""]]}, {"id": "2102.01585", "submitter": "Kai Cui", "authors": "Kai Cui, Heinz Koeppl", "title": "Approximately Solving Mean Field Games via Entropy-Regularized Deep\n  Reinforcement Learning", "comments": "Accepted to the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent mean field game (MFG) formalism facilitates otherwise intractable\ncomputation of approximate Nash equilibria in many-agent settings. In this\npaper, we consider discrete-time finite MFGs subject to finite-horizon\nobjectives. We show that all discrete-time finite MFGs with non-constant fixed\npoint operators fail to be contractive as typically assumed in existing MFG\nliterature, barring convergence via fixed point iteration. Instead, we\nincorporate entropy-regularization and Boltzmann policies into the fixed point\niteration. As a result, we obtain provable convergence to approximate fixed\npoints where existing methods fail, and reach the original goal of approximate\nNash equilibria. All proposed methods are evaluated with respect to their\nexploitability, on both instructive examples with tractable exact solutions and\nhigh-dimensional problems where exact methods become intractable. In\nhigh-dimensional scenarios, we apply established deep reinforcement learning\nmethods and empirically combine fictitious play with our approximations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:22:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Cui", "Kai", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2102.01995", "submitter": "Wojciech Jamroga", "authors": "Gergei Bana, Wojciech Jamroga, David Naccache, Peter Y. A. Ryan", "title": "Convergence Voting: From Pairwise Comparisons to Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important aspect of AI design and ethics is to create systems that reflect\naggregate preferences of the society. To this end, the techniques of social\nchoice theory are often utilized. We propose a new social choice function\nmotivated by the PageRank algorithm. The function ranks voting options based on\nthe Condorcet graph of pairwise comparisons. To this end, we transform the\nCondorcet graph into a Markov chain whose stationary distribution provides the\nscores of the options. We show how the values in the stationary distribution\ncan be interpreted as quantified aggregate support for the voting options, to\nwhich the community of voters converges through an imaginary sequence of\nnegotiating steps. Because of that, we suggest the name \"convergence voting\"\nfor the new voting scheme, and \"negotiated community support\" for the resulting\nstationary allocation of scores.\n  Our social choice function can be viewed as a consensus voting method,\nsitting somewhere between Copeland and Borda. On the one hand, it does not\nnecessarily choose the Condorcet winner, as strong support from a part of the\nsociety can outweigh mediocre uniform support. On the other hand, the influence\nof unpopular candidates on the outcome is smaller than in the primary technique\nof consensus voting, i.e., the Borda count. We achieve that without having to\nintroduce an ad hoc weighting that some other methods do.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:50:41 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 11:00:22 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 09:17:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Bana", "Gergei", ""], ["Jamroga", "Wojciech", ""], ["Naccache", "David", ""], ["Ryan", "Peter Y. A.", ""]]}, {"id": "2102.02078", "submitter": "Yicheng Chen", "authors": "Huan Chang, Yicheng Chen, Baochang Zhang, David Doermann", "title": "Multi-UAV Mobile Edge Computing and Path Planning Platform based on\n  Reinforcement Learning", "comments": "The source code can be found at https://github.com/bczhangbczhang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial vehicles (UAVs) are widely used as network processors in\nmobile networks, but more recently, UAVs have been used in Mobile Edge\nComputing as mobile servers. However, there are significant challenges to use\nUAVs in complex environments with obstacles and cooperation between UAVs. We\nintroduce a new multi-UAV Mobile Edge Computing platform, which aims to provide\nbetter Quality-of-Service and path planning based on reinforcement learning to\naddress these issues. The contributions of our work include: 1) optimizing the\nquality of service for mobile edge computing and path planning in the same\nreinforcement learning framework; 2) using a sigmoid-like function to depict\nthe terminal users' demand to ensure a higher quality of service; 3) applying\nsynthetic considerations of the terminal users' demand, risk and geometric\ndistance in reinforcement learning reward matrix to ensure the quality of\nservice, risk avoidance, and the cost-savings. Simulations have shown the\neffectiveness and feasibility of our platform, which can help advance related\nresearches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:22:36 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 02:15:25 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:44:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chang", "Huan", ""], ["Chen", "Yicheng", ""], ["Zhang", "Baochang", ""], ["Doermann", "David", ""]]}, {"id": "2102.02274", "submitter": "Pol Moreno", "authors": "Pol Moreno, Edward Hughes, Kevin R. McKee, Bernardo Avila Pires,\n  Th\\'eophane Weber", "title": "Neural Recursive Belief States in Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-agent reinforcement learning, the problem of learning to act is\nparticularly difficult because the policies of co-players may be heavily\nconditioned on information only observed by them. On the other hand, humans\nreadily form beliefs about the knowledge possessed by their peers and leverage\nbeliefs to inform decision-making. Such abilities underlie individual success\nin a wide range of Markov games, from bluffing in Poker to conditional\ncooperation in the Prisoner's Dilemma, to convention-building in Bridge.\nClassical methods are usually not applicable to complex domains due to the\nintractable nature of hierarchical beliefs (i.e. beliefs of other agents'\nbeliefs). We propose a scalable method to approximate these belief structures\nusing recursive deep generative models, and to use the belief models to obtain\nrepresentations useful to acting in complex tasks. Our agents trained with\nbelief models outperform model-free baselines with equivalent representational\ncapacity using common training paradigms. We also show that higher-order belief\nmodels outperform agents with lower-order models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:10:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Moreno", "Pol", ""], ["Hughes", "Edward", ""], ["McKee", "Kevin R.", ""], ["Pires", "Bernardo Avila", ""], ["Weber", "Th\u00e9ophane", ""]]}, {"id": "2102.02304", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Zeki Doruk Erden, Boi Faltings", "title": "Improved Cooperation by Exploiting a Common Signal", "comments": "Accepted to the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can artificial agents benefit from human conventions? Human societies manage\nto successfully self-organize and resolve the tragedy of the commons in\ncommon-pool resources, in spite of the bleak prediction of non-cooperative game\ntheory. On top of that, real-world problems are inherently large-scale and of\nlow observability. One key concept that facilitates human coordination in such\nsettings is the use of conventions. Inspired by human behavior, we investigate\nthe learning dynamics and emergence of temporal conventions, focusing on\ncommon-pool resources. Extra emphasis was given in designing a realistic\nevaluation setting: (a) environment dynamics are modeled on real-world\nfisheries, (b) we assume decentralized learning, where agents can observe only\ntheir own history, and (c) we run large-scale simulations (up to 64 agents).\n  Uncoupled policies and low observability make cooperation hard to achieve; as\nthe number of agents grow, the probability of taking a correct gradient\ndirection decreases exponentially. By introducing an arbitrary common signal\n(e.g., date, time, or any periodic set of numbers) as a means to couple the\nlearning process, we show that temporal conventions can emerge and agents reach\nsustainable harvesting strategies. The introduction of the signal consistently\nimproves the social welfare (by 258% on average, up to 3306%), the range of\nenvironmental parameters where sustainability can be achieved (by 46% on\naverage, up to 300%), and the convergence speed in low abundance settings (by\n13% on average, up to 53%).\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:27:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Erden", "Zeki Doruk", ""], ["Faltings", "Boi", ""]]}, {"id": "2102.02351", "submitter": "Jayam Umesh Patel", "authors": "Jayam Patel, Prajankya Sonar and Carlo Pinciroli", "title": "On Multi-Human Multi-Robot Remote Interaction: A Study of Transparency,\n  Inter-Human Communication, and Information Loss in Remote Interaction", "comments": "44 Pages, Submitted to the Springer Journal of Swarm Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate how to design an effective interface for remote\nmulti-human multi-robot interaction. While significant research exists on\ninterfaces for individual human operators, little research exists for the\nmulti-human case. Yet, this is a critical problem to solve to make complex,\nlarge-scale missions achievable in which direct human involvement is impossible\nor undesirable, and robot swarms act as a semi-autonomous agents. This paper's\ncontribution is twofold. The first contribution is an exploration of the design\nspace of computer-based interfaces for multi-human multi-robot operations. In\nparticular, we focus on information transparency and on the factors that affect\ninter-human communication in ideal conditions, i.e., without communication\nissues. Our second contribution concerns the same problem, but considering\nincreasing degrees of information loss, defined as intermittent reception of\ndata with noticeable gaps between individual receipts. We derived a set of\ndesign recommendations based on two user studies involving 48 participants.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 00:40:48 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Patel", "Jayam", ""], ["Sonar", "Prajankya", ""], ["Pinciroli", "Carlo", ""]]}, {"id": "2102.02439", "submitter": "Zahi Kakish", "authors": "Zahi Kakish, Sritanay Vedartham and Spring Berman", "title": "Towards Decentralized Human-Swarm Interaction by Means of Sequential\n  Hand Gesture Recognition", "comments": "7 Pages. 4 Figures. Multi-robot systems paper. Rejected ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present preliminary work on a novel method for Human-Swarm\nInteraction (HSI) that can be used to change the macroscopic behavior of a\nswarm of robots with decentralized sensing and control. By integrating a small\nyet capable hand gesture recognition convolutional neural network (CNN) with\nthe next-generation Robot Operating System \\emph{ros2}, which enables\ndecentralized implementation of robot software for multi-robot applications, we\ndemonstrate the feasibility of programming a swarm of robots to recognize and\nrespond to a sequence of hand gestures that capable of correspond to different\ntypes of swarm behaviors. We test our approach using a sequence of gestures\nthat modifies the target inter-robot distance in a group of three Turtlebot3\nBurger robots in order to prevent robot collisions with obstacles. The approach\nis validated in three different Gazebo simulation environments and in a\nphysical testbed that reproduces one of the simulated environments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:43:14 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Kakish", "Zahi", ""], ["Vedartham", "Sritanay", ""], ["Berman", "Spring", ""]]}, {"id": "2102.02441", "submitter": "Francisco Cruz", "authors": "Adam Bignold and Francisco Cruz and Richard Dazeley and Peter Vamplew\n  and Cameron Foale", "title": "Persistent Rule-based Interactive Reinforcement Learning", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive reinforcement learning has allowed speeding up the learning\nprocess in autonomous agents by including a human trainer providing extra\ninformation to the agent in real-time. Current interactive reinforcement\nlearning research has been limited to interactions that offer relevant advice\nto the current state only. Additionally, the information provided by each\ninteraction is not retained and instead discarded by the agent after a\nsingle-use. In this work, we propose a persistent rule-based interactive\nreinforcement learning approach, i.e., a method for retaining and reusing\nprovided knowledge, allowing trainers to give general advice relevant to more\nthan just the current state. Our experimental results show persistent advice\nsubstantially improves the performance of the agent while reducing the number\nof interactions required for the trainer. Moreover, rule-based advice shows\nsimilar performance impact as state-based advice, but with a substantially\nreduced interaction count.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:48:57 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bignold", "Adam", ""], ["Cruz", "Francisco", ""], ["Dazeley", "Richard", ""], ["Vamplew", "Peter", ""], ["Foale", "Cameron", ""]]}, {"id": "2102.02881", "submitter": "Francesca Toni", "authors": "Stefan Lauren and Francesco Belardinelli and Francesca Toni", "title": "Aggregating Bipolar Opinions (With Appendix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method to aggregate Bipolar Argumentation (BA)\nFrameworks expressing opinions by different parties in debates. We use Bipolar\nAssumption-based Argumentation (ABA) as an all-encompassing formalism for BA\nunder different semantics. By leveraging on recent results on judgement\naggregation in Social Choice Theory, we prove several preservation results,\nboth positive and negative, for relevant properties of Bipolar ABA.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:43:30 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lauren", "Stefan", ""], ["Belardinelli", "Francesco", ""], ["Toni", "Francesca", ""]]}, {"id": "2102.02919", "submitter": "Tianqi Li", "authors": "Tianqi Li, Lucas W. Krakow and Swaminathan Gopalswamy", "title": "Optimizing Consensus-based Multi-target Tracking with Multiagent Rollout\n  Control Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers a multiagent, connected, robotic fleet where the primary\nfunctionality of the agents is sensing. A distributed multi-sensor control\nstrategy maximizes the value of the collective sensing capability of the fleet,\nusing an information-driven approach. Each agent individually performs sensor\nprocessing (Kalman Filtering and Joint Probabilistic Data Association) to\nidentify trajectories (and associated distributions). Using communications with\nits neighbors the agents enhance the prediction of the trajectories using a\nConsensus of Information approach that iteratively calculates the\nKullback-Leibler average of trajectory distributions, enabling the calculation\nof the value of the collective information for the fleet. The dynamics of the\nagents, the evolution of the identified trajectories for each agent, and the\ndynamics of individual observed objects are captured as a Partially Observable\nMarkov Decision Process (POMDP). Using this POMDP and applying rollout with\nreceding horizon control, an optimized non-myopic control policy that maximizes\nthe collective fleet information value is synthesized. Simulations are\nperformed for a scenario with three heterogeneous UAVs performing coordinated\ntarget tracking that illustrate the proposed methodology and compare the\ncentralized approach with a contemporary sequential multiagent distributed\ndecision technique.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:34:26 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Li", "Tianqi", ""], ["Krakow", "Lucas W.", ""], ["Gopalswamy", "Swaminathan", ""]]}, {"id": "2102.03022", "submitter": "Tim Miller", "authors": "Zhengshang Liu, Yue Yang, Tim Miller, and Peta Masters", "title": "Deceptive Reinforcement Learning for Privacy-Preserving Planning", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of deceptive reinforcement learning to\npreserve the privacy of a reward function. Reinforcement learning is the\nproblem of finding a behaviour policy based on rewards received from\nexploratory behaviour. A key ingredient in reinforcement learning is a reward\nfunction, which determines how much reward (negative or positive) is given and\nwhen. However, in some situations, we may want to keep a reward function\nprivate; that is, to make it difficult for an observer to determine the reward\nfunction used. We define the problem of privacy-preserving reinforcement\nlearning, and present two models for solving it. These models are based on\ndissimulation -- a form of deception that `hides the truth'. We evaluate our\nmodels both computationally and via human behavioural experiments. Results show\nthat the resulting policies are indeed deceptive, and that participants can\ndetermine the true reward function less reliably than that of an honest agent.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 06:50:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Liu", "Zhengshang", ""], ["Yang", "Yue", ""], ["Miller", "Tim", ""], ["Masters", "Peta", ""]]}, {"id": "2102.03291", "submitter": "Michael Alcorn", "authors": "Michael A. Alcorn and Anh Nguyen", "title": "baller2vec: A Multi-Entity Transformer For Multi-Agent Spatiotemporal\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent spatiotemporal modeling is a challenging task from both an\nalgorithmic design and computational complexity perspective. Recent work has\nexplored the efficacy of traditional deep sequential models in this domain, but\nthese architectures are slow and cumbersome to train, particularly as model\nsize increases. Further, prior attempts to model interactions between agents\nacross time have limitations, such as imposing an order on the agents, or\nmaking assumptions about their relationships. In this paper, we introduce\nballer2vec, a multi-entity generalization of the standard Transformer that can,\nwith minimal assumptions, simultaneously and efficiently integrate information\nacross entities and time. We test the effectiveness of baller2vec for\nmulti-agent spatiotemporal modeling by training it to perform two different\nbasketball-related tasks: (1) simultaneously forecasting the trajectories of\nall players on the court and (2) forecasting the trajectory of the ball. Not\nonly does baller2vec learn to perform these tasks well (outperforming a graph\nrecurrent neural network with a similar number of parameters by a wide margin),\nit also appears to \"understand\" the game of basketball, encoding idiosyncratic\nqualities of players in its embeddings, and performing basketball-relevant\nfunctions with its attention heads.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:02:04 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 11:35:03 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Alcorn", "Michael A.", ""], ["Nguyen", "Anh", ""]]}, {"id": "2102.03382", "submitter": "Tu Le", "authors": "Tu Le, Danny Yuxing Huang, Noah Apthorpe, Yuan Tian", "title": "SkillBot: Identifying Risky Content for Children in Alexa Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many households include children who use voice personal assistants (VPA) such\nas Amazon Alexa. Children benefit from the rich functionalities of VPAs and\nthird-party apps but are also exposed to new risks in the VPA ecosystem (e.g.,\ninappropriate content or information collection). To study the risks VPAs pose\nto children, we build a Natural Language Processing (NLP)-based system to\nautomatically interact with VPA apps and analyze the resulting conversations to\nidentify contents risky to children. We identify 28 child-directed apps with\nrisky contents and maintain a growing dataset of 31,966 non-overlapping app\nbehaviors collected from 3,434 Alexa apps. Our findings suggest that although\nvoice apps designed for children are subject to more policy requirements and\nintensive vetting, children are still vulnerable to risky content. We then\nconduct a user study showing that parents are more concerned about VPA apps\nwith inappropriate content than those that ask for personal information, but\nmany parents are not aware that risky apps of either type exist. Finally, we\nidentify a new threat to users of VPA apps: confounding utterances, or voice\ncommands shared by multiple apps that may cause a user to invoke or interact\nwith a different app than intended. We identify 4,487 confounding utterances,\nincluding 581 shared by child-directed and non-child-directed apps.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:07:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Le", "Tu", ""], ["Huang", "Danny Yuxing", ""], ["Apthorpe", "Noah", ""], ["Tian", "Yuan", ""]]}, {"id": "2102.03461", "submitter": "Theodor Cimpeanu", "authors": "Theodor Cimpeanu, Cedric Perret and The Anh Han", "title": "Promoting Fair Proposers, Fair Responders or Both? Cost-Efficient\n  Interference in the Spatial Ultimatum Game", "comments": "39 pages, 17 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Institutions and investors face the constant challenge of making accurate\ndecisions and predictions regarding how best they should distribute their\nendowments. The problem of achieving an optimal outcome at minimal cost has\nbeen extensively studied and resolved using several heuristics. However, these\nworks usually fail to address how an external party can target different types\nof fair behaviour or do not take into account how limited information can shape\nthis complex interplay. Here, we consider the well-known Ultimatum game in a\nspatial setting and propose a hierarchy of interference mechanisms based on the\namount of information available to an external decision-maker and desired\nstandards of fairness. Our analysis reveals that monitoring the population at a\nmacroscopic level requires more strict information gathering in order to obtain\nan optimal outcome and that local observations can mediate this requirement.\nMoreover, we identify the conditions which must be met for an individual to be\neligible for investment in order to avoid unnecessary spending. We further\nexplore the effects of varying mutation or behavioural exploration rates on the\nchoice of investment strategy and total accumulated costs to the investor.\nOverall, our analysis provides new insights about efficient heuristics for\ncost-efficient promotion of fairness in societies. Finally, we discuss the\ndifferences between our findings and previous work done on the PD and present\nour suggestions for promoting fairness as an external decision-maker.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 00:44:58 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Cimpeanu", "Theodor", ""], ["Perret", "Cedric", ""], ["Han", "The Anh", ""]]}, {"id": "2102.03479", "submitter": "Jian Hu", "authors": "Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, Shih-wei Liao", "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in\n  Cooperative Multi-Agent Reinforcement Learning", "comments": "add experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex multi-robot systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 02:28:09 GMT"}, {"version": "v10", "created": "Thu, 13 May 2021 13:06:58 GMT"}, {"version": "v11", "created": "Mon, 7 Jun 2021 07:54:45 GMT"}, {"version": "v12", "created": "Thu, 1 Jul 2021 07:37:31 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:01:38 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 15:37:05 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 13:46:31 GMT"}, {"version": "v5", "created": "Thu, 11 Mar 2021 08:29:17 GMT"}, {"version": "v6", "created": "Mon, 15 Mar 2021 06:21:44 GMT"}, {"version": "v7", "created": "Fri, 9 Apr 2021 03:39:11 GMT"}, {"version": "v8", "created": "Mon, 19 Apr 2021 09:00:01 GMT"}, {"version": "v9", "created": "Wed, 21 Apr 2021 08:38:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hu", "Jian", ""], ["Jiang", "Siyang", ""], ["Harding", "Seth Austin", ""], ["Wu", "Haibin", ""], ["Liao", "Shih-wei", ""]]}, {"id": "2102.03588", "submitter": "Ayan Sengupta", "authors": "Ayan Sengupta, Yasser Mohammad, Shinji Nakadai", "title": "An Autonomous Negotiating Agent Framework with Reinforcement Learning\n  Based Strategies and Adaptive Strategy Switching Mechanism", "comments": "Accepted at AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite abundant negotiation strategies in literature, the complexity of\nautomated negotiation forbids a single strategy from being dominant against all\nothers in different negotiation scenarios. To overcome this, one approach is to\nuse mixture of experts, but at the same time, one problem of this method is the\nselection of experts, as this approach is limited by the competency of the\nexperts selected. Another problem with most negotiation strategies is their\nincapability of adapting to dynamic variation of the opponent's behaviour\nwithin a single negotiation session resulting in poor performance. This work\nfocuses on both, solving the problem of expert selection and adapting to the\nopponent's behaviour with our Autonomous Negotiating Agent Framework. This\nframework allows real-time classification of opponent's behaviour and provides\na mechanism to select, switch or combine strategies within a single negotiation\nsession. Additionally, our framework has a reviewer component which enables\nself-enhancement capability by deciding to include new strategies or replace\nold ones with better strategies periodically. We demonstrate an instance of our\nframework by implementing maximum entropy reinforcement learning based\nstrategies with a deep learning based opponent classifier. Finally, we evaluate\nthe performance of our agent against state-of-the-art negotiators under varied\nnegotiation scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 14:38:03 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 11:34:40 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sengupta", "Ayan", ""], ["Mohammad", "Yasser", ""], ["Nakadai", "Shinji", ""]]}, {"id": "2102.03689", "submitter": "Lei Yan", "authors": "Lei Yan, Theodoros Stouraitis, and Sethu Vijayakumar", "title": "Decentralized Ability-Aware Adaptive Control for Multi-robot\n  Collaborative Manipulation", "comments": "The article has been submitted to IEEE Robotics and Automation\n  Letters (RA-L) with ICRA 2021 conference option; the article has been\n  accepted for publication in RA-L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-robot teams can achieve more dexterous, complex and heavier payload\ntasks than a single robot, yet effective collaboration is required. Multi-robot\ncollaboration is extremely challenging due to the different kinematic and\ndynamics capabilities of the robots, the limited communication between them,\nand the uncertainty of the system parameters. In this paper, a Decentralized\nAbility-Aware Adaptive Control is proposed to address these challenges based on\ntwo key features. Firstly, the common manipulation task is represented by the\nproposed nominal task ellipsoid, which is used to maximize each robot force\ncapability online via optimizing its configuration. Secondly, a decentralized\nadaptive controller is designed to be Lyapunov stable in spite of heterogeneous\nactuation constraints of the robots and uncertain physical parameters of the\nobject and environment. In the proposed framework, decentralized coordination\nand load distribution between the robots is achieved without communication,\nwhile only the control deficiency is broadcast if any of the robots reaches its\nforce limits. In this case, the object reference trajectory is modified in a\ndecentralized manner to guarantee stable interaction. Finally, we perform\nseveral numerical and physical simulations to analyse and verify the proposed\nmethod with heterogeneous multi-robot teams in collaborative manipulation\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 00:04:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yan", "Lei", ""], ["Stouraitis", "Theodoros", ""], ["Vijayakumar", "Sethu", ""]]}, {"id": "2102.03840", "submitter": "Chiara Ravazzi", "authors": "Chiara Ravazzi, Giacomo Como, Michele Garetto, Emilio Leonardi,\n  Alberto Tarable", "title": "Asynchronous semi-anonymous dynamics over large-scale networks", "comments": "64 pages, 15 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a class of stochastic processes, referred to as asynchronous and\nsemi-anonymous dynamics (ASD), over directed labeled random networks. These\nprocesses are a natural tool to describe general best-response and noisy\nbest-response dynamics in network games where each agent, at random times\ngoverned by independent Poisson clocks, can choose among a finite set of\nactions. The payoff is determined by the relative popularity of different\nactions among neighbors, while being independent of the specific identities of\nneighbors.\n  Using a mean-field approach, we prove that, under certain conditions on the\nnetwork and initial node configuration, the evolution of ASD can be\napproximated, in the limit of large network sizes, by the solution of a system\nof non-linear ordinary differential equations. Our framework is very general\nand applies to a large class of graph ensembles for which the typical random\ngraph locally behaves like a tree. In particular, we will focus on labeled\nconfiguration-model random graphs, a generalization of the traditional\nconfiguration model which allows different classes of nodes to be mixed\ntogether in the network, permitting us, for example, to incorporate a community\nstructure in the system. Our analysis also applies to configuration-model\ngraphs having a power-law degree distribution, an essential feature of many\nreal systems. To demonstrate the power and flexibility of our framework, we\nconsider several examples of dynamics belonging to our class of stochastic\nprocesses. Moreover, we illustrate by simulation the applicability of our\nanalysis to realistic scenarios by running our example dynamics over a real\nsocial network graph.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 16:37:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ravazzi", "Chiara", ""], ["Como", "Giacomo", ""], ["Garetto", "Michele", ""], ["Leonardi", "Emilio", ""], ["Tarable", "Alberto", ""]]}, {"id": "2102.04361", "submitter": "Daniel Stan", "authors": "Daniel Stan and Anthony Widjaja Lin", "title": "Regular Model Checking Approach to Knowledge Reasoning over\n  Parameterized Systems (technical report)", "comments": "Extended version, version of record accepted at the 20th\n  International Conference on Autonomous Agents and Multiagent Systems\n  (AAMAS-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for modelling and verifying epistemic\nproperties over parameterized multi-agent systems that communicate by truthful\npublic announcements. In our framework, the number of agents or the amount of\ncertain resources are parameterized (i.e. not known a priori), and the\ncorresponding verification problem asks whether a given epistemic property is\ntrue regardless of the instantiation of the parameters. For example, in a muddy\nchildren puzzle, one could ask whether each child will eventually find out\nwhether (s)he is muddy, regardless of the number of children.\n  Our framework is regular model checking (RMC)-based, wherein synchronous\nfinite-state automata (equivalently, monadic second-order logic over words) are\nused to specify the systems. We propose an extension of public announcement\nlogic as specification language. Of special interests is the addition of the\nso-called iterated public announcement operators, which are crucial for\nreasoning about knowledge in parameterized systems. Although the operators make\nthe model checking problem undecidable, we show that this becomes decidable\nwhen an appropriate \"disappearance relation\" is given. Further, we show how\nAngluin's L*-algorithm for learning finite automata can be applied to find a\ndisappearance relation, which is guaranteed to terminate if it is regular. We\nhave implemented the algorithm and apply this to such examples as the Muddy\nChildren Puzzle, the Russian Card Problem, and Large Number Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:10:24 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 21:50:29 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 19:20:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Stan", "Daniel", ""], ["Lin", "Anthony Widjaja", ""]]}, {"id": "2102.04542", "submitter": "Rahul Chandan", "authors": "Rahul Chandan, Dario Paccagnan and Jason R. Marden", "title": "Tractable mechanisms for computing near-optimal utility functions", "comments": "13 pages, 3 figures, to appear in Proceedings of the 20th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale multiagent systems must rely on distributed decision making, as\ncentralized coordination is either impractical or impossible. Recent works\napproach this problem under a game theoretic lens, whereby utility functions\nare assigned to each of the agents with the hope that their local optimization\napproximates the centralized optimal solution. Yet, formal guarantees on the\nresulting performance cannot be obtained for broad classes of problems without\ncompromising on their accuracy. In this work, we address this concern relative\nto the well-studied problem of resource allocation with nondecreasing concave\nwelfare functions. We show that optimally designed local utilities achieve an\napproximation ratio (price of anarchy) of 1-c/e, where c is the function's\ncurvature and e is Euler's constant. The upshot of our contributions is the\ndesign of approximation algorithms that are distributed and efficient, and\nwhose performance matches that of the best existing polynomial-time (and\ncentralized) schemes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:46:56 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Chandan", "Rahul", ""], ["Paccagnan", "Dario", ""], ["Marden", "Jason R.", ""]]}, {"id": "2102.04775", "submitter": "Wenhao Li", "authors": "Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, Yun Hua and Hongyuan\n  Zha", "title": "Structured Diversification Emergence via Reinforced Organization Control\n  and Hierarchical Consensus Learning", "comments": "AAMAS 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When solving a complex task, humans will spontaneously form teams and to\ncomplete different parts of the whole task, respectively. Meanwhile, the\ncooperation between teammates will improve efficiency. However, for current\ncooperative MARL methods, the cooperation team is constructed through either\nheuristics or end-to-end blackbox optimization. In order to improve the\nefficiency of cooperation and exploration, we propose a structured\ndiversification emergence MARL framework named {\\sc{Rochico}} based on\nreinforced organization control and hierarchical consensus learning.\n{\\sc{Rochico}} first learns an adaptive grouping policy through the\norganization control module, which is established by independent multi-agent\nreinforcement learning. Further, the hierarchical consensus module based on the\nhierarchical intentions with consensus constraint is introduced after team\nformation. Simultaneously, utilizing the hierarchical consensus module and a\nself-supervised intrinsic reward enhanced decision module, the proposed\ncooperative MARL algorithm {\\sc{Rochico}} can output the final diversified\nmulti-agent cooperative policy. All three modules are organically combined to\npromote the structured diversification emergence. Comparative experiments on\nfour large-scale cooperation tasks show that {\\sc{Rochico}} is significantly\nbetter than the current SOTA algorithms in terms of exploration efficiency and\ncooperation strength.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:46:12 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Li", "Wenhao", ""], ["Wang", "Xiangfeng", ""], ["Jin", "Bo", ""], ["Sheng", "Junjie", ""], ["Hua", "Yun", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2102.04958", "submitter": "Robyn Kozierok", "authors": "Robyn Kozierok, John Aberdeen, Cheryl Clark, Christopher Garay,\n  Bradley Goodman, Tonia Korves, Lynette Hirschman, Patricia L. McDermott,\n  Matthew W. Peterson", "title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program", "comments": "20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": "MITRE Document Number: MTR210002", "categories": "cs.HC cs.AI cs.CL cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:13:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kozierok", "Robyn", ""], ["Aberdeen", "John", ""], ["Clark", "Cheryl", ""], ["Garay", "Christopher", ""], ["Goodman", "Bradley", ""], ["Korves", "Tonia", ""], ["Hirschman", "Lynette", ""], ["McDermott", "Patricia L.", ""], ["Peterson", "Matthew W.", ""]]}, {"id": "2102.05008", "submitter": "Lewis Hammond", "authors": "Lewis Hammond, James Fox, Tom Everitt, Alessandro Abate, Michael\n  Wooldridge", "title": "Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and\n  Practice", "comments": "Accepted to the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent influence diagrams (MAIDs) are a popular form of graphical model\nthat, for certain classes of games, have been shown to offer key complexity and\nexplainability advantages over traditional extensive form game (EFG)\nrepresentations. In this paper, we extend previous work on MAIDs by introducing\nthe concept of a MAID subgame, as well as subgame perfect and trembling hand\nperfect equilibrium refinements. We then prove several equivalence results\nbetween MAIDs and EFGs. Finally, we describe an open source implementation for\nreasoning about MAIDs and computing their equilibria.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:20:50 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hammond", "Lewis", ""], ["Fox", "James", ""], ["Everitt", "Tom", ""], ["Abate", "Alessandro", ""], ["Wooldridge", "Michael", ""]]}, {"id": "2102.05026", "submitter": "Marco Ciccone", "authors": "Federico Cacciamani, Andrea Celli, Marco Ciccone, Nicola Gatti", "title": "Multi-Agent Coordination in Adversarial Environments through Signal\n  Mediated Strategies", "comments": "Accepted at AAMAS 2021 (full paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world scenarios involve teams of agents that have to coordinate\ntheir actions to reach a shared goal. We focus on the setting in which a team\nof agents faces an opponent in a zero-sum, imperfect-information game. Team\nmembers can coordinate their strategies before the beginning of the game, but\nare unable to communicate during the playing phase of the game. This is the\ncase, for example, in Bridge, collusion in poker, and collusion in bidding. In\nthis setting, model-free RL methods are oftentimes unable to capture\ncoordination because agents' policies are executed in a decentralized fashion.\nOur first contribution is a game-theoretic centralized training regimen to\neffectively perform trajectory sampling so as to foster team coordination. When\nteam members can observe each other actions, we show that this approach\nprovably yields equilibrium strategies. Then, we introduce a signaling-based\nframework to represent team coordinated strategies given a buffer of past\nexperiences. Each team member's policy is parametrized as a neural network\nwhose output is conditioned on a suitable exogenous signal, drawn from a\nlearned probability distribution. By combining these two elements, we\nempirically show convergence to coordinated equilibria in cases where previous\nstate-of-the-art multi-agent RL algorithms did not.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:44:16 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Cacciamani", "Federico", ""], ["Celli", "Andrea", ""], ["Ciccone", "Marco", ""], ["Gatti", "Nicola", ""]]}, {"id": "2102.05037", "submitter": "Wenjie Chu", "authors": "Wenjie Chu, Wei Zhang, Haiyan Zhao, Zhi Jin, Hong Mei", "title": "Massive Self-Assembly in Grid Environments", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-assembly plays an essential role in many natural processes, involving\nthe formation and evolution of living or non-living structures, and shows\npotential applications in many emerging domains. In existing research and\npractice, there still lacks an ideal self-assembly mechanism that manifests\nefficiency, scalability, and stability at the same time. Inspired by phototaxis\nobserved in nature, we propose a computational approach for massive\nself-assembly of connected shapes in grid environments. The key component of\nthis approach is an artificial light field superimposed on a grid environment,\nwhich is determined by the positions of all agents and at the same time drives\nall agents to change their positions, forming a dynamic mutual feedback\nprocess. This work advances the understanding and potential applications of\nself-assembly.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:37:29 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 13:56:59 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chu", "Wenjie", ""], ["Zhang", "Wei", ""], ["Zhao", "Haiyan", ""], ["Jin", "Zhi", ""], ["Mei", "Hong", ""]]}, {"id": "2102.05405", "submitter": "Andrea Vandin", "authors": "Andrea Vandin, Daniele Giachini, Francesco Lamperti, Francesca\n  Chiaromonte", "title": "Automated and Distributed Statistical Analysis of Economic Agent-Based\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.MA cs.PF q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to the statistical analysis of simulation models\nand, especially, agent-based models (ABMs). Our main goal is to provide a fully\nautomated and model-independent tool-kit to inspect simulations and perform\ncounterfactual analysis. Our approach: (i) is easy-to-use by the modeller, (ii)\nimproves reproducibility of results, (iii) optimizes running time given the\nmodeller's machine, (iv) automatically chooses the number of required\nsimulations and simulation steps to reach user-specified statistical\nconfidence, and (v) automatically performs a variety of statistical tests. In\nparticular, our framework is designed to distinguish the transient dynamics of\nthe model from its steady-state behaviour (if any), estimate properties of the\nmodel in both \"phases\", and provide indications on the ergodic (or non-ergodic)\nnature of the simulated processes -- which, in turns allows one to gauge the\nreliability of a steady-state analysis. Estimates are equipped with statistical\nguarantees, allowing for robust comparisons across computational experiments.\nTo demonstrate the effectiveness of our approach, we apply it to two models\nfrom the literature: a large scale macro-financial ABM and a small scale\nprediction market model. Compared to prior analyses of these models, we obtain\nnew insights and we are able to identify and fix some erroneous conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:39:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Vandin", "Andrea", ""], ["Giachini", "Daniele", ""], ["Lamperti", "Francesco", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "2102.05838", "submitter": "Dhruva Kartik", "authors": "Dhruva Kartik, Ashutosh Nayyar, Urbashi Mitra", "title": "Common Information Belief based Dynamic Programs for Stochastic Zero-sum\n  Games with Competing Teams", "comments": "arXiv admin note: text overlap with arXiv:1909.01445", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized team problems where players have asymmetric information about\nthe state of the underlying stochastic system have been actively studied, but\ngames between such teams are less understood. We consider a general model of\nzero-sum stochastic games between two competing teams. This model subsumes many\npreviously considered team and zero-sum game models. For this general model, we\nprovide bounds on the upper (min-max) and lower (max-min) values of the game.\nFurthermore, if the upper and lower values of the game are identical (i.e., if\nthe game has a value), our bounds coincide with the value of the game. Our\nbounds are obtained using two dynamic programs based on a sufficient statistic\nknown as the common information belief (CIB). We also identify certain\ninformation structures in which only the minimizing team controls the evolution\nof the CIB. In these cases, we show that one of our CIB based dynamic programs\ncan be used to find the min-max strategy (in addition to the min-max value). We\npropose an approximate dynamic programming approach for computing the values\n(and the strategy when applicable) and illustrate our results with the help of\nan example.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 04:07:17 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Kartik", "Dhruva", ""], ["Nayyar", "Ashutosh", ""], ["Mitra", "Urbashi", ""]]}, {"id": "2102.06246", "submitter": "Sarah Cen", "authors": "Sarah H. Cen and Devavrat Shah", "title": "Regret, stability, and fairness in matching markets with bandit learners", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the two-sided matching market with bandit learners. In the\nstandard matching problem, users and providers are matched to ensure incentive\ncompatibility via the notion of stability. However, contrary to the core\nassumption of the matching problem, users and providers do not know their true\npreferences a priori and must learn them. To address this assumption, recent\nworks propose to blend the matching and multi-armed bandit problems. They\nestablish that it is possible to assign matchings that are stable (i.e.,\nincentive-compatible) at every time step while also allowing agents to learn\nenough so that the system converges to matchings that are stable under the\nagents' true preferences. However, while some agents may incur low regret under\nthese matchings, others can incur high regret -- specifically, $\\Omega(T)$\noptimal regret where $T$ is the time horizon. In this work, we incorporate\ncosts and transfers in the two-sided matching market with bandit learners in\norder to faithfully model competition between agents. We prove that, under our\nframework, it is possible to simultaneously guarantee four desiderata: (1)\nincentive compatibility, i.e., stability, (2) low regret, i.e., $O(\\log(T))$\noptimal regret, (3) fairness in the distribution of regret among agents, and\n(4) high social welfare.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 20:18:12 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cen", "Sarah H.", ""], ["Shah", "Devavrat", ""]]}, {"id": "2102.06265", "submitter": "Matthew Malencia", "authors": "Matthew Malencia, Vijay Kumar, George Pappas, Amanda Prorok", "title": "Fair Robust Assignment using Redundancy", "comments": "(c) 2021 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the consideration of fairness in redundant assignment for\nmulti-agent task allocation. It has recently been shown that redundant\nassignment of agents to tasks provides robustness to uncertainty in task\nperformance. However, the question of how to fairly assign these redundant\nresources across tasks remains unaddressed. In this paper, we present a novel\nproblem formulation for fair redundant task allocation, which we cast as the\noptimization of worst-case task costs under a cardinality constraint. Solving\nthis problem optimally is NP-hard. We exploit properties of supermodularity to\npropose a polynomial-time, near-optimal solution. In supermodular redundant\nassignment, the use of additional agents always improves task costs. Therefore,\nwe provide a solution set that is $\\alpha$ times larger than the cardinality\nconstraint. This constraint relaxation enables our approach to achieve a\nsuper-optimal cost by using a sub-optimal assignment size. We derive the\nsub-optimality bound on this cardinality relaxation, $\\alpha$. Additionally, we\ndemonstrate that our algorithm performs near-optimally without the cardinality\nrelaxation. We show simulations of redundant assignments of robots to goal\nnodes on transport networks with uncertain travel times. Empirically, our\nalgorithm outperforms benchmarks, scales to large problems, and provides\nimprovements in both fairness and average utility.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 20:48:16 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:47:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Malencia", "Matthew", ""], ["Kumar", "Vijay", ""], ["Pappas", "George", ""], ["Prorok", "Amanda", ""]]}, {"id": "2102.06440", "submitter": "Vikram Manjunath", "authors": "Vikram Manjunath and Thayer Morrill", "title": "Interview Hoarding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many centralized matching markets are preceded by interviews between the\nparticipants. We study the impact on the final match of an increase in the\nnumber of interviews for one side of the market. Our motivation is the match\nbetween residents and hospitals where, due to the COVID-19 pandemic, interviews\nfor the 2020-21 season of the National Residency Matching Program were switched\nto a virtual format. This drastically reduced the cost to applicants of\naccepting interview invitations. However, the reduction in cost was not\nsymmetric since applicants, not programs, previously bore most of the costs of\nin-person interviews. We show that, starting from a situation where the final\nmatching is stable, if doctors can accept more interviews, but the hospitals do\nnot increase the number of interviews they offer, then no doctor is better off\nand many doctors are potentially harmed. This adverse consequence is the result\nof what we call interview hoarding. We prove this analytically and characterize\noptimal mitigation strategies for special cases. We use simulations to extend\nthese insights to more general settings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 11:06:18 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 20:38:15 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 17:31:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Manjunath", "Vikram", ""], ["Morrill", "Thayer", ""]]}, {"id": "2102.06607", "submitter": "Sabrina Kirrane", "authors": "Sabrina Kirrane", "title": "Intelligent Software Web Agents: A Gap Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic web technologies have shown their effectiveness, especially when it\ncomes to knowledge representation, reasoning, and data integration. However,\nthe original semantic web vision, whereby machine readable web data could be\nautomatically actioned upon by intelligent software web agents, has yet to be\nrealised. In order to better understand the existing technological\nopportunities and challenges, in this paper we examine the status quo in terms\nof intelligent software web agents, guided by research with respect to\nrequirements and architectural components, coming from the agents community. We\nuse the identified requirements to both further elaborate on the semantic web\nagent motivating use case scenario, and to summarise different perspectives on\nthe requirements from the semantic web agent literature. We subsequently\npropose a hybrid semantic web agent architecture, and use the various\ncomponents and subcomponents in order to provide a focused discussion on the\nrole played by existing semantic web standards and community activities.\nFinally, we highlight open research opportunities and challenges and take a\nbroader perspective of the research by discussing the potential for intelligent\nsoftware web agents as an enabling technology for emerging domains, such as\ndigital assistants, cloud computing, and the internet of things.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:32:02 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 11:23:15 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 11:35:30 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kirrane", "Sabrina", ""]]}, {"id": "2102.06752", "submitter": "Usman Khan", "authors": "Ran Xin and Usman A. Khan and Soummya Kar", "title": "A Hybrid Variance-Reduced Method for Decentralized Stochastic Non-Convex\n  Optimization", "comments": "Accepted in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers decentralized stochastic optimization over a network of\n$n$ nodes, where each node possesses a smooth non-convex local cost function\nand the goal of the networked nodes is to find an $\\epsilon$-accurate\nfirst-order stationary point of the sum of the local costs. We focus on an\nonline setting, where each node accesses its local cost only by means of a\nstochastic first-order oracle that returns a noisy version of the exact\ngradient. In this context, we propose a novel single-loop decentralized hybrid\nvariance-reduced stochastic gradient method, called GT-HSGD, that outperforms\nthe existing approaches in terms of both the oracle complexity and practical\nimplementation. The GT-HSGD algorithm implements specialized local hybrid\nstochastic gradient estimators that are fused over the network to track the\nglobal gradient. Remarkably, GT-HSGD achieves a network topology-independent\noracle complexity of $O(n^{-1}\\epsilon^{-3})$ when the required error tolerance\n$\\epsilon$ is small enough, leading to a linear speedup with respect to the\ncentralized optimal online variance-reduced approaches that operate on a single\nnode. Numerical experiments are provided to illustrate our main technical\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:13:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 16:03:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xin", "Ran", ""], ["Khan", "Usman A.", ""], ["Kar", "Soummya", ""]]}, {"id": "2102.06911", "submitter": "Michiel Bakker", "authors": "Michiel A. Bakker, Richard Everett, Laura Weidinger, Iason Gabriel,\n  William S. Isaac, Joel Z. Leibo, Edward Hughes", "title": "Modelling Cooperation in Network Games with Spatio-Temporal Complexity", "comments": "AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real world is awash with multi-agent problems that require collective\naction by self-interested agents, from the routing of packets across a computer\nnetwork to the management of irrigation systems. Such systems have local\nincentives for individuals, whose behavior has an impact on the global outcome\nfor the group. Given appropriate mechanisms describing agent interaction,\ngroups may achieve socially beneficial outcomes, even in the face of short-term\nselfish incentives. In many cases, collective action problems possess an\nunderlying graph structure, whose topology crucially determines the\nrelationship between local decisions and emergent global effects. Such\nscenarios have received great attention through the lens of network games.\nHowever, this abstraction typically collapses important dimensions, such as\ngeometry and time, relevant to the design of mechanisms promoting cooperation.\nIn parallel work, multi-agent deep reinforcement learning has shown great\npromise in modelling the emergence of self-organized cooperation in complex\ngridworld domains. Here we apply this paradigm in graph-structured collective\naction problems. Using multi-agent deep reinforcement learning, we simulate an\nagent society for a variety of plausible mechanisms, finding clear transitions\nbetween different equilibria over time. We define analytic tools inspired by\nrelated literatures to measure the social outcomes, and use these to draw\nconclusions about the efficacy of different environmental interventions. Our\nmethods have implications for mechanism design in both human and artificial\nagent systems.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 12:04:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bakker", "Michiel A.", ""], ["Everett", "Richard", ""], ["Weidinger", "Laura", ""], ["Gabriel", "Iason", ""], ["Isaac", "William S.", ""], ["Leibo", "Joel Z.", ""], ["Hughes", "Edward", ""]]}, {"id": "2102.06954", "submitter": "M Anandaraj", "authors": "S Naganandhini, M Anandaraj, S Manojkumar, K Selvaraj, P Ganeshkumar", "title": "An Efficient Framework for Piece Selection Problem in P2P Content\n  Distribution Network Using Fuzzy Programming Approach", "comments": "18 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fuzzy programming approach is used in this article for solving the piece\nselection problem in P2P network with multiple objectives, in which some of the\nfactors are fuzzy in nature. A piece selection problem has been prepared as a\nfuzzy mixed integer goal programming piece selection problem that includes\nthree primary goals: minimizing the download cost and download time and\nmaximizing speed and useful information transmission subject to realistic\nconstraints regarding peer's demand, peer's capacity, peer's dynamicity, etc.\nThe proposed approach has the ability to handle practical situations in a fuzzy\nenvironment and offers a better decision tool for the piece selection decision\nin a dynamic P2P network. An extensive simulation is carried out to demonstrate\nthe effectiveness of the proposed model. The proposed mechanism has the\ncapability to handle practical situations in a fuzzy environment and offers a\nbetter decision tool for the piece selection decision in a decentralized P2P\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 16:47:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Naganandhini", "S", ""], ["Anandaraj", "M", ""], ["Manojkumar", "S", ""], ["Selvaraj", "K", ""], ["Ganeshkumar", "P", ""]]}, {"id": "2102.07017", "submitter": "Sandhya Saisubramanian", "authors": "Sandhya Saisubramanian and Shlomo Zilberstein", "title": "Mitigating Negative Side Effects via Environment Shaping", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents operating in unstructured environments often produce negative side\neffects (NSE), which are difficult to identify at design time. While the agent\ncan learn to mitigate the side effects from human feedback, such feedback is\noften expensive and the rate of learning is sensitive to the agent's state\nrepresentation. We examine how humans can assist an agent, beyond providing\nfeedback, and exploit their broader scope of knowledge to mitigate the impacts\nof NSE. We formulate this problem as a human-agent team with decoupled\nobjectives. The agent optimizes its assigned task, during which its actions may\nproduce NSE. The human shapes the environment through minor reconfiguration\nactions so as to mitigate the impacts of the agent's side effects, without\naffecting the agent's ability to complete its assigned task. We present an\nalgorithm to solve this problem and analyze its theoretical properties. Through\nexperiments with human subjects, we assess the willingness of users to perform\nminor environment modifications to mitigate the impacts of NSE. Empirical\nevaluation of our approach shows that the proposed framework can successfully\nmitigate NSE, without affecting the agent's ability to complete its assigned\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 22:15:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Saisubramanian", "Sandhya", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "2102.07152", "submitter": "Tao Zhang", "authors": "Tao Zhang, Quanyan Zhu", "title": "On the Equilibrium Elicitation of Markov Games Through Information\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a novel information design problem and studies how the\ncraft of payoff-relevant environmental signals solely can influence the\nbehaviors of intelligent agents. The agents' strategic interactions are\ncaptured by an incomplete-information Markov game, in which each agent first\nselects one environmental signal from multiple signal sources as additional\npayoff-relevant information and then takes an action. There is a rational\ninformation designer (designer) who possesses one signal source and aims to\ncontrol the equilibrium behaviors of the agents by designing the information\nstructure of her signals sent to the agents. An obedient principle is\nestablished which states that it is without loss of generality to focus on the\ndirect information design when the information design incentivizes each agent\nto select the signal sent by the designer, such that the design process avoids\nthe predictions of the agents' strategic selection behaviors. We then introduce\nthe design protocol given a goal of the designer referred to as obedient\nimplementability (OIL) and characterize the OIL in a class of obedient perfect\nBayesian Markov Nash equilibria (O-PBME). A new framework for information\ndesign is proposed based on an approach of maximizing the optimal slack\nvariables. Finally, we formulate the designer's goal selection problem and\ncharacterize it in terms of information design by establishing a relationship\nbetween the O-PBME and the Bayesian Markov correlated equilibria, in which we\nbuild upon the revelation principle in classic information design in economics.\nThe proposed approach can be applied to elicit desired behaviors of multi-agent\nsystems in competing as well as cooperating settings and be extended to\nheterogeneous stochastic games in the complete- and the incomplete-information\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 13:30:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Tao", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2102.07166", "submitter": "Thang X. Vu", "authors": "Arsham Mostaani, Thang X. Vu, Shree Krishna Sharma, Qi Liao, and\n  Symeon Chatzinotas", "title": "Task-oriented Communication System Design in Cyber-Physical Systems: A\n  Survey on Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.MA math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communications system design has been traditionally guided by task-agnostic\nprinciples, which aim at efficiently transmitting as many correct bits as\npossible through a given channel. However, in the era of cyber-physical\nsystems, the effectiveness of communications is not dictated simply by the bit\nrate, but most importantly by the efficient completion of the task in hand,\ne.g., controlling remotely a robot, automating a production line or\ncollaboratively sensing through a drone swarm. In parallel, it is projected\nthat by 2023, half of the worldwide network connections will be among machines\nrather than humans. In this context, it is crucial to establish a new paradigm\nfor designing communications strategies for multi-agent cyber-physical systems.\nThis is a daunting task, since it requires a combination of principles from\ninformation, communication, control theories and computer science in order to\nformalize a general framework for task-oriented communication design. In this\ndirection, this paper reviews and structures the relevant theoretical work\nacross a wide range of scientific communities. Subsequently, it proposes a\ngeneral conceptual framework for task-oriented communication design, along with\nits specializations according to the targeted use case. Furthermore, it\nprovides a survey of relevant contributions in dominant applications, such as\nindustrial internet of things, multi-UAV systems, tactile internet, autonomous\nvehicles, distributed learning systems, internet of skills, smart manufacturing\nplants and 5G and beyond self-organizing networks. Finally, it highlights the\nmost important open research topics from both the theoretical framework and\napplication points of view.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 14:51:38 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 10:04:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Mostaani", "Arsham", ""], ["Vu", "Thang X.", ""], ["Sharma", "Shree Krishna", ""], ["Liao", "Qi", ""], ["Chatzinotas", "Symeon", ""]]}, {"id": "2102.07185", "submitter": "Rotem Lev Lehman", "authors": "Rotem Lev Lehman (1), Guy Shani (1), Roni Stern (1 and 2) ((1)\n  Software and Information Systems Engineering, Ben Gurion University of the\n  Negev, Be'er Sheva, Israel, (2) Palo Alto Research Center, Palo Alto, CA,\n  USA)", "title": "Partial Disclosure of Private Dependencies in Privacy Preserving\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In collaborative privacy preserving planning (CPPP), a group of agents\njointly creates a plan to achieve a set of goals while preserving each others'\nprivacy. During planning, agents often reveal the private dependencies between\ntheir public actions to other agents, that is, which public action facilitates\nthe preconditions of another public action. Previous work in CPPP does not\nlimit the disclosure of such dependencies. In this paper, we explicitly limit\nthe amount of disclosed dependencies, allowing agents to publish only a part of\ntheir private dependencies. We investigate different strategies for deciding\nwhich dependencies to publish, and how they affect the ability to find\nsolutions. We evaluate the ability of two solvers -- distribute forward search\nand centralized planning based on a single-agent projection -- to produce plans\nunder this constraint. Experiments over standard CPPP domains show that the\nproposed dependency-sharing strategies enable generating plans while sharing\nonly a small fraction of all private dependencies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 16:10:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lehman", "Rotem Lev", "", "1 and 2"], ["Shani", "Guy", "", "1 and 2"], ["Stern", "Roni", "", "1 and 2"]]}, {"id": "2102.07308", "submitter": "Xintong Wang", "authors": "Miroslav Dud\\'ik and Xintong Wang and David M. Pennock and David M.\n  Rothschild", "title": "Log-time Prediction Markets for Interval Securities", "comments": "To appear in AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a prediction market to recover a complete and fully general\nprobability distribution over a random variable. Traders buy and sell interval\nsecurities that pay \\$1 if the outcome falls into an interval and \\$0\notherwise. Our market takes the form of a central automated market maker and\nallows traders to express interval endpoints of arbitrary precision. We present\ntwo designs in both of which market operations take time logarithmic in the\nnumber of intervals (that traders distinguish), providing the first\ncomputationally efficient market for a continuous variable. Our first design\nreplicates the popular logarithmic market scoring rule (LMSR), but operates\nexponentially faster than a standard LMSR by exploiting its modularity\nproperties to construct a balanced binary tree and decompose computations along\nthe tree nodes. The second design consists of two or more parallel LMSR market\nmakers that mediate submarkets of increasingly fine-grained outcome partitions.\nThis design remains computationally efficient for all operations, including\narbitrage removal across submarkets. It adds two additional benefits for the\nmarket designer: (1) the ability to express utility for information at various\nresolutions by assigning different liquidity values, and (2) the ability to\nguarantee a true constant bounded loss by appropriately decreasing the\nliquidity in each submarket.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 02:26:53 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 01:56:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Dud\u00edk", "Miroslav", ""], ["Wang", "Xintong", ""], ["Pennock", "David M.", ""], ["Rothschild", "David M.", ""]]}, {"id": "2102.07381", "submitter": "Shridhar Velhal", "authors": "Shridhar Velhal, Suresh Sundaram, and Narasimhan Sundararajan", "title": "A Decentralized Multi-UAV Spatio-Temporal Multi-Task Allocation Approach\n  for Perimeter Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a new solution approach to a multi-player perimeter\ndefense game, in which the intruders' team tries to enter the territory, and a\nteam of defenders protects the territory by capturing intruders on the\nperimeter of the territory. The objective of the defenders is to detect and\ncapture the intruders before the intruders enter the territory. Each defender\nindependently senses the intruder and computes his trajectory to capture the\nassigned intruders in a cooperative fashion. The intruder is estimated to reach\na specific location on the perimeter at a specific time. Each intruder is\nviewed as a spatio-temporal task, and the defenders are assigned to execute\nthese spatio-temporal tasks. At any given time, the perimeter defense problem\nis converted into a Decentralized Multi-UAV Spatio-Temporal Multi-Task\nAllocation (DMUST-MTA) problem. The cost of executing a task for a trajectory\nis defined by a composite cost function of both the spatial and temporal\ncomponents. In this paper, a decentralized consensus-based bundle algorithm has\nbeen modified to solve the spatio-temporal multi-task allocation problem, and\nthe performance evaluation of the proposed approach is carried out based on\nMonte-Carlo simulations. The simulation results show the effectiveness of the\nproposed approach to solve the perimeter defense game under different\nscenarios. Performance comparison with a state-of-the-art centralized approach\nwith full observability, clearly indicates that DMUST-MTA achieves similar\nperformance in a decentralized way with partial observability conditions with a\nlesser computational time and easy scaling up.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:45:56 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Velhal", "Shridhar", ""], ["Sundaram", "Suresh", ""], ["Sundararajan", "Narasimhan", ""]]}, {"id": "2102.07475", "submitter": "Filippos Christianos", "authors": "Filippos Christianos, Georgios Papoudakis, Arrasy Rahman, Stefano V.\n  Albrecht", "title": "Scaling Multi-Agent Reinforcement Learning with Selective Parameter\n  Sharing", "comments": "To be published In Proceedings of the 38th International Conference\n  on Machine Learning (ICML), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing parameters in multi-agent deep reinforcement learning has played an\nessential role in allowing algorithms to scale to a large number of agents.\nParameter sharing between agents significantly decreases the number of\ntrainable parameters, shortening training times to tractable levels, and has\nbeen linked to more efficient learning. However, having all agents share the\nsame parameters can also have a detrimental effect on learning. We demonstrate\nthe impact of parameter sharing methods on training speed and converged\nreturns, establishing that when applied indiscriminately, their effectiveness\nis highly dependent on the environment. We propose a novel method to\nautomatically identify agents which may benefit from sharing parameters by\npartitioning them based on their abilities and goals. Our approach combines the\nincreased sample efficiency of parameter sharing with the representational\ncapacity of multiple independent networks to reduce training time and increase\nfinal returns.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:33:52 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:41:01 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Christianos", "Filippos", ""], ["Papoudakis", "Georgios", ""], ["Rahman", "Arrasy", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "2102.07505", "submitter": "Johannes Nguyen", "authors": "Johannes Nguyen, Simon T. Powers, Neil Urquhart, Thomas Farrenkopf,\n  Michael Guckert", "title": "An Overview of Agent-based Traffic Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most countries population in urban areas is growing, while available\ntravel infrastructure and resources are limited. At the same time desires to\nminimise environmental impact and energy use have led to new requirements in\nthe field of inner-city transportation. As a result, the portfolio of mobility\nservices provided is developing in order to improve the use of the available\nresources. Computer-based simulation is an accepted means for investigating the\neffects of new transportation policies and services. Many researchers are faced\nwith the question of choosing a suitable simulator for their specific research\nquestion. In this paper, we review a broad spectrum of recent and historically\nimportant applications, in order to provide an overview of available work and\nto help researchers make a more informed decision on the selection of a\nsuitable simulator. We discuss strengths and weaknesses of the applications and\nidentify gaps for which we argue that more detailed work is required.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:13:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Nguyen", "Johannes", ""], ["Powers", "Simon T.", ""], ["Urquhart", "Neil", ""], ["Farrenkopf", "Thomas", ""], ["Guckert", "Michael", ""]]}, {"id": "2102.07523", "submitter": "Nicolas Anastassacos", "authors": "Nicolas Anastassacos, Julian Garc\\'ia, Stephen Hailes, Mirco Musolesi", "title": "Cooperation and Reputation Dynamics with Reinforcement Learning", "comments": "Published in AAMAS'21, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating incentives for cooperation is a challenge in natural and artificial\nsystems. One potential answer is reputation, whereby agents trade the immediate\ncost of cooperation for the future benefits of having a good reputation. Game\ntheoretical models have shown that specific social norms can make cooperation\nstable, but how agents can independently learn to establish effective\nreputation mechanisms on their own is less understood. We use a simple model of\nreinforcement learning to show that reputation mechanisms generate two\ncoordination problems: agents need to learn how to coordinate on the meaning of\nexisting reputations and collectively agree on a social norm to assign\nreputations to others based on their behavior. These coordination problems\nexhibit multiple equilibria, some of which effectively establish cooperation.\nWhen we train agents with a standard Q-learning algorithm in an environment\nwith the presence of reputation mechanisms, convergence to undesirable\nequilibria is widespread. We propose two mechanisms to alleviate this: (i)\nseeding a proportion of the system with fixed agents that steer others towards\ngood equilibria; and (ii), intrinsic rewards based on the idea of\nintrospection, i.e., augmenting agents' rewards by an amount proportionate to\nthe performance of their own strategy against themselves. A combination of\nthese simple mechanisms is successful in stabilizing cooperation, even in a\nfully decentralized version of the problem where agents learn to use and assign\nreputations simultaneously. We show how our results relate to the literature in\nEvolutionary Game Theory, and discuss implications for artificial, human and\nhybrid systems, where reputations can be used as a way to establish trust and\ncooperation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:48:56 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Anastassacos", "Nicolas", ""], ["Garc\u00eda", "Julian", ""], ["Hailes", "Stephen", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2102.07659", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Jun Luo, Ying Wen, Oliver Slumbers, Daniel Graves,\n  Haitham Bou Ammar, Jun Wang, Matthew E. Taylor", "title": "Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent\n  Learning Systems", "comments": "AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent reinforcement learning (MARL) has achieved a remarkable amount of\nsuccess in solving various types of video games. A cornerstone of this success\nis the auto-curriculum framework, which shapes the learning process by\ncontinually creating new challenging tasks for agents to adapt to, thereby\nfacilitating the acquisition of new skills. In order to extend MARL methods to\nreal-world domains outside of video games, we envision in this blue sky paper\nthat maintaining a diversity-aware auto-curriculum is critical for successful\nMARL applications. Specifically, we argue that \\emph{behavioural diversity} is\na pivotal, yet under-explored, component for real-world multiagent learning\nsystems, and that significant work remains in understanding how to design a\ndiversity-aware auto-curriculum. We list four open challenges for\nauto-curriculum techniques, which we believe deserve more attention from this\ncommunity. Towards validating our vision, we recommend modelling realistic\ninteractive behaviours in autonomous driving as an important test bed, and\nrecommend the SMARTS/ULTRA benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:40:02 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 11:31:07 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Yang", "Yaodong", ""], ["Luo", "Jun", ""], ["Wen", "Ying", ""], ["Slumbers", "Oliver", ""], ["Graves", "Daniel", ""], ["Ammar", "Haitham Bou", ""], ["Wang", "Jun", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "2102.07767", "submitter": "Mohammad Taha Toghani", "authors": "Mohammad Taha Toghani, Cesar A. Uribe", "title": "Communication-Efficient Distributed Cooperative Learning with Compressed\n  Beliefs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed cooperative learning, where a group of\nagents seek to agree on a set of hypotheses that best describes a sequence of\nprivate observations. In the scenario where the set of hypotheses is large, we\npropose a belief update rule where agents share compressed (either sparse or\nquantized) beliefs with an arbitrary positive compression rate. Our algorithm\nleverages a unified and straightforward communication rule that enables agents\nto access wide-ranging compression operators as black-box modules. We prove the\nalmost sure asymptotic exponential convergence of beliefs around the set of\noptimal hypotheses. Additionally, we show a non-asymptotic, explicit, and\nlinear concentration rate in probability of the beliefs on the optimal\nhypothesis set. We provide numerical experiments to illustrate the\ncommunication benefits of our method. The simulation results show that the\nnumber of transmitted bits can be reduced to 5-10% of the non-compressed method\nin the studied scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 06:19:36 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Toghani", "Mohammad Taha", ""], ["Uribe", "Cesar A.", ""]]}, {"id": "2102.07936", "submitter": "Wei-Fang Sun", "authors": "Wei-Fang Sun, Cheng-Kuang Lee, Chun-Yi Lee", "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for\n  Multi-Agent Distributional Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fully cooperative multi-agent reinforcement learning (MARL) settings, the\nenvironments are highly stochastic due to the partial observability of each\nagent and the continuously changing policies of the other agents. To address\nthe above issues, we integrate distributional RL and value function\nfactorization methods by proposing a Distributional Value Function\nFactorization (DFAC) framework to generalize expected value function\nfactorization methods to their DFAC variants. DFAC extends the individual\nutility functions from deterministic variables to random variables, and models\nthe quantile function of the total return as a quantile mixture. To validate\nDFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game\nwith stochastic rewards and perform experiments on all Super Hard tasks of\nStarCraft Multi-Agent Challenge, showing that DFAC is able to outperform\nexpected value function factorization baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 03:16:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Sun", "Wei-Fang", ""], ["Lee", "Cheng-Kuang", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "2102.08159", "submitter": "Wei Qiu", "authors": "Wei Qiu, Xinrun Wang, Runsheng Yu, Xu He, Rundong Wang, Bo An,\n  Svetlana Obraztsova, Zinovi Rabinovich", "title": "RMIX: Learning Risk-Sensitive Policies for Cooperative Reinforcement\n  Learning Agents", "comments": "ICLR 2021 submission version:\n  https://openreview.net/forum?id=1EVb8XRBDNr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current value-based multi-agent reinforcement learning methods optimize\nindividual Q values to guide individuals' behaviours via centralized training\nwith decentralized execution (CTDE). However, such expected, i.e.,\nrisk-neutral, Q value is not sufficient even with CTDE due to the randomness of\nrewards and the uncertainty in environments, which causes the failure of these\nmethods to train coordinating agents in complex environments. To address these\nissues, we propose RMIX, a novel cooperative MARL method with the Conditional\nValue at Risk (CVaR) measure over the learned distributions of individuals' Q\nvalues. Specifically, we first learn the return distributions of individuals to\nanalytically calculate CVaR for decentralized execution. Then, to handle the\ntemporal nature of the stochastic outcomes during executions, we propose a\ndynamic risk level predictor for risk level tuning. Finally, we optimize the\nCVaR policies with CVaR values used to estimate the target in TD error during\ncentralized training and the CVaR values are used as auxiliary local rewards to\nupdate the local distribution via Quantile Regression loss. Empirically, we\nshow that our method significantly outperforms state-of-the-art methods on\nchallenging StarCraft II tasks, demonstrating enhanced coordination and\nimproved sample efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:58:25 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 02:02:32 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 05:38:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Qiu", "Wei", ""], ["Wang", "Xinrun", ""], ["Yu", "Runsheng", ""], ["He", "Xu", ""], ["Wang", "Rundong", ""], ["An", "Bo", ""], ["Obraztsova", "Svetlana", ""], ["Rabinovich", "Zinovi", ""]]}, {"id": "2102.08370", "submitter": "Kevin McKee", "authors": "Kevin R. McKee and Joel Z. Leibo and Charlie Beattie and Richard\n  Everett", "title": "Quantifying environment and population diversity in multi-agent\n  reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is a major challenge for multi-agent reinforcement learning.\nHow well does an agent perform when placed in novel environments and in\ninteractions with new co-players? In this paper, we investigate and quantify\nthe relationship between generalization and diversity in the multi-agent\ndomain. Across the range of multi-agent environments considered here,\nprocedurally generating training levels significantly improves agent\nperformance on held-out levels. However, agent performance on the specific\nlevels used in training sometimes declines as a result. To better understand\nthe effects of co-player variation, our experiments introduce a new\nenvironment-agnostic measure of behavioral diversity. Results demonstrate that\npopulation size and intrinsic motivation are both effective methods of\ngenerating greater population diversity. In turn, training with a diverse set\nof co-players strengthens agent performance in some (but not all) cases.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:54:39 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["McKee", "Kevin R.", ""], ["Leibo", "Joel Z.", ""], ["Beattie", "Charlie", ""], ["Everett", "Richard", ""]]}, {"id": "2102.08462", "submitter": "Mridul Agarwal", "authors": "Mridul Agarwal, Vaneet Aggarwal, Kamyar Azizzadenesheli", "title": "Multi-Agent Multi-Armed Bandits with Limited Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem where $N$ agents collaboratively interact with an\ninstance of a stochastic $K$ arm bandit problem for $K \\gg N$. The agents aim\nto simultaneously minimize the cumulative regret over all the agents for a\ntotal of $T$ time steps, the number of communication rounds, and the number of\nbits in each communication round. We present Limited Communication\nCollaboration - Upper Confidence Bound (LCC-UCB), a doubling-epoch based\nalgorithm where each agent communicates only after the end of the epoch and\nshares the index of the best arm it knows. With our algorithm, LCC-UCB, each\nagent enjoys a regret of $\\tilde{O}\\left(\\sqrt{({K/N}+ N)T}\\right)$,\ncommunicates for $O(\\log T)$ steps and broadcasts $O(\\log K)$ bits in each\ncommunication step. We extend the work to sparse graphs with maximum degree\n$K_G$, and diameter $D$ and propose LCC-UCB-GRAPH which enjoys a regret bound\nof $\\tilde{O}\\left(D\\sqrt{(K/N+ K_G)DT}\\right)$. Finally, we empirically show\nthat the LCC-UCB and the LCC-UCB-GRAPH algorithm perform well and outperform\nstrategies that communicate through a central node\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 06:28:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Agarwal", "Mridul", ""], ["Aggarwal", "Vaneet", ""], ["Azizzadenesheli", "Kamyar", ""]]}, {"id": "2102.08507", "submitter": "Vaibhav Vasant Unhelkar", "authors": "Sangwon Seo, Lauren R. Kennedy-Metz, Marco A. Zenati, Julie A. Shah,\n  Roger D. Dias, Vaibhav V. Unhelkar", "title": "Towards an AI Coach to Infer Team Mental Model Alignment in Healthcare", "comments": "Submitted to the 2021 IEEE Conference on Cognitive and Computational\n  Aspects of Situation Management (CogSIMA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared mental models are critical to team success; however, in practice, team\nmembers may have misaligned models due to a variety of factors. In\nsafety-critical domains (e.g., aviation, healthcare), lack of shared mental\nmodels can lead to preventable errors and harm. Towards the goal of mitigating\nsuch preventable errors, here, we present a Bayesian approach to infer\nmisalignment in team members' mental models during complex healthcare task\nexecution. As an exemplary application, we demonstrate our approach using two\nsimulated team-based scenarios, derived from actual teamwork in cardiac\nsurgery. In these simulated experiments, our approach inferred model\nmisalignment with over 75% recall, thereby providing a building block for\nenabling computer-assisted interventions to augment human cognition in the\noperating room and improve teamwork.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:14:08 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Seo", "Sangwon", ""], ["Kennedy-Metz", "Lauren R.", ""], ["Zenati", "Marco A.", ""], ["Shah", "Julie A.", ""], ["Dias", "Roger D.", ""], ["Unhelkar", "Vaibhav V.", ""]]}, {"id": "2102.08814", "submitter": "Majid Raeis", "authors": "Majid Raeis, S. Jamaloddin Golestani", "title": "Distributed Fair Scheduling for Information Exchange in Multi-Agent\n  Systems", "comments": "9 pages, Accepted at ICAPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information exchange is a crucial component of many real-world multi-agent\nsystems. However, the communication between the agents involves two major\nchallenges: the limited bandwidth, and the shared communication medium between\nthe agents, which restricts the number of agents that can simultaneously\nexchange information. While both of these issues need to be addressed in\npractice, the impact of the latter problem on the performance of the\nmulti-agent systems has often been neglected. This becomes even more important\nwhen the agents' information or observations have different importance, in\nwhich case the agents require different priorities for accessing the medium and\nsharing their information. Representing the agents' priorities by fairness\nweights and normalizing each agent's share by the assigned fairness weight, the\ngoal can be expressed as equalizing the agents' normalized shares of the\ncommunication medium. To achieve this goal, we adopt a queueing theoretic\napproach and propose a distributed fair scheduling algorithm for providing\nweighted fairness in single-hop networks. Our proposed algorithm guarantees an\nupper-bound on the normalized share disparity among any pair of agents. This\ncan particularly improve the short-term fairness, which is important in\nreal-time applications. Moreover, our scheduling algorithm adjusts itself\ndynamically to achieve a high throughput at the same time. The simulation\nresults validate our claims and comparisons with the existing methods show our\nalgorithm's superiority in providing short-term fairness, while achieving a\nhigh throughput.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:20:26 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Raeis", "Majid", ""], ["Golestani", "S. Jamaloddin", ""]]}, {"id": "2102.08915", "submitter": "S. Rasoul Etesami", "authors": "S. Rasoul Etesami", "title": "Maximizing Social Welfare Subject to Network Externalities: A Unifying\n  Submodular Optimization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.MA cs.SY eess.SY math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of allocating multiple indivisible items to a set of\nnetworked agents to maximize the social welfare subject to network\nexternalities. Here, the social welfare is given by the sum of agents'\nutilities and externalities capture the effect that one user of an item has on\nthe item's value to others. We first provide a general formulation that\ncaptures some of the existing models as a special case. We then show that the\nsocial welfare maximization problem benefits some nice diminishing or\nincreasing marginal return properties. That allows us to devise polynomial-time\napproximation algorithms using the Lovasz extension and multilinear extension\nof the objective functions. Our principled approach recovers or improves some\nof the existing algorithms and provides a simple and unifying framework for\nmaximizing social welfare subject to network externalities.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:12:32 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 04:18:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Etesami", "S. Rasoul", ""]]}, {"id": "2102.09104", "submitter": "Neng Wan", "authors": "Neng Wan, Aditya Gahlawat, Naira Hovakimyan, Evangelos A. Theodorou,\n  Petros G. Voulgaris", "title": "Distributed Algorithms for Linearly-Solvable Optimal Control in\n  Networked Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms for both discrete-time and continuous-time linearly\nsolvable optimal control (LSOC) problems of networked multi-agent systems\n(MASs) are investigated in this paper. A distributed framework is proposed to\npartition the optimal control problem of a networked MAS into several local\noptimal control problems in factorial subsystems, such that each (central)\nagent behaves optimally to minimize the joint cost function of a subsystem that\ncomprises a central agent and its neighboring agents, and the local control\nactions (policies) only rely on the knowledge of local observations. Under this\nframework, we not only preserve the correlations between neighboring agents,\nbut moderate the communication and computational complexities by decentralizing\nthe sampling and computational processes over the network. For discrete-time\nsystems modeled by Markov decision processes, the joint Bellman equation of\neach subsystem is transformed into a system of linear equations and solved\nusing parallel programming. For continuous-time systems modeled by It\\^o\ndiffusion processes, the joint optimality equation of each subsystem is\nconverted into a linear partial differential equation, whose solution is\napproximated by a path integral formulation and a sample-efficient relative\nentropy policy search algorithm, respectively. The learned control policies are\ngeneralized to solve the unlearned tasks by resorting to the compositionality\nprinciple, and illustrative examples of cooperative UAV teams are provided to\nverify the effectiveness and advantages of these algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:31:17 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wan", "Neng", ""], ["Gahlawat", "Aditya", ""], ["Hovakimyan", "Naira", ""], ["Theodorou", "Evangelos A.", ""], ["Voulgaris", "Petros G.", ""]]}, {"id": "2102.09117", "submitter": "Jiachen Li", "authors": "Jiachen Li and Hengbo Ma and Zhihao Zhang and Jinning Li and Masayoshi\n  Tomizuka", "title": "Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction\n  and Tracking", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective understanding of the environment and accurate trajectory\nprediction of surrounding dynamic obstacles are indispensable for intelligent\nmobile systems (e.g. autonomous vehicles and social robots) to achieve safe and\nhigh-quality planning when they navigate in highly interactive and crowded\nscenarios. Due to the existence of frequent interactions and uncertainty in the\nscene evolution, it is desired for the prediction system to enable relational\nreasoning on different entities and provide a distribution of future\ntrajectories for each agent. In this paper, we propose a generic generative\nneural system (called STG-DAT) for multi-agent trajectory prediction involving\nheterogeneous agents. The system takes a step forward to explicit interaction\nmodeling by incorporating relational inductive biases with a dynamic graph\nrepresentation and leverages both trajectory and scene context information. We\nalso employ an efficient kinematic constraint layer applied to vehicle\ntrajectory prediction. The constraint not only ensures physical feasibility but\nalso enhances model performance. Moreover, the proposed prediction model can be\neasily adopted by multi-target tracking frameworks. The tracking accuracy\nproves to be improved by empirical results. The proposed system is evaluated on\nthree public benchmark datasets for trajectory prediction, where the agents\ncover pedestrians, cyclists and on-road vehicles. The experimental results\ndemonstrate that our model achieves better performance than various baseline\napproaches in terms of prediction and tracking accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:25:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Zhang", "Zhihao", ""], ["Li", "Jinning", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2102.09285", "submitter": "Lorenzo Zino", "authors": "Lorenzo Zino, Mengbin Ye, Ming Cao", "title": "A two-layer model for coevolving opinion dynamics and collective\n  decision-making in complex social systems", "comments": "This article may be downloaded for personal use only. Any other use\n  requires prior permission of the author and AIP Publishing. This article\n  appeared in Chaos 30, 083107 (2020) and may be found at\n  https://doi.org/10.1063/5.0004787", "journal-ref": "Chaos 30(8), 083107 (2020)", "doi": "10.1063/5.0004787", "report-no": null, "categories": "cs.SI cs.MA cs.SY eess.SY math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the literature on opinion dynamics and evolutionary game theory,\nwe propose a novel mathematical framework to model the intertwined coevolution\nof opinions and decision-making in a complex social system. In the proposed\nframework, the members of a social community update their opinions and revise\ntheir actions as they learn of others' opinions shared on a communication\nchannel, and observe of others' actions through an influence channel; these\ninteractions determine a two-layer network structure. We offer an application\nof the proposed framework by tailoring it to study the adoption of a novel\nsocial norm, demonstrating that the model is able to capture the emergence of\nseveral real-world collective phenomena such as paradigm shifts and unpopular\nnorms. Through the establishment of analytical conditions and Monte Carlo\nnumerical simulations, we shed light on the role of the coupling between\nopinion dynamics and decision-making, and of the network structure, in shaping\nthe emergence of complex collective behavior in social systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:04:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zino", "Lorenzo", ""], ["Ye", "Mengbin", ""], ["Cao", "Ming", ""]]}, {"id": "2102.09354", "submitter": "Carlo Cenedese", "authors": "Carlo Cenedese, Michele Cucuzzella, Jacquelien M. A. Scherpen, Sergio\n  Grammatico, Ming Cao", "title": "Highway Traffic Control via Smart e-Mobility -- Part I: Theory", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how to alleviate highway traffic congestion by\nencouraging plug-in hybrid and electric vehicles to stop at a charging station\naround peak congestion times. Specifically, we design a pricing policy to make\nthe charging price dynamic and dependent on the traffic congestion, predicted\nvia the cell transmission model, and the availability of charging spots.\nFurthermore, we develop a novel framework to model how this policy affects the\ndrivers' decisions by formulating a mixed-integer potential game. Technically,\nwe introduce the concept of \"road-to-station\" (r2s) and \"station-to-road\" (s2r)\nflows, and show that the selfish actions of the drivers converge to charging\nschedules that are individually optimal in the sense of Nash. In the second\npart of this work, submitted as a separate paper (Part II: Case Study), we\nvalidate the proposed strategy on a simulated highway stretch between The Hague\nand Rotterdam, in The Netherlands.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:05:06 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 17:05:59 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cenedese", "Carlo", ""], ["Cucuzzella", "Michele", ""], ["Scherpen", "Jacquelien M. A.", ""], ["Grammatico", "Sergio", ""], ["Cao", "Ming", ""]]}, {"id": "2102.09433", "submitter": "Carlo Cenedese", "authors": "Carlo Cenedese, Michele Cucuzzella, Jacquelien M. A. Scherpen, Sergio\n  Grammatico, Ming Cao", "title": "Highway Traffic Control via Smart e-Mobility -- Part II: Dutch A13 Case\n  Study", "comments": "10 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how to alleviate highway traffic congestions by\nencouraging plug-in electric and hybrid vehicles to stop at charging stations\naround peak congestion times. Specifically, we focus on a case study and\nsimulate the adoption of a dynamic charging price depending on the traffic\ncongestion. We use real traffic data of the A13 highway stretch between The\nHague and Rotterdam, in The Netherlands, to identify the Cell Transmission\nModel. Then, we apply the algorithm proposed in (Part I: Theory) to different\nscenarios, validating the theoretical results and showing the benefits of our\nstrategy in terms of traffic congestion alleviation. Finally, we carry out a\nsensitivity analysis of the proposed algorithm and discuss how to optimize its\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:48:19 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 16:23:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cenedese", "Carlo", ""], ["Cucuzzella", "Michele", ""], ["Scherpen", "Jacquelien M. A.", ""], ["Grammatico", "Sergio", ""], ["Cao", "Ming", ""]]}, {"id": "2102.09655", "submitter": "Bryce Ferguson", "authors": "Bryce L. Ferguson, Philip N. Brown, Jason R. Marden", "title": "The Effectiveness of Subsidies and Tolls in Congestion Games", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.02343", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Are rewards or penalties more effective in influencing user behavior? This\nwork compares the effectiveness of subsidies and tolls in incentivizing user\nbehavior in congestion games. The predominantly studied method of influencing\nuser behavior in network routing problems is to institute taxes which alter\nusers' observed costs in a manner that causes their self-interested choices to\nmore closely align with a system-level objective. Another conceivable method to\naccomplish the same goal is to subsidize the users' actions that are preferable\nfrom a system-level perspective. We show that, when users behave similarly and\npredictably, subsidies offer superior performance guarantees to tolls under\nsimilar budgetary constraints; however, in the presence of unknown player\nheterogeneity, subsidies fail to offer the same robustness as tolls.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 22:48:27 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ferguson", "Bryce L.", ""], ["Brown", "Philip N.", ""], ["Marden", "Jason R.", ""]]}, {"id": "2102.09824", "submitter": "Andreas Schuderer", "authors": "Andreas Schuderer (1 and 2), Stefano Bromuri (1) and Marko van Eekelen\n  (1 and 3) ((1) Open University of the Netherlands, (2) APG Algemene Pensioen\n  Groep N.V., (3) Radboud University)", "title": "Sim-Env: Decoupling OpenAI Gym Environments from Simulation Models", "comments": "17 pages, 6 figures, v2 adds supplementary material (4 pages) and 2\n  ancillary code files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) is one of the most active fields of AI research.\nDespite the interest demonstrated by the research community in reinforcement\nlearning, the development methodology still lags behind, with a severe lack of\nstandard APIs to foster the development of RL applications. OpenAI Gym is\nprobably the most used environment to develop RL applications and simulations,\nbut most of the abstractions proposed in such a framework are still assuming a\nsemi-structured methodology. This is particularly relevant for agent-based\nmodels whose purpose is to analyse adaptive behaviour displayed by\nself-learning agents in the simulation. In order to bridge this gap, we present\na workflow and tools for the decoupled development and maintenance of\nmulti-purpose agent-based models and derived single-purpose reinforcement\nlearning environments, enabling the researcher to swap out environments with\nones representing different perspectives or different reward models, all while\nkeeping the underlying domain model intact and separate. The Sim-Env Python\nlibrary generates OpenAI-Gym-compatible reinforcement learning environments\nthat use existing or purposely created domain models as their simulation\nback-ends. Its design emphasizes ease-of-use, modularity and code separation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 09:25:21 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:02:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Schuderer", "Andreas", "", "1 and 2"], ["Bromuri", "Stefano", "", "1 and 3"], ["van Eekelen", "Marko", "", "1 and 3"]]}, {"id": "2102.09965", "submitter": "Mahieddine Djoudi", "authors": "Hichem Rahab, Abdelhafid Zitouni, Mahieddine Djoudi (TECHN\\'E - EA\n  6316)", "title": "An Enhanced Corpus for Arabic Newspapers Comments", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.00459", "journal-ref": "International Arab Journal of Information Technology, Colleges of\n  Computing and Information Society (CCIS), 2020, 17 (5), pp.789-798", "doi": "10.34028/iajit/17/5/12", "report-no": null, "categories": "cs.IR cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose our enhanced approach to create a dedicated corpus\nfor Algerian Arabic newspapers comments. The developed approach has to enhance\nan existing approach by the enrichment of the available corpus and the\ninclusion of the annotation step by following the Model Annotate Train Test\nEvaluate Revise (MATTER) approach. A corpus is created by collecting comments\nfrom web sites of three well know Algerian newspapers. Three classifiers,\nsupport vector machines, na{\\\"i}ve Bayes, and k-nearest neighbors, were used\nfor classification of comments into positive and negative classes. To identify\nthe influence of the stemming in the obtained results, the classification was\ntested with and without stemming. Obtained results show that stemming does not\nenhance considerably the classification due to the nature of Algerian comments\ntied to Algerian Arabic Dialect. The promising results constitute a motivation\nfor us to improve our approach especially in dealing with non Arabic sentences,\nespecially Dialectal and French ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:15:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Rahab", "Hichem", "", "TECHN\u00c9 - EA\n  6316"], ["Zitouni", "Abdelhafid", "", "TECHN\u00c9 - EA\n  6316"], ["Djoudi", "Mahieddine", "", "TECHN\u00c9 - EA\n  6316"]]}, {"id": "2102.10362", "submitter": "Thomas Spooner", "authors": "Thomas Spooner, Nelson Vadori, Sumitra Ganesh", "title": "Causal Policy Gradients: Leveraging Structure for Efficient Learning in\n  (Factored) MOMDPs", "comments": "19 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods can solve complex tasks but often fail when the\ndimensionality of the action-space or objective multiplicity grow very large.\nThis occurs, in part, because the variance on score-based gradient estimators\nscales quadratically. In this paper, we address this problem through a causal\nbaseline which exploits independence structure encoded in a novel action-target\ninfluence network. Causal policy gradients (CPGs), which follow, provide a\ncommon framework for analysing key state-of-the-art algorithms, are shown to\ngeneralise traditional policy gradients, and yield a principled way of\nincorporating prior knowledge of a problem domain's generative processes. We\nprovide an analysis of the proposed estimator and identify the conditions under\nwhich variance is reduced. The algorithmic aspects of CPGs are discussed,\nincluding optimal policy factorisation, as characterised by minimum biclique\ncoverings, and the implications for the bias-variance trade-off of incorrectly\nspecifying the network. Finally, we demonstrate the performance advantages of\nour algorithm on large-scale bandit and traffic intersection problems,\nproviding a novel contribution to the latter in the form of a spatio-causal\napproximation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 14:51:12 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 09:45:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Spooner", "Thomas", ""], ["Vadori", "Nelson", ""], ["Ganesh", "Sumitra", ""]]}, {"id": "2102.10540", "submitter": "Luis Perez", "authors": "Luis Perez", "title": "Mastering Terra Mystica: Applying Self-Play to Multi-agent Cooperative\n  Board Games", "comments": "9 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore and compare multiple algorithms for solving the\ncomplex strategy game of Terra Mystica, hereafter abbreviated as TM. Previous\nwork in the area of super-human game-play using AI has proven effective, with\nrecent break-through for generic algorithms in games such as Go, Chess, and\nShogi \\cite{AlphaZero}. We directly apply these breakthroughs to a novel\nstate-representation of TM with the goal of creating an AI that will rival\nhuman players. Specifically, we present the initial results of applying\nAlphaZero to this state-representation and analyze the strategies developed. A\nbrief analysis is presented. We call this modified algorithm with our novel\nstate-representation AlphaTM. In the end, we discuss the success and\nshortcomings of this method by comparing against multiple baselines and typical\nhuman scores. All code used for this paper is available at on\n\\href{https://github.com/kandluis/terrazero}{GitHub}.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 07:53:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Perez", "Luis", ""]]}, {"id": "2102.10616", "submitter": "Wenhao Li", "authors": "Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, Hongyuan Zha", "title": "Dealing with Non-Stationarity in Multi-Agent Reinforcement Learning via\n  Trust Region Decomposition", "comments": "32 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationarity is one thorny issue in multi-agent reinforcement learning,\nwhich is caused by the policy changes of agents during the learning procedure.\nCurrent works to solve this problem have their own limitations in effectiveness\nand scalability, such as centralized critic and decentralized actor (CCDA),\npopulation-based self-play, modeling of others and etc. In this paper, we\nnovelly introduce a $\\delta$-stationarity measurement to explicitly model the\nstationarity of a policy sequence, which is theoretically proved to be\nproportional to the joint policy divergence. However, simple policy\nfactorization like mean-field approximation will mislead to larger policy\ndivergence, which can be considered as trust region decomposition dilemma. We\nmodel the joint policy as a general Markov random field and propose a trust\nregion decomposition network based on message passing to estimate the joint\npolicy divergence more accurately. The Multi-Agent Mirror descent policy\nalgorithm with Trust region decomposition, called MAMT, is established with the\npurpose to satisfy $\\delta$-stationarity. MAMT can adjust the trust region of\nthe local policies adaptively in an end-to-end manner, thereby approximately\nconstraining the divergence of joint policy to alleviate the non-stationary\nproblem. Our method can bring noticeable and stable performance improvement\ncompared with baselines in coordination tasks of different complexity.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 14:46:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Wenhao", ""], ["Wang", "Xiangfeng", ""], ["Jin", "Bo", ""], ["Sheng", "Junjie", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2102.10642", "submitter": "Dipankar Maity", "authors": "Dipankar Maity and Panagiotis Tsiotras", "title": "Multi-Agent Consensus Subject to Communication and Privacy Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a multi-agent consensus problem in the presence of adversarial\nagents. The adversaries are able to listen to the inter-agent communications\nand try to estimate the state of the agents. The agents have a limited bit-rate\nfor communication and are required to quantize the transmitted signal in order\nto meet the bit-rate constraint of the communication channel. We propose a\nconsensus protocol that is protected against the adversaries, i.e., the\nexpected mean-square error of the adversary state estimate is lower bounded. In\norder to deal with the bit-rate constraint, we propose a dynamic quantization\nscheme that guarantees protected consensus.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 16:46:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Maity", "Dipankar", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "2102.10646", "submitter": "Shahin Jabbari", "authors": "Feiran Jia, Aditya Mate, Zun Li, Shahin Jabbari, Mithun Chakraborty,\n  Milind Tambe, Michael Wellman, Yevgeniy Vorobeychik", "title": "A Game-Theoretic Approach for Hierarchical Policy-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and analysis of a multi-level game-theoretic model of\nhierarchical policy-making, inspired by policy responses to the COVID-19\npandemic. Our model captures the potentially mismatched priorities among a\nhierarchy of policy-makers (e.g., federal, state, and local governments) with\nrespect to two main cost components that have opposite dependence on the policy\nstrength, such as post-intervention infection rates and the cost of policy\nimplementation. Our model further includes a crucial third factor in decisions:\na cost of non-compliance with the policy-maker immediately above in the\nhierarchy, such as non-compliance of state with federal policies. Our first\ncontribution is a closed-form approximation of a recently published agent-based\nmodel to compute the number of infections for any implemented policy. Second,\nwe present a novel equilibrium selection criterion that addresses common issues\nwith equilibrium multiplicity in our setting. Third, we propose a hierarchical\nalgorithm based on best response dynamics for computing an approximate\nequilibrium of the hierarchical policy-making game consistent with our solution\nconcept. Finally, we present an empirical investigation of equilibrium policy\nstrategies in this game in terms of the extent of free riding as well as\nfairness in the distribution of costs depending on game parameters such as the\ndegree of centralization and disagreements about policy priorities among the\nagents.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 17:01:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jia", "Feiran", ""], ["Mate", "Aditya", ""], ["Li", "Zun", ""], ["Jabbari", "Shahin", ""], ["Chakraborty", "Mithun", ""], ["Tambe", "Milind", ""], ["Wellman", "Michael", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2102.10740", "submitter": "Mridul Agarwal", "authors": "Mridul Agarwal, Bhargav Ganguly, Vaneet Aggarwal", "title": "Communication Efficient Parallel Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the problem where $M$ agents interact with $M$ identical and\nindependent environments with $S$ states and $A$ actions using reinforcement\nlearning for $T$ rounds. The agents share their data with a central server to\nminimize their regret. We aim to find an algorithm that allows the agents to\nminimize the regret with infrequent communication rounds. We provide \\NAM\\\nwhich runs at each agent and prove that the total cumulative regret of $M$\nagents is upper bounded as $\\Tilde{O}(DS\\sqrt{MAT})$ for a Markov Decision\nProcess with diameter $D$, number of states $S$, and number of actions $A$. The\nagents synchronize after their visitations to any state-action pair exceeds a\ncertain threshold. Using this, we obtain a bound of $O\\left(MSA\\log(MT)\\right)$\non the total number of communications rounds. Finally, we evaluate the\nalgorithm against multiple environments and demonstrate that the proposed\nalgorithm performs at par with an always communication version of the UCRL2\nalgorithm, while with significantly lower communication.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 02:46:36 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Agarwal", "Mridul", ""], ["Ganguly", "Bhargav", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2102.10925", "submitter": "Ivan Jericevich", "authors": "Ivan Jericevich and Dharmesh Sing and Tim Gebbie", "title": "CoinTossX: An open-source low-latency high-throughput matching engine", "comments": "21 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA q-fin.CP q-fin.TR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We deploy and demonstrate the CoinTossX low-latency, high-throughput,\nopen-source matching engine with orders sent using the Julia and Python\nlanguages. We show how this can be deployed for small-scale local desk-top\ntesting and discuss a larger scale, but local hosting, with multiple traded\ninstruments managed concurrently and managed by multiple clients. We then\ndemonstrate a cloud based deployment using Microsoft Azure, with large-scale\nindustrial and simulation research use cases in mind. The system is exposed and\ninteracted with via sockets using UDP SBE message protocols and can be\nmonitored using a simple web browser interface using HTTP. We give examples\nshowing how orders can be be sent to the system and market data feeds monitored\nusing the Julia and Python languages. The system is developed in Java with\norders submitted as binary encodings (SBE) via UDP protocols using the Aeron\nMedia Driver as the low-latency, high throughput message transport. The system\nseparates the order-generation and simulation environments e.g. agent-based\nmodel simulation, from the matching of orders, data-feeds and various\nmodularised components of the order-book system. This ensures a more natural\nand realistic asynchronicity between events generating orders, and the events\nassociated with order-book dynamics and market data-feeds. We promote the use\nof Julia as the preferred order submission and simulation environment.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:50:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jericevich", "Ivan", ""], ["Sing", "Dharmesh", ""], ["Gebbie", "Tim", ""]]}, {"id": "2102.11615", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate", "title": "Models we Can Trust: Toward a Systematic Discipline of (Agent-Based)\n  Model Interpretation and Validation", "comments": "prevliminary version of paper to appear in AAMAS'21 Blue Sky Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate the development of a discipline of interacting with and\nextracting information from models, both mathematical (e.g. game-theoretic\nones) and computational (e.g. agent-based models). We outline some directions\nfor the development of a such a discipline:\n  - the development of logical frameworks for the systematic formal\nspecification of stylized facts and social mechanisms in (mathematical and\ncomputational) social science. Such frameworks would bring to attention new\nissues, such as phase transitions, i.e. dramatical changes in the validity of\nthe stylized facts beyond some critical values in parameter space. We argue\nthat such statements are useful for those logical frameworks describing\nproperties of ABM.\n  - the adaptation of tools from the theory of reactive systems (such as\nbisimulation) to obtain practically relevant notions of two systems \"having the\nsame behavior\".\n  - the systematic development of an adversarial theory of model perturbations,\nthat investigates the robustness of conclusions derived from models of social\nbehavior to variations in several features of the social dynamics. These may\ninclude: activation order, the underlying social network, individual agent\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:52:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Istrate", "Gabriel", ""]]}, {"id": "2102.11717", "submitter": "Yuhui Wang", "authors": "Yuhui Wang, Qingyuan Wu, Pengcheng He, Xiaoyang Tan", "title": "A Novel Greedy-Step Bellman Optimality Equation for Efficient Value\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently propagating credit to responsible actions is a central and\nchallenging task in reinforcement learning. To accelerate information\npropagation, this paper presents a new method that bridges a highway that\nallows unimpeded information to flow across long horizons. The key to our\nmethod is a newly proposed Bellman equation, called Greedy-Step Bellman\nOptimality Equation, through which the high-credit information can fast\npropagate across a long horizon. We theoretically show that the solution of the\nnew equation is exactly the optimal value function and the corresponding\noperator converges faster than the classical operator. Besides, it leads to a\nnew multi-step off-policy algorithm, which is capable of safely utilizing any\noff-policy data collected by the arbitrary policy. Experiments reveal that the\nproposed method is reliable, easy to implement. Moreover, without employing\nadditional components of Rainbow except Double DQN, our method achieves\ncompetitive performance with Rainbow on the benchmark tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 14:32:20 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 15:06:27 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 09:45:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Yuhui", ""], ["Wu", "Qingyuan", ""], ["He", "Pengcheng", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "2102.11762", "submitter": "Hardik Meisheri", "authors": "Omkar Shelke, Hardik Meisheri, Harshad Khadilkar", "title": "School of hard knocks: Curriculum analysis for Pommerman with a fixed\n  computational budget", "comments": "8 pages, Submitted to ALA workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pommerman is a hybrid cooperative/adversarial multi-agent environment, with\nchallenging characteristics in terms of partial observability, limited or no\ncommunication, sparse and delayed rewards, and restrictive computational time\nlimits. This makes it a challenging environment for reinforcement learning (RL)\napproaches. In this paper, we focus on developing a curriculum for learning a\nrobust and promising policy in a constrained computational budget of 100,000\ngames, starting from a fixed base policy (which is itself trained to imitate a\nnoisy expert policy). All RL algorithms starting from the base policy use\nvanilla proximal-policy optimization (PPO) with the same reward function, and\nthe only difference between their training is the mix and sequence of opponent\npolicies. One expects that beginning training with simpler opponents and then\ngradually increasing the opponent difficulty will facilitate faster learning,\nleading to more robust policies compared against a baseline where all available\nopponent policies are introduced from the start. We test this hypothesis and\nshow that within constrained computational budgets, it is in fact better to\n\"learn in the school of hard knocks\", i.e., against all available opponent\npolicies nearly from the start. We also include ablation studies where we study\nthe effect of modifying the base environment properties of ammo and bomb blast\nstrength on the agent performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:43:09 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 07:54:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Shelke", "Omkar", ""], ["Meisheri", "Hardik", ""], ["Khadilkar", "Harshad", ""]]}, {"id": "2102.11920", "submitter": "Dengwang Tang", "authors": "Dengwang Tang, Hamidreza Tavafoghi, Vijay Subramanian, Ashutosh\n  Nayyar, Demosthenis Teneketzis", "title": "Dynamic Games among Teams with Delayed Intra-Team Information Sharing", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT math.OC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We analyze a class of stochastic dynamic games among teams with asymmetric\ninformation, where members of a team share their observations internally with a\ndelay of $d$. Each team is associated with a controlled Markov Chain, whose\ndynamics are coupled through the players' actions. These games exhibit\nchallenges in both theory and practice due to the presence of signaling and the\nincreasing domain of information over time. We develop a general approach to\ncharacterize a subset of Nash Equilibria where the agents can use a compressed\nversion of their information, instead of the full information, to choose their\nactions. We identify two subclasses of strategies: Sufficient Private\nInformation Based (SPIB) strategies, which only compress private information,\nand Compressed Information Based (CIB) strategies, which compress both common\nand private information. We show that while SPIB-strategy-based equilibria\nalways exist, the same is not true for CIB-strategy-based equilibria. We\ndevelop a backward inductive sequential procedure, whose solution (if it\nexists) provides a CIB strategy-based equilibrium. We identify some instances\nwhere we can guarantee the existence of a solution to the above procedure. Our\nresults highlight the tension among compression of information, existence of\n(compression based) equilibria, and backward inductive sequential computation\nof such equilibria in stochastic dynamic games with asymmetric information.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 20:06:56 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 14:40:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Tang", "Dengwang", ""], ["Tavafoghi", "Hamidreza", ""], ["Subramanian", "Vijay", ""], ["Nayyar", "Ashutosh", ""], ["Teneketzis", "Demosthenis", ""]]}, {"id": "2102.11929", "submitter": "Bernardo Furtado", "authors": "Bernardo Alves Furtado", "title": "PolicySpace2: modeling markets and endogenous housing policies", "comments": "29 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA econ.GN physics.soc-ph q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policymakers decide on alternative policies facing restricted budgets and\nuncertain, ever-changing future. Designing housing policies is further\ndifficult given the heterogeneous characteristics of properties themselves and\nthe intricacy of housing markets and the spatial context of cities. We propose\nPolicySpace2 (PS2) as an adapted and extended version of the open source\nPolicySpace agent-based model. PS2 is a computer simulation that relies on\nempirically detailed spatial data to model real estate, along with labor,\ncredit, and goods and services markets. Interaction among workers, firms, a\nbank, households and municipalities follow the literature benchmarks to\nintegrate economic, spatial and transport literature. PS2 is applied to a\ncomparison among three competing municipal housing policies aimed at\nalleviating poverty: (a) property acquisition and distribution, (b) rental\nvouchers, and (c) monetary aid. Within the model context, the monetary aid,\nthat is, smaller amounts of help for a larger number of households, makes the\neconomy perform better in terms of production, consumption, reduction of\ninequality, and maintenance of financial duties. PS2 as such is also a\nframework that may be further adapted to a number of related research\nquestions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 20:29:59 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:30:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Furtado", "Bernardo Alves", ""]]}, {"id": "2102.12156", "submitter": "Luidnel Maignan", "authors": "Alexandre Fernandez (LACL), Luidnel Maignan (LACL), Antoine Spicher\n  (LACL)", "title": "Cellular Automata and Kan Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA math.CT math.DS nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formalize precisely the sense in which the application of\ncellular automaton to partial configuration is a natural extension of its local\ntransition function through the categorical notion of Kan extension. In fact,\nthe two possible ways to do such an extension and the ingredients involved in\ntheir definition are related through Kan extensions in many ways. These\nrelations provide additional links between computer science and category\ntheory, and also give a new point of view on the famous Curtis-Hedlung theorem\nof cellular automata from the extended topological point of view provided by\ncategory theory. These relations provide additional links between computer\nscience and category theory. No prior knowledge of category theory is assumed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:24:40 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:40:11 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Fernandez", "Alexandre", "", "LACL"], ["Maignan", "Luidnel", "", "LACL"], ["Spicher", "Antoine", "", "LACL"]]}, {"id": "2102.12307", "submitter": "Dmitry Ivanov", "authors": "Dmitry Ivanov, Vladimir Egorov, Aleksei Shpilman", "title": "Balancing Rational and Other-Regarding Preferences in\n  Cooperative-Competitive Environments", "comments": "Short version of this paper is accepted to AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reinforcement learning studies extensively explore the interplay\nbetween cooperative and competitive behaviour in mixed environments. Unlike\ncooperative environments where agents strive towards a common goal, mixed\nenvironments are notorious for the conflicts of selfish and social interests.\nAs a consequence, purely rational agents often struggle to achieve and maintain\ncooperation. A prevalent approach to induce cooperative behaviour is to assign\nadditional rewards based on other agents' well-being. However, this approach\nsuffers from the issue of multi-agent credit assignment, which can hinder\nperformance. This issue is efficiently alleviated in cooperative setting with\nsuch state-of-the-art algorithms as QMIX and COMA. Still, when applied to mixed\nenvironments, these algorithms may result in unfair allocation of rewards. We\npropose BAROCCO, an extension of these algorithms capable to balance individual\nand social incentives. The mechanism behind BAROCCO is to train two distinct\nbut interwoven components that jointly affect each agent's decisions. Our\nmeta-algorithm is compatible with both Q-learning and Actor-Critic frameworks.\nWe experimentally confirm the advantages over the existing methods and explore\nthe behavioural aspects of BAROCCO in two mixed multi-agent setups.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:35:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ivanov", "Dmitry", ""], ["Egorov", "Vladimir", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "2102.12365", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "Modelling SARS-CoV-2 coevolution with genetic algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the end of 2020, policy responses to the SARS-CoV-2 outbreak have been\nshaken by the emergence of virus variants, impacting public health and policy\nmeasures worldwide. The emergence of these strains suspected to be more\ncontagious, more severe, or even resistant to antibodies and vaccines, seem to\nhave taken by surprise health services and policymakers, struggling to adapt to\nthe new variants constraints. Anticipating the emergence of these mutations to\nplan ahead adequate policies, and understanding how human behaviors may affect\nthe evolution of viruses by coevolution, are key challenges. In this article,\nwe propose coevolution with genetic algorithms (GAs) as a credible approach to\nmodel this relationship, highlighting its implications, potential and\nchallenges. Because of their qualities of exploration of large spaces of\npossible solutions, capacity to generate novelty, and natural genetic focus,\nGAs are relevant for this issue. We present a dual GA model in which both\nviruses aiming for survival and policy measures aiming at minimising infection\nrates in the population, competitively evolve. This artificial coevolution\nsystem may offer us a laboratory to \"debug\" our current policy measures,\nidentify the weaknesses of our current strategies, and anticipate the evolution\nof the virus to plan ahead relevant policies. It also constitutes a decisive\nopportunity to develop new genetic algorithms capable of simulating much more\ncomplex objects. We highlight some structural innovations for GAs for that\nvirus evolution context that may carry promising developments in evolutionary\ncomputation, artificial life and AI.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:49:20 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2102.12461", "submitter": "Jingyao Ren", "authors": "Jingyao Ren, Vikraman Sathiyanarayanan, Eric Ewing, Baskin Senbaslar\n  and Nora Ayanian", "title": "MAPFAST: A Deep Algorithm Selector for Multi Agent Path Finding using\n  Shortest Path Embeddings", "comments": "To appear in AAMAS-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be\nNP-Hard for both make-span and total arrival time minimization. While many\nalgorithms have been developed to solve MAPF problems, there is no dominating\noptimal MAPF algorithm that works well in all types of problems and no standard\nguidelines for when to use which algorithm. In this work, we develop the deep\nconvolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor),\nwhich takes a MAPF problem instance and attempts to select the fastest\nalgorithm to use from a portfolio of algorithms. We improve the performance of\nour model by including single-agent shortest paths in the instance embedding\ngiven to our model and by utilizing supplemental loss functions in addition to\na classification loss. We evaluate our model on a large and diverse dataset of\nMAPF instances, showing that it outperforms all individual algorithms in its\nportfolio as well as the state-of-the-art optimal MAPF algorithm selector. We\nalso provide an analysis of algorithm behavior in our dataset to gain a deeper\nunderstanding of optimal MAPF algorithms' strengths and weaknesses to help\nother researchers leverage different heuristics in algorithm designs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:41:37 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ren", "Jingyao", ""], ["Sathiyanarayanan", "Vikraman", ""], ["Ewing", "Eric", ""], ["Senbaslar", "Baskin", ""], ["Ayanian", "Nora", ""]]}, {"id": "2102.12550", "submitter": "Sheng Li", "authors": "Sheng Li, Yutai Zhou, Ross Allen, Mykel J. Kochenderfer", "title": "Learning Emergent Discrete Message Communication for Cooperative\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is a important factor that enables agents work cooperatively in\nmulti-agent reinforcement learning (MARL). Most previous work uses continuous\nmessage communication whose high representational capacity comes at the expense\nof interpretability. Allowing agents to learn their own discrete message\ncommunication protocol emerged from a variety of domains can increase the\ninterpretability for human designers and other agents.This paper proposes a\nmethod to generate discrete messages analogous to human languages, and achieve\ncommunication by a broadcast-and-listen mechanism based on self-attention. We\nshow that discrete message communication has performance comparable to\ncontinuous message communication but with much a much smaller vocabulary\nsize.Furthermore, we propose an approach that allows humans to interactively\nsend discrete messages to agents.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 20:44:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Li", "Sheng", ""], ["Zhou", "Yutai", ""], ["Allen", "Ross", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2102.12748", "submitter": "Keisuke Okumura", "authors": "Shota Kameyama, Keisuke Okumura, Yasumasa Tamura and Xavier D\\'efago", "title": "Active Modular Environment for Robot Navigation", "comments": "7 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel robot-environment interaction in navigation tasks\nsuch that robots have neither a representation of their working space nor\nplanning function, instead, an active environment takes charge of these\naspects. This is realized by spatially deploying computing units, called cells,\nand making cells manage traffic in their respective physical region. Different\nfrom stigmegic approaches, cells interact with each other to manage\nenvironmental information and to construct instructions on how robots move.\n  As a proof-of-concept, we present an architecture called AFADA and its\nprototype, consisting of modular cells and robots moving on the cells. The\ninstructions from cells are based on a distributed routing algorithm and a\nreservation protocol. We demonstrate that AFADA achieves efficient robot moves\nfor single-robot navigation in a dynamic environment changing its topology with\na stochastic model, comparing to self-navigation by a robot itself. This is\nfollowed by several demos, including multi-robot navigation, highlighting the\npower of offloading both representation and planning from robots to the\nenvironment. We expect that the concept of AFADA contributes to developing the\ninfrastructure for multiple robots because it can engage online and lifelong\nplanning and execution.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:23:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Kameyama", "Shota", ""], ["Okumura", "Keisuke", ""], ["Tamura", "Yasumasa", ""], ["D\u00e9fago", "Xavier", ""]]}, {"id": "2102.12957", "submitter": "Jianzhun Shao", "authors": "Jianzhun Shao, Hongchang Zhang, Yuhang Jiang, Shuncheng He, Xiangyang\n  Ji", "title": "Credit Assignment with Meta-Policy Gradient for Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward decomposition is a critical problem in centralized training with\ndecentralized execution~(CTDE) paradigm for multi-agent reinforcement learning.\nTo take full advantage of global information, which exploits the states from\nall agents and the related environment for decomposing Q values into individual\ncredits, we propose a general meta-learning-based Mixing Network with Meta\nPolicy Gradient~(MNMPG) framework to distill the global hierarchy for delicate\nreward decomposition. The excitation signal for learning global hierarchy is\ndeduced from the episode reward difference between before and after \"exercise\nupdates\" through the utility network. Our method is generally applicable to the\nCTDE method using a monotonic mixing network. Experiments on the StarCraft II\nmicromanagement benchmark demonstrate that our method just with a simple\nutility network is able to outperform the current state-of-the-art MARL\nalgorithms on 4 of 5 super hard scenarios. Better performance can be further\nachieved when combined with a role-based utility network.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:03:37 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shao", "Jianzhun", ""], ["Zhang", "Hongchang", ""], ["Jiang", "Yuhang", ""], ["He", "Shuncheng", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2102.13047", "submitter": "Furkan Sezer", "authors": "Furkan Sezer, Hossein Khazaei, and Ceyhun Eksin", "title": "Social Welfare Maximization and Conformism via Information Design in\n  Linear-Quadratic-Gaussian Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY econ.GN eess.SY q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider linear-quadratic Gaussian (LQG) games in which players have\nquadratic payoffs that depend on the players' actions and an unknown\npayoff-relevant state, and signals on the state that follow a Gaussian\ndistribution conditional on the state realization. An information designer\ndecides the fidelity of information revealed to the players in order to\nmaximize the social welfare of the players or reduce the disagreement among\nplayers' actions. Leveraging the semi-definiteness of the information design\nproblem, we derive analytical solutions for these objectives under specific LQG\ngames. We show that full information disclosure maximizes social welfare when\nthere is a common payoff-relevant state, when there is strategic\nsubstitutability in the actions of players, or when the signals are public.\nNumerical results show that as strategic substitution increases, the value of\nthe information disclosure increases. When the objective is to induce\nconformity among players' actions, hiding information is optimal. Lastly, we\nconsider the information design objective that is a weighted combination of\nsocial welfare and cohesiveness of players' actions. We obtain an interval for\nthe weights where full information disclosure is optimal under public signals\nfor games with strategic substitutability. Numerical solutions show that the\nactual interval where full information disclosure is optimal gets close to the\nanalytical interval obtained as substitution increases.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:56:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sezer", "Furkan", ""], ["Khazaei", "Hossein", ""], ["Eksin", "Ceyhun", ""]]}, {"id": "2102.13281", "submitter": "Senthil Hariharan Arul", "authors": "Senthil Hariharan Arul and Dinesh Manocha", "title": "V-RVO: Decentralized Multi-Agent Collision Avoidance using Voronoi\n  Diagrams and Reciprocal Velocity Obstacles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a decentralized collision avoidance method for dense environments\nthat is based on buffered Voronoi cells (BVC) and reciprocal velocity obstacles\n(RVO). Our approach is designed for scenarios with large number of close\nproximity agents and provides passive-friendly collision avoidance guarantees.\nThe Voronoi cells are superimposed with RVO cones to compute a suitable\ndirection for each agent and we use that direction for computing a local\ncollision-free path. Our approach can satisfy double-integrator dynamics\nconstraints and we use the properties of the BVC to formulate a simple,\ndecentralized deadlock resolution strategy. We demonstrate the benefits of\nV-RVO in complex scenarios with tens of agents in close proximity. In practice,\nV-RVO's performance is comparable to prior velocity-obstacle methods and the\ncollision avoidance behavior is significantly less conservative than ORCA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:56:42 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Arul", "Senthil Hariharan", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2102.13351", "submitter": "Micha Sende", "authors": "Micha Sende, Melanie Schranz, Gianluca Prato, Etienne Brosse, Omar\n  Morando, Martina Umlauft", "title": "Engineering Swarms of Cyber-Physical Systems with the CPSwarm Workbench", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Engineering swarms of cyber-physical systems (CPSs) is a complex process. We\npresent the CPSwarm workbench that creates an automated design workflow to ease\nthis process. This formalized workflow guides the user from modeling, to code\ngeneration, to deployment, both in simulation and on CPS hardware platforms.\nThe workbench combines existing and emerging tools to solve real-world CPS\nswarm problems. As a proof-of-concept, we use the workbench to design a swarm\nof unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) for a\nsearch and rescue (SAR) use case. We evaluate the resulting swarm behaviors on\nthree levels. First, abstract simulations for rapid prototyping. Second,\ndetailed simulation to test the correctness of the results. Third, deployment\non hardware to demonstrate the applicability. We measure the swarm performance\nin terms of area covered and victims rescued. The results show that the\nperformance of the swarm is proportional to its size. Despite some manual\nsteps, the proposed workbench shows to be well suited to ease the complicated\ntask of deploying a swarm of CPSs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 08:01:08 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Sende", "Micha", ""], ["Schranz", "Melanie", ""], ["Prato", "Gianluca", ""], ["Brosse", "Etienne", ""], ["Morando", "Omar", ""], ["Umlauft", "Martina", ""]]}, {"id": "2102.13454", "submitter": "Kentaro Wada", "authors": "Jiahua Zhang, Kentaro Wada, Takashi Oguchi", "title": "Morning commute in congested urban rail transit system: A macroscopic\n  model for equilibrium distribution of passenger arrivals", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a macroscopic model to describe the equilibrium\ndistribution of passenger arrivals for the morning commute problem in a\ncongested urban rail transit system. We employ a macroscopic train operation\nsub-model developed by Seo et al. (2017a,b) to express the interaction between\ndynamics of passengers and trains in a simplified manner while maintaining\ntheir essential physical relations. We derive the equilibrium conditions of the\nproposed model and discuss the existence of equilibrium. The characteristics of\nthe equilibrium are then examined through numerical examples under different\npassenger demand settings. As an application of the proposed model, we finally\nanalyze a simple time-dependent timetable optimization problem with equilibrium\nconstraints and show that there exists a \"capacity increasing paradox\" in which\na higher dispatch frequency can increase the equilibrium cost. Further insights\ninto the design of the timetable and its influence on passengers' equilibrium\ntravel costs are also obtained.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 13:18:12 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhang", "Jiahua", ""], ["Wada", "Kentaro", ""], ["Oguchi", "Takashi", ""]]}, {"id": "2102.13477", "submitter": "Lam Duc Nguyen", "authors": "Lam Duc Nguyen, Amari N. Lewis, Israel Leyva-Mayorga, Amelia Regan,\n  and Petar Popovski", "title": "B-ETS: A Trusted Blockchain-based Emissions Trading System for\n  Vehicle-to-Vehicle Networks", "comments": "Paper got accepted in 7th International Conference on Vehicle\n  Technology and Intelligent Transport Systems (VEHITS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban areas are negatively impacted by Carbon Dioxide (CO2 ) and Nitrogen\nOxide (NOx) emissions. In order to achieve a cost-effective reduction of\ngreenhouse gas emissions and to combat climate change, the European Union (EU)\nintroduced an Emissions Trading System (ETS) where organizations can buy or\nreceive emission allowances as needed. The current ETS is a centralized one,\nconsisting of a set of complex rules. It is currently administered at the\norganizational level and is used for fixed-point sources of pollution such as\nfactories, power plants, and refineries. However, the current ETS cannot\nefficiently cope with vehicle mobility, even though vehicles are one of the\nprimary sources of CO2 and NOx emissions. In this study, we propose a new\ndistributed Blockchain-based emissions allowance trading system called B-ETS.\nThis system enables transparent and trustworthy data exchange as well as\ntrading of allowances among vehicles, relying on vehicle-to-vehicle\ncommunication. In addition, we introduce an economic incentive-based mechanism\nthat appeals to individual drivers and leads them to modify their driving\nbehavior in order to reduce emissions. The efficiency of the proposed system is\nstudied through extensive simulations, showing how increased vehicle\nconnectivity can lead to a reduction of the emissions generated from those\nvehicles. We demonstrate that our method can be used for full life-cycle\nmonitoring and fuel economy reporting. This leads us to conjecture that the\nproposed system could lead to important behavioral changes among the drivers\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:52:56 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 01:30:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nguyen", "Lam Duc", ""], ["Lewis", "Amari N.", ""], ["Leyva-Mayorga", "Israel", ""], ["Regan", "Amelia", ""], ["Popovski", "Petar", ""]]}, {"id": "2102.13621", "submitter": "Dante Kalise", "authors": "Young-Pil Choi, Dante Kalise, Andr\\'es A. Peters", "title": "Collisionless and Decentralized Formation Control for Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA math.DS nlin.AO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A decentralized feedback controller for multi-agent systems, inspired by\nvehicle platooning, is proposed. The closed-loop resulting from the\ndecentralized control action has three distinctive features: the generation of\ncollision-free trajectories, flocking of the system towards a consensus state\nin velocity, and asymptotic convergence to a prescribed pattern of distances\nbetween agents. For each feature, a rigorous dynamical analysis is provided,\nyielding a characterization of the set of parameters and initial configurations\nwhere collision avoidance, flocking, and pattern formation is guaranteed.\nNumerical tests assess the theoretical results presented.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 17:39:32 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Choi", "Young-Pil", ""], ["Kalise", "Dante", ""], ["Peters", "Andr\u00e9s A.", ""]]}]