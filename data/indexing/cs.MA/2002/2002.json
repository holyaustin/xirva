[{"id": "2002.00497", "submitter": "Karl Kurzer", "authors": "Karl Kurzer, Marcus Fechner and J. Marius Z\\\"ollner", "title": "Accelerating Cooperative Planning for Automated Vehicles with Learned\n  Heuristics and Monte Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient driving in urban traffic scenarios requires foresight. The\nobservation of other traffic participants and the inference of their possible\nnext actions depending on the own action is considered cooperative prediction\nand planning. Humans are well equipped with the capability to predict the\nactions of multiple interacting traffic participants and plan accordingly,\nwithout the need to directly communicate with others. Prior work has shown that\nit is possible to achieve effective cooperative planning without the need for\nexplicit communication. However, the search space for cooperative plans is so\nlarge that most of the computational budget is spent on exploring the search\nspace in unpromising regions that are far away from the solution. To accelerate\nthe planning process, we combined learned heuristics with a cooperative\nplanning method to guide the search towards regions with promising actions,\nyielding better solutions at lower computational costs.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 21:41:35 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 14:46:05 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Kurzer", "Karl", ""], ["Fechner", "Marcus", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "2002.00836", "submitter": "Yongjie Yang", "authors": "Yongjie Yang", "title": "On the Complexity of Destructive Bribery in Approval-Based Multi-winner\n  Voting", "comments": "19pages, to appear in AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of constructive manipulation, control, and bribery for\napproval-based multi-winner voting have been extensively studied very recently.\nHowever, their destructive counterparts seem to be less studied in the\nliterature so far. This paper aims to fill this gap by exploring the complexity\nof several destructive bribery problems under five prestigious approval-based\nmulti-winner voting rules. Generally speaking, these problems are to determine\nif a number of given candidates can be excluded from any winning committees by\nperforming a series of modification operations yet without exceeding a given\nbudget. We consider five operations. We offer a complete landscape of the\ncomplexity of the problems studied in this paper, and for NP-hard problems we\nstudy their parameterized complexity with respect to meaningful parameters.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:49:26 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 04:19:10 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yang", "Yongjie", ""]]}, {"id": "2002.01093", "submitter": "Abhinav Gupta", "authors": "Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, Joelle Pineau", "title": "On the interaction between supervision and self-play in emergent\n  communication", "comments": "The first two authors contributed equally. Accepted at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising approach for teaching artificial agents to use natural language\ninvolves using human-in-the-loop training. However, recent work suggests that\ncurrent machine learning methods are too data inefficient to be trained in this\nway from scratch. In this paper, we investigate the relationship between two\ncategories of learning signals with the ultimate goal of improving sample\nefficiency: imitating human language data via supervised learning, and\nmaximizing reward in a simulated multi-agent environment via self-play (as done\nin emergent communication), and introduce the term supervised self-play (S2P)\nfor algorithms using both of these signals. We find that first training agents\nvia supervised learning on human data followed by self-play outperforms the\nconverse, suggesting that it is not beneficial to emerge languages from\nscratch. We then empirically investigate various S2P schedules that begin with\nsupervised learning in two environments: a Lewis signaling game with symbolic\ninputs, and an image-based referential game with natural language descriptions.\nLastly, we introduce population based approaches to S2P, which further improves\nthe performance over single-agent methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:35:19 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:48:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lowe", "Ryan", ""], ["Gupta", "Abhinav", ""], ["Foerster", "Jakob", ""], ["Kiela", "Douwe", ""], ["Pineau", "Joelle", ""]]}, {"id": "2002.01335", "submitter": "Abhinav Gupta", "authors": "Agnieszka S{\\l}owik, Abhinav Gupta, William L. Hamilton, Mateja\n  Jamnik, Sean B. Holden, Christopher Pal", "title": "Structural Inductive Biases in Emergent Communication", "comments": "The first two authors contributed equally. Poster presented at CogSci\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to communicate, humans flatten a complex representation of ideas and\ntheir attributes into a single word or a sentence. We investigate the impact of\nrepresentation learning in artificial agents by developing graph referential\ngames. We empirically show that agents parametrized by graph neural networks\ndevelop a more compositional language compared to bag-of-words and sequence\nmodels, which allows them to systematically generalize to new combinations of\nfamiliar features.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:59:08 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 18:57:45 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 22:05:34 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 04:13:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["S\u0142owik", "Agnieszka", ""], ["Gupta", "Abhinav", ""], ["Hamilton", "William L.", ""], ["Jamnik", "Mateja", ""], ["Holden", "Sean B.", ""], ["Pal", "Christopher", ""]]}, {"id": "2002.01969", "submitter": "Mohamadreza Ahmadi", "authors": "Mohamadreza Ahmadi, Arun A. Viswanathan, Michel D. Ingham, Kymie Tan,\n  and Aaron D. Ames", "title": "Partially Observable Games for Secure Autonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CR cs.FL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology development efforts in autonomy and cyber-defense have been\nevolving independently of each other, over the past decade. In this paper, we\nreport our ongoing effort to integrate these two presently distinct areas into\na single framework. To this end, we propose the two-player partially observable\nstochastic game formalism to capture both high-level autonomous mission\nplanning under uncertainty and adversarial decision making subject to imperfect\ninformation. We show that synthesizing sub-optimal strategies for such games is\npossible under finite-memory assumptions for both the autonomous decision maker\nand the cyber-adversary. We then describe an experimental testbed to evaluate\nthe efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 19:31:56 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ahmadi", "Mohamadreza", ""], ["Viswanathan", "Arun A.", ""], ["Ingham", "Michel D.", ""], ["Tan", "Kymie", ""], ["Ames", "Aaron D.", ""]]}, {"id": "2002.02219", "submitter": "Farzam Fanitabasi", "authors": "Farzam Fanitabasi, Edward Gaere, Evangelos Pournaras", "title": "A Self-Integration Testbed for Decentralized Socio-technical Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2020.07.036", "report-no": null, "categories": "cs.MA cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things comes along with new challenges for experimenting,\ntesting, and operating decentralized socio-technical systems at large-scale. In\nsuch systems, autonomous agents interact locally with their users, and remotely\nwith other agents to make intelligent collective choices. Via these\ninteractions they self-regulate the consumption and production of distributed\nresources. While such complex systems are often deployed and operated using\ncentralized computing infrastructures, the socio-technical nature of these\ndecentralized systems requires new value-sensitive design paradigms; empowering\ntrust, transparency, and alignment with citizens' social values, such as\nprivacy preservation, autonomy, and fairness among citizens' choices.\nCurrently, instruments and tools to study such systems and guide the\nprototyping process from simulation to live deployment are missing, or not\npractical in this distributed socio-technical context. This paper bridges this\ngap by introducing a novel testbed architecture for decentralized\nsocio-technical systems running on IoT. This new architecture is designed for a\nseamless reusability of (i) application-independent decentralized services by\nan IoT application, and (ii) different IoT applications by the same\ndecentralized service. This dual self-integration promises IoT applications\nthat are simpler to prototype, and can interoperate with decentralized services\nduring runtime to self-integrate more complex functionality. Such integration\nprovides stronger validation of IoT applications, and improves resource\nutilization. Pressure and crash tests during continuous operations of several\nweeks, with more than 80K network joining and leaving of agents, 2.4M parameter\nchanges, and 100M communicated messages, confirm the robustness and\npracticality of the testbed architecture.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 12:18:28 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 09:25:48 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Fanitabasi", "Farzam", ""], ["Gaere", "Edward", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2002.02325", "submitter": "Kevin McKee", "authors": "Kevin R. McKee, Ian Gemp, Brian McWilliams, Edgar A.\n  Du\\'e\\~nez-Guzm\\'an, Edward Hughes, and Joel Z. Leibo", "title": "Social diversity and social preferences in mixed-motive reinforcement\n  learning", "comments": "Proceedings of the 19th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on reinforcement learning in pure-conflict and pure-common\ninterest games has emphasized the importance of population heterogeneity. In\ncontrast, studies of reinforcement learning in mixed-motive games have\nprimarily leveraged homogeneous approaches. Given the defining characteristic\nof mixed-motive games--the imperfect correlation of incentives between group\nmembers--we study the effect of population heterogeneity on mixed-motive\nreinforcement learning. We draw on interdependence theory from social\npsychology and imbue reinforcement learning agents with Social Value\nOrientation (SVO), a flexible formalization of preferences over group outcome\ndistributions. We subsequently explore the effects of diversity in SVO on\npopulations of reinforcement learning agents in two mixed-motive Markov games.\nWe demonstrate that heterogeneity in SVO generates meaningful and complex\nbehavioral variation among agents similar to that suggested by interdependence\ntheory. Empirical results in these mixed-motive dilemmas suggest agents trained\nin heterogeneous populations develop particularly generalized, high-performing\npolicies relative to those trained in homogeneous populations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:07:02 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 19:35:05 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["McKee", "Kevin R.", ""], ["Gemp", "Ian", ""], ["McWilliams", "Brian", ""], ["Du\u00e9\u00f1ez-Guzm\u00e1n", "Edgar A.", ""], ["Hughes", "Edward", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "2002.02345", "submitter": "Eric Silverman", "authors": "Eric Silverman, Umberto Gostoli, Stefano Picascia, Jonatan Almagor,\n  Mark McCann, Richard Shaw, Claudio Angione", "title": "Situating Agent-Based Modelling in Population Health Research", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's most troublesome population health challenges are often driven by\nsocial and environmental determinants, which are difficult to model using\ntraditional epidemiological methods. We agree with those who have argued for\nthe wider adoption of agent-based modelling (ABM) in taking on these\nchallenges. However, while ABM has been used occasionally in population health,\nwe argue that for ABM to be most effective in the field it should be used as a\nmeans for answering questions normally inaccessible to the traditional\nepidemiological toolkit. In an effort to clearly illustrate the utility of ABM\nfor population health research, and to clear up persistent misunderstandings\nregarding the method's conceptual underpinnings, we offer a detailed\npresentation of the core concepts of complex systems theory, and summarise why\nsimulations are essential to the study of complex systems. We then examine the\ncurrent state of the art in ABM for population health, and propose they are\nwell-suited for the study of the `wicked' problems in population health, and\ncould make significant contributions to theory and intervention development in\nthese areas.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:45:40 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Silverman", "Eric", ""], ["Gostoli", "Umberto", ""], ["Picascia", "Stefano", ""], ["Almagor", "Jonatan", ""], ["McCann", "Mark", ""], ["Shaw", "Richard", ""], ["Angione", "Claudio", ""]]}, {"id": "2002.02513", "submitter": "Sriram Ganapathi Subramanian", "authors": "Sriram Ganapathi Subramanian and Pascal Poupart and Matthew E. Taylor\n  and Nidhi Hegde", "title": "Multi Type Mean Field Reinforcement Learning", "comments": "Paper to appear in the Proceedings of International Conference on\n  Autonomous Agents and Multi-Agent Systems (AAMAS) 2020. Revised version has\n  some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field theory provides an effective way of scaling multiagent\nreinforcement learning algorithms to environments with many agents that can be\nabstracted by a virtual mean agent. In this paper, we extend mean field\nmultiagent algorithms to multiple types. The types enable the relaxation of a\ncore assumption in mean field games, which is that all agents in the\nenvironment are playing almost similar strategies and have the same goal. We\nconduct experiments on three different testbeds for the field of many agent\nreinforcement learning, based on the standard MAgents framework. We consider\ntwo different kinds of mean field games: a) Games where agents belong to\npredefined types that are known a priori and b) Games where the type of each\nagent is unknown and therefore must be learned based on observations. We\nintroduce new algorithms for each type of game and demonstrate their superior\nperformance over state of the art algorithms that assume that all agents belong\nto the same type and other baseline algorithms in the MAgent framework.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 20:58:58 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 13:22:40 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 14:38:52 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 14:02:16 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Subramanian", "Sriram Ganapathi", ""], ["Poupart", "Pascal", ""], ["Taylor", "Matthew E.", ""], ["Hegde", "Nidhi", ""]]}, {"id": "2002.02529", "submitter": "Hesham Rakha", "authors": "Kyungwon Kang and Hesham A Rakha", "title": "A Repeated Game Freeway Lane Changing Model", "comments": "The paper has been submitted to Sensors", "journal-ref": "Sensors, 2020, 20, 1554", "doi": "10.3390/s20061554", "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane changes are complex safety and throughput critical driver actions. Most\nlane changing models deal with lane-changing maneuvers solely from the merging\ndriver's standpoint and thus ignore driver interaction. To overcome this\nshortcoming, we develop a game-theoretical decision-making model and validate\nthe model using empirical merging maneuver data at a freeway on-ramp.\nSpecifically, this paper advances our repeated game model in a previous paper\nby using updated payoff functions. Validation results using the NGSIM empirical\ndata show that the developed game-theoretical model provides better prediction\naccuracy compared to previous work, with correct predictions approximately 86\npercent of the time. In addition, a sensitivity analysis demonstrates the\nrationality and sensitivity of the model to variations in various factors. To\nprovide evidence of the benefits of the repeated game approach, which takes\ninto account previous decision-making results, a case study is conducted using\nan agent-based simulation model. The proposed repeated game model produces\nsuperior performance to a one-shot game model, when simulating actual freeway\nmerging behaviors. Finally, this lane change model, which captures the\ncollective decision-making between human drivers, can be used to develop\nautomated vehicle driving strategies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 18:09:09 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kang", "Kyungwon", ""], ["Rakha", "Hesham A", ""]]}, {"id": "2002.02728", "submitter": "Samuel Thiriot", "authors": "Samuel Thiriot", "title": "Impact of the Interaction Network on the Dynamics of Word-of-Mouth with\n  Information Seeking", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word-of-Mouth refers to the dynamics of interpersonal communication occurring\nduring the diffusion of innovations (novel practices, ideas or products).\nAccording to field studies, word-of-mouth is made of both information seeking\nand proactive communication: individuals first become aware of the existence of\nan innovation, then start actively seeking out for the expert knowledge\nrequired to evaluate the innovation; when they hold the expert knowledge, they\nmight start promoting it pro-actively. Successful diffusion of innovation\nrequires the individuals to hold both awareness and expert knowledge, so they\ncan evaluate the innovation and use it properly. A computational model\n\"USA/IPK\" was recently proposed to study the role and impact of information\nseeking on the dynamics of word-of-mouth. We propose here an analysis of the\nimpact of the network of interaction on the dynamics of this model. We compare\nthe dynamics of the model over networks generated with different algorithms\nwith the original dynamics. The results demonstrate the dynamics of the model\nare similar across tested networks, with the noticeable exception of the\nefficiency of the diffusion which varies between networks having similar\ndensities and sizes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 11:56:15 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Thiriot", "Samuel", ""]]}, {"id": "2002.03117", "submitter": "Artur Niewiadomski", "authors": "Magdalena Kacprzak, Artur Niewiadomski, Wojciech Penczek", "title": "SAT-Based ATL Satisfiability Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis of models and strategies is a very important problem in software\nengineering. The main element here is checking the satisfiability of formulae\nexpressing the specification of a system to be implemented. This paper puts\nforward a novel method for deciding the satisfiability of formulae of\nAlternating-time Temporal Logic (ATL). The method presented expands on one for\nCTL exploit ing SAT Modulo Monotonic Theories solvers. Similarly to the CTL\ncase, our approach appears to be very efficient. The experimental results show\nthat we can quickly test the satisfiability of large ATL formulae that have\nbeen out of reach of the existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 08:48:51 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kacprzak", "Magdalena", ""], ["Niewiadomski", "Artur", ""], ["Penczek", "Wojciech", ""]]}, {"id": "2002.03246", "submitter": "Andrew Best", "authors": "Andrew Best, Sahil Narang, Dinesh Manocha", "title": "SPA: Verbal Interactions between Agents and Avatars in Shared Virtual\n  Environments using Propositional Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for generating plausible verbal interactions\nbetween virtual human-like agents and user avatars in shared virtual\nenvironments. Sense-Plan-Ask, or SPA, extends prior work in propositional\nplanning and natural language processing to enable agents to plan with\nuncertain information, and leverage question and answer dialogue with other\nagents and avatars to obtain the needed information and complete their goals.\nThe agents are additionally able to respond to questions from the avatars and\nother agents using natural-language enabling real-time multi-agent multi-avatar\ncommunication environments.\n  Our algorithm can simulate tens of virtual agents at interactive rates\ninteracting, moving, communicating, planning, and replanning. We find that our\nalgorithm creates a small runtime cost and enables agents to complete their\ngoals more effectively than agents without the ability to leverage\nnatural-language communication. We demonstrate quantitative results on a set of\nsimulated benchmarks and detail the results of a preliminary user-study\nconducted to evaluate the plausibility of the virtual interactions generated by\nSPA. Overall, we find that participants prefer SPA to prior techniques in 84\\%\nof responses including significant benefits in terms of the plausibility of\nnatural-language interactions and the positive impact of those interactions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 23:15:06 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Best", "Andrew", ""], ["Narang", "Sahil", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2002.03267", "submitter": "Jun  Yamada", "authors": "Jun Yamada, John Shawe-Taylor, Zafeirios Fountas", "title": "Evolution of a Complex Predator-Prey Ecosystem on Large-scale\n  Multi-Agent Deep Reinforcement Learning", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of population dynamics is a central research theme in\ncomputational biology, which contributes to understanding the interactions\nbetween predators and preys. Conventional mathematical tools of this theme,\nhowever, are incapable of accounting for several important attributes of such\nsystems, such as the intelligent and adaptive behavior exhibited by individual\nagents. This unrealistic setting is often insufficient to simulate properties\nof population dynamics found in the real-world. In this work, we leverage\nmulti-agent deep reinforcement learning, and we propose a new model of\nlarge-scale predator-prey ecosystems. Using different variants of our proposed\nenvironment, we show that multi-agent simulations can exhibit key real-world\ndynamical properties. To obtain this behavior, we firstly define a mating\nmechanism such that existing agents reproduce new individuals bound by the\nconditions of the environment. Furthermore, we incorporate a real-time\nevolutionary algorithm and show that reinforcement learning enhances the\nevolution of the agents' physical properties such as speed, attack and\nresilience against attacks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 02:33:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yamada", "Jun", ""], ["Shawe-Taylor", "John", ""], ["Fountas", "Zafeirios", ""]]}, {"id": "2002.03269", "submitter": "Yi Peng", "authors": "Jinlong Lei, Peng Yi, Jie Chen, Yiguang Hong", "title": "Linearly Convergent Algorithm with Variance Reduction for Distributed\n  Stochastic Optimization", "comments": "8 pages,3 figure, the paper will be presented on European Control\n  Conference,2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a distributed stochastic strongly convex optimization,\nwhere agents connected over a network aim to cooperatively minimize the average\nof all agents' local cost functions. Due to the stochasticity of gradient\nestimation and distributedness of local objective, fast linearly convergent\ndistributed algorithms have not been achieved yet. This work proposes a novel\ndistributed stochastic gradient tracking algorithm with variance reduction,\nwhere the local gradients are estimated by an increasing batch-size of sampled\ngradients. With an undirected connected communication graph and a geometrically\nincreasing batch-size, the iterates are shown to converge in mean to the\noptimal solution at a geometric rate (achieving linear convergence). The\niteration, communication, and oracle complexity for obtaining an\n$\\epsilon$-optimal solution are established as well. Particulary, the\ncommunication complexity is $\\mathcal{O}(\\ln (1/\\epsilon))$ while the oracle\ncomplexity (number of sampled gradients) is $\\mathcal{O}(1/\\epsilon^2)$, which\nis of the same order as that of centralized approaches.\n  Hence, the proposed scheme is communication-efficient without requiring extra\nsampled gradients. Numerical simulations are given to demonstrate the theoretic\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 02:57:58 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 06:25:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lei", "Jinlong", ""], ["Yi", "Peng", ""], ["Chen", "Jie", ""], ["Hong", "Yiguang", ""]]}, {"id": "2002.03541", "submitter": "Jian Hou", "authors": "Jian Hou, Zhiyong Chen, ZhiyunLin, Mengfan Xiang", "title": "Resilient Consensus via Weight Learning and Its Application in\n  Fault-Tolerant Clock Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the distributed consensus problem in the presence of\nfaulty nodes. A novel weight learning algorithm is introduced such that neither\nnetwork connectivity nor a sequence of history records is required to achieve\nresilient consensus. The critical idea is to dynamically update the interaction\nweights among neighbors learnt from their credibility measurement. Basically,\nwe define a reward function that is inversely proportional to the distance to\nits neighbor, and then adjust the credibility based on the reward derived at\nthe present step and the previous credibility. In such a way, the interaction\nweights are updated at every step, which integrates the historic information\nand degrades the influences from faulty nodes. Both fixed and stochastic\ntopologies are considered in this paper. Furthermore, we apply this novel\napproach in clock synchronization problem. By updating the logical clock skew\nand offset via the corresponding weight learning algorithms, respectively, the\nlogical clock synchronization is eventually achieved regardless of faulty\nnodes. Simulations are provided to illustrate the effectiveness of the\nstrategy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 04:47:04 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Hou", "Jian", ""], ["Chen", "Zhiyong", ""], ["ZhiyunLin", "", ""], ["Xiang", "Mengfan", ""]]}, {"id": "2002.03939", "submitter": "Yaodong Yang Mr.", "authors": "Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong\n  Liu, Hongyao Tang", "title": "Qatten: A General Framework for Cooperative Multiagent Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world tasks, multiple agents must learn to coordinate with each\nother given their private observations and limited communication ability. Deep\nmultiagent reinforcement learning (Deep-MARL) algorithms have shown superior\nperformance in such challenging settings. One representative class of work is\nmultiagent value decomposition, which decomposes the global shared multiagent\nQ-value $Q_{tot}$ into individual Q-values $Q^{i}$ to guide individuals'\nbehaviors, i.e. VDN imposing an additive formation and QMIX adopting a\nmonotonic assumption using an implicit mixing method. However, most of the\nprevious efforts impose certain assumptions between $Q_{tot}$ and $Q^{i}$ and\nlack theoretical groundings. Besides, they do not explicitly consider the\nagent-level impact of individuals to the whole system when transforming\nindividual $Q^{i}$s into $Q_{tot}$. In this paper, we theoretically derive a\ngeneral formula of $Q_{tot}$ in terms of $Q^{i}$, based on which we can\nnaturally implement a multi-head attention formation to approximate $Q_{tot}$,\nresulting in not only a refined representation of $Q_{tot}$ with an agent-level\nattention mechanism, but also a tractable maximization algorithm of\ndecentralized policies. Extensive experiments demonstrate that our method\noutperforms state-of-the-art MARL methods on the widely adopted StarCraft\nbenchmark across different scenarios, and attention analysis is further\nconducted with valuable insights.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:48:03 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 05:20:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yang", "Yaodong", ""], ["Hao", "Jianye", ""], ["Liao", "Ben", ""], ["Shao", "Kun", ""], ["Chen", "Guangyong", ""], ["Liu", "Wulong", ""], ["Tang", "Hongyao", ""]]}, {"id": "2002.03950", "submitter": "Yaodong Yang", "authors": "Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen,\n  Yujing Hu, Changjie Fan, Zhongyu Wei", "title": "Q-value Path Decomposition for Deep Multiagent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep multiagent reinforcement learning (MARL) has become a highly\nactive research area as many real-world problems can be inherently viewed as\nmultiagent systems. A particularly interesting and widely applicable class of\nproblems is the partially observable cooperative multiagent setting, in which a\nteam of agents learns to coordinate their behaviors conditioning on their\nprivate observations and commonly shared global reward signals. One natural\nsolution is to resort to the centralized training and decentralized execution\nparadigm. During centralized training, one key challenge is the multiagent\ncredit assignment: how to allocate the global rewards for individual agent\npolicies for better coordination towards maximizing system-level's benefits. In\nthis paper, we propose a new method called Q-value Path Decomposition (QPD) to\ndecompose the system's global Q-values into individual agents' Q-values. Unlike\nprevious works which restrict the representation relation of the individual\nQ-values and the global one, we leverage the integrated gradient attribution\ntechnique into deep MARL to directly decompose global Q-values along trajectory\npaths to assign credits for agents. We evaluate QPD on the challenging\nStarCraft II micromanagement tasks and show that QPD achieves the\nstate-of-the-art performance in both homogeneous and heterogeneous multiagent\nscenarios compared with existing cooperative MARL algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:03:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yang", "Yaodong", ""], ["Hao", "Jianye", ""], ["Chen", "Guangyong", ""], ["Tang", "Hongyao", ""], ["Chen", "Yingfeng", ""], ["Hu", "Yujing", ""], ["Fan", "Changjie", ""], ["Wei", "Zhongyu", ""]]}, {"id": "2002.04302", "submitter": "David Pelta", "authors": "David A. Pelta and Jose L. Verdegay and Maria T. Lamata and Carlos\n  Cruz Corona", "title": "Trust dynamics and user attitudes on recommendation errors: preliminary\n  results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GT cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence based systems may be used as digital nudging\ntechniques that can steer or coerce users to make decisions not always aligned\nwith their true interests. When such systems properly address the issues of\nFairness, Accountability, Transparency, and Ethics, then the trust of the user\nin the system would just depend on the system's output. The aim of this paper\nis to propose a model for exploring how good and bad recommendations affect the\noverall trust in an idealized recommender system that issues recommendations\nover a resource with limited capacity. The impact of different users attitudes\non trust dynamics is also considered. Using simulations, we ran a large set of\nexperiments that allowed to observe that: 1) under certain circumstances, all\nthe users ended accepting the recommendations; and 2) the user attitude\n(controlled by a single parameter balancing the gain/loss of trust after a\ngood/bad recommendation) has a great impact in the trust dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:52:53 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Pelta", "David A.", ""], ["Verdegay", "Jose L.", ""], ["Lamata", "Maria T.", ""], ["Corona", "Carlos Cruz", ""]]}, {"id": "2002.04517", "submitter": "Phillip Hyatt", "authors": "Phillip Hyatt, Zachary Brock, Marc D. Killpack", "title": "A Versatile Multi-Robot Monte Carlo Tree Search Planner for On-Line\n  Coverage Path Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots hold great promise in reducing the need for humans to perform\njobs such as vacuuming, seeding,harvesting, painting, search and rescue, and\ninspection. In practice, these tasks must often be done without an exact map of\nthe area and could be completed more quickly through the use of multiple robots\nworking together. The task of simultaneously covering and mapping an area with\nmultiple robots is known as multi-robot on-line coverage and is a growing area\nof research. Many multi-robot on-line coverage path planning algorithms have\nbeen developed as extensions of well established off-line coverage algorithms.\nIn this work we introduce a novel approach to multi-robot on-line coverage path\nplanning based on a method borrowed from game theory and machine learning-\nMonte Carlo Tree Search. We implement a Monte Carlo Tree Search planner and\ncompare completion times against a Boustrophedon-based on-line multi-robot\nplanner. The MCTS planner is shown to perform on par with the conventional\nBoustrophedon algorithm in simulations varying the number of robots and the\ndensity of obstacles in the map. The versatility of the MCTS planner is\ndemonstrated by incorporating secondary objectives such as turn minimization\nwhile performing the same coverage task. The versatility of the MCTS planner\nsuggests it is well suited to many multi-objective tasks that arise in mobile\nrobotics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:18:09 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Hyatt", "Phillip", ""], ["Brock", "Zachary", ""], ["Killpack", "Marc D.", ""]]}, {"id": "2002.04525", "submitter": "William Derigent", "authors": "William Derigent (CRAN), Olivier Cardin (PSI), Damien Trentesaux\n  (LAMIH)", "title": "Industry 4.0: contributions of holonic manufacturing control\n  architectures and future challenges", "comments": "Journal of Intelligent Manufacturing, Springer Verlag (Germany), 2020", "journal-ref": null, "doi": "10.1007/s10845-020-01532-x", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility claimed by the next generation production systems induces a\ndeep modification of the behaviour and the core itself of the control systems.\nOver-connectivity and data management abilities targeted by Industry 4.0\nparadigm enable the emergence of more flexible and reactive control systems,\nbased on the cooperation of autonomous and connected entities in the\ndecision-making process. From most relevant articles extracted from existing\nliterature, a list of 10 key enablers for Industry 4.0 is first presented.\nDuring the last 20 years, the holonic paradigm has become a major paradigm of\nIntelligent Manufacturing Systems. After the presentation of the holonic\nparadigm and holon properties, this article highlights how historical and\ncurrent holonic control architectures can partly fulfil I4.0 key enablers. The\nremaining unfulfilled key enablers are then the subject of an extensive\ndiscussion on the remaining research perspectives on holonic architectures\nneeded to achieve a complete support of Industry4.0.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:39:39 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Derigent", "William", "", "CRAN"], ["Cardin", "Olivier", "", "PSI"], ["Trentesaux", "Damien", "", "LAMIH"]]}, {"id": "2002.04734", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "Fast Complete Algorithm for Multiplayer Nash Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA econ.TH math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new complete algorithm for computing Nash equilibrium in\nmultiplayer general-sum games, based on a quadratically-constrained feasibility\nprogram formulation. We demonstrate that the algorithm runs significantly\nfaster than the prior fastest complete algorithm on several game classes\npreviously studied and that its runtimes even outperform the best incomplete\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 23:42:14 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 19:14:48 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 06:00:01 GMT"}, {"version": "v4", "created": "Tue, 26 May 2020 01:59:58 GMT"}, {"version": "v5", "created": "Fri, 5 Jun 2020 21:23:10 GMT"}, {"version": "v6", "created": "Mon, 27 Jul 2020 05:49:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "2002.04946", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Virginia Bordignon, Augusto Santos, Ali H. Sayed", "title": "Learning Graph Influence from Social Interactions", "comments": "To be presented at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social learning, agents form their opinions or beliefs about certain\nhypotheses by exchanging local information. This work considers the recent\nparadigm of weak graphs, where the network is partitioned into sending and\nreceiving components, with the former having the possibility of exerting a\ndomineering effect on the latter. Such graph structures are prevalent over\nsocial platforms. We will not be focusing on the direct social learning problem\n(which examines what agents learn), but rather on the dual or reverse learning\nproblem (which examines how agents learned). Specifically, from observations of\nthe stream of beliefs at certain agents, we would like to examine whether it is\npossible to learn the strength of the connections (influences) from sending\ncomponents in the network to these receiving agents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 12:27:07 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Matta", "Vincenzo", ""], ["Bordignon", "Virginia", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2002.05147", "submitter": "Kyle Tilbury", "authors": "Kyle Tilbury and Jesse Hoey", "title": "Multi-Agent Reinforcement Learning and Human Social Factors in Climate\n  Change Mitigation", "comments": "Accepted paper at COMARL AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex real-world problems, such as climate change mitigation, are\nintertwined with human social factors. Climate change mitigation, a social\ndilemma made difficult by the inherent complexities of human behavior, has an\nimpact at a global scale. We propose applying multi-agent reinforcement\nlearning (MARL) in this setting to develop intelligent agents that can\ninfluence the social factors at play in climate change mitigation. There are\nethical, practical, and technical challenges that must be addressed when\ndeploying MARL in this way. In this paper, we present these challenges and\noutline an approach to address them. Understanding how intelligent agents can\nbe used to impact human social factors is important to prevent their abuse and\ncan be beneficial in furthering our knowledge of these complex problems as a\nwhole. The challenges we present are not limited to our specific application\nbut are applicable to broader MARL. Thus, developing MARL for social factors in\nclimate change mitigation helps address general problems hindering MARL's\napplicability to other real-world problems while also motivating discussion on\nthe social implications of MARL deployment.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:46:48 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tilbury", "Kyle", ""], ["Hoey", "Jesse", ""]]}, {"id": "2002.05188", "submitter": "Eric Silverman", "authors": "Umberto Gostoli, Eric Silverman", "title": "Social and Child Care Provision in Kinship Networks: an Agent-Based\n  Model", "comments": "24 pages, 20 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242779", "report-no": null, "categories": "cs.CY cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing for the needs of the vulnerable is a critical component of social\nand health policy-making. In particular, caring for children and for vulnerable\nolder people is vital to the wellbeing of millions of families throughout the\nworld. In most developed countries, this care is provided through both formal\nand informal means, and is therefore governed by complex policies that interact\nin non-obvious ways with other areas of policy-making. In this paper we present\nan agent-based model of social and child care provision in the UK, in which\nagents can provide informal care or pay for private care for their relatives.\nAgents make care decisions based on numerous factors including their health\nstatus, employment, financial situation, and social and physical distance to\nthose in need. Simulation results show that the model can produce plausible\npatterns of care need and availability, and therefore can provide an important\naid to this complex area of policy-making. We conclude that the model's use of\nkinship networks for distributing care and the explicit modelling of\ninteractions between social care and child care will enable policy-makers to\ndevelop more informed policy interventions in these critical areas.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 19:31:39 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 15:13:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gostoli", "Umberto", ""], ["Silverman", "Eric", ""]]}, {"id": "2002.05233", "submitter": "Emanuele Pesce Mr.", "authors": "Emanuele Pesce, Giovanni Montana", "title": "Learning Multi-Agent Coordination through Graph-driven Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem of learning collaborative behaviour through\ncommunication in multi-agent systems using deep reinforcement learning. A\nconnectivity-driven communication (CDC) algorithm is proposed to address three\nkey aspects: what agents to involve in the communication, what information\ncontent to share, and how often to share it. The multi-agent system is modelled\nas a weighted graph with nodes representing agents. The unknown edge weights\nreflect the degree of communication between pairs of agents, which depends on a\ndiffusion process on the graph - the heat kernel. An optimal communication\nstrategy, tightly coupled with overall graph topology, is learned end-to-end\nconcurrently with the agents' policy so as to maximise future expected returns.\nEmpirical results show that CDC is capable of superior performance over\nalternative algorithms for a range of cooperative navigation tasks, and that\nthe learned graph structures can be interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 20:58:33 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 20:00:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Pesce", "Emanuele", ""], ["Montana", "Giovanni", ""]]}, {"id": "2002.05373", "submitter": "Usman Khan", "authors": "Ran Xin, Soummya Kar, Usman A. Khan", "title": "Gradient tracking and variance reduction for decentralized optimization\n  and machine learning", "comments": "accepted for publication, IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized methods to solve finite-sum minimization problems are important\nin many signal processing and machine learning tasks where the data is\ndistributed over a network of nodes and raw data sharing is not permitted due\nto privacy and/or resource constraints. In this article, we review\ndecentralized stochastic first-order methods and provide a unified algorithmic\nframework that combines variance-reduction with gradient tracking to achieve\nboth robust performance and fast convergence. We provide explicit theoretical\nguarantees of the corresponding methods when the objective functions are smooth\nand strongly-convex, and show their applicability to non-convex problems via\nnumerical experiments. Throughout the article, we provide intuitive\nillustrations of the main technical ideas by casting appropriate tradeoffs and\ncomparisons among the methods of interest and by highlighting applications to\ndecentralized training of machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 07:17:07 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Xin", "Ran", ""], ["Kar", "Soummya", ""], ["Khan", "Usman A.", ""]]}, {"id": "2002.05410", "submitter": "Tchuitcheu Willy Carlos", "authors": "Willy Carlos Tchuitcheu, Christophe Bobda, and Md Jubaer Hossain\n  Pantho", "title": "Internet of Smart-Cameras for Traffic Lights Optimization in Smart\n  Cities", "comments": "12 pages", "journal-ref": "Internet of Things(2020)", "doi": "10.1016/j.iot.2020.100207", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart and decentralized control systems have recently been proposed to handle\nthe growing traffic congestion in urban cities. Proposed smart traffic light\nsolutions based on Wireless Sensor Network and Vehicular Ad-hoc NETwork are\neither unreliable and inflexible or complex and costly. Furthermore, the\nhandling of special vehicles such as emergency is still not viable, especially\nduring busy hours. Inspired by the emergence of distributed smart cameras, we\npresent a novel approach to traffic control at intersections. Our approach uses\nsmart cameras at intersections along with image understanding for real-time\ntraffic monitoring and assessment. Besides understanding the traffic flow, the\ncameras can detect and track special vehicles and help prioritize emergency\ncases. Traffic violations can be identified as well and traffic statistics\ncollected. In this paper, we introduce a flexible, adaptive and distributed\ncontrol algorithm that uses the information provided by distributed smart\ncameras to efficiently control traffic signals. Experimental results show that\nour collision-free approach outperforms the state-of-the-art of the average\nuser's waiting time in the queue and improves the routing of emergency vehicles\nin a cross congestion area.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 09:51:34 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tchuitcheu", "Willy Carlos", ""], ["Bobda", "Christophe", ""], ["Pantho", "Md Jubaer Hossain", ""]]}, {"id": "2002.05706", "submitter": "Junqi Wang", "authors": "Junqi Wang, Pei Wang, Patrick Shafto", "title": "Sequential Cooperative Bayesian Inference", "comments": "25 pages, 22 figures, accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation is often implicitly assumed when learning from other agents.\nCooperation implies that the agent selecting the data, and the agent learning\nfrom the data, have the same goal, that the learner infer the intended\nhypothesis. Recent models in human and machine learning have demonstrated the\npossibility of cooperation. We seek foundational theoretical results for\ncooperative inference by Bayesian agents through sequential data. We develop\nnovel approaches analyzing consistency, rate of convergence and stability of\nSequential Cooperative Bayesian Inference (SCBI). Our analysis of the\neffectiveness, sample efficiency and robustness show that cooperation is not\nonly possible in specific instances but theoretically well-founded in general.\nWe discuss implications for human-human and human-machine cooperation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:48:06 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 21:21:52 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 13:25:21 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Wang", "Junqi", ""], ["Wang", "Pei", ""], ["Shafto", "Patrick", ""]]}, {"id": "2002.05755", "submitter": "Maximilian Kloock", "authors": "Maximilian Kloock, Patrick Scheffe, Isabelle T\\\"ulleners, Janis\n  Maczijewski, Stefan Kowalewski, and Bassam Alrifaee", "title": "Vision-Based Real-Time Indoor Positioning System for Multiple Vehicles", "comments": "Accepted in IFAC Wolrd Congress 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel external indoor positioning system that computes the\nposition and orientation of multiple model-scale vehicles. For this purpose, we\nuse a camera mounted at a height of 3.3m and LEDs attached to each vehicle. We\nreach an accuracy of about 1.1 cm for the position and around 0.6 {\\deg} for\nthe orientation in the mean. Our system is real-time capable with a soft\ndeadline of 20 ms. Moreover, it is robust against changing lighting conditions\nand reflections.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 19:35:30 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 08:05:38 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kloock", "Maximilian", ""], ["Scheffe", "Patrick", ""], ["T\u00fclleners", "Isabelle", ""], ["Maczijewski", "Janis", ""], ["Kowalewski", "Stefan", ""], ["Alrifaee", "Bassam", ""]]}, {"id": "2002.05966", "submitter": "Hao Cheng", "authors": "Hao Cheng, Wentong Liao, Michael Ying Yang, Monika Sester, Bodo\n  Rosenhahn", "title": "MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory\n  Prediction in Mixed Traffic", "comments": "8 pages, 5 figures, code is available on\n  https://github.com/haohao11/MCENET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction in urban mixed-traffic zones (a.k.a. shared spaces) is\ncritical for many intelligent transportation systems, such as intent detection\nfor autonomous driving. However, there are many challenges to predict the\ntrajectories of heterogeneous road agents (pedestrians, cyclists and vehicles)\nat a microscopical level. For example, an agent might be able to choose\nmultiple plausible paths in complex interactions with other agents in varying\nenvironments. To this end, we propose an approach named Multi-Context Encoder\nNetwork (MCENET) that is trained by encoding both past and future scene\ncontext, interaction context and motion information to capture the patterns and\nvariations of the future trajectories using a set of stochastic latent\nvariables. In inference time, we combine the past context and motion\ninformation of the target agent with samplings of the latent variables to\npredict multiple realistic trajectories in the future. Through experiments on\nseveral datasets of varying scenes, our method outperforms some of the recent\nstate-of-the-art methods for mixed traffic trajectory prediction by a large\nmargin and more robust in a very challenging environment. The impact of each\ncontext is justified via ablation studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:04:41 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 15:53:02 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 13:39:05 GMT"}, {"version": "v4", "created": "Sun, 5 Apr 2020 12:08:51 GMT"}, {"version": "v5", "created": "Tue, 23 Jun 2020 13:06:17 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Cheng", "Hao", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Sester", "Monika", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2002.06000", "submitter": "Borja Gonzalez Leon", "authors": "Borja G. Le\\'on and Francesco Belardinelli", "title": "Extended Markov Games to Learn Multiple Tasks in Multi-Agent\n  Reinforcement Learning", "comments": "Long version of the correspondent ECAI 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of Formal Methods with Reinforcement Learning (RL) has\nrecently attracted interest as a way for single-agent RL to learn multiple-task\nspecifications. In this paper we extend this convergence to multi-agent\nsettings and formally define Extended Markov Games as a general mathematical\nmodel that allows multiple RL agents to concurrently learn various\nnon-Markovian specifications. To introduce this new model we provide formal\ndefinitions and proofs as well as empirical tests of RL algorithms running on\nthis framework. Specifically, we use our model to train two different\nlogic-based multi-agent RL algorithms to solve diverse settings of\nnon-Markovian co-safe LTL specifications.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:37:41 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Le\u00f3n", "Borja G.", ""], ["Belardinelli", "Francesco", ""]]}, {"id": "2002.06080", "submitter": "Ezra Tampubolon", "authors": "Ezra Tampubolon and Haris Ceribasic and Holger Boche", "title": "Resource-Aware Control via Dynamic Pricing for Congestion Game with\n  Finite-Time Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.GT cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congestion game is a widely used model for modern networked applications. A\ncentral issue in such applications is that the selfish behavior of the\nparticipants may result in resource overloading and negative externalities for\nthe system participants. In this work, we propose a pricing mechanism that\nguarantees the sub-linear increase of the time-cumulative violation of the\nresource load constraints. The feature of our method is that it is\nresource-centric in the sense that it depends on the congestion state of the\nresources and not on specific characteristics of the system participants. This\nfeature makes our mechanism scalable, flexible, and privacy-preserving.\nMoreover, we show by numerical simulations that our pricing mechanism has no\nsignificant effect on the agents' welfare in contrast to the improvement of the\ncapacity violation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:36:04 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Tampubolon", "Ezra", ""], ["Ceribasic", "Haris", ""], ["Boche", "Holger", ""]]}, {"id": "2002.06215", "submitter": "Navid Naderializadeh", "authors": "Navid Naderializadeh, Jaroslaw Sydir, Meryem Simsek, Hosein Nikopour", "title": "Resource Management in Wireless Networks via Multi-Agent Deep\n  Reinforcement Learning", "comments": "Final version to appear in IEEE Transactions on Wireless\n  Communications", "journal-ref": null, "doi": "10.1109/TWC.2021.3051163", "report-no": null, "categories": "cs.LG cs.IT cs.MA eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mechanism for distributed resource management and interference\nmitigation in wireless networks using multi-agent deep reinforcement learning\n(RL). We equip each transmitter in the network with a deep RL agent that\nreceives delayed observations from its associated users, while also exchanging\nobservations with its neighboring agents, and decides on which user to serve\nand what transmit power to use at each scheduling interval. Our proposed\nframework enables agents to make decisions simultaneously and in a distributed\nmanner, unaware of the concurrent decisions of other agents. Moreover, our\ndesign of the agents' observation and action spaces is scalable, in the sense\nthat an agent trained on a scenario with a specific number of transmitters and\nusers can be applied to scenarios with different numbers of transmitters and/or\nusers. Simulation results demonstrate the superiority of our proposed approach\ncompared to decentralized baselines in terms of the tradeoff between average\nand $5^{th}$ percentile user rates, while achieving performance close to, and\neven in certain cases outperforming, that of a centralized\ninformation-theoretic baseline. We also show that our trained agents are robust\nand maintain their performance gains when experiencing mismatches between train\nand test deployments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 19:01:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 06:04:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Naderializadeh", "Navid", ""], ["Sydir", "Jaroslaw", ""], ["Simsek", "Meryem", ""], ["Nikopour", "Hosein", ""]]}, {"id": "2002.06241", "submitter": "Jiachen Li", "authors": "Jiachen Li, Hengbo Ma, Zhihao Zhang, Masayoshi Tomizuka", "title": "Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein\n  Graph Double-Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective understanding of the environment and accurate trajectory prediction\nof surrounding dynamic obstacles are indispensable for intelligent mobile\nsystems (like autonomous vehicles and social robots) to achieve safe and\nhigh-quality planning when they navigate in highly interactive and crowded\nscenarios. Due to the existence of frequent interactions and uncertainty in the\nscene evolution, it is desired for the prediction system to enable relational\nreasoning on different entities and provide a distribution of future\ntrajectories for each agent. In this paper, we propose a generic generative\nneural system (called Social-WaGDAT) for multi-agent trajectory prediction,\nwhich makes a step forward to explicit interaction modeling by incorporating\nrelational inductive biases with a dynamic graph representation and leverages\nboth trajectory and scene context information. We also employ an efficient\nkinematic constraint layer applied to vehicle trajectory prediction which not\nonly ensures physical feasibility but also enhances model performance. The\nproposed system is evaluated on three public benchmark datasets for trajectory\nprediction, where the agents cover pedestrians, cyclists and on-road vehicles.\nThe experimental results demonstrate that our model achieves better performance\nthan various baseline approaches in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 20:11:13 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Zhang", "Zhihao", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2002.06306", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios and Abulhair Saparov and Tom Mitchell", "title": "Jelly Bean World: A Testbed for Never-Ending Learning", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": "International Conference on Learning Representations 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has shown growing success in recent years. However, current\nmachine learning systems are highly specialized, trained for particular\nproblems or domains, and typically on a single narrow dataset. Human learning,\non the other hand, is highly general and adaptable. Never-ending learning is a\nmachine learning paradigm that aims to bridge this gap, with the goal of\nencouraging researchers to design machine learning systems that can learn to\nperform a wider variety of inter-related tasks in more complex environments. To\ndate, there is no environment or testbed to facilitate the development and\nevaluation of never-ending learning systems. To this end, we propose the Jelly\nBean World testbed. The Jelly Bean World allows experimentation over\ntwo-dimensional grid worlds which are filled with items and in which agents can\nnavigate. This testbed provides environments that are sufficiently complex and\nwhere more generally intelligent algorithms ought to perform better than\ncurrent state-of-the-art reinforcement learning approaches. It does so by\nproducing non-stationary environments and facilitating experimentation with\nmulti-task, multi-agent, multi-modal, and curriculum learning settings. We hope\nthat this new freely-available software will prompt new research and interest\nin the development and evaluation of never-ending learning systems and more\nbroadly, general intelligence systems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 02:43:16 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""], ["Saparov", "Abulhair", ""], ["Mitchell", "Tom", ""]]}, {"id": "2002.06417", "submitter": "Chao Wang", "authors": "Chao Wang, Stephan Hasler, Manuel Muehlig, Frank Joublin, Antonello\n  Ceravola, Joerg Deigmoeller, Lydia Fischer", "title": "Designing Interaction for Multi-agent Cooperative System in an Office\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future intelligent system will involve very various types of artificial\nagents, such as mobile robots, smart home infrastructure or personal devices,\nwhich share data and collaborate with each other to execute certain\ntasks.Designing an efficient human-machine interface, which can support users\nto express needs to the system, supervise the collaboration progress of\ndifferent entities and evaluate the result, will be challengeable. This paper\npresents the design and implementation of the human-machine interface of\nIntelligent Cyber-Physical system (ICPS),which is a multi-entity coordination\nsystem of robots and other smart devices in a working environment. ICPS gathers\nsensory data from entities and then receives users' command, then optimizes\nplans to utilize the capability of different entities to serve people. Using\nmulti-model interaction methods, e.g. graphical interfaces, speech interaction,\ngestures and facial expressions, ICPS is able to receive inputs from users\nthrough different entities, keep users aware of the progress and accomplish the\ntask efficiently\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 17:36:00 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wang", "Chao", ""], ["Hasler", "Stephan", ""], ["Muehlig", "Manuel", ""], ["Joublin", "Frank", ""], ["Ceravola", "Antonello", ""], ["Deigmoeller", "Joerg", ""], ["Fischer", "Lydia", ""]]}, {"id": "2002.06684", "submitter": "Rose Wang", "authors": "Rose E. Wang, Michael Everett, Jonathan P. How", "title": "R-MADDPG for Partially Observable Environments and Limited Communication", "comments": "Reinforcement Learning for Real Life (RL4RealLife) Workshop in the\n  36th International Conference on Machine Learning, Long Beach, California,\n  USA, 2019", "journal-ref": "Reinforcement Learning for Real Life (RL4RealLife) Workshop in the\n  36th International Conference on Machine Learning, Long Beach, California,\n  USA, 2019", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are several real-world tasks that would benefit from applying\nmultiagent reinforcement learning (MARL) algorithms, including the coordination\namong self-driving cars. The real world has challenging conditions for\nmultiagent learning systems, such as its partial observable and nonstationary\nnature. Moreover, if agents must share a limited resource (e.g. network\nbandwidth) they must all learn how to coordinate resource use. This paper\nintroduces a deep recurrent multiagent actor-critic framework (R-MADDPG) for\nhandling multiagent coordination under partial observable set-tings and limited\ncommunication. We investigate recurrency effects on performance and\ncommunication use of a team of agents. We demonstrate that the resulting\nframework learns time dependencies for sharing missing observations, handling\nresource limitations, and developing different communication patterns among\nagents.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 21:25:44 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 02:55:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Wang", "Rose E.", ""], ["Everett", "Michael", ""], ["How", "Jonathan P.", ""]]}, {"id": "2002.06723", "submitter": "Zhenyu Shou", "authors": "Zhenyu Shou, Xuan Di", "title": "Reward Design for Driver Repositioning Using Multi-Agent Reinforcement\n  Learning", "comments": "28 pages, 20 figures, published in Transportation Research Part C 119\n  (2020) 102738", "journal-ref": null, "doi": "10.1016/j.trc.2020.102738", "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large portion of passenger requests is reportedly unserviced, partially due\nto vacant for-hire drivers' cruising behavior during the passenger seeking\nprocess. This paper aims to model the multi-driver repositioning task through a\nmean field multi-agent reinforcement learning (MARL) approach that captures\ncompetition among multiple agents. Because the direct application of MARL to\nthe multi-driver system under a given reward mechanism will likely yield a\nsuboptimal equilibrium due to the selfishness of drivers, this study proposes a\nreward design scheme with which a more desired equilibrium can be reached. To\neffectively solve the bilevel optimization problem with upper level as the\nreward design and the lower level as a multi-agent system, a Bayesian\noptimization (BO) algorithm is adopted to speed up the learning process. We\nthen apply the bilevel optimization model to two case studies, namely,\ne-hailing driver repositioning under service charge and multiclass taxi driver\nrepositioning under NYC congestion pricing. In the first case study, the model\nis validated by the agreement between the derived optimal control from BO and\nthat from an analytical solution. With a simple piecewise linear service\ncharge, the objective of the e-hailing platform can be increased by 8.4%. In\nthe second case study, an optimal toll charge of $5.1 is solved using BO, which\nimproves the objective of city planners by 7.9%, compared to that without any\ntoll charge. Under this optimal toll charge, the number of taxis in the NYC\ncentral business district is decreased, indicating a better traffic condition,\nwithout substantially increasing the crowdedness of the subway system.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 00:10:58 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 17:51:40 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 16:48:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Shou", "Zhenyu", ""], ["Di", "Xuan", ""]]}, {"id": "2002.07066", "submitter": "Yudong Chen", "authors": "Qiaomin Xie, Yudong Chen, Zhaoran Wang, Zhuoran Yang", "title": "Learning Zero-Sum Simultaneous-Move Markov Games Using Function\n  Approximation and Correlated Equilibrium", "comments": "Accepted for presentation at COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop provably efficient reinforcement learning algorithms for\ntwo-player zero-sum finite-horizon Markov games with simultaneous moves. To\nincorporate function approximation, we consider a family of Markov games where\nthe reward function and transition kernel possess a linear structure. Both the\noffline and online settings of the problems are considered. In the offline\nsetting, we control both players and aim to find the Nash Equilibrium by\nminimizing the duality gap. In the online setting, we control a single player\nplaying against an arbitrary opponent and aim to minimize the regret. For both\nsettings, we propose an optimistic variant of the least-squares minimax value\niteration algorithm. We show that our algorithm is computationally efficient\nand provably achieves an $\\tilde O(\\sqrt{d^3 H^3 T} )$ upper bound on the\nduality gap and regret, where $d$ is the linear dimension, $H$ the horizon and\n$T$ the total number of timesteps. Our results do not require additional\nassumptions on the sampling model.\n  Our setting requires overcoming several new challenges that are absent in\nMarkov decision processes or turn-based Markov games. In particular, to achieve\noptimism with simultaneous moves, we construct both upper and lower confidence\nbounds of the value function, and then compute the optimistic policy by solving\na general-sum matrix game with these bounds as the payoff matrices. As finding\nthe Nash Equilibrium of a general-sum game is computationally hard, our\nalgorithm instead solves for a Coarse Correlated Equilibrium (CCE), which can\nbe obtained efficiently. To our best knowledge, such a CCE-based scheme for\noptimism has not appeared in the literature and might be of interest in its own\nright.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:04:16 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 21:38:20 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 21:09:42 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Xie", "Qiaomin", ""], ["Chen", "Yudong", ""], ["Wang", "Zhaoran", ""], ["Yang", "Zhuoran", ""]]}, {"id": "2002.07378", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Keyou You, Tamer Ba\\c{s}ar", "title": "Distributed Adaptive Newton Methods with Globally Superlinear\n  Convergence", "comments": "Submitted to IEEE Transactions on Automatic Control. 14 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the distributed optimization problem over a network\nwhere the global objective is to optimize a sum of local functions using only\nlocal computation and communication. Since the existing algorithms either adopt\na linear consensus mechanism, which converges at best linearly, or assume that\neach node starts sufficiently close to an optimal solution, they cannot achieve\nglobally superlinear convergence. To break through the linear consensus rate,\nwe propose a finite-time set-consensus method, and then incorporate it into\nPolyak's adaptive Newton method, leading to our distributed adaptive Newton\nalgorithm (DAN). To avoid transmitting local Hessians, we adopt a low-rank\napproximation idea to compress the Hessian and design a communication-efficient\nDAN-LA. Then, the size of transmitted messages in DAN-LA is reduced to $O(p)$\nper iteration, where $p$ is the dimension of decision vectors and is the same\nas the first-order methods. We show that DAN and DAN-LA can globally achieve\nquadratic and superlinear convergence rates, respectively. Numerical\nexperiments on logistic regression problems are finally conducted to show the\nadvantages over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 05:29:30 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:43:48 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "2002.07596", "submitter": "Thomas Budzinski", "authors": "S\\'ebastien Bubeck and Thomas Budzinski", "title": "Coordination without communication: optimal regret in two players\n  multi-armed bandits", "comments": "28 pages, 5 figures. V2: minor revision", "journal-ref": "COLT 2020", "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two agents playing simultaneously the same stochastic three-armed\nbandit problem. The two agents are cooperating but they cannot communicate. We\npropose a strategy with no collisions at all between the players (with very\nhigh probability), and with near-optimal regret $O(\\sqrt{T \\log(T)})$. We also\nargue that the extra logarithmic term $\\sqrt{\\log(T)}$ should be necessary by\nproving a lower bound for a full information variant of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:35:42 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 19:11:02 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Budzinski", "Thomas", ""]]}, {"id": "2002.07788", "submitter": "Ho-Chun Herbert Chang", "authors": "Ho-Chun Herbert Chang", "title": "Multi-Issue Bargaining With Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negotiation is a process where agents aim to work through disputes and\nmaximize their surplus. As the use of deep reinforcement learning in bargaining\ngames is unexplored, this paper evaluates its ability to exploit, adapt, and\ncooperate to produce fair outcomes. Two actor-critic networks were trained for\nthe bidding and acceptance strategy, against time-based agents, behavior-based\nagents, and through self-play. Gameplay against these agents reveals three key\nfindings. 1) Neural agents learn to exploit time-based agents, achieving clear\ntransitions in decision preference values. The Cauchy distribution emerges as\nsuitable for sampling offers, due to its peaky center and heavy tails. The\nkurtosis and variance sensitivity of the probability distributions used for\ncontinuous control produce trade-offs in exploration and exploitation. 2)\nNeural agents demonstrate adaptive behavior against different combinations of\nconcession, discount factors, and behavior-based strategies. 3) Most\nimportantly, neural agents learn to cooperate with other behavior-based agents,\nin certain cases utilizing non-credible threats to force fairer results. This\nbears similarities with reputation-based strategies in the evolutionary\ndynamics, and departs from equilibria in classical game theory.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:33:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Chang", "Ho-Chun Herbert", ""]]}, {"id": "2002.08030", "submitter": "Tianpei Yang", "authors": "Tianpei Yang, Weixun Wang, Hongyao Tang, Jianye Hao, Zhaopeng Meng,\n  Hangyu Mao, Dong Li, Wulong Liu, Chengwei Zhang, Yujing Hu, Yingfeng Chen and\n  Changjie Fan", "title": "Transfer among Agents: An Efficient Multiagent Transfer Learning\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer Learning has shown great potential to enhance the single-agent\nReinforcement Learning (RL) efficiency. Similarly, Multiagent RL (MARL) can\nalso be accelerated if agents can share knowledge with each other. However, it\nremains a problem of how an agent should learn from other agents. In this\npaper, we propose a novel Multiagent Option-based Policy Transfer (MAOPT)\nframework to improve MARL efficiency. MAOPT learns what advice to provide and\nwhen to terminate it for each agent by modeling multiagent policy transfer as\nthe option learning problem. Our framework provides two kinds of option\nlearning methods in terms of what experience is used during training. One is\nthe global option advisor, which uses the global experience for the update. The\nother is the local option advisor, which uses each agent's local experience\nwhen only each agent's local experiences can be obtained due to partial\nobservability. While in this setting, each agent's experience may be\ninconsistent with each other, which may cause the inaccuracy and oscillation of\nthe option-value's estimation. Therefore, we propose the successor\nrepresentation option learning to solve it by decoupling the environment\ndynamics from rewards and learning the option-value under each agent's\npreference. MAOPT can be easily combined with existing deep RL and MARL\napproaches, and experimental results show it significantly boosts the\nperformance of existing methods in both discrete and continuous state spaces.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 07:01:38 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:02:29 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 06:30:01 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yang", "Tianpei", ""], ["Wang", "Weixun", ""], ["Tang", "Hongyao", ""], ["Hao", "Jianye", ""], ["Meng", "Zhaopeng", ""], ["Mao", "Hangyu", ""], ["Li", "Dong", ""], ["Liu", "Wulong", ""], ["Zhang", "Chengwei", ""], ["Hu", "Yujing", ""], ["Chen", "Yingfeng", ""], ["Fan", "Changjie", ""]]}, {"id": "2002.08567", "submitter": "Md. Shirajum Munir", "authors": "Md. Shirajum Munir, Nguyen H. Tran, Walid Saad, Choong Seon Hong", "title": "Multi-Agent Meta-Reinforcement Learning for Self-Powered and Sustainable\n  Edge Computing Systems", "comments": "Accepted article by IEEE Transactions on Network and Service\n  Management, DOI: 10.1109/TNSM.2021.3057960. Copyright 2021 IEEE", "journal-ref": null, "doi": "10.1109/TNSM.2021.3057960", "report-no": null, "categories": "cs.LG cs.MA eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The stringent requirements of mobile edge computing (MEC) applications and\nfunctions fathom the high capacity and dense deployment of MEC hosts to the\nupcoming wireless networks. However, operating such high capacity MEC hosts can\nsignificantly increase energy consumption. Thus, a base station (BS) unit can\nact as a self-powered BS. In this paper, an effective energy dispatch mechanism\nfor self-powered wireless networks with edge computing capabilities is studied.\nFirst, a two-stage linear stochastic programming problem is formulated with the\ngoal of minimizing the total energy consumption cost of the system while\nfulfilling the energy demand. Second, a semi-distributed data-driven solution\nis proposed by developing a novel multi-agent meta-reinforcement learning\n(MAMRL) framework to solve the formulated problem. In particular, each BS plays\nthe role of a local agent that explores a Markovian behavior for both energy\nconsumption and generation while each BS transfers time-varying features to a\nmeta-agent. Sequentially, the meta-agent optimizes (i.e., exploits) the energy\ndispatch decision by accepting only the observations from each local agent with\nits own state information. Meanwhile, each BS agent estimates its own energy\ndispatch policy by applying the learned parameters from meta-agent. Finally,\nthe proposed MAMRL framework is benchmarked by analyzing deterministic,\nasymmetric, and stochastic environments in terms of non-renewable energy\nusages, energy cost, and accuracy. Experimental results show that the proposed\nMAMRL model can reduce up to 11% non-renewable energy usage and by 22.4% the\nenergy cost (with 95.8% prediction accuracy), compared to other baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 04:58:07 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 07:10:35 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 02:47:56 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Munir", "Md. Shirajum", ""], ["Tran", "Nguyen H.", ""], ["Saad", "Walid", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2002.08720", "submitter": "Hossein Khazaei", "authors": "Hossein Khazaei, Ramin Moslemi, Ratnesh Sharma", "title": "Stochastic Decision-Making Model for Aggregation of Residential Units\n  with PV-Systems and Storages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many residential energy consumers have installed photovoltaic (PV) panels and\nenergy storage systems. These residential users can aggregate and participate\nin the energy markets. A stochastic decision making model for an aggregation of\nthese residential units for participation in two-settlement markets is proposed\nin this paper. Scenarios are generated using Seasonal Autoregressive Integrated\nMoving Average (SARIMA) model and joint probability distribution function of\nthe forecast errors to model the uncertainties of the real-time prices, PV\ngenerations and demands. The proposed scenario generation model of this paper\ntreats forecast errors as random variable, which allows to reflect new\ninformation observed in the real-time market into scenario generation process\nwithout retraining SARIMA or re-fitting probability distribution functions over\nthe forecast errors. This approach significantly improves the computational\ntime of the proposed model. A simulation study is conducted for an aggregation\nof 6 residential units, and the results highlights the benefits of aggregation\nas well as the proposed stochastic decision-making model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:23:31 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Khazaei", "Hossein", ""], ["Moslemi", "Ramin", ""], ["Sharma", "Ratnesh", ""]]}, {"id": "2002.08878", "submitter": "Clement Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier and Pierre-Yves Oudeyer", "title": "Multi-Agent Reinforcement Learning as a Computational Tool for Language\n  Evolution Research: Historical Context and Future Challenges", "comments": null, "journal-ref": "Challenges and Opportunities for Multi-Agent Reinforcement\n  Learning (COMARL AAAI 2020-2021), AAAI Spring Symposium Series, Stanford\n  University, Palo Alto, California, USA", "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models of emergent communication in agent populations are\ncurrently gaining interest in the machine learning community due to recent\nadvances in Multi-Agent Reinforcement Learning (MARL). Current contributions\nare however still relatively disconnected from the earlier theoretical and\ncomputational literature aiming at understanding how language might have\nemerged from a prelinguistic substance. The goal of this paper is to position\nrecent MARL contributions within the historical context of language evolution\nresearch, as well as to extract from this theoretical and computational\nbackground a few challenges for future research.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:26:46 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 13:54:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2002.09047", "submitter": "Xiao Xu", "authors": "Xiao Xu, Qing Zhao", "title": "Distributed No-Regret Learning in Multi-Agent Systems", "comments": "Accepted for publication in IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial article, we give an overview of new challenges and\nrepresentative results on distributed no-regret learning in multi-agent systems\nmodeled as repeated unknown games. Four emerging game\ncharacteristics---dynamicity, incomplete and imperfect feedback, bounded\nrationality, and heterogeneity---that challenge canonical game models are\nexplored. For each of the four characteristics, we illuminate its implications\nand ramifications in game modeling, notions of regret, feasible game outcomes,\nand the design and analysis of distributed learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:30:17 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Xu", "Xiao", ""], ["Zhao", "Qing", ""]]}, {"id": "2002.09109", "submitter": "Mike Borowczak", "authors": "Shaya Wolf, Rafer Cooley, Mike Borowczak", "title": "Adversarial Impacts on Autonomous Decentralized Lightweight Swarms", "comments": "7 pages, 4 figures, 5th International IEEE Smart Cities Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decreased size and cost of Unmanned Aerial Vehicles (UAVs) and Unmanned\nGround Vehicles (UGVs) has enabled the use of swarms of unmanned autonomous\nvehicles to accomplish a variety of tasks. By utilizing swarming behaviors, it\nis possible to efficiently accomplish coordinated tasks while minimizing\nper-drone computational requirements. Some drones rely on decentralized\nprotocols that exhibit emergent behavior across the swarm. While fully\ndecentralized algorithms remove obvious attack vectors their susceptibility to\nexternal influence is less understood. This work investigates the influences\nthat can compromise the functionality of an autonomous swarm leading to\nhazardous situations and cascading vulnerabilities. When a swarm is tasked with\nmissions involving the safety or health of humans, external influences could\nhave serious consequences. The adversarial swarm in this work utilizes an\nattack vector embedded within the decentralized movement algorithm of a\npreviously defined autonomous swarm designed to create a perimeter sentry\nswarm. Various simulations confirm the adversarial swarm's ability to capture\nsignificant portions (6-23%) of the perimeter.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:31:04 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Wolf", "Shaya", ""], ["Cooley", "Rafer", ""], ["Borowczak", "Mike", ""]]}, {"id": "2002.09366", "submitter": "Nuno Crokidakis", "authors": "A. L. Oestereich, M. A. Pires, S. M. Duarte Queir\\'os, N. Crokidakis", "title": "Hysteresis and disorder-induced order in continuous kinetic-like opinion\n  dynamics in complex networks", "comments": "12 pages, 4 figures, submitted", "journal-ref": "Chaos, Solitons & Fractals 137, 109893 (2020)", "doi": "10.1016/j.chaos.2020.109893", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we tackle a kinetic-like model of opinions dynamics in a\nnetworked population endued with a quenched plurality and polarization.\nAdditionally, we consider pairwise interactions that are restrictive, which is\nmodeled with a smooth bounded confidence. Our results show the interesting\nemergence of nonequilibrium hysteresis and heterogeneity-assisted ordering.\nSuch counterintuitive phenomena are robust to different types of network\narchitectures such as random, small-world and scale-free.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 15:47:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Oestereich", "A. L.", ""], ["Pires", "M. A.", ""], ["Queir\u00f3s", "S. M. Duarte", ""], ["Crokidakis", "N.", ""]]}, {"id": "2002.09598", "submitter": "Barton E. Lee", "authors": "Haris Aziz and Barton E. Lee", "title": "A characterization of proportionally representative committees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known axiom for proportional representation is Proportionality of\nSolid Coalitions (PSC). We characterize committees satisfying PSC as possible\noutcomes of the Minimal Demand rule, which generalizes an approach pioneered by\nMichael Dummett.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 01:48:56 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 22:31:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Aziz", "Haris", ""], ["Lee", "Barton E.", ""]]}, {"id": "2002.09808", "submitter": "Ilai Bistritz", "authors": "Ilai Bistritz, Tavor Z. Baharav, Amir Leshem, Nicholas Bambos", "title": "My Fair Bandit: Distributed Learning of Max-Min Fairness with\n  Multi-player Bandits", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider N cooperative but non-communicating players where each plays one out\nof M arms for T turns. Players have different utilities for each arm,\nrepresentable as an NxM matrix. These utilities are unknown to the players. In\neach turn players select an arm and receive a noisy observation of their\nutility for it. However, if any other players selected the same arm that turn,\nall colliding players will all receive zero utility due to the conflict. No\nother communication or coordination between the players is possible. Our goal\nis to design a distributed algorithm that learns the matching between players\nand arms that achieves max-min fairness while minimizing the regret. We present\nan algorithm and prove that it is regret optimal up to a $\\log\\log T$ factor.\nThis is the first max-min fairness multi-player bandit algorithm with (near)\norder optimal regret.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 02:05:55 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 05:26:02 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 21:44:53 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 05:15:47 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Bistritz", "Ilai", ""], ["Baharav", "Tavor Z.", ""], ["Leshem", "Amir", ""], ["Bambos", "Nicholas", ""]]}, {"id": "2002.09827", "submitter": "Ron van der Meyden", "authors": "Ron van der Meyden", "title": "A Formal Treatment of Contract Signature", "comments": "28 pages. This is a significantly revised and retitled version of an\n  earlier paper \"Signature in Counterparts, a Formal Treatment\". Revisions\n  include changes to the semantics, addition of a treatment of offer and\n  acceptance, added proof material and discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper develops a logical understanding of processes for signature of\nlegal contracts, motivated by applications to legal recognition of smart\ncontracts on blockchain platforms. A number of axioms and rules of inference\nare developed that can be used to justify a \"meeting of the minds\" precondition\nfor contract formation from the fact that certain content has been signed. In\naddition to an \"offer and acceptance\" process, the paper considers \"signature\nin counterparts\", a legal process that permits a contract between two or more\nparties to be brought into force by having the parties independently (possibly,\nremotely) sign different copies of the contract, rather than placing their\nsignatures on a common copy at a physical meeting. It is argued that a\nsatisfactory account of signature in counterparts benefits from a logic with\nsyntactic self-reference. The axioms used are supported by a formal semantics,\nand a number of further properties of the logic are investigated. In\nparticular, it is shown that the logic implies that when a contract has been\nsigned, the parties do not just agree, but are in mutual agreement (a\ncommon-knowledge-like notion) about the terms of the contract.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 04:39:56 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 08:48:28 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["van der Meyden", "Ron", ""]]}, {"id": "2002.09853", "submitter": "Azhar Hussain", "authors": "Azhar Hussain, Tong Wang and Cao Jiahua", "title": "Optimizing Traffic Lights with Multi-agent Deep Reinforcement Learning\n  and V2X communication", "comments": "7 Figure, Table 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a system to optimize duration of traffic signals using\nmulti-agent deep reinforcement learning and Vehicle-to-Everything (V2X)\ncommunication. This system aims at analyzing independent and shared rewards for\nmulti-agents to control duration of traffic lights. A learning agent traffic\nlight gets information along its lanes within a circular V2X coverage. The\nduration cycles of traffic light are modeled as Markov decision Processes. We\ninvestigate four variations of reward functions. The first two are\nunshared-rewards: based on waiting number, and waiting time of vehicles between\ntwo cycles of traffic light. The third and fourth functions are: shared-rewards\nbased on waiting cars, and waiting time for all agents. Each agent has a memory\nfor optimization through target network and prioritized experience replay. We\nevaluate multi-agents through the Simulation of Urban MObility (SUMO)\nsimulator. The results prove effectiveness of the proposed system to optimize\ntraffic signals and reduce average waiting cars to 41.5 % as compared to the\ntraditional periodic traffic control system.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 07:43:12 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Hussain", "Azhar", ""], ["Wang", "Tong", ""], ["Jiahua", "Cao", ""]]}, {"id": "2002.09964", "submitter": "Hossein Taheri", "authors": "Hossein Taheri, Aryan Mokhtari, Hamed Hassani, Ramtin Pedarsani", "title": "Quantized Decentralized Stochastic Learning over Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized stochastic learning problem where data points are\ndistributed among computing nodes communicating over a directed graph. As the\nmodel size gets large, decentralized learning faces a major bottleneck that is\nthe heavy communication load due to each node transmitting large messages\n(model updates) to its neighbors. To tackle this bottleneck, we propose the\nquantized decentralized stochastic learning algorithm over directed graphs that\nis based on the push-sum algorithm in decentralized consensus optimization.\nMore importantly, we prove that our algorithm achieves the same convergence\nrates of the decentralized stochastic learning algorithm with\nexact-communication for both convex and non-convex losses. Numerical\nevaluations corroborate our main theoretical results and illustrate significant\nspeed-up compared to the exact-communication methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 18:25:39 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 09:12:25 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 07:41:26 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 17:06:54 GMT"}, {"version": "v5", "created": "Mon, 28 Dec 2020 10:02:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Taheri", "Hossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "2002.10113", "submitter": "Alex Tong Lin", "authors": "Alex Tong Lin, Samy Wu Fung, Wuchen Li, Levon Nurbekyan, Stanley J.\n  Osher", "title": "Alternating the Population and Control Neural Networks to Solve\n  High-Dimensional Stochastic Mean-Field Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present APAC-Net, an alternating population and agent control neural\nnetwork for solving stochastic mean field games (MFGs). Our algorithm is geared\ntoward high-dimensional instances of MFGs that are beyond reach with existing\nsolution methods. We achieve this in two steps. First, we take advantage of the\nunderlying variational primal-dual structure that MFGs exhibit and phrase it as\na convex-concave saddle point problem. Second, we parameterize the value and\ndensity functions by two neural networks, respectively. By phrasing the problem\nin this manner, solving the MFG can be interpreted as a special case of\ntraining a generative adversarial network (GAN). We show the potential of our\nmethod on up to 100-dimensional MFG problems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 08:24:52 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 17:23:39 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 23:36:31 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lin", "Alex Tong", ""], ["Fung", "Samy Wu", ""], ["Li", "Wuchen", ""], ["Nurbekyan", "Levon", ""], ["Osher", "Stanley J.", ""]]}, {"id": "2002.10172", "submitter": "Iain Johnston", "authors": "Iain G. Johnston", "title": "Optimal strategies in the Fighting Fantasy gaming system: influencing\n  stochastic dynamics by gambling with limited resource", "comments": "Keyword: stochastic game; Markov decision problem; stochastic\n  simulation; dynamic programming; resource allocation; stochastic optimal\n  control; Bellman equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fighting Fantasy is a popular recreational fantasy gaming system worldwide.\nCombat in this system progresses through a stochastic game involving a series\nof rounds, each of which may be won or lost. Each round, a limited resource\n(`luck') may be spent on a gamble to amplify the benefit from a win or mitigate\nthe deficit from a loss. However, the success of this gamble depends on the\namount of remaining resource, and if the gamble is unsuccessful, benefits are\nreduced and deficits increased. Players thus dynamically choose to expend\nresource to attempt to influence the stochastic dynamics of the game, with\ndiminishing probability of positive return. The identification of the optimal\nstrategy for victory is a Markov decision problem that has not yet been solved.\nHere, we combine stochastic analysis and simulation with dynamic programming to\ncharacterise the dynamical behaviour of the system in the absence and presence\nof gambling policy. We derive a simple expression for the victory probability\nwithout luck-based strategy. We use a backward induction approach to solve the\nBellman equation for the system and identify the optimal strategy for any given\nstate during the game. The optimal control strategies can dramatically enhance\nsuccess probabilities, but take detailed forms; we use stochastic simulation to\napproximate these optimal strategies with simple heuristics that can be\npractically employed. Our findings provide a roadmap to improving success in\nthe games that millions of people play worldwide, and inform a class of\nresource allocation problems with diminishing returns in stochastic games.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:31:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Johnston", "Iain G.", ""]]}, {"id": "2002.10185", "submitter": "Lasse Peters", "authors": "Lasse Peters, Zachary N. Sunberg", "title": "iLQGames.jl: Rapidly Designing and Solving Differential Games in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems that involve multiple decision making agents, optimal\nchoices for each agent depend on the choices of others. Differential game\ntheory provides a principled formalism for expressing these coupled\ninteractions and recent work offers efficient approximations to solve these\nproblems to non-cooperative equilibria. iLQGames.jl is a framework for\ndesigning and solving differential games, built around the iterative\nlinear-quadratic method. It is written in the Julia programming language to\nallow flexible prototyping and integration with other research software, while\nleveraging the high-performance nature of the language to allow real-time\nexecution. The open-source software package can be found at\nhttps://github.com/lassepe/iLQGames.jl.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:01:05 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 13:59:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Peters", "Lasse", ""], ["Sunberg", "Zachary N.", ""]]}, {"id": "2002.10524", "submitter": "Carlos Martin", "authors": "Carlos Martin, Tuomas Sandholm", "title": "Efficient exploration of zero-sum stochastic games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the increasingly important and common game-solving setting\nwhere we do not have an explicit description of the game but only oracle access\nto it through gameplay, such as in financial or military simulations and\ncomputer games. During a limited-duration learning phase, the algorithm can\ncontrol the actions of both players in order to try to learn the game and how\nto play it well. After that, the algorithm has to produce a strategy that has\nlow exploitability. Our motivation is to quickly learn strategies that have low\nexploitability in situations where evaluating the payoffs of a queried strategy\nprofile is costly. For the stochastic game setting, we propose using the\ndistribution of state-action value functions induced by a belief distribution\nover possible environments. We compare the performance of various exploration\nstrategies for this task, including generalizations of Thompson sampling and\nBayes-UCB to this new setting. These two consistently outperform other\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:30:38 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Martin", "Carlos", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "2002.10525", "submitter": "Wonseok Jeon", "authors": "Wonseok Jeon, Paul Barde, Derek Nowrouzezahrai, Joelle Pineau", "title": "Scalable Multi-Agent Inverse Reinforcement Learning via\n  Actor-Attention-Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent adversarial inverse reinforcement learning (MA-AIRL) is a recent\napproach that applies single-agent AIRL to multi-agent problems where we seek\nto recover both policies for our agents and reward functions that promote\nexpert-like behavior. While MA-AIRL has promising results on cooperative and\ncompetitive tasks, it is sample-inefficient and has only been validated\nempirically for small numbers of agents -- its ability to scale to many agents\nremains an open question. We propose a multi-agent inverse RL algorithm that is\nmore sample-efficient and scalable than previous works. Specifically, we employ\nmulti-agent actor-attention-critic (MAAC) -- an off-policy multi-agent RL\n(MARL) method -- for the RL inner loop of the inverse RL procedure. In doing\nso, we are able to increase sample efficiency compared to state-of-the-art\nbaselines, across both small- and large-scale tasks. Moreover, the RL agents\ntrained on the rewards recovered by our method better match the experts than\nthose trained on the rewards derived from the baselines. Finally, our method\nrequires far fewer agent-environment interactions, particularly as the number\nof agents increases.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:30:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Jeon", "Wonseok", ""], ["Barde", "Paul", ""], ["Nowrouzezahrai", "Derek", ""], ["Pineau", "Joelle", ""]]}, {"id": "2002.10627", "submitter": "Sixie Yu", "authors": "David Kempe, Sixie Yu, Yevgeniy Vorobeychik", "title": "Inducing Equilibria in Networked Public Goods Games through Network\n  Structure Modification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Networked public goods games model scenarios in which self-interested agents\ndecide whether or how much to invest in an action that benefits not only\nthemselves, but also their network neighbors. Examples include vaccination,\nsecurity investment, and crime reporting. While every agent's utility is\nincreasing in their neighbors' joint investment, the specific form can vary\nwidely depending on the scenario. A principal, such as a policymaker, may wish\nto induce large investment from the agents. Besides direct incentives, an\nimportant lever here is the network structure itself: by adding and removing\nedges, for example, through community meetings, the principal can change the\nnature of the utility functions, resulting in different, and perhaps socially\npreferable, equilibrium outcomes. We initiate an algorithmic study of targeted\nnetwork modifications with the goal of inducing equilibria of a particular\nform. We study this question for a variety of equilibrium forms (induce all\nagents to invest, at least a given set $S$, exactly a given set $S$, at least\n$k$ agents), and for a variety of utility functions. While we show that the\nproblem is NP-complete for a number of these scenarios, we exhibit a broad\narray of scenarios in which the problem can be solved in polynomial time by\nnon-trivial reductions to (minimum-cost) matching problems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 02:30:52 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Kempe", "David", ""], ["Yu", "Sixie", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2002.10641", "submitter": "Chen Dingding", "authors": "Ziyu Chen, Wenxin Zhang, Yanchen Deng, Dingding Chen, Qing Li", "title": "RMB-DPOP: Refining MB-DPOP by Reducing Redundant Inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MB-DPOP is an important complete algorithm for solving Distributed Constraint\nOptimization Problems (DCOPs) by exploiting a cycle-cut idea to implement\nmemory-bounded inference. However, each cluster root in the algorithm is\nresponsible for enumerating all the instantiations of its cycle-cut nodes,\nwhich would cause redundant inferences when its branches do not have the same\ncycle-cut nodes. Additionally, a large number of cycle-cut nodes and the\niterative nature of MB-DPOP further exacerbate the pathology. As a result,\nMB-DPOP could suffer from huge coordination overheads and cannot scale up well.\nTherefore, we present RMB-DPOP which incorporates several novel mechanisms to\nreduce redundant inferences and improve the scalability of MB-DPOP. First,\nusing the independence among the cycle-cut nodes in different branches, we\ndistribute the enumeration of instantiations into different branches whereby\nthe number of nonconcurrent instantiations reduces significantly and each\nbranch can perform memory bounded inference asynchronously. Then, taking the\ntopology into the consideration, we propose an iterative allocation mechanism\nto choose the cycle-cut nodes that cover a maximum of active nodes in a cluster\nand break ties according to their relative positions in a pseudo-tree. Finally,\na caching mechanism is proposed to further reduce unnecessary inferences when\nthe historical results are compatible with the current instantiations. We\ntheoretically show that with the same number of cycle-cut nodes RMB-DPOP\nrequires as many messages as MB-DPOP in the worst case and the experimental\nresults show our superiorities over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:19:29 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chen", "Ziyu", ""], ["Zhang", "Wenxin", ""], ["Deng", "Yanchen", ""], ["Chen", "Dingding", ""], ["Li", "Qing", ""]]}, {"id": "2002.10860", "submitter": "Tomoichi Takahashi", "authors": "Tomoichi Takahashi", "title": "Toward dynamical crowd control to prevent hazardous situations", "comments": "3pages, 7 figures, submitted to PED2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for large crowds to gather to attend games, exhibitions,\npolitical rallies, and other events. Thus, careful designs and operational\nplans are made to ensure the safe, secure, and efficient movement of people in\nthese crowded environments. However, the congestion created by large crowds has\nresulted in hazardous incidents across the world. Developments in information\ntechnology can provide new means to disseminate public information, thus\nchanging human behavior in situations of danger and duress. In this paper, we\npropose a crowd control and evacuation guidance management system using digital\npromotional signage to demonstrate the effects of crowd control via\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 09:52:05 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Takahashi", "Tomoichi", ""]]}, {"id": "2002.11174", "submitter": "Corban Rivera", "authors": "Corban G. Rivera, Olivia Lyons, Arielle Summitt, Ayman Fatima, Ji Pak,\n  William Shao, Robert Chalmers, Aryeh Englander, Edward W. Staley, I-Jeng\n  Wang, Ashley J. Llorens", "title": "TanksWorld: A Multi-Agent Environment for AI Safety Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to create artificial intelligence (AI) capable of performing\ncomplex tasks is rapidly outpacing our ability to ensure the safe and assured\noperation of AI-enabled systems. Fortunately, a landscape of AI safety research\nis emerging in response to this asymmetry and yet there is a long way to go. In\nparticular, recent simulation environments created to illustrate AI safety\nrisks are relatively simple or narrowly-focused on a particular issue. Hence,\nwe see a critical need for AI safety research environments that abstract\nessential aspects of complex real-world applications. In this work, we\nintroduce the AI safety TanksWorld as an environment for AI safety research\nwith three essential aspects: competing performance objectives, human-machine\nteaming, and multi-agent competition. The AI safety TanksWorld aims to\naccelerate the advancement of safe multi-agent decision-making algorithms by\nproviding a software framework to support competitions with both system\nperformance and safety objectives. As a work in progress, this paper introduces\nour research objectives and learning environment with reference code and\nbaseline performance metrics to follow in a future work.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 21:00:52 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Rivera", "Corban G.", ""], ["Lyons", "Olivia", ""], ["Summitt", "Arielle", ""], ["Fatima", "Ayman", ""], ["Pak", "Ji", ""], ["Shao", "William", ""], ["Chalmers", "Robert", ""], ["Englander", "Aryeh", ""], ["Staley", "Edward W.", ""], ["Wang", "I-Jeng", ""], ["Llorens", "Ashley J.", ""]]}, {"id": "2002.11507", "submitter": "Kashif Zia Dr.", "authors": "Kashif Zia", "title": "A Simulation Model Demonstrating the Impact of Social Aspects on Social\n  Internet of Things", "comments": "22 pages, 11 figures, 1 table. Presented in The 21st International\n  Conference on Information Integration and Web-based Applications & Services\n  (iiWAS2019), 2-4 December 2019, Munich Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to seamless connectivity and smartness, the objects in the\nInternet of Things (IoT) are expected to have the social capabilities -- these\nobjects are termed as ``social objects''. In this paper, an intuitive paradigm\nof social interactions between these objects are argued and modeled. The impact\nof social behavior on the interaction pattern of social objects is studied\ntaking Peer-to-Peer (P2P) resource sharing as an example application. The model\nproposed in this paper studies the implications of competitive vs. cooperative\nsocial paradigm, while peers attempt to attain the shared resources / services.\nThe simulation results divulge that the social capabilities of the peers impart\na significant increase in the quality of interactions between social objects.\nThrough an agent-based simulation study, it is proved that cooperative strategy\nis more efficient than competitive strategy. Moreover, cooperation with an\nunderpinning on real-life networking structure and mobility does not negatively\nimpact the efficiency of the system at all; rather it helps.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 07:18:39 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Zia", "Kashif", ""]]}, {"id": "2002.11534", "submitter": "Jinming Xu", "authors": "Jinming Xu, Ye Tian, Ying Sun, Gesualdo Scutari", "title": "Distributed Algorithms for Composite Optimization: Unified Framework and\n  Convergence Analysis", "comments": "arXiv admin note: text overlap with arXiv:1910.09817", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed composite optimization over networks: agents minimize a\nsum of smooth (strongly) convex functions, the agents' sum-utility, plus a\nnonsmooth (extended-valued) convex one. We propose a general unified\nalgorithmic framework for such a class of problems and provide a unified\nconvergence analysis leveraging the theory of operator splitting.\nDistinguishing features of our scheme are: (i) When the agents' functions are\nstrongly convex, the algorithm converges at a linear rate, whose dependence on\nthe agents' functions and network topology is decoupled, matching the typical\nrates of centralized optimization; the rate expression improves on existing\nresults; (ii) When the objective function is convex (but not strongly convex),\nsimilar separation as in (i) is established for the coefficient of the proved\nsublinear rate; (iii) The algorithm can adjust the ratio between the number of\ncommunications and computations to achieve a rate (in terms of computations)\nindependent on the network connectivity; and (iv) A by-product of our analysis\nis a tuning recommendation for several existing (non accelerated) distributed\nalgorithms yielding the fastest provably (worst-case) convergence rate. This is\nthe first time that a general distributed algorithmic framework applicable to\ncomposite optimization enjoys all such properties.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:34:40 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 00:22:39 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Xu", "Jinming", ""], ["Tian", "Ye", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "2002.11861", "submitter": "Ziyi Zhao", "authors": "Zhao Jin, Ziyi Zhao, Chen Luo, Franco Basti, Adrian Solomon, M. Cenk\n  Gursoy, Carlos Caicedo, Qinru Qiu", "title": "Simulation of Real-time Routing for UAS traffic Management with\n  Communication and Airspace Safety Considerations", "comments": "The 38th AIAA/IEEE Digital Avionics Systems Conference (DASC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small Unmanned Aircraft Systems (sUAS) will be an important component of the\nsmart city and intelligent transportation environments of the near future. The\ndemand for sUAS related applications, such as commercial delivery and land\nsurveying, is expected to grow rapidly in next few years. In general, sUAS\ntraffic routing and management functions are needed to coordinate the launching\nof sUAS from different launch sites and determine their trajectories to avoid\nconflict while considering several other constraints such as expected arrival\ntime, minimum flight energy, and availability of communication resources.\nHowever, as the airborne sUAS density grows in a certain area, it is difficult\nto foresee the potential airspace and communications resource conflicts and\nmake immediate decisions to avoid them. To address this challenge, we present a\ntemporal and spatial routing algorithm and simulation platform for sUAS\ntrajectory management in a high density urban area that plans sUAS movements in\na spatial and temporal maze taking into account obstacles that are either\nstatic or dynamic in time. The routing allows the sUAS to avoid static no-fly\nareas (i.e. static obstacles) or other in-flight sUAS and areas that have\ncongested communication resources (i.e. dynamic obstacles). The algorithm is\nevaluated using an agent-based simulation platform. The simulation results show\nthat the proposed algorithm outperforms other route management algorithms in\nmany areas, especially in processing speed and memory efficiency. Detailed\ncomparisons are provided for the sUAS flight time, the overall throughput,\nconflict rate and communication resource utilization. The results demonstrate\nthat our proposed algorithm can be used to address the airspace and\ncommunication resource utilization needs for a next generation smart city and\nsmart transportation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:54:11 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Jin", "Zhao", ""], ["Zhao", "Ziyi", ""], ["Luo", "Chen", ""], ["Basti", "Franco", ""], ["Solomon", "Adrian", ""], ["Gursoy", "M. Cenk", ""], ["Caicedo", "Carlos", ""], ["Qiu", "Qinru", ""]]}, {"id": "2002.11874", "submitter": "Junjia Liu", "authors": "Junjia Liu, Huimin Zhang, Zhuang Fu and Yao Wang", "title": "Learning Scalable Multi-Agent Coordination by Spatial Differentiation\n  for Traffic Signal Control", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intelligent control of the traffic signal is critical to the optimization\nof transportation systems. To achieve global optimal traffic efficiency in\nlarge-scale road networks, recent works have focused on coordination among\nintersections, which have shown promising results. However, existing studies\npaid more attention to observations sharing among intersections (both explicit\nand implicit) and did not care about the consequences after decisions. In this\npaper, we design a multiagent coordination framework based on Deep\nReinforcement Learning methods for traffic signal control, defined as\n{\\gamma}-Reward that includes both original {\\gamma}-Reward and\n{\\gamma}-Attention-Reward. Specifically, we propose the Spatial Differentiation\nmethod for coordination which uses the temporal-spatial information in the\nreplay buffer to amend the reward of each action. A concise theoretical\nanalysis that proves the proposed model can converge to Nash equilibrium is\ngiven. By extending the idea of Markov Chain to the dimension of space-time,\nthis truly decentralized coordination mechanism replaces the graph attention\nmethod and realizes the decoupling of the road network, which is more scalable\nand more in line with practice. The simulation results show that the proposed\nmodel remains a state-of-the-art performance even not use a centralized\nsetting. Code is available in https://github.com/Skylark0924/Gamma Reward.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:16:00 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 11:58:50 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:25:54 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Liu", "Junjia", ""], ["Zhang", "Huimin", ""], ["Fu", "Zhuang", ""], ["Wang", "Yao", ""]]}, {"id": "2002.11882", "submitter": "Thanh Thi Nguyen", "authors": "Ngoc Duy Nguyen, Thanh Thi Nguyen, Doug Creighton, Saeid Nahavandi", "title": "A Visual Communication Map for Multi-Agent Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.13433.62563", "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning has been applied successfully to solve various\nreal-world problems and the number of its applications in the multi-agent\nsettings has been increasing. Multi-agent learning distinctly poses significant\nchallenges in the effort to allocate a concealed communication medium. Agents\nreceive thorough knowledge from the medium to determine subsequent actions in a\ndistributed nature. Apparently, the goal is to leverage the cooperation of\nmultiple agents to achieve a designated objective efficiently. Recent studies\ntypically combine a specialized neural network with reinforcement learning to\nenable communication between agents. This approach, however, limits the number\nof agents or necessitates the homogeneity of the system. In this paper, we have\nproposed a more scalable approach that not only deals with a great number of\nagents but also enables collaboration between dissimilar functional agents and\ncompatibly combined with any deep reinforcement learning methods. Specifically,\nwe create a global communication map to represent the status of each agent in\nthe system visually. The visual map and the environmental state are fed to a\nshared-parameter network to train multiple agents concurrently. Finally, we\nselect the Asynchronous Advantage Actor-Critic (A3C) algorithm to demonstrate\nour proposed scheme, namely Visual communication map for Multi-agent A3C\n(VMA3C). Simulation results show that the use of visual communication map\nimproves the performance of A3C regarding learning speed, reward achievement,\nand robustness in multi-agent problems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:38:21 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 12:12:47 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Nguyen", "Ngoc Duy", ""], ["Nguyen", "Thanh Thi", ""], ["Creighton", "Doug", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2002.12001", "submitter": "Saaduddin Mahmud", "authors": "Saaduddin Mahmud, Md. Mosaddek Khan, Moumita Choudhury, Long\n  Tran-Thanh and Nicholas R. Jennings", "title": "Learning Optimal Temperature Region for Solving Mixed Integer Functional\n  DCOPs", "comments": "Proceedings of the Twenty-Ninth International Joint Conference on\n  Artificial Intelligence Main track. Pages 268-275", "journal-ref": null, "doi": "10.24963/ijcai.2020/38", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Constraint Optimization Problems (DCOPs) are an important\nframework for modeling coordinated decision-making problems in multi-agent\nsystems with a set of discrete variables. Later works have extended DCOPs to\nmodel problems with a set of continuous variables, named Functional DCOPs\n(F-DCOPs). In this paper, we combine both of these frameworks into the Mixed\nInteger Functional DCOP (MIF-DCOP) framework that can deal with problems\nregardless of their variables' type. We then propose a novel algorithm $-$\nDistributed Parallel Simulated Annealing (DPSA), where agents cooperatively\nlearn the optimal parameter configuration for the algorithm while also solving\nthe given problem using the learned knowledge. Finally, we empirically evaluate\nour approach in DCOP, F-DCOP, and MIF-DCOP settings and show that DPSA produces\nsolutions of significantly better quality than the state-of-the-art non-exact\nalgorithms in their corresponding settings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:46:40 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 06:17:37 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mahmud", "Saaduddin", ""], ["Khan", "Md. Mosaddek", ""], ["Choudhury", "Moumita", ""], ["Tran-Thanh", "Long", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "2002.12217", "submitter": "Pegah Rokhforoz", "authors": "Pegah Rokhforoz and Blazhe Gjorgiev and Giovanni Sansavini and Olga\n  Fink", "title": "Multi-agent maintenance scheduling based on the coordination between\n  central operator and decentralized producers in an electricity market", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condition-based and predictive maintenance enable early detection of critical\nsystem conditions and thereby enable decision makers to forestall faults and\nmitigate them. However, decision makers also need to take the operational and\nproduction needs into consideration for optimal decision-making when scheduling\nmaintenance activities. Particularly in network systems, such as power grids,\ndecisions on the maintenance of single assets can affect the entire network and\nare, therefore, more complex. This paper proposes a two-level multi-agent\ndecision support systems for the generation maintenance decision (GMS) of power\ngrids in an electricity markets. The aim of the GMS is to minimize the\ngeneration cost while maximizing the system reliability. The proposed framework\nintegrates a central coordination system, i.e. the transmission system operator\n(TSO), and distributed agents representing power generation units that act to\nmaximize their profit and decide about the optimal maintenance time slots while\nensuring the fulfilment of the energy demand. The objective function of agents\n(power generation companies) is based on the reward and the penalty that they\nobtain from the interplay between power production and loss of production due\nto failure, respectively. The optimal strategy of agents is then derived using\na distributed algorithm, where agents choose their optimal maintenance decision\nand send their decisions to the central coordinating system. The TSO decides\nwhether to accept the agents' decisions by considering the market reliability\naspects and power supply constraints. To solve this coordination problem, we\npropose a negotiation algorithm using an incentive signal to coordinate the\nagents' and central system's decisions such that all the agents' decisions can\nbe accepted by the central system. We demonstrate the efficiency of our\nproposed algorithm using a IEEE 39 bus system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:04:36 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 21:00:46 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 12:16:22 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Rokhforoz", "Pegah", ""], ["Gjorgiev", "Blazhe", ""], ["Sansavini", "Giovanni", ""], ["Fink", "Olga", ""]]}, {"id": "2002.12313", "submitter": "Robin Brown", "authors": "Robin Brown, Federico Rossi, Kiril Solovey, Michael T. Wolf, and Marco\n  Pavone", "title": "On Local Computation for Optimization in Multi-Agent Systems", "comments": "Add additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of prototypical optimization problems in multi-agent systems (e.g.,\ntask allocation and network load-sharing) exhibit a highly local structure:\nthat is, each agent's decision variables are only directly coupled to few other\nagent's variables through the objective function or the constraints.\nNevertheless, existing algorithms for distributed optimization generally do not\nexploit the locality structure of the problem, requiring all agents to compute\nor exchange the full set of decision variables. In this paper, we develop a\nrigorous notion of \"locality\" that quantifies the degree to which agents can\ncompute their portion of the global solution based solely on information in\ntheir local neighborhood. This notion provides a theoretical basis for a rather\nsimple algorithm in which agents individually solve a truncated sub-problem of\nthe global problem, where the size of the sub-problem used depends on the\nlocality of the problem, and the desired accuracy. Numerical results show that\nthe proposed theoretical bounds are remarkably tight for well-conditioned\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:31:00 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 16:52:44 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Brown", "Robin", ""], ["Rossi", "Federico", ""], ["Solovey", "Kiril", ""], ["Wolf", "Michael T.", ""], ["Pavone", "Marco", ""]]}, {"id": "2002.12427", "submitter": "Amit Sarker", "authors": "Amit Sarker, Abdullahil Baki Arif, Moumita Choudhury, Md. Mosaddek\n  Khan", "title": "C-CoCoA: A Continuous Cooperative Constraint Approximation Algorithm to\n  Solve Functional DCOPs", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Constraint Optimization Problems (DCOPs) have been widely used to\ncoordinate interactions (i.e. constraints) in cooperative multi-agent systems.\nThe traditional DCOP model assumes that variables owned by the agents can take\nonly discrete values and constraints' cost functions are defined for every\npossible value assignment of a set of variables. While this formulation is\noften reasonable, there are many applications where the variables are\ncontinuous decision variables and constraints are in functional form. To\novercome this limitation, Functional DCOP (F-DCOP) model is proposed that is\nable to model problems with continuous variables. The existing F-DCOPs\nalgorithms experience huge computation and communication overhead. This paper\napplies continuous non-linear optimization methods on Cooperative Constraint\nApproximation (CoCoA) algorithm. We empirically show that our algorithm is able\nto provide high-quality solutions at the expense of smaller communication cost\nand execution time compared to the existing F-DCOP algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:44:25 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Sarker", "Amit", ""], ["Arif", "Abdullahil Baki", ""], ["Choudhury", "Moumita", ""], ["Khan", "Md. Mosaddek", ""]]}, {"id": "2002.12609", "submitter": "Hyeongseok Jeon", "authors": "Hyeongseok Jeon, Junwon Choi, Dongsuk Kum", "title": "SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random\n  Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural\n  Network", "comments": "8 pages, 9 figures, and 2 tables, the paper is submitted to the\n  conference IROS2020 and is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future trajectory of surrounding vehicles in a randomly\nvarying traffic level is one of the most challenging problems in developing an\nautonomous vehicle. Since there is no pre-defined number of interacting\nvehicles participate in, the prediction network has to be scalable with respect\nto the vehicle number in order to guarantee the consistency in terms of both\naccuracy and computational load. In this paper, the first fully scalable\ntrajectory prediction network, SCALE-Net, is proposed that can ensure both\nhigher prediction performance and consistent computational load regardless of\nthe number of surrounding vehicles. The SCALE-Net employs the Edge-enhance\nGraph Convolutional Neural Network (EGCN) for the inter-vehicular interaction\nembedding network. Since the proposed EGCN is inherently scalable with respect\nto the graph node (an agent in this study), the model can be operated\nindependently from the total number of vehicles considered. We evaluated the\nscalability of the SCALE-Net on the publically available NGSIM datasets by\ncomparing variations on computation time and prediction accuracy per single\ndriving scene with respect to the varying vehicle number. The experimental test\nshows that both computation time and prediction performance of the SCALE-Net\nconsistently outperform those of previous models regardless of the level of\ntraffic complexities.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:25:01 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jeon", "Hyeongseok", ""], ["Choi", "Junwon", ""], ["Kum", "Dongsuk", ""]]}, {"id": "2002.12654", "submitter": "Misha Abraham", "authors": "Misha Abraham and Himajit Aithal and Krishnan Mohan", "title": "Real time Smart Contracts for IoT using Blockchain and Collaborative\n  Intelligence based Dynamic Pricing for the next generation Smart Toll\n  Application", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The confluence of Internet of Things(IoT) , Blockchain(BC) and Artificial\nIntelligence(AI) acts as a key accelerator for enabling Machine Economy. To be\nready for future businesses these technologies needs to be adapted by extending\nthe IoT capabilities to Economy of Things (EoT) capabilities. In this paper we\nfocus on one such implementation experience for Smart Toll Transaction\napplication in the domain of mobility. Our paper showcases a possible solution\nby leveraging negotiations, decision making, distributed learning capabilities\nat the devices level using AI-enabled Multi-Agent Systems and the real-time\nsmart contracts between the Cars and Tolls using Blockchain. This solution also\nshowcases the monetization of real time data coming from various IoT devices\nwhich are part of vehicles and infrastructure. While blockchain secures the\nprivacy of the participants it also acts as an economic transactional layer and\ngovernance layer between the devices in the networ\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:16:26 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Abraham", "Misha", ""], ["Aithal", "Himajit", ""], ["Mohan", "Krishnan", ""]]}, {"id": "2002.12926", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "Cities as they could be: Artificial Life and Urban Systems", "comments": "8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metaphor of cities as organisms has a long history in urban planning, and\na few urban modeling approaches have explicitly been linked to Artificial Life.\nWe propose in that paper to explore the extent of Artificial Life and\nArtificial Intelligence application to urban issues, by constructing and\nexploring a citation network of around 225,000 papers. It shows that most of\nthe literature is indeed application of methodologies and a rather strong\nmodularity of approaches. We finally develop ALife concepts which have a strong\npotential for the development of new urban theories.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 18:54:37 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Raimbault", "Juste", ""]]}]