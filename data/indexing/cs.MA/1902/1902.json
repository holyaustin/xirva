[{"id": "1902.00294", "submitter": "Ariel Barel Dr.", "authors": "Ariel Barel, Thomas Dag\\`es, Rotem Manor, Alfred M. Bruckstein", "title": "Probabilistic Gathering Of Agents With Simple Sensors", "comments": "20 pages plus supplementary material", "journal-ref": "ANTS2018", "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gathering is a fundamental task for multi-agent systems and the problem has\nbeen studied under various assumptions on the sensing capabilities of mobile\nagents. This paper addresses the problem for a group of agents that are\nidentical and indistinguishable, oblivious, and lack the capacity of direct\ncommunication. At the beginning of unit-time intervals, the agents select\nrandom headings in the plane and then detect the presence of other agents\nbehind them. Then they move forward only if no agents are detected in their\nsensing \"back half-plane\". Two types of motion are considered: when no peers\nare detected behind them, either the agents perform unit jumps forward, or they\nstart to move with unit speed while continuously sensing their back half-plane,\nand stop whenever another agent appears there. For the first type of motion\nextensive empirical evidence suggests that with high probability clustering\noccurs in finite expected time to a small region with diameter of about the\nsize of the unit jump, while for continuous sensing and motion we can prove\ngathering in finite expected time if a \"blind-zone\" is assumed in their sensing\nhalf-plane. Relationships between the number of agents or the size of the\nblind-zone and convergence time are empirically studied and compared to a\ntheoretical upper-bound dependent on these factors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 12:07:34 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 09:36:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Barel", "Ariel", ""], ["Dag\u00e8s", "Thomas", ""], ["Manor", "Rotem", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1902.00380", "submitter": "Mingliang Xu", "authors": "Chaochao Li, Pei Lv, Dinesh Manocha, Hua Wang, Yafei Li, Bing Zhou,\n  and Mingliang Xu", "title": "ACSEE: Antagonistic Crowd Simulation Model with Emotional Contagion and\n  Evolutionary Game Theory", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing (2019)", "doi": "10.1109/TAFFC.2019.2954394", "report-no": null, "categories": "cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antagonistic crowd behaviors are often observed in cases of serious conflict.\nAntagonistic emotions, which is the typical psychological state of agents in\ndifferent roles (i.e. cops, activists, and civilians) in crowd violent scenes,\nand the way they spread through contagion in a crowd are important causes of\ncrowd antagonistic behaviors. Moreover, games, which refers to the interaction\nbetween opposing groups adopting different strategies to obtain higher benefits\nand less casualties, determine the level of crowd violence. We present an\nantagonistic crowd simulation model, ACSEE, which is integrated with\nantagonistic emotional contagion and evolutionary game theories. Our approach\nmodels the antagonistic emotions between agents in different roles using two\ncomponents: mental emotion and external emotion. We combine enhanced\nsusceptible-infectious-susceptible (SIS) and game approaches to evaluate the\nrole of antagonistic emotional contagion in crowd violence. Our evolutionary\ngame theoretic approach incorporates antagonistic emotional contagion through\ndeterrent force, which is modelled by a mixture of emotional forces and\nphysical forces defeating the opponents. Antagonistic emotional contagion and\nevolutionary game theories influence each other to determine antagonistic crowd\nbehaviors. We evaluate our approach on real-world scenarios consisting of\ndifferent kinds of agents. We also compare the simulated crowd behaviors with\nreal-world crowd videos and use our approach to predict the trends of crowd\nmovements in violence incidents. We investigate the impact of various factors\n(number of agents, emotion, strategy, etc.) on the outcome of crowd violence.\nWe present results from user studies suggesting that our model can simulate\nantagonistic crowd behaviors similar to those seen in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:21:43 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 01:25:06 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 12:17:38 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Li", "Chaochao", ""], ["Lv", "Pei", ""], ["Manocha", "Dinesh", ""], ["Wang", "Hua", ""], ["Li", "Yafei", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "1902.00385", "submitter": "Ariel Barel Dr.", "authors": "Ariel Barel, Rotem Manor, Alfred M. Bruckstein", "title": "On Steering Swarms", "comments": null, "journal-ref": "ANTS 2018", "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper is a novel method allowing an external\nobserver/controller to steer and guide swarms of identical and\nindistinguishable agents, in spite of the agents' lack of information on\nabsolute location and orientation. Importantly, this is done via simple global\nbroadcast signals, based on the observed average swarm location, with no need\nto send control signals to any specific agent in the swarm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:58:09 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Barel", "Ariel", ""], ["Manor", "Rotem", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1902.01024", "submitter": "Li Li", "authors": "Huile Xu, Yi Zhang, Li Li, Weixia Li", "title": "Cooperative Driving at Unsignalized Intersections Using Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new cooperative driving strategy for connected\nand automated vehicles (CAVs) at unsignalized intersections. Based on the tree\nrepresentation of the solution space for the passing order, we combine Monte\nCarlo tree search (MCTS) and some heuristic rules to find a nearly\nglobal-optimal passing order (leaf node) within a very short planning time.\nTesting results show that this new strategy can keep a good tradeoff between\nperformance and computation flexibility.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 03:43:47 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Xu", "Huile", ""], ["Zhang", "Yi", ""], ["Li", "Li", ""], ["Li", "Weixia", ""]]}, {"id": "1902.01131", "submitter": "Angelo Ferrando", "authors": "Angelo Ferrando, Michael Winikoff, Stephen Cranefield, Frank Dignum,\n  Viviana Mascardi", "title": "On the Enactability of Agent Interaction Protocols: Toward a Unified\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between agents are usually designed from a global viewpoint.\nHowever, the implementation of a multi-agent interaction is distributed. This\ndifference can introduce issues. For instance, it is possible to specify\nprotocols from a global viewpoint that cannot be implemented as a collection of\nindividual agents. This leads naturally to the question of whether a given\n(global) protocol is enactable. We consider this question in a powerful setting\n(trace expression), considering a range of message ordering interpretations\n(what does it mean to say that an interaction step occurs before another), and\na range of possible constraints on the semantics of message delivery,\ncorresponding to different properties of underlying communication middleware.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:39:54 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 22:19:51 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 08:35:21 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 14:24:31 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ferrando", "Angelo", ""], ["Winikoff", "Michael", ""], ["Cranefield", "Stephen", ""], ["Dignum", "Frank", ""], ["Mascardi", "Viviana", ""]]}, {"id": "1902.01455", "submitter": "Ariel Barel Dr.", "authors": "Ariel Barel, Rotem Manor, Alfred M. Bruckstein", "title": "COME TOGETHER: Multi-Agent Geometric Consensus (Gathering, Rendezvous,\n  Clustering, Aggregation)", "comments": "85 pages survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report surveys results on distributed systems comprising mobile agents\nthat are identical and anonymous, oblivious and interact solely by adjusting\ntheir motion according to the relative location of their neighbours. The agents\nare assumed capable of sensing the presence of other agents within a given\nsensing range and able to implement rules of motion based on full or partial\ninformation on the geometric constellation of their neighbouring agents. Eight\ndifferent problems that cover assumptions of finite vs infinite sensing range,\ndirection and distance vs direction only sensing and discrete vs continuous\nmotion, are analyzed in the context of geometric consensus, clustering or\ngathering tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:47:45 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Barel", "Ariel", ""], ["Manor", "Rotem", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1902.01554", "submitter": "Daewoo Kim", "authors": "Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee,\n  Kyunghwan Son, Yung Yi", "title": "Learning to Schedule Communication in Multi-agent Reinforcement Learning", "comments": "Accepted in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world reinforcement learning tasks require multiple agents to make\nsequential decisions under the agents' interaction, where well-coordinated\nactions among the agents are crucial to achieve the target goal better at these\ntasks. One way to accelerate the coordination effect is to enable multiple\nagents to communicate with each other in a distributed manner and behave as a\ngroup. In this paper, we study a practical scenario when (i) the communication\nbandwidth is limited and (ii) the agents share the communication medium so that\nonly a restricted number of agents are able to simultaneously use the medium,\nas in the state-of-the-art wireless networking standards. This calls for a\ncertain form of communication scheduling. In that regard, we propose a\nmulti-agent deep reinforcement learning framework, called SchedNet, in which\nagents learn how to schedule themselves, how to encode the messages, and how to\nselect actions based on received messages. SchedNet is capable of deciding\nwhich agents should be entitled to broadcasting their (encoded) messages, by\nlearning the importance of each agent's partially observed information. We\nevaluate SchedNet against multiple baselines under two different applications,\nnamely, cooperative communication and navigation, and predator-prey. Our\nexperiments show a non-negligible performance gap between SchedNet and other\nmechanisms such as the ones without communication and with vanilla scheduling\nmethods, e.g., round robin, ranging from 32% to 43%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 05:51:36 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Kim", "Daewoo", ""], ["Moon", "Sangwoo", ""], ["Hostallero", "David", ""], ["Kang", "Wan Ju", ""], ["Lee", "Taeyoung", ""], ["Son", "Kyunghwan", ""], ["Yi", "Yung", ""]]}, {"id": "1902.01642", "submitter": "Peer-Olaf Siebers", "authors": "Daniel Stroud, Christian Wagner, Peer-Olaf Siebers", "title": "Agent-Based Simulation Modelling for Reflecting on Consequences of\n  Digital Mental Health", "comments": "16 pages, 18 figures, 3 tables, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The premise of this working paper is based around agent-based simulation\nmodels and how to go about creating them from given incomplete information.\nAgent-based simulations are stochastic simulations that revolve around groups\nof agents that each have their own characteristics and can make decisions. Such\nsimulations can be used to emulate real life situations and to create\nhypothetical situations without the need for real-world testing prior. Here we\ndescribe the development of an agent-based simulation model for studying future\ndigital mental health scenarios. An incomplete conceptual model has been used\nas the basis for this development. To define differences in responses to\nstimuli we employed fuzzy decision making logic. The model has been implemented\nbut not been used for structured experimentation yet. This is planned as our\nnext step.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 11:15:55 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Stroud", "Daniel", ""], ["Wagner", "Christian", ""], ["Siebers", "Peer-Olaf", ""]]}, {"id": "1902.02256", "submitter": "Kaveh Fathian", "authors": "Kaveh Fathian, Kasra Khosoussi, Yulun Tian, Parker Lusk, Jonathan P.\n  How", "title": "CLEAR: A Consistent Lifting, Embedding, and Alignment Rectification\n  Algorithm for Multi-View Data Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotics applications require alignment and fusion of observations\nobtained at multiple views to form a global model of the environment. Multi-way\ndata association methods provide a mechanism to improve alignment accuracy of\npairwise associations and ensure their consistency. However, existing methods\nthat solve this computationally challenging problem are often too slow for\nreal-time applications. Furthermore, some of the existing techniques can\nviolate the cycle consistency principle, thus drastically reducing the fusion\naccuracy. This work presents the CLEAR (Consistent Lifting, Embedding, and\nAlignment Rectification) algorithm to address these issues. By leveraging\ninsights from the multi-way matching and spectral graph clustering literature,\nCLEAR provides cycle consistent and accurate solutions in a computationally\nefficient manner. Numerical experiments on both synthetic and real datasets are\ncarried out to demonstrate the scalability and superior performance of our\nalgorithm in real-world problems. This algorithmic framework can provide\nsignificant improvement in the accuracy and efficiency of existing discrete\nassignment problems, which traditionally use pairwise (but potentially\ninconsistent) correspondences. An implementation of CLEAR is made publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 16:12:57 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 21:39:56 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 23:08:51 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Fathian", "Kaveh", ""], ["Khosoussi", "Kasra", ""], ["Tian", "Yulun", ""], ["Lusk", "Parker", ""], ["How", "Jonathan P.", ""]]}, {"id": "1902.02311", "submitter": "Alex Tong Lin", "authors": "Alex Tong Lin, Mark J. Debord, Katia Estabridis, Gary Hewer, Guido\n  Montufar, Stanley Osher", "title": "Decentralized Multi-Agents by Imitation of a Centralized Controller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-agent reinforcement learning problem where each agent\nseeks to maximize a shared reward while interacting with other agents, and they\nmay or may not be able to communicate. Typically the agents do not have access\nto other agent policies and thus each agent is situated in a non-stationary and\npartially-observable environment. In order to obtain multi-agents that act in a\ndecentralized manner, we introduce a novel algorithm under the popular\nframework of centralized training, but decentralized execution. This training\nframework first obtains solutions to a multi-agent problem with a single\ncentralized joint-space learner, which is then used to guide imitation learning\nfor independent decentralized multi-agents. This framework has the flexibility\nto use any reinforcement learning algorithm to obtain the expert as well as any\nimitation learning algorithm to obtain the decentralized agents. This is in\ncontrast to other multi-agent learning algorithms that, for example, can\nrequire more specific structures. We present some theoretical bounds for our\nmethod, and we show that one can obtain decentralized solutions to a\nmulti-agent problem through imitation learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 18:14:31 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 14:48:32 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 01:39:00 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 18:59:26 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lin", "Alex Tong", ""], ["Debord", "Mark J.", ""], ["Estabridis", "Katia", ""], ["Hewer", "Gary", ""], ["Montufar", "Guido", ""], ["Osher", "Stanley", ""]]}, {"id": "1902.02393", "submitter": "Sudarshanan Bharadwaj", "authors": "Suda Bharadwaj, Rayna Dimitrova, Ufuk Topcu", "title": "Distributed Synthesis of Surveillance Strategies for Mobile Sensors", "comments": null, "journal-ref": "2018 IEEE Conference on Decision and Control (CDC), FL, USA, 2018,\n  pp. 3335-3342", "doi": "10.1109/CDC.2018.8619145", "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of synthesizing strategies for a mobile sensor network\nto conduct surveillance in partnership with static alarm triggers. We formulate\nthe problem as a multi-agent reactive synthesis problem with surveillance\nobjectives specified as temporal logic formulas. In order to avoid the state\nspace blow-up arising from a centralized strategy computation, we propose a\nmethod to decentralize the surveillance strategy synthesis by decomposing the\nmulti-agent game into subgames that can be solved independently. We also\ndecompose the global surveillance specification into local specifications for\neach sensor, and show that if the sensors satisfy their local surveillance\nspecifications, then the sensor network as a whole will satisfy the global\nsurveillance objective. Thus, our method is able to guarantee global\nsurveillance properties in a mobile sensor network while synthesizing\ncompletely decentralized strategies with no need for coordination between the\nsensors. We also present a case study in which we demonstrate an application of\ndecentralized surveillance strategy synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:41:51 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Bharadwaj", "Suda", ""], ["Dimitrova", "Rayna", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1902.03039", "submitter": "Uthman Baroudi Dr", "authors": "Gamal Sallam and Uthman Baroudi", "title": "A Framework for Autonomous Robot Deployment with Perfect Demand\n  Satisfaction using Virtual Forces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, robots autonomous deployment is preferable and\nsometimes it is the only affordable solution. To address this issue, virtual\nforce (VF) is one of the prominent approaches to performing multirobot\ndeployment autonomously. However, most of the existing VF-based approaches\nconsider only a uniform deployment to maximize the covered area while ignoring\nthe criticality of specific locations during the deployment process. To\novercome these limitations, we present a framework for autonomously deploy\nrobots or vehicles using virtual force. The framework is composed of two\nstages. In the first stage, a two-hop Cooperative Virtual Force based Robots\nDeployment (Two-hop COVER) is employed where a cooperative relation between\nrobots and neighboring landmarks is established to satisfy mission\nrequirements. The second stage complements the first stage and ensures perfect\ndemand satisfaction by utilizing the Trace Fingerprint technique which\ncollected traces while each robot traversing the deployment area. Finally, a\nfairness-aware version of Two-hop COVER is presented to consider scenarios\nwhere the mission requirements are greater than the available resources (i.e.\nrobots). We evaluate our framework via extensive simulations. The results\ndemonstrate outstanding performance compared to contemporary approaches in\nterms of total travelled distance, total exchanged messages, total deployment\ntime, and Jain fairness index.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:03:16 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Sallam", "Gamal", ""], ["Baroudi", "Uthman", ""]]}, {"id": "1902.03079", "submitter": "Zehong Cao Prof.", "authors": "Zehong Cao, Chin-Teng Lin", "title": "Reinforcement Learning from Hierarchical Critics", "comments": "This paper is submitted to IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the use of global information to speed up the\nlearning process and increase the cumulative rewards of reinforcement learning\n(RL) in competition tasks. Within the actor-critic RL, we introduce multiple\ncooperative critics from two levels of the hierarchy and propose a\nreinforcement learning from hierarchical critics (RLHC) algorithm. In our\napproach, each agent receives value information from local and global critics\nregarding a competition task and accesses multiple cooperative critics in a\ntop-down hierarchy. Thus, each agent not only receives low-level details but\nalso considers coordination from higher levels, thereby obtaining global\ninformation to improve the training performance. Then, we test the proposed\nRLHC algorithm against the benchmark algorithm, proximal policy optimisation\n(PPO), for two experimental scenarios performed in a Unity environment\nconsisting of tennis and soccer agents' competitions. The results showed that\nRLHC outperforms the benchmark on both competition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 13:55:11 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 01:59:25 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 14:34:16 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2020 12:20:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Cao", "Zehong", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1902.03101", "submitter": "Giulia Michieletto", "authors": "Giulia Michieletto, Angelo Cenedese and Daniel Zelazo", "title": "A Unified Dissertation on Bearing Rigidity Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the bearing rigidity theory, namely the branch of\nknowledge investigating the structural properties necessary for multi-element\nsystems to preserve the inter-units bearings when exposed to deformations. The\noriginal contributions are twofold. The first one consists in the definition of\na general framework for the statement of the principal definitions and results\nthat are then particularized by evaluating the most studied metric spaces,\nproviding a complete overview of the existing literature about the bearing\nrigidity theory. The second one rests on the determination of a necessary and\nsufficient condition guaranteeing the rigidity properties of a given\nmulti-element system, independently of its metric space.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 07:07:04 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:07:38 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 09:07:33 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 14:32:32 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Michieletto", "Giulia", ""], ["Cenedese", "Angelo", ""], ["Zelazo", "Daniel", ""]]}, {"id": "1902.03185", "submitter": "Nicolas Anastassacos", "authors": "Nicolas Anastassacos, Stephen Hailes, Mirco Musolesi", "title": "Partner Selection for the Emergence of Cooperation in Multi-Agent\n  Systems Using Reinforcement Learning", "comments": "8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social dilemmas have been widely studied to explain how humans are able to\ncooperate in society. Considerable effort has been invested in designing\nartificial agents for social dilemmas that incorporate explicit agent\nmotivations that are chosen to favor coordinated or cooperative responses. The\nprevalence of this general approach points towards the importance of achieving\nan understanding of both an agent's internal design and external environment\ndynamics that facilitate cooperative behavior. In this paper, we investigate\nhow partner selection can promote cooperative behavior between agents who are\ntrained to maximize a purely selfish objective function. Our experiments reveal\nthat agents trained with this dynamic learn a strategy that retaliates against\ndefectors while promoting cooperation with other agents resulting in a\nprosocial society.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:47:00 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 15:54:26 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 12:18:00 GMT"}, {"version": "v4", "created": "Thu, 28 Nov 2019 14:59:18 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Anastassacos", "Nicolas", ""], ["Hailes", "Stephen", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1902.03693", "submitter": "Denis Pankratov", "authors": "Stefan Dobrev, Lata Narayanan, Jaroslav Opatrny, Denis Pankratov", "title": "Exploration of High-Dimensional Grids by Finite State Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a treasure at an unknown point of an\n$n$-dimensional infinite grid, $n\\geq 3$, by initially collocated finite state\nagents (scouts/robots). Recently, the problem has been well characterized for 2\ndimensions for deterministic as well as randomized agents, both in synchronous\nand semi-synchronous models. It has been conjectured that $n+1$ randomized\nagents are necessary to solve this problem in the $n$-dimensional grid. In this\npaper we disprove the conjecture in a strong sense: we show that three\nrandomized synchronous agents suffice to explore an $n$-dimensional grid for\nany $n$. Our algorithm is optimal in terms of the number of the agents. Our key\ninsight is that a constant number of finite state machine agents can, by their\npositions and movements, implement a stack, which can store the path being\nexplored. We also show how to implement our algorithm using: four randomized\nsemi-synchronous agents; four deterministic synchronous agents; or five\ndeterministic semi-synchronous agents.\n  We give a different algorithm that uses $4$ deterministic semi-synchronous\nagents for the $3$-dimensional grid. This is provably optimal, and\nsurprisingly, matches the result for $2$ dimensions. For $n\\geq 4$, the time\ncomplexity of the solutions mentioned above is exponential in distance $D$ of\nthe treasure from the starting point of the agents. We show that in the\ndeterministic case, one additional agent brings the time down to a polynomial.\nFinally, we focus on algorithms that never venture much beyond the distance\n$D$. We describe an algorithm that uses $O(\\sqrt{n})$ semi-synchronous\ndeterministic agents that never go beyond $2D$, as well as show that any\nalgorithm using $3$ synchronous deterministic agents in $3$ dimensions must\ntravel beyond $\\Omega(D^{3/2})$ from the origin.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 01:08:07 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Dobrev", "Stefan", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Pankratov", "Denis", ""]]}, {"id": "1902.04043", "submitter": "Mikayel Samvelyan", "authors": "Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory\n  Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S.\n  Torr, Jakob Foerster, Shimon Whiteson", "title": "The StarCraft Multi-Agent Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep multi-agent reinforcement learning (RL) has\nbecome a highly active area of research. A particularly challenging class of\nproblems in this area is partially observable, cooperative, multi-agent\nlearning, in which teams of agents must learn to coordinate their behaviour\nwhile conditioning only on their private observations. This is an attractive\nresearch area since such problems are relevant to a large number of real-world\nsystems and are also more amenable to evaluation than general-sum problems.\nStandardised environments such as the ALE and MuJoCo have allowed single-agent\nRL to move beyond toy domains, such as grid worlds. However, there is no\ncomparable benchmark for cooperative multi-agent RL. As a result, most papers\nin this field use one-off toy problems, making it difficult to measure real\nprogress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC)\nas a benchmark problem to fill this gap. SMAC is based on the popular real-time\nstrategy game StarCraft II and focuses on micromanagement challenges where each\nunit is controlled by an independent agent that must act based on local\nobservations. We offer a diverse set of challenge maps and recommendations for\nbest practices in benchmarking and evaluations. We also open-source a deep\nmulti-agent RL learning framework including state-of-the-art algorithms. We\nbelieve that SMAC can provide a standard benchmark environment for years to\ncome. Videos of our best agents for several SMAC scenarios are available at:\nhttps://youtu.be/VZ7zmQ_obZ0.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 18:43:53 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 13:42:54 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 19:38:36 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 14:52:00 GMT"}, {"version": "v5", "created": "Mon, 9 Dec 2019 07:26:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Samvelyan", "Mikayel", ""], ["Rashid", "Tabish", ""], ["de Witt", "Christian Schroeder", ""], ["Farquhar", "Gregory", ""], ["Nardelli", "Nantas", ""], ["Rudner", "Tim G. J.", ""], ["Hung", "Chia-Man", ""], ["Torr", "Philip H. S.", ""], ["Foerster", "Jakob", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1902.05943", "submitter": "Marius Silaghi", "authors": "Viorel D. Silaghi, Marius C. Silaghi, Ren\\'e Mandiau", "title": "Privacy of Existence of Secrets: Introducing Steganographic DCOPs and\n  Revisiting DCOP Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we identify a type of privacy concern in Distributed Constraint\nOptimization (DCOPs) not previously addressed in literature, despite its\nimportance and impact on the application field: the privacy of existence of\nsecrets. Science only starts where metrics and assumptions are clearly defined.\nThe area of Distributed Constraint Optimization has emerged at the intersection\nof the multi-agent system community and constraint programming. For the\nmulti-agent community, the constraint optimization problems are an elegant way\nto express many of the problems occurring in trading and distributed robotics.\nFor the theoretical constraint programming community the DCOPs are a natural\nextension of their main object of study, the constraint satisfaction problem.\nAs such, the understanding of the DCOP framework has been refined with the\nneeds of the two communities, but sometimes without spelling the new\nassumptions formally and therefore making it difficult to compare techniques.\nHere we give a direction to the efforts for structuring concepts in this area.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 18:52:26 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Silaghi", "Viorel D.", ""], ["Silaghi", "Marius C.", ""], ["Mandiau", "Ren\u00e9", ""]]}, {"id": "1902.06039", "submitter": "Chen Dingding", "authors": "Yanchen Deng, Ziyu Chen, Dingding Chen, Xingqiong Jiang, Qiang Li", "title": "PT-ISABB: A Hybrid Tree-based Complete Algorithm to Solve Asymmetric\n  Distributed Constraint Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric Distributed Constraint Optimization Problems (ADCOPs) have emerged\nas an important formalism in multi-agent community due to their ability to\ncapture personal preferences. However, the existing search-based complete\nalgorithms for ADCOPs can only use local knowledge to compute lower bounds,\nwhich leads to inefficient pruning and prohibits them from solving large scale\nproblems. On the other hand, inference-based complete algorithms (e.g., DPOP)\nfor Distributed Constraint Optimization Problems (DCOPs) require only a linear\nnumber of messages, but they cannot be directly applied into ADCOPs due to a\nprivacy concern. Therefore, in the paper, we consider the possibility of\ncombining inference and search to effectively solve ADCOPs at an acceptable\nloss of privacy. Specifically, we propose a hybrid complete algorithm called\nPT-ISABB which uses a tailored inference algorithm to provide tight lower\nbounds and a tree-based complete search algorithm to exhaust the search space.\nWe prove the correctness of our algorithm and the experimental results\ndemonstrate its superiority over other state-of-the-art complete algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:30:05 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 03:45:23 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 14:25:30 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 02:54:29 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Deng", "Yanchen", ""], ["Chen", "Ziyu", ""], ["Chen", "Dingding", ""], ["Jiang", "Xingqiong", ""], ["Li", "Qiang", ""]]}, {"id": "1902.06228", "submitter": "Feng Xiao", "authors": "Jintao Ke, Feng Xiao, Hai Yang and Jieping Ye", "title": "Optimizing Online Matching for Ride-Sourcing Services with Multi-Agent\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ride-sourcing services are now reshaping the way people travel by effectively\nconnecting drivers and passengers through mobile internets. Online matching\nbetween idle drivers and waiting passengers is one of the most key components\nin a ride-sourcing system. The average pickup distance or time is an important\nmeasurement of system efficiency since it affects both passengers' waiting time\nand drivers' utilization rate. It is naturally expected that a more effective\nbipartite matching (with smaller average pickup time) can be implemented if the\nplatform accumulates more idle drivers and waiting passengers in the matching\npool. A specific passenger request can also benefit from a delayed matching\nsince he/she may be matched with closer idle drivers after waiting for a few\nseconds. Motivated by the potential benefits of delayed matching, this paper\nestablishes a two-stage framework which incorporates a combinatorial\noptimization and multi-agent deep reinforcement learning methods. The\nmulti-agent reinforcement learning methods are used to dynamically determine\nthe delayed time for each passenger request (or the time at which each request\nenters the matching pool), while the combinatorial optimization conducts an\noptimal bipartite matching between idle drivers and waiting passengers in the\nmatching pool. Two reinforcement learning methods, spatio-temporal multi-agent\ndeep Q learning (ST-M-DQN) and spatio-temporal multi-agent actor-critic\n(ST-M-A2C) are developed. Through extensive empirical experiments with a\nwell-designed simulator, we show that the proposed framework is able to\nremarkably improve system performances.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 09:08:52 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ke", "Jintao", ""], ["Xiao", "Feng", ""], ["Yang", "Hai", ""], ["Ye", "Jieping", ""]]}, {"id": "1902.06335", "submitter": "Christian Kroer", "authors": "Christian Kroer and Tuomas Sandholm", "title": "Limited Lookahead in Imperfect-Information Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited lookahead has been studied for decades in perfect-information games.\nWe initiate a new direction via two simultaneous deviation points:\ngeneralization to imperfect-information games and a game-theoretic approach. We\nstudy how one should act when facing an opponent whose lookahead is limited. We\nstudy this for opponents that differ based on their lookahead depth, based on\nwhether they, too, have imperfect information, and based on how they break\nties. We characterize the hardness of finding a Nash equilibrium or an optimal\ncommitment strategy for either player, showing that in some of these variations\nthe problem can be solved in polynomial time while in others it is PPAD-hard,\nNP-hard, or inapproximable. We proceed to design algorithms for computing\noptimal commitment strategies---for when the opponent breaks ties favorably,\naccording to a fixed rule, or adversarially. We then experimentally investigate\nthe impact of limited lookahead. The limited-lookahead player often obtains the\nvalue of the game if she knows the expected values of nodes in the game tree\nfor some equilibrium---but we prove this is not sufficient in general. Finally,\nwe study the impact of noise in those estimates and different lookahead depths.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:50:05 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 13:42:28 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kroer", "Christian", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1902.06527", "submitter": "Woojun Kim", "authors": "Woojun Kim, Myungsik Cho, Youngchul Sung", "title": "Message-Dropout: An Efficient Training Method for Multi-Agent Deep\n  Reinforcement Learning", "comments": "The 33rd AAAI Conference on Artificial Intelligence (AAAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new learning technique named message-dropout to\nimprove the performance for multi-agent deep reinforcement learning under two\napplication scenarios: 1) classical multi-agent reinforcement learning with\ndirect message communication among agents and 2) centralized training with\ndecentralized execution. In the first application scenario of multi-agent\nsystems in which direct message communication among agents is allowed, the\nmessage-dropout technique drops out the received messages from other agents in\na block-wise manner with a certain probability in the training phase and\ncompensates for this effect by multiplying the weights of the dropped-out block\nunits with a correction probability. The applied message-dropout technique\neffectively handles the increased input dimension in multi-agent reinforcement\nlearning with communication and makes learning robust against communication\nerrors in the execution phase. In the second application scenario of\ncentralized training with decentralized execution, we particularly consider the\napplication of the proposed message-dropout to Multi-Agent Deep Deterministic\nPolicy Gradient (MADDPG), which uses a centralized critic to train a\ndecentralized actor for each agent. We evaluate the proposed message-dropout\ntechnique for several games, and numerical results show that the proposed\nmessage-dropout technique with proper dropout rate improves the reinforcement\nlearning performance significantly in terms of the training speed and the\nsteady-state performance in the execution phase.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 11:40:29 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kim", "Woojun", ""], ["Cho", "Myungsik", ""], ["Sung", "Youngchul", ""]]}, {"id": "1902.06897", "submitter": "Shubham Gupta", "authors": "Shubham Gupta and Ambedkar Dukkipati", "title": "Winning an Election: On Emergent Strategic Communication in Multi-Agent\n  Networks", "comments": "A shorter version of this paper has been accepted as an extended\n  abstract at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans use language to collectively execute abstract strategies besides using\nit as a referential tool for identifying physical entities. Recently, multiple\nattempts at replicating the process of emergence of language in artificial\nagents have been made. While existing approaches study emergent languages as\nreferential tools, in this paper, we study their role in discovering and\nimplementing strategies. We formulate the problem using a voting game where two\ncandidate agents contest in an election with the goal of convincing population\nmembers (other agents), that are connected to each other via an underlying\nnetwork, to vote for them. To achieve this goal, agents are only allowed to\nexchange messages in the form of sequences of discrete symbols to spread their\npropaganda. We use neural networks with Gumbel-Softmax relaxation for sampling\ncategorical random variables to parameterize the policies followed by all\nagents. Using our proposed framework, we provide concrete answers to the\nfollowing questions: (i) Do the agents learn to communicate in a meaningful way\nand does the emergent communication play a role in deciding the winner? (ii)\nDoes the system evolve as expected under various reward structures? (iii) How\nis the emergent language affected by the community structure in the network? To\nthe best of our knowledge, we are the first to explore emergence of\ncommunication for discovering and implementing strategies in a setting where\nagents communicate over a network.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 05:14:14 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 05:41:09 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Gupta", "Shubham", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1902.06996", "submitter": "Hao Hao Tan", "authors": "Hao Hao Tan", "title": "Agent Madoff: A Heuristic-Based Negotiation Agent For The Diplomacy\n  Strategy Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present the strategy of Agent Madoff, which is a\nheuristic-based negotiation agent that won 2nd place at the Automated\nNegotiating Agents Competition (ANAC 2017). Agent Madoff is implemented to play\nthe game Diplomacy, which is a strategic board game that mimics the situation\nduring World War I. Each player represents a major European power which has to\nnegotiate with other forces and win possession of a majority supply centers on\nthe map. We propose a design architecture which consists of 3 components:\nheuristic module, acceptance strategy and bidding strategy. The heuristic\nmodule, responsible for evaluating which regions on the graph are more worthy,\nconsiders the type of region and the number of supply centers adjacent to the\nregion and return a utility value for each region on the map. The acceptance\nstrategy is done on a case-by-case basis according to the type of the order by\ncalculating the acceptance probability using a composite function. The bidding\nstrategy adopts a defensive approach aimed to neutralize attacks and resolve\nconflict moves with other players to minimize our loss on supply centers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 11:13:38 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Tan", "Hao Hao", ""]]}, {"id": "1902.07497", "submitter": "Jacopo Castellini", "authors": "Jacopo Castellini, Frans A. Oliehoek, Rahul Savani, Shimon Whiteson", "title": "The Representational Capacity of Action-Value Networks for Multi-Agent\n  Reinforcement Learning", "comments": "This work as been accepted as an Extended Abstract in Proc. of the\n  18th International Conference on Autonomous Agents and Multiagent Systems\n  (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 2019,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the application of deep reinforcement learning\ntechniques to cooperative multi-agent systems, with great empirical success.\nHowever, given the lack of theoretical insight, it remains unclear what the\nemployed neural networks are learning, or how we should enhance their\nrepresentational power to address the problems on which they fail. In this\nwork, we empirically investigate the representational power of various network\narchitectures on a series of one-shot games. Despite their simplicity, these\ngames capture many of the crucial problems that arise in the multi-agent\nsetting, such as an exponential number of joint actions or the lack of an\nexplicit coordination mechanism. Our results quantify how well various\napproaches can represent the requisite value functions, and help us identify\nissues that can impede good performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 10:47:19 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 11:40:39 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 13:46:37 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Castellini", "Jacopo", ""], ["Oliehoek", "Frans A.", ""], ["Savani", "Rahul", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1902.07781", "submitter": "Timotheus Kampik", "authors": "Timotheus Kampik, Juan Carlos Nieves, Helena Lindgren", "title": "Empathic Autonomous Agents", "comments": "Accepted for: Engineering Multi-Agent Systems, 6th International\n  Workshop, EMAS 2018, Stockholm, Sweden, 14-15 July, 2018, Revised, Selected,\n  and Invited Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and resolving conflicts of interests is a key challenge when\ndesigning autonomous agents. For example, such conflicts often occur when\ncomplex information systems interact persuasively with humans and are in the\nfuture likely to arise in non-human agent-to-agent interaction. We introduce a\ntheoretical framework for an empathic autonomous agent that proactively\nidentifies potential conflicts of interests in interactions with other agents\n(and humans) by considering their utility functions and comparing them with its\nown preferences using a system of shared values to find a solution all agents\nconsider acceptable. To illustrate how empathic autonomous agents work, we\nprovide running examples and a simple prototype implementation in a\ngeneral-purpose programing language. To give a high-level overview of our work,\nwe propose a reasoning-loop architecture for our empathic agent.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 21:12:44 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Kampik", "Timotheus", ""], ["Nieves", "Juan Carlos", ""], ["Lindgren", "Helena", ""]]}, {"id": "1902.07860", "submitter": "George Leu", "authors": "George Leu and Jiangjun Tang", "title": "Survivable Networks via UAV Swarms Guided by Decentralized Real-Time\n  Evolutionary Computation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survivable network concept refers to contexts where the wireless\ncommunication between ground agents needs to be maintained as much as possible\nat all times, regardless of any adverse conditions that may arise. In this\npaper we propose a nature-inspired approach to survivable networks, in which we\nbring together swarm intelligence and evolutionary computation. We use an\non-line real-time Genetic Algorithm to optimize the movements of an UAV swarm\ntowards maintaining communication between the ground agents. The proposed\napproach models the ground agents and the UAVs as boids-based swarms, and\noptimizes the movement of the UAVs using different instances of the GA running\nindependently on each UAV. The UAV coordination mechanism is an implicit one,\nembedded in the fitness function of the Genetic Algorithm instances. The\nbehaviors of the individual UAVs emerge into an aggregated optimization of the\noverall network survivability. The results show that the proposed approach is\nable to maintain satisfactory network survivability levels regardless of the\nground agents' movements, including for cases as complex as random walks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 03:51:54 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Leu", "George", ""], ["Tang", "Jiangjun", ""]]}, {"id": "1902.08183", "submitter": "Jose Fontanari", "authors": "Sandro M. Reia, Paulo F. Gomes and Jos\\'e F. Fontanari", "title": "Policies for allocation of information in task-oriented groups: elitism\n  and egalitarianism outperform welfarism", "comments": null, "journal-ref": "Eur. Phys. J. B (2019) 92: 205", "doi": "10.1140/epjb/e2019-100345-7", "report-no": null, "categories": "cs.MA cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication or influence networks are probably the most controllable of all\nfactors that are known to impact on the problem-solving capability of\ntask-forces. In the case connections are costly, it is necessary to implement a\npolicy to allocate them to the individuals. Here we use an agent-based model to\nstudy how distinct allocation policies affect the performance of a group of\nagents whose task is to find the global maxima of NK fitness landscapes. Agents\ncooperate by broadcasting messages informing on their fitness and use this\ninformation to imitate the fittest agent in their influence neighborhoods. The\nlarger the influence neighborhood of an agent, the more links, and hence\ninformation, the agent receives. We find that the elitist policy in which\nagents with above-average fitness have their influence neighborhoods amplified,\nwhereas agents with below-average fitness have theirs deflated, is optimal for\nsmooth landscapes, provided the group size is not too small. For rugged\nlandscapes, however, the elitist policy can perform very poorly for certain\ngroup sizes. In addition, we find that the egalitarian policy, in which the\nsize of the influence neighborhood is the same for all agents, is optimal for\nboth smooth and rugged landscapes in the case of small groups. The welfarist\npolicy, in which the actions of the elitist policy are reversed, is always\nsuboptimal, i.e., depending on the group size it is outperformed by either the\nelitist or the egalitarian policies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 18:51:44 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 17:05:52 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 19:55:57 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Reia", "Sandro M.", ""], ["Gomes", "Paulo F.", ""], ["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "1902.08274", "submitter": "Abhishek Dubey", "authors": "Ayan Mukhopadhyay and Geoffrey Pettet and Chinmaya Samal and Abhishek\n  Dubey and Yevgeniy Vorobeychik", "title": "An Online Decision-Theoretic Pipeline for Responder Dispatch", "comments": "Appeared in ICCPS 2019", "journal-ref": null, "doi": "10.1145/3302509.3311055", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dispatching emergency responders to service traffic accidents,\nfire, distress calls and crimes plagues urban areas across the globe. While\nsuch problems have been extensively looked at, most approaches are offline.\nSuch methodologies fail to capture the dynamically changing environments under\nwhich critical emergency response occurs, and therefore, fail to be implemented\nin practice. Any holistic approach towards creating a pipeline for effective\nemergency response must also look at other challenges that it subsumes -\npredicting when and where incidents happen and understanding the changing\nenvironmental dynamics. We describe a system that collectively deals with all\nthese problems in an online manner, meaning that the models get updated with\nstreaming data sources. We highlight why such an approach is crucial to the\neffectiveness of emergency response, and present an algorithmic framework that\ncan compute promising actions for a given decision-theoretic model for\nresponder dispatch. We argue that carefully crafted heuristic measures can\nbalance the trade-off between computational time and the quality of solutions\nachieved and highlight why such an approach is more scalable and tractable than\ntraditional approaches. We also present an online mechanism for incident\nprediction, as well as an approach based on recurrent neural networks for\nlearning and predicting environmental features that affect responder dispatch.\nWe compare our methodology with prior state-of-the-art and existing dispatch\nstrategies in the field, which show that our approach results in a reduction in\nresponse time with a drastic reduction in computational time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 21:27:43 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Mukhopadhyay", "Ayan", ""], ["Pettet", "Geoffrey", ""], ["Samal", "Chinmaya", ""], ["Dubey", "Abhishek", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1902.08594", "submitter": "Roel Dobbe", "authors": "Oscar Sondermeijer, Roel Dobbe, Daniel Arnold, Claire Tomlin, Tam\\'as\n  Keviczky", "title": "Regression-based Inverter Control for Decentralized Optimal Power Flow\n  and Voltage Regulation", "comments": "Cite as: Oscar Sondermeijer, Roel Dobbe, Daniel Arnold, Claire Tomlin\n  and Tam\\'as Keviczky, \"Regression-based Inverter Control for Decentralized\n  Optimal Power Flow and Voltage Regulation\", IEEE Power & Energy Society\n  General Meeting, Boston, July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic power inverters are capable of quickly delivering reactive power\nto maintain customer voltages within operating tolerances and to reduce system\nlosses in distribution grids. This paper proposes a systematic and data-driven\napproach to determine reactive power inverter output as a function of local\nmeasurements in a manner that obtains near optimal results. First, we use a\nnetwork model and historic load and generation data and do optimal power flow\nto compute globally optimal reactive power injections for all controllable\ninverters in the network. Subsequently, we use regression to find a function\nfor each inverter that maps its local historical data to an approximation of\nits optimal reactive power injection. The resulting functions then serve as\ndecentralized controllers in the participating inverters to predict the optimal\ninjection based on a new local measurements. The method achieves near-optimal\nresults when performing voltage- and capacity-constrained loss minimization and\nvoltage flattening, and allows for an efficient volt-VAR optimization (VVO)\nscheme in which legacy control equipment collaborates with existing inverters\nto facilitate safe operation of distribution networks with higher levels of\ndistributed generation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 23:20:52 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Sondermeijer", "Oscar", ""], ["Dobbe", "Roel", ""], ["Arnold", "Daniel", ""], ["Tomlin", "Claire", ""], ["Keviczky", "Tam\u00e1s", ""]]}, {"id": "1902.08905", "submitter": "Ashis Banerjee", "authors": "Veniamin Tereshchuk, John Stewart, Nikolay Bykov, Samuel Pedigo,\n  Santosh Devasia, and Ashis G. Banerjee", "title": "An Efficient Scheduling Algorithm for Multi-Robot Task Allocation in\n  Assembling Aircraft Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient utilization of cooperating robots in the assembly of aircraft\nstructures relies on balancing the workload of the robots and ensuring\ncollision-free scheduling. We cast this problem as that of allocating a large\nnumber of repetitive assembly tasks, such as drilling holes and installing\nfasteners, among multiple robots. Such task allocation is often formulated as a\nTraveling Salesman Problem (TSP), which is NP-hard, implying that computing an\nexactly optimal solution is computationally prohibitive for real-world\napplications. The problem complexity is further exacerbated by intermittent\nrobot failures necessitating real-time task reallocation. In this letter, we\npresent an efficient method that exploits workpart geometry and problem\nstructure to initially generate balanced and conflict-free robot schedules\nunder nominal conditions. Subsequently, we deal with the failures by allowing\nthe robots to first complete their nominal schedules and then employing a\nmarket-based optimizer to allocate the leftover tasks. Results show an\nimprovement of 11.5\\% in schedule efficiency as compared to an optimized greedy\nmulti-agent scheduler on a four robot system, which is especially promising for\naircraft assembly processes that take many hours to complete. Moreover, the\ncomputation times are similar and small, typically hundreds of milliseconds.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 07:38:22 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 02:29:46 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Tereshchuk", "Veniamin", ""], ["Stewart", "John", ""], ["Bykov", "Nikolay", ""], ["Pedigo", "Samuel", ""], ["Devasia", "Santosh", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "1902.09097", "submitter": "Joe Booth", "authors": "Joe Booth, Jackson Booth", "title": "Marathon Environments: Multi-Agent Continuous Control Benchmarks in a\n  Modern Video Game Engine", "comments": "AAAI-2019 Workshop on Games and Simulations for Artificial\n  Intelligence", "journal-ref": "AAAI-2019 Workshop on Games and Simulations for Artificial\n  Intelligence", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep reinforcement learning in the paradigm of locomotion\nusing continuous control have raised the interest of game makers for the\npotential of digital actors using active ragdoll. Currently, the available\noptions to develop these ideas are either researchers' limited codebase or\nproprietary closed systems. We present Marathon Environments, a suite of open\nsource, continuous control benchmarks implemented on the Unity game engine,\nusing the Unity ML- Agents Toolkit. We demonstrate through these benchmarks\nthat continuous control research is transferable to a commercial game engine.\nFurthermore, we exhibit the robustness of these environments by reproducing\nadvanced continuous control research, such as learning to walk, run and\nbackflip from motion capture data; learning to navigate complex terrains; and\nby implementing a video game input control system. We show further robustness\nby training with alternative algorithms found in OpenAI.Baselines. Finally, we\nshare strategies for significantly reducing the training time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 05:56:35 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Booth", "Joe", ""], ["Booth", "Jackson", ""]]}, {"id": "1902.09359", "submitter": "Panayiotis Danassis", "authors": "Panayiotis Danassis, Aris Filos-Ratsikas, Boi Faltings", "title": "Anytime Heuristic for Weighted Matching Through Altruism-Inspired\n  Behavior", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2019/31", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel anytime heuristic (ALMA), inspired by the human principle\nof altruism, for solving the assignment problem. ALMA is decentralized,\ncompletely uncoupled, and requires no communication between the participants.\nWe prove an upper bound on the convergence speed that is polynomial in the\ndesired number of resources and competing agents per resource; crucially, in\nthe realistic case where the aforementioned quantities are bounded\nindependently of the total number of agents/resources, the convergence time\nremains constant as the total problem size increases.\n  We have evaluated ALMA under three test cases: (i) an anti-coordination\nscenario where agents with similar preferences compete over the same set of\nactions, (ii) a resource allocation scenario in an urban environment, under a\nconstant-time constraint, and finally, (iii) an on-line matching scenario using\nreal passenger-taxi data. In all of the cases, ALMA was able to reach high\nsocial welfare, while being orders of magnitude faster than the centralized,\noptimal algorithm. The latter allows our algorithm to scale to realistic\nscenarios with hundreds of thousands of agents, e.g., vehicle coordination in\nurban environments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:24:19 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Danassis", "Panayiotis", ""], ["Filos-Ratsikas", "Aris", ""], ["Faltings", "Boi", ""]]}, {"id": "1902.09687", "submitter": "Bin Song", "authors": "Dan Wang, Wei Zhang, Bin Song, Xiaojiang Du and Mohsen Guizani", "title": "Market-Based Model in CR-WSN: A Q-Probabilistic Multi-agent Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasingly urban populations and their material demands have\nbrought unprecedented burdens to cities. Smart cities leverage emerging\ntechnologies like the Internet of Things (IoT), Cognitive Radio Wireless Sensor\nNetwork (CR-WSN) to provide better QoE and QoS for all citizens. However,\nresource scarcity is an important challenge in CR-WSN. Generally, this problem\nis handled by auction theory or game theory. To make CR-WSN nodes intelligent\nand more autonomous in resource allocation, we propose a multi-agent\nreinforcement learning (MARL) algorithm to learn the optimal resource\nallocation strategy in the oligopoly market model. Firstly, we model a\nmulti-agent scenario, in which the primary users (PUs) is the sellers and the\nsecondary users (SUs) is the buyers. Then, we propose the Q-probabilistic\nmultiagent learning (QPML) and apply it to allocate resources in the market. In\nthe multi-agent interactive learning process, the PUs and SUs learn strategies\nto maximize their benefits and improve spectrum utilization. Experimental\nresults show the efficiency of our QPML approach, which can also converge\nquickly.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:15:47 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Wang", "Dan", ""], ["Zhang", "Wei", ""], ["Song", "Bin", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1902.09840", "submitter": "Mikko Lauri", "authors": "Mikko Lauri, Joni Pajarinen, Jan Peters", "title": "Information Gathering in Decentralized POMDPs by Policy Graph\n  Improvement", "comments": "18th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized policies for information gathering are required when multiple\nautonomous agents are deployed to collect data about a phenomenon of interest\nwithout the ability to communicate. Decentralized partially observable Markov\ndecision processes (Dec-POMDPs) are a general, principled model well-suited for\nsuch decentralized multiagent decision-making problems. In this paper, we\ninvestigate Dec-POMDPs for decentralized information gathering problems. An\noptimal solution of a Dec-POMDP maximizes the expected sum of rewards over\ntime. To encourage information gathering, we set the reward as a function of\nthe agents' state information, for example the negative Shannon entropy. We\nprove that if the reward is convex, then the finite-horizon value function of\nthe corresponding Dec-POMDP is also convex. We propose the first heuristic\nalgorithm for information gathering Dec-POMDPs, and empirically prove its\neffectiveness by solving problems an order of magnitude larger than previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:18:48 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Lauri", "Mikko", ""], ["Pajarinen", "Joni", ""], ["Peters", "Jan", ""]]}, {"id": "1902.10535", "submitter": "Manuel Sorge", "authors": "Jiehua Chen and Piotr Skowron and Manuel Sorge", "title": "Matchings under Preferences: Strength of Stability and Trade-offs", "comments": "47 pages. Accepted for presentation at the 20th ACM Conference on\n  Economics and Computation (EC '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two solution concepts for matchings under preferences: robustness\nand near stability. The former strengthens while the latter relaxes the classic\ndefinition of stability by Gale and Shapley (1962). Informally speaking,\nrobustness requires that a matching must be stable in the classic sense, even\nif the agents slightly change their preferences. Near stability, on the other\nhand, imposes that a matching must become stable (again, in the classic sense)\nprovided the agents are willing to adjust their preferences a bit. Both of our\nconcepts are quantitative; together they provide means for a fine-grained\nanalysis of stability for matchings. Moreover, our concepts allow to explore\nthe trade-offs between stability and other criteria of societal optimality,\nsuch as the egalitarian cost and the number of unmatched agents. We investigate\nthe computational complexity of finding matchings that implement certain\npredefined trade-offs. We provide a polynomial-time algorithm that given\nagents' preferences returns a socially optimal robust matching, and we prove\nthat finding a socially optimal and nearly stable matching is computationally\nhard.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 11:19:24 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 16:42:57 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Jiehua", ""], ["Skowron", "Piotr", ""], ["Sorge", "Manuel", ""]]}, {"id": "1902.10662", "submitter": "Siddharth Mayya", "authors": "Siddharth Mayya, Gennaro Notomista, Dylan Shell, Seth Hutchinson,\n  Magnus Egerstedt", "title": "Non-Uniform Robot Densities in Vibration Driven Swarms Using Phase\n  Separation Theory", "comments": "8 pages, submitted for possible publication to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In robot swarms operating under highly restrictive sensing and communication\nconstraints, individuals may need to use direct physical proximity to\nfacilitate information exchange. However, in certain task-related scenarios,\nthis requirement might conflict with the need for robots to spread out in the\nenvironment, e.g., for distributed sensing or surveillance applications. This\npaper demonstrates how a swarm of minimally-equipped robots can form\nhigh-density robot aggregates which coexist with lower robot densities in the\ndomain. We envision a scenario where a swarm of vibration-driven robots---which\nsit atop bristles and achieve directed motion by vibrating them---move somewhat\nrandomly in an environment while colliding with each other. Theoretical\ntechniques from the study of far-from-equilibrium collectives and statistical\nmechanics clarify the mechanisms underlying the formation of these high and low\ndensity regions. Specifically, we capitalize on a transformation that connects\nthe collective properties of a system of self-propelled particles with that of\na well-studied molecular fluid system, thereby inheriting the rich theory of\nequilibrium thermodynamics. This connection is a formal one and is a relatively\nrecent result in studies of motility induced phase separation; it is previously\nunexplored in the context of robotics. Real robot experiments as well as\nsimulations illustrate how inter-robot collisions can precipitate the formation\nof non-uniform robot densities in a closed and bounded region.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:53:25 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 08:04:06 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Mayya", "Siddharth", ""], ["Notomista", "Gennaro", ""], ["Shell", "Dylan", ""], ["Hutchinson", "Seth", ""], ["Egerstedt", "Magnus", ""]]}, {"id": "1902.11015", "submitter": "Xiuhui Peng", "authors": "Xiuhui Peng, Zhiyong Sun, Kexin Guo, Zhiyong Geng", "title": "Mobile Formation Coordination and Tracking Control for Multiple\n  Non-holonomic Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses forward motion control for trajectory tracking and\nmobile formation coordination for a group of non-holonomic vehicles on SE(2).\nFirstly, by constructing an intermediate attitude variable which involves\nvehicles' position information and desired attitude, the translational and\nrotational control inputs are designed in two stages to solve the trajectory\ntracking problem. Secondly, the coordination relationships of relative\npositions and headings are explored thoroughly for a group of non-holonomic\nvehicles to maintain a mobile formation with rigid body motion constraints. We\nprove that, except for the cases of parallel formation and translational\nstraight line formation, a mobile formation with strict rigid-body motion can\nbe achieved if and only if the ratios of linear speed to angular speed for each\nindividual vehicle are constants. Motion properties for mobile formation with\nweak rigid-body motion are also demonstrated. Thereafter, based on the proposed\ntrajectory tracking approach, a distributed mobile formation control law is\ndesigned under a directed tree graph. The performance of the proposed\ncontrollers is validated by both numerical simulations and experiments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:59:01 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Peng", "Xiuhui", ""], ["Sun", "Zhiyong", ""], ["Guo", "Kexin", ""], ["Geng", "Zhiyong", ""]]}, {"id": "1902.11212", "submitter": "Manxing Du", "authors": "Manxing Du, Alexander I. Cowen-Rivers, Ying Wen, Phu Sakulwongtana,\n  Jun Wang, Mats Brorsson, Radu State", "title": "Infer Your Enemies and Know Yourself, Learning in Real-Time Bidding with\n  Partially Observable Opponents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time bidding, as one of the most popular mechanisms for selling online\nad slots, facilitates advertisers to reach their potential customers. The goal\nof bidding optimization is to maximize the advertisers' return on investment\n(ROI) under a certain budget setting. A straightforward solution is to model\nthe bidding function in an explicit form. However, the static functional\nsolutions lack generality in practice and are insensitive to the stochastic\nbehaviour of other bidders in the environment. In this paper, we propose a\ngeneral multi-agent framework with actor-critic solutions facing against\nplaying imperfect information games. We firstly introduce a novel Deep\nAttentive Survival Analysis (DASA) model to infer the censored data in the\nsecond price auctions which outperforms start-of-the-art survival analysis.\nFurthermore, our approach introduces the DASA model as the opponent model into\nthe policy learning process for each agent and develop a mean field equilibrium\nanalysis of the second price auctions. The experiments have shown that with the\ninference of the market, the market converges to the equilibrium much faster\nwhile playing against both fixed strategy agents and dynamic learning agents.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:52:14 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Du", "Manxing", ""], ["Cowen-Rivers", "Alexander I.", ""], ["Wen", "Ying", ""], ["Sakulwongtana", "Phu", ""], ["Wang", "Jun", ""], ["Brorsson", "Mats", ""], ["State", "Radu", ""]]}]