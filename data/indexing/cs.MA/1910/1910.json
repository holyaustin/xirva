[{"id": "1910.00091", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer, Vitaly Kurin, Shimon Whiteson", "title": "Deep Coordination Graphs", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the deep coordination graph (DCG) for collaborative\nmulti-agent reinforcement learning. DCG strikes a flexible trade-off between\nrepresentational capacity and generalization by factoring the joint value\nfunction of all agents according to a coordination graph into payoffs between\npairs of agents. The value can be maximized by local message passing along the\ngraph, which allows training of the value function end-to-end with Q-learning.\nPayoff functions are approximated with deep neural networks that employ\nparameter sharing and low-rank approximations to significantly improve sample\nefficiency. We show that DCG can solve predator-prey tasks that highlight the\nrelative overgeneralization pathology, as well as challenging StarCraft II\nmicromanagement tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 17:25:41 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 16:13:15 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 16:12:47 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 17:28:04 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Kurin", "Vitaly", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1910.00120", "submitter": "Dimitri Bertsekas", "authors": "Dimitri Bertsekas", "title": "Multiagent Rollout Algorithms and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider finite and infinite horizon dynamic programming problems, where\nthe control at each stage consists of several distinct decisions, each one made\nby one of several agents. We introduce an approach, whereby at every stage,\neach agent's decision is made by executing a local rollout algorithm that uses\na base policy, together with some coordinating information from the other\nagents. The amount of local computation required at every stage by each agent\nis independent of the number of agents, while the amount of total computation\n(over all agents) grows linearly with the number of agents. By contrast, with\nthe standard rollout algorithm, the amount of total computation grows\nexponentially with the number of agents. Despite the drastic reduction in\nrequired computation, we show that our algorithm has the fundamental cost\nimprovement property of rollout: an improved performance relative to the base\npolicy. We also discuss possibilities to improve further the method's\ncomputational efficiency through limited agent coordination and parallelization\nof the agents' computations. Finally, we explore related approximate policy\niteration algorithms for infinite horizon problems, and we prove that the cost\nimprovement property steers the algorithm towards convergence to an\nagent-by-agent optimal policy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 21:39:07 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 11:47:13 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 20:55:05 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Bertsekas", "Dimitri", ""]]}, {"id": "1910.00193", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried, Conner Laughlin, Charles Morefield", "title": "Parallel Algorithm for Approximating Nash Equilibrium in Multiplayer\n  Stochastic Games with Application to Naval Strategic Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CR cs.MA econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world domains contain multiple agents behaving strategically with\nprobabilistic transitions and uncertain (potentially infinite) duration. Such\nsettings can be modeled as stochastic games. While algorithms have been\ndeveloped for solving (i.e., computing a game-theoretic solution concept such\nas Nash equilibrium) two-player zero-sum stochastic games, research on\nalgorithms for non-zero-sum and multiplayer stochastic games is limited. We\npresent a new algorithm for these settings, which constitutes the first\nparallel algorithm for multiplayer stochastic games. We present experimental\nresults on a 4-player stochastic game motivated by a naval strategic planning\nscenario, showing that our algorithm is able to quickly compute strategies\nconstituting Nash equilibrium up to a very small degree of approximation error.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 04:08:14 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 02:07:18 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 03:35:08 GMT"}, {"version": "v4", "created": "Fri, 13 Mar 2020 18:54:19 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ganzfried", "Sam", ""], ["Laughlin", "Conner", ""], ["Morefield", "Charles", ""]]}, {"id": "1910.00225", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "Mistakes in Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new concept of \"mistake\" strategies and actions for\nstrategic-form and extensive-form games, analyze the relationship to prior main\ngame-theoretic solution concepts, study algorithms for computation, and explore\npracticality. This concept has potential applications to cybersecurity, for\nexample detecting whether a human player is illegally using real-time\nassistance in games like poker.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 07:01:26 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 19:28:04 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2019 14:31:04 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "1910.00681", "submitter": "David Fridovich-Keil", "authors": "David Fridovich-Keil, Vicenc Rubies-Royo, and Claire J. Tomlin", "title": "An Iterative Quadratic Method for General-Sum Differential Games with\n  Feedback Linearizable Dynamics", "comments": "7 pages, 5 figures, accepted to IEEE International Conference on\n  Robotics and Automation (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.GT cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative linear-quadratic (ILQ) methods are widely used in the nonlinear\noptimal control community. Recent work has applied similar methodology in the\nsetting of multiplayer general-sum differential games. Here, ILQ methods are\ncapable of finding local equilibria in interactive motion planning problems in\nreal-time. As in most iterative procedures, however, this approach can be\nsensitive to initial conditions and hyperparameter choices, which can result in\npoor computational performance or even unsafe trajectories. In this paper, we\nfocus our attention on a broad class of dynamical systems which are feedback\nlinearizable, and exploit this structure to improve both algorithmic\nreliability and runtime. We showcase our new algorithm in three distinct\ntraffic scenarios, and observe that in practice our method converges\nsignificantly more often and more quickly than was possible without exploiting\nthe feedback linearizable structure.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:34:06 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 13:08:27 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Fridovich-Keil", "David", ""], ["Rubies-Royo", "Vicenc", ""], ["Tomlin", "Claire J.", ""]]}, {"id": "1910.00738", "submitter": "Gang Qiao", "authors": "Gang Qiao, Honglu Zhou, Mubbasir Kapadia, Sejong Yoon, Vladimir\n  Pavlovic", "title": "Scenario Generalization of Data-driven Imitation Models in Crowd\n  Simulation", "comments": "12 pages, MIG 2019 - ACM SIGGRAPH Conference on Motion, Interaction\n  and Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd simulation, the study of the movement of multiple agents in complex\nenvironments, presents a unique application domain for machine learning. One\nchallenge in crowd simulation is to imitate the movement of expert agents in\nhighly dense crowds. An imitation model could substitute an expert agent if the\nmodel behaves as good as the expert. This will bring many exciting\napplications. However, we believe no prior studies have considered the critical\nquestion of how training data and training methods affect imitators when these\nmodels are applied to novel scenarios. In this work, a general imitation model\nis represented by applying either the Behavior Cloning (BC) training method or\na more sophisticated Generative Adversarial Imitation Learning (GAIL) method,\non three typical types of data domains: standard benchmarks for evaluating\ncrowd models, random sampling of state-action pairs, and egocentric scenarios\nthat capture local interactions. Simulated results suggest that (i) simpler\ntraining methods are overall better than more complex training methods, (ii)\ntraining samples with diverse agent-agent and agent-obstacle interactions are\nbeneficial for reducing collisions when the trained models are applied to new\nscenarios. We additionally evaluated our models in their ability to imitate\nreal world crowd trajectories observed from surveillance videos. Our findings\nindicate that models trained on representative scenarios generalize to new,\nunseen situations observed in real human crowds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:25:32 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Qiao", "Gang", ""], ["Zhou", "Honglu", ""], ["Kapadia", "Mubbasir", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1910.00741", "submitter": "Shresth Verma", "authors": "Shresth Verma, Joydip Dhar", "title": "Emergence of Writing Systems Through Multi-Agent Cooperation", "comments": "Under Review as Student Abstract at AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to communicate is considered an essential task to develop a general\nAI. While recent literature in language evolution has studied emergent language\nthrough discrete or continuous message symbols, there has been little work in\nthe emergence of writing systems in artificial agents. In this paper, we\npresent a referential game setup with two agents, where the mode of\ncommunication is a written language system that emerges during the play. We\nshow that the agents can learn to coordinate successfully using this mode of\ncommunication. Further, we study how the game rules affect the writing system\ntaxonomy by proposing a consistency metric.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:35:14 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Verma", "Shresth", ""], ["Dhar", "Joydip", ""]]}, {"id": "1910.00767", "submitter": "Rohit K. Dubey Mr", "authors": "Rohit K. Dubey, Samuel S. Sohn, Christoph Hoelscher, Mubbasir Kapadia", "title": "Cognitive Agent Based Simulation Model For Improving Disaster Response\n  Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the event of a disaster, saving human lives is of utmost importance. For\ndeveloping proper evacuation procedures and guidance systems, behavioural data\non how people respond during panic and stress is crucial. In the absence of\nreal human data on building evacuation, there is a need for a crowd simulator\nto model egress and decision-making under uncertainty. In this paper, we\npropose an agent-based simulation tool, which is grounded in human cognition\nand decision-making, for evaluating and improving the effectiveness of building\nevacuation procedures and guidance systems during a disaster. Specifically, we\npropose a predictive agent-wayfinding framework based on information theory\nthat is applied at intersections with variable route choices where it fuses N\ndynamic information sources. The proposed framework can be used to visualize\ntrajectories and prediction results (i.e., total evacuation time, number of\npeople evacuated) for different combinations of reinforcing or contradicting\ninformation sources (i.e., signage, crowd flow, familiarity, and spatial\nlayout). This tool can enable designers to recreate various disaster scenarios\nand generate simulation data for improving the evacuation procedures and\nexisting guidance systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:54:11 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Dubey", "Rohit K.", ""], ["Sohn", "Samuel S.", ""], ["Hoelscher", "Christoph", ""], ["Kapadia", "Mubbasir", ""]]}, {"id": "1910.01208", "submitter": "Lifeng Zhou", "authors": "Lifeng Zhou, Vasileios Tzoumas, George J. Pappas, and Pratap Tokekar", "title": "Distributed Attack-Robust Submodular Maximization for Multi-Robot\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design algorithms to protect swarm-robotics applications\nagainst attacks that result in robot removals. We focus on applications\nrequiring the robots to jointly select actions, e.g., which trajectory to\nfollow, among a set of available ones. Such applications are central in\nlarge-scale robotic applications, such as multi-robot motion planning for\ntarget tracking. But the current attack-robust algorithms are centralized. In\nthis paper, we propose a general-purpose distributed algorithm towards robust\noptimization at scale, with local communications only. We name it Distributed\nRobust Maximization (DRM). DRM proposes a divide-and-conquer approach that\ndistributively partitions the problem among cliques of robots. Then, the\ncliques optimize in parallel, independently of each other. We prove DRM\nachieves a close-to-optimal performance. We demonstrate DRM's performance in\nboth Gazebo and MATLAB simulations, in scenarios of active target tracking with\nswarms of robots. In the simulations, DRM achieves computational speed-ups,\nbeing 3-4 orders faster than the centralized algorithms; yet, it nearly matches\nthe tracking performance of the centralized counterparts. However, DRM\noverestimates the number of attacks in each clique. To amend this\nconservativeness of DRM, in this paper we also introduce an Improved\nDistributed Robust Maximization (IDRM) algorithm. IDRM infers the number of\nattacks in each clique less conservatively than DRM by leveraging 3-hop\nneighboring communications. We verify IDRM improves DRM's performance in\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:35:40 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 01:19:20 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 13:44:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Lifeng", ""], ["Tzoumas", "Vasileios", ""], ["Pappas", "George J.", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1910.01465", "submitter": "Johannes Ackermann", "authors": "Johannes Ackermann, Volker Gabler, Takayuki Osa, Masashi Sugiyama", "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double\n  Centralized Critics", "comments": "Accepted for the Deep RL Workshop at NeurIPS 2019; Changes for v2:\n  Changed Figures 3,4, due to an error in the implementation of MATD3. Please\n  refer to this version for fair evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world tasks require multiple agents to work together. Multi-agent\nreinforcement learning (RL) methods have been proposed in recent years to solve\nthese tasks, but current methods often fail to efficiently learn policies. We\nthus investigate the presence of a common weakness in single-agent RL, namely\nvalue function overestimation bias, in the multi-agent setting. Based on our\nfindings, we propose an approach that reduces this bias by using double\ncentralized critics. We evaluate it on six mixed cooperative-competitive tasks,\nshowing a significant advantage over current methods. Finally, we investigate\nthe application of multi-agent methods to high-dimensional robotic tasks and\nshow that our approach can be used to learn decentralized policies in this\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:40:46 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:00:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ackermann", "Johannes", ""], ["Gabler", "Volker", ""], ["Osa", "Takayuki", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1910.01917", "submitter": "Ragesh K Ramachandran", "authors": "Ragesh K. Ramachandran, Lifeng Zhou James A. Preiss, and Gaurav S.\n  Sukhatme", "title": "Resilient Coverage: Exploring the Local-to-Global Trade-off", "comments": "8 pages, 5 figures, submitted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a centralized control framework to select suitable robots from a\nheterogeneous pool and place them at appropriate locations to monitor a region\nfor events of interest. In the event of a robot failure, the framework\nrepositions robots in a user-defined local neighborhood of the failed robot to\ncompensate for the coverage loss. The central controller augments the team with\nadditional robots from the robot pool when simply repositioning robots fails to\nattain a user-specified level of desired coverage. The size of the local\nneighborhood around the failed robot and the desired coverage over the region\nare two objectives that can be manipulated to achieve a user-specified balance.\nWe investigate the trade-off between the coverage compensation achieved through\nlocal repositioning and the computation required to plan the new robot\nlocations. We also study the relationship between the size of the local\nneighborhood and the number of additional robots added to the team for a given\nuser-specified level of desired coverage. We use extensive simulations and an\nexperiment with a team of seven quadrotors to verify the effectiveness of our\nframework. Additionally, we show that to reach a high level of coverage in a\nneighborhood with a large robot population, it is more efficient to enlarge the\nneighborhood size, instead of adding additional robots and repositioning them.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 04:41:13 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 16:37:27 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ramachandran", "Ragesh K.", ""], ["Preiss", "Lifeng Zhou James A.", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1910.02059", "submitter": "Georgios Birmpas", "authors": "Georgios Birmpas, Elias Koutsoupias, Philip Lazos, Francisco J.\n  Marmolejo-Coss\\'io", "title": "Fairness and Efficiency in DAG-based Cryptocurrencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a decentralised digital currency that serves as an alternative to\nexisting transaction systems based on an external central authority for\nsecurity. Although Bitcoin has many desirable properties, one of its\nfundamental shortcomings is its inability to process transactions at high\nrates. To address this challenge, many subsequent protocols either modify the\nrules of block acceptance (longest chain rule) and reward, or alter the\ngraphical structure of the public ledger from a tree to a directed acyclic\ngraph (DAG). Motivated by these approaches, we introduce a new general\nframework that captures ledger growth for a large class of DAG-based\nimplementations. With this in hand, and by assuming honest miner behaviour, we\n(experimentally) explore how different DAG-based protocols perform in terms of\nfairness, i.e., if the block reward of a miner is proportional to their hash\npower, as well as efficiency, i.e. what proportion of user transactions a\nledger deems valid after a certain length of time. Our results demonstrate\nfundamental structural limits on how well DAG-based ledger protocols cope with\na high transaction load. More specifically, we show that even in a scenario\nwhere every miner on the system is honest in terms of when they publish blocks,\nwhat they point to, and what transactions each block contains, fairness and\nefficiency of the ledger can break down at specific hash rates if miners have\ndiffering levels of connectivity to the P2P network sustaining the protocol.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:35:46 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Birmpas", "Georgios", ""], ["Koutsoupias", "Elias", ""], ["Lazos", "Philip", ""], ["Marmolejo-Coss\u00edo", "Francisco J.", ""]]}, {"id": "1910.02274", "submitter": "Vito Trianni", "authors": "Roman Miletitch, Andreagiovanni Reina, Marco Dorigo and Vito Trianni", "title": "Emergent naming of resources in a foraging robot swarm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the emergence of language convention within a swarm of robots\nforaging in an open environment from two identical resources. While foraging,\nthe swarm needs to explore and decide which resource to exploit, moving through\ncomplex transitory dynamics towards different possible equilibria, such as,\nselection of a single resource or spread across the two. Our point of interest\nis the understanding of possible correlations between the emergent, evolving,\ntask-induced interaction network and the language dynamics. In particular, our\ngoal is to determine whether the dynamics of the interaction network are\nsufficient to determine emergent naming conventions that represent features of\nthe task execution (e.g., choice of one or the other resource) and of the\nenvironment, In other words, we look for an emergent vocabulary that is both\ncomplete (a word for each resource) and correct (no misnomer) for as long as\neach resource is relevant to the swarm. In this study, robots are playing two\nvariants of the minimal language game. The classic one, where words are created\nwhen needed, and a new variant we introduce in this article: the spatial\nminimal naming game, where the creation of words is linked with the discovery\nof resources by exploring robots. We end the article by proposing a proof of\nconcept extension of the spatial minimal naming game that assures the\ncompleteness and correctness of the swarms vocabulary.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 14:18:50 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Miletitch", "Roman", ""], ["Reina", "Andreagiovanni", ""], ["Dorigo", "Marco", ""], ["Trianni", "Vito", ""]]}, {"id": "1910.02591", "submitter": "Ming Zhou", "authors": "Ming Zhou, Jiarui Jin, Weinan Zhang, Zhiwei Qin, Yan Jiao, Chenxi\n  Wang, Guobin Wu, Yong Yu, Jieping Ye", "title": "Multi-Agent Reinforcement Learning for Order-dispatching via\n  Order-Vehicle Distribution Matching", "comments": "9 pages,13 figures", "journal-ref": null, "doi": "10.1145/3357384.3357799", "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the efficiency of dispatching orders to vehicles is a research\nhotspot in online ride-hailing systems. Most of the existing solutions for\norder-dispatching are centralized controlling, which require to consider all\npossible matches between available orders and vehicles. For large-scale\nride-sharing platforms, there are thousands of vehicles and orders to be\nmatched at every second which is of very high computational cost. In this\npaper, we propose a decentralized execution order-dispatching method based on\nmulti-agent reinforcement learning to address the large-scale order-dispatching\nproblem. Different from the previous cooperative multi-agent reinforcement\nlearning algorithms, in our method, all agents work independently with the\nguidance from an evaluation of the joint policy since there is no need for\ncommunication or explicit cooperation between agents. Furthermore, we use\nKL-divergence optimization at each time step to speed up the learning process\nand to balance the vehicles (supply) and orders (demand). Experiments on both\nthe explanatory environment and real-world simulator show that the proposed\nmethod outperforms the baselines in terms of accumulated driver income (ADI)\nand Order Response Rate (ORR) in various traffic environments. Besides, with\nthe support of the online platform of Didi Chuxing, we designed a hybrid system\nto deploy our model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:32:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhou", "Ming", ""], ["Jin", "Jiarui", ""], ["Zhang", "Weinan", ""], ["Qin", "Zhiwei", ""], ["Jiao", "Yan", ""], ["Wang", "Chenxi", ""], ["Wu", "Guobin", ""], ["Yu", "Yong", ""], ["Ye", "Jieping", ""]]}, {"id": "1910.02607", "submitter": "Abeer Alshehri", "authors": "Abeer Alshehri (1 and 2) and Tim Miller and Liz Sonenberg ((1) School\n  of Computing and Information Systems, University of Melbourne, Victoria,\n  Australia (2) Department of Computer Science and Information Systems, King\n  Khalid University, Abha, Saudi Arabia)", "title": "Modeling Communication of Collaborative Multi-Agent System under\n  Epistemic Planning", "comments": "19 pages, 6 figures, 4 tables Submitted to International Journal of\n  Intelligent Systems", "journal-ref": "Int J Intell Syst. 2021; 1- 22", "doi": "10.1002/int.22536", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most multiagent applications, communication is essential among agents to\ncoordinate their actions, and thus achieve their goal. However, communication\noften has a related cost that affects overall system performance. In this\npaper, we draw inspiration from studies of epistemic planning to develop a\ncommunication model for agents that allows them to cooperate and make\ncommunication decisions effectively within a planning task. The proposed model\ntreats a communication process as an action that modifies the epistemic state\nof the team. In two simulated tasks, we evaluate whether agents can cooperate\neffectively and achieve higher performance using communication protocol modeled\nin our epistemic planning framework. Based on an empirical study conducted\nusing search and rescue tasks with different scenarios, our results show that\nthe proposed model improved team performance across all scenarios compared with\nbaseline models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 04:36:31 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 05:39:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Alshehri", "Abeer", "", "1 and 2"], ["Miller", "Tim", ""], ["Sonenberg", "Liz", ""]]}, {"id": "1910.02822", "submitter": "Pei Wang", "authors": "Pei Wang, Junqi Wang, Pushpi Paranamana, and Patrick Shafto", "title": "A mathematical theory of cooperative communication", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33 (NIPS 2030)", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative communication plays a central role in theories of human\ncognition, language, development, culture, and human-robot interaction. Prior\nmodels of cooperative communication are algorithmic in nature and do not shed\nlight on why cooperation may yield effective belief transmission and what\nlimitations may arise due to differences between beliefs of agents. Through a\nconnection to the theory of optimal transport, we establishing a mathematical\nframework for cooperative communication. We derive prior models as special\ncases, statistical interpretations of belief transfer plans, and proofs of\nrobustness and instability. Computational simulations support and elaborate our\ntheoretical results, and demonstrate fit to human behavior. The results show\nthat cooperative communication provably enables effective, robust belief\ntransmission which is required to explain feats of human learning and improve\nhuman-machine interaction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:35:22 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 20:26:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Pei", ""], ["Wang", "Junqi", ""], ["Paranamana", "Pushpi", ""], ["Shafto", "Patrick", ""]]}, {"id": "1910.03058", "submitter": "Kevin Corder", "authors": "Kevin Corder, Manuel M. Vindiola, Keith Decker", "title": "Decentralized Multi-Agent Actor-Critic with Generative Inference", "comments": "8 pages. Accepted to Deep Reinforcement Learning Workshop at NeurIPS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent multi-agent actor-critic methods have utilized centralized training\nwith decentralized execution to address the non-stationarity of co-adapting\nagents. This training paradigm constrains learning to the centralized phase\nsuch that only pre-learned policies may be used during the decentralized phase,\nwhich performs poorly when agent communications are delayed, noisy, or\ndisrupted. In this work, we propose a new system that can gracefully handle\npartially-observable information due to communication disruptions during\ndecentralized execution. Our approach augments the multi-agent actor-critic\nmethod's centralized training phase with generative modeling so that agents may\ninfer other agents' observations when provided with locally available context.\nOur method is evaluated on three tasks that require agents to combine local and\nremote observations communicated by other agents. We evaluate our approach by\nintroducing both partial observability during decentralized execution, and show\nthat decentralized training on inferred observations performs as well or better\nthan existing actor-critic methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 20:02:46 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Corder", "Kevin", ""], ["Vindiola", "Manuel M.", ""], ["Decker", "Keith", ""]]}, {"id": "1910.03094", "submitter": "Michael V Sullins", "authors": "Ian A. Kash, Michael Sullins, Katja Hofmann", "title": "Combining No-regret and Q-learning", "comments": "Presented as conference paper at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Regret Minimization (CFR) has found success in settings like\npoker which have both terminal states and perfect recall. We seek to understand\nhow to relax these requirements. As a first step, we introduce a simple\nalgorithm, local no-regret learning (LONR), which uses a Q-learning-like update\nrule to allow learning without terminal states or perfect recall. We prove its\nconvergence for the basic case of MDPs (and limited extensions of them) and\npresent empirical results showing that it achieves last iterate convergence in\na number of settings, most notably NoSDE games, a class of Markov games\nspecifically designed to be challenging to learn where no prior algorithm is\nknown to achieve convergence to a stationary equilibrium even on average.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:13:55 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:58:54 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Kash", "Ian A.", ""], ["Sullins", "Michael", ""], ["Hofmann", "Katja", ""]]}, {"id": "1910.03101", "submitter": "Clayton Mangette", "authors": "Clayton Mangette and Pratap Tokekar", "title": "Multi-Robot Coordinated Planning in Confined Environments under\n  Kinematic Constraints", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of multi-robot coordinated planning in\nenvironments where the robots may have to operate in close proximity to each\nother. We seek computationally efficient planners that ensure safe paths and\nadherence to kinematic constraints. We extend the central planner dRRT* with\nour variant, fast-dRRT (fdRRT), with the intention being to use in tight\nenvironments that lead to a high degree of coupling between robots. Our\nalgorithm is empirically shown to achieve the trade-off between computational\ntime and solution quality, especially in tight environments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:43:22 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mangette", "Clayton", ""], ["Tokekar", "Pratap", ""]]}, {"id": "1910.03640", "submitter": "Jaein Lim", "authors": "Jaein Lim, Panagiotis Tsiotras", "title": "MAMS-A*: Multi-Agent Multi-Scale A*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-scale forward search algorithm for distributed agents to\nsolve single-query shortest path planning problems. Each agent first builds a\nrepresentation of its own search space of the common environment as a\nmulti-resolution graph, it communicates with the other agents the result of its\nlocal search, and it uses received information from other agents to refine its\nown graph and update the local inconsistency conditions. As a result, all\nagents attain a common subgraph that includes a provably optimal path in the\nmost informative graph available among all agents, if one exists, without\nnecessarily communicating the entire graph. We prove the completeness and\noptimality of the proposed algorithm, and present numerical results supporting\nthe advantages of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:51:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Lim", "Jaein", ""], ["Tsiotras", "Panagiotis", ""]]}, {"id": "1910.03779", "submitter": "Juntao Wang Mr", "authors": "Juntao Wang, Yang Liu, Yiling Chen", "title": "Forecast Aggregation via Peer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a popular paradigm for soliciting forecasts on future\nevents. As people may have different forecasts, how to aggregate solicited\nforecasts into a single accurate prediction remains to be an important\nchallenge, especially when no historical accuracy information is available for\nidentifying experts. In this paper, we borrow ideas from the peer prediction\nliterature and assess the prediction accuracy of participants using solely the\ncollected forecasts. This approach leverages the correlations among peer\nreports to cross-validate each participant's forecasts and allows us to assign\na \"peer assessment score (PAS)\" for each agent as a proxy for the agent's\nprediction accuracy. We identify several empirically effective methods to\ngenerate PAS and propose an aggregation framework that uses PAS to identify\nexperts and to boost existing aggregators' prediction accuracy. We evaluate our\nmethods over 14 real-world datasets and show that i) PAS generated from peer\nprediction methods can approximately reflect the prediction accuracy of agents,\nand ii) our aggregation framework demonstrates consistent and significant\nimprovement in the prediction accuracy over existing aggregators for both\nbinary and multi-choice questions under three popular accuracy measures: Brier\nscore (mean square error), log score (cross-entropy loss) and AUC-ROC.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:07:13 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 00:27:10 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 19:39:40 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 05:03:42 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 07:28:11 GMT"}, {"version": "v6", "created": "Tue, 27 Apr 2021 21:11:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Juntao", ""], ["Liu", "Yang", ""], ["Chen", "Yiling", ""]]}, {"id": "1910.03964", "submitter": "Gerrit Gro{\\ss}mann", "authors": "Gerrit Gro{\\ss}mann, Luca Bortolussi, Verena Wolf", "title": "Rejection-Based Simulation of Non-Markovian Agents on Complex Networks", "comments": "14 pages including 2 pages Appendix and 2 pages references", "journal-ref": null, "doi": "10.1371/journal.pone.0241394", "report-no": null, "categories": "cs.SI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models in which agents interact with their neighborhood according\nto a network topology are a powerful modeling framework to study the emergence\nof complex dynamic patterns in real-world systems. Stochastic simulations are\noften the preferred - sometimes the only feasible - way to investigate such\nsystems. Previous research focused primarily on Markovian models where the\nrandom time until an interaction happens follows an exponential distribution.\nIn this work, we study a general framework to model systems where each agent is\nin one of several states. Agents can change their state at random, influenced\nby their complete neighborhood, while the time to the next event can follow an\narbitrary probability distribution. Classically, these simulations are hindered\nby high computational costs of updating the rates of interconnected agents and\nsampling the random residence times from arbitrary distributions. We propose a\nrejection-based, event-driven simulation algorithm to overcome these\nlimitations. Our method over-approximates the instantaneous rates corresponding\nto inter-event times while rejection events counterbalance these\nover-approximations. We demonstrate the effectiveness of our approach on models\nof epidemic and information spreading.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 13:06:58 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gro\u00dfmann", "Gerrit", ""], ["Bortolussi", "Luca", ""], ["Wolf", "Verena", ""]]}, {"id": "1910.04041", "submitter": "Ramy E. Ali", "authors": "Ramy E. Ali, Bilgehan Erman, Ejder Ba\\c{s}tu\\u{g} and Bruce Cilli", "title": "Hierarchical Deep Double Q-Routing", "comments": "IEEE ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT cs.LG cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a deep reinforcement learning approach applied to the\npacket routing problem with high-dimensional constraints instigated by dynamic\nand autonomous communication networks. Our approach is motivated by the fact\nthat centralized path calculation approaches are often not scalable, whereas\nthe distributed approaches with locally acting nodes are not fully aware of the\nend-to-end performance. We instead hierarchically distribute the path\ncalculation over designated nodes in the network while taking into account the\nend-to-end performance. Specifically, we develop a hierarchical\ncluster-oriented adaptive per-flow path calculation mechanism by leveraging the\nDeep Double Q-network (DDQN) algorithm, where the end-to-end paths are\ncalculated by the source nodes with the assistance of cluster (group) leaders\nat different hierarchical levels. In our approach, a deferred composite reward\nis designed to capture the end-to-end performance through a feedback signal\nfrom the source nodes to the group leaders and captures the local network\nperformance through the local resource assessments by the group leaders. This\napproach scales in large networks, adapts to the dynamic demand, utilizes the\nnetwork resources efficiently and can be applied to segment routing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:03:07 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:15:59 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 20:41:06 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Ali", "Ramy E.", ""], ["Erman", "Bilgehan", ""], ["Ba\u015ftu\u011f", "Ejder", ""], ["Cilli", "Bruce", ""]]}, {"id": "1910.04250", "submitter": "Terrence W.K. Mak", "authors": "Terrence W.K. Mak, Ferdinando Fioretto, Pascal Van Hentenryck", "title": "Privacy-Preserving Obfuscation for Distributed Power Systems", "comments": "Total 10 pages: main body 8 pages, reference 1 page, appendix 2 pages", "journal-ref": "IEEE Transactions on Power Systems ( Volume: 35 , Issue: 2 , March\n  2020 )", "doi": "10.1109/TPWRS.2019.2945069", "report-no": null, "categories": "math.OC cs.CR cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of releasing privacy-preserving load data of\na decentralized operated power system. The paper focuses on data used to solve\nOptimal Power Flow (OPF) problems and proposes a distributed algorithm that\ncomplies with the notion of Differential Privacy, a strong privacy framework\nused to bound the risk of re-identification. The problem is challenging since\nthe application of traditional differential privacy mechanisms to the load data\nfundamentally changes the nature of the underlying optimization problem and\noften leads to severe feasibility issues. The proposed differentially private\ndistributed algorithm is based on the Alternating Direction Method of\nMultipliers (ADMM) and guarantees that the released privacy-preserving data\nretains high fidelity and satisfies the AC power flow constraints. Experimental\nresults on a variety of OPF benchmarks demonstrate the effectiveness of the\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:45:34 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Mak", "Terrence W. K.", ""], ["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1910.04337", "submitter": "Francesco Zanlungo Dr.", "authors": "Francesco Zanlungo, Luca Crociani, Zeynep Y\\\"ucel and Takayuki Kanda", "title": "The effect of social groups on the dynamics of bi-directional pedestrian\n  flow: a numerical study", "comments": "Presented at TGF 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effect of groups on a bi-directional flow, by using novel\ncomputational methods. Our focus is on self-organisation phenomena, and more\nspecifically on the time needed for the occurrence of pedestrian lanes, their\nstability and their effect on the velocity-density relation. Moreover, we are\ninterested in understanding the amount of physical contact in the crowd. To\nthis end, we use a novel model considering the asymmetrical shape of the human\nbody and describing its rotation during collision avoidance, and combine it to\na mathematical model of group behaviour. We configure several scenarios by\nvarying the global density $\\rho$ of pedestrians and the ratio $r_g$ describing\nthe percentage of grouped pedestrians in the simulation. Our results show that\nthe presence of groups has a significant effect on velocity and lane\norganisation, and a dramatic one on collision. We are well aware of the\nlimitations of our approach, in particular concerning (i) the lack of\ncalibration of body rotation in collision avoidance on actual data and (ii)\nstraightforward application of a low density group model to higher density\nsettings. We nevertheless want to stress that it is not our intention to state\nthat our results reproduce the actual effect of groups on bi-directional flow.\nIn particular, it seems highly unrealistic that crowds with groups collide\nextremely more often. Nevertheless we believe that our results show the great\ntheoretical and practical implication of the consideration of realistic group\nbehaviour in pedestrian models, and suggest that realistic results may hardly\nbe achieved simply by adding together modular models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:43:30 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Zanlungo", "Francesco", ""], ["Crociani", "Luca", ""], ["Y\u00fccel", "Zeynep", ""], ["Kanda", "Takayuki", ""]]}, {"id": "1910.04537", "submitter": "Arpit Garg", "authors": "Arpit Garg, Yazied A. Hasan, Adam Ya\\~nez and Lydia Tapia", "title": "Defensive Escort Teams via Multi-Agent Deep Reinforcement Learning", "comments": "IEEE Robotics and Automation Letters with International Conference on\n  Robotics and Automation (ICRA) option, 2020, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinated defensive escorts can aid a navigating payload by positioning\nthemselves in order to maintain the safety of the payload from obstacles. In\nthis paper, we present a novel, end-to-end solution for coordinating an escort\nteam for protecting high-value payloads. Our solution employs deep\nreinforcement learning (RL) in order to train a team of escorts to maintain\npayload safety while navigating alongside the payload. This is done in a\ndistributed fashion, relying only on limited range positional information of\nother escorts, the payload, and the obstacles. When compared to a state-of-art\nalgorithm for obstacle avoidance, our solution with a single escort increases\nnavigation success up to 31%. Additionally, escort teams increase success rate\nby up to 75% percent over escorts in static formations. We also show that this\nlearned solution is general to several adaptations in the scenario including: a\nchanging number of escorts in the team, changing obstacle density, and changes\nin payload conformation. Video: https://youtu.be/SoYesKti4VA.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:57:49 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Garg", "Arpit", ""], ["Hasan", "Yazied A.", ""], ["Ya\u00f1ez", "Adam", ""], ["Tapia", "Lydia", ""]]}, {"id": "1910.04600", "submitter": "Martin Helfrich", "authors": "Michael Blondin, Javier Esparza, Blaise Genest, Martin Helfrich and\n  Stefan Jaax", "title": "Succinct Population Protocols for Presburger Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angluin et al. proved that population protocols compute exactly the\npredicates definable in Presburger arithmetic (PA), the first-order theory of\naddition. As part of this result, they presented a procedure that translates\nany formula $\\varphi$ of quantifier-free PA with remainder predicates (which\nhas the same expressive power as full PA) into a population protocol with\n$2^{O(\\text{poly}(|\\varphi|))}$ states that computes $\\varphi$. More precisely,\nthe number of states of the protocol is exponential in both the bit length of\nthe largest coefficient in the formula, and the number of nodes of its syntax\ntree.\n  In this paper, we prove that every formula $\\varphi$ of quantifier-free PA\nwith remainder predicates is computable by a leaderless population protocol\nwith $O(\\text{poly}(|\\varphi|))$ states. Our proof is based on several new\nconstructions, which may be of independent interest. Given a formula $\\varphi$\nof quantifier-free PA with remainder predicates, a first construction produces\na succinct protocol (with $O(|\\varphi|^3)$ leaders) that computes $\\varphi$;\nthis completes the work initiated in [STACS'18], where we constructed such\nprotocols for a fragment of PA. For large enough inputs, we can get rid of\nthese leaders. If the input is not large enough, then it is small, and we\ndesign another construction producing a succinct protocol with one leader that\ncomputes $\\varphi$. Our last construction gets rid of this leader for small\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:28:13 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:07:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Genest", "Blaise", ""], ["Helfrich", "Martin", ""], ["Jaax", "Stefan", ""]]}, {"id": "1910.05092", "submitter": "B. Mert Albaba", "authors": "Mert Albaba, Yildiray Yildiz", "title": "Modeling Cyber-Physical Human Systems via an Interplay Between\n  Reinforcement Learning and Game Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the outcomes of cyber-physical systems with multiple human\ninteractions is a challenging problem. This article reviews a game theoretical\napproach to address this issue, where reinforcement learning is employed to\npredict the time-extended interaction dynamics. We explain that the most\nattractive feature of the method is proposing a computationally feasible\napproach to simultaneously model multiple humans as decision makers, instead of\ndetermining the decision dynamics of the intelligent agent of interest and\nforcing the others to obey certain kinematic and dynamic constraints imposed by\nthe environment. We present two recent exploitations of the method to model 1)\nunmanned aircraft integration into the National Airspace System and 2) highway\ntraffic. We conclude the article by providing ongoing and future work about\nemploying, improving and validating the method. We also provide related open\nproblems and research opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:41:03 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Albaba", "Mert", ""], ["Yildiz", "Yildiray", ""]]}, {"id": "1910.05424", "submitter": "James Watson", "authors": "James R. Watson and A. John Woodill", "title": "Anticipating Illegal Maritime Activities from Anomalous Multiscale Fleet\n  Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illegal fishing is prevalent throughout the world and heavily impacts the\nhealth of our oceans, the sustainability and profitability of fisheries, and\neven acts to destabilize geopolitical relations. To achieve the United Nations'\nSustainable Development Goal of \"Life Below Water\", our ability to detect and\npredict illegal fishing must improve. Recent advances have been made through\nthe use of vessel location data, however, most analyses to date focus on\nanomalous spatial behaviors of vessels one at a time. To improve predictions,\nwe develop a method inspired by complex systems theory to monitor the anomalous\nmulti-scale behavior of whole fleets as they respond to nearby illegal\nactivities. Specifically, we analyze changes in the multiscale geospatial\norganization of fishing fleets operating on the Patagonia Shelf, an important\nfishing region with chronic exposure to illegal fishing. We show that legally\noperating (and visible) vessels respond anomalously to nearby illegal\nactivities (by vessels that are difficult to detect). Indeed, precursor\nbehaviors are identified, suggesting a path towards pre-empting illegal\nactivities. This approach offers a promising step towards a global system for\ndetecting, predicting and deterring illegal activities at sea in near\nreal-time. Doing so will be a big step forward to achieving sustainable life\nunderwater.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:46:09 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Watson", "James R.", ""], ["Woodill", "A. John", ""]]}, {"id": "1910.05512", "submitter": "Tonghan Wang", "authors": "Tonghan Wang, Jianhao Wang, Yi Wu, Chongjie Zhang", "title": "Influence-Based Multi-Agent Exploration", "comments": null, "journal-ref": "International Conference on Learning Representations, 2020,\n  spotlight", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically motivated reinforcement learning aims to address the\nexploration challenge for sparse-reward tasks. However, the study of\nexploration methods in transition-dependent multi-agent settings is largely\nabsent from the literature. We aim to take a step towards solving this problem.\nWe present two exploration methods: exploration via information-theoretic\ninfluence (EITI) and exploration via decision-theoretic influence (EDTI), by\nexploiting the role of interaction in coordinated behaviors of agents. EITI\nuses mutual information to capture influence transition dynamics. EDTI uses a\nnovel intrinsic reward, called Value of Interaction (VoI), to characterize and\nquantify the influence of one agent's behavior on expected returns of other\nagents. By optimizing EITI or EDTI objective as a regularizer, agents are\nencouraged to coordinate their exploration and learn policies to optimize team\nperformance. We show how to optimize these regularizers so that they can be\neasily integrated with policy gradient reinforcement learning. The resulting\nupdate rule draws a connection between coordinated exploration and intrinsic\nreward distribution. Finally, we empirically demonstrate the significant\nstrength of our method in a variety of multi-agent scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 07:14:33 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Tonghan", ""], ["Wang", "Jianhao", ""], ["Wu", "Yi", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1910.05599", "submitter": "Sayan Mitra", "authors": "Peter Du, Zhe Huang, Tianqi Liu, Ke Xu, Qichao Gao, Hussein Sibai,\n  Katherine Driggs-Campbell, Sayan Mitra", "title": "Online monitoring for safe pedestrian-vehicle interactions", "comments": "15 pages, 5 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous systems begin to operate amongst humans, methods for safe\ninteraction must be investigated. We consider an example of a small autonomous\nvehicle in a pedestrian zone that must safely maneuver around people in a\nfree-form fashion. We investigate two key questions: How can we effectively\nintegrate pedestrian intent estimation into our autonomous stack. Can we\ndevelop an online monitoring framework to give formal guarantees on the safety\nof such human-robot interactions. We present a pedestrian intent estimation\nframework that can accurately predict future pedestrian trajectories given\nmultiple possible goal locations. We integrate this into a reachability-based\nonline monitoring scheme that formally assesses the safety of these\ninteractions with nearly real-time performance (approximately 0.3 seconds).\nThese techniques are integrated on a test vehicle with a complete in-house\nautonomous stack, demonstrating effective and safe interaction in real-world\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 17:08:07 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 22:29:52 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Du", "Peter", ""], ["Huang", "Zhe", ""], ["Liu", "Tianqi", ""], ["Xu", "Ke", ""], ["Gao", "Qichao", ""], ["Sibai", "Hussein", ""], ["Driggs-Campbell", "Katherine", ""], ["Mitra", "Sayan", ""]]}, {"id": "1910.06044", "submitter": "Lixu Wang", "authors": "Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu", "title": "Eavesdrop the Composition Proportion of Training Labels in Federated\n  Learning", "comments": "15 pages, 10 figures, security conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as a new form of collaborative\nmachine learning, where a common model can be learned while keeping all the\ntraining data on local devices. Although it is designed for enhancing the data\nprivacy, we demonstrated in this paper a new direction in inference attacks in\nthe context of FL, where valuable information about training data can be\nobtained by adversaries with very limited power. In particular, we proposed\nthree new types of attacks to exploit this vulnerability. The first type of\nattack, Class Sniffing, can detect whether a certain label appears in training.\nThe other two types of attacks can determine the quantity of each label, i.e.,\nQuantity Inference attack determines the composition proportion of the training\nlabel owned by the selected clients in a single round, while Whole\nDetermination attack determines that of the whole training process. We\nevaluated our attacks on a variety of tasks and datasets with different\nsettings, and the corresponding results showed that our attacks work well\ngenerally. Finally, we analyzed the impact of major hyper-parameters to our\nattacks and discussed possible defenses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 11:26:10 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 03:21:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Lixu", ""], ["Xu", "Shichao", ""], ["Wang", "Xiao", ""], ["Zhu", "Qi", ""]]}, {"id": "1910.06079", "submitter": "Tomek Korbak", "authors": "Tomasz Korbak and Julian Zubek and {\\L}ukasz Kuci\\'nski and Piotr\n  Mi{\\l}o\\'s and Joanna R\\k{a}czaszek-Leonardi", "title": "Developmentally motivated emergence of compositional communication via\n  template transfer", "comments": "Accepted for NeurIPS 2019 workshop Emergent Communication: Towards\n  Natural Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a novel approach to achieving emergent compositional\ncommunication in multi-agent systems. We propose a training regime implementing\ntemplate transfer, the idea of carrying over learned biases across contexts. In\nour method, a sender-receiver pair is first trained with disentangled loss\nfunctions and then the receiver is transferred to train a new sender with a\nstandard loss. Unlike other methods (e.g. the obverter algorithm), our approach\ndoes not require imposing inductive biases on the architecture of the agents.\nWe experimentally show the emergence of compositional communication using\ntopographical similarity, zero-shot generalization and context independence as\nevaluation metrics. The presented approach is connected to an important line of\nwork in semiotics and developmental psycholinguistics: it supports a conjecture\nthat compositional communication is scaffolded on simpler communication\nprotocols.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:04:53 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Korbak", "Tomasz", ""], ["Zubek", "Julian", ""], ["Kuci\u0144ski", "\u0141ukasz", ""], ["Mi\u0142o\u015b", "Piotr", ""], ["R\u0105czaszek-Leonardi", "Joanna", ""]]}, {"id": "1910.06412", "submitter": "Chris Taylor", "authors": "Chris Taylor and Cameron Nowzari", "title": "The impact of catastrophic collisions and collision avoidance on a\n  swarming behavior", "comments": "Current submission to RAS, to appear", "journal-ref": null, "doi": "10.1016/j.robot.2021.103754", "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarms of autonomous agents are useful in many applications due to their\nability to accomplish tasks in a decentralized manner, making them more robust\nto failures. Due to the difficulty in running experiments with large numbers of\nhardware agents, researchers often make simplifying assumptions and remove\nconstraints that might be present in a real swarm deployment. While simplifying\naway some constraints is tolerable, we feel that two in particular have been\noverlooked: one, that agents in a swarm take up physical space, and two, that\nagents might be damaged in collisions. Many existing works assume agents have\nnegligible size or pass through each other with no added penalty. It seems\npossible to ignore these constraints using collision avoidance, but we show\nusing an illustrative example that this is easier said than done. In\nparticular, we show that collision avoidance can interfere with the intended\nswarming behavior and significant parameter tuning is necessary to ensure the\nbehavior emerges as best as possible while collisions are avoided. We compare\nfour different collision avoidance algorithms, two of which we consider to be\nthe best decentralized collision avoidance algorithms available. Despite\nputting significant effort into tuning each algorithm to perform at its best,\nwe believe our results show that further research is necessary to develop\nswarming behaviors that can achieve their goal while avoiding collisions with\nagents of non-negligible volume.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:33:55 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 14:27:01 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 21:13:36 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 14:27:05 GMT"}, {"version": "v5", "created": "Wed, 3 Mar 2021 14:36:41 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Taylor", "Chris", ""], ["Nowzari", "Cameron", ""]]}, {"id": "1910.06561", "submitter": "Andr\\'e Ferrari", "authors": "Andr\\'e Ferrari and C\\'edric Richard and Louis Verduci", "title": "Distributed Change Detection in Streaming Graph Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting abrupt changes in streaming graph signals is relevant in a variety\nof applications ranging from energy and water supplies, to environmental\nmonitoring. In this paper, we address this problem when anomalies activate\nlocalized groups of nodes in a network. We introduce an online change-point\ndetection algorithm, which is fully distributed across nodes to monitor\nlarge-scale dynamic networks. We analyze the detection statistics for\ncontrolling the probability of a global type 1 error. Finally we illustrate the\ndetection and localization performance with simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 06:58:57 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ferrari", "Andr\u00e9", ""], ["Richard", "C\u00e9dric", ""], ["Verduci", "Louis", ""]]}, {"id": "1910.07298", "submitter": "Pavel Naumov", "authors": "Pavel Naumov and Yuan Yuan", "title": "Intelligence in Strategic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LO cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article considers strategies of coalitions that are based on intelligence\ninformation about moves of some of the other agents. The main technical result\nis a sound and complete logical system that describes the interplay between\ncoalition power modality with intelligence and distributed knowledge modality\nin games with imperfect information.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:42:50 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Naumov", "Pavel", ""], ["Yuan", "Yuan", ""]]}, {"id": "1910.07498", "submitter": "Zuyue Fu", "authors": "Zuyue Fu, Zhuoran Yang, Yongxin Chen, Zhaoran Wang", "title": "Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic\n  Mean-Field Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study discrete-time mean-field Markov games with infinite numbers of\nagents where each agent aims to minimize its ergodic cost. We consider the\nsetting where the agents have identical linear state transitions and quadratic\ncost functions, while the aggregated effect of the agents is captured by the\npopulation mean of their states, namely, the mean-field state. For such a game,\nbased on the Nash certainty equivalence principle, we provide sufficient\nconditions for the existence and uniqueness of its Nash equilibrium. Moreover,\nto find the Nash equilibrium, we propose a mean-field actor-critic algorithm\nwith linear function approximation, which does not require knowing the model of\ndynamics. Specifically, at each iteration of our algorithm, we use the\nsingle-agent actor-critic algorithm to approximately obtain the optimal policy\nof the each agent given the current mean-field state, and then update the\nmean-field state. In particular, we prove that our algorithm converges to the\nNash equilibrium at a linear rate. To the best of our knowledge, this is the\nfirst success of applying model-free reinforcement learning with function\napproximation to discrete-time mean-field Markov games with provable\nnon-asymptotic global convergence guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:59:20 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Fu", "Zuyue", ""], ["Yang", "Zhuoran", ""], ["Chen", "Yongxin", ""], ["Wang", "Zhaoran", ""]]}, {"id": "1910.07780", "submitter": "Sagar Verma", "authors": "Sagar Verma and Richa Verma and P.B. Sujit", "title": "MAPEL: Multi-Agent Pursuer-Evader Learning using Situation Report", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider a territory guarding game involving pursuers,\nevaders and a target in an environment that contains obstacles. The goal of the\nevaders is to capture the target, while that of the pursuers is to capture the\nevaders before they reach the target. All the agents have limited sensing range\nand can only detect each other when they are in their observation space. We\nfocus on the challenge of effective cooperation between agents of a team.\nFinding exact solutions for such multi-agent systems is difficult because of\nthe inherent complexity. We present Multi-Agent Pursuer-Evader Learning\n(MAPEL), a class of algorithms that use spatio-temporal graph representation to\nlearn structured cooperation. The key concept is that the learning takes place\nin a decentralized manner and agents use situation report updates to learn\nabout the whole environment from each others' partial observations. We use\nRecurrent Neural Networks (RNNs) to parameterize the spatio-temporal graph. An\nagent in MAPEL only updates all the other agents if an opponent or the target\nis inside its observation space by using situation report. We present two\nmethods for cooperation via situation report update: a) Peer-to-Peer Situation\nReport (P2PSR) and b) Ring Situation Report (RSR). We present a detailed\nanalysis of how these two cooperation methods perform when the number of agents\nin the game are increased. We provide empirical results to show how agents\ncooperate under these two methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:16:11 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Verma", "Sagar", ""], ["Verma", "Richa", ""], ["Sujit", "P. B.", ""]]}, {"id": "1910.07882", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Shuran Song, Hod Lipson, Carl Vondrick", "title": "Visual Hide and Seek", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train embodied agents to play Visual Hide and Seek where a prey must\nnavigate in a simulated environment in order to avoid capture from a predator.\nWe place a variety of obstacles in the environment for the prey to hide behind,\nand we only give the agents partial observations of their environment using an\negocentric perspective. Although we train the model to play this game from\nscratch, experiments and visualizations suggest that the agent learns to\npredict its own visibility in the environment. Furthermore, we quantitatively\nanalyze how agent weaknesses, such as slower speed, effect the learned policy.\nOur results suggest that, although agent weaknesses make the learning problem\nmore challenging, they also cause more useful features to be learned. Our\nproject website is available at: http://www.cs.columbia.edu/\n~bchen/visualhideseek/.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 01:27:09 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Chen", "Boyuan", ""], ["Song", "Shuran", ""], ["Lipson", "Hod", ""], ["Vondrick", "Carl", ""]]}, {"id": "1910.07999", "submitter": "Ramya Akula", "authors": "Ramya Akula, Niloofar Yousefi and Ivan Garibay", "title": "DeepFork: Supervised Prediction of Information Diffusion in GitHub", "comments": "12 Pages, 7 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information spreads on complex social networks extremely fast, in other\nwords, a piece of information can go viral within no time. Often it is hard to\nbarricade this diffusion prior to the significant occurrence of chaos, be it a\nsocial media or an online coding platform. GitHub is one such trending online\nfocal point for any business to reach their potential contributors and\ncustomers, simultaneously. By exploiting such software development paradigm,\nmillions of free software emerged lately in diverse communities. To understand\nhuman influence, information spread and evolution of transmitted information\namong assorted users in GitHub, we developed a deep neural network model:\nDeepFork, a supervised machine learning based approach that aims to predict\ninformation diffusion in complex social networks; considering node as well as\ntopological features. In our empirical studies, we observed that information\ndiffusion can be detected by link prediction using supervised learning.\nDeepFork outperforms other machine learning models as it better learns the\ndiscriminative patterns from the input features. DeepFork aids in understanding\ninformation spread and evolution through a bipartite network of users and\nrepositories i.e., information flow from a user to repository to user.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:18:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Akula", "Ramya", ""], ["Yousefi", "Niloofar", ""], ["Garibay", "Ivan", ""]]}, {"id": "1910.08809", "submitter": "Nicolas Carion", "authors": "Nicolas Carion, Gabriel Synnaeve, Alessandro Lazaric, Nicolas Usunier", "title": "A Structured Prediction Approach for Generalization in Cooperative\n  Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective coordination is crucial to solve multi-agent collaborative (MAC)\nproblems. While centralized reinforcement learning methods can optimally solve\nsmall MAC instances, they do not scale to large problems and they fail to\ngeneralize to scenarios different from those seen during training. In this\npaper, we consider MAC problems with some intrinsic notion of locality (e.g.,\ngeographic proximity) such that interactions between agents and tasks are\nlocally limited. By leveraging this property, we introduce a novel structured\nprediction approach to assign agents to tasks. At each step, the assignment is\nobtained by solving a centralized optimization problem (the inference\nprocedure) whose objective function is parameterized by a learned scoring\nmodel. We propose different combinations of inference procedures and scoring\nmodels able to represent coordination patterns of increasing complexity. The\nresulting assignment policy can be efficiently learned on small problem\ninstances and readily reused in problems with more agents and tasks (i.e.,\nzero-shot generalization). We report experimental results on a toy search and\nrescue problem and on several target selection scenarios in StarCraft: Brood\nWar, in which our model significantly outperforms strong rule-based baselines\non instances with 5 times more agents and tasks than those seen during\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 17:30:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Carion", "Nicolas", ""], ["Synnaeve", "Gabriel", ""], ["Lazaric", "Alessandro", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1910.08841", "submitter": "Yuan Chen", "authors": "Yuan Chen, Soummya Kar, and Jos\\'e M. F. Moura", "title": "Resilient Distributed Recovery of Large Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the resilient distributed recovery of large fields under\nmeasurement attacks, by a team of agents, where each measures a small subset of\nthe components of a large spatially distributed field. An adversary corrupts\nsome of the measurements. The agents collaborate to process their measurements,\nand each is interested in recovering only a fraction of the field. We present a\nfield recovery consensus+innovations type distributed algorithm that is\nresilient to measurement attacks, where an agent maintains and updates a local\nstate based on its neighbors states and its own measurement. Under sufficient\nconditions on the attacker and the connectivity of the communication network,\neach agent's state, even those with compromised measurements, converges to the\ntrue value of the field components that it is interested in recovering.\nFinally, we illustrate the performance of our algorithm through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 20:46:36 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chen", "Yuan", ""], ["Kar", "Soummya", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1910.08942", "submitter": "Leonardo Andr\\'es Espinosa Leal EspinosaLeal", "authors": "Leonardo A. Espinosa Leal, Magnus Westerlund, Anthony Chapman", "title": "Autonomous Industrial Management via Reinforcement Learning:\n  Self-Learning Agents for Decision-Making -- A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry has always been in the pursuit of becoming more economically\nefficient and the current focus has been to reduce human labour using modern\ntechnologies. Even with cutting edge technologies, which range from packaging\nrobots to AI for fault detection, there is still some ambiguity on the aims of\nsome new systems, namely, whether they are automated or autonomous. In this\npaper we indicate the distinctions between automated and autonomous system as\nwell as review the current literature and identify the core challenges for\ncreating learning mechanisms of autonomous agents. We discuss using different\ntypes of extended realities, such as digital twins, to train reinforcement\nlearning agents to learn specific tasks through generalization. Once\ngeneralization is achieved, we discuss how these can be used to develop\nself-learning agents. We then introduce self-play scenarios and how they can be\nused to teach self-learning agents through a supportive environment which\nfocuses on how the agents can adapt to different real-world environments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 10:10:21 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Leal", "Leonardo A. Espinosa", ""], ["Westerlund", "Magnus", ""], ["Chapman", "Anthony", ""]]}, {"id": "1910.09314", "submitter": "Ezra Tampubolon", "authors": "Ezra Tampubolon and Holger Boche", "title": "Pricing Mechanism for Resource Sustainability in Competitive Online\n  Learning Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA cs.SY econ.TH eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of resource congestion control for\ncompeting online learning agents. On the basis of non-cooperative game as the\nmodel for the interaction between the agents, and the noisy online mirror\nascent as the model for rational behavior of the agents, we propose a novel\npricing mechanism which gives the agents incentives for sustainable use of the\nresources. Our mechanism is distributed and resource-centric, in the sense that\nit is done by the resources themselves and not by a centralized instance, and\nthat it is based rather on the congestion state of the resources than the\npreferences of the agents. In case that the noise is persistent, and for\nseveral choices of the intrinsic parameter of the agents, such as their\nlearning rate, and of the mechanism parameters, such as the learning rate of -,\nthe progressivity of the price-setters, and the extrinsic price sensitivity of\nthe agents, we show that the accumulative violation of the resource constraints\nof the resulted iterates is sub-linear w.r.t. the time horizon. Moreover, we\nprovide numerical simulations to support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:49:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Tampubolon", "Ezra", ""], ["Boche", "Holger", ""]]}, {"id": "1910.09441", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Tingxiang Fan, Jia Pan, Dinesh Manocha", "title": "DeepMNavigate: Deep Reinforced Multi-Robot Navigation Unifying Local &\n  Global Collision Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm (DeepMNavigate) for global multi-agent\nnavigation in dense scenarios using deep reinforcement learning (DRL). Our\napproach uses local and global information for each robot from motion\ninformation maps. We use a three-layer CNN that takes these maps as input to\ngenerate a suitable action to drive each robot to its goal position. Our\napproach is general, learns an optimal policy using a multi-scenario,\nmulti-state training algorithm, and can directly handle raw sensor measurements\nfor local observations. We demonstrate the performance on dense, complex\nbenchmarks with narrow passages and environments with tens of agents. We\nhighlight the algorithm's benefits over prior learning methods and geometric\ndecentralized algorithms in complex scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:20:36 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 16:01:07 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 05:34:47 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 04:03:06 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 23:02:16 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Tan", "Qingyang", ""], ["Fan", "Tingxiang", ""], ["Pan", "Jia", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1910.09442", "submitter": "Daniel Tang", "authors": "Daniel Tang", "title": "Data assimilation in Agent-based models using creation and annihilation\n  operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Agent-based models are a powerful tool for studying the behaviour of complex\nsystems that can be described in terms of multiple, interacting ``agents''.\nHowever, because of their inherently discrete and often highly non-linear\nnature, it is very difficult to reason about the relationship between the state\nof the model, on the one hand, and our observations of the real world on the\nother. In this paper we consider agents that have a discrete set of states\nthat, at any instant, act with a probability that may depend on the environment\nor the state of other agents. Given this, we show how the mathematical\napparatus of quantum field theory can be used to reason probabilistically about\nthe state and dynamics the model, and describe an algorithm to update our\nbelief in the state of the model in the light of new, real-world observations.\nUsing a simple predator-prey model on a 2-dimensional spatial grid as an\nexample, we demonstrate the assimilation of incomplete, noisy observations and\nshow that this leads to an increase in the mutual information between the\nactual state of the observed system and the posterior distribution given the\nobservations, when compared to a null model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:15:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Tang", "Daniel", ""]]}, {"id": "1910.09508", "submitter": "Dongge Han", "authors": "Dongge Han, Wendelin Boehmer, Michael Wooldridge, Alex Rogers", "title": "Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination", "comments": "PRICAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-29911-8_7", "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-agent system, an agent's optimal policy will typically depend on\nthe policies chosen by others. Therefore, a key issue in multi-agent systems\nresearch is that of predicting the behaviours of others, and responding\npromptly to changes in such behaviours. One obvious possibility is for each\nagent to broadcast their current intention, for example, the currently executed\noption in a hierarchical reinforcement learning framework. However, this\napproach results in inflexibility of agents if options have an extended\nduration and are dynamic. While adjusting the executed option at each step\nimproves flexibility from a single-agent perspective, frequent changes in\noptions can induce inconsistency between an agent's actual behaviour and its\nbroadcast intention. In order to balance flexibility and predictability, we\npropose a dynamic termination Bellman equation that allows the agents to\nflexibly terminate their options. We evaluate our model empirically on a set of\nmulti-agent pursuit and taxi tasks, and show that our agents learn to adapt\nflexibly across scenarios that require different termination behaviours.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:54:49 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Han", "Dongge", ""], ["Boehmer", "Wendelin", ""], ["Wooldridge", "Michael", ""], ["Rogers", "Alex", ""]]}, {"id": "1910.09587", "submitter": "Brian Swenson", "authors": "Brian Swenson, Anirudh Sridhar, H. Vincent Poor", "title": "On Distributed Stochastic Gradient Algorithms for Global Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of network-based computation of global minima\nin smooth nonconvex optimization problems. It is known that distributed\ngradient-descent-type algorithms can achieve convergence to the set of global\nminima by adding slowly decaying Gaussian noise in order escape local minima.\nHowever, the technical assumptions under which convergence is known to occur\ncan be restrictive in practice. In particular, in known convergence results,\nthe local objective functions possessed by agents are required to satisfy a\nhighly restrictive bounded-gradient-dissimilarity condition. The paper\ndemonstrates convergence to the set of global minima while relaxing this key\nassumption.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:25:36 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 17:45:35 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Swenson", "Brian", ""], ["Sridhar", "Anirudh", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1910.09721", "submitter": "Samuel Alexander", "authors": "Samuel Allen Alexander", "title": "Intelligence via ultrafilters: structural properties of some\n  intelligence comparators of deterministic Legg-Hutter agents", "comments": "22 pages", "journal-ref": "Journal of Artificial General Intelligence 10(1) 24--45, 2019", "doi": "10.2478/jagi-2019-0003", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Legg and Hutter, as well as subsequent authors, considered intelligent agents\nthrough the lens of interaction with reward-giving environments, attempting to\nassign numeric intelligence measures to such agents, with the guiding principle\nthat a more intelligent agent should gain higher rewards from environments in\nsome aggregate sense. In this paper, we consider a related question: rather\nthan measure numeric intelligence of one Legg- Hutter agent, how can we compare\nthe relative intelligence of two Legg-Hutter agents? We propose an elegant\nanswer based on the following insight: we can view Legg-Hutter agents as\ncandidates in an election, whose voters are environments, letting each\nenvironment vote (via its rewards) which agent (if either) is more intelligent.\nThis leads to an abstract family of comparators simple enough that we can prove\nsome structural theorems about them. It is an open question whether these\nstructural theorems apply to more practical intelligence measures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 01:50:20 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 19:09:54 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Alexander", "Samuel Allen", ""]]}, {"id": "1910.10109", "submitter": "Mhadi Shamsi", "authors": "Mahdi Shamsi, Alireza Moslemi Haghighi, Farokh Marvasti", "title": "Distributed interference cancellation in multi-agent scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.RO cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detecting impaired and noisy nodes over\nnetwork. In a distributed algorithm, lots of processing units are incorporating\nand communicating with each other to reach a global goal. Due to each one's\nstate in the shared environment, they can help the other nodes or mislead them\n(due to noise or a deliberate attempt). Previous works mainly focused on proper\nlocating agents and weight assignment based on initial environment state to\nminimize malfunctioning of noisy nodes. We propose an algorithm to be able to\nadapt sharing weights according to behavior of the agents. Applying the\nintroduced algorithm to a multi-agent RL scenario and the well-known diffusion\nLMS demonstrates its capability and generality.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:53:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Shamsi", "Mahdi", ""], ["Haghighi", "Alireza Moslemi", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1910.10142", "submitter": "Ao Li", "authors": "Ao Li, Liting Sun, Wei Zhan, Masayoshi Tomizuka", "title": "Multiple criteria decision-making for lane-change model", "comments": "Submitted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation has long been an essential part of testing autonomous driving\nsystems, but only recently has simulation been useful for building and training\nself-driving vehicles. Vehicle behavioural models are necessary to simulate the\ninteractions between robot cars. This paper proposed a new method to formalize\nthe lane-changing model in urban driving scenarios. We define human incentives\nfrom different perspectives, speed incentive, route change incentive, comfort\nincentive and courtesy incentive etc. We applied a decision-theoretical tool,\ncalled Multi-Criteria Decision Making (MCDM) to take these incentive policies\ninto account. The strategy of combination is according to different driving\nstyle which varies for each driving. Thus a lane-changing decision selection\nalgorithm is proposed. Not only our method allows for varying the motivation of\nlane-changing from the purely egoistic desire to a more courtesy concern, but\nalso they can mimic drivers' state, inattentive or concentrate, which\ninfluences their driving Behaviour. We define some cost functions and calibrate\nthe parameters with different scenarios of traffic data. Distinguishing driving\nstyles are used to aggregate decision-makers' assessments about various\ncriteria weightings to obtain the action drivers desire most. Our result\ndemonstrates the proposed method can produce varied lane-changing behaviour.\nUnlike other lane-changing models based on artificial intelligence methods, our\nmodel has more flexible controllability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:58:39 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Li", "Ao", ""], ["Sun", "Liting", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1910.10251", "submitter": "Ali Ayub", "authors": "Ali Ayub, Aldo Morales and Amit Banerjee", "title": "Using Markov Decision Process to Model Deception for Robotic and\n  Interactive Game Applications", "comments": "Accepted at IEEE International Conference on Consumer Electronics\n  (ICCE) 2021", "journal-ref": "2021 IEEE International Conference on Consumer Electronics (ICCE)", "doi": "10.1109/ICCE50685.2021.9427633", "report-no": null, "categories": "cs.HC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates deception in the context of motion using a simulated\nmobile robot. We analyze some previously designed deceptive strategies on a\nmobile robot simulator. We then present a novel approach to adaptively choose\ntarget-oriented deceptive trajectories to deceive humans for multiple\ninteractions. Additionally, we propose a new metric to evaluate deception on\ndata collected from the users when interacting with the mobile robot simulator.\nWe performed a user study to test our proposed adaptive deceptive algorithm,\nwhich shows that our algorithm deceives humans even for multiple interactions\nand it is more effective than random choice of deceptive strategies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 22:13:25 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 01:12:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ayub", "Ali", ""], ["Morales", "Aldo", ""], ["Banerjee", "Amit", ""]]}, {"id": "1910.10272", "submitter": "Carlo Cenedese", "authors": "Carlo Cenedese, Filippo Fabiani, Michele Cucuzzella, Jacquelien M. A.\n  Scherpen, Ming Cao and Sergio Grammatico", "title": "Charging plug-in electric vehicles as a mixed-integer aggregative game", "comments": "Accepted to the 58th IEEE Conference on Decision and Control 2019\n  Nice", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the charge scheduling coordination of a fleet of plug-in electric\nvehicles, developing a hybrid decision-making framework for efficient and\nprofitable usage of the distribution grid. Each charging dynamics, affected by\nthe aggregate behavior of the whole fleet, is modelled as an inter-dependent,\nmixed-logical-dynamical system. The coordination problem is formalized as a\ngeneralized mixed-integer aggregative potential game, and solved via\nsemi-decentralized implementation of a sequential best-response algorithm that\nleads to an approximated equilibrium of the game.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 23:15:38 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Cenedese", "Carlo", ""], ["Fabiani", "Filippo", ""], ["Cucuzzella", "Michele", ""], ["Scherpen", "Jacquelien M. A.", ""], ["Cao", "Ming", ""], ["Grammatico", "Sergio", ""]]}, {"id": "1910.10380", "submitter": "Dhananjay Raju", "authors": "Dhananjay Raju, Suda Bharadwaj and Ufuk Topcu", "title": "Online Synthesis for Runtime Enforcement of Safety in Multi-Agent\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A shield is attached to a system to guarantee safety by correcting the\nsystem's behavior at runtime. Existing methods that employ design-time\nsynthesis of shields do not scale to multi-agent systems. Moreover, such\nshields are typically implemented in a centralized manner, requiring global\ninformation on the state of all agents in the system. We address these\nlimitations through a new approach where the shields are synthesized at runtime\nand do not require global information. There is a shield onboard every agent,\nwhich can only modify the behavior of the corresponding agent. In this\napproach, which is fundamentally decentralized, the shield on every agent has\ntwo components: a pathfinder that corrects the behavior of the agent and an\nordering mechanism that dynamically modifies the priority of the agent. The\ncurrent priority determines if the shield uses the pathfinder to modify\nbehavior of the agent. We derive an upper bound on the maximum deviation for\nany agent from its original behavior. We prove that the worst-case synthesis\ntime is quadratic in the number of agents at runtime as opposed to exponential\nat design-time for existing methods. We test the performance of the\ndecentralized, runtime shield synthesis approach on a collision-avoidance\nproblem. For 50 agents in a 50x50 grid, the synthesis at runtime requires a few\nseconds per agent whenever a potential collision is detected. In contrast, the\ncentralized design-time synthesis of shields for a similar setting is\nintractable beyond 4 agents in a 5x5 grid.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 06:32:30 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 20:24:14 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Raju", "Dhananjay", ""], ["Bharadwaj", "Suda", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1910.10748", "submitter": "Koray Kachar", "authors": "Koray G. Kachar and Alex A. Gorodetsky", "title": "Dynamic multi-agent assignment via discrete optimal transport", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY eess.SY math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal solution to a deterministic dynamic assignment problem\nby leveraging connections to the theory of discrete optimal transport to\nconvert the combinatorial assignment problem into a tractable linear program.\nWe seek to allow a multi-vehicle swarm to accomplish a dynamically changing\ntask, for example tracking a multi-target swarm. Our approach simultaneously\ndetermines the optimal assignment and the control of the individual agents. As\na result, the assignment policy accounts for the dynamics and capabilities of a\nheterogeneous set of agents and targets. In contrast to a majority of existing\nassignment schemes, this approach improves upon distance-based metrics for\nassignments by considering cost metrics that account for the underlying\ndynamics manifold. We provide a theoretical justification for the reformulation\nof this problem, and show that the minimizer of the dynamic assignment problem\nis equivalent to the minimizer of the associated Monge problem arising in\noptimal transport. We prove that by accounting for dynamics, we only require\ncomputing an assignment once over the operating lifetime --- significantly\ndecreasing computational expense. Furthermore, we show that the cost benefits\nachieved by our approach increase as the swarm size increases, achieving almost\n50\\% cost reduction compared with distance-based metrics. We demonstrate our\napproach through simulation on several linear and linearized problems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:08:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Kachar", "Koray G.", ""], ["Gorodetsky", "Alex A.", ""]]}, {"id": "1910.10795", "submitter": "Shalabh Gupta", "authors": "James Z. Hare, Junnan Song, Shalabh Gupta, Thomas A. Wettergren", "title": "POSE.R: Prediction-based Opportunistic Sensing for Resilient and\n  Efficient Sensor Networks", "comments": null, "journal-ref": "ACM Transactions on Sensor Networks, Vol. 17, Issue 1, Article 5,\n  pp. 1-41, 2020", "doi": "10.1145/3419755", "report-no": null, "categories": "eess.SY cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a distributed algorithm, called Prediction-based\nOpportunistic Sensing for Resilient and Efficient Sensor Networks (POSE.R),\nwhere the sensor nodes utilize predictions of the targets positions to\nprobabilistically control their multi-modal operating states to track the\ntarget. There are two desired features of the algorithm: energy-efficiency and\nresilience. If the target is traveling through a high node density area, then\nan optimal sensor selection approach is employed that maximizes a joint cost\nfunction of remaining energy and geometric diversity around the targets\nposition. This provides energy-efficiency and increases the network lifetime\nwhile preventing redundant nodes from tracking the target. On the other hand,\nif the target is traveling through a low node density area or in a coverage gap\n(e.g., formed by node failures or non-uniform node deployment), then a\npotential game is played amongst the surrounding nodes to optimally expand\ntheir sensing ranges via minimizing energy consumption and maximizing target\ncoverage. This provides resilience, that is the self-healing capability to\ntrack the target in the presence of low node densities and coverage gaps. The\nalgorithm is comparatively evaluated against existing approaches through Monte\nCarlo simulations which demonstrate its superiority in terms of tracking\nperformance, network-resilience and network-lifetime.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:16:26 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 22:04:55 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hare", "James Z.", ""], ["Song", "Junnan", ""], ["Gupta", "Shalabh", ""], ["Wettergren", "Thomas A.", ""]]}, {"id": "1910.10887", "submitter": "Hao Li", "authors": "Hao Li, Bowen Weng, Abhishek Gupta, Jia Pan, Wei Zhang", "title": "Reciprocal Collision Avoidance for General Nonlinear Agents using\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding feasible and collision-free paths for multiple nonlinear agents is\nchallenging in the decentralized scenarios due to limited available information\nof other agents and complex dynamics constraints. In this paper, we propose a\nfast multi-agent collision avoidance algorithm for general nonlinear agents\nwith continuous action space, where each agent observes only positions and\nvelocities of nearby agents. To reduce online computation, we first decompose\nthe multi-agent scenario and solve a two agents collision avoidance problem\nusing reinforcement learning (RL). When extending the trained policy to a\nmulti-agent problem, safety is ensured by introducing the optimal reciprocal\ncollision avoidance (ORCA) as linear constraints and the overall collision\navoidance action could be found through simple convex optimization. Most\nexisting RL-based multi-agent collision avoidance algorithms rely on the direct\ncontrol of agent velocities. In sharp contrasts, our approach is applicable to\ngeneral nonlinear agents. Realistic simulations based on nonlinear bicycle\nagent models are performed with various challenging scenarios, indicating a\ncompetitive performance of the proposed method in avoiding collisions,\ncongestion and deadlock with smooth trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:28:44 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 01:53:27 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Li", "Hao", ""], ["Weng", "Bowen", ""], ["Gupta", "Abhishek", ""], ["Pan", "Jia", ""], ["Zhang", "Wei", ""]]}, {"id": "1910.11027", "submitter": "Martin Comis", "authors": "Martin Comis, Catherine Cleophas, Christina B\\\"using", "title": "Patients, Primary Care, and Policy: Simulation Modeling for Health Care\n  Decision Support", "comments": "Modifications w.r.t. the previous version: corrected typos; fixed a\n  bug in SiM-Care implementation; revised Section 4 using the updated SiM-Care\n  implementation; extended Section 3.7.3 to clearify walk-in behavior; extended\n  Section 3.7.2 to clearify the process of arranging follow-up visits; authors\n  are no longer arranged alphabetically", "journal-ref": "Health Care Management Science 2021", "doi": "10.1007/s10729-021-09556-2", "report-no": null, "categories": "cs.MA cs.CY cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand for health care is constantly increasing due to the ongoing\ndemographic change, while at the same time health service providers face\ndifficulties in finding skilled personnel. This creates pressure on health care\nsystems around the world, such that the efficient, nationwide provision of\nprimary health care has become one of society's greatest challenges. Due to the\ncomplexity of health care systems, unforeseen future events, and a frequent\nlack of data, analyzing and optimizing the performance of health care systems\nmeans tackling a wicked problem. To support this task for primary care, this\npaper introduces the hybrid agent-based simulation model SiM-Care. SiM-Care\nmodels the interactions of patients and primary care physicians on an\nindividual level. By tracking agent interactions, it enables modelers to assess\nmultiple key indicators such as patient waiting times and physician\nutilization. Based on these indicators, primary care systems can be assessed\nand compared. Moreover, changes in the infrastructure, patient behavior, and\nservice design can be directly evaluated. To showcase the opportunities offered\nby SiM-Care and aid model validation, we present a case study for a primary\ncare system in Germany. Specifically, we investigate the effects of an aging\npopulation, a decrease in the number of physicians, as well as the combined\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:03:23 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 15:02:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Comis", "Martin", ""], ["Cleophas", "Catherine", ""], ["B\u00fcsing", "Christina", ""]]}, {"id": "1910.11262", "submitter": "Gabriele Valentini", "authors": "Gabriele Valentini", "title": "How robots in a large group make decisions as a whole? From biological\n  inspiration to the design of distributed algorithms", "comments": "journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature provides us with abundant examples of how large numbers of individuals\ncan make decisions without the coordination of a central authority. Social\ninsects, birds, fishes, and many other living collectives, rely on simple\ninteraction mechanisms to do so. They individually gather information from the\nenvironment; small bits of a much larger picture that are then shared locally\namong the members of the collective and processed together to output a commonly\nagreed choice. Throughout evolution, Nature found solutions to collective\ndecision-making problems that are intriguing to engineers for their robustness\nto malfunctioning or lost individuals, their flexibility in face of dynamic\nenvironments, and their ability to scale with large numbers of members. In the\nlast decades, whereas biologists amassed large amounts of experimental\nevidence, engineers took inspiration from these and other examples to design\ndistributed algorithms that, while maintaining the same properties of their\nnatural counterparts, come with guarantees on their performance in the form of\npredictive mathematical models. In this paper, we review the fundamental\nprocesses that lead to a collective decision. We discuss examples of collective\ndecisions in biological systems and show how similar processes can be\nengineered to design artificial ones. During this journey, we review a\nframework to design distributed decision-making algorithms that are modular,\ncan be instantiated and extended in different ways, and are supported by a suit\nof predictive mathematical models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 16:11:35 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 18:10:32 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Valentini", "Gabriele", ""]]}, {"id": "1910.11424", "submitter": "Abhinav Gupta", "authors": "Cinjon Resnick, Abhinav Gupta, Jakob Foerster, Andrew M. Dai,\n  Kyunghyun Cho", "title": "Capacity, Bandwidth, and Compositionality in Emergent Language Learning", "comments": "The first two authors contributed equally. Accepted at AAMAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works have discussed the propensity, or lack thereof, for\nemergent languages to exhibit properties of natural languages. A favorite in\nthe literature is learning compositionality. We note that most of those works\nhave focused on communicative bandwidth as being of primary importance. While\nimportant, it is not the only contributing factor. In this paper, we\ninvestigate the learning biases that affect the efficacy and compositionality\nof emergent languages. Our foremost contribution is to explore how capacity of\na neural network impacts its ability to learn a compositional language. We\nadditionally introduce a set of evaluation metrics with which we analyze the\nlearned languages. Our hypothesis is that there should be a specific range of\nmodel capacity and channel bandwidth that induces compositional structure in\nthe resulting language and consequently encourages systematic generalization.\nWhile we empirically see evidence for the bottom of this range, we curiously do\nnot find evidence for the top part of the range and believe that this is an\nopen question for the community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:06:38 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 22:36:24 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 07:54:53 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Resnick", "Cinjon", ""], ["Gupta", "Abhinav", ""], ["Foerster", "Jakob", ""], ["Dai", "Andrew M.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1910.11580", "submitter": "Daiki Miyagawa", "authors": "Daiki Miyagawa and Genki Ichinose", "title": "Cellular automaton model with turning behavior in crowd evacuation", "comments": "9 pages with 5 figures", "journal-ref": "Physica A 549(2020)", "doi": "10.1016/j.physa.2020.124376", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective evacuation policies in emergency situations are important to save\nlives. To develop such policies, simulation models based on cellular automata\nhave been used for crowd evacuation dynamics. In most previous studies of crowd\nevacuations, an evacuee is represented by a $1 \\times 1$ square. However, a\nrectangle ($1 \\times 2$) representation is more suitable for such models than\nthe square representation because of evacuees' shoulder width. The rectangle\nrepresentation gives two new features to evacuees' behaviors: moving sideways\nand turning. We study the effects of these behaviors on crowd evacuation\ndynamics. Hence, we constructed a cellular automaton model where evacuees whose\nshoulder widths are $1 \\times 2$ try to escape from a room in an emergency\nsituation. The simulation results showed that turning behavior can make the\nevacuation time shorter and there is an optimal turning rate for the crowd\nevacuation. Our findings contribute to the effective control of evacuees in\nemergency situations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 09:02:50 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 11:49:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Miyagawa", "Daiki", ""], ["Ichinose", "Genki", ""]]}, {"id": "1910.11608", "submitter": "Mattia Bianchi", "authors": "Mattia Bianchi, Sergio Grammatico", "title": "A continuous-time distributed generalized Nash equilibrium seeking\n  algorithm over networks for double-integrator agents", "comments": "Accepted to the ECC2020", "journal-ref": null, "doi": "10.23919/ECC51009.2020.9143714", "report-no": null, "categories": "math.OC cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a system of single- or double integrator agents playing a\ngeneralized Nash game over a network, in a partial-information scenario. We\naddress the generalized Nash equilibrium seeking problem by designing a\nfully-distributed dynamic controller, based on continuous-time consensus and\nprimal-dual gradient dynamics. Our main technical contribution is to show\nconvergence of the closed-loop system to a variational equilibrium, under\nstrong monotonicity and Lipschitz continuity of the game mapping, by leveraging\nmonotonicity properties and stability theory for projected dynamical systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:39:46 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 18:35:09 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bianchi", "Mattia", ""], ["Grammatico", "Sergio", ""]]}, {"id": "1910.12062", "submitter": "Mohammad Sina Kiarostami", "authors": "Mohammadreza Daneshvaramoli, Mohammad Sina Kiarostami, Saleh Khalaj\n  Monfared, Helia Karisani, Hamed Khashehchi, Dara Rahmati, Saeid Gorgin and\n  Amir Rahmati", "title": "Decentralized Cooperative Communication-less Multi-Agent Task Assignment\n  with Monte-Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cooperative task assignment is an important subject in multi-agent systems\nwith a wide range of applications. These systems are usually designed with\nmassive communication among the agents to minimize the error in pursuit of the\ngeneral goal of the entire system. In this work, we propose a novel approach\nfor Decentralized Cooperative Communication-less Multi-Agent Task Assignment\n(DCCMATA) employing Monte-Carlo Tree Search (MCTS). Here, each agent can assign\nthe optimal task by itself for itself. We design the system to automatically\nmaximize the success rate, achieving the collective goal effectively. To put it\nanother way, the agents optimally compute each following step, only by knowing\nthe current location of other agents, with no additional communication\noverhead. In contrast with the previously proposed methods which rely on the\ntask assignment procedure for similar problems, we describe a method in which\nthe agents move towards the collective goal. This may lead to scenarios where\nsome agents not necessarily move towards the closest goal. However, the total\nefficiency (makespan) and effectiveness (success ratio) in these cases are\nsignificantly improved. To evaluate our approach, we have tested the algorithm\nwith a wide range of parameters(agents, size, goal). Our implementation\ncompletely solves (Success Rate = %100) a 20*20 grid with 20 goals by 20 agents\nin 7.9 s runtime for each agent. Also, the proposed algorithm runs with the\ncomplexity of O(N^2I^2 + IN^4), where the I and N are the MCTS iterative index\nand grid size, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 13:20:32 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 08:02:14 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 15:01:11 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Daneshvaramoli", "Mohammadreza", ""], ["Kiarostami", "Mohammad Sina", ""], ["Monfared", "Saleh Khalaj", ""], ["Karisani", "Helia", ""], ["Khashehchi", "Hamed", ""], ["Rahmati", "Dara", ""], ["Gorgin", "Saeid", ""], ["Rahmati", "Amir", ""]]}, {"id": "1910.12293", "submitter": "Marzio Pennisi", "authors": "Marzio Pennisi, Miguel A. Juarez, Giulia Russo, Marco Viceconti,\n  Francesco Pappalardo", "title": "Generation of digital patients for the simulation of tuberculosis with\n  UISS-TB", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  EC funded STriTuVaD project aims to test, through a phase IIb clinical trial,\ntwo of the most advanced therapeutic vaccines against tuberculosis. In\nparallel, we have extended the Universal Immune System Simulator to include all\nrelevant determinants of such clinical trial, to establish its predictive\naccuracy against the individual patients recruited in the trial, to use it to\ngenerate digital patients and predict their response to the HRT being tested,\nand to combine them to the observations made on physical patients using a new\nin silico-augmented clinical trial approach that uses a Bayesian adaptive\ndesign. This approach, where found effective could drastically reduce the cost\nof innovation in this critical sector of public healthcare. One of the most\nchallenging task is to develop a methodology to reproduce biological diversity\nof the subjects that have to be simulated, i.e., provide an appropriate\nstrategy for the generation of libraries of digital patients. This has been\nachieved through the the creation of the initial immune system repertoire in a\nstochastic way, and though the identification of a \"vector of features\" that\ncombines both biological and pathophysiological parameters that personalize the\ndigital patient to reproduce the physiology and the pathophysiology of the\nsubject.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 16:09:43 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Pennisi", "Marzio", ""], ["Juarez", "Miguel A.", ""], ["Russo", "Giulia", ""], ["Viceconti", "Marco", ""], ["Pappalardo", "Francesco", ""]]}, {"id": "1910.12415", "submitter": "Phillip Smith Mr", "authors": "Phillip Smith, Aldeida Aleti, Vincent C.S. Lee, Robert Hunjet, Asad\n  Khan", "title": "Robotic Hierarchical Graph Neurons. A novel implementation of HGN for\n  swarm robotic behaviour control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of a novel form of Hierarchical Graph Neurons\n(HGN) for in-operation behaviour selection in a swarm of robotic agents. This\nnew HGN is called Robotic-HGN (R-HGN), as it matches robot environment\nobservations to environment labels via fusion of match probabilities from both\ntemporal and intra-swarm collections. This approach is novel for HGN as it\naddresses robotic observations being pseudo-continuous numbers, rather than\ncategorical values. Additionally, the proposed approach is memory and\ncomputation-power conservative and thus is acceptable for use in mobile devices\nsuch as single-board computers, which are often used in mobile robotic agents.\nThis R-HGN approach is validated against individual behaviour implementation\nand random behaviour selection. This contrast is made in two sets of simulated\nenvironments: environments designed to challenge the held behaviours of the\nR-HGN, and randomly generated environments which are more challenging for the\nrobotic swarm than R-HGN training conditions. R-HGN has been found to enable\nappropriate behaviour selection in both these sets, allowing significant swarm\nperformance in pre-trained and unexpected environment conditions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:11:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Smith", "Phillip", ""], ["Aleti", "Aldeida", ""], ["Lee", "Vincent C. S.", ""], ["Hunjet", "Robert", ""], ["Khan", "Asad", ""]]}, {"id": "1910.12579", "submitter": "Abhishek Dubey", "authors": "Scott Eisele and Taha Eghtesad and Keegan Campanelli and Prakhar\n  Agrawal and Aron Laszka and Abhishek Dubey", "title": "Safe and Private Forward-Trading Platform for Transactive Microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactive microgrids have emerged as a transformative solution for the\nproblems faced by distribution system operators due to an increase in the use\nof distributed energy resources and rapid growth in renewable energy\ngeneration. Transactive microgrids are tightly coupled cyber and physical\nsystems, which require resilient and robust financial markets where\ntransactions can be submitted and cleared, while ensuring that erroneous or\nmalicious transactions cannot destabilize the grid. In this paper, we introduce\nTRANSAX, a novel decentralized platform for transactive microgrids. TRANSAX\nenables participants to trade in an energy futures market, which improves\nefficiency by finding feasible matches for energy trades, reducing the load on\nthe distribution system operator. TRANSAX provides privacy to participants by\nanonymizing their trading activity using a distributed mixing service, while\nalso enforcing constraints that limit trading activity based on safety\nrequirements, such as keeping power flow below line capacity. We show that\nTRANSAX can satisfy the seemingly conflicting requirements of efficiency,\nsafety, and privacy, and we demonstrate its performance using simulation\nresults\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:13:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Eisele", "Scott", ""], ["Eghtesad", "Taha", ""], ["Campanelli", "Keegan", ""], ["Agrawal", "Prakhar", ""], ["Laszka", "Aron", ""], ["Dubey", "Abhishek", ""]]}, {"id": "1910.12639", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Osbert Bastani, Vijay Kumar", "title": "MAMPS: Safe Multi-Agent Reinforcement Learning via Model Predictive\n  Shielding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a promising approach to learning control policies\nfor performing complex multi-agent robotics tasks. However, a policy learned in\nsimulation often fails to guarantee even simple safety properties such as\nobstacle avoidance. To ensure safety, we propose multi-agent model predictive\nshielding (MAMPS), an algorithm that provably guarantees safety for an\narbitrary learned policy. In particular, it operates by using the learned\npolicy as often as possible, but instead uses a backup policy in cases where it\ncannot guarantee the safety of the learned policy. Using a multi-agent\nsimulation environment, we show how MAMPS can achieve good performance while\nensuring safety.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:26:04 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 03:03:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhang", "Wenbo", ""], ["Bastani", "Osbert", ""], ["Kumar", "Vijay", ""]]}, {"id": "1910.12804", "submitter": "Anna Guerra", "authors": "Anna Guerra, Davide Dardari, Petar M. Djuric", "title": "Joint Indoor Localization and Navigation of UAVs for Network Formation\n  Control", "comments": null, "journal-ref": null, "doi": "10.1109/ACSSC.2018.8645291", "report-no": null, "categories": "eess.SP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a joint indoor localization and navigation\nalgorithm to enable a swarm of unmanned aerial vehicles (UAVs) to deploy in a\nspecific spatial formation in indoor environments. In the envisioned scenario,\nwe consider a static user acting as a central unit whose main task is to\nacquire all the UAV measurements carrying position-dependent information and to\nestimate the UAV positions when there is no existing infrastructure for\npositioning. Subsequently, the user exploits the estimated positions as inputs\nfor the navigation control with the aim of deploying the UAVs in a desired\nformation in space (formation shaping). The user plans the trajectory of each\nUAV in real time, guaranteeing a safe navigation in the presence of obstacles.\nThe proposed algorithm guides the UAVs to their desired final locations with\ngood accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:59:21 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Guerra", "Anna", ""], ["Dardari", "Davide", ""], ["Djuric", "Petar M.", ""]]}, {"id": "1910.13166", "submitter": "Johanne Trippas R.", "authors": "Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo\n  Joho, Lawrence Cavedon", "title": "Towards a Model for Spoken Conversational Search", "comments": "Paper accepted at Information Processing & Management on October 29,\n  2019, Spoken Conversational Search, Information Seeking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation is the natural mode for information exchange in daily life, a\nspoken conversational interaction for search input and output is a logical\nformat for information seeking. However, the conceptualisation of user-system\ninteractions or information exchange in spoken conversational search (SCS) has\nnot been explored. The first step in conceptualising SCS is to understand the\nconversational moves used in an audio-only communication channel for search.\nThis paper explores conversational actions for the task of search. We define a\nqualitative methodology for creating conversational datasets, propose analysis\nprotocols, and develop the SCSdata. Furthermore, we use the SCSdata to create\nthe first annotation schema for SCS: the SCoSAS, enabling us to investigate\ninteractivity in SCS. We further establish that SCS needs to incorporate\ninteractivity and pro-activity to overcome the complexity that the information\nseeking process in an audio-only channel poses. In summary, this exploratory\nstudy unpacks the breadth of SCS. Our results highlight the need for\nintegrating discourse in future SCS models and contributes the advancement in\nthe formalisation of SCS models and the design of SCS systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:22:48 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 01:10:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Trippas", "Johanne R.", ""], ["Spina", "Damiano", ""], ["Thomas", "Paul", ""], ["Sanderson", "Mark", ""], ["Joho", "Hideo", ""], ["Cavedon", "Lawrence", ""]]}, {"id": "1910.13196", "submitter": "Florian K\\\"opf", "authors": "Florian K\\\"opf, Samuel Tesfazgi, Michael Flad, S\\\"oren Hohmann", "title": "Deep Decentralized Reinforcement Learning for Cooperative Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to collaborate efficiently with unknown partners in cooperative\ncontrol settings, adaptation of the partners based on online experience is\nrequired. The rather general and widely applicable control setting, where each\ncooperation partner might strive for individual goals while the control laws\nand objectives of the partners are unknown, entails various challenges such as\nthe non-stationarity of the environment, the multi-agent credit assignment\nproblem, the alter-exploration problem and the coordination problem. We propose\nnew, modular deep decentralized Multi-Agent Reinforcement Learning mechanisms\nto account for these challenges. Therefore, our method uses a time-dependent\nprioritization of samples, incorporates a model of the system dynamics and\nutilizes variable, accountability-driven learning rates and simulated,\nartificial experiences in order to guide the learning process. The\neffectiveness of our method is demonstrated by means of a simulated, nonlinear\ncooperative control task.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:06:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["K\u00f6pf", "Florian", ""], ["Tesfazgi", "Samuel", ""], ["Flad", "Michael", ""], ["Hohmann", "S\u00f6ren", ""]]}, {"id": "1910.13852", "submitter": "Stefan Vlaski", "authors": "Stefan Vlaski, Ali H. Sayed", "title": "Linear Speedup in Saddle-Point Escape for Decentralized Non-Convex\n  Optimization", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under appropriate cooperation protocols and parameter choices, fully\ndecentralized solutions for stochastic optimization have been shown to match\nthe performance of centralized solutions and result in linear speedup (in the\nnumber of agents) relative to non-cooperative approaches in the strongly-convex\nsetting. More recently, these results have been extended to the pursuit of\nfirst-order stationary points in non-convex environments. In this work, we\nexamine in detail the dependence of second-order convergence guarantees on the\nspectral properties of the combination policy for non-convex multi agent\noptimization. We establish linear speedup in saddle-point escape time in the\nnumber of agents for symmetric combination policies and study the potential for\nfurther improvement by employing asymmetric combination weights. The results\nimply that a linear speedup can be expected in the pursuit of second-order\nstationary points, which exclude local maxima as well as strict saddle-points\nand correspond to local or even global minima in many important learning\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:51:47 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1910.13880", "submitter": "Yevgeniy Vorobeychik", "authors": "Yi Li and Yevgeniy Vorobeychik", "title": "Path Planning Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning is a fundamental and extensively explored problem in robotic\ncontrol. We present a novel economic perspective on path planning.\nSpecifically, we investigate strategic interactions among path planning agents\nusing a game theoretic path planning framework. Our focus is on economic\ntension between two important objectives: efficiency in the agents' achieving\ntheir goals, and safety in navigating towards these. We begin by developing a\nnovel mathematical formulation for path planning that trades off these\nobjectives, when behavior of other agents is fixed. We then use this\nformulation for approximating Nash equilibria in path planning games, as well\nas to develop a multi-agent cooperative path planning formulation. Through\nseveral case studies, we show that in a path planning game, safety is often\nsignificantly compromised compared to a cooperative solution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:26:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Li", "Yi", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1910.13889", "submitter": "Virginia Bordignon", "authors": "Virginia Bordignon, Vincenzo Matta and Ali H. Sayed", "title": "Social Learning with Partial Information Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the learning abilities of agents sharing partial beliefs\nover social networks. The agents observe data that could have risen from one of\nseveral hypotheses and interact locally to decide whether the observations they\nare receiving have risen from a particular hypothesis of interest. To do so, we\nestablish the conditions under which it is sufficient to share partial\ninformation about the agents' belief in relation to the hypothesis of interest.\nSome interesting convergence regimes arise.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:29:34 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Bordignon", "Virginia", ""], ["Matta", "Vincenzo", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1910.13905", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Virginia Bordignon, Augusto Santos, Ali H. Sayed", "title": "Interplay between Topology and Social Learning over Weak Graphs", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a social learning problem, where a network of agents is\ninterested in selecting one among a finite number of hypotheses. We focus on\nweakly-connected graphs where the network is partitioned into a sending part\nand a receiving part. The data collected by the agents might be heterogeneous.\nFor example, some sub-networks might intentionally generate data from a fake\nhypothesis in order to influence other agents. The social learning task is\naccomplished via a diffusion strategy where each agent: i) updates individually\nits belief using its private data; ii) computes a new belief by exponentiating\na linear combination of the log-beliefs of its neighbors. First, we examine\nwhat agents learn over weak graphs (social learning problem). We obtain\nanalytical formulas for the beliefs at the different agents, which reveal how\nthe agents' detection capability and the network topology interact to influence\nthe beliefs. In particular, the formulas allow us to predict when a\nleader-follower behavior is possible, where some sending agents can control the\nmind of the receiving agents by forcing them to choose a particular hypothesis.\nSecond, we consider the dual or reverse learning problem that reveals how\nagents learned: given a stream of beliefs collected at a receiving agent, we\nwould like to discover the global influence that any sending component exerts\non this receiving agent (topology learning problem). A remarkable and perhaps\nunexpected interplay between social and topology learning is observed: given\n$H$ hypotheses and $S$ sending components, topology learning can be feasible\nwhen $H\\geq S$. The latter being only a necessary condition, we examine the\nfeasibility of topology learning for two useful classes of problems. The\nanalysis reveals that a critical element to enable faithful topology learning\nis the diversity in the statistical models of the sending sub-networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:49:14 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Matta", "Vincenzo", ""], ["Bordignon", "Virginia", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1910.14002", "submitter": "Vaneet Aggarwal", "authors": "Ashutosh Singh and Abubakr Alabbasi and Vaneet Aggarwal", "title": "A Distributed Model-Free Algorithm for Multi-hop Ride-sharing using Deep\n  Reinforcement Learning", "comments": "This is an extended version of the work presented in NeurIPS Workshop\n  2019. arXiv admin note: text overlap with arXiv:1903.03882", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of autonomous vehicles, ridesharing systems, and self driving\ntechnology will bring a shift in the way ride hailing platforms plan out their\nservices. However, these advances in technology coupled with road congestion,\nenvironmental concerns, fuel usage, vehicles emissions, and the high cost of\nthe vehicle usage have brought more attention to better utilize the use of\nvehicles and their capacities. In this paper, we propose a novel multi-hop\nride-sharing (MHRS) algorithm that uses deep reinforcement learning to learn\noptimal vehicle dispatch and matching decisions by interacting with the\nexternal environment. By allowing customers to transfer between vehicles, i.e.,\nride with one vehicle for sometime and then transfer to another one, MHRS helps\nin attaining 30\\% lower cost and 20\\% more efficient utilization of fleets, as\ncompared to the ride-sharing algorithms. This flexibility of multi-hop feature\ngives a seamless experience to customers and ride-sharing companies, and thus\nimproves ride-sharing services.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:40:32 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Singh", "Ashutosh", ""], ["Alabbasi", "Abubakr", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1910.14081", "submitter": "S. Rasoul Etesami", "authors": "S. Rasoul Etesami", "title": "Duality and Stability in Complex Multiagent State-Dependent Network\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.MA cs.SY math.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress on stability analysis of conventional multiagent\nnetworked systems with weakly coupled state-network dynamics, most of the\nexisting results have shortcomings in addressing multiagent systems with highly\ncoupled state-network dynamics. Motivated by numerous applications of such\ndynamics, in our previous work [1], we initiated a new direction for stability\nanalysis of such systems that uses a sequential optimization framework.\nBuilding upon that, in this paper, we extend our results by providing another\nangle on multiagent network dynamics from a duality perspective, which allows\nus to view the network structure as dual variables of a constrained nonlinear\nprogram. Leveraging that idea, we show that the evolution of the coupled\nstate-network multiagent dynamics can be viewed as iterates of a primal-dual\nalgorithm for a static constrained optimization/saddle-point problem. This view\nbridges the Lyapunov stability of state-dependent network dynamics and\nfrequently used optimization techniques such as block coordinated descent,\nmirror descent, the Newton method, and the subgradient method. As a result, we\ndevelop a systematic framework for analyzing the Lyapunov stability of\nstate-dependent network dynamics using techniques from nonlinear optimization.\nFinally, we support our theoretical results through numerical simulations from\nsocial science.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:51:52 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 07:08:03 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 18:10:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Etesami", "S. Rasoul", ""]]}, {"id": "1910.14472", "submitter": "Zongqing Lu", "authors": "Jiechuan Jiang and Zongqing Lu", "title": "Learning Fairness in Multi-Agent Systems", "comments": "NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is essential for human society, contributing to stability and\nproductivity. Similarly, fairness is also the key for many multi-agent systems.\nTaking fairness into multi-agent learning could help multi-agent systems become\nboth efficient and stable. However, learning efficiency and fairness\nsimultaneously is a complex, multi-objective, joint-policy optimization. To\ntackle these difficulties, we propose FEN, a novel hierarchical reinforcement\nlearning model. We first decompose fairness for each agent and propose\nfair-efficient reward that each agent learns its own policy to optimize. To\navoid multi-objective conflict, we design a hierarchy consisting of a\ncontroller and several sub-policies, where the controller maximizes the\nfair-efficient reward by switching among the sub-policies that provides diverse\nbehaviors to interact with the environment. FEN can be trained in a fully\ndecentralized way, making it easy to be deployed in real-world applications.\nEmpirically, we show that FEN easily learns both fairness and efficiency and\nsignificantly outperforms baselines in a variety of multi-agent scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 13:59:37 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Jiechuan", ""], ["Lu", "Zongqing", ""]]}, {"id": "1910.14562", "submitter": "Giona Casiraghi", "authors": "Giona Casiraghi and Frank Schweitzer", "title": "Improving the robustness of online social networks: A simulation\n  approach of network interventions", "comments": "20 pages, 6 figures", "journal-ref": "Front. Robot. AI (2020) 7:57", "doi": "10.3389/frobt.2020.00057", "report-no": null, "categories": "physics.soc-ph cs.MA cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks (OSN) are prime examples of socio-technical systems in\nwhich individuals interact via a technical platform. OSN are very volatile\nbecause users enter and exit and frequently change their interactions. This\nmakes the robustness of such systems difficult to measure and to control. To\nquantify robustness, we propose a coreness value obtained from the directed\ninteraction network. We study the emergence of large drop-out cascades of users\nleaving the OSN by means of an agent-based model. For agents, we define a\nutility function that depends on their relative reputation and their costs for\ninteractions. The decision of agents to leave the OSN depends on this utility.\nOur aim is to prevent drop-out cascades by influencing specific agents with low\nutility. We identify strategies to control agents in the core and the periphery\nof the OSN such that drop-out cascades are significantly reduced, and the\nrobustness of the OSN is increased.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:09:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Casiraghi", "Giona", ""], ["Schweitzer", "Frank", ""]]}]