[{"id": "1903.00206", "submitter": "Anna Dai", "authors": "Anna Dai, Zhifeng Zhao, Honggang Zhang, Rongpeng Li, Yugeng Zhou", "title": "Evaluation Mechanism of Collective Intelligence for Heterogeneous Agents\n  Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Collective intelligence is manifested when multiple agents coherently work in\nobservation, interaction, decision-making and action. In this paper, we define\nand quantify the intelligence level of heterogeneous agents group with the\nimproved Anytime Universal Intelligence Test(AUIT), based on an extension of\nthe existing evaluation of homogeneous agents group. The relationship of\nintelligence level with agents composition, group size, spatial complexity and\ntesting time is analyzed. The intelligence level of heterogeneous agents groups\nis compared with the homogeneous ones to analyze the effects of heterogeneity\non collective intelligence. Our work will help to understand the essence of\ncollective intelligence more deeply and reveal the effect of various key\nfactors on group intelligence level.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 08:45:59 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:00:27 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Dai", "Anna", ""], ["Zhao", "Zhifeng", ""], ["Zhang", "Honggang", ""], ["Li", "Rongpeng", ""], ["Zhou", "Yugeng", ""]]}, {"id": "1903.00290", "submitter": "Julien Hendrickx", "authors": "Julien M. Hendrickx, Balazs Gerencser and Baris Fidan", "title": "Trajectory convergence from coordinate-wise decrease of quadratic energy\n  functions, and applications to platoons", "comments": "6 pages, 2 figures, double columns", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider trajectories where the sign of the derivative of each entry is\nopposite to that of the corresponding entry in the gradient of an energy\nfunction. We show that this condition guarantees convergence when the energy\nfunction is quadratic and positive definite and partly extend that result to\nsome classes of positive semi-definite quadratic functions including those\ndefined using a graph Laplacian. We show how this condition allows establishing\nthe convergence of a platoon application in which it naturally appears, due to\ndeadzones in the control laws designed to avoid instabilities caused by\ninconsistent measurements of the same distance by different agents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:34:58 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 14:36:25 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 20:45:16 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Hendrickx", "Julien M.", ""], ["Gerencser", "Balazs", ""], ["Fidan", "Baris", ""]]}, {"id": "1903.00714", "submitter": "Xihan Li", "authors": "Xihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu", "title": "A Cooperative Multi-Agent Reinforcement Learning Framework for Resource\n  Balancing in Complex Logistics Network", "comments": "Accepted by AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource balancing within complex transportation networks is one of the most\nimportant problems in real logistics domain. Traditional solutions on these\nproblems leverage combinatorial optimization with demand and supply\nforecasting. However, the high complexity of transportation routes, severe\nuncertainty of future demand and supply, together with non-convex business\nconstraints make it extremely challenging in the traditional resource\nmanagement field. In this paper, we propose a novel sophisticated multi-agent\nreinforcement learning approach to address these challenges. In particular,\ninspired by the externalities especially the interactions among resource\nagents, we introduce an innovative cooperative mechanism for state and reward\ndesign resulting in more effective and efficient transportation. Extensive\nexperiments on a simulated ocean transportation service demonstrate that our\nnew approach can stimulate cooperation among agents and lead to much better\nperformance. Compared with traditional solutions based on combinatorial\noptimization, our approach can give rise to a significant improvement in terms\nof both performance and stability.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:55:40 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Xihan", ""], ["Zhang", "Jia", ""], ["Bian", "Jiang", ""], ["Tong", "Yunhai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1903.00742", "submitter": "Joel Leibo", "authors": "Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel", "title": "Autocurricula and the Emergence of Innovation from Social Interaction: A\n  Manifesto for Multi-Agent Intelligence Research", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution has produced a multi-scale mosaic of interacting adaptive units.\nInnovations arise when perturbations push parts of the system away from stable\nequilibria into new regimes where previously well-adapted solutions no longer\nwork. Here we explore the hypothesis that multi-agent systems sometimes display\nintrinsic dynamics arising from competition and cooperation that provide a\nnaturally emergent curriculum, which we term an autocurriculum. The solution of\none social task often begets new social tasks, continually generating novel\nchallenges, and thereby promoting innovation. Under certain conditions these\nchallenges may become increasingly complex over time, demanding that agents\naccumulate ever more innovations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 18:13:25 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 15:25:43 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Leibo", "Joel Z.", ""], ["Hughes", "Edward", ""], ["Lanctot", "Marc", ""], ["Graepel", "Thore", ""]]}, {"id": "1903.00784", "submitter": "Joseph Suarez", "authors": "Joseph Suarez, Yilun Du, Phillip Isola, Igor Mordatch", "title": "Neural MMO: A Massively Multiagent Game Environment for Training and\n  Evaluating Intelligent Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of complex life on Earth is often attributed to the arms race\nthat ensued from a huge number of organisms all competing for finite resources.\nWe present an artificial intelligence research environment, inspired by the\nhuman game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games,\na.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs\nand the real world alike, our environment is persistent and supports a large\nand variable number of agents. Our environment is well suited to the study of\nlarge-scale multiagent interaction: it requires that agents learn robust combat\nand navigation policies in the presence of large populations attempting to do\nthe same. Baseline experiments reveal that population size magnifies and\nincentivizes the development of skillful behaviors and results in agents that\noutcompete agents trained in smaller populations. We further show that the\npolicies of agents with unshared weights naturally diverge to fill different\nniches in order to avoid competition.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:42:33 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Suarez", "Joseph", ""], ["Du", "Yilun", ""], ["Isola", "Phillip", ""], ["Mordatch", "Igor", ""]]}, {"id": "1903.01165", "submitter": "Gabriel Istrate", "authors": "Gabriel Istrate, Cosmin Bonchi\\c{s}, Alin Br\\^indu\\c{s}escu", "title": "Attacking Power Indices by Manipulating Player Reliability", "comments": "A revised version of this manuscript will appear in the Proceedings\n  of AAMAS'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the manipulation of power indices in TU-cooperative games by\nstimulating (subject to a budget constraint) changes in the propensity of other\nplayers to participate to the game.\n  We display several algorithms that show that the problem is often tractable\nfor so-called network centrality games and influence attribution games, as well\nas an example when optimal manipulation is intractable, even though computing\npower indices is feasible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 10:44:19 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 05:49:49 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Istrate", "Gabriel", ""], ["Bonchi\u015f", "Cosmin", ""], ["Br\u00eendu\u015fescu", "Alin", ""]]}, {"id": "1903.01229", "submitter": "Zhixuan Zhou", "authors": "Zhixuan Zhou, Zhiguo Zhou and Huiyu Cai", "title": "An Analysis of Pedestrians' Behavior in Emergency Evacuation Using\n  Cellular Automata Simulation", "comments": "The authors will make major modifications to the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize property loss and death count in terror attacks and other\nemergent scenarios, attention given to timely and effective evacuation cannot\nbe enough. Due to limited evacuation resource, i.e., number of available exits,\nthere exists interdependence among pedestrians such as cooperation, competition\nand herd effect. Thus human factors - more specifically, pedestrians' behavior\nin emergency evacuation - play a significant role in evacuation research.\nEffective evacuation can only be reached when route planning are considered in\nconjunction with psychological dynamics, which is often ignored. As another\ndrawback, previous research assumes the environment including available exits\nas stationary. However, we note that during emergency, some exits which are not\noften utilized in normal times are opened, which potentially helps if\npedestrians are aware of them. In this paper, we analyze the effect of\npedestrians' behavior, i.e., herd effect and knowledge of changing environment\nwith Cellular Automata (CA) simulation. Results of the simulation show the\nharmful effect of herd effect as well as highlight the importance of timely\ninforming pedestrians of environmental change. Accordingly, we propose policy\nand procedural recommendations for emergency management of large, crowded\nstructures. Our future work includes considering more human factors and\napplying our model to log data provided by videos in public venues, which can\nfurther show effectiveness of our model in real scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 10:49:47 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 03:24:38 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zhou", "Zhixuan", ""], ["Zhou", "Zhiguo", ""], ["Cai", "Huiyu", ""]]}, {"id": "1903.01365", "submitter": "Giulio Bacchiani", "authors": "Giulio Bacchiani, Daniele Molinari, Marco Patander", "title": "Microscopic Traffic Simulation by Cooperative Multi-agent Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert human drivers perform actions relying on traffic laws and their\nprevious experience. While traffic laws are easily embedded into an artificial\nbrain, modeling human complex behaviors which come from past experience is a\nmore challenging task. One of these behaviors is the capability of\ncommunicating intentions and negotiating the right of way through driving\nactions, as when a driver is entering a crowded roundabout and observes other\ncars movements to guess the best time to merge in. In addition, each driver has\nits own unique driving style, which is conditioned by both its personal\ncharacteristics, such as age and quality of sight, and external factors, such\nas being late or in a bad mood. For these reasons, the interaction between\ndifferent drivers is not trivial to simulate in a realistic manner. In this\npaper, this problem is addressed by developing a microscopic simulator using a\nDeep Reinforcement Learning Algorithm based on a combination of visual frames,\nrepresenting the perception around the vehicle, and a vector of numerical\nparameters. In particular, the algorithm called Asynchronous Advantage\nActor-Critic has been extended to a multi-agent scenario in which every agent\nneeds to learn to interact with other similar agents. Moreover, the model\nincludes a novel architecture such that the driving style of each vehicle is\nadjustable by tuning some of its input parameters, permitting to simulate\ndrivers with different levels of aggressiveness and desired cruising speeds.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:05:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Bacchiani", "Giulio", ""], ["Molinari", "Daniele", ""], ["Patander", "Marco", ""]]}, {"id": "1903.01373", "submitter": "Shayegan Omidshafiei", "authors": "Shayegan Omidshafiei, Christos Papadimitriou, Georgios Piliouras, Karl\n  Tuyls, Mark Rowland, Jean-Baptiste Lespiau, Wojciech M. Czarnecki, Marc\n  Lanctot, Julien Perolat, and Remi Munos", "title": "$\\alpha$-Rank: Multi-Agent Evaluation by Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $\\alpha$-Rank, a principled evolutionary dynamics methodology\nfor the evaluation and ranking of agents in large-scale multi-agent\ninteractions, grounded in a novel dynamical game-theoretic solution concept\ncalled Markov-Conley chains (MCCs). The approach leverages continuous- and\ndiscrete-time evolutionary dynamical systems applied to empirical games, and\nscales tractably in the number of agents, the type of interactions, and the\ntype of empirical games (symmetric and asymmetric). Current models are\nfundamentally limited in one or more of these dimensions and are not guaranteed\nto converge to the desired game-theoretic solution concept (typically the Nash\nequilibrium). $\\alpha$-Rank provides a ranking over the set of agents under\nevaluation and provides insights into their strengths, weaknesses, and\nlong-term dynamics. This is a consequence of the links we establish to the MCC\nsolution concept when the underlying evolutionary model's ranking-intensity\nparameter, $\\alpha$, is chosen to be large, which exactly forms the basis of\n$\\alpha$-Rank. In contrast to the Nash equilibrium, which is a static concept\nbased on fixed points, MCCs are a dynamical solution concept based on the\nMarkov chain formalism, Conley's Fundamental Theorem of Dynamical Systems, and\nthe core ingredients of dynamical systems: fixed points, recurrent sets,\nperiodic orbits, and limit cycles. $\\alpha$-Rank runs in polynomial time with\nrespect to the total number of pure strategy profiles, whereas computing a Nash\nequilibrium for a general-sum game is known to be intractable. We introduce\nproofs that not only provide a unifying perspective of existing continuous- and\ndiscrete-time evolutionary evaluation models, but also reveal the formal\nunderpinnings of the $\\alpha$-Rank methodology. We empirically validate the\nmethod in several domains including AlphaGo, AlphaZero, MuJoCo Soccer, and\nPoker.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:13:40 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 11:25:21 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 17:10:21 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 15:22:09 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Omidshafiei", "Shayegan", ""], ["Papadimitriou", "Christos", ""], ["Piliouras", "Georgios", ""], ["Tuyls", "Karl", ""], ["Rowland", "Mark", ""], ["Lespiau", "Jean-Baptiste", ""], ["Czarnecki", "Wojciech M.", ""], ["Lanctot", "Marc", ""], ["Perolat", "Julien", ""], ["Munos", "Remi", ""]]}, {"id": "1903.01537", "submitter": "Navyata Sanghvi", "authors": "Navyata Sanghvi, Ryo Yonetani, Kris Kitani", "title": "MGpi: A Computational Model of Multiagent Group Perception and\n  Interaction", "comments": "To be published in: Proceedings of the 19th International Conference\n  on Autonomous Agents and Multiagent Systems (AAMAS 2020), May 2020, Auckland,\n  New Zealand", "journal-ref": "Proceedings of the 19th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2020), pp. 1196-1205", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toward enabling next-generation robots capable of socially intelligent\ninteraction with humans, we present a $\\mathbf{computational\\; model}$ of\ninteractions in a social environment of multiple agents and multiple groups.\nThe Multiagent Group Perception and Interaction (MGpi) network is a deep neural\nnetwork that predicts the appropriate social action to execute in a group\nconversation (e.g., speak, listen, respond, leave), taking into account\nneighbors' observable features (e.g., location of people, gaze orientation,\ndistraction, etc.). A central component of MGpi is the Kinesic-Proxemic-Message\n(KPM) gate, that performs social signal gating to extract important information\nfrom a group conversation. In particular, KPM gate filters incoming social cues\nfrom nearby agents by observing their body gestures (kinesics) and spatial\nbehavior (proxemics). The MGpi network and its KPM gate are learned via\nimitation learning, using demonstrations from our designed $\\mathbf{social\\;\ninteraction\\; simulator}$. Further, we demonstrate the efficacy of the KPM gate\nas a social attention mechanism, achieving state-of-the-art performance on the\ntask of $\\mathbf{group\\; identification}$ without using explicit group\nannotations, layout assumptions, or manually chosen parameters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:04:22 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 20:39:57 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sanghvi", "Navyata", ""], ["Yonetani", "Ryo", ""], ["Kitani", "Kris", ""]]}, {"id": "1903.01539", "submitter": "Atrisha Sarkar", "authors": "Atrisha Sarkar and Krzysztof Czarnecki", "title": "A behavior driven approach for sampling rare event situations for\n  autonomous vehicles", "comments": null, "journal-ref": null, "doi": "10.1109/iros40897.2019.8967715", "report-no": null, "categories": "cs.SY cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance evaluation of urban autonomous vehicles requires a realistic\nmodel of the behavior of other road users in the environment. Learning such\nmodels from data involves collecting naturalistic data of real-world human\nbehavior. In many cases, acquisition of this data can be prohibitively\nexpensive or intrusive. Additionally, the available data often contain only\ntypical behaviors and exclude behaviors that are classified as rare events. To\nevaluate the performance of AV in such situations, we develop a model of\ntraffic behavior based on the theory of bounded rationality. Based on the\nexperiments performed on a large naturalistic driving data, we show that the\ndeveloped model can be applied to estimate probability of rare events, as well\nas to generate new traffic situations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:09:52 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Sarkar", "Atrisha", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1903.01556", "submitter": "Johannes M\\\"uller", "authors": "Johannes M\\\"uller, Michael Gabb, and Michael Buchholz", "title": "A Subjective-Logic-based Reliability Estimation Mechanism for\n  Cooperative Information with Application to IV's Safety", "comments": "7 pages, accepted at 30th IEEE Intelligent Vehicles Symposium (IV\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of cooperative information, distributed by road-side units, offers large\npotential for intelligent vehicles (IVs). As vehicle automation progresses and\ncooperative perception is used to fill the blind spots of onboard sensors, the\nquestion of reliability of the data becomes increasingly important in safety\nconsiderations (SOTIF, Safety of the Intended Functionality).\n  This paper addresses the problem to estimate the reliability of cooperative\ninformation for in-vehicle use. We propose a novel method to infer the\nreliability of received data based on the theory of Subjective Logic (SL).\nUsing SL, we fuse multiple information sources, which individually only provide\nmild cues of the reliability, into a holistic estimate, which is statistically\nsound through an end-to-end modeling within the theory of SL.\n  Using the proposed scheme for probabilistic SL-based fusion, IVs are able to\nseparate faulty from correct data samples with a large margin of safety. Real\nworld experiments show the applicability and effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 14:59:40 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 09:43:38 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["M\u00fcller", "Johannes", ""], ["Gabb", "Michael", ""], ["Buchholz", "Michael", ""]]}, {"id": "1903.01632", "submitter": "Logan Beaver", "authors": "Logan E. Beaver, Behdad Chalaki, AM Ishtiaque Mahbub, Liuhui Zhao, Ray\n  Zayas, Andreas A. Malikopoulos", "title": "Demonstration of a Time-Efficient Mobility System Using a Scaled Smart\n  City", "comments": null, "journal-ref": "Vehicle System Dynamics 58 (2020) 787-804", "doi": "10.1080/00423114.2020.1730412", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of connected and automated vehicle (CAV) technologies\nenables a novel computational framework to deliver real-time control actions\nthat optimize travel time, energy, and safety. Hardware is an integral part of\nany practical implementation of CAVs, and as such, it should be incorporated in\nany validation method. However, high costs associated with full scale, field\ntesting of CAVs have proven to be a significant barrier. In this paper, we\npresent the implementation of a decentralized control framework, which was\ndeveloped previously, in a scaled-city using robotic CAVs, and discuss the\nimplications of CAVs on travel time. Supplemental information and videos can be\nfound at https://sites.google.com/view/ud-ids-lab/tfms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 02:13:46 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 15:35:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Beaver", "Logan E.", ""], ["Chalaki", "Behdad", ""], ["Mahbub", "AM Ishtiaque", ""], ["Zhao", "Liuhui", ""], ["Zayas", "Ray", ""], ["Malikopoulos", "Andreas A.", ""]]}, {"id": "1903.01720", "submitter": "James Bailey", "authors": "James P. Bailey, Georgios Piliouras", "title": "Multi-Agent Learning in Network Zero-Sum Games is a Hamiltonian System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-sum games are natural, if informal, analogues of closed physical systems\nwhere no energy/utility can enter or exit. This analogy can be extended even\nfurther if we consider zero-sum network (polymatrix) games where multiple\nagents interact in a closed economy. Typically, (network) zero-sum games are\nstudied from the perspective of Nash equilibria. Nevertheless, this comes in\ncontrast with the way we typically think about closed physical systems, e.g.,\nEarth-moon systems which move perpetually along recurrent trajectories of\nconstant energy.\n  We establish a formal and robust connection between multi-agent systems and\nHamiltonian dynamics -- the same dynamics that describe conservative systems in\nphysics. Specifically, we show that no matter the size, or network structure of\nsuch closed economies, even if agents use different online learning dynamics\nfrom the standard class of Follow-the-Regularized-Leader, they yield\nHamiltonian dynamics. This approach generalizes the known connection to\nHamiltonians for the special case of replicator dynamics in two agent zero-sum\ngames developed by Hofbauer. Moreover, our results extend beyond zero-sum\nsettings and provide a type of a Rosetta stone (see e.g. Table 1) that helps to\ntranslate results and techniques between online optimization, convex analysis,\ngames theory, and physics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 08:11:13 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bailey", "James P.", ""], ["Piliouras", "Georgios", ""]]}, {"id": "1903.02273", "submitter": "Bar Light", "authors": "Bar Light and Gabriel Weintraub", "title": "Mean Field Equilibrium: Uniqueness, Existence, and Comparative Statics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.MA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard solution concept for stochastic games is Markov perfect\nequilibrium (MPE); however, its computation becomes intractable as the number\nof players increases. Instead, we consider mean field equilibrium (MFE) that\nhas been popularized in the recent literature. MFE takes advantage of averaging\neffects in models with a large number of players. We make three main\ncontributions. First, our main result provides conditions that ensure the\nuniqueness of an MFE. We believe this uniqueness result is the first of its\nnature in the class of models we study. Second, we generalize previous MFE\nexistence results. Third, we provide general comparative statics results. We\napply our results to dynamic oligopoly models and to heterogeneous agent\nmacroeconomic models commonly used in previous work in economics and\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 09:56:58 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 07:25:28 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 07:36:09 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Light", "Bar", ""], ["Weintraub", "Gabriel", ""]]}, {"id": "1903.02589", "submitter": "Hsu-Chieh Hu", "authors": "Hsu-Chieh Hu, Stephen F. Smith", "title": "Softpressure: A Schedule-Driven Backpressure Algorithm for Coping with\n  Network Congestion", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the delay of jobs moving through a\ndirected graph of service nodes. In this problem, each node may have several\nlinks and is constrained to serve one link at a time. As jobs move through the\nnetwork, they can pass through a node only after they have been serviced by\nthat node. The objective is to minimize the delay jobs incur sitting in queues\nwaiting to be serviced. Two distinct approaches to this problem have emerged\nfrom respective work in queuing theory and dynamic scheduling: the backpressure\nalgorithm and schedule-driven control. In this paper, we present a hybrid\napproach of those two methods that incorporates the stability of queuing theory\ninto a schedule-driven control framework. We then demonstrate how this hybrid\nmethod outperforms the other two in a real-time traffic signal control problem,\nwhere the nodes are traffic lights, the links are roads, and the jobs are\nvehicles. We show through simulations that, in scenarios with heavy congestion,\nthe hybrid method results in 50% and 15% reductions in delay over\nschedule-driven control and backpressure respectively. A theoretical analysis\nalso justifies our results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 19:26:48 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hu", "Hsu-Chieh", ""], ["Smith", "Stephen F.", ""]]}, {"id": "1903.02868", "submitter": "Tonghan Wang", "authors": "Xinliang Song, Tonghan Wang, Chongjie Zhang", "title": "Convergence of Multi-Agent Learning with a Finite Step Size in\n  General-Sum Games", "comments": "AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in a multi-agent system is challenging because agents are\nsimultaneously learning and the environment is not stationary, undermining\nconvergence guarantees. To address this challenge, this paper presents a new\ngradient-based learning algorithm, called Gradient Ascent with Shrinking Policy\nPrediction (GA-SPP), which augments the basic gradient ascent approach with the\nconcept of shrinking policy prediction. The key idea behind this algorithm is\nthat an agent adjusts its strategy in response to the forecasted strategy of\nthe other agent, instead of its current one. GA-SPP is shown formally to have\nNash convergence in larger settings than existing gradient-based multi-agent\nlearning methods. Furthermore, unlike existing gradient-based methods, GA-SPP's\ntheoretical guarantees do not assume the learning rate to be infinitesimal.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 12:23:15 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Song", "Xinliang", ""], ["Wang", "Tonghan", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1903.03053", "submitter": "Paulin Jacquot Mr", "authors": "Paulin Jacquot, Olivier Beaude, Pascal Benchimol, St\\'ephane Gaubert,\n  Nadia Oudjane", "title": "A Privacy-preserving Disaggregation Algorithm for Non-intrusive\n  Management of Flexible Energy", "comments": "8 pages, 2 figures, submitted to CDC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a resource allocation problem involving a large number of agents\nwith individual constraints subject to privacy, and a central operator whose\nobjective is to optimizing a global, possibly non-convex, cost while satisfying\nthe agents'c onstraints. We focus on the practical case of the management of\nenergy consumption flexibilities by the operator of a microgrid. This paper\nprovides a privacy-preserving algorithm that does compute the optimal\nallocation of resources, avoiding each agent to reveal her private information\n(constraints and individual solution profile) neither to the central operator\nnor to a third party. Our method relies on an aggregation procedure: we\nmaintain a global allocation of resources, and gradually disaggregate this\nallocation to enforce the satisfaction of private contraints, by a protocol\ninvolving the generation of polyhedral cuts and secure multiparty computations\n(SMC). To obtain these cuts, we use an alternate projections method \\`a la Von\nNeumann, which is implemented locally by each agent, preserving her privacy\nneeds. Our theoretical and numerical results show that the method scales well\nas the number of agents gets large, and thus can be used to solve the\nallocation problem in high dimension, while addressing privacy issues.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 17:17:23 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Jacquot", "Paulin", ""], ["Beaude", "Olivier", ""], ["Benchimol", "Pascal", ""], ["Gaubert", "St\u00e9phane", ""], ["Oudjane", "Nadia", ""]]}, {"id": "1903.03086", "submitter": "Michael Fowler", "authors": "Michael C. Fowler and T. Charles Clancy and Ryan K. Williams", "title": "Intelligent Knowledge Distribution: Constrained-Action POMDPs for\n  Resource-Aware Multi-Agent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a fundamental question of multi-agent knowledge\ndistribution: what information should be sent to whom and when, with the\nlimited resources available to each agent? Communication requirements for\nmulti-agent systems can be rather high when an accurate picture of the\nenvironment and the state of other agents must be maintained. To reduce the\nimpact of multi-agent coordination on networked systems, e.g., power and\nbandwidth, this paper introduces two concepts for partially observable Markov\ndecision processes (POMDPs): 1) action-based constraints which yield\nconstrained-action POMDPs (CA-POMDPs); and 2) soft probabilistic constraint\nsatisfaction for the resulting infinite-horizon controllers. To enable\nconstraint analysis over an infinite horizon, an unconstrained policy is first\nrepresented as a Finite State Controller (FSC) and optimized with policy\niteration. The FSC representation then allows for a combination of Markov chain\nMonte Carlo and discrete optimization to improve the probabilistic constraint\nsatisfaction of the controller while minimizing the impact to the value\nfunction. Within the CA-POMDP framework we then propose Intelligent Knowledge\nDistribution (IKD) which yields per-agent policies for distributing knowledge\nbetween agents subject to interaction constraints. Finally, the CA-POMDP and\nIKD concepts are validated using an asset tracking problem where multiple\nunmanned aerial vehicles (UAVs) with heterogeneous sensors collaborate to\nlocalize a ground asset to assist in avoiding unseen obstacles in a disaster\narea. The IKD model was able to maintain asset tracking through multi-agent\ncommunications while only violating soft power and bandwidth constraints 3% of\nthe time, while greedy and naive approaches violated constraints more than 60%\nof the time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:23:53 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Fowler", "Michael C.", ""], ["Clancy", "T. Charles", ""], ["Williams", "Ryan K.", ""]]}, {"id": "1903.03189", "submitter": "Stephen Cranefield", "authors": "Stephen Cranefield and Frank Dignum", "title": "Incorporating social practices in BDI agent systems", "comments": "An extended abstract of this paper has been accepted for the\n  Eighteenth International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When agents interact with humans, either through embodied agents or because\nthey are embedded in a robot, it would be easy if they could use fixed\ninteraction protocols as they do with other agents. However, people do not keep\nfixed protocols in their day-to-day interactions and the environments are often\ndynamic, making it impossible to use fixed protocols. Deliberating about\ninteractions from fundamentals is not very scalable either, because in that\ncase all possible reactions of a user have to be considered in the plans. In\nthis paper we argue that social practices can be used as an inspiration for\ndesigning flexible and scalable interaction mechanisms that are also robust.\nHowever, using social practices requires extending the traditional BDI\ndeliberation cycle to monitor landmark states and perform expected actions by\nleveraging existing plans. We define and implement this mechanism in Jason\nusing a periodically run meta-deliberation plan, supported by a\nmetainterpreter, and illustrate its use in a realistic scenario.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 21:32:27 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Cranefield", "Stephen", ""], ["Dignum", "Frank", ""]]}, {"id": "1903.03216", "submitter": "Dong-Ki Kim", "authors": "Dong-Ki Kim, Miao Liu, Shayegan Omidshafiei, Sebastian Lopez-Cot,\n  Matthew Riemer, Golnaz Habibi, Gerald Tesauro, Sami Mourad, Murray Campbell,\n  Jonathan P. How", "title": "Learning Hierarchical Teaching Policies for Cooperative Agents", "comments": "Presented at AAMAS 2020; arXiv version added with the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective learning can be greatly enhanced when agents effectively exchange\nknowledge with their peers. In particular, recent work studying agents that\nlearn to teach other teammates has demonstrated that action advising\naccelerates team-wide learning. However, the prior work has simplified the\nlearning of advising policies by using simple function approximations and only\nconsidered advising with primitive (low-level) actions, limiting the\nscalability of learning and teaching to complex domains. This paper introduces\na novel learning-to-teach framework, called hierarchical multiagent teaching\n(HMAT), that improves scalability to complex environments by using the deep\nrepresentation for student policies and by advising with more expressive\nextended action sequences over multiple levels of temporal abstraction. Our\nempirical evaluations demonstrate that HMAT improves team-wide learning\nprogress in large, complex domains where previous approaches fail. HMAT also\nlearns teaching policies that can effectively transfer knowledge to different\nteammates with knowledge of different tasks, even when the teammates have\nheterogeneous action spaces.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 23:12:30 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 20:13:48 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 15:08:14 GMT"}, {"version": "v4", "created": "Fri, 29 Nov 2019 05:26:30 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2020 16:50:32 GMT"}, {"version": "v6", "created": "Mon, 18 May 2020 15:50:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kim", "Dong-Ki", ""], ["Liu", "Miao", ""], ["Omidshafiei", "Shayegan", ""], ["Lopez-Cot", "Sebastian", ""], ["Riemer", "Matthew", ""], ["Habibi", "Golnaz", ""], ["Tesauro", "Gerald", ""], ["Mourad", "Sami", ""], ["Campbell", "Murray", ""], ["How", "Jonathan P.", ""]]}, {"id": "1903.03218", "submitter": "Rylo Ashmore", "authors": "Rylo Ashmore, Arie Gurfinkel, Richard Trefler", "title": "Local Reasoning for Parameterized First Order Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  First Order Logic (FOL) is a powerful reasoning tool for program\nverification. Recent work on Ivy shows that FOL is well suited for verification\nof parameterized distributed systems. However, specifying many natural objects,\nsuch as a ring topology, in FOL is unexpectedly inconvenient. We present a\nframework based on FOL for specifying distributed multi-process protocols in a\nprocess-local manner together with an implicit network topology. In the\nspecification framework, we provide an auto-active analysis technique to reason\nabout the protocols locally, in a process-modular way. Our goal is to mirror\nthe way designers often describe and reason about protocols. By hiding the\ntopology behind the FOL structure, we simplify the modelling, but complicate\nthe reasoning. To deal with that, we use an oracle for the topology to develop\na sound and relatively complete proof rule that reduces reasoning about the\nimplicit topology back to pure FOL. This completely avoids the need to\naxiomatize the topology. Using the rule, we establish a property that reduces\nverification to a fixed number of processes bounded by the size of local\nneighbourhoods. We show how to use the framework on two examples, including\nleader election on a ring.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 23:21:59 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Ashmore", "Rylo", ""], ["Gurfinkel", "Arie", ""], ["Trefler", "Richard", ""]]}, {"id": "1903.03259", "submitter": "Michael Amir", "authors": "Michael Amir, Alfred M. Bruckstein", "title": "Minimizing Travel in the Uniform Dispersal Problem for Robotic Sensors", "comments": "to appear in AAMAS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited energy capacity of individual robotic agents in a swarm often\nlimits the possible cooperative tasks they can perform. In this work, we\ninvestigate the problem of covering an unknown connected grid environment (e.g.\na maze or connected corridors) with a robotic swarm so as to minimize the\nmaximal number of steps that each member of the swarm makes and their activity\ntime before their work is finished, thereby minimizing the energy requirements.\nThe robots are autonomous, anonymous and identical, with local sensors and\nfinite memory, and possess no communication capabilities. They are assumed to\ndisperse over time from a fixed location, and to move synchronously. The robots\nare tasked with occupying every cell of the environment, while avoiding\ncollisions.\n  In the literature such topics are known as \\textit{uniform dispersal\nproblems}. The goal of minimizing the number of steps traveled by the robots\nhas previously been studied in this context. Our contribution is a local\nrobotic strategy for simply connected grid environments that, by exploiting\ntheir topology, achieves optimal makespan (the amount of time it takes to cover\nthe environment) and minimizes the maximal number of steps taken by the\nindividual robots before their deactivation. The robots succeed in discovering\noptimal paths to their eventual destinations, and finish the covering process\nin $2V-1$ time steps, where $V$ is the number of cells in the environment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:01:25 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Amir", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1903.03443", "submitter": "Shrisha Rao", "authors": "Nanda Kishore Sreenivas, Shrisha Rao", "title": "Egocentric Bias and Doubt in Cognitive Agents", "comments": "Full paper in AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling social interactions based on individual behavior has always been an\narea of interest, but prior literature generally presumes rational behavior.\nThus, such models may miss out on capturing the effects of biases humans are\nsusceptible to. This work presents a method to model egocentric bias, the\nreal-life tendency to emphasize one's own opinion heavily when presented with\nmultiple opinions. We use a symmetric distribution centered at an agent's own\nopinion, as opposed to the Bounded Confidence (BC) model used in prior work. We\nconsider a game of iterated interactions where an agent cooperates based on its\nopinion about an opponent. Our model also includes the concept of domain-based\nself-doubt, which varies as the interaction succeeds or not. An increase in\ndoubt makes an agent reduce its egocentricity in subsequent interactions, thus\nenabling the agent to learn reactively. The agent system is modeled with\nfactions not having a single leader, to overcome some of the issues associated\nwith leader-follower factions. We find that agents belonging to factions\nperform better than individual agents. We observe that an intermediate level of\negocentricity helps the agent perform at its best, which concurs with\nconventional wisdom that neither overconfidence nor low self-esteem brings\nbenefits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 12:18:01 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Sreenivas", "Nanda Kishore", ""], ["Rao", "Shrisha", ""]]}, {"id": "1903.03841", "submitter": "Elio Tuci PhD", "authors": "Ziya Firat and Eliseo Ferrante and Yannick Gillet and Elio Tuci", "title": "On self-organised aggregation dynamics in swarms of robots with informed\n  robots", "comments": "Submitted Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we use simulated swarms of robots to further explore the\naggregation dynamics generated by these simple individual mechanisms. Our\nobjective is to study the introduction of \"informed robots\", and to study how\nmany of these are needed to direct the aggregation process toward a pre-defined\nsite among those available in the environment. Informed robots are members of a\ngroup that selectively avoid the site/s where no aggregate should emerge, and\nstop only on the experimenter predefined site/s for aggregation. We study the\naggregation process with informed robots in three different scenarios: two that\nare morphologically symmetric, whereby the different types of aggregation site\nare equally represented in the environment; and an asymmetric scenario, whereby\nthe target site has an area that is half the area of the sites that should be\navoided. We first show what happens when no robot in the swarm is informed: in\nsymmetric environments, the swarm is able to break the symmetry and aggregates\non one of the two types of site at random, not necessarily on the target site,\nwhile in the asymmetric environment, the swarm tends to aggregate on the sites\nthat are most represented in terms of area. As a further valuable contribution\nof this study, we provide analytical results by studying a system of Ordinary\nDifferential Equations' (ODEs) that is an extension of a well known model.\nUsing this model, we show how, for certain values of the parameters, the model\ncan predict the dynamics observed with simulated robots in one of the two\nsymmetric scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 18:08:30 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Firat", "Ziya", ""], ["Ferrante", "Eliseo", ""], ["Gillet", "Yannick", ""], ["Tuci", "Elio", ""]]}, {"id": "1903.04102", "submitter": "Meir Friedenberg", "authors": "Meir Friedenberg, Joseph Y. Halpern", "title": "Blameworthiness in Multi-Agent Settings", "comments": "Appears in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a formal definition of blameworthiness in settings where multiple\nagents can collaborate to avoid a negative outcome. We first provide a method\nfor ascribing blameworthiness to groups relative to an epistemic state (a\ndistribution over causal models that describe how the outcome might arise). We\nthen show how we can go from an ascription of blameworthiness for groups to an\nascription of blameworthiness for individuals using a standard notion from\ncooperative game theory, the Shapley value. We believe that getting a good\nnotion of blameworthiness in a group setting will be critical for designing\nautonomous agents that behave in a moral manner.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 02:26:30 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Friedenberg", "Meir", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "1903.04278", "submitter": "Hsu-Chieh Hu", "authors": "Hsu-Chieh Hu, Stephen F. Smith", "title": "Coping with Large Traffic Volumes in Schedule-Driven Traffic Signal\n  Control", "comments": "ICAPS 2017. Twenty-Seventh International Conference on Automated\n  Planning and Scheduling. arXiv admin note: text overlap with arXiv:1903.02589", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in decentralized, schedule-driven traffic control has\ndemonstrated the ability to significantly improve traffic flow efficiency in\ncomplex urban road networks. However, in situations where vehicle volumes\nincrease to the point that the physical capacity of a road network reaches or\nexceeds saturation, it has been observed that the effectiveness of a\nschedule-driven approach begins to degrade, leading to progressively higher\nnetwork congestion. In essence, the traffic control problem becomes less of a\nscheduling problem and more of a queue management problem in this circumstance.\nIn this paper we propose a composite approach to real-time traffic control that\nuses sensed information on queue lengths to influence scheduling decisions and\ngracefully shift the signal control strategy to queue management in high\nvolume/high congestion settings. Specifically, queue-length information is used\nto establish weights for the sensed vehicle clusters that must be scheduled\nthrough a given intersection at any point, and hence bias the wait time\nminimization calculation. To compute these weights, we develop a model in which\nsuccessive movement phases are viewed as different states of an Ising model,\nand parameters quantify strength of interactions. To ensure scalability, queue\ninformation is only exchanged between direct neighbors and the asynchronous\nnature of local intersection scheduling is preserved. We demonstrate the\npotential of the approach through microscopic traffic simulation of a\nreal-world road network, showing a 60% reduction in average wait times over the\nbaseline schedule-driven approach in heavy traffic scenarios. We also report\ninitial field test results, which show the ability to reduce queues during\nheavy traffic periods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 19:42:36 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hu", "Hsu-Chieh", ""], ["Smith", "Stephen F.", ""]]}, {"id": "1903.04300", "submitter": "Arthur Queffelec", "authors": "Tristan Charrier, Arthur Queffelec, Ocan Sankur and Fran\\c{c}ois\n  Schwarzentruber", "title": "Reachability and Coverage Planning for Connected Agents: Extended\n  Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing appeal of robots in information-gathering\nmissions, we study multi-agent path planning problems in which the agents must\nremain interconnected. We model an area by a topological graph specifying the\nmovement and the connectivity constraints of the agents. We study the\ntheoretical complexity of the reachability and the coverage problems of a fleet\nof connected agents on various classes of topological graphs. We establish the\ncomplexity of these problems on known classes, and introduce a new class called\nsight-moveable graphs which admit efficient algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 13:52:57 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Charrier", "Tristan", ""], ["Queffelec", "Arthur", ""], ["Sankur", "Ocan", ""], ["Schwarzentruber", "Fran\u00e7ois", ""]]}, {"id": "1903.04714", "submitter": "Michael Teng", "authors": "Michael Teng, Tuan Anh Le, Adam Scibior, Frank Wood", "title": "Imitation Learning of Factored Multi-agent Reactive Models", "comments": "incorporated into another paper with different motivations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply recent advances in deep generative modeling to the task of imitation\nlearning from biological agents. Specifically, we apply variations of the\nvariational recurrent neural network model to a multi-agent setting where we\nlearn policies of individual uncoordinated agents acting based on their\nperceptual inputs and their hidden belief state. We learn stochastic policies\nfor these agents directly from observational data, without constructing a\nreward function. An inference network learned jointly with the policy allows\nfor efficient inference over the agent's belief state given a sequence of its\ncurrent perceptual inputs and the prior actions it performed, which lets us\nextrapolate observed sequences of behavior into the future while maintaining\nuncertainty estimates over future trajectories. We test our approach on a\ndataset of flies interacting in a 2D environment, where we demonstrate better\npredictive performance than existing approaches which learn deterministic\npolicies with recurrent neural networks. We further show that the uncertainty\nestimates over future trajectories we obtain are well calibrated, which makes\nthem useful for a variety of downstream processing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 03:50:27 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 21:13:23 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Teng", "Michael", ""], ["Le", "Tuan Anh", ""], ["Scibior", "Adam", ""], ["Wood", "Frank", ""]]}, {"id": "1903.04726", "submitter": "Daniel Tabatabai", "authors": "Daniel Tabatabai and Mohanad Ajina and Cameron Nowzari", "title": "Self-triggered distributed k-order coverage control", "comments": "13 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A k-order coverage control problem is studied where a network of agents must\ndeploy over a desired area. The objective is to deploy all the agents in a\ndecentralized manner such that a certain coverage performance metric of the\nnetwork is maximized. Unlike many prior works that consider multi-agent\ndeployment, we explicitly consider applications where more than one agent may\nbe required to service an event that randomly occurs anywhere in the domain.\nThe proposed method ensures the distributed agents autonomously cover the area\nwhile simultaneously relaxing the requirement of constant communication among\nthe agents. In order to achieve the stated goals, a self-triggered coordination\nmethod is developed that both determines how agents should move without having\nto continuously acquire information from other agents, as well as exactly when\nto communicate and acquire new information. Through analysis, the proposed\nstrategy is shown to provide asymptotic convergence similar to that of\ncontinuous or periodic methods. Simulation results demonstrate that the\nproposed method can reduce the number of messages exchanged as well as the\namount of communication power necessary to accomplish the deployment task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 04:40:53 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Tabatabai", "Daniel", ""], ["Ajina", "Mohanad", ""], ["Nowzari", "Cameron", ""]]}, {"id": "1903.04954", "submitter": "Omar A. Guerrero", "authors": "Robert L. Axtell, Omar A. Guerrero and Eduardo L\\'opez", "title": "Frictional Unemployment on Labor Flow Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.MA physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an alternative theory to the aggregate matching function in which\nworkers search for jobs through a network of firms: the labor flow network. The\nlack of an edge between two companies indicates the impossibility of labor\nflows between them due to high frictions. In equilibrium, firms' hiring\nbehavior correlates through the network, generating highly disaggregated local\nunemployment. Hence, aggregation depends on the topology of the network in\nnon-trivial ways. This theory provides new micro-foundations for the Beveridge\ncurve, wage dispersion, and the employer-size premium. We apply our model to\nemployer-employee matched records and find that network topologies with\nPareto-distributed connections cause disproportionately large changes on\naggregate unemployment under high labor supply elasticity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:30:46 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Axtell", "Robert L.", ""], ["Guerrero", "Omar A.", ""], ["L\u00f3pez", "Eduardo", ""]]}, {"id": "1903.04959", "submitter": "Haotian Fu", "authors": "Haotian Fu, Hongyao Tang, Jianye Hao, Zihan Lei, Yingfeng Chen,\n  Changjie Fan", "title": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid\n  Action Spaces", "comments": null, "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) has been applied to address a variety of\ncooperative multi-agent problems with either discrete action spaces or\ncontinuous action spaces. However, to the best of our knowledge, no previous\nwork has ever succeeded in applying DRL to multi-agent problems with\ndiscrete-continuous hybrid (or parameterized) action spaces which is very\ncommon in practice. Our work fills this gap by proposing two novel algorithms:\nDeep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized\ntraining but decentralized execution paradigm: different levels of\ncommunication between different agents are used to facilitate the training\nprocess, while each agent executes its policy independently based on local\nobservations during execution. Our empirical results on several challenging\ntasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN\nand Deep MAHHQN are effective and significantly outperform existing independent\ndeep parameterized Q-learning method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:40:32 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Fu", "Haotian", ""], ["Tang", "Hongyao", ""], ["Hao", "Jianye", ""], ["Lei", "Zihan", ""], ["Chen", "Yingfeng", ""], ["Fan", "Changjie", ""]]}, {"id": "1903.05149", "submitter": "Harish Ravichandar", "authors": "Harish Ravichandar, Kenneth Shaw, Sonia Chernova", "title": "STRATA: A Unified Framework for Task Assignments in Large Teams of\n  Heterogeneous Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large teams of heterogeneous agents have the potential to solve complex\nmulti-task problems that are intractable for a single agent working\nindependently. However, solving complex multi-task problems requires leveraging\nthe relative strengths of the different kinds of agents in the team. We present\nStochastic TRAit-based Task Assignment (STRATA), a unified framework that\nmodels large teams of heterogeneous agents and performs effective task\nassignments. Specifically, given information on which traits (capabilities) are\nrequired for various tasks, STRATA computes the assignments of agents to tasks\nsuch that the trait requirements are achieved. Inspired by prior work in robot\nswarms and biodiversity, we categorize agents into different species (groups)\nbased on their traits. We model each trait as a continuous variable and\ndifferentiate between traits that can and cannot be aggregated from different\nagents. STRATA is capable of reasoning about both species-level and agent-level\nvariability in traits. Further, we define measures of diversity for any given\nteam based on the team's continuous-space trait model. We illustrate the\nnecessity and effectiveness of STRATA using detailed experiments based in\nsimulation and in a capture-the-flag game environment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 19:02:09 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 18:51:43 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 19:38:05 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Ravichandar", "Harish", ""], ["Shaw", "Kenneth", ""], ["Chernova", "Sonia", ""]]}, {"id": "1903.05431", "submitter": "Kleanthis Malialis", "authors": "Kleanthis Malialis and Sam Devlin and Daniel Kudenko", "title": "Resource Abstraction for Reinforcement Learning in Multiagent Congestion\n  Problems", "comments": "Keywords: congestion problems, resource management, multiagent\n  reinforcement learning, multiagent systems, multiagent learning, resource\n  abstraction. In Proceedings of the 2016 International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS '16)", "journal-ref": "Proceedings of the International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS), 2016/", "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world congestion problems (e.g. traffic congestion) are typically very\ncomplex and large-scale. Multiagent reinforcement learning (MARL) is a\npromising candidate for dealing with this emerging complexity by providing an\nautonomous and distributed solution to these problems. However, there are three\nlimiting factors that affect the deployability of MARL approaches to congestion\nproblems. These are learning time, scalability and decentralised coordination\ni.e. no communication between the learning agents. In this paper we introduce\nResource Abstraction, an approach that addresses these challenges by allocating\nthe available resources into abstract groups. This abstraction creates new\nreward functions that provide a more informative signal to the learning agents\nand aid the coordination amongst them. Experimental work is conducted on two\nbenchmark domains from the literature, an abstract congestion problem and a\nrealistic traffic congestion problem. The current state-of-the-art for solving\nmultiagent congestion problems is a form of reward shaping called difference\nrewards. We show that the system using Resource Abstraction significantly\nimproves the learning speed and scalability, and achieves the highest possible\nor near-highest joint performance/social welfare for both congestion problems\nin large-scale scenarios involving up to 1000 reinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 11:54:04 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Malialis", "Kleanthis", ""], ["Devlin", "Sam", ""], ["Kudenko", "Daniel", ""]]}, {"id": "1903.05444", "submitter": "Hannes Hornischer", "authors": "Hannes Hornischer, Joshua Cherian Varughese, Ronald Thenius, Franz\n  Wotawa, Manfred F\\\"ullsack, Thomas Schmickl", "title": "CIMAX: Collective Information Maximization in Robotic Swarms Using Local\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic swarms and mobile sensor networks are used for environmental\nmonitoring in various domains and areas of operation. Especially in otherwise\ninaccessible environments decentralized robotic swarms can be advantageous due\nto their high spatial resolution of measurements and resilience to failure of\nindividuals in the swarm. However, such robotic swarms might need to be able to\ncompensate misplacement during deployment or adapt to dynamical changes in the\nenvironment. Reaching a collective decision in a swarm with limited\ncommunication abilities without a central entity serving as decision-maker can\nbe a challenging task. Here we present the CIMAX algorithm for collective\ndecision making for maximizing the information gathered by the swarm as a\nwhole. Agents negotiate based on their individual sensor readings and\nultimately make a decision for collectively moving in a particular direction so\nthat the swarm as a whole increases the amount of relevant measurements and\nthus accessible information. We use both simulation and real robotic\nexperiments for presenting, testing and validating our algorithm. CIMAX is\ndesigned to be used in underwater swarm robots for troubleshooting an oxygen\ndepletion phenomenon known as \"anoxia\".\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 12:21:49 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Hornischer", "Hannes", ""], ["Varughese", "Joshua Cherian", ""], ["Thenius", "Ronald", ""], ["Wotawa", "Franz", ""], ["F\u00fcllsack", "Manfred", ""], ["Schmickl", "Thomas", ""]]}, {"id": "1903.05561", "submitter": "Hangyu Mao", "authors": "Hangyu Mao, Zhibo Gong, Zhengchao Zhang, Zhen Xiao and Yan Ni", "title": "Learning Multi-agent Communication under Limited-bandwidth Restriction\n  for Internet Packet Routing", "comments": "This paper proposes a gating mechanism with several crucial designs\n  for adaptively prunning the unprofitable communication messages among\n  multiple agents, such that the limited-bandwidth restriction existing in many\n  real-world muli-agent systems can be resolved. Experiments show that our\n  method can prune quite a lot of unprofitable messages with little damage to\n  the performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is an important factor for the big multi-agent world to stay\norganized and productive. Recently, the AI community has applied the Deep\nReinforcement Learning (DRL) to learn the communication strategy and the\ncontrol policy for multiple agents. However, when implementing the\ncommunication for real-world multi-agent applications, there is a more\npractical limited-bandwidth restriction, which has been largely ignored by the\nexisting DRL-based methods. Specifically, agents trained by most previous\nmethods keep sending messages incessantly in every control cycle; due to\nemitting too many messages, these methods are unsuitable to be applied to the\nreal-world systems that have a limited bandwidth to transmit the messages. To\nhandle this problem, we propose a gating mechanism to adaptively prune\nunprofitable messages. Results show that the gating mechanism can prune more\nthan 80% messages with little damage to the performance. Moreover, our method\noutperforms several state-of-the-art DRL-based and rule-based methods by a\nlarge margin in both the real-world packet routing tasks and four benchmark\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:41:40 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Mao", "Hangyu", ""], ["Gong", "Zhibo", ""], ["Zhang", "Zhengchao", ""], ["Xiao", "Zhen", ""], ["Ni", "Yan", ""]]}, {"id": "1903.05599", "submitter": "Ruwan Wickramarachchi", "authors": "Sameh M. Saad, Terrence Perera and Ruwan Wickramarachchi", "title": "Simulation of distributed manufacturing enterprises: A new approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The globalization of markets and world-wide competition forces manufacturing\nenterprises to enter into alliances leading to the creation of distributed\nmanufacturing enterprises. Before forming a partnership it is essential to\nevaluate viability of proposed enterprise as well as how a companys operations\nare affected by the proposed virtual enterprise. Distributed simulation\nprovides an attractive tool to make decisions on such situations. However, due\nto its complexity and high cost distributed simulation failed to gain a wide\nacceptance from industrial users. This paper presents a new approach for\ndistributed manufacturing simulation (DMS) including a formal methodology for\nDMS and, implementation approach using current commercial simulation software,\nemploying widely available and cost effective technologies. The main objective\nof this work is to promote the use of distributed simulation particularly in\ndistributed manufacturing by making it fast to develop and less complicated for\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 17:53:59 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Saad", "Sameh M.", ""], ["Perera", "Terrence", ""], ["Wickramarachchi", "Ruwan", ""]]}, {"id": "1903.05766", "submitter": "Raunak Bhattacharyya", "authors": "Raunak P. Bhattacharyya, Derek J. Phillips, Changliu Liu, Jayesh K.\n  Gupta, Katherine Driggs-Campbell, Mykel J. Kochenderfer", "title": "Simulating Emergent Properties of Human Driving Behavior Using\n  Multi-Agent Reward Augmented Imitation Learning", "comments": "Accepted for publication at ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in multi-agent imitation learning have shown promising\nresults for modeling the behavior of human drivers. However, it is challenging\nto capture emergent traffic behaviors that are observed in real-world datasets.\nSuch behaviors arise due to the many local interactions between agents that are\nnot commonly accounted for in imitation learning. This paper proposes Reward\nAugmented Imitation Learning (RAIL), which integrates reward augmentation into\nthe multi-agent imitation learning framework and allows the designer to specify\nprior knowledge in a principled fashion. We prove that convergence guarantees\nfor the imitation learning process are preserved under the application of\nreward augmentation. This method is validated in a driving scenario, where an\nentire traffic scene is controlled by driving policies learned using our\nproposed algorithm. Further, we demonstrate improved performance in comparison\nto traditional imitation learning algorithms both in terms of the local actions\nof a single agent and the behavior of emergent properties in complex,\nmulti-agent settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 00:02:03 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Bhattacharyya", "Raunak P.", ""], ["Phillips", "Derek J.", ""], ["Liu", "Changliu", ""], ["Gupta", "Jayesh K.", ""], ["Driggs-Campbell", "Katherine", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1903.06017", "submitter": "Hancheng Min", "authors": "Hancheng Min and Enrique Mallada", "title": "Dynamics Concentration of Large-Scale Tightly-Connected Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to achieve coordinated behavior --engineered or emergent-- on\nnetworked systems has attracted widespread interest over several fields. This\nhas led to remarkable advances on the development of a theoretical\nunderstanding of the conditions under which agents within a network can reach\nagreement (consensus) or develop coordinated behaviors such as synchronization.\nHowever, fewer advances have been made toward explaining another commonly\nobserved phenomena in tightly-connected networks systems: output responses of\nnodes in the networks are almost identical to each other despite heterogeneity\nin their individual dynamics. In this paper, we leverage tools from\nhigh-dimensional probability to provide an initial answer to this phenomena.\nMore precisely, we show that for linear networks of nodal random transfer\nfunctions, as the network size and connectivity grows, every node in the\nnetwork follows the same response to an input or disturbance --irrespectively\nof the source of this input. We term this behavior as dynamics concentration\nsince it stems from the fact that the network transfer matrix uniformly\nconverges in probability, i.e., it concentrates, to a unique dynamic response\ndetermined by the distribution of the random transfer function of each node. We\nfurther discuss the implications of our analysis in the context of model\nreduction and robustness, and provide numerical evidence that similar phenomena\noccur in small deterministic networks over a properly defined frequency band.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:16:37 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 18:07:03 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 12:29:28 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2019 16:29:14 GMT"}, {"version": "v5", "created": "Thu, 12 Sep 2019 20:57:58 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Min", "Hancheng", ""], ["Mallada", "Enrique", ""]]}, {"id": "1903.06288", "submitter": "Rahul Chandan", "authors": "Rahul Chandan, Dario Paccagnan, Jason R. Marden", "title": "Optimal Price of Anarchy in Cost-Sharing Games", "comments": "8 pages, double column, 1 figure, 2 tables, submitted to 2019\n  American Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.GT cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design of distributed algorithms is central to the study of multiagent\nsystems control. In this paper, we consider a class of combinatorial\ncost-minimization problems and propose a framework for designing distributed\nalgorithms with a priori performance guarantees that are near-optimal. We\napproach this problem from a game-theoretic perspective, assigning agents cost\nfunctions such that the equilibrium efficiency (price of anarchy) is optimized.\nOnce agents' cost functions have been specified, any algorithm capable of\ncomputing a Nash equilibrium of the system inherits a performance guarantee\nmatching the price of anarchy. Towards this goal, we formulate the problem of\ncomputing the price of anarchy as a tractable linear program. We then present a\nframework for designing agents' local cost functions in order to optimize for\nthe worst-case equilibrium efficiency. Finally, we investigate the implications\nof our findings when this framework is applied to systems with convex,\nnondecreasing costs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:44:55 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Chandan", "Rahul", ""], ["Paccagnan", "Dario", ""], ["Marden", "Jason R.", ""]]}, {"id": "1903.06847", "submitter": "Matthew Gombolay", "authors": "Esmaeil Seraj and Andrew Silva and Matthew Gombolay", "title": "Safe Coordination of Human-Robot Firefighting Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfires are destructive and inflict massive, irreversible harm to victims'\nlives and natural resources. Researchers have proposed commissioning unmanned\naerial vehicles (UAVs) to provide firefighters with real-time tracking\ninformation; yet, these UAVs are not able to reason about a fire's track,\nincluding current location, measurement, and uncertainty, as well as\npropagation. We propose a model-predictive, probabilistically safe distributed\ncontrol algorithm for human-robot collaboration in wildfire fighting. The\nproposed algorithm overcomes the limitations of prior work by explicitly\nestimating the latent fire propagation dynamics to enable intelligent,\ntime-extended coordination of the UAVs in support of on-the-ground human\nfirefighters. We derive a novel, analytical bound that enables UAVs to\ndistribute their resources and provides a probabilistic guarantee of the\nhumans' safety while preserving the UAVs' ability to cover an entire fire.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 00:14:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Seraj", "Esmaeil", ""], ["Silva", "Andrew", ""], ["Gombolay", "Matthew", ""]]}, {"id": "1903.06979", "submitter": "Salar Safarkhani", "authors": "Salar Safarkhani, Vikranth Reddy Kattakuri, Ilias Bilionis, Jitesh\n  Panchal", "title": "A Principal-Agent Model of Systems Engineering Processes with\n  Application to Satellite Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principal-agent model of a one-shot, shallow, systems\nengineering process. The process is one-shot in the sense that decisions are\nmade during one time step and that they are final. The term shallow refers to a\none-layer hierarchy of the process. Specifically, we assume that the systems\nengineer has already decomposed the problem in subsystems, and that each\nsubsystem is assigned to a different subsystem engineer. Each subsystem\nengineer works independently to maximize their own expected payoff. The goal of\nthe systems engineer is to maximize the system-level payoff by incentivizing\nthe subsystem engineers. We restrict our attention to requirement-based\nsystem-level payoffs, i.e., the systems engineer makes a profit only if all the\ndesign requirements are met. We illustrate the model using the design of an\nEarth-orbiting satellite system where the systems engineer determines the\noptimum incentive structures and requirements for two subsystems: the\npropulsion subsystem and the power subsystem. The model enables the analysis of\na systems engineer's decisions about optimal passed-down requirements and\nincentives for sub-system engineers under different levels of task difficulty\nand associated costs. Sample results, for the case of risk-neutral systems and\nsubsystems engineers, show that it is not always in the best interest of the\nsystems engineer to pass down the true requirements. As expected, the model\npredicts that for small to moderate task uncertainties the optimal requirements\nare higher than the true ones, effectively eliminating the probability of\nfailure for the systems engineer. In contrast, the model predicts that for\nlarge task uncertainties the optimal requirements should be smaller than the\ntrue ones in order to lure the subsystem engineers into participation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 19:59:41 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Safarkhani", "Salar", ""], ["Kattakuri", "Vikranth Reddy", ""], ["Bilionis", "Ilias", ""], ["Panchal", "Jitesh", ""]]}, {"id": "1903.07266", "submitter": "Usman Khan", "authors": "Ran Xin, Anit Kumar Sahu, Usman A. Khan, and Soummya Kar", "title": "Distributed stochastic optimization with gradient tracking over\n  strongly-connected networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed stochastic optimization to minimize a sum\nof smooth and strongly-convex local cost functions over a network of agents,\ncommunicating over a strongly-connected graph. Assuming that each agent has\naccess to a stochastic first-order oracle ($\\mathcal{SFO}$), we propose a novel\ndistributed method, called $\\mathcal{S}$-$\\mathcal{AB}$, where each agent uses\nan auxiliary variable to asymptotically track the gradient of the global cost\nin expectation. The $\\mathcal{S}$-$\\mathcal{AB}$ algorithm employs row- and\ncolumn-stochastic weights simultaneously to ensure both consensus and\noptimality. Since doubly-stochastic weights are not used,\n$\\mathcal{S}$-$\\mathcal{AB}$ is applicable to arbitrary strongly-connected\ngraphs. We show that under a sufficiently small constant step-size,\n$\\mathcal{S}$-$\\mathcal{AB}$ converges linearly (in expected mean-square sense)\nto a neighborhood of the global minimizer. We present numerical simulations\nbased on real-world data sets to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 06:29:08 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 22:23:17 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Xin", "Ran", ""], ["Sahu", "Anit Kumar", ""], ["Khan", "Usman A.", ""], ["Kar", "Soummya", ""]]}, {"id": "1903.07559", "submitter": "Pedro Hespanhol", "authors": "Pedro Hespanhol and Anil Aswani", "title": "Surrogate Optimal Control for Strategic Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to design a platform to optimally control constrained\nmulti-agent systems with a single coordinator and multiple strategic agents. In\nour setting, the agents cannot apply control inputs and only the coordinator\napplies control inputs; however, the coordinator does not know the objective\nfunctions of the agents, and so must choose control actions based on\ninformation provided by the agents. One major challenge is that if the platform\nis not correctly designed then the agents may provide false information to the\ncoordinator in order to achieve improved outcomes for themselves at the expense\nof the overall system efficiency. Here, we design an interaction mechanism\nbetween the agents and the coordinator such that the mechanism: ensures agents\ntruthfully report their information, has low communication requirements, and\nleads to a control action that achieves efficiency by achieving a Nash\nequilibrium. In particular, we design a mechanism in which each agent does not\nneed to posses full knowledge of the system dynamics nor the objective\nfunctions of other agents. We illustrate our proposed mechanism in a model\npredictive control (MPC) application involving heating, ventilation,\nair-conditioning (HVAC) control by a building manager of an apartment building.\nOur results showcase how such a mechanism can be potentially used in the\ncontext of distributed MPC.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:53:30 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Hespanhol", "Pedro", ""], ["Aswani", "Anil", ""]]}, {"id": "1903.07797", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Richard Cole, Vasilis Gkatzelis, Jason D. Hartline", "title": "A Truthful Cardinal Mechanism for One-Sided Matching", "comments": "Appears in SODA 2020", "journal-ref": "Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\n  Discrete Algorithms (pp. 2096-2113). Society for Industrial and Applied\n  Mathematics (SIAM)", "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the well-studied problem of designing mechanisms for one-sided\nmatching markets, where a set of $n$ agents needs to be matched to a set of $n$\nheterogeneous items. Each agent $i$ has a value $v_{i,j}$ for each item $j$,\nand these values are private information that the agents may misreport if doing\nso leads to a preferred outcome. Ensuring that the agents have no incentive to\nmisreport requires a careful design of the matching mechanism, and mechanisms\nproposed in the literature mitigate this issue by eliciting only the\n\\emph{ordinal} preferences of the agents, i.e., their ranking of the items from\nmost to least preferred. However, the efficiency guarantees of these mechanisms\nare based only on weak measures that are oblivious to the underlying values. In\nthis paper we achieve stronger performance guarantees by introducing a\nmechanism that truthfully elicits the full \\emph{cardinal} preferences of the\nagents, i.e., all of the $v_{i,j}$ values. We evaluate the performance of this\nmechanism using the much more demanding Nash bargaining solution as a\nbenchmark, and we prove that our mechanism significantly outperforms all\nordinal mechanisms (even non-truthful ones). To prove our approximation bounds,\nwe also study the population monotonicity of the Nash bargaining solution in\nthe context of matching markets, providing both upper and lower bounds which\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 02:25:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 21:45:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Abebe", "Rediet", ""], ["Cole", "Richard", ""], ["Gkatzelis", "Vasilis", ""], ["Hartline", "Jason D.", ""]]}, {"id": "1903.07807", "submitter": "Kunhee Ryu", "authors": "Kunhee Ryu and Juhoon Back", "title": "Distributed Kalman-filtering: Distributed optimization viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Kalman-filtering problem with multiple sensors which are\nconnected through a communication network. If all measurements are delivered to\none place called fusion center and processed together, we call the process\ncentralized Kalman-filtering (CKF). When there is no fusion center, each sensor\ncan also solve the problem by using local measurements and exchanging\ninformation with its neighboring sensors, which is called distributed\nKalman-filtering (DKF). Noting that CKF problem is a maximum likelihood\nestimation problem, which is a quadratic optimization problem, we reformulate\nDKF problem as a consensus optimization problem, resulting in that DKF problem\ncan be solved by many existing distributed optimization algorithms. A new DKF\nalgorithm employing the distributed dual ascent method is provided and its\nperformance is evaluated through numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 03:16:04 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 12:11:38 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Ryu", "Kunhee", ""], ["Back", "Juhoon", ""]]}, {"id": "1903.08082", "submitter": "Tom Eccles", "authors": "Tom Eccles, Edward Hughes, J\\'anos Kram\\'ar, Steven Wheelwright, Joel\n  Z. Leibo", "title": "Learning Reciprocity in Complex Sequential Social Dilemmas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocity is an important feature of human social interaction and underpins\nour cooperative nature. What is more, simple forms of reciprocity have proved\nremarkably resilient in matrix game social dilemmas. Most famously, the\ntit-for-tat strategy performs very well in tournaments of Prisoner's Dilemma.\nUnfortunately this strategy is not readily applicable to the real world, in\nwhich options to cooperate or defect are temporally and spatially extended.\nHere, we present a general online reinforcement learning algorithm that\ndisplays reciprocal behavior towards its co-players. We show that it can induce\npro-social outcomes for the wider group when learning alongside selfish agents,\nboth in a $2$-player Markov game, and in $5$-player intertemporal social\ndilemmas. We analyse the resulting policies to show that the reciprocating\nagents are strongly influenced by their co-players' behavior.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 16:18:00 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Eccles", "Tom", ""], ["Hughes", "Edward", ""], ["Kram\u00e1r", "J\u00e1nos", ""], ["Wheelwright", "Steven", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "1903.08228", "submitter": "Olaf Witkowski", "authors": "Olaf Witkowski, Takashi Ikegami", "title": "How to Make Swarms Open-Ended? Evolving Collective Intelligence Through\n  a Constricted Exploration of Adjacent Possibles", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach of open-ended evolution via the simulation of swarm\ndynamics. In nature, swarms possess remarkable properties, which allow many\norganisms, from swarming bacteria to ants and flocking birds, to form\nhigher-order structures that enhance their behavior as a group. Swarm\nsimulations highlight three important factors to create novelty and diversity:\n(a) communication generates combinatorial cooperative dynamics, (b) concurrency\nallows for separation of timescales, and (c) complexity and size increases push\nthe system towards transitions in innovation. We illustrate these three\ncomponents in a model computing the continuous evolution of a swarm of agents.\nThe results, divided in three distinct applications, show how emergent\nstructures are capable of filtering information through the bottleneck of their\nmemory, to produce meaningful novelty and diversity within their simulated\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:34:49 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Witkowski", "Olaf", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1903.09259", "submitter": "Ariel Barel Dr.", "authors": "Rotem Manor, Ariel Barel, and Alfred M. Bruckstein", "title": "Local Interactions for Cohesive Flexible Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed gathering algorithms aim to achieve complete visibility graphs\nvia a \"never lose a neighbour\" policy. We suggest a method to maintain\nconnected graph topologies, while reducing the number of effective edges in the\ngraph to order n. This allows to achieve different goals and swarming\nbehaviours: the system remains connected but flexible, hence can maneuver in\nenvironments that are replete with obstacles and narrow passages, etc.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 22:14:46 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Manor", "Rotem", ""], ["Barel", "Ariel", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1903.09658", "submitter": "William Bentz", "authors": "William Bentz and Dimitra Panagou", "title": "A Hybrid Approach to Persistent Coverage in Stochastic Environments", "comments": "Extended version of work conditionally accepted for publication in\n  Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the persistent coverage of a 2-D manifold that has been\nembedded in 3-D space. The manifold is subject to continual impact by intruders\nwhich travel at constant velocities along arbitrarily oriented straight-line\ntrajectories. The trajectories of intruders are estimated online with an\nextended Kalman filter and their predicted impact points contribute normally\ndistributed decay terms to the coverage level. A formal hybrid control strategy\nis presented that allows for power-constrained 3-D free-flyer agents to\npersistently monitor the domain, track and intercept intruders, and\nperiodically deploy from and return to a single charging station on the\nmanifold. Guarantees on intruder interception with respect to agent power\nlifespans are formally proven. The efficacy of the algorithm is demonstrated\nthrough simulation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 18:06:23 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 22:38:56 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Bentz", "William", ""], ["Panagou", "Dimitra", ""]]}, {"id": "1903.10545", "submitter": "Ahmad Beirami", "authors": "Yunqi Zhao, Igor Borovikov, Fernando de Mesentier Silva, Ahmad\n  Beirami, Jason Rupert, Caedmon Somers, Jesse Harder, John Kolen, Jervis\n  Pinto, Reza Pourabolghasem, James Pestrak, Harold Chaput, Mohsen Sardari,\n  Long Lin, Sundeep Narravula, Navid Aghdaie, Kazi Zaman", "title": "Winning Isn't Everything: Enhancing Game Development with Intelligent\n  Agents", "comments": "Accepted to IEEE Trans. Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been several high-profile achievements of agents\nlearning to play games against humans and beat them. In this paper, we study\nthe problem of training intelligent agents in service of game development.\nUnlike the agents built to \"beat the game\", our agents aim to produce\nhuman-like behavior to help with game evaluation and balancing. We discuss two\nfundamental metrics based on which we measure the human-likeness of agents,\nnamely skill and style, which are multi-faceted concepts with practical\nimplications outlined in this paper. We report four case studies in which the\nstyle and skill requirements inform the choice of algorithms and metrics used\nto train agents; ranging from A* search to state-of-the-art deep reinforcement\nlearning. We, further, show that the learning potential of state-of-the-art\ndeep RL models does not seamlessly transfer from the benchmark environments to\ntarget ones without heavily tuning their hyperparameters, leading to linear\nscaling of the engineering efforts and computational cost with the number of\ntarget domains.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:39:04 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 00:19:51 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 04:37:58 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2020 18:36:10 GMT"}, {"version": "v5", "created": "Tue, 28 Apr 2020 03:29:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhao", "Yunqi", ""], ["Borovikov", "Igor", ""], ["Silva", "Fernando de Mesentier", ""], ["Beirami", "Ahmad", ""], ["Rupert", "Jason", ""], ["Somers", "Caedmon", ""], ["Harder", "Jesse", ""], ["Kolen", "John", ""], ["Pinto", "Jervis", ""], ["Pourabolghasem", "Reza", ""], ["Pestrak", "James", ""], ["Chaput", "Harold", ""], ["Sardari", "Mohsen", ""], ["Lin", "Long", ""], ["Narravula", "Sundeep", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi", ""]]}, {"id": "1903.11041", "submitter": "Ilya Afanasyev", "authors": "Ilya Afanasyev, Alexander Kolotov, Ruslan Rezin, Konstantin Danilov,\n  Alexey Kashevnik, Vladimir Jotsov", "title": "Blockchain Solutions for Multi-Agent Robotic Systems: Related Work and\n  Open Questions", "comments": "5 pages, FRUCT-2019 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibilities of decentralization and immutability make blockchain\nprobably one of the most breakthrough and promising technological innovations\nin recent years. This paper presents an overview, analysis, and classification\nof possible blockchain solutions for practical tasks facing multi-agent robotic\nsystems. The paper discusses blockchain-based applications that demonstrate how\ndistributed ledger can be used to extend the existing number of research\nplatforms and libraries for multi-agent robotic systems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:39:46 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Afanasyev", "Ilya", ""], ["Kolotov", "Alexander", ""], ["Rezin", "Ruslan", ""], ["Danilov", "Konstantin", ""], ["Kashevnik", "Alexey", ""], ["Jotsov", "Vladimir", ""]]}, {"id": "1903.12086", "submitter": "Salar Safarkhani", "authors": "Salar Safarkhani, Ilias Bilionis, Jitesh Panchal", "title": "Towards a Theory of Systems Engineering Processes: A Principal-Agent\n  Model of a One-Shot, Shallow Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems engineering processes coordinate the effort of different individuals\nto generate a product satisfying certain requirements. As the involved\nengineers are self-interested agents, the goals at different levels of the\nsystems engineering hierarchy may deviate from the system-level goals which may\ncause budget and schedule overruns. Therefore, there is a need of a systems\nengineering theory that accounts for the human behavior in systems design. To\nthis end, the objective of this paper is to develop and analyze a\nprincipal-agent model of a one-shot (single iteration), shallow (one level of\nhierarchy) systems engineering process. We assume that the systems engineer\nmaximizes the expected utility of the system, while the subsystem engineers\nseek to maximize their expected utilities. Furthermore, the systems engineer is\nunable to monitor the effort of the subsystem engineer and may not have a\ncomplete information about their types or the complexity of the design task.\nHowever, the systems engineer can incentivize the subsystem engineers by\nproposing specific contracts. To obtain an optimal incentive, we pose and solve\nnumerically a bi-level optimization problem. Through extensive simulations, we\nstudy the optimal incentives arising from different system-level value\nfunctions under various combinations of effort costs, problem-solving skills,\nand task complexities.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:09:20 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 18:35:40 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Safarkhani", "Salar", ""], ["Bilionis", "Ilias", ""], ["Panchal", "Jitesh", ""]]}, {"id": "1903.12118", "submitter": "Mar\\'ia Santos", "authors": "Mar\\'ia Santos and Magnus Egerstedt", "title": "From Motions to Emotions: Can the Fundamental Emotions be Expressed in a\n  Robot Swarm?", "comments": "13 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper explores the expressive capabilities of a swarm of miniature\nmobile robots within the context of inter-robot interactions and their mapping\nto the so-called fundamental emotions. In particular, we investigate how motion\nand shape descriptors that are psychologically associated with different\nemotions can be incorporated into different swarm behaviors for the purpose of\nartistic expositions. Based on these characterizations from social psychology,\na set of swarm behaviors is created, where each behavior corresponds to a\nfundamental emotion. The effectiveness of these behaviors is evaluated in a\nsurvey in which the participants are asked to associate different swarm\nbehaviors with the fundamental emotions. The results of the survey show that\nmost of the research participants assigned to each video the emotion intended\nto be portrayed by design. These results confirm that abstract descriptors\nassociated with the different fundamental emotions in social psychology provide\nuseful motion characterizations that can be effectively transformed into\nexpressive behaviors for a swarm of simple ground mobile robots.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:03:15 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 19:42:47 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Santos", "Mar\u00eda", ""], ["Egerstedt", "Magnus", ""]]}, {"id": "1903.12411", "submitter": "Cedric Buron", "authors": "C\\'edric Buron, Zahia Guessoum (SMA), Sylvain Ductor (UECE)", "title": "MCTS-based Automated Negotiation Agent (Extended Abstract)", "comments": null, "journal-ref": "AAMAS 2019, May 2019, Montreal, Canada", "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new Negotiating Agent for automated negotiation on\ncontinuous domains and without considering a specified deadline. The agent\nbidding strategy relies on Monte Carlo Tree Search, which is a trendy method\nsince it has been used with success on games with high branching factor such as\nGo. It uses two opponent modeling techniques for its bidding strategy and its\nutility: Gaussian process regression and Bayesian learning. Evaluation is done\nby confronting the existing agents that are able to negotiate in such context:\nRandom Walker, Tit-for-tat and Nice Tit-for-Tat. None of those agents succeeds\nin beating our agent; moreover the modular and adaptive nature of our approach\nis a huge advantage when it comes to optimize it in specific applicative\ncontexts.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 09:21:53 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Buron", "C\u00e9dric", "", "SMA"], ["Guessoum", "Zahia", "", "SMA"], ["Ductor", "Sylvain", "", "UECE"]]}]