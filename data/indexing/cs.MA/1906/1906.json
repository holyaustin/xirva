[{"id": "1906.00110", "submitter": "Laura Schmid", "authors": "Krishnendu Chatterjee, Laura Schmid, Stefan Schmid", "title": "The Evolutionary Price of Anarchy: Locally Bounded Agents in a Dynamic\n  Virus Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Price of Anarchy (PoA) is a well-established game-theoretic concept to\nshed light on coordination issues arising in open distributed systems. Leaving\nagents to selfishly optimize comes with the risk of ending up in sub-optimal\nstates (in terms of performance and/or costs), compared to a centralized system\ndesign. However, the PoA relies on strong assumptions about agents' rationality\n(e.g., resources and information) and interactions, whereas in many distributed\nsystems agents interact locally with bounded resources. They do so repeatedly\nover time (in contrast to \"one-shot games\"), and their strategies may evolve.\nUsing a more realistic evolutionary game model, this paper introduces a\nrealized evolutionary Price of Anarchy (ePoA). The ePoA allows an exploration\nof equilibrium selection in dynamic distributed systems with multiple\nequilibria, based on local interactions of simple memoryless agents.\nConsidering a fundamental game related to virus propagation on networks, we\npresent analytical bounds on the ePoA in basic network topologies and for\ndifferent strategy update dynamics. In particular, deriving stationary\ndistributions of the stochastic evolutionary process, we find that the Nash\nequilibria are not always the most abundant states, and that different\nprocesses can feature significant off-equilibrium behavior, leading to a\nsignificantly higher ePoA compared to the PoA studied traditionally in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:39:14 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Schmid", "Laura", ""], ["Schmid", "Stefan", ""]]}, {"id": "1906.00182", "submitter": "Jie Zhang", "authors": "Yansong Gao and Jie Zhang", "title": "Average-case Analysis of the Assignment Problem with Independent\n  Preferences", "comments": "To appear in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental assignment problem is in search of welfare maximization\nmechanisms to allocate items to agents when the private preferences over\nindivisible items are provided by self-interested agents. The mainstream\nmechanism \\textit{Random Priority} is asymptotically the best mechanism for\nthis purpose, when comparing its welfare to the optimal social welfare using\nthe canonical \\textit{worst-case approximation ratio}. Despite its popularity,\nthe efficiency loss indicated by the worst-case ratio does not have a constant\nbound. Recently, [Deng, Gao, Zhang 2017] show that when the agents' preferences\nare drawn from a uniform distribution, its \\textit{average-case approximation\nratio} is upper bounded by 3.718. They left it as an open question of whether a\nconstant ratio holds for general scenarios. In this paper, we offer an\naffirmative answer to this question by showing that the ratio is bounded by\n$1/\\mu$ when the preference values are independent and identically distributed\nrandom variables, where $\\mu$ is the expectation of the value distribution.\nThis upper bound also improves the upper bound of 3.718 in [Deng, Gao, Zhang\n2017] for the Uniform distribution. Moreover, under mild conditions, the ratio\nhas a \\textit{constant} bound for any independent random values. En route to\nthese results, we develop powerful tools to show the insights that in most\ninstances the efficiency loss is small.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 08:29:24 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Gao", "Yansong", ""], ["Zhang", "Jie", ""]]}, {"id": "1906.00401", "submitter": "Ayush Datta", "authors": "Ayush Datta, Rahul Tallamraju and Kamalakar Karlapalem", "title": "Multiple Drones driven Hexagonally Partitioned Area Exploration:\n  Simulation and Evaluation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we simulated a distributed, cooperative path planning\ntechnique for multiple drones (~200) to explore an unknown region (~10,000\nconnected units) in the presence of obstacles. The map of an unknown region is\ndynamically created based on the information obtained from sensors and other\ndrones. The unknown area is considered a connected region made up of hexagonal\nunit cells. These cells are grouped to form larger cells called sub-areas. We\nuse long range and short range communication. The short-range communication\nwithin drones in smaller proximity helps avoid re-exploration of cells already\nexplored by companion drones located in the same subarea. The long-range\ncommunication helps drones identify next subarea to be targeted based on\nweighted RNN (Reverse nearest neighbor). Simulation results show that weighted\nRNN in a hexagonal representation makes exploration more efficient, scalable\nand resilient to communication failures.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 13:30:18 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 05:06:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Datta", "Ayush", ""], ["Tallamraju", "Rahul", ""], ["Karlapalem", "Kamalakar", ""]]}, {"id": "1906.00580", "submitter": "Hongyu Zang", "authors": "Hongyu Zang and Xiaojun Wan", "title": "Massive Styles Transfer with Limited Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language style transfer has attracted more and more attention in the past few\nyears. Recent researches focus on improving neural models targeting at\ntransferring from one style to the other with labeled data. However,\ntransferring across multiple styles is often very useful in real-life\napplications. Previous researches of language style transfer have two main\ndeficiencies: dependency on massive labeled data and neglect of mutual\ninfluence among different style transfer tasks. In this paper, we propose a\nmulti-agent style transfer system (MAST) for addressing multiple style transfer\ntasks with limited labeled data, by leveraging abundant unlabeled data and the\nmutual benefit among the multiple styles. A style transfer agent in our system\nnot only learns from unlabeled data by using techniques like denoising\nauto-encoder and back-translation, but also learns to cooperate with other\nstyle transfer agents in a self-organization manner. We conduct our experiments\nby simulating a set of real-world style transfer tasks with multiple versions\nof the Bible. Our model significantly outperforms the other competitive\nmethods. Extensive results and analysis further verify the efficacy of our\nproposed system.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 05:27:05 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Zang", "Hongyu", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1906.00772", "submitter": "Oscar J. Romero", "authors": "Oscar J. Romero", "title": "Dynamic Service Composition Orchestrated by Cognitive Agents in Mobile &\n  Pervasive Computing", "comments": "This paper will appear on SCC'19 (IEEE International Conference on\n  Services Computing) on July 13. arXiv admin note: substantial text overlap\n  with arXiv:1905.12630", "journal-ref": "2019 IEEE World Congress on Services (SERVICES)", "doi": "10.1109/SERVICES.2019.00118", "report-no": null, "categories": "cs.SE cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic service composition in mobile and pervasive computing faces many\nchallenges due to the complex nature of the environment. Common approaches\naddress service composition from optimization perspectives which are not\nfeasible in practice due to the intractability of the problem, limited\ncomputational resources of smart devices, service host's mobility, and time\nconstraints. Our main contribution is the development of a cognitively-inspired\nagent-based service composition model focused on bounded rationality rather\nthan optimality, which allows the system to compensate for limited resources by\nselectively filtering out continuous streams of data. The evaluation of our\napproach shows promising results when compared against state-of-the-art service\ncomposition models.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:45:18 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Romero", "Oscar J.", ""]]}, {"id": "1906.00854", "submitter": "Philip Brown", "authors": "Philip N. Brown", "title": "Designing for Emergent Security in Heterogeneous Human-Machine Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work seeks to design decisionmaking rules for autonomous agents to\njointly influence and optimize the behavior of teamed human decisionmakers in\nthe presence of an adversary. We study a situation in which computational jobs\nare scheduled on servers by a collection of autonomous machines in concert with\nself-interested human decisionmakers, and the human and machine schedulers must\nreact to an adversary's attack on one of the servers. We show a simple machine\nscheduling policy such that if all schedulers have permission to schedule jobs\non all servers, increasing the penetration of machine schedulers always\nincreases the level of security in the system, even when the machine schedulers\nhave no explicit coordination or communication amongst themselves. However, we\nshow a companion result in which simple constraints on server availability can\nnullify the machine schedulers' ability to effectively influence human\nschedulers; here, even if machine schedulers control an overwhelming majority\nof jobs, are socially-aware, and fully coordinated amongst themselves, they are\nincapable of influencing human decisionmakers to mitigate the harm of an\nattack.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:06:55 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Brown", "Philip N.", ""]]}, {"id": "1906.01202", "submitter": "Sumit Kumar", "authors": "Akshat Agarwal, Sumit Kumar and Katia Sycara", "title": "Learning Transferable Cooperative Behavior in Multi-Agent Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While multi-agent interactions can be naturally modeled as a graph, the\nenvironment has traditionally been considered as a black box. We propose to\ncreate a shared agent-entity graph, where agents and environmental entities\nform vertices, and edges exist between the vertices which can communicate with\neach other. Agents learn to cooperate by exchanging messages along the edges of\nthis graph. Our proposed multi-agent reinforcement learning framework is\ninvariant to the number of agents or entities present in the system as well as\npermutation invariance, both of which are desirable properties for any\nmulti-agent system representation. We present state-of-the-art results on\ncoverage, formation and line control tasks for multi-agent teams in a fully\ndecentralized framework and further show that the learned policies quickly\ntransfer to scenarios with different team sizes along with strong zero-shot\ngeneralization performance. This is an important step towards developing\nmulti-agent teams which can be realistically deployed in the real world without\nassuming complete prior knowledge or instantaneous communication at unbounded\ndistances.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:36:43 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Agarwal", "Akshat", ""], ["Kumar", "Sumit", ""], ["Sycara", "Katia", ""]]}, {"id": "1906.01470", "submitter": "Alexander Vezhnevets", "authors": "Alexander Sasha Vezhnevets, Yuhuai Wu, Remi Leblond, Joel Z. Leibo", "title": "Options as responses: Grounding behavioural hierarchies in multi-agent\n  RL", "comments": "First two authors contributed equally", "journal-ref": "International Conference on Machine Learning 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates generalisation in multi-agent games, where the\ngenerality of the agent can be evaluated by playing against opponents it hasn't\nseen during training. We propose two new games with concealed information and\ncomplex, non-transitive reward structure (think rock/paper/scissors). It turns\nout that most current deep reinforcement learning methods fail to efficiently\nexplore the strategy space, thus learning policies that generalise poorly to\nunseen opponents. We then propose a novel hierarchical agent architecture,\nwhere the hierarchy is grounded in the game-theoretic structure of the game --\nthe top level chooses strategic responses to opponents, while the low level\nimplements them into policy over primitive actions. This grounding facilitates\ncredit assignment across the levels of hierarchy. Our experiments show that the\nproposed hierarchical agent is capable of generalisation to unseen opponents,\nwhile conventional baselines fail to generalise whatsoever.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:18:47 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 15:10:59 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 13:31:16 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vezhnevets", "Alexander Sasha", ""], ["Wu", "Yuhuai", ""], ["Leblond", "Remi", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "1906.01957", "submitter": "Anthony Chen", "authors": "Anthony Chen, John Harwell, Maria Gini", "title": "Maximizing Energy Battery Efficiency in Swarm Robotics", "comments": "Presented as ARMS Workshop paper at AAMAS 2019 Conference\n  (http://u.cs.biu.ac.il/~agmon/arms2019/program.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Miniaturization and cost, two of the main attractive factors of swarm\nrobotics, have motivated its use as a solution in object collecting tasks,\nsearch & rescue missions, and other applications. However, in the current\nliterature only a few papers consider energy allocation efficiency within a\nswarm. Generally, robots recharge to their maximum level every time\nunconditionally, and do not incorporate estimates of the energy needed for\ntheir next task. In this paper we present an energy efficiency maximization\nmethod that minimizes the overall energy cost within a swarm while\nsimultaneously maximizing swarm performance on an object gathering task. The\nmethod utilizes dynamic thresholds for upper and lower battery limits. This\nmethod has also shown to improve the efficiency of existing energy management\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 11:52:50 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Anthony", ""], ["Harwell", "John", ""], ["Gini", "Maria", ""]]}, {"id": "1906.01983", "submitter": "Mark Ho", "authors": "Mark K. Ho and Joanna Korman and Thomas L. Griffiths", "title": "The Computational Structure of Unintentional Meaning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Speech-acts can have literal meaning as well as pragmatic meaning, but these\nboth involve consequences typically intended by a speaker. Speech-acts can also\nhave unintentional meaning, in which what is conveyed goes above and beyond\nwhat was intended. Here, we present a Bayesian analysis of how, to a listener,\nthe meaning of an utterance can significantly differ from a speaker's intended\nmeaning. Our model emphasizes how comprehending the intentional and\nunintentional meaning of speech-acts requires listeners to engage in\nsophisticated model-based perspective-taking and reasoning about the history of\nthe state of the world, each other's actions, and each other's observations. To\ntest our model, we have human participants make judgments about vignettes where\nspeakers make utterances that could be interpreted as intentional insults or\nunintentional faux pas. In elucidating the mechanics of speech-acts with\nunintentional meanings, our account provides insight into how communication\nboth functions and malfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:26:36 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ho", "Mark K.", ""], ["Korman", "Joanna", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1906.02010", "submitter": "Sujit Khanna", "authors": "Sujit Pramod Khanna and Alexander Ororbia II", "title": "A Hybrid Algorithm for Metaheuristic Optimization", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, flexible algorithm for combining together\nmetaheuristicoptimizers for non-convex optimization problems. Our approach\ntreatsthe constituent optimizers as a team of complex agents that\ncommunicateinformation amongst each other at various intervals during the\nsimulationprocess. The information produced by each individual agent can be\ncombinedin various ways via higher-level operators. In our experiments on\nkeybenchmark functions, we investigate how the performance of our\nalgorithmvaries with respect to several of its key modifiable properties.\nFinally,we apply our proposed algorithm to classification problems involving\ntheoptimization of support-vector machine classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 10:45:58 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Khanna", "Sujit Pramod", ""], ["Ororbia", "Alexander", "II"]]}, {"id": "1906.02330", "submitter": "Max Kleiman-Weiner", "authors": "Jack Serrino, Max Kleiman-Weiner, David C. Parkes, Joshua B. Tenenbaum", "title": "Finding Friend and Foe in Multi-Agent Games", "comments": "Jack Serrino and Max Kleiman-Weiner contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota,\nhave seen great strides in recent years. Yet none of these games address the\nreal-life challenge of cooperation in the presence of unknown and uncertain\nteammates. This challenge is a key game mechanism in hidden role games. Here we\ndevelop the DeepRole algorithm, a multi-agent reinforcement learning agent that\nwe test on The Resistance: Avalon, the most popular hidden role game. DeepRole\ncombines counterfactual regret minimization (CFR) with deep value networks\ntrained through self-play. Our algorithm integrates deductive reasoning into\nvector-form CFR to reason about joint beliefs and deduce partially observable\nactions. We augment deep value networks with constraints that yield\ninterpretable representations of win probabilities. These innovations enable\nDeepRole to scale to the full Avalon game. Empirical game-theoretic methods\nshow that DeepRole outperforms other hand-crafted and learned agents in\nfive-player Avalon. DeepRole played with and against human players on the web\nin hybrid human-agent teams. We find that DeepRole outperforms human players as\nboth a cooperator and a competitor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:07:27 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Serrino", "Jack", ""], ["Kleiman-Weiner", "Max", ""], ["Parkes", "David C.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1906.02403", "submitter": "Fushan Li", "authors": "Fushan Li, Michael Bowling", "title": "Ease-of-Teaching and Language Structure from Emergent Communication", "comments": "Accepted at Neural Information Processing Systems (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial agents have been shown to learn to communicate when needed to\ncomplete a cooperative task. Some level of language structure (e.g.,\ncompositionality) has been found in the learned communication protocols. This\nobserved structure is often the result of specific environmental pressures\nduring training. By introducing new agents periodically to replace old ones,\nsequentially and within a population, we explore such a new pressure -- ease of\nteaching -- and show its impact on the structure of the resulting language.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 03:59:37 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 22:51:26 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Li", "Fushan", ""], ["Bowling", "Michael", ""]]}, {"id": "1906.02649", "submitter": "Ping Xu", "authors": "Ping Xu, Cameron Nowzari, and Zhi Tian", "title": "A Class of Distributed Event-Triggered Average Consensus Algorithms for\n  Multi-Agent Systems", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a class of distributed event-triggered algorithms that\nsolve the average consensus problem in multi-agent systems. By designing events\nsuch that a specifically chosen Lyapunov function is monotonically decreasing,\nevent-triggered algorithms succeed in reducing communications among agents\nwhile still ensuring that the entire system converges to the desired state.\nHowever, depending on the chosen Lyapunov function the transient behaviors can\nbe very different. Moreover, performance requirements also vary from\napplication to application. Consequently, we are instead interested in\nconsidering a class of Lyapunov functions such that each Lyapunov function\nproduces a different event-triggered coordination algorithm to solve the\nmulti-agent average consensus problem. The proposed class of algorithms all\nguarantee exponential convergence of the resulting system and exclusion of Zeno\nbehaviors. This allows us to easily implement different algorithms that all\nguarantee correctness to meet varying performance needs. We show that our\nfindings can be applied to the practical clock synchronization problem in\nwireless sensor networks (WSNs) and further corroborate their effectiveness\nwith simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 15:30:11 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 02:05:17 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Xu", "Ping", ""], ["Nowzari", "Cameron", ""], ["Tian", "Zhi", ""]]}, {"id": "1906.02702", "submitter": "Shi Pu", "authors": "Shi Pu, Alex Olshevsky, Ioannis Ch. Paschalidis", "title": "A Sharp Estimate on the Transient Time of Distributed Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with minimizing the average of $n$ cost functions\nover a network in which agents may communicate and exchange information with\neach other. We consider the setting where only noisy gradient information is\navailable. To solve the problem, we study the distributed stochastic gradient\ndescent (DSGD) method and perform a non-asymptotic convergence analysis. For\nstrongly convex and smooth objective functions, DSGD asymptotically achieves\nthe optimal network independent convergence rate compared to centralized\nstochastic gradient descent (SGD). Our main contribution is to characterize the\ntransient time needed for DSGD to approach the asymptotic convergence rate,\nwhich we show behaves as $K_T=\\mathcal{O}\\left(\\frac{n}{(1-\\rho_w)^2}\\right)$,\nwhere $1-\\rho_w$ denotes the spectral gap of the mixing matrix. Moreover, we\nconstruct a \"hard\" optimization problem for which we show the transient time\nneeded for DSGD to approach the asymptotic convergence rate is lower bounded by\n$\\Omega \\left(\\frac{n}{(1-\\rho_w)^2} \\right)$, implying the sharpness of the\nobtained result. Numerical experiments demonstrate the tightness of the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 16:57:42 GMT"}, {"version": "v10", "created": "Tue, 3 Mar 2020 02:27:07 GMT"}, {"version": "v11", "created": "Sat, 30 Jan 2021 01:31:32 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 02:29:22 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 15:36:12 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 16:51:40 GMT"}, {"version": "v5", "created": "Tue, 23 Jul 2019 15:04:31 GMT"}, {"version": "v6", "created": "Fri, 26 Jul 2019 19:17:53 GMT"}, {"version": "v7", "created": "Fri, 11 Oct 2019 14:24:59 GMT"}, {"version": "v8", "created": "Thu, 6 Feb 2020 05:43:06 GMT"}, {"version": "v9", "created": "Wed, 12 Feb 2020 12:08:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Pu", "Shi", ""], ["Olshevsky", "Alex", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1906.02775", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Christian Kroer", "title": "Fair Division Without Disparate Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dividing items between individuals in a way that\nis fair both in the sense of distributional fairness and in the sense of not\nhaving disparate impact across protected classes. An important existing\nmechanism for distributionally fair division is competitive equilibrium from\nequal incomes (CEEI). Unfortunately, CEEI will not, in general, respect\ndisparate impact constraints. We consider two types of disparate impact\nmeasures: requiring that allocations be similar across protected classes and\nrequiring that average utility levels be similar across protected classes. We\nmodify the standard CEEI algorithm in two ways: equitable equilibrium from\nequal incomes, which removes disparate impact in allocations, and competitive\nequilibrium from equitable incomes which removes disparate impact in attained\nutility levels. We show analytically that removing disparate impact in outcomes\nbreaks several of CEEI's desirable properties such as envy, regret, Pareto\noptimality, and incentive compatibility. By contrast, we can remove disparate\nimpact in attained utility levels without affecting these properties. Finally,\nwe experimentally evaluate the tradeoffs between efficiency, equity, and\ndisparate impact in a recommender-system based market.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 18:56:17 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Kroer", "Christian", ""]]}, {"id": "1906.03037", "submitter": "Vidyasagar Sadhu", "authors": "Vidyasagar Sadhu, Gabriel Salles-Loustau, Dario Pompili, Saman Zonouz,\n  Vincent Sritapan", "title": "Argus: Smartphone-enabled Human Cooperation via Multi-Agent\n  Reinforcement Learning for Disaster Situational Awareness", "comments": null, "journal-ref": "2016 IEEE International Conference on Autonomic Computing (ICAC),\n  Wurzburg, 2016, pp. 251-256", "doi": "10.1109/ICAC.2016.43", "report-no": null, "categories": "cs.CY cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argus exploits a Multi-Agent Reinforcement Learning (MARL) framework to\ncreate a 3D mapping of the disaster scene using agents present around the\nincident zone to facilitate the rescue operations. The agents can be both human\nbystanders at the disaster scene as well as drones or robots that can assist\nthe humans. The agents are involved in capturing the images of the scene using\ntheir smartphones (or on-board cameras in case of drones) as directed by the\nMARL algorithm. These images are used to build real time a 3D map of the\ndisaster scene. Via both simulations and real experiments, an evaluation of the\nframework in terms of effectiveness in tracking random dynamicity of the\nenvironment is presented.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 02:16:32 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Sadhu", "Vidyasagar", ""], ["Salles-Loustau", "Gabriel", ""], ["Pompili", "Dario", ""], ["Zonouz", "Saman", ""], ["Sritapan", "Vincent", ""]]}, {"id": "1906.03394", "submitter": "Jiyao Li", "authors": "Jiyao Li, Vicki H. Allan", "title": "A Ride-Matching Strategy For Large Scale Dynamic Ridesharing Services\n  Based on Polar Coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a challenging problem of how to pool multiple\nride-share trip requests in real time under an uncertain environment. The goals\nare better performance metrics of efficiency and acceptable satisfaction of\nriders. To solve the problem effectively, an objective function that\ncompromises the benefits and losses of dynamic ridesharing service is proposed.\nThe Polar Coordinates based Ride-Matching strategy (PCRM) that can adapt to the\nsatisfaction of riders on board is also addressed. In the experiment, large\nscale data sets from New York City (NYC) are applied. We do a case study to\nidentify the best set of parameters of the dynamic ridesharing service with a\ntraining set of 135,252 trip requests. In addition, we also use a testing set\ncontaining 427,799 trip requests and two state-of-the-art approaches as\nbaselines to estimate the effectiveness of our method. The experimental results\nshow that on average 38% of traveling distance can be saved, nearly 100% of\npassengers can be served and each rider only spends an additional 3.8 minutes\nin ridesharing trips compared to single rider service.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 05:50:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Jiyao", ""], ["Allan", "Vicki H.", ""]]}, {"id": "1906.03458", "submitter": "Dominik Baumann", "authors": "Dominik Baumann, Fabian Mager, Marco Zimmerling, Sebastian Trimpe", "title": "Control-guided Communication: Efficient Resource Arbitration and\n  Allocation in Multi-hop Wireless Control Systems", "comments": "Accepted final version to appear in: IEEE Control Systems Letters", "journal-ref": null, "doi": "10.1109/LCSYS.2019.2922188", "report-no": null, "categories": "cs.SY cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In future autonomous systems, wireless multi-hop communication is key to\nenable collaboration among distributed agents at low cost and high flexibility.\nWhen many agents need to transmit information over the same wireless network,\ncommunication becomes a shared and contested resource. Event-triggered and\nself-triggered control account for this by transmitting data only when needed,\nenabling significant energy savings. However, a solution that brings those\nbenefits to multi-hop networks and can reallocate freed up bandwidth to\nadditional agents or data sources is still missing. To fill this gap, we\npropose control-guided communication, a novel co-design approach for\ndistributed self-triggered control over wireless multi-hop networks. The\ncontrol system informs the communication system of its transmission demands\nahead of time, and the communication system allocates resources accordingly.\nExperiments on a cyber-physical testbed show that multiple cart-poles can be\nsynchronized over wireless, while serving other traffic when resources are\navailable, or saving energy. These experiments are the first to demonstrate and\nevaluate distributed self-triggered control over low-power multi-hop wireless\nnetworks at update rates of tens of milliseconds.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 14:00:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Baumann", "Dominik", ""], ["Mager", "Fabian", ""], ["Zimmerling", "Marco", ""], ["Trimpe", "Sebastian", ""]]}, {"id": "1906.03595", "submitter": "Rajagopal A", "authors": "Rajagopal. A, Nirmala. V", "title": "Federated AI lets a team imagine together: Federated Learning of GANs", "comments": "Keywords. Artificial Intelligence, Distributed Machine Learning,\n  Generative Deep Learning, Generative Adversarial Networks, Federated\n  learning, Creative AI, AI based Collaboration, AI planning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envisioning a new imaginative idea together is a popular human need.\nImagining together as a team can often lead to breakthrough ideas, but the\ncollaboration effort can also be challenging, especially when the team members\nare separated by time and space. What if there is a AI that can assist the team\nto collaboratively envision new ideas?. Is it possible to develop a working\nmodel of such an AI? This paper aims to design such an intelligence. This paper\nproposes a approach to design a creative and collaborative intelligence by\nemploying a form of distributed machine learning approach called Federated\nLearning along with fusion on Generative Adversarial Networks, GAN. This\ncollaborative creative AI presents a new paradigm in AI, one that lets a team\nof two or more to come together to imagine and envision ideas that synergies\nwell with interests of all members of the team. In short, this paper explores\nthe design of a novel type of AI paradigm, called Federated AI Imagination, one\nthat lets geographically distributed teams to collaboratively imagine.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 08:44:23 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["A", "Rajagopal.", ""], ["V", "Nirmala.", ""]]}, {"id": "1906.03623", "submitter": "Seyed Amir Alavi", "authors": "Seyed Amir Alavi, Kamyar Mehran, Yang Hao, Ardavan Rahimian, Hamed\n  Mirsaeedi, Vahid Vahidinasab", "title": "A Distributed Event-Triggered Control Strategy for DC Microgrids Based\n  on Publish-Subscribe Model Over Industrial Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSG.2018.2856893", "report-no": null, "categories": "eess.SP cs.DC cs.MA cs.NI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a complete design, analysis, and performance evaluation\nof a novel distributed event-triggered control and estimation strategy for DC\nmicrogrids. The primary objective of this work is to efficiently stabilize the\ngrid voltage, and to further balance the energy level of the energy storage\n(ES) systems. The locally-installed distributed controllers are utilised to\nreduce the number of transmitted packets and battery usage of the installed\nsensors, based on a proposed event-triggered communication scheme. Also, to\nreduce the network traffic, an optimal observer is employed which utilizes a\nmodified Kalman consensus filter (KCF) to estimate the state of the DC\nmicrogrid via the distributed sensors. Furthermore, in order to effectively\nprovide an intelligent data exchange mechanism for the proposed event-triggered\ncontroller, the publish-subscribe communication model is employed to setup a\ndistributed control infrastructure in industrial wireless sensor networks\n(WSNs). The performance of the proposed control and estimation strategy is\nvalidated via the simulations of a DC microgrid composed of renewable energy\nsources (RESs). The results confirm the appropriateness of the implemented\nstrategy for the optimal utilization of the advanced industrial network\narchitectures in the smart grids.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 11:45:40 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Alavi", "Seyed Amir", ""], ["Mehran", "Kamyar", ""], ["Hao", "Yang", ""], ["Rahimian", "Ardavan", ""], ["Mirsaeedi", "Hamed", ""], ["Vahidinasab", "Vahid", ""]]}, {"id": "1906.04585", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Joshua Romoff, Nicolas Ballas, Joelle Pineau, Michael\n  Rabbat", "title": "Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems (2019)\n  13299-13309", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-simulator training has contributed to the recent success of Deep\nReinforcement Learning by stabilizing learning and allowing for higher training\nthroughputs. We propose Gossip-based Actor-Learner Architectures (GALA) where\nseveral actor-learners (such as A2C agents) are organized in a peer-to-peer\ncommunication topology, and exchange information through asynchronous gossip in\norder to take advantage of a large number of distributed simulators. We prove\nthat GALA agents remain within an epsilon-ball of one-another during training\nwhen using loosely coupled asynchronous communication. By reducing the amount\nof synchronization between agents, GALA is more computationally efficient and\nscalable compared to A2C, its fully-synchronous counterpart. GALA also\noutperforms A2C, being more robust and sample efficient. We show that we can\nrun several loosely coupled GALA agents in parallel on a single GPU and achieve\nsignificantly higher hardware utilization and frame-rates than vanilla A2C at\ncomparable power draws.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 16:15:43 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 23:56:00 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Assran", "Mahmoud", ""], ["Romoff", "Joshua", ""], ["Ballas", "Nicolas", ""], ["Pineau", "Joelle", ""], ["Rabbat", "Michael", ""]]}, {"id": "1906.04656", "submitter": "Maria Lombardi", "authors": "Maria Lombardi, Davide Liuzza, Mario di Bernardo", "title": "Deep learning control of artificial avatars in group coordination tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many joint-action scenarios, humans and robots have to coordinate their\nmovements to accomplish a given shared task. Lifting an object together, sawing\na wood log, transferring objects from a point to another are all examples where\nmotor coordination between humans and machines is a crucial requirement. While\nthe dyadic coordination between a human and a robot has been studied in\nprevious investigations, the multi-agent scenario in which a robot has to be\nintegrated into a human group still remains a less explored field of research.\nIn this paper we discuss how to synthesise an artificial agent able to\ncoordinate its motion in human ensembles. Driven by a control architecture\nbased on deep reinforcement learning, such an artificial agent will be able to\nautonomously move itself in order to synchronise its motion with that of the\ngroup while exhibiting human-like kinematic features. As a paradigmatic\ncoordination task we take a group version of the so-called mirror-game which is\nhighlighted as a good benchmark in the human movement literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:36:18 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Lombardi", "Maria", ""], ["Liuzza", "Davide", ""], ["di Bernardo", "Mario", ""]]}, {"id": "1906.04737", "submitter": "Georgios Papoudakis", "authors": "Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, Stefano V.\n  Albrecht", "title": "Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in deep reinforcement learning are concerned with\ncreating decision-making agents which can perform well in various complex\ndomains. A particular approach which has received increasing attention is\nmulti-agent reinforcement learning, in which multiple agents learn concurrently\nto coordinate their actions. In such multi-agent environments, additional\nlearning problems arise due to the continually changing decision-making\npolicies of agents. This paper surveys recent works that address the\nnon-stationarity problem in multi-agent deep reinforcement learning. The\nsurveyed methods range from modifications in the training procedure, such as\ncentralized training, to learning representations of the opponent's policy,\nmeta-learning, communication, and decentralized learning. The survey concludes\nwith a list of open problems and possible lines of future research.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 09:42:00 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Papoudakis", "Georgios", ""], ["Christianos", "Filippos", ""], ["Rahman", "Arrasy", ""], ["Albrecht", "Stefano V.", ""]]}, {"id": "1906.05283", "submitter": "Carlos E. Budde", "authors": "Jaime Arias, Carlos E. Budde, Wojciech Penczek, Laure Petrucci,\n  Mari\\\"elle Stoelinga", "title": "Hackers vs. Security: Attack-Defence Trees as Asynchronous Multi-Agent\n  Systems", "comments": "This work was partially funded by the NWO project SEQUOIA (grant\n  15474), EU project SUCCESS (102112) and the PHC van Gogh PAMPAS. The work of\n  Arias and Petrucci has been supported by the BQR project AMoJAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.FL cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Attack-Defence Trees (ADTs) are well-suited to assess possible attacks to\nsystems and the efficiency of counter-measures. In this paper, we first enrich\nthe available constructs with reactive patterns that cover further security\nscenarios, and equip all constructs with attributes such as time and cost to\nallow quantitative analyses. Then, ADTs are modelled as (an extension of)\nAsynchronous Multi-Agents Systems--EAMAS. The ADT-EAMAS transformation is\nperformed in a systematic manner that ensures correctness. The transformation\nallows us to quantify the impact of different agents configurations on metrics\nsuch as attack time. Using EAMAS also permits parametric verification: we\nderive constraints for property satisfaction. Our approach is exercised on\nseveral case studies using the Uppaal and IMITATOR tools.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:27:58 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Arias", "Jaime", ""], ["Budde", "Carlos E.", ""], ["Penczek", "Wojciech", ""], ["Petrucci", "Laure", ""], ["Stoelinga", "Mari\u00eblle", ""]]}, {"id": "1906.05363", "submitter": "Lydia T. Liu", "authors": "Lydia T. Liu, Horia Mania, Michael I. Jordan", "title": "Competing Bandits in Matching Markets", "comments": "15 pages, 3 figures. A version appears in the Proceedings of The 23nd\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable matching, a classical model for two-sided markets, has long been\nstudied with little consideration for how each side's preferences are learned.\nWith the advent of massive online markets powered by data-driven matching\nplatforms, it has become necessary to better understand the interplay between\nlearning and market objectives. We propose a statistical learning model in\nwhich one side of the market does not have a priori knowledge about its\npreferences for the other side and is required to learn these from stochastic\nrewards. Our model extends the standard multi-armed bandits framework to\nmultiple players, with the added feature that arms have preferences over\nplayers. We study both centralized and decentralized approaches to this problem\nand show surprising exploration-exploitation trade-offs compared to the single\nplayer multi-armed bandits setting.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 20:04:25 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 21:48:30 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Lydia T.", ""], ["Mania", "Horia", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1906.05616", "submitter": "Thomas Kent", "authors": "Thomas E. Kent and Arthur G. Richards", "title": "Decentralised Multi-Demic Evolutionary Approach to the Dynamic\n  Multi-Agent Travelling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Travelling Salesman and its variations are some of the most well known NP\nhard optimisation problems. This paper looks to use both centralised and\ndecentralised implementations of Evolutionary Algorithms (EA) to solve a\ndynamic variant of the Multi-Agent Travelling Salesman Problem (MATSP). The\nproblem is dynamic, requiring an on-line solution, whereby tasks are completed\nduring simulation with new tasks added and completed ones removed. The problem\nis allocating an active set of tasks to a set of agents whilst simultaneously\nplanning the route for each agent. The allocation and routing are closely\ncoupled parts of the same problem making it difficult to decompose, instead\nthis paper uses multiple populations with well defined interactions to exploit\nthe problem structure. This work attempts to align the real world\nimplementation demands of a decentralised solution, where agents are far apart\nand have communication limits, to that of the structure of the multi-demic EA\nsolution process, ultimately allowing decentralised parts of the problem to be\nsolved `on board' agents and allow for robust communication and exchange of\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 11:49:44 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kent", "Thomas E.", ""], ["Richards", "Arthur G.", ""]]}, {"id": "1906.05793", "submitter": "Juan Afanador", "authors": "Juan Afanador, Maria Araujo, Murilo Baptista, Nir Oren", "title": "Extending Eigentrust with the Max-Plus Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigentrust is a simple and widely used algorithm, which quantifies trust\nbased on the repeated application of an update matrix to a vector of initial\ntrust values. In some cases, however, this procedure is rendered uninformative.\nHere, we characterise such situations and trace their origin to the algebraic\nconditions guaranteeing the convergence of the Power Method. We overcome the\nidentified limitations by extending Eigentrust's core ideas into the Max-Plus\nAlgebra. The empirical evaluation of our max-plus approach demonstrates\nimprovements over Eigentrust.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:28:02 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Afanador", "Juan", ""], ["Araujo", "Maria", ""], ["Baptista", "Murilo", ""], ["Oren", "Nir", ""]]}, {"id": "1906.06047", "submitter": "Rasmus Kr{\\ae}mmer Rendsvig", "authors": "Andr\\'es Occhipinti Liberman, Andreas Achen, Rasmus Kr{\\ae}mmer\n  Rendsvig", "title": "Dynamic Term-Modal Logics for First-Order Epistemic Planning", "comments": null, "journal-ref": null, "doi": "10.1016/j.artint.2020.103305", "report-no": null, "categories": "cs.LO cs.AI cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical planning frameworks are built on first-order languages. The\nfirst-order expressive power is desirable for compactly representing actions\nvia schemas, and for specifying quantified conditions such as $\\neg\\exists\nx\\mathsf{blocks\\_door}(x)$. In contrast, several recent epistemic planning\nframeworks are built on propositional epistemic logic. The epistemic language\nis useful to describe planning problems involving higher-order reasoning or\nepistemic goals such as $K_{a}\\neg\\mathsf{problem}$.\n  This paper develops a first-order version of Dynamic Epistemic Logic (DEL).\nIn this framework, for example, $\\exists xK_{x}\\exists\ny\\mathsf{blocks\\_door}(y)$ is a formula. The formalism combines the strengths\nof DEL (higher-order reasoning) with those of first-order logic (lifted\nrepresentation) to model multi-agent epistemic planning. The paper introduces\nan epistemic language with a possible-worlds semantics, followed by novel\ndynamics given by first-order action models and their execution via product\nupdates. Taking advantage of the first-order machinery, epistemic action\nschemas are defined to provide compact, problem-independent domain\ndescriptions, in the spirit of PDDL.\n  Concerning metatheory, the paper defines axiomatic normal term-modal logics,\nshows a Canonical Model Theorem-like result which allows establishing\ncompleteness through frame characterization formulas, shows decidability for\nthe finite agent case, and shows a general completeness result for the dynamic\nextension by reduction axioms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 06:51:25 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 22:56:16 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liberman", "Andr\u00e9s Occhipinti", ""], ["Achen", "Andreas", ""], ["Rendsvig", "Rasmus Kr\u00e6mmer", ""]]}, {"id": "1906.06612", "submitter": "Yuanyuan Shi", "authors": "Yuanyuan Shi, Baosen Zhang", "title": "No-regret Learning in Cournot Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the convergence of no-regret learning in Cournot games\nwith continuous actions. Cournot games are the essential model for many\nsocio-economic systems, where players compete by strategically setting their\noutput quantity. We assume that players do not have full information of the\ngame and thus cannot pre-compute a Nash equilibrium. Two types of feedback are\nconsidered: one is bandit feedback and the other is gradient feedback. To study\nthe convergence of the induced sequence of play, we introduce the notion of\nconvergence in measure, and show that the players' actual sequence of action\nconverges to the unique Nash equilibrium. In addition, our results naturally\nextend the no-regret learning algorithms' time-average regret bounds to obtain\nthe final-iteration convergence rates. Together, our work presents\nsignificantly sharper convergence results for learning in games without strong\nassumptions on game property (e.g., monotonicity) and shows how exploiting the\ngame information feedback can influence the convergence rates.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 20:56:59 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 08:08:57 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 18:09:33 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Shi", "Yuanyuan", ""], ["Zhang", "Baosen", ""]]}, {"id": "1906.06863", "submitter": "Chen Dingding", "authors": "Ziyu Chen, Xingqiong Jiang, Yanchen Deng, Dingding Chen, Zhongshi He", "title": "A Generic Approach for Accelerating Belief Propagation based DCOP\n  Algorithms via A Branch-and-Bound Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation approaches, such as Max-Sum and its variants, are a kind\nof important methods to solve large-scale Distributed Constraint Optimization\nProblems (DCOPs). However, for problems with n-ary constraints, these\nalgorithms face a huge challenge since their computational complexity scales\nexponentially with the number of variables a function holds. In this paper, we\npresent a generic and easy-to-use method based on a branch-and-bound technique\nto solve the issue, called Function Decomposing and State Pruning (FDSP). We\ntheoretically prove that FDSP can provide monotonically non-increasing upper\nbounds and speed up belief propagation based DCOP algorithms without an effect\non solution quality. Also, our empirically evaluation indicates that FDSP can\nreduce 97\\% of the search space at least and effectively accelerate Max-Sum,\ncompared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 06:36:32 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 13:01:39 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Chen", "Ziyu", ""], ["Jiang", "Xingqiong", ""], ["Deng", "Yanchen", ""], ["Chen", "Dingding", ""], ["He", "Zhongshi", ""]]}, {"id": "1906.07030", "submitter": "Simon Obute O", "authors": "Simon O. Obute, Mehmet R. Dogar and Jordan H. Boyle", "title": "Simple Swarm Foraging Algorithm Based on Gradient Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm foraging is a common test case application for multi-robot systems. In\nthis paper we present a novel algorithm for controlling swarm robots with\nlimited communication range and storage capacity to efficiently search for and\nretrieve targets within an unknown environment. In our approach, robots search\nusing random walk and adjust their turn probability based on attraction and\nrepulsion signals they sense from other robots. We compared our algorithm with\nfive different variations reflecting absence or presence of attractive and/or\nrepulsive communication signals. Our results show that best performance is\nachieved when both signals are used by robots for communication. Furthermore,\nwe show through hardware experiments how the communication model we used in the\nsimulation could be realized on real robots.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 13:38:12 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Obute", "Simon O.", ""], ["Dogar", "Mehmet R.", ""], ["Boyle", "Jordan H.", ""]]}, {"id": "1906.07063", "submitter": "Linh Nguyen PhD", "authors": "Linh Nguyen and Hoc T. Nguyen", "title": "Mobility based network lifetime in wireless sensor networks: A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly emerging technologies in micro-electromechanical systems and\nwireless communications allows a mobile wireless sensor networks (MWSN) to be a\nmore and more powerful mean in many applications such as habitat and\nenvironmental monitoring, traffic observing, battlefield surveillance, smart\nhomes and smart cities. Nevertheless, due to sensor battery constraints,\nenergy-efficiently operating a MWSN is paramount importance in those\napplications; and a plethora of approaches have been proposed to elongate the\nnetwork longevity at most possible. Therefore, this paper provides a\ncomprehensive review on the developed methods that exploit mobility of sensor\nnodes and/or sink(s) to effectively maximize the lifetime of a MWSN. The survey\nsystematically classifies the algorithms into categories where the MWSN is\nequipped with mobile sensor nodes, one mobile sink or multiple mobile sinks.\nHow to drive the mobile sink(s) for energy efficiency in the network is also\nfully reviewed and reported.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 00:48:37 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 01:47:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Nguyen", "Linh", ""], ["Nguyen", "Hoc T.", ""]]}, {"id": "1906.07071", "submitter": "Alexandros A. Voudouris", "authors": "Edith Elkind, Jiarui Gan, Svetlana Obraztsova, Zinovi Rabinovich,\n  Alexandros A. Voudouris", "title": "Protecting Elections by Recounting Ballots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complexity of voting manipulation is a prominent topic in computational\nsocial choice. In this work, we consider a two-stage voting manipulation\nscenario. First, a malicious party (an attacker) attempts to manipulate the\nelection outcome in favor of a preferred candidate by changing the vote counts\nin some of the voting districts. Afterwards, another party (a defender), which\ncares about the voters' wishes, demands a recount in a subset of the\nmanipulated districts, restoring their vote counts to their original values. We\ninvestigate the resulting Stackelberg game for the case where votes are\naggregated using two variants of the Plurality rule, and obtain an almost\ncomplete picture of the complexity landscape, both from the attacker's and from\nthe defender's perspective.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 14:59:08 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Elkind", "Edith", ""], ["Gan", "Jiarui", ""], ["Obraztsova", "Svetlana", ""], ["Rabinovich", "Zinovi", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1906.07315", "submitter": "Somdeb Majumdar", "authors": "Shauharda Khadka and Somdeb Majumdar and Santiago Miret and Stephen\n  McAleer and Kagan Tumer", "title": "Evolutionary Reinforcement Learning for Sample-Efficient Multiagent\n  Coordination", "comments": "Proceedings of the 37th International Conference on Machine Learning,\n  Vienna, Austria, PMLR 108, 2020", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, Vienna, Austria, PMLR 119, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cooperative multiagent reinforcement learning environments provide\nagents with a sparse team-based reward, as well as a dense agent-specific\nreward that incentivizes learning basic skills. Training policies solely on the\nteam-based reward is often difficult due to its sparsity. Furthermore, relying\nsolely on the agent-specific reward is sub-optimal because it usually does not\ncapture the team coordination objective. A common approach is to use reward\nshaping to construct a proxy reward by combining the individual rewards.\nHowever, this requires manual tuning for each environment. We introduce\nMultiagent Evolutionary Reinforcement Learning (MERL), a split-level training\nplatform that handles the two objectives separately through two optimization\nprocesses. An evolutionary algorithm maximizes the sparse team-based objective\nthrough neuroevolution on a population of teams. Concurrently, a gradient-based\noptimizer trains policies to only maximize the dense agent-specific rewards.\nThe gradient-based policies are periodically added to the evolutionary\npopulation as a way of information transfer between the two optimization\nprocesses. This enables the evolutionary algorithm to use skills learned via\nthe agent-specific rewards toward optimizing the global objective. Results\ndemonstrate that MERL significantly outperforms state-of-the-art methods, such\nas MADDPG, on a number of difficult coordination benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:25:27 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:24:50 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 17:03:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Khadka", "Shauharda", ""], ["Majumdar", "Somdeb", ""], ["Miret", "Santiago", ""], ["McAleer", "Stephen", ""], ["Tumer", "Kagan", ""]]}, {"id": "1906.07492", "submitter": "Simon Obute O", "authors": "Simon O. Obute, Mehmet R. Dogar and Jordan H. Boyle", "title": "Chemotaxis Based Virtual Fence for Swarm Robots in Unbounded\n  Environments", "comments": "Paper accepted for living machines 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel swarm robotics application of chemotaxis\nbehaviour observed in microorganisms. This approach was used to cause\nexploration robots to return to a work area around the swarm's nest within a\nboundless environment. We investigate the performance of our algorithm through\nextensive simulation studies and hardware validation. Results show that the\nchemotaxis approach is effective for keeping the swarm close to both stationary\nand moving nests. Performance comparison of these results with the unrealistic\ncase where a boundary wall was used to keep the swarm within a target search\narea showed that our chemotaxis approach produced competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 10:57:38 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Obute", "Simon O.", ""], ["Dogar", "Mehmet R.", ""], ["Boyle", "Jordan H.", ""]]}, {"id": "1906.07588", "submitter": "Reza Vosooghi", "authors": "Reza Vosooghi (LGI, IRT SystemX), Jakob Puchinger (LGI, IRT SystemX),\n  Marija Jankovic (LGI), Anthony Vouillon", "title": "Shared Autonomous Vehicle Simulation and Service Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, driverless cars, as a new technology that allows a more accessible,\ndynamic and intelligent form of Shared Mobility, are expected to revolutionize\nurban transportation. One of the conceivable mobility services based on\ndriverless cars is shared autonomous vehicles (SAVs). This service could merge\ncabs, carsharing, and ridesharing systems into a singular transportation mode.\nHowever, the success and competitiveness of future SAV services depend on their\noperational models, which are linked intrinsically to the service configuration\nand fleet specification. In addition, any change in operational models will\nresult in a different demand. Using a comprehensive framework of SAV simulation\nin a multi-modal dynamic demand system with integrated SAV user taste\nvariation, this study evaluates the performance of various SAV fleets and\nvehicle capacities serving travelers across the Rouen Normandie metropolitan\narea in France. Also, the impact of ridesharing and rebalancing strategies on\nservice performance is investigated.Research results suggest that the\nperformance of SAV is strongly correlated with the fleet size and the strategy\nof individual or shared rides. Further analysis indicates that for the pricing\nscheme proposed in this study (i.e., 20% lower for ridesharing scenario), the\nstandard 4-seats car with shared ride remains the best option among all\nscenarios. The results also underline that enabling vehicle-rebalancing\nstrategies may have an important effect on both user and service-related\nmetrics. The estimated SAV average and maximum driven distance prove the\nimportance of vehicle range and charging station deployment.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 13:58:42 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 08:31:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Vosooghi", "Reza", "", "LGI, IRT SystemX"], ["Puchinger", "Jakob", "", "LGI, IRT SystemX"], ["Jankovic", "Marija", "", "LGI"], ["Vouillon", "Anthony", ""]]}, {"id": "1906.07967", "submitter": "Jose Javier Ramasco", "authors": "Aleix Bassolas, Riccardo Gallotti, Fabio Lamanna, Maxime Lenormand and\n  Jose J. Ramasco", "title": "Scaling in the recovery of urban transportation systems from special\n  events", "comments": "10 pages, 7 figures, plus other 10 pages of appendices", "journal-ref": "Scientific Reports 10, 2746 (2020)", "doi": "10.1038/s41598-020-59576-1", "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public transportation is a fundamental infrastructure for the daily mobility\nin cities. Although its capacity is prepared for the usual demand, congestion\nmay rise when huge crowds concentrate in special events such as massive\ndemonstrations, concerts or sport events. In this work, we study the resilience\nand recovery of public transportation networks from massive gatherings by means\nof a stylized model mimicking the mobility of individuals through the\nmultilayer transportation network. We focus on the delays produced by the\ncongestion in the trips of both event participants and of other citizens doing\ntheir usual traveling in the background. Our model can be solved analytically\nfor regular lattices showing that the average delay scales with the number of\nevent participants with an exponent equal to the inverse of the lattice\ndimension. We then switch to real transportation networks of eight worldwide\ncities, and observe that there is a whole range of exponents depending on where\nthe event is located. These exponents are distributed around 1/2, which\nindicates that most of the local structure of the network is two dimensional.\nYet, some of the exponents are below (above) that value, implying a local\ndimension higher (lower) than 2 as a consequence of the multimodality and\nmultifractality of transportation networks. In fact, these exponents can be\nalso obtained from the scaling of the capacity with the distance from the\nevent. Overall, our methodology allows to dynamically probe the local\ndimensionality of a transportation network and identify the most vulnerable\nspots in cities for the celebration of massive events.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 08:28:36 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Bassolas", "Aleix", ""], ["Gallotti", "Riccardo", ""], ["Lamanna", "Fabio", ""], ["Lenormand", "Maxime", ""], ["Ramasco", "Jose J.", ""]]}, {"id": "1906.08291", "submitter": "Hang Ma", "authors": "Roni Stern and Nathan Sturtevant and Ariel Felner and Sven Koenig and\n  Hang Ma and Thayne Walker and Jiaoyang Li and Dor Atzmon and Liron Cohen and\n  T. K. Satish Kumar and Eli Boyarski and Roman Bartak", "title": "Multi-Agent Pathfinding: Definitions, Variants, and Benchmarks", "comments": "Accepted to SoCS 2019: The 12th Annual Symposium on Combinatorial\n  Search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MAPF problem is the fundamental problem of planning paths for multiple\nagents, where the key constraint is that the agents will be able to follow\nthese paths concurrently without colliding with each other. Applications of\nMAPF include automated warehouses and autonomous vehicles. Research on MAPF has\nbeen flourishing in the past couple of years. Different MAPF research papers\nmake different assumptions, e.g., whether agents can traverse the same road at\nthe same time, and have different objective functions, e.g., minimize makespan\nor sum of agents' actions costs. These assumptions and objectives are sometimes\nimplicitly assumed or described informally. This makes it difficult to\nestablish appropriate baselines for comparison in research papers, as well as\nmaking it difficult for practitioners to find the papers relevant to their\nconcrete application. This paper aims to fill this gap and support researchers\nand practitioners by providing a unifying terminology for describing common\nMAPF assumptions and objectives. In addition, we also provide pointers to two\nMAPF benchmarks. In particular, we introduce a new grid-based benchmark for\nMAPF, and demonstrate experimentally that it poses a challenge to contemporary\nMAPF algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 18:17:14 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Stern", "Roni", ""], ["Sturtevant", "Nathan", ""], ["Felner", "Ariel", ""], ["Koenig", "Sven", ""], ["Ma", "Hang", ""], ["Walker", "Thayne", ""], ["Li", "Jiaoyang", ""], ["Atzmon", "Dor", ""], ["Cohen", "Liron", ""], ["Kumar", "T. K. Satish", ""], ["Boyarski", "Eli", ""], ["Bartak", "Roman", ""]]}, {"id": "1906.08308", "submitter": "Lane A. Hemaspaandra", "authors": "Edith Hemaspaandra, Lane A. Hemaspaandra, Joerg Rothe", "title": "The Complexity of Online Bribery in Sequential Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on the complexity of bribery assumes that the bribery happens\nsimultaneously, and that the briber has full knowledge of all voters' votes.\nBut neither of those assumptions always holds. In many real-world settings,\nvotes come in sequentially, and the briber may have a use-it-or-lose-it moment\nto decide whether to bribe/alter a given vote, and at the time of making that\ndecision, the briber may not know what votes remaining voters are planning on\ncasting.\n  In this paper, we introduce a model for, and initiate the study of, bribery\nin such an online, sequential setting. We show that even for election systems\nwhose winner-determination problem is polynomial-time computable, an online,\nsequential setting may vastly increase the complexity of bribery, in fact\njumping the problem up to completeness for high levels of the polynomial\nhierarchy or even PSPACE. On the other hand, we show that for some natural,\nimportant election systems, such a dramatic complexity increase does not occur,\nand we pinpoint the complexity of their bribery problems in the online,\nsequential setting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 19:04:09 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Hemaspaandra", "Edith", ""], ["Hemaspaandra", "Lane A.", ""], ["Rothe", "Joerg", ""]]}, {"id": "1906.09029", "submitter": "Vincenzo Matta", "authors": "Augusto Santos, Vincenzo Matta, and Ali H. Sayed", "title": "Topology Inference over Networks with Nonlinear Coupling", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem of topology inference over discrete-time\nnonlinear stochastic networked dynamical systems. The goal is to recover the\nunderlying digraph linking the network agents, from observations of their\nstate-evolution. The dynamical law governing the state-evolution of the\ninteracting agents might be nonlinear, i.e., the next state of an agent can\ndepend nonlinearly on its current state and on the states of its immediate\nneighbors. We establish sufficient conditions that allow consistent graph\nlearning over a special class of networked systems, namely, logistic-type\ndynamical systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 09:44:29 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Santos", "Augusto", ""], ["Matta", "Vincenzo", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1906.09248", "submitter": "Selim Ickin", "authors": "Selim Ickin, Konstantinos Vandikas, Markus Fiedler", "title": "Privacy Preserving QoE Modeling using Collaborative Learning", "comments": "6 pages, 4 figures, 7 tables, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning based Quality of Experience (QoE) models potentially suffer\nfrom over-fitting due to limitations including low data volume, and limited\nparticipant profiles. This prevents models from becoming generic. Consequently,\nthese trained models may under-perform when tested outside the experimented\npopulation. One reason for the limited datasets, which we refer in this paper\nas small QoE data lakes, is due to the fact that often these datasets\npotentially contain user sensitive information and are only collected\nthroughout expensive user studies with special user consent. Thus, sharing of\ndatasets amongst researchers is often not allowed. In recent years, privacy\npreserving machine learning models have become important and so have techniques\nthat enable model training without sharing datasets but instead relying on\nsecure communication protocols. Following this trend, in this paper, we present\nRound-Robin based Collaborative Machine Learning model training, where the\nmodel is trained in a sequential manner amongst the collaborated partner nodes.\nWe benchmark this work using our customized Federated Learning mechanism as\nwell as conventional Centralized and Isolated Learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 17:14:53 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 13:25:59 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Ickin", "Selim", ""], ["Vandikas", "Konstantinos", ""], ["Fiedler", "Markus", ""]]}, {"id": "1906.09591", "submitter": "Luigi Freda", "authors": "Luigi Freda, Mario Gianni, Fiora Pirri, Abel Gawel, Renaud Dube,\n  Roland Siegwart, Cesar Cadena", "title": "3D Multi-Robot Patrolling with a Two-Level Coordination Strategy", "comments": null, "journal-ref": null, "doi": "10.1007/s10514-018-09822-3", "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teams of UGVs patrolling harsh and complex 3D environments can experience\ninterference and spatial conflicts with one another. Neglecting the occurrence\nof these events crucially hinders both soundness and reliability of a\npatrolling process. This work presents a distributed multi-robot patrolling\ntechnique, which uses a two-level coordination strategy to minimize and\nexplicitly manage the occurrence of conflicts and interference. The first level\nguides the agents to single out exclusive target nodes on a topological map.\nThis target selection relies on a shared idleness representation and a\ncoordination mechanism preventing topological conflicts. The second level hosts\ncoordination strategies based on a metric representation of space and is\nsupported by a 3D SLAM system. Here, each robot path planner negotiates spatial\nconflicts by applying a multi-robot traversability function. Continuous\ninteractions between these two levels ensure coordination and conflicts\nresolution. Both simulations and real-world experiments are presented to\nvalidate the performances of the proposed patrolling strategy in 3D\nenvironments. Results show this is a promising solution for managing spatial\nconflicts and preventing deadlocks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 12:51:05 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Freda", "Luigi", ""], ["Gianni", "Mario", ""], ["Pirri", "Fiora", ""], ["Gawel", "Abel", ""], ["Dube", "Renaud", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1906.09788", "submitter": "Wenchao Ding", "authors": "Wenchao Ding and Lu Zhang and Jing Chen and Shaojie Shen", "title": "Safe Trajectory Generation for Complex Urban Environments Using\n  Spatio-temporal Semantic Corridor", "comments": "Accepted by IEEE Robotics and Automation Letters (IEEE RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning safe trajectories for autonomous vehicles in complex urban\nenvironments is challenging since there are numerous semantic elements (such as\ndynamic agents, traffic lights and speed limits) to consider. These semantic\nelements may have different mathematical descriptions such as obstacle,\nconstraint and cost. It is non-trivial to tune the effects from different\ncombinations of semantic elements for a stable and generalizable behavior. In\nthis paper, we propose a novel unified spatio-temporal semantic corridor (SSC)\nstructure, which provides a level of abstraction for different types of\nsemantic elements. The SSC consists of a series of mutually connected\ncollision-free cubes with dynamical constraints posed by the semantic elements\nin the spatio-temporal domain. The trajectory generation problem then boils\ndown to a general quadratic programming (QP) formulation. Thanks to the unified\nSSC representation, our framework can generalize to any combination of semantic\nelements. Moreover, our formulation provides a theoretical guarantee that the\nentire trajectory is safe and constraint-satisfied, by using the convex hull\nand hodograph properties of piecewise Bezier curve parameterization. We also\nrelease the code of our method to accommodate benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:49:17 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ding", "Wenchao", ""], ["Zhang", "Lu", ""], ["Chen", "Jing", ""], ["Shen", "Shaojie", ""]]}, {"id": "1906.09874", "submitter": "William Long", "authors": "William Long", "title": "Escaping the State of Nature: A Hobbesian Approach to Cooperation in\n  Multi-agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation is a phenomenon that has been widely studied across many\ndifferent disciplines. In the field of computer science, the modularity and\nrobustness of multi-agent systems offer significant practical advantages over\nindividual machines. At the same time, agents using standard reinforcement\nlearning algorithms often fail to achieve long-term, cooperative strategies in\nunstable environments when there are short-term incentives to defect. Political\nphilosophy, on the other hand, studies the evolution of cooperation in humans\nwho face similar incentives to act individualistically, but nevertheless\nsucceed in forming societies. Thomas Hobbes in Leviathan provides the classic\nanalysis of the transition from a pre-social State of Nature, where consistent\ndefection results in a constant state of war, to stable political community\nthrough the institution of an absolute Sovereign. This thesis argues that\nHobbes's natural and moral philosophy are strikingly applicable to artificially\nintelligent agents and aims to show that his political solutions are\nexperimentally successful in producing cooperation among modified Q-Learning\nagents. Cooperative play is achieved in a novel Sequential Social Dilemma\ncalled the Civilization Game, which models the State of Nature by introducing\nthe Hobbesian mechanisms of opponent learning awareness and majoritarian\nvoting, leading to the establishment of a Sovereign.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:44:36 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Long", "William", ""]]}, {"id": "1906.10124", "submitter": "Ahmad Beirami", "authors": "Yunqi Zhao and Igor Borovikov and Jason Rupert and Caedmon Somers and\n  Ahmad Beirami", "title": "On Multi-Agent Learning in Team Sports Games", "comments": "Presented at ICML 2019 Workshop on Imitation, Intent, and Interaction\n  (I3). arXiv admin note: substantial text overlap with arXiv:1903.10545", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, reinforcement learning has been successful in solving video\ngames from Atari to Star Craft II. However, the end-to-end model-free\nreinforcement learning (RL) is not sample efficient and requires a significant\namount of computational resources to achieve superhuman level performance.\nModel-free RL is also unlikely to produce human-like agents for playtesting and\ngameplaying AI in the development cycle of complex video games. In this paper,\nwe present a hierarchical approach to training agents with the goal of\nachieving human-like style and high skill level in team sports games. While\nthis is still work in progress, our preliminary results show that the presented\napproach holds promise for solving the posed multi-agent learning problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:18:10 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhao", "Yunqi", ""], ["Borovikov", "Igor", ""], ["Rupert", "Jason", ""], ["Somers", "Caedmon", ""], ["Beirami", "Ahmad", ""]]}, {"id": "1906.10165", "submitter": "Mark Woodward", "authors": "Mark Woodward and Chelsea Finn and Karol Hausman", "title": "Training an Interactive Helper", "comments": "The paper \"Learning to Interactively Learn and Assist\" (LILA), at\n  arXiv:1906.10187, supersedes this paper. This preliminary workshop paper\n  appeared in the Emergent Communication Workshop and Workshop on Learning by\n  Instruction at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing agents that can quickly adapt their behavior to new tasks remains\na challenge. Meta-learning has been applied to this problem, but previous\nmethods require either specifying a reward function which can be tedious or\nproviding demonstrations which can be inefficient. In this paper, we\ninvestigate if, and how, a \"helper\" agent can be trained to interactively adapt\ntheir behavior to maximize the reward of another agent, whom we call the\n\"prime\" agent, without observing their reward or receiving explicit\ndemonstrations. To this end, we propose to meta-learn a helper agent along with\na prime agent, who, during training, observes the reward function and serves as\na surrogate for a human prime. We introduce a distribution of multi-agent\ncooperative foraging tasks, in which only the prime agent knows the objects\nthat should be collected. We demonstrate that, from the emerged physical\ncommunication, the trained helper rapidly infers and collects the correct\nobjects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:37:33 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 02:49:14 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Woodward", "Mark", ""], ["Finn", "Chelsea", ""], ["Hausman", "Karol", ""]]}, {"id": "1906.10187", "submitter": "Mark Woodward", "authors": "Mark Woodward and Chelsea Finn and Karol Hausman", "title": "Learning to Interactively Learn and Assist", "comments": "AAAI 2020. Video overview at https://youtu.be/8yBvDBuAPrw, paper\n  website with videos and interactive game at\n  http://interactive-learning.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deploying autonomous agents in the real world, we need effective ways of\ncommunicating objectives to them. Traditional skill learning has revolved\naround reinforcement and imitation learning, each with rigid constraints on the\nformat of information exchanged between the human and the agent. While scalar\nrewards carry little information, demonstrations require significant effort to\nprovide and may carry more information than is necessary. Furthermore, rewards\nand demonstrations are often defined and collected before training begins, when\nthe human is most uncertain about what information would help the agent. In\ncontrast, when humans communicate objectives with each other, they make use of\na large vocabulary of informative behaviors, including non-verbal\ncommunication, and often communicate throughout learning, responding to\nobserved behavior. In this way, humans communicate intent with minimal effort.\nIn this paper, we propose such interactive learning as an alternative to reward\nor demonstration-driven learning. To accomplish this, we introduce a\nmulti-agent training framework that enables an agent to learn from another\nagent who knows the current task. Through a series of experiments, we\ndemonstrate the emergence of a variety of interactive learning behaviors,\nincluding information-sharing, information-seeking, and question-answering.\nMost importantly, we find that our approach produces an agent that is capable\nof learning interactively from a human user, without a set of explicit\ndemonstrations or a reward function, and achieving significantly better\nperformance cooperatively with a human than a human performing the task alone.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:23:27 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 23:07:37 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 21:04:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Woodward", "Mark", ""], ["Finn", "Chelsea", ""], ["Hausman", "Karol", ""]]}, {"id": "1906.10250", "submitter": "Nicolas Maudet", "authors": "Aur\\'elie Beynier, Nicolas Maudet, Simon Rey, and Parham Shams", "title": "Swap Dynamics in Single-Peaked Housing Markets", "comments": "Replaces our previous submission: \"House Markets and Single-Peaked\n  Preferences: From Centralized to Decentralized Allocation Procedures\".\n  Following reviewers' comments, leaves out our contribution on a variant of\n  the Crawler procedure (goes in a separate submission) to concentrate on swap\n  dynamics (new results added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of fairly and efficiently allocating\nresources to agents. We consider a specific setting, usually referred to as a\nhousing market, where each agent must receive exactly one resource (and\ninitially owns one). In this framework, in the domain of linear preferences,\nthe Top Trading Cycle (TTC) algorithm is the only procedure satisfying\nPareto-optimality, individual rationality and strategy-proofness. Under the\nrestriction of single-peaked preferences, Crawler enjoys the same properties.\nThese two centralized procedures might however involve long trading cycles. In\nthis paper we focus instead on procedures involving the shortest cycles:\nbilateral swap-deals. In such swap dynamics, the agents perform pairwise\nmutually improving deals until reaching a swap-stable allocation (no improving\nswap-deal is possible). We prove that in the single-peaked domain every\nswap-stable allocation is Pareto-optimal, showing the efficiency of the swap\ndynamics. In fact, this domain turns out to be maximal when it comes to\nguaranteeing this property. Besides, both the outcome of TTC and Crawler can\nalways be reached by sequences of swaps. However, some Pareto-optimal\nallocations are not reachable through improving swap-deals. We further analyze\nthe outcome of swap dynamics through social welfare notions, in our context the\naverage or minimum rank of the resources obtained by agents in the final\nallocation. We start by providing a worst-case analysis of these procedures.\nFinally, we present an extensive experimental study in which different versions\nof swap dynamics are compared to other existing allocation procedures. We show\nthat they exhibit good results on average in this domain, under different\ncultures for generating synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:12:01 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 12:38:34 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 13:55:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Beynier", "Aur\u00e9lie", ""], ["Maudet", "Nicolas", ""], ["Rey", "Simon", ""], ["Shams", "Parham", ""]]}, {"id": "1906.11064", "submitter": "Stefano Albrecht", "authors": "Stefano V. Albrecht, Peter Stone", "title": "Reasoning about Hypothetical Agent Behaviours and their Parameters", "comments": "Proceedings of the 16th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents can achieve effective interaction with previously unknown other agents\nby maintaining beliefs over a set of hypothetical behaviours, or types, that\nthese agents may have. A current limitation in this method is that it does not\nrecognise parameters within type specifications, because types are viewed as\nblackbox mappings from interaction histories to probability distributions over\nactions. In this work, we propose a general method which allows an agent to\nreason about both the relative likelihood of types and the values of any\nbounded continuous parameters within types. The method maintains individual\nparameter estimates for each type and selectively updates the estimates for\nsome types after each observation. We propose different methods for the\nselection of types and the estimation of parameter values. The proposed methods\nare evaluated in detailed experiments, showing that updating the parameter\nestimates of a single type after each observation can be sufficient to achieve\ngood performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:00:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Albrecht", "Stefano V.", ""], ["Stone", "Peter", ""]]}, {"id": "1906.11286", "submitter": "Baihan Lin", "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, Irina\n  Rish", "title": "A Story of Two Streams: Reinforcement Learning Models from Human\n  Behavior and Neuropsychiatry", "comments": "Published in AAMAS 2020 as a full paper. This article supersedes our\n  work arXiv:1706.02897 into RL setting and extends extensively into RL games,\n  cognitive modeling, and gambling tasks in lifelong learning setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing an inspiration from behavioral studies of human decision making, we\npropose here a more general and flexible parametric framework for reinforcement\nlearning that extends standard Q-learning to a two-stream model for processing\npositive and negative rewards, and allows to incorporate a wide range of\nreward-processing biases -- an important component of human decision making\nwhich can help us better understand a wide spectrum of multi-agent interactions\nin complex real-world socioeconomic systems, as well as various\nneuropsychiatric conditions associated with disruptions in normal reward\nprocessing. From the computational perspective, we observe that the proposed\nSplit-QL model and its clinically inspired variants consistently outperform\nstandard Q-Learning and SARSA methods, as well as recently proposed Double\nQ-Learning approaches, on simulated tasks with particular reward distributions,\na real-world dataset capturing human decision-making in gambling tasks, and the\nPac-Man game in a lifelong learning setting across different reward\nstationarities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 03:31:37 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:14:11 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 04:34:06 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 19:21:21 GMT"}, {"version": "v5", "created": "Wed, 26 Feb 2020 18:56:09 GMT"}, {"version": "v6", "created": "Tue, 10 Mar 2020 20:52:24 GMT"}, {"version": "v7", "created": "Tue, 14 Apr 2020 17:26:49 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Lin", "Baihan", ""], ["Cecchi", "Guillermo", ""], ["Bouneffouf", "Djallel", ""], ["Reinen", "Jenna", ""], ["Rish", "Irina", ""]]}, {"id": "1906.11362", "submitter": "Hossein Rastgoftar", "authors": "Hossein Rastgoftar", "title": "Interactive Physics-Inspired Traffic Congestion Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new physics-based approach to effectively control\ncongestion in a network of interconnected roads (NOIR). The paper integrates\nmass flow conservation and diffusion-based dynamics to model traffic\ncoordination in a NOIR. The mass conservation law is used to model the traffic\ndensity dynamics across the NOIR while the diffusion law is applied to include\ntraffic speed and motion direction into planning. This paper offers an analogy\nbetween traffic coordination in a transportation system and heat flux in a\nthermal system to define a potential filed over the NOIR. The paper also\ndevelops an interactive light-based and boundary control to manage traffic\ncongestion through optimizing the traffic signal operations and controlling\ntraffic flows at the NOIR boundary nodes. More specifically, a model predictive\nboundary control optimizes the NOIR inflow traffic while a receding horizon\noptimizer assigns the optimal movement phases at the NOIR intersections. For\nsimulation, the paper models traffic congestion in a heterogeneous NOIR with a\nlarge number of interior and boundary nodes where the proposed interactive\ncontrol can successfully manage the congestion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 22:13:10 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 04:04:59 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Rastgoftar", "Hossein", ""]]}, {"id": "1906.11452", "submitter": "Pulkit Verma", "authors": "Yahnit Sirineni, Pulkit Verma, Kamalakar Karlapalem", "title": "Traffic Management Strategies for Multi-Robotic Rigid Payload Transport\n  Systems", "comments": "7 Pages, Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems, Jun 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address traffic management of multiple payload transport\nsystems comprising of non-holonomic robots. We consider loosely coupled rigid\nrobot formations carrying a payload from one place to another. Each payload\ntransport system (PTS) moves in various kinds of environments with obstacles.\nWe ensure each PTS completes its given task by avoiding collisions with other\npayload systems and obstacles as well. Each PTS has one leader and multiple\nfollowers and the followers maintain a desired distance and angle with respect\nto the leader using a decentralized leader-follower control architecture while\nmoving in the traffic. We showcase, through simulations the time taken by each\nPTS to traverse its respective trajectory with and without other PTS and\nobstacles. We show that our strategies help manage the traffic for a large\nnumber of PTS moving from one place to another.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 06:31:29 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Sirineni", "Yahnit", ""], ["Verma", "Pulkit", ""], ["Karlapalem", "Kamalakar", ""]]}, {"id": "1906.11614", "submitter": "Maksym Figat", "authors": "Maksym Figat and Cezary Zieli\\'nski", "title": "Methodology of Designing Multi-agent Robot Control Systems Utilising\n  Hierarchical Petri Nets", "comments": "IEEE International Conference on Robotics and Automation, ICRA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot system is designed as a set of embodied agents. An embodied agent is\ndecomposed into cooperating subsystems. In our previous work activities of\nsubsystems were defined by hierarchical finite state machines. With their\nstates, activities were associated. In that approach communication between\nsubsystems was treated as an implementation issue. This paper represents the\nactivities of a robot system using hierarchical Petri nets with conditions.\nSuch net is created by specifying consecutive layers: multi-agent robot system\nlayer, agent layer, subsystem layer, behaviour layer and communication layer.\nThis decomposition not only organizes in a systematic manner the development of\na robot system but also introduces a comprehensive description of concurrently\nacting subsystems. Based on those theoretical considerations, a tool was\ncreated for producing hierarchical Petri nets defining the model of a robotic\nsystem and enabling automatic generation of the robot controller code,\nresulting in a significant acceleration of the implementation phase. The\ncapabilities of the tool are presented by the development of a robot controller\nperforming a rudimentary task.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:21:38 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 23:48:11 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Figat", "Maksym", ""], ["Zieli\u0144ski", "Cezary", ""]]}, {"id": "1906.12110", "submitter": "Vee-Liem Saw", "authors": "Vee-Liem Saw and Lock Yue Chew", "title": "No-boarding buses: Agents allowed to cooperate or defect", "comments": "26 pages, 7 figures, accepted for publication by Journal of Physics:\n  Complexity", "journal-ref": "Journal of Physics: Complexity 1, 015005 (2020)", "doi": "10.1088/2632-072X/ab4af5", "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a bus system with a no-boarding policy, where a \"slow\" bus may\ndisallow passengers from boarding if it meets some criteria. When the\nno-boarding policy is activated, people waiting to board at the bus stop are\ngiven the choices of \\emph{cooperating} or \\emph{defecting}. The people's\nheterogeneous behaviours are modelled by inductive reasoning and bounded\nrationality, inspired by the El Farol problem and the minority game. In\ndefecting the no-boarding policy, instead of the minority group being the\nwinning group, we investigate several scenarios where defectors win if the\nnumber of defectors does not exceed the maximum number of allowed defectors but\nlose otherwise. Contrary to the classical minority game which has $N$ agents\nrepeatedly playing amongst themselves, many real-world situations like boarding\na bus involves only a subset of agents who \"play each round\", with\n\\emph{different subsets playing at different rounds}. We find for such\nrealistic situations, there is no phase transition with no herding behaviour\nwhen the usual control paramater $2^m/N$ is small. The absence of the herding\nbehaviour assures feasible and sustainable implementation of the no-boarding\npolicy with allowance for defections, without leading to bus bunching.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:26:31 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 08:54:13 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Saw", "Vee-Liem", ""], ["Chew", "Lock Yue", ""]]}, {"id": "1906.12250", "submitter": "Roula Nassif", "authors": "Roula Nassif, Stefan Vlaski, Ali H. Sayed", "title": "Adaptation and learning over networks under subspace constraints -- Part\n  II: Performance Analysis", "comments": "arXiv admin note: text overlap with arXiv:1905.08750", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part I of this paper considered optimization problems over networks where\nagents have individual objectives to meet, or individual parameter vectors to\nestimate, subject to subspace constraints that require the objectives across\nthe network to lie in low-dimensional subspaces. Starting from the centralized\nprojected gradient descent, an iterative and distributed solution was proposed\nthat responds to streaming data and employs stochastic approximations in place\nof actual gradient vectors, which are generally unavailable. We examined the\nsecond-order stability of the learning algorithm and we showed that, for small\nstep-sizes $\\mu$, the proposed strategy leads to small estimation errors on the\norder of $\\mu$. This Part II examines steady-state performance. The results\nreveal explicitly the influence of the gradient noise, data characteristics,\nand subspace constraints, on the network performance. The results also show\nthat in the small step-size regime, the iterates generated by the distributed\nalgorithm achieve the centralized steady-state performance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:05:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Nassif", "Roula", ""], ["Vlaski", "Stefan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1906.12345", "submitter": "Shi Pu", "authors": "Shi Pu, Alex Olshevsky, Ioannis Ch. Paschalidis", "title": "Asymptotic Network Independence in Distributed Stochastic Optimization\n  for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a discussion of several recent results which, in certain\nscenarios, are able to overcome a barrier in distributed stochastic\noptimization for machine learning. Our focus is the so-called asymptotic\nnetwork independence property, which is achieved whenever a distributed method\nexecuted over a network of n nodes asymptotically converges to the optimal\nsolution at a comparable rate to a centralized method with the same\ncomputational power as the entire network. We explain this property through an\nexample involving the training of ML models and sketch a short mathematical\nanalysis for comparing the performance of distributed stochastic gradient\ndescent (DSGD) with centralized stochastic gradient decent (SGD).\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:55:19 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 15:15:16 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 19:00:47 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2019 07:42:02 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 06:58:00 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Pu", "Shi", ""], ["Olshevsky", "Alex", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1906.12350", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi", "title": "Split Q Learning: Reinforcement Learning with Two-Stream Rewards", "comments": "IJCAI 2019. This article supersedes our work arXiv:1706.02897 into RL\n  setting, with a different focus by applying Inverse Reinforcement Learning to\n  model human clinical behavioral bias. It also precedes our work\n  arXiv:1906.11286 which introduces extensive emphases in RL games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing an inspiration from behavioral studies of human decision making, we\npropose here a general parametric framework for a reinforcement learning\nproblem, which extends the standard Q-learning approach to incorporate a\ntwo-stream framework of reward processing with biases biologically associated\nwith several neurological and psychiatric conditions, including Parkinson's and\nAlzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD),\naddiction, and chronic pain. For AI community, the development of agents that\nreact differently to different types of rewards can enable us to understand a\nwide spectrum of multi-agent interactions in complex real-world socioeconomic\nsystems. Moreover, from the behavioral modeling perspective, our parametric\nframework can be viewed as a first step towards a unifying computational model\ncapturing reward processing abnormalities across multiple mental conditions and\nuser preferences in long-term recommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:59:52 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 19:10:05 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""]]}]