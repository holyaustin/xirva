[{"id": "1809.00543", "submitter": "Fabian Schilling", "authors": "Fabian Schilling, Julien Lecoeur, Fabrizio Schiano, Dario Floreano", "title": "Learning Vision-based Cohesive Flight in Drone Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven approach to learning vision-based\ncollective behavior from a simple flocking algorithm. We simulate a swarm of\nquadrotor drones and formulate the controller as a regression problem in which\nwe generate 3D velocity commands directly from raw camera images. The dataset\nis created by simultaneously acquiring omnidirectional images and computing the\ncorresponding control command from the flocking algorithm. We show that a\nconvolutional neural network trained on the visual inputs of the drone can\nlearn not only robust collision avoidance but also coherence of the flock in a\nsample-efficient manner. The neural controller effectively learns to localize\nother agents in the visual input, which we show by visualizing the regions with\nthe most influence on the motion of an agent. This weakly supervised saliency\nmap can be computed efficiently and may be used as a prior for subsequent\ndetection and relative localization of other agents. We remove the dependence\non sharing positions among flock members by taking only local visual\ninformation into account for control. Our work can therefore be seen as the\nfirst step towards a fully decentralized, vision-based flock without the need\nfor communication or visual markers to aid detection of other agents.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 10:44:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Schilling", "Fabian", ""], ["Lecoeur", "Julien", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "1809.00564", "submitter": "Stefano Cerri", "authors": "Philippe Lemoisson (UMR TETIS), Stefano A. Cerri (SMILE)", "title": "ViewpointS: towards a Collective Brain", "comments": null, "journal-ref": "Ngoc Thanh Nguyen; Elias Pimenidis; Zaheer Khan; Bogdan\n  Trawi{\\'n}ski. ICCCI: International Conference on Computational Collective\n  Intelligence, Sep 2018, Bristol, United Kingdom. Springer Verlag, 10th\n  International Conference on Computational Collective Intelligence, LNCS\n  (11055), pp.3-12, 2018, http://www.iccci2018.org", "doi": "10.1007/978-3-319-98443-8_1", "report-no": null, "categories": "cs.MA cs.AI cs.SI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracing knowledge acquisition and linking learning events to interaction\nbetween peers is a major challenge of our times. We have conceived, designed\nand evaluated a new paradigm for constructing and using collective knowledge by\nWeb interactions that we called ViewpointS. By exploiting the similarity with\nEdelman's Theory of Neuronal Group Selection (TNGS), we conjecture that it may\nbe metaphorically considered a Collective Brain, especially effective in the\ncase of trans-disciplinary representations. Far from being without doubts, in\nthe paper we present the reasons (and the limits) of our proposal that aims to\nbecome a useful integrating tool for future quantitative explorations of\nindividual as well as collective learning at different degrees of granu-larity.\nWe are therefore challenging each of the current approaches: the logical one in\nthe semantic Web, the statistical one in mining and deep learning, the social\none in recommender systems based on authority and trust; not in each of their\nown preferred field of operation, rather in their integration weaknesses far\nfrom the holistic and dynamic behavior of the human brain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 11:51:13 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lemoisson", "Philippe", "", "UMR TETIS"], ["Cerri", "Stefano A.", "", "SMILE"]]}, {"id": "1809.00710", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Soomin Lee and Alexander Gasnikov and Angelia\n  Nedi\\'c", "title": "A Dual Approach for Optimal Algorithms in Distributed Optimization over\n  Networks", "comments": "This work is an extended version of the manuscript: Optimal\n  Algorithms for Distributed Optimization arXiv:1712.00232", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dual-based algorithms for distributed convex optimization problems\nover networks, where the objective is to minimize a sum $\\sum_{i=1}^{m}f_i(z)$\nof functions over in a network. We provide complexity bounds for four different\ncases, namely: each function $f_i$ is strongly convex and smooth, each function\nis either strongly convex or smooth, and when it is convex but neither strongly\nconvex nor smooth. Our approach is based on the dual of an appropriately\nformulated primal problem, which includes a graph that models the communication\nrestrictions. We propose distributed algorithms that achieve the same optimal\nrates as their centralized counterparts (up to constant and logarithmic\nfactors), with an additional optimal cost related to the spectral properties of\nthe network. Initially, we focus on functions for which we can explicitly\nminimize its Legendre-Fenchel conjugate, i.e., admissible or dual friendly\nfunctions. Then, we study distributed optimization algorithms for non-dual\nfriendly functions, as well as a method to improve the dependency on the\nparameters of the functions involved. Numerical analysis of the proposed\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 20:13:25 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 21:27:17 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 13:55:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Lee", "Soomin", ""], ["Gasnikov", "Alexander", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1809.00861", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "Unveiling co-evolutionary patterns in systems of cities: a systematic\n  exploration of the SimpopNet model", "comments": "13 pages, 4 figures, 1 table", "journal-ref": "In: Pumain D. (eds) Theories and Models of Urbanization (pp.\n  261-278). Lecture Notes in Morphogenesis. Springer, Cham (2020)", "doi": "10.1007/978-3-030-36656-8_14", "report-no": null, "categories": "physics.soc-ph cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-evolutionary processes are according to the evolutionary urban theory at\nthe center of urban systems dynamics. Their empirical observation or within\nmodels of simulation remains however relatively rare. This chapter is focused\non the co-evolution of transportation networks and cities and applies high\nperformance computing numerical experiments to the SimpopNet co-evolution model\nin order to understand its behavior. We introduce specific indicators to\nquantify trajectories of such models for systems of cities, and apply these to\nexhibit co-evolutionary regimes of the model. This illustrates how the\nsystematic exploration of a simulation model can qualitatively transform the\nknowledge it provides.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:42:45 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 15:14:11 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Raimbault", "Juste", ""]]}, {"id": "1809.01106", "submitter": "Ying Sun", "authors": "Gesualdo Scutari and Ying Sun", "title": "Distributed Nonconvex Constrained Optimization over Time-Varying\n  Digraphs", "comments": "Submitted June 3, 2017, revised June 5, 2108. Part of this work has\n  been presented at the 2016 Asilomar Conference on System, Signal and\n  Computers and the 2017 IEEE ICASSP Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers nonconvex distributed constrained optimization over\nnetworks, modeled as directed (possibly time-varying) graphs. We introduce the\nfirst algorithmic framework for the minimization of the sum of a smooth\nnonconvex (nonseparable) function--the agent's sum-utility--plus a\nDifference-of-Convex (DC) function (with nonsmooth convex part). This general\nformulation arises in many applications, from statistical machine learning to\nengineering. The proposed distributed method combines successive convex\napproximation techniques with a judiciously designed perturbed push-sum\nconsensus mechanism that aims to track locally the gradient of the (smooth part\nof the) sum-utility. Sublinear convergence rate is proved when a fixed\nstep-size (possibly different among the agents) is employed whereas asymptotic\nconvergence to stationary solutions is proved using a diminishing step-size.\nNumerical results show that our algorithms compare favorably with current\nschemes on both convex and nonconvex problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:19:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Scutari", "Gesualdo", ""], ["Sun", "Ying", ""]]}, {"id": "1809.03143", "submitter": "Swapnil Dhamal", "authors": "Swapnil Dhamal, Walid Ben-Ameur, Tijani Chahed, Eitan Altman, Albert\n  Sunny, Sudheer Poojary", "title": "A Stochastic Game Framework for Analyzing Computational Investment\n  Strategies in Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a stochastic game framework with dynamic set of players, for\nmodeling and analyzing their computational investment strategies in distributed\ncomputing. Players obtain a certain reward for solving the problem or for\nproviding their computational resources, while incur a certain cost based on\nthe invested time and computational power. We first study a scenario where the\nreward is offered for solving the problem, such as in blockchain mining. We\nshow that, in Markov perfect equilibrium, players with cost parameters\nexceeding a certain threshold, do not invest; while those with cost parameters\nless than this threshold, invest maximal power. Here, players need not know the\nsystem state. We then consider a scenario where the reward is offered for\ncontributing to the computational power of a common central entity, such as in\nvolunteer computing. Here, in Markov perfect equilibrium, only players with\ncost parameters in a relatively low range in a given state, invest. For the\ncase where players are homogeneous, they invest proportionally to the 'reward\nto cost' ratio. For both the scenarios, we study the effects of players'\narrival and departure rates on their utilities using simulations and provide\nadditional insights.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:43:12 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 07:18:43 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 16:43:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dhamal", "Swapnil", ""], ["Ben-Ameur", "Walid", ""], ["Chahed", "Tijani", ""], ["Altman", "Eitan", ""], ["Sunny", "Albert", ""], ["Poojary", "Sudheer", ""]]}, {"id": "1809.03656", "submitter": "Francesco Olivieri", "authors": "Francesco Olivieri, Guido Governatori, Matteo Cristani, Nick van\n  Beest, Silvano Colombo-Tosatto", "title": "Resource-driven Substructural Defeasible Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Logic and Defeasible Logic have been adopted to formalise different\nfeatures relevant to agents: consumption of resources, and reasoning with\nexceptions. We propose a framework to combine sub-structural features,\ncorresponding to the consumption of resources, with defeasibility aspects, and\nwe discuss the design choices for the framework.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:09:03 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Olivieri", "Francesco", ""], ["Governatori", "Guido", ""], ["Cristani", "Matteo", ""], ["van Beest", "Nick", ""], ["Colombo-Tosatto", "Silvano", ""]]}, {"id": "1809.03738", "submitter": "Ming Zhou", "authors": "Ming Zhou, Yong Chen, Ying Wen, Yaodong Yang, Yufeng Su, Weinan Zhang,\n  Dell Zhang, Jun Wang", "title": "Factorized Q-Learning for Large-Scale Multi-Agent Systems", "comments": "7 pages, 5 figures, DAI 2019", "journal-ref": null, "doi": "10.1145/3356464.3357707", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Q-learning has achieved significant success in single-agent decision\nmaking tasks. However, it is challenging to extend Q-learning to large-scale\nmulti-agent scenarios, due to the explosion of action space resulting from the\ncomplex dynamics between the environment and the agents. In this paper, we\npropose to make the computation of multi-agent Q-learning tractable by treating\nthe Q-function (w.r.t. state and joint-action) as a high-order high-dimensional\ntensor and then approximate it with factorized pairwise interactions.\nFurthermore, we utilize a composite deep neural network architecture for\ncomputing the factorized Q-function, share the model parameters among all the\nagents within the same group, and estimate the agents' optimal joint actions\nthrough a coordinate descent type algorithm. All these simplifications greatly\nreduce the model complexity and accelerate the learning process. Extensive\nexperiments on two different multi-agent problems demonstrate the performance\ngain of our proposed approach in comparison with strong baselines, particularly\nwhen there are a large number of agents.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:31:05 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 08:02:46 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 14:07:51 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 11:29:12 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zhou", "Ming", ""], ["Chen", "Yong", ""], ["Wen", "Ying", ""], ["Yang", "Yaodong", ""], ["Su", "Yufeng", ""], ["Zhang", "Weinan", ""], ["Zhang", "Dell", ""], ["Wang", "Jun", ""]]}, {"id": "1809.04136", "submitter": "Juntao Wang Mr", "authors": "Yiling Chen and Yang Liu and Juntao Wang", "title": "Randomized Wagering Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wagering mechanisms are one-shot betting mechanisms that elicit agents'\npredictions of an event. For deterministic wagering mechanisms, an existing\nimpossibility result has shown incompatibility of some desirable theoretical\nproperties. In particular, Pareto optimality (no profitable side bet before\nallocation) can not be achieved together with weak incentive compatibility,\nweak budget balance and individual rationality. In this paper, we expand the\ndesign space of wagering mechanisms to allow randomization and ask whether\nthere are randomized wagering mechanisms that can achieve all previously\nconsidered desirable properties, including Pareto optimality. We answer this\nquestion positively with two classes of randomized wagering mechanisms: i) one\nsimple randomized lottery-type implementation of existing deterministic\nwagering mechanisms, and ii) another family of simple and randomized wagering\nmechanisms which we call surrogate wagering mechanisms, which are robust to\nnoisy ground truth. This family of mechanisms builds on the idea of learning\nwith noisy labels (Natarajan et al. 2013) as well as a recent extension of this\nidea to the information elicitation without verification setting (Liu and Chen\n2018). We show that a broad family of randomized wagering mechanisms satisfy\nall desirable theoretical properties.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:06:03 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 03:40:24 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 16:01:29 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 06:49:37 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Yiling", ""], ["Liu", "Yang", ""], ["Wang", "Juntao", ""]]}, {"id": "1809.04230", "submitter": "Carlos Luis Goncalves", "authors": "Carlos E. Luis and Angela P. Schoellig", "title": "Trajectory Generation for Multiagent Point-To-Point Transitions via\n  Distributed Model Predictive Control", "comments": "8 pages, 7 figures", "journal-ref": "IEEE Robotics and Automation Letters, vol. 4, iss. 2, pp. 375-382,\n  2019", "doi": "10.1109/LRA.2018.2890572", "report-no": null, "categories": "cs.RO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel algorithm for multiagent offline trajectory\ngeneration based on distributed model predictive control. Central to the\nalgorithm's scalability and success is the development of an on-demand\ncollision avoidance strategy. By predicting future states and sharing this\ninformation with their neighbors, the agents are able to detect and avoid\ncollisions while moving toward their goals. The proposed algorithm can be\nimplemented in a distributed fashion and reduces the computation time by more\nthan 85% compared to previous optimization approaches based on sequential\nconvex programming, while only having a small impact on the optimality of the\nplans. The approach was validated both through extensive simulations and\nexperimentally with teams of up to 25 quadrotors flying in confined indoor\nspaces.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 02:37:27 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 15:39:57 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Luis", "Carlos E.", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1809.04240", "submitter": "Tianpei Yang", "authors": "Tianpei Yang and Zhaopeng Meng and Jianye Hao and Chongjie Zhang and\n  Yan Zheng and Ze Zheng", "title": "Towards Efficient Detection and Optimal Response against Sophisticated\n  Opponents", "comments": "Accepted to International Joint Conference on Artificial Intelligence\n  (IJCA2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent algorithms often aim to accurately predict the behaviors of other\nagents and find a best response accordingly. Previous works usually assume an\nopponent uses a stationary strategy or randomly switches among several\nstationary ones. However, an opponent may exhibit more sophisticated behaviors\nby adopting more advanced reasoning strategies, e.g., using a Bayesian\nreasoning strategy. This paper proposes a novel approach called Bayes-ToMoP\nwhich can efficiently detect the strategy of opponents using either stationary\nor higher-level reasoning strategies. Bayes-ToMoP also supports the detection\nof previously unseen policies and learning a best-response policy accordingly.\nWe provide a theoretical guarantee of the optimality on detecting the\nopponent's strategies. We also propose a deep version of Bayes-ToMoP by\nextending Bayes-ToMoP with DRL techniques. Experimental results show both\nBayes-ToMoP and deep Bayes-ToMoP outperform the state-of-the-art approaches\nwhen faced with different types of opponents in two-agent competitive games.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 03:07:01 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 07:46:44 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 08:59:47 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 02:21:15 GMT"}, {"version": "v5", "created": "Wed, 29 May 2019 05:21:39 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Yang", "Tianpei", ""], ["Meng", "Zhaopeng", ""], ["Hao", "Jianye", ""], ["Zhang", "Chongjie", ""], ["Zheng", "Yan", ""], ["Zheng", "Ze", ""]]}, {"id": "1809.04328", "submitter": "Andrea Baronchelli", "authors": "Roberta Amato, Lucas Lacasa, Albert D\\'iaz-Guilera and Andrea\n  Baronchelli", "title": "The Dynamics of Norm Change in the Cultural Evolution of Language", "comments": "Full SI available online", "journal-ref": "Proc. Natl. Acad. Sci. USA 115, 8260 (2018)", "doi": "10.1073/pnas.1721059115", "report-no": null, "categories": "physics.soc-ph cs.MA q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What happens when a new social convention replaces an old one? While the\npossible forces favoring norm change - such as institutions or committed\nactivists - have been identified since a long time, little is known about how a\npopulation adopts a new convention, due to the difficulties of finding\nrepresentative data. Here we address this issue by looking at changes occurred\nto 2,541 orthographic and lexical norms in English and Spanish through the\nanalysis of a large corpora of books published between the years 1800 and 2008.\nWe detect three markedly distinct patterns in the data, depending on whether\nthe behavioral change results from the action of a formal institution, an\ninformal authority or a spontaneous process of unregulated evolution. We\npropose a simple evolutionary model able to capture all the observed behaviors\nand we show that it reproduces quantitatively the empirical data. This work\nidentifies general mechanisms of norm change and we anticipate that it will be\nof interest to researchers investigating the cultural evolution of language\nand, more broadly, human collective behavior.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 09:31:31 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Amato", "Roberta", ""], ["Lacasa", "Lucas", ""], ["D\u00edaz-Guilera", "Albert", ""], ["Baronchelli", "Andrea", ""]]}, {"id": "1809.04500", "submitter": "Hassam Sheikh", "authors": "Hassam Ullah Sheikh and Ladislau Boloni", "title": "Emergence of Scenario-Appropriate Collaborative Behaviors for Teams of\n  Robotic Bodyguards", "comments": "Accepted for publication at AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are considering the problem of controlling a team of robotic bodyguards\nprotecting a VIP from physical assault in the presence of neutral and/or\nadversarial bystanders. This task is part of a much larger class of problems\ninvolving coordinated robot behavior in the presence of humans. This problem is\nchallenging due to the large number of active entities with different agendas,\nthe need of cooperation between the robots as well as the requirement to take\ninto consideration criteria such as social norms and unobtrusiveness in\naddition to the main goal of VIP safety. Furthermore, different settings such\nas street, public space or red carpet require very different behavior from the\nrobot. We describe how a multi-agent reinforcement learning approach can evolve\nbehavior policies for teams of robot bodyguards that compare well with\nhand-engineered approaches. Furthermore, we show that an algorithm inspired by\nuniversal value function approximators can learn policies that exhibit\nappropriate, distinct behavior in environments with different requirements.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:55:19 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:32:55 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 19:30:21 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Sheikh", "Hassam Ullah", ""], ["Boloni", "Ladislau", ""]]}, {"id": "1809.04587", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Stefano Marano", "title": "Distributed Chernoff Test: Optimal decision systems over networks", "comments": "A part of this work has been accepted in ISIT 2018 and CDC 2018;\n  Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.MA math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \"active\" decision making over sensor networks where the sensors'\nsequential probing actions are actively chosen by continuously learning from\npast observations. We consider two network settings: with and without central\ncoordination. In the first case, the network nodes interact with each other\nthrough a central entity, which plays the role of a fusion center. In the\nsecond case, the network nodes interact in a fully distributed fashion. In both\nof these scenarios, we propose sequential and adaptive hypothesis tests\nextending the classic Chernoff test. We compare the performance of the proposed\ntests to the optimal sequential test. In the presence of a fusion center, our\ntest achieves the same asymptotic optimality of the Chernoff test, minimizing\nthe risk, expressed by the expected cost required to reach a decision plus the\nexpected cost of making a wrong decision, when the observation cost per unit\ntime tends to zero. The test is also asymptotically optimal in the higher\nmoments of the time required to reach a decision. Additionally, the test is\nparsimonious in terms of communications, and the expected number of channel\nuses per network node tends to a small constant. In the distributed setup, our\ntest achieves the same asymptotic optimality of Chernoff's test, up to a\nmultiplicative constant in terms of both risk and the higher moments of the\ndecision time. Additionally, the test is parsimonious in terms of\ncommunications in comparison to state-of-the-art schemes proposed in the\nliterature. The analysis of these tests is also extended to account for message\nquantization and communication over channels with random erasures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:51:30 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 14:31:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Marano", "Stefano", ""]]}, {"id": "1809.04918", "submitter": "Sean Barton", "authors": "Sean L. Barton, Nicholas R. Waytowich, Derrik E. Asher", "title": "Coordination-driven learning in multi-agent problem spaces", "comments": "AAAI Fall Symposium 2018, Concept Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the role of coordination as a direct learning objective in\nmulti-agent reinforcement learning (MARL) domains. To this end, we present a\nnovel means of quantifying coordination in multi-agent systems, and discuss the\nimplications of using such a measure to optimize coordinated agent policies.\nThis concept has important implications for adversary-aware RL, which we take\nto be a sub-domain of multi-agent learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 12:44:48 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Barton", "Sean L.", ""], ["Waytowich", "Nicholas R.", ""], ["Asher", "Derrik E.", ""]]}, {"id": "1809.05096", "submitter": "Gregory Palmer", "authors": "Gregory Palmer, Rahul Savani, Karl Tuyls", "title": "Negative Update Intervals in Deep Multi-Agent Reinforcement Learning", "comments": "11 Pages, 6 Figures, AAMAS2019 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Multi-Agent Reinforcement Learning (MA-RL), independent cooperative\nlearners must overcome a number of pathologies to learn optimal joint policies.\nAddressing one pathology often leaves approaches vulnerable towards others. For\ninstance, hysteretic Q-learning addresses miscoordination while leaving agents\nvulnerable towards misleading stochastic rewards. Other methods, such as\nleniency, have proven more robust when dealing with multiple pathologies\nsimultaneously. However, leniency has predominately been studied within the\ncontext of strategic form games (bimatrix games) and fully observable Markov\ngames consisting of a small number of probabilistic state transitions. This\nraises the question of whether these findings scale to more complex domains.\nFor this purpose we implement a temporally extend version of the Climb Game,\nwithin which agents must overcome multiple pathologies simultaneously,\nincluding relative overgeneralisation, stochasticity, the alter-exploration and\nmoving target problems, while learning from a large observation space. We find\nthat existing lenient and hysteretic approaches fail to consistently learn near\noptimal joint-policies in this environment. To address these pathologies we\nintroduce Negative Update Intervals-DDQN (NUI-DDQN), a Deep MA-RL algorithm\nwhich discards episodes yielding cumulative rewards outside the range of\nexpanding intervals. NUI-DDQN consistently gravitates towards optimal\njoint-policies in our environment, overcoming the outlined pathologies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 09:20:12 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 09:34:03 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Palmer", "Gregory", ""], ["Savani", "Rahul", ""], ["Tuyls", "Karl", ""]]}, {"id": "1809.05188", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, Hongyuan\n  Zha", "title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement\n  Learning", "comments": "Published at International Conference on Learning Representations\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of cooperative multi-agent control problems require agents to\nachieve individual goals while contributing to collective success. This\nmulti-goal multi-agent setting poses difficulties for recent algorithms, which\nprimarily target settings with a single global reward, due to two new\nchallenges: efficient exploration for learning both individual goal attainment\nand cooperation for others' success, and credit-assignment for interactions\nbetween actions and goals of different agents. To address both challenges, we\nrestructure the problem into a novel two-stage curriculum, in which\nsingle-agent goal attainment is learned prior to learning multi-agent\ncooperation, and we derive a new multi-goal multi-agent policy gradient with a\ncredit function for localized credit assignment. We use a function augmentation\nscheme to bridge value and policy functions across the curriculum. The complete\narchitecture, called CM3, learns significantly faster than direct adaptations\nof existing algorithms on three challenging multi-goal multi-agent problems:\ncooperative navigation in difficult formations, negotiating multi-vehicle lane\nchanges in the SUMO traffic simulator, and strategic cooperation in a Checkers\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 21:46:54 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 05:04:45 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 21:24:17 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yang", "Jiachen", ""], ["Nakhaei", "Alireza", ""], ["Isele", "David", ""], ["Fujimura", "Kikuo", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1809.05245", "submitter": "Kamal Chaturvedi", "authors": "Kamal Chaturvedi, Jia Yuan Yu, Shrisha Rao", "title": "Distributed and Efficient Resource Balancing Among Many Suppliers and\n  Consumers", "comments": "6 pages, 12 figures, IEEE International Conference on Systems, Man\n  and Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Achieving a balance of supply and demand in a multi-agent system with many\nindividual self-interested and rational agents that act as suppliers and\nconsumers is a natural problem in a variety of real-life domains---smart power\ngrids, data centers, and others. In this paper, we address the\nprofit-maximization problem for a group of distributed supplier and consumer\nagents, with no inter-agent communication. We simulate a scenario of a market\nwith $S$ suppliers and $C$ consumers such that at every instant, each supplier\nagent supplies a certain quantity and simultaneously, each consumer agent\nconsumes a certain quantity. The information about the total amount supplied\nand consumed is only kept with the center. The proposed algorithm is a\ncombination of the classical additive-increase multiplicative-decrease (AIMD)\nalgorithm in conjunction with a probabilistic rule for the agents to respond to\na capacity signal. This leads to a nonhomogeneous Markov chain and we show\nalmost sure convergence of this chain to the social optimum, for our market of\ndistributed supplier and consumer agents. Employing this AIMD-type algorithm,\nthe center sends a feedback message to the agents in the supplier side if there\nis a scenario of excess supply, or to the consumer agents if there is excess\nconsumption. Each agent has a concave utility function whose derivative tends\nto 0 when an optimum quantity is supplied/consumed. Hence when social\nconvergence is reached, each agent supplies or consumes a quantity which leads\nto its individual maximum profit, without the need of any communication. So\neventually, each agent supplies or consumes a quantity which leads to its\nindividual maximum profit, without communicating with any other agents. Our\nsimulations show the efficacy of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 04:02:21 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Chaturvedi", "Kamal", ""], ["Yu", "Jia Yuan", ""], ["Rao", "Shrisha", ""]]}, {"id": "1809.05309", "submitter": "Vaishak Belle", "authors": "Vaishak Belle", "title": "On Plans With Loops and Noise", "comments": "Proceedings of AAMAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an influential paper, Levesque proposed a formal specification for\nanalysing the correctness of program-like plans, such as conditional plans,\niterative plans, and knowledge-based plans. He motivated a logical\ncharacterisation within the situation calculus that included binary sensing\nactions. While the characterisation does not immediately yield a practical\nalgorithm, the specification serves as a general skeleton to explore the\nsynthesis of program-like plans for reasonable, tractable fragments.\n  Increasingly, classical plan structures are being applied to stochastic\nenvironments such as robotics applications. This raises the question as to what\nthe specification for correctness should look like, since Levesque's account\nmakes the assumption that sensing is exact and actions are deterministic.\nBuilding on a situation calculus theory for reasoning about degrees of belief\nand noise, we revisit the execution semantics of generalised plans. The\nspecification is then used to analyse the correctness of example plans.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 08:58:49 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Belle", "Vaishak", ""]]}, {"id": "1809.05485", "submitter": "Pavel Naumov", "authors": "Pavel Naumov and Jia Tao", "title": "Blameworthiness in Strategic Games", "comments": null, "journal-ref": "33rd AAAI Conference on Artificial Intelligence (AAAI-19), January\n  27-February 1, 2019, Honolulu, Hawaii, USA", "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO cs.MA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple notions of coalitional responsibility. The focus of this\npaper is on the blameworthiness defined through the principle of alternative\npossibilities: a coalition is blamable for a statement if the statement is\ntrue, but the coalition had a strategy to prevent it. The main technical result\nis a sound and complete bimodal logical system that describes properties of\nblameworthiness in one-shot games.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:09:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Naumov", "Pavel", ""], ["Tao", "Jia", ""]]}, {"id": "1809.05897", "submitter": "Sebastian Gottwald", "authors": "Sebastian Gottwald, Daniel A. Braun", "title": "Systems of bounded rational agents with information-theoretic\n  constraints", "comments": "35 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialization and hierarchical organization are important features of\nefficient collaboration in economical, artificial, and biological systems.\nHere, we investigate the hypothesis that both features can be explained by the\nfact that each entity of such a system is limited in a certain way. We propose\nan information-theoretic approach based on a Free Energy principle, in order to\ncomputationally analyze systems of bounded rational agents that deal with such\nlimitations optimally. We find that specialization allows to focus on fewer\ntasks, thus leading to a more efficient execution, but in turn requires\ncoordination in hierarchical structures of specialized experts and coordinating\nunits. Our results suggest that hierarchical architectures of specialized units\nat lower levels that are coordinated by units at higher levels are optimal,\ngiven that each unit's information-processing capability is limited and\nconforms to constraints on complexity costs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 15:43:11 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Gottwald", "Sebastian", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1809.06027", "submitter": "Dave Cliff", "authors": "Dave Cliff", "title": "BSE: A Minimal Simulation of a Limit-Order-Book Stock Exchange", "comments": "10 pages, 6 figures. To appear in Proceedings of 30th European\n  Modelling and Simulation Symposium (EMSS-2018), Budapest, Hungary, September\n  17-19, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MA q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design, implementation, and successful use of the\nBristol Stock Exchange (BSE), a novel minimal simulation of a centralised\nfinancial market, based on a Limit Order Book (LOB) such as is common in major\nstock exchanges. Construction of BSE was motivated by the fact that most of the\nworld's major financial markets have automated, with trading activity that\npreviously was the responsibility of human traders now being performed by\nhigh-speed autonomous automated trading systems. Research aimed at\nunderstanding the dynamics of this new style of financial market is hampered by\nthe fact that no operational real-world exchange is ever likely to allow\nexperimental probing of that market while it is open and running live, forcing\nresearchers to work primarily from time-series of past trading data. Similarly,\nuniversity-level education of the engineers who can create next-generation\nautomated trading systems requires that they have hands-on learning experience\nin a sufficiently realistic teaching environment. BSE as described here\naddresses both those needs: it has been successfully used for teaching and\nresearch in a leading UK university since 2012, and the BSE program code is\nfreely available as open-source on GitHuB.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 05:30:01 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Cliff", "Dave", ""]]}, {"id": "1809.06049", "submitter": "Dmitry Rabinovich", "authors": "Dmitry Rabinovich, Alfred M. Bruckstein", "title": "Erratic Extremism causes Dynamic Consensus (a new model for\n  one-dimensional opinion dynamics)", "comments": null, "journal-ref": null, "doi": null, "report-no": "CIS-2018-02", "categories": "cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A society of agents, with ideological positions, or \"opinions\" measured by\nreal values ranging from $-\\infty$ (the \"far left\") to $+\\infty$ (the \"far\nright\"), is considered. At fixed (unit) time intervals agents repeatedly\nreconsider and change their opinions if and only if they find themselves at the\nextremes of the range of ideological positions held by members of the society.\nExtremist agents are erratic: they become either more radical, and move away\nfrom the positions of other agents, with probability $\\varepsilon$, or more\nmoderate, and move towards the positions held by peers, with probability $(1 -\n\\varepsilon)$. The change in the opinion of the extremists is one unit on the\nreal line. We prove that the agent positions cluster in time, with all\nnon-extremist agents located within a unit interval. However, the consensus\nopinion is dynamic. Due to the extremists' erratic behavior the clustered\nopinion set performs a \"sluggish\" random walk on the entire range of possible\nideological positions (the real line). The inertia of the group, the reluctance\nof the society's agents to change their consensus opinion, increases with the\nsize of the group. The extremists perform biased random walk excursions to the\nright and left and, in time, their actions succeed to move the society of\nagents in random directions. The \"far left\" agent effectively pushes the group\nconsensus toward the right, while the \"far right\" agent counter-balances the\npush and causes the consensus to move toward the left.\n  We believe that this model, and some of its variations, has the potential to\nexplain the real world swings in societal ideologies that we see around us.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:23:27 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 19:31:20 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 11:56:28 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Rabinovich", "Dmitry", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1809.06134", "submitter": "Sven Banisch", "authors": "Sven Banisch and Eckehard Olbrich", "title": "An Argument Communication Model of Polarization and Ideological\n  Alignment", "comments": "See www.universecity.de/demos/SCSIssueAlignment.html for an\n  interactive online demonstration. This project has received funding from the\n  European Union's Horizon 2020 research and innovation programme under grant\n  agreement No 732942 (Opinion Dynamics and Cultural Conflict in European\n  Spaces -- www.Odycceus.eu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-level model of opinion formation is presented which takes into\naccount that attitudes on different issues are usually not independent. In the\nmodel, agents exchange beliefs regarding a series of facts. A cognitive\nstructure of evaluative associations links different (partially overlapping)\nsets of facts to different political issues and determines an agents'\nattitudinal positions in a way borrowed from expectancy value theory. If agents\npreferentially interact with other agents that hold similar attitudes on one or\nseveral issues, this leads to biased argument pools and polarization in the\nsense that groups of agents selectively belief in distinct subsets of facts.\nBesides the emergence of a bi-modal distribution of opinions on single issues\nthat most previous opinion polarization models address, our model also accounts\nfor the alignment of attitudes across several issues along ideological\ndimensions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:26:09 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 09:44:43 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Banisch", "Sven", ""], ["Olbrich", "Eckehard", ""]]}, {"id": "1809.06440", "submitter": "Chang-Shen Lee", "authors": "Chang-Shen Lee, Nicol\\`o Michelusi, Gesualdo Scutari", "title": "Limited Rate Distributed Weight-Balancing and Average Consensus Over\n  Digraphs", "comments": "Part of this work will be presented at the 57th IEEE Conference on\n  Decision and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed quantized weight-balancing and average consensus over fixed\ndigraphs are considered. A digraph with non-negative weights associated to its\nedges is weight-balanced if, for each node, the sum of the weights of its\nout-going edges is equal to that of its incoming edges. This paper proposes and\nanalyzes the first distributed algorithm that solves the weight-balancing\nproblem using only finite rate and simplex communications among nodes\n(compliant to the directed nature of the graph edges). Asymptotic convergence\nof the scheme is proved and a convergence rate analysis is provided. Building\non this result, a novel distributed algorithm is proposed that solves the\naverage consensus problem over digraphs, using, at each iteration, finite rate\nsimplex communications between adjacent nodes -- some bits for the\nweight-balancing problem, other for the average consensus. Convergence of the\nproposed quantized consensus algorithm to the average of the real (i.e.,\nunquantized) agent's initial values is proved, both almost surely and in $r$th\nmean for all positive integer $r$. Finally, numerical results validate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 20:53:25 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Lee", "Chang-Shen", ""], ["Michelusi", "Nicol\u00f2", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1809.06727", "submitter": "Anders Martinsson", "authors": "Bernhard Haeupler, Fabian Kuhn, Anders Martinsson, Kalina Petrova, and\n  Pascal Pfister", "title": "Optimal strategies for patrolling fences", "comments": "19 pages, 3 figures. Part of our main result (circle strategy) is new\n  to this version of the paper. A shorter version of this is to appear in the\n  proceedings of ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.MA math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical multi-agent fence patrolling problem asks: What is the maximum\nlength $L$ of a line that $k$ agents with maximum speeds $v_1,\\ldots,v_k$ can\npatrol if each point on the line needs to be visited at least once every unit\nof time. It is easy to see that $L = \\alpha \\sum_{i=1}^k v_i$ for some\nefficiency $\\alpha \\in [\\frac{1}{2},1)$. After a series of works giving better\nand better efficiencies, it was conjectured that the best possible efficiency\napproaches $\\frac{2}{3}$. No upper bounds on the efficiency below $1$ were\nknown. We prove the first such upper bounds and tightly bound the optimal\nefficiency in terms of the minimum ratio of speeds $s = {v_{\\max}}/{v_{\\min}}$\nand the number of agents $k$. Guided by our upper bounds, we construct a scheme\nwhose efficiency approaches $1$, disproving the conjecture of Kawamura and\nSoejima. Our scheme asymptotically matches our upper bounds in terms of the\nmaximal speed difference and the number of agents used, proving them to be\nasymptotically tight.\n  A variation of the fence patrolling problem considers a circular fence\ninstead and asks for its circumference to be maximized. We consider the\nunidirectional case of this variation, where all agents are only allowed to\nmove in one direction, say clockwise. At first, a strategy yielding $L =\n\\max_{r \\in [k]} r \\cdot v_r$ where $v_1 \\geq v_2 \\geq \\dots \\geq v_k$ was\nconjectured to be optimal by Czyzowicz et al. This was proven not to be the\ncase by giving constructions for only specific numbers of agents with marginal\nimprovements of $L$. We give a general construction that yields $L =\n\\frac{1}{33 \\log_e\\log_2(k)} \\sum_{i=1}^k v_i$ for any set of agents, which in\nparticular for the case $1, 1/2, \\dots, 1/k$ diverges as $k \\rightarrow\n\\infty$, thus resolving a conjecture by Kawamura and Soejima affirmatively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:45:14 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 20:46:27 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Kuhn", "Fabian", ""], ["Martinsson", "Anders", ""], ["Petrova", "Kalina", ""], ["Pfister", "Pascal", ""]]}, {"id": "1809.07066", "submitter": "Vishal Sunder", "authors": "Vishal Sunder, Lovekesh Vig, Arnab Chatterjee, Gautam Shroff", "title": "Prosocial or Selfish? Agents with different behaviors for Contract\n  Negotiation using Reinforcement Learning", "comments": "Proceedings of the 11th International Workshop on Automated\n  Negotiations (held in conjunction with IJCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective technique for training deep learning agents capable\nof negotiating on a set of clauses in a contract agreement using a simple\ncommunication protocol. We use Multi Agent Reinforcement Learning to train both\nagents simultaneously as they negotiate with each other in the training\nenvironment. We also model selfish and prosocial behavior to varying degrees in\nthese agents. Empirical evidence is provided showing consistency in agent\nbehaviors. We further train a meta agent with a mixture of behaviors by\nlearning an ensemble of different models using reinforcement learning. Finally,\nto ascertain the deployability of the negotiating agents, we conducted\nexperiments pitting the trained agents against human players. Results\ndemonstrate that the agents are able to hold their own against human players,\noften emerging as winners in the negotiation. Our experiments demonstrate that\nthe meta agent is able to reasonably emulate human behavior.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:46:34 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Sunder", "Vishal", ""], ["Vig", "Lovekesh", ""], ["Chatterjee", "Arnab", ""], ["Shroff", "Gautam", ""]]}, {"id": "1809.07087", "submitter": "Arnab Chatterjee", "authors": "Sachin Thukral, Hardik Meisheri, Tushar Kataria, Aman Agarwal, Ishan\n  Verma, Arnab Chatterjee, Lipika Dey", "title": "Analyzing behavioral trends in community driven discussion platforms\n  like Reddit", "comments": "8 pages, 9 figs, ASONAM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present methods to systematically analyze\nindividual and group behavioral patterns observed in community driven\ndiscussion platforms like Reddit where users exchange information and views on\nvarious topics of current interest. We conduct this study by analyzing the\nstatistical behavior of posts and modeling user interactions around them. We\nhave chosen Reddit as an example, since it has grown exponentially from a small\ncommunity to one of the biggest social network platforms in the recent times.\nDue to its large user base and popularity, a variety of behavior is present\namong users in terms of their activity. Our study provides interesting insights\nabout a large number of inactive posts which fail to gather attention despite\ntheir authors exhibiting Cyborg-like behavior to draw attention. We also\npresent interesting insights about short-lived but extremely active posts\nemulating a phenomenon like Mayfly Buzz. Further, we present methods to find\nthe nature of activity around highly active posts to determine the presence of\nLimelight hogging activity, if any. We analyzed over $2$ million posts and more\nthan $7$ million user responses to them during entire 2008 and over $63$\nmillion posts and over $608$ million user responses to them from August 2014 to\nJuly 2015 amounting to two one-year periods, in order to understand how social\nmedia space has evolved over the years.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:20:05 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Thukral", "Sachin", ""], ["Meisheri", "Hardik", ""], ["Kataria", "Tushar", ""], ["Agarwal", "Aman", ""], ["Verma", "Ishan", ""], ["Chatterjee", "Arnab", ""], ["Dey", "Lipika", ""]]}, {"id": "1809.07098", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Hirotaka Takano, Junichi Murata", "title": "Novelty-organizing team of classifiers in noisy and dynamic environments", "comments": null, "journal-ref": "2015 IEEE Congress on Evolutionary Computation (CEC)", "doi": "10.1109/CEC.2015.7257254", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, the environment is constantly changing with the input\nvariables under the effect of noise. However, few algorithms were shown to be\nable to work under those circumstances. Here, Novelty-Organizing Team of\nClassifiers (NOTC) is applied to the continuous action mountain car as well as\ntwo variations of it: a noisy mountain car and an unstable weather mountain\ncar. These problems take respectively noise and change of problem dynamics into\naccount. Moreover, NOTC is compared with NeuroEvolution of Augmenting\nTopologies (NEAT) in these problems, revealing a trade-off between the\napproaches. While NOTC achieves the best performance in all of the problems,\nNEAT needs less trials to converge. It is demonstrated that NOTC achieves\nbetter performance because of its division of the input space (creating easier\nproblems). Unfortunately, this division of input space also requires a bit of\ntime to bootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:38:20 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Takano", "Hirotaka", ""], ["Murata", "Junichi", ""]]}, {"id": "1809.07124", "submitter": "Cinjon Resnick", "authors": "Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster,\n  Julian Togelius, Kyunghyun Cho, Joan Bruna", "title": "Pommerman: A Multi-Agent Playground", "comments": "Oral at the AIIDE Multi-Agent Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Pommerman, a multi-agent environment based on the classic console\ngame Bomberman. Pommerman consists of a set of scenarios, each having at least\nfour players and containing both cooperative and competitive aspects. We\nbelieve that success in Pommerman will require a diverse set of tools and\nmethods, including planning, opponent/teammate modeling, game theory, and\ncommunication, and consequently can serve well as a multi-agent benchmark. To\ndate, we have already hosted one competition, and our next one will be featured\nin the NIPS 2018 competition track.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 11:27:25 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Resnick", "Cinjon", ""], ["Eldridge", "Wes", ""], ["Ha", "David", ""], ["Britz", "Denny", ""], ["Foerster", "Jakob", ""], ["Togelius", "Julian", ""], ["Cho", "Kyunghyun", ""], ["Bruna", "Joan", ""]]}, {"id": "1809.07225", "submitter": "Wolfram Barfuss", "authors": "Wolfram Barfuss, Jonathan F. Donges and J\\\"urgen Kurths", "title": "Deterministic limit of temporal difference reinforcement learning for\n  stochastic games", "comments": "18 pages, 10 figures, adjustments to fit approx. PRE version", "journal-ref": "Phys. Rev. E 99, 043305 (2019)", "doi": "10.1103/PhysRevE.99.043305", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning in multiagent systems has been studied in the fields\nof economic game theory, artificial intelligence and statistical physics by\ndeveloping an analytical understanding of the learning dynamics (often in\nrelation to the replicator dynamics of evolutionary game theory). However, the\nmajority of these analytical studies focuses on repeated normal form games,\nwhich only have a single environmental state. Environmental dynamics, i.e.,\nchanges in the state of an environment affecting the agents' payoffs has\nreceived less attention, lacking a universal method to obtain deterministic\nequations from established multistate reinforcement learning algorithms.\n  In this work we present a novel methodological extension, separating the\ninteraction from the adaptation time scale, to derive the deterministic limit\nof a general class of reinforcement learning algorithms, called temporal\ndifference learning. This form of learning is equipped to function in more\nrealistic multistate environments by using the estimated value of future\nenvironmental states to adapt the agent's behavior. We demonstrate the\npotential of our method with the three well established learning algorithms Q\nlearning, SARSA learning and Actor-Critic learning. Illustrations of their\ndynamics on two multiagent, multistate environments reveal a wide range of\ndifferent dynamical regimes, such as convergence to fixed points, limit cycles,\nand even deterministic chaos.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:48:22 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 10:34:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Barfuss", "Wolfram", ""], ["Donges", "Jonathan F.", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "1809.07262", "submitter": "Kam Fai Elvis Tsang", "authors": "Kam Fai Elvis Tsang, Yuqing Ni, Cheuk Fung Raphael Wong and Ling Shi", "title": "A Novel Warehouse Multi-Robot Automation System with Semi-Complete and\n  Computationally Efficient Path Planning and Adaptive Genetic Task Allocation\n  Algorithms", "comments": "Accepted by the 15th International Conference on Control, Automation,\n  Robotics and Vision, ICARCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of warehouse multi-robot automation system in\ndiscrete-time and discrete-space configuration with focus on the task\nallocation and conflict-free path planning. We present a system design where a\ncentralized server handles the task allocation and each robot performs local\npath planning distributively. A genetic-based task allocation algorithm is\nfirstly presented, with modification to enable heuristic learning. A\nsemi-complete potential field based local path planning algorithm is then\nproposed, named the recursive excitation/relaxation artificial potential field\n(RERAPF). A mathematical proof is also presented to show the semi-completeness\nof the RERAPF algorithm. The main contribution of this paper is the\nmodification of conventional artificial potential field (APF) to be\nsemi-complete while computationally efficient, resolving the traditional issue\nof incompleteness. Simulation results are also presented for performance\nevaluation of the proposed path planning algorithm and the overall system.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:02:03 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 13:06:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Tsang", "Kam Fai Elvis", ""], ["Ni", "Yuqing", ""], ["Wong", "Cheuk Fung Raphael", ""], ["Shi", "Ling", ""]]}, {"id": "1809.07392", "submitter": "Aria Rezaei", "authors": "Aria Rezaei, Jie Gao, Jeff M. Phillips, Csaba D. T\\'oth", "title": "Improved Bounds on Information Dissemination by Manhattan Random\n  Waypoint Model", "comments": "10 pages, ACM SIGSPATIAL 2018, Seattle, US", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (SIGSPATIAL 18), 2018, Seattle, WA, USA", "doi": "10.1145/3274895.3274917", "report-no": null, "categories": "cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of portable wireless devices it is important to model and\npredict how information or contagions spread by natural human mobility -- for\nunderstanding the spreading of deadly infectious diseases and for improving\ndelay tolerant communication schemes. Formally, we model this problem by\nconsidering $M$ moving agents, where each agent initially carries a\n\\emph{distinct} bit of information. When two agents are at the same location or\nin close proximity to one another, they share all their information with each\nother. We would like to know the time it takes until all bits of information\nreach all agents, called the \\textit{flood time}, and how it depends on the way\nagents move, the size and shape of the network and the number of agents moving\nin the network.\n  We provide rigorous analysis for the \\MRWP model (which takes paths with\nminimum number of turns), a convenient model used previously to analyze mobile\nagents, and find that with high probability the flood time is bounded by\n$O\\big(N\\log M\\lceil(N/M) \\log(NM)\\rceil\\big)$, where $M$ agents move on an\n$N\\times N$ grid. In addition to extensive simulations, we use a data set of\ntaxi trajectories to show that our method can successfully predict flood times\nin both experimental settings and the real world.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:05:55 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Rezaei", "Aria", ""], ["Gao", "Jie", ""], ["Phillips", "Jeff M.", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1809.07830", "submitter": "Yize Chen", "authors": "Yize Chen, Hao Wang", "title": "IntelligentCrowd: Mobile Crowdsensing via Multi-Agent Reinforcement\n  Learning", "comments": "Accepted paper at 2020 IEEE Transactions on Emerging Topics in\n  Computational Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prosperity of smart mobile devices has made mobile crowdsensing (MCS) a\npromising paradigm for completing complex sensing and computation tasks. In the\npast, great efforts have been made on the design of incentive mechanisms and\ntask allocation strategies from MCS platform's perspective to motivate mobile\nusers' participation. However, in practice, MCS participants face many\nuncertainties coming from their sensing environment as well as other\nparticipants' strategies, and how do they interact with each other and make\nsensing decisions is not well understood. In this paper, we take MCS\nparticipants' perspective to derive an online sensing policy to maximize their\npayoffs via MCS participation. Specifically, we model the interactions of\nmobile users and sensing environments as a multi-agent Markov decision process.\nEach participant cannot observe others' decisions, but needs to decide her\neffort level in sensing tasks only based on local information, e.g., its own\nrecord of sensed signals' quality. To cope with the stochastic sensing\nenvironment, we develop an intelligent crowdsensing algorithm IntelligentCrowd\nby leveraging the power of multi-agent reinforcement learning (MARL). Our\nalgorithm leads to the optimal sensing policy for each user to maximize the\nexpected payoff against stochastic sensing environments, and can be implemented\nat individual participant's level in a distributed fashion. Numerical\nsimulations demonstrate that IntelligentCrowd significantly improves users'\npayoffs in sequential MCS tasks under various sensing dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:56:08 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 19:25:26 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 04:21:42 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chen", "Yize", ""], ["Wang", "Hao", ""]]}, {"id": "1809.09332", "submitter": "Hongyao Tang", "authors": "Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang Zhang,\n  Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan, Li Wang", "title": "Hierarchical Deep Multiagent Reinforcement Learning with Temporal\n  Abstraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent reinforcement learning (MARL) is commonly considered to suffer\nfrom non-stationary environments and exponentially increasing policy space. It\nwould be even more challenging when rewards are sparse and delayed over long\ntrajectories. In this paper, we study hierarchical deep MARL in cooperative\nmultiagent problems with sparse and delayed reward. With temporal abstraction,\nwe decompose the problem into a hierarchy of different time scales and\ninvestigate how agents can learn high-level coordination based on the\nindependent skills learned at the low level. Three hierarchical deep MARL\narchitectures are proposed to learn hierarchical policies under different MARL\nparadigms. Besides, we propose a new experience replay mechanism to alleviate\nthe issue of the sparse transitions at the high level of abstraction and the\nnon-stationarity of multiagent learning. We empirically demonstrate the\neffectiveness of our approaches in two domains with extremely sparse feedback:\n(1) a variety of Multiagent Trash Collection tasks, and (2) a challenging\nonline mobile game, i.e., Fever Basketball Defense.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 06:19:22 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 11:00:01 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Tang", "Hongyao", ""], ["Hao", "Jianye", ""], ["Lv", "Tangjie", ""], ["Chen", "Yingfeng", ""], ["Zhang", "Zongzhang", ""], ["Jia", "Hangtian", ""], ["Ren", "Chunxu", ""], ["Zheng", "Yan", ""], ["Meng", "Zhaopeng", ""], ["Fan", "Changjie", ""], ["Wang", "Li", ""]]}, {"id": "1809.09876", "submitter": "Ozer Ozkahraman", "authors": "\\\"Ozer \\\"Ozkahraman and Petter \\\"Ogren", "title": "Underwater Caging and Capture for Autonomous Underwater Vehicles", "comments": "To be appear in: Proceedings of Global OCEANS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of caging and eventual capture of an\nunderwater entity using multiple Autonomous Underwater Vehicles (AUVs) in a 3D\nwater volume We solve this problem both with and without taking bathymetry into\naccount. Our proposed algorithm for range-limited sensing in 3D environments\ncaptures a finite-speed entity based on sparse and irregular observations.\nAfter an isolated initial sighting of the entity, the uncertainty of its\nwhereabouts grows while deployment of the AUV system is underway. To contain\nthe entity, an initial cage, or barrier of sensing footprints, is created\naround the initial sighting, using islands and other terrain as part of the\ncage if available. After the initial cage is established, the system waits for\na second sighting, and the possible opportunity to create a smaller, shrinkable\ncage. This process continues until at some point it is possible to create this\nsmaller cage, resulting in capture, meaning the entity is sensed directly and\ncontinuously. We present a set of algorithms for addressing the scenario above,\nand illustrate their performance on a set of examples. The proposed algorithm\nis a combination of solutions to the min-cut problem, the set cover problem,\nthe linear bottleneck assignment problem and the Thomson problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:51:20 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 15:53:52 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 09:27:06 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["\u00d6zkahraman", "\u00d6zer", ""], ["\u00d6gren", "Petter", ""]]}, {"id": "1809.10007", "submitter": "Nicolas Anastassacos", "authors": "Nicolas Anastassacos, Mirco Musolesi", "title": "Learning through Probing: a decentralized reinforcement learning\n  architecture for social dilemmas", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning has received significant interest in\nrecent years notably due to the advancements made in deep reinforcement\nlearning which have allowed for the developments of new architectures and\nlearning algorithms. Using social dilemmas as the training ground, we present a\nnovel learning architecture, Learning through Probing (LTP), where agents\nutilize a probing mechanism to incorporate how their opponent's behavior\nchanges when an agent takes an action. We use distinct training phases and\nadjust rewards according to the overall outcome of the experiences accounting\nfor changes to the opponents behavior. We introduce a parameter eta to\ndetermine the significance of these future changes to opponent behavior. When\napplied to the Iterated Prisoner's Dilemma (IPD), LTP agents demonstrate that\nthey can learn to cooperate with each other, achieving higher average\ncumulative rewards than other reinforcement learning methods while also\nmaintaining good performance in playing against static agents that are present\nin Axelrod tournaments. We compare this method with traditional reinforcement\nlearning algorithms and agent-tracking techniques to highlight key differences\nand potential applications. We also draw attention to the differences between\nsolving games and societal-like interactions and analyze the training of\nQ-learning agents in makeshift societies. This is to emphasize how cooperation\nmay emerge in societies and demonstrate this using environments where\ninteractions with opponents are determined through a random encounter format of\nthe IPD.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 14:10:13 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 13:49:32 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Anastassacos", "Nicolas", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1809.10250", "submitter": "Matthew Romano", "authors": "Matthew Romano, Prince Kuevor, Derek Lukacs, Owen Marshall, Mia\n  Stevens, Hossein Rastgoftar, James Cutler, Ella Atkins", "title": "Experimental Evaluation of Continuum Deformation with a Five Quadrotor\n  Team", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper experimentally evaluates continuum deformation cooperative control\nfor the first time. Theoretical results are expanded to place a bounding\ntriangle on the leader-follower system such that the team is contained despite\nnontrivial tracking error. Flight tests were conducted with custom quadrotors\nrunning a modified version of ArduPilot on a BeagleBone Blue in M-Air, an\noutdoor netted flight facility. Motion capture and an onboard inertial\nmeasurement unit were used for state estimation. Position error was\ncharacterized in single vehicle tests using quintic spline trajectories and\ndifferent reference velocities. Five-quadrotor leader trajectories were\ngenerated, and followers executed the continuum deformation control law\nin-flight. Flight tests successfully demonstrated continuum deformation; future\nwork in characterizing error propagation from leaders to followers is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 22:06:13 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 18:48:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Romano", "Matthew", ""], ["Kuevor", "Prince", ""], ["Lukacs", "Derek", ""], ["Marshall", "Owen", ""], ["Stevens", "Mia", ""], ["Rastgoftar", "Hossein", ""], ["Cutler", "James", ""], ["Atkins", "Ella", ""]]}, {"id": "1809.10339", "submitter": "Marcelo Amanajas Pires Marcelo A. Pires", "authors": "Marcelo A. Pires, S\\'ilvio M. Duarte Queir\\'os", "title": "Optimal diffusion in ecological dynamics with Allee effect in a\n  metapopulation", "comments": "16 pages; 6 figures", "journal-ref": "PLoS One, 2019", "doi": "10.1371/journal.pone.0218087", "report-no": null, "categories": "q-bio.PE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How diffusion impacts on ecological dynamics under the Allee effect and\nspatial constraints? That is the question we address. Employing a microscopic\nminimal model in a metapopulation (without imposing nonlinear birth and death\nrates) we evince --- both numerically and analitically --- the emergence of an\noptimal diffusion that maximises the survival probability. Even though, at\nfirst such result seems counter-intuitive, it has empirical support from recent\nexperiments with engineered bacteria. Moreover, we show that this optimal\ndiffusion disappears for loose spatial constraints.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 04:41:27 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Pires", "Marcelo A.", ""], ["Queir\u00f3s", "S\u00edlvio M. Duarte", ""]]}, {"id": "1809.10503", "submitter": "Shaull Almagor", "authors": "Shaull Almagor, Rajeev Alur, and Suguman Bansal", "title": "Equilibria in Quantitative Concurrent Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis of finite-state controllers from high-level specifications in\nmulti-agent systems can be reduced to solving multi-player concurrent games\nover finite graphs. The complexity of solving such games with qualitative\nobjectives for agents, such as reaching a target set, is well understood\nresulting in tools with applications in robotics. In this paper, we introduce\nquantitative concurrent graph games, where transitions have separate costs for\ndifferent agents, and each agent attempts to reach its target set while\nminimizing its own cost along the path. In this model, a solution to the game\ncorresponds to a set of strategies, one per agent, that forms a Nash\nequilibrium. We study the problem of computing the set of all Pareto-optimal\nNash equilibria, and give a comprehensive analysis of its complexity and\nrelated problems such as the price of stability and the price of anarchy. In\nparticular, while checking the existence of a Nash equilibrium is NP-complete\nin general, with multiple parameters contributing to the computational hardness\nseparately, two-player games with bounded costs on individual transitions admit\na polynomial-time solution.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:14:17 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Almagor", "Shaull", ""], ["Alur", "Rajeev", ""], ["Bansal", "Suguman", ""]]}, {"id": "1809.10766", "submitter": "Pavel Chebotarev", "authors": "Pavel Chebotarev, Yana Tsodikova, Anton Loginov, Zoya Lezina, Vadim\n  Afonkin, Vitaly Malyshev", "title": "Comparative Efficiency of Altruism and Egoism as Voting Strategies in\n  Stochastic Environment", "comments": "20 pages, 10 figures, Accepted for publication in Automation & Remote\n  Control", "journal-ref": "Automation and Remote Control. 2018. V. 79. No. 11. P. 2052-2072", "doi": "10.1134/S0005117918110097", "report-no": null, "categories": "math.OC cs.MA cs.SY math.PR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the efficiency of egoistic and altruistic strategies\nwithin the model of social dynamics determined by voting in a stochastic\nenvironment (the ViSE model) using two criteria: maximizing the average capital\nincrement and minimizing the number of bankrupt participants. The proposals are\ngenerated stochastically; three families of the corresponding distributions are\nconsidered: normal distributions, symmetrized Pareto distributions, and\nStudent's $t$-distributions. It is found that the \"pit of losses\" paradox\ndescribed earlier does not occur in the case of heavy-tailed distributions. The\negoistic strategy better protects agents from extinction in aggressive\nenvironments than the altruistic ones, however, the efficiency of altruism is\nhigher in more favorable environments. A comparison of altruistic strategies\nwith each other shows that in aggressive environments, everyone should be\nsupported to minimize extinction, while under more favorable conditions, it is\nmore efficient to support the weakest participants. Studying the dynamics of\nparticipants' capitals we identify situations where the two considered criteria\ncontradict each other. At the next stage of the study, combined voting\nstrategies and societies involving participants with selfish and altruistic\nstrategies will be explored.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 21:05:34 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chebotarev", "Pavel", ""], ["Tsodikova", "Yana", ""], ["Loginov", "Anton", ""], ["Lezina", "Zoya", ""], ["Afonkin", "Vadim", ""], ["Malyshev", "Vitaly", ""]]}, {"id": "1809.11044", "submitter": "Andrea Tacchetti", "authors": "Andrea Tacchetti, H. Francis Song, Pedro A. M. Mediano, Vinicius\n  Zambaldi, Neil C. Rabinowitz, Thore Graepel, Matthew Botvinick, Peter W.\n  Battaglia", "title": "Relational Forward Models for Multi-Agent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavioral dynamics of multi-agent systems have a rich and orderly\nstructure, which can be leveraged to understand these systems, and to improve\nhow artificial agents learn to operate in them. Here we introduce Relational\nForward Models (RFM) for multi-agent learning, networks that can learn to make\naccurate predictions of agents' future behavior in multi-agent environments.\nBecause these models operate on the discrete entities and relations present in\nthe environment, they produce interpretable intermediate representations which\noffer insights into what drives agents' behavior, and what events mediate the\nintensity and valence of social interactions. Furthermore, we show that\nembedding RFM modules inside agents results in faster learning systems compared\nto non-augmented baselines. As more and more of the autonomous systems we\ndevelop and interact with become multi-agent in nature, developing richer\nanalysis tools for characterizing how and why agents make decisions is\nincreasingly necessary. Moreover, developing artificial agents that quickly and\nsafely learn to coordinate with one another, and with humans in shared\nenvironments, is crucial.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:10:39 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Song", "H. Francis", ""], ["Mediano", "Pedro A. M.", ""], ["Zambaldi", "Vinicius", ""], ["Rabinowitz", "Neil C.", ""], ["Graepel", "Thore", ""], ["Botvinick", "Matthew", ""], ["Battaglia", "Peter W.", ""]]}]