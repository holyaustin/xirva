[{"id": "0705.0760", "submitter": "Sujay Sanghavi", "authors": "Sujay Sanghavi", "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG cs.NI math.IT", "license": null, "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2007 18:57:47 GMT"}], "update_date": "2007-07-13", "authors_parsed": [["Sanghavi", "Sujay", ""]]}, {"id": "0705.1585", "submitter": "Tshilidzi Marwala", "authors": "Unathi Mahola, Fulufhelo V. Nelwamondo, Tshilidzi Marwala", "title": "HMM Speaker Identification Using Linear and Non-linear Merging\n  Techniques", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2007 04:54:54 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Mahola", "Unathi", ""], ["Nelwamondo", "Fulufhelo V.", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0705.2318", "submitter": "Seiji Miyoshi", "authors": "Hideto Utsumi, Seiji Miyoshi, Masato Okada", "title": "Statistical Mechanics of Nonlinear On-line Learning for Ensemble\n  Teachers", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1143/JPSJ.76.114001", "report-no": null, "categories": "cs.LG cond-mat.dis-nn", "license": null, "abstract": "  We analyze the generalization performance of a student in a model composed of\nnonlinear perceptrons: a true teacher, ensemble teachers, and the student. We\ncalculate the generalization error of the student analytically or numerically\nusing statistical mechanics in the framework of on-line learning. We treat two\nwell-known learning rules: Hebbian learning and perceptron learning. As a\nresult, it is proven that the nonlinear model shows qualitatively different\nbehaviors from the linear model. Moreover, it is clarified that Hebbian\nlearning and perceptron learning show qualitatively different behaviors from\neach other. In Hebbian learning, we can analytically obtain the solutions. In\nthis case, the generalization error monotonically decreases. The steady value\nof the generalization error is independent of the learning rate. The larger the\nnumber of teachers is and the more variety the ensemble teachers have, the\nsmaller the generalization error is. In perceptron learning, we have to\nnumerically obtain the solutions. In this case, the dynamical behaviors of the\ngeneralization error are non-monotonic. The smaller the learning rate is, the\nlarger the number of teachers is; and the more variety the ensemble teachers\nhave, the smaller the minimum value of the generalization error is.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2007 09:58:39 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Utsumi", "Hideto", ""], ["Miyoshi", "Seiji", ""], ["Okada", "Masato", ""]]}, {"id": "0705.2765", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov", "title": "On the monotonization of the training set", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": null, "abstract": "  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2007 19:44:19 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Takhanov", "Rustem", ""]]}, {"id": "0705.4485", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, David M Blei, Stephen E Fienberg, Eric P Xing", "title": "Mixed membership stochastic blockmodels", "comments": "46 pages, 14 figures, 3 tables", "journal-ref": "Journal of Machine Learning Research, 9, 1981-2014.", "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": null, "abstract": "  Observations consisting of measurements on relationships for pairs of objects\narise in many settings, such as protein interaction and gene regulatory\nnetworks, collections of author-recipient email, and social networks. Analyzing\nsuch data with probabilisic models can be delicate because the simple\nexchangeability assumptions underlying many boilerplate models no longer hold.\nIn this paper, we describe a latent variable model of such data called the\nmixed membership stochastic blockmodel. This model extends blockmodels for\nrelational data to ones which capture mixed membership latent relational\nstructure, thus providing an object-specific low-dimensional representation. We\ndevelop a general variational inference algorithm for fast approximate\nposterior inference. We explore applications to social and protein interaction\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2007 23:22:59 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Blei", "David M", ""], ["Fienberg", "Stephen E", ""], ["Xing", "Eric P", ""]]}, {"id": "0705.4566", "submitter": "Bastian Wemmenhove", "authors": "Bastian Wemmenhove and Bert Kappen", "title": "Loop corrections for message passing algorithms in continuous variable\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": null, "abstract": "  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2007 10:35:07 GMT"}], "update_date": "2007-06-01", "authors_parsed": [["Wemmenhove", "Bastian", ""], ["Kappen", "Bert", ""]]}]