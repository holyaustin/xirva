[{"id": "0804.0188", "submitter": "Alexandre d'Aspremont", "authors": "Ronny Luss, Alexandre d'Aspremont", "title": "Support Vector Machine Classification with Indefinite Kernels", "comments": "Final journal version. A few typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for support vector machine classification using\nindefinite kernels. Instead of directly minimizing or stabilizing a nonconvex\nloss function, our algorithm simultaneously computes support vectors and a\nproxy kernel matrix used in forming the loss. This can be interpreted as a\npenalized kernel learning problem where indefinite kernel matrices are treated\nas a noisy observations of a true Mercer kernel. Our formulation keeps the\nproblem convex and relatively large problems can be solved efficiently using\nthe projected gradient or analytic center cutting plane methods. We compare the\nperformance of our technique with other methods on several classic data sets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2008 14:55:33 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2009 11:48:14 GMT"}], "update_date": "2009-08-04", "authors_parsed": [["Luss", "Ronny", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "0804.0924", "submitter": "Ratthachat Chatpatanasiri", "authors": "Ratthachat Chatpatanasiri and Boonserm Kijsirikul", "title": "A Unified Semi-Supervised Dimensionality Reduction Framework for\n  Manifold Learning", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework of semi-supervised dimensionality reduction\nfor manifold learning which naturally generalizes existing supervised and\nunsupervised learning frameworks which apply the spectral decomposition.\nAlgorithms derived under our framework are able to employ both labeled and\nunlabeled examples and are able to handle complex problems where data form\nseparate clusters of manifolds. Our framework offers simple views, explains\nrelationships among existing frameworks and provides further extensions which\ncan improve existing algorithms. Furthermore, a new semi-supervised\nkernelization framework called ``KPCA trick'' is proposed to handle non-linear\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2008 18:14:34 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2009 04:25:24 GMT"}], "update_date": "2009-07-29", "authors_parsed": [["Chatpatanasiri", "Ratthachat", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "0804.1302", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Bolasso: model consistent Lasso estimation through the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-square linear regression problem with regularization by\nthe l1-norm, a problem usually referred to as the Lasso. In this paper, we\npresent a detailed asymptotic analysis of model consistency of the Lasso. For\nvarious decays of the regularization parameter, we compute asymptotic\nequivalents of the probability of correct model selection (i.e., variable\nselection). For a specific rate decay, we show that the Lasso selects all the\nvariables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection algorithm, referred to as the Bolasso, is\ncompared favorably to other linear regression methods on synthetic data and\ndatasets from the UCI machine learning repository.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2008 15:40:03 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0804.1441", "submitter": "Ratthachat Chatpatanasiri", "authors": "Ratthachat Chatpatanasiri, Teesid Korsrilabutr, Pasakorn\n  Tangchanachaianan and Boonserm Kijsirikul", "title": "On Kernelization of Supervised Mahalanobis Distance Learners", "comments": "23 pages, 5 figures. There is a seriously wrong formula in derivation\n  of a gradient formula of the \"kernel NCA\" in the two previous versions. In\n  this new version, a new theoretical result is provided to properly account\n  kernel NCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of kernelizing an existing supervised\nMahalanobis distance learner. The following features are included in the paper.\nFirstly, three popular learners, namely, \"neighborhood component analysis\",\n\"large margin nearest neighbors\" and \"discriminant neighborhood embedding\",\nwhich do not have kernel versions are kernelized in order to improve their\nclassification performances. Secondly, an alternative kernelization framework\ncalled \"KPCA trick\" is presented. Implementing a learner in the new framework\ngains several advantages over the standard framework, e.g. no mathematical\nformulas and no reprogramming are required for a kernel implementation, the\nframework avoids troublesome problems such as singularity, etc. Thirdly, while\nthe truths of representer theorems are just assumptions in previous papers\nrelated to ours, here, representer theorems are formally proven. The proofs\nvalidate both the kernel trick and the KPCA trick in the context of Mahalanobis\ndistance learning. Fourthly, unlike previous works which always apply brute\nforce methods to select a kernel, we investigate two approaches which can be\nefficiently adopted to construct an appropriate kernel for a given dataset.\nFinally, numerical results on various real-world datasets are presented.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2008 09:40:51 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2008 09:51:46 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2009 02:19:27 GMT"}], "update_date": "2009-01-30", "authors_parsed": [["Chatpatanasiri", "Ratthachat", ""], ["Korsrilabutr", "Teesid", ""], ["Tangchanachaianan", "Pasakorn", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "0804.3575", "submitter": "S. Charles Brubaker", "authors": "S. Charles Brubaker and Santosh S. Vempala", "title": "Isotropic PCA and Affine-Invariant Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for clustering points in R^n. The key property of\nthe algorithm is that it is affine-invariant, i.e., it produces the same\npartition for any affine transformation of the input. It has strong guarantees\nwhen the input is drawn from a mixture model. For a mixture of two arbitrary\nGaussians, the algorithm correctly classifies the sample assuming only that the\ntwo components are separable by a hyperplane, i.e., there exists a halfspace\nthat contains most of one Gaussian and almost none of the other in probability\nmass. This is nearly the best possible, improving known results substantially.\nFor k > 2 components, the algorithm requires only that there be some\n(k-1)-dimensional subspace in which the emoverlap in every direction is small.\nHere we define overlap to be the ratio of the following two quantities: 1) the\naverage squared distance between a point and the mean of its component, and 2)\nthe average squared distance between a point and the mean of the mixture. The\nmain result may also be stated in the language of linear discriminant analysis:\nif the standard Fisher discriminant is small enough, labels are not needed to\nestimate the optimal subspace for projection. Our main tools are isotropic\ntransformation, spectral projection and a simple reweighting technique. We call\nthis combination isotropic PCA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2008 17:59:03 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2008 19:28:46 GMT"}], "update_date": "2008-08-04", "authors_parsed": [["Brubaker", "S. Charles", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "0804.3817", "submitter": "Jan Arpe", "authors": "Jan Arpe and Elchanan Mossel", "title": "Multiple Random Oracles Are Better Than One", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2008 23:18:00 GMT"}], "update_date": "2008-04-25", "authors_parsed": [["Arpe", "Jan", ""], ["Mossel", "Elchanan", ""]]}, {"id": "0804.4451", "submitter": "Jian Ma", "authors": "Jian Ma and Zengqi Sun", "title": "Dependence Structure Estimation via Copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependence strucuture estimation is one of the important problems in machine\nlearning domain and has many applications in different scientific areas. In\nthis paper, a theoretical framework for such estimation based on copula and\ncopula entropy -- the probabilistic theory of representation and measurement of\nstatistical dependence, is proposed. Graphical models are considered as a\nspecial case of the copula framework. A method of the framework for estimating\nmaximum spanning copula is proposed. Due to copula, the method is irrelevant to\nthe properties of individual variables, insensitive to outlier and able to deal\nwith non-Gaussianity. Experiments on both simulated data and real dataset\ndemonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2008 17:14:53 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 00:29:28 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ma", "Jian", ""], ["Sun", "Zengqi", ""]]}, {"id": "0804.4682", "submitter": "Tshilidzi Marwala", "authors": "Vukosi Marivate and Tshilidzi Marwala", "title": "Introduction to Relational Networks for Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of computational intelligence techniques for classification has been\nused in numerous applications. This paper compares the use of a Multi Layer\nPerceptron Neural Network and a new Relational Network on classifying the HIV\nstatus of women at ante-natal clinics. The paper discusses the architecture of\nthe relational network and its merits compared to a neural network and most\nother computational intelligence classifiers. Results gathered from the study\nindicate comparable classification accuracies as well as revealed relationships\nbetween data features in the classification data. Much higher classification\naccuracies are recommended for future research in the area of HIV\nclassification as well as missing data estimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2008 19:25:07 GMT"}], "update_date": "2008-04-30", "authors_parsed": [["Marivate", "Vukosi", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0804.4741", "submitter": "Tshilidzi Marwala", "authors": "Lesedi Masisi, Fulufhelo V. Nelwamondo and Tshilidzi Marwala", "title": "The Effect of Structural Diversity of an Ensemble of Classifiers on\n  Classification Accuracy", "comments": "6 pages,IASTED International Conference on Modelling and Simulation\n  (Africa-MS), 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to showcase the measure of structural diversity of an\nensemble of 9 classifiers and then map a relationship between this structural\ndiversity and accuracy. The structural diversity was induced by having\ndifferent architectures or structures of the classifiers The Genetical\nAlgorithms (GA) were used to derive the relationship between diversity and the\nclassification accuracy by evolving the classifiers and then picking 9\nclassifiers out on an ensemble of 60 classifiers. It was found that as the\nensemble became diverse the accuracy improved. However at a certain diversity\nmeasure the accuracy began to drop. The Kohavi-Wolpert variance method is used\nto measure the diversity of the ensemble. A method of voting is used to\naggregate the results from each classifier. The lowest error was observed at a\ndiversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024\nas maximum diversity measured. The parameters that were varied were: the number\nof hidden nodes, learning rate and the activation function.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2008 06:07:45 GMT"}], "update_date": "2008-05-01", "authors_parsed": [["Masisi", "Lesedi", ""], ["Nelwamondo", "Fulufhelo V.", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0804.4898", "submitter": "Yann Guermeur", "authors": "Emmanuel Monfrini (LORIA), Yann Guermeur (LORIA)", "title": "A Quadratic Loss Multi-Class SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a support vector machine requires to set two types of hyperparameters:\nthe soft margin parameter C and the parameters of the kernel. To perform this\nmodel selection task, the method of choice is cross-validation. Its\nleave-one-out variant is known to produce an estimator of the generalization\nerror which is almost unbiased. Its major drawback rests in its time\nrequirement. To overcome this difficulty, several upper bounds on the\nleave-one-out error of the pattern recognition SVM have been derived. Among\nthose bounds, the most popular one is probably the radius-margin bound. It\napplies to the hard margin pattern recognition SVM, and by extension to the\n2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\nas a direct extension of the 2-norm SVM to the multi-class case. For this\nmachine, a generalized radius-margin bound is then established.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2008 19:59:56 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Monfrini", "Emmanuel", "", "LORIA"], ["Guermeur", "Yann", "", "LORIA"]]}]