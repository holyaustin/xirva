[{"id": "1310.0110", "submitter": "Arun Konagurthu", "authors": "Arun Konagurthu and James Collier", "title": "An information measure for comparing top $k$ lists", "comments": "11 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing the top $k$ elements between two or more ranked results is a common\ntask in many contexts and settings. A few measures have been proposed to\ncompare top $k$ lists with attractive mathematical properties, but they face a\nnumber of pitfalls and shortcomings in practice. This work introduces a new\nmeasure to compare any two top k lists based on measuring the information these\nlists convey. Our method investigates the compressibility of the lists, and the\nlength of the message to losslessly encode them gives a natural and robust\nmeasure of their variability. This information-theoretic measure objectively\nreconciles all the main considerations that arise when measuring\n(dis-)similarity between lists: the extent of their non-overlapping elements in\neach of the lists; the amount of disarray among overlapping elements between\nthe lists; the measurement of displacement of actual ranks of their overlapping\nelements.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 00:52:42 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Konagurthu", "Arun", ""], ["Collier", "James", ""]]}, {"id": "1310.0154", "submitter": "Yudong Chen", "authors": "Yudong Chen", "title": "Incoherence-Optimal Matrix Completion", "comments": "Fixed a minor error in Theorem 3 for matrix decomposition. To appear\n  in the IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2015.2415195", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the matrix completion problem. We show that it is not\nnecessary to assume joint incoherence, which is a standard but unintuitive and\nrestrictive condition that is imposed by previous studies. This leads to a\nsample complexity bound that is order-wise optimal with respect to the\nincoherence parameter (as well as to the rank $r$ and the matrix dimension $n$\nup to a log factor). As a consequence, we improve the sample complexity of\nrecovering a semidefinite matrix from $O(nr^{2}\\log^{2}n)$ to $O(nr\\log^{2}n)$,\nand the highest allowable rank from $\\Theta(\\sqrt{n}/\\log n)$ to\n$\\Theta(n/\\log^{2}n)$. The key step in proof is to obtain new bounds on the\n$\\ell_{\\infty,2}$-norm, defined as the maximum of the row and column norms of a\nmatrix. To illustrate the applicability of our techniques, we discuss\nextensions to SVD projection, structured matrix completion and semi-supervised\nclustering, for which we provide order-wise improvements over existing results.\nFinally, we turn to the closely-related problem of low-rank-plus-sparse matrix\ndecomposition. We show that the joint incoherence condition is unavoidable here\nfor polynomial-time algorithms conditioned on the Planted Clique conjecture.\nThis means it is intractable in general to separate a rank-$\\omega(\\sqrt{n})$\npositive semidefinite matrix and a sparse matrix. Interestingly, our results\nshow that the standard and joint incoherence conditions are associated\nrespectively with the information (statistical) and computational aspects of\nthe matrix decomposition problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 06:37:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 04:25:30 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2013 06:36:42 GMT"}, {"version": "v4", "created": "Fri, 13 Feb 2015 11:18:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Yudong", ""]]}, {"id": "1310.0354", "submitter": "Gary Huang", "authors": "Gary B. Huang and Viren Jain", "title": "Deep and Wide Multiscale Recursive Networks for Robust Image Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward multilayer networks trained by supervised learning have recently\ndemonstrated state of the art performance on image labeling problems such as\nboundary prediction and scene parsing. As even very low error rates can limit\npractical usage of such systems, methods that perform closer to human accuracy\nremain desirable. In this work, we propose a new type of network with the\nfollowing properties that address what we hypothesize to be limiting aspects of\nexisting methods: (1) a `wide' structure with thousands of features, (2) a\nlarge field of view, (3) recursive iterations that exploit statistical\ndependencies in label space, and (4) a parallelizable architecture that can be\ntrained in a fraction of the time compared to benchmark multilayer\nconvolutional networks. For the specific image labeling problem of boundary\nprediction, we also introduce a novel example weighting algorithm that improves\nsegmentation accuracy. Experiments in the challenging domain of connectomic\nreconstruction of neural circuity from 3d electron microscopy data show that\nthese \"Deep And Wide Multiscale Recursive\" (DAWMR) networks lead to new levels\nof image labeling performance. The highest performing architecture has twelve\nlayers, interwoven supervised and unsupervised stages, and uses an input field\nof view of 157,464 voxels ($54^3$) to make a prediction at each image location.\nWe present an associated open source software package that enables the simple\nand flexible creation of DAWMR networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 15:42:54 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 21:16:45 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 17:00:03 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Huang", "Gary B.", ""], ["Jain", "Viren", ""]]}, {"id": "1310.0432", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie", "title": "Online Learning of Dynamic Parameters in Social Networks", "comments": "12 pages, To appear in Neural Information Processing Systems (NIPS)\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of online learning in a dynamic setting. We\nconsider a social network in which each individual observes a private signal\nabout the underlying state of the world and communicates with her neighbors at\neach time period. Unlike many existing approaches, the underlying state is\ndynamic, and evolves according to a geometric random walk. We view the scenario\nas an optimization problem where agents aim to learn the true state while\nsuffering the smallest possible loss. Based on the decomposition of the global\nloss function, we introduce two update mechanisms, each of which generates an\nestimate of the true state. We establish a tight bound on the rate of change of\nthe underlying state, under which individuals can track the parameter with a\nbounded variance. Then, we characterize explicit expressions for the steady\nstate mean-square deviation(MSD) of the estimates from the truth, per\nindividual. We observe that only one of the estimators recovers the optimal\nMSD, which underscores the impact of the objective function decomposition on\nthe learning quality. Finally, we provide an upper bound on the regret of the\nproposed methods, measured as an average of errors in estimating the parameter\nin a finite time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 19:08:04 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rakhlin", "Alexander", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1310.0509", "submitter": "Isik Baris Fidaner", "authors": "I\\c{s}{\\i}k Bar{\\i}\\c{s} Fidaner and Ali Taylan Cemgil", "title": "Summary Statistics for Partitionings and Feature Allocations", "comments": "Accepted to NIPS 2013:\n  https://nips.cc/Conferences/2013/Program/event.php?ID=3763", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Infinite mixture models are commonly used for clustering. One can sample from\nthe posterior of mixture assignments by Monte Carlo methods or find its maximum\na posteriori solution by optimization. However, in some problems the posterior\nis diffuse and it is hard to interpret the sampled partitionings. In this\npaper, we introduce novel statistics based on block sizes for representing\nsample sets of partitionings and feature allocations. We develop an\nelement-based definition of entropy to quantify segmentation among their\nelements. Then we propose a simple algorithm called entropy agglomeration (EA)\nto summarize and visualize this information. Experiments on various infinite\nmixture posteriors as well as a feature allocation dataset demonstrate that the\nproposed statistics are useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 22:34:18 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 06:28:18 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2013 18:26:44 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2013 08:43:59 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Fidaner", "I\u015f\u0131k Bar\u0131\u015f", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1310.0576", "submitter": "Christian Retor\\'e", "authors": "Roberto Bonato and Christian Retor\\'e", "title": "Learning Lambek grammars from proof frames", "comments": "A revised version will appear in a volume in honour of Lambek 90th\n  birthday", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to their limpid interface with semantics, categorial grammars\nenjoy another important property: learnability. This was first noticed by\nBuskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorial\ngrammars.\n  What about Lambek categorial grammars? In a previous paper we showed that\nproduct free Lambek grammars where learnable from structured sentences, the\nstructures being incomplete natural deductions. These grammars were shown to be\nunlearnable from strings by Foret and Le Nir. In the present paper we show that\nLambek grammars, possibly with product, are learnable from proof frames that\nare incomplete proof nets.\n  After a short reminder on grammatical inference \\`a la Gold, we provide an\nalgorithm that learns Lambek grammars with product from proof frames and we\nprove its convergence. We do so for 1-valued also known as rigid Lambek\ngrammars with product, since standard techniques can extend our result to\n$k$-valued grammars. Because of the correspondence between cut-free proof nets\nand normal natural deductions, our initial result on product free Lambek\ngrammars can be recovered.\n  We are sad to dedicate the present paper to Philippe Darondeau, with whom we\nstarted to study such questions in Rennes at the beginning of the millennium,\nand who passed away prematurely.\n  We are glad to dedicate the present paper to Jim Lambek for his 90 birthday:\nhe is the living proof that research is an eternal learning process.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 06:06:02 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Bonato", "Roberto", ""], ["Retor\u00e9", "Christian", ""]]}, {"id": "1310.0740", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Mark Girolami", "title": "Pseudo-Marginal Bayesian Inference for Gaussian Processes", "comments": "14 pages double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenges that arise when adopting Gaussian Process priors in\nprobabilistic modeling are how to carry out exact Bayesian inference and how to\naccount for uncertainty on model parameters when making model-based predictions\non out-of-sample data. Using probit regression as an illustrative working\nexample, this paper presents a general and effective methodology based on the\npseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses\nboth of these issues. The results presented in this paper show improvements\nover existing sampling methods to simulate from the posterior distribution over\nthe parameters defining the covariance function of the Gaussian Process prior.\nThis is particularly important as it offers a powerful tool to carry out full\nBayesian inference of Gaussian Process based hierarchic statistical models in\ngeneral. The results also demonstrate that Monte Carlo based integration of all\nmodel parameters is actually feasible in this class of models providing a\nsuperior quantification of uncertainty in predictions. Extensive comparisons\nwith respect to state-of-the-art probabilistic classifiers confirm this\nassertion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 15:29:28 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 10:41:39 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 08:53:34 GMT"}, {"version": "v4", "created": "Mon, 7 Apr 2014 09:42:58 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Filippone", "Maurizio", ""], ["Girolami", "Mark", ""]]}, {"id": "1310.0807", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Yuejie Chi and Andrea Goldsmith", "title": "Exact and Stable Covariance Estimation from Quadratic Sampling via\n  Convex Programming", "comments": "accepted to IEEE Transactions on Information Theory, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference and information processing of high-dimensional data\noften require efficient and accurate estimation of their second-order\nstatistics. With rapidly changing data, limited processing power and storage at\nthe acquisition devices, it is desirable to extract the covariance structure\nfrom a single pass over the data and a small number of stored measurements. In\nthis paper, we explore a quadratic (or rank-one) measurement model which\nimposes minimal memory requirements and low computational complexity during the\nsampling process, and is shown to be optimal in preserving various\nlow-dimensional covariance structures. Specifically, four popular structural\nassumptions of covariance matrices, namely low rank, Toeplitz low rank,\nsparsity, jointly rank-one and sparse structure, are investigated, while\nrecovery is achieved via convex relaxation paradigms for the respective\nstructure.\n  The proposed quadratic sampling framework has a variety of potential\napplications including streaming data processing, high-frequency wireless\ncommunication, phase space tomography and phase retrieval in optics, and\nnon-coherent subspace detection. Our method admits universally accurate\ncovariance estimation in the absence of noise, as soon as the number of\nmeasurements exceeds the information theoretic limits. We also demonstrate the\nrobustness of this approach against noise and imperfect structural assumptions.\nOur analysis is established upon a novel notion called the mixed-norm\nrestricted isometry property (RIP-$\\ell_{2}/\\ell_{1}$), as well as the\nconventional RIP-$\\ell_{2}/\\ell_{2}$ for near-isotropic and bounded\nmeasurements. In addition, our results improve upon the best-known phase\nretrieval (including both dense and sparse signals) guarantees using PhaseLift\nwith a significantly simpler approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 19:52:57 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 19:32:50 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2013 17:25:57 GMT"}, {"version": "v4", "created": "Sat, 21 Dec 2013 02:07:28 GMT"}, {"version": "v5", "created": "Thu, 19 Mar 2015 21:23:21 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1310.0865", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos and Yu Zhang and Georgios B. Giannakis", "title": "Electricity Market Forecasting via Low-Rank Multi-Kernel Learning", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/JSTSP.2014.2336611", "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smart grid vision entails advanced information technology and data\nanalytics to enhance the efficiency, sustainability, and economics of the power\ngrid infrastructure. Aligned to this end, modern statistical learning tools are\nleveraged here for electricity market inference. Day-ahead price forecasting is\ncast as a low-rank kernel learning problem. Uniquely exploiting the market\nclearing process, congestion patterns are modeled as rank-one components in the\nmatrix of spatio-temporally varying prices. Through a novel nuclear norm-based\nregularization, kernels across pricing nodes and hours can be systematically\nselected. Even though market-wide forecasting is beneficial from a learning\nperspective, it involves processing high-dimensional market data. The latter\nbecomes possible after devising a block-coordinate descent algorithm for\nsolving the non-convex optimization problem involved. The algorithm utilizes\nresults from block-sparse vector recovery and is guaranteed to converge to a\nstationary point. Numerical tests on real data from the Midwest ISO (MISO)\nmarket corroborate the prediction accuracy, computational efficiency, and the\ninterpretative merits of the developed approach over existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 23:51:38 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 17:33:35 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Zhang", "Yu", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1310.0890", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Luping Zhou, Chunhua Shen, Jianping Yin", "title": "Multiple Kernel Learning in the Primal for Multi-modal Alzheimer's\n  Disease Classification", "comments": "7 pages. Appearing in IEEE Journal of Biomedical and Health\n  Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve effective and efficient detection of Alzheimer's disease (AD),\nmany machine learning methods have been introduced into this realm. However,\nthe general case of limited training samples, as well as different feature\nrepresentations typically makes this problem challenging. In this work, we\npropose a novel multiple kernel learning framework to combine multi-modal\nfeatures for AD classification, which is scalable and easy to implement.\nContrary to the usual way of solving the problem in the dual space, we look at\nthe optimization from a new perspective. By conducting Fourier transform on the\nGaussian kernel, we explicitly compute the mapping function, which leads to a\nmore straightforward solution of the problem in the primal space. Furthermore,\nwe impose the mixed $L_{21}$ norm constraint on the kernel weights, known as\nthe group lasso regularization, to enforce group sparsity among different\nfeature modalities. This actually acts as a role of feature modality selection,\nwhile at the same time exploiting complementary information among different\nkernels. Therefore it is able to extract the most discriminative features for\nclassification. Experiments on the ADNI data set demonstrate the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 03:53:22 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Liu", "Fayao", ""], ["Zhou", "Luping", ""], ["Shen", "Chunhua", ""], ["Yin", "Jianping", ""]]}, {"id": "1310.0900", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Efficient pedestrian detection by directly optimize the partial area\n  under the ROC curve", "comments": "10 pages. Appearing in Int. Conf. Computer Vision (ICCV) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many typical applications of object detection operate within a prescribed\nfalse-positive range. In this situation the performance of a detector should be\nassessed on the basis of the area under the ROC curve over that range, rather\nthan over the full curve, as the performance outside the range is irrelevant.\n  This measure is labelled as the partial area under the ROC curve (pAUC).\nEffective cascade-based classification, for example, depends on training node\nclassifiers that achieve the maximal detection rate at a moderate false\npositive rate, e.g., around 40% to 50%. We propose a novel ensemble learning\nmethod which achieves a maximal detection rate at a user-defined range of false\npositive rates by directly optimizing the partial AUC using structured\nlearning. By optimizing for different ranges of false positive rates, the\nproposed method can be used to train either a single strong classifier or a\nnode classifier forming part of a cascade classifier. Experimental results on\nboth synthetic and real-world data sets demonstrate the effectiveness of our\napproach, and we show that it is possible to train state-of-the-art pedestrian\ndetectors using the proposed structured ensemble learning method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 05:50:40 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1310.1076", "submitter": "Ping Li", "authors": "Ping Li, Cun-Hui Zhang, Tong Zhang", "title": "Compressed Counting Meets Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (sparse signal recovery) has been a popular and important\nresearch topic in recent years. By observing that natural signals are often\nnonnegative, we propose a new framework for nonnegative signal recovery using\nCompressed Counting (CC). CC is a technique built on maximally-skewed p-stable\nrandom projections originally developed for data stream computations. Our\nrecovery procedure is computationally very efficient in that it requires only\none linear scan of the coordinates. Our analysis demonstrates that, when\n0<p<=0.5, it suffices to use M= O(C/eps^p log N) measurements so that all\ncoordinates will be recovered within eps additive precision, in one scan of the\ncoordinates. The constant C=1 when p->0 and C=pi/2 when p=0.5. In particular,\nwhen p->0 the required number of measurements is essentially M=K\\log N, where K\nis the number of nonzero coordinates of the signal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 19:48:44 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1310.1177", "submitter": "Weixiang Shao", "authors": "Weixiang Shao (1), Xiaoxiao Shi (1) and Philip S. Yu (1) ((1)\n  University of Illinois at Chicago)", "title": "Clustering on Multiple Incomplete Datasets via Collective Kernel\n  Learning", "comments": "ICDM 2013, Code available at\n  https://github.com/software-shao/Collective-Kernel-Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple datasets containing different types of features may be available for\na given task. For instance, users' profiles can be used to group users for\nrecommendation systems. In addition, a model can also use users' historical\nbehaviors and credit history to group users. Each dataset contains different\ninformation and suffices for learning. A number of clustering algorithms on\nmultiple datasets were proposed during the past few years. These algorithms\nassume that at least one dataset is complete. So far as we know, all the\nprevious methods will not be applicable if there is no complete dataset\navailable. However, in reality, there are many situations where no dataset is\ncomplete. As in building a recommendation system, some new users may not have a\nprofile or historical behaviors, while some may not have a credit history.\nHence, no available dataset is complete. In order to solve this problem, we\npropose an approach called Collective Kernel Learning to infer hidden sample\nsimilarity from multiple incomplete datasets. The idea is to collectively\ncompletes the kernel matrices of incomplete datasets by optimizing the\nalignment of the shared instances of the datasets. Furthermore, a clustering\nalgorithm is proposed based on the kernel matrix. The experiments on both\nsynthetic and real datasets demonstrate the effectiveness of the proposed\napproach. The proposed clustering algorithm outperforms the comparison\nalgorithms by as much as two times in normalized mutual information.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 06:18:59 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 23:35:13 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Shao", "Weixiang", ""], ["Shi", "Xiaoxiao", ""], ["Yu", "Philip S.", ""]]}, {"id": "1310.1187", "submitter": "Johan Pensar", "authors": "Johan Pensar, Henrik Nyman, Timo Koski and Jukka Corander", "title": "Labeled Directed Acyclic Graphs: a generalization of context-specific\n  independence in directed graphical models", "comments": "26 pages, 17 figures", "journal-ref": null, "doi": "10.1007/s10618-014-0355-0", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of labeled directed acyclic graph (LDAG) models\nfor finite sets of discrete variables. LDAGs generalize earlier proposals for\nallowing local structures in the conditional probability distribution of a\nnode, such that unrestricted label sets determine which edges can be deleted\nfrom the underlying directed acyclic graph (DAG) for a given context. Several\nproperties of these models are derived, including a generalization of the\nconcept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is\nenabled by introducing an LDAG-based factorization of the Dirichlet prior for\nthe model parameters, such that the marginal likelihood can be calculated\nanalytically. In addition, we develop a novel prior distribution for the model\nstructures that can appropriately penalize a model for its labeling complexity.\nA non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill\nclimbing approach is used for illustrating the useful properties of LDAG models\nfor both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 07:29:08 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Koski", "Timo", ""], ["Corander", "Jukka", ""]]}, {"id": "1310.1250", "submitter": "Rui Vilela-Mendes", "authors": "Rui Ligeiro and R. Vilela Mendes", "title": "Learning ambiguous functions by neural networks", "comments": "13 pages, 9 figures", "journal-ref": "Soft Computing 22 (2018) 2695 - 2703", "doi": "10.1007/s00500-017-2525-7", "report-no": null, "categories": "cs.NE cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not, in general, possible to have access to all variables that\ndetermine the behavior of a system. Having identified a number of variables\nwhose values can be accessed, there may still be hidden variables which\ninfluence the dynamics of the system. The result is model ambiguity in the\nsense that, for the same (or very similar) input values, different objective\noutputs should have been obtained. In addition, the degree of ambiguity may\nvary widely across the whole range of input values. Thus, to evaluate the\naccuracy of a model it is of utmost importance to create a method to obtain the\ndegree of reliability of each output result. In this paper we present such a\nscheme composed of two coupled artificial neural networks: the first one being\nresponsible for outputting the predicted value, whereas the other evaluates the\nreliability of the output, which is learned from the error values of the first\none. As an illustration, the scheme is applied to a model for tracking slopes\nin a straw chamber and to a credit scoring model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 10:16:49 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Ligeiro", "Rui", ""], ["Mendes", "R. Vilela", ""]]}, {"id": "1310.1363", "submitter": "Stefan Wager", "authors": "Stefan Wager, Alexander Blocker, Niall Cardin", "title": "Weakly supervised clustering: Learning fine-grained signals from coarse\n  labels", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS812 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 801-820", "doi": "10.1214/15-AOAS812", "report-no": "IMS-AOAS-AOAS812", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a classification problem where we do not have access to labels for\nindividual training examples, but only have average labels over subpopulations.\nWe give practical examples of this setup and show how such a classification\ntask can usefully be analyzed as a weakly supervised clustering problem. We\npropose three approaches to solving the weakly supervised clustering problem,\nincluding a latent variables model that performs well in our experiments. We\nillustrate our methods on an analysis of aggregated elections data and an\nindustry data set that was the original motivation for this research.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 18:34:54 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 18:45:35 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 07:57:08 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Wager", "Stefan", ""], ["Blocker", "Alexander", ""], ["Cardin", "Niall", ""]]}, {"id": "1310.1404", "submitter": "Luke Bornn", "authors": "Michael Cherkassky and Luke Bornn", "title": "Sequential Monte Carlo Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a flexible and efficient framework for handling\nmulti-armed bandits, combining sequential Monte Carlo algorithms with\nhierarchical Bayesian modeling techniques. The framework naturally encompasses\nrestless bandits, contextual bandits, and other bandit variants under a single\ninferential model. Despite the model's generality, we propose efficient Monte\nCarlo algorithms to make inference scalable, based on recent developments in\nsequential Monte Carlo methods. Through two simulation studies, the framework\nis shown to outperform other empirical methods, while also naturally scaling to\nmore complex problems for which existing approaches can not cope. Additionally,\nwe successfully apply our framework to online video-based advertising\nrecommendation, and show its increased efficacy as compared to current state of\nthe art bandit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 20:19:56 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Cherkassky", "Michael", ""], ["Bornn", "Luke", ""]]}, {"id": "1310.1415", "submitter": "Misha Denil", "authors": "Misha Denil, David Matheson, Nando de Freitas", "title": "Narrowing the Gap: Random Forests In Theory and In Practice", "comments": "Under review by the International Conference on Machine Learning\n  (ICML) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread interest and practical use, the theoretical properties of\nrandom forests are still not well understood. In this paper we contribute to\nthis understanding in two ways. We present a new theoretically tractable\nvariant of random regression forests and prove that our algorithm is\nconsistent. We also provide an empirical evaluation, comparing our algorithm\nand other theoretically tractable random forest models to the random forest\nalgorithm used in practice. Our experiments provide insight into the relative\nimportance of different simplifications that theoreticians have made to obtain\ntractable models for analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 22:33:35 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Denil", "Misha", ""], ["Matheson", "David", ""], ["de Freitas", "Nando", ""]]}, {"id": "1310.1502", "submitter": "John T. Holodnak", "authors": "John T. Holodnak, Ilse C. F. Ipsen", "title": "Randomized Approximation of the Gram Matrix: Exact Computation and\n  Probabilistic Bounds", "comments": "Update to title in third version. Major revisions in second version\n  including new bounds and a more detailed experimental section. Submitted to\n  SIMAX", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a real matrix A with n columns, the problem is to approximate the Gram\nproduct AA^T by c << n weighted outer products of columns of A. Necessary and\nsufficient conditions for the exact computation of AA^T (in exact arithmetic)\nfrom c >= rank(A) columns depend on the right singular vector matrix of A. For\na Monte-Carlo matrix multiplication algorithm by Drineas et al. that samples\nouter products, we present probabilistic bounds for the 2-norm relative error\ndue to randomization. The bounds depend on the stable rank or the rank of A,\nbut not on the matrix dimensions. Numerical experiments illustrate that the\nbounds are informative, even for stringent success probabilities and matrices\nof small dimension. We also derive bounds for the smallest singular value and\nthe condition number of matrices obtained by sampling rows from orthonormal\nmatrices.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 18:09:50 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 19:46:17 GMT"}, {"version": "v3", "created": "Thu, 15 May 2014 16:32:14 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Holodnak", "John T.", ""], ["Ipsen", "Ilse C. F.", ""]]}, {"id": "1310.1518", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra, Amit Kumar Mishra", "title": "Contraction Principle based Robust Iterative Algorithms for Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative algorithms are ubiquitous in the field of data mining. Widely known\nexamples of such algorithms are the least mean square algorithm,\nbackpropagation algorithm of neural networks. Our contribution in this paper is\nan improvement upon this iterative algorithms in terms of their respective\nperformance metrics and robustness. This improvement is achieved by a new\nscaling factor which is multiplied to the error term. Our analysis shows that\nin essence, we are minimizing the corresponding LASSO cost function, which is\nthe reason of its increased robustness. We also give closed form expressions\nfor the number of iterations for convergence and the MSE floor of the original\ncost function for a minimum targeted value of the L1 norm. As a concluding\ntheme based on the stochastic subgradient algorithm, we give a comparison\nbetween the well known Dantzig selector and our algorithm based on contraction\nprinciple. By these simulations we attempt to show the optimality of our\napproach for any widely used parent iterative optimization problem.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 20:49:37 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Mitra", "Rangeet", ""], ["Mishra", "Amit Kumar", ""]]}, {"id": "1310.1533", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann, Jonas Peters, Jan Ernest", "title": "CAM: Causal additive models, high-dimensional order search and penalized\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1260 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2526-2556", "doi": "10.1214/14-AOS1260", "report-no": "IMS-AOS-AOS1260", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 03:12:34 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 12:31:45 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""], ["Ernest", "Jan", ""]]}, {"id": "1310.1545", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Richard Yi Da Xu, Longbing Cao, Yin Song", "title": "Learning Hidden Structures with Relational Models by Adequately\n  Involving Rich Information in A Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively modelling hidden structures in a network is very practical but\ntheoretically challenging. Existing relational models only involve very limited\ninformation, namely the binary directional link data, embedded in a network to\nlearn hidden networking structures. There is other rich and meaningful\ninformation (e.g., various attributes of entities and more granular information\nthan binary elements such as \"like\" or \"dislike\") missed, which play a critical\nrole in forming and understanding relations in a network. In this work, we\npropose an informative relational model (InfRM) framework to adequately involve\nrich information and its granularity in a network, including metadata\ninformation about each entity and various forms of link data. Firstly, an\neffective metadata information incorporation method is employed on the prior\ninformation from relational models MMSB and LFRM. This is to encourage the\nentities with similar metadata information to have similar hidden structures.\nSecondly, we propose various solutions to cater for alternative forms of link\ndata. Substantial efforts have been made towards modelling appropriateness and\nefficiency, for example, using conjugate priors. We evaluate our framework and\nits inference algorithms in different datasets, which shows the generality and\neffectiveness of our models in capturing implicit structures in networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 05:47:50 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Fan", "Xuhui", ""], ["Da Xu", "Richard Yi", ""], ["Cao", "Longbing", ""], ["Song", "Yin", ""]]}, {"id": "1310.1659", "submitter": "Dan He", "authors": "Dan He, Irina Rish, David Haws, Simon Teyssedre, Zivan Karaman, Laxmi\n  Parida", "title": "MINT: Mutual Information based Transductive Feature Selection for\n  Genetic Trait Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole genome prediction of complex phenotypic traits using high-density\ngenotyping arrays has attracted a great deal of attention, as it is relevant to\nthe fields of plant and animal breeding and genetic epidemiology. As the number\nof genotypes is generally much bigger than the number of samples, predictive\nmodels suffer from the curse-of-dimensionality. The curse-of-dimensionality\nproblem not only affects the computational efficiency of a particular genomic\nselection method, but can also lead to poor performance, mainly due to\ncorrelation among markers. In this work we proposed the first transductive\nfeature selection method based on the MRMR (Max-Relevance and Min-Redundancy)\ncriterion which we call MINT. We applied MINT on genetic trait prediction\nproblems and showed that in general MINT is a better feature selection method\nthan the state-of-the-art inductive method mRMR.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 02:26:45 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["He", "Dan", ""], ["Rish", "Irina", ""], ["Haws", "David", ""], ["Teyssedre", "Simon", ""], ["Karaman", "Zivan", ""], ["Parida", "Laxmi", ""]]}, {"id": "1310.1757", "submitter": "Iain Murray", "authors": "Benigno Uria, Iain Murray, Hugo Larochelle", "title": "A Deep and Tractable Density Estimator", "comments": "9 pages, 4 tables, 1 algorithm, 5 figures. To appear ICML 2014, JMLR\n  W&CP volume 32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Autoregressive Distribution Estimator (NADE) and its real-valued\nversion RNADE are competitive density models of multidimensional data across a\nvariety of domains. These models use a fixed, arbitrary ordering of the data\ndimensions. One can easily condition on variables at the beginning of the\nordering, and marginalize out variables at the end of the ordering, however\nother inference tasks require approximate inference. In this work we introduce\nan efficient procedure to simultaneously train a NADE model for each possible\nordering of the variables, by sharing parameters across all these models. We\ncan thus use the most convenient model for each inference task at hand, and\nensembles of such models with different orderings are immediately available.\nMoreover, unlike the original NADE, our training procedure scales to deep\nmodels. Empirically, ensembles of Deep NADE models obtain state of the art\ndensity estimation performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 12:42:41 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 17:13:56 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Uria", "Benigno", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1310.1840", "submitter": "Olivier Fercoq", "authors": "Olivier Fercoq", "title": "Parallel coordinate descent for the Adaboost problem", "comments": "7 pages, 3 figures, extended version of the paper presented to\n  ICMLA'13", "journal-ref": null, "doi": "10.1109/ICMLA.2013.72", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a randomised parallel version of Adaboost based on previous studies\non parallel coordinate descent. The algorithm uses the fact that the logarithm\nof the exponential loss is a function with coordinate-wise Lipschitz continuous\ngradient, in order to define the step lengths. We provide the proof of\nconvergence for this randomised Adaboost algorithm and a theoretical\nparallelisation speedup factor. We finally provide numerical examples on\nlearning problems of various sizes that show that the algorithm is competitive\nwith concurrent approaches, especially for large scale problems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 16:04:28 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Fercoq", "Olivier", ""]]}, {"id": "1310.1934", "submitter": "Nikos Karampatziakis", "authors": "Nikos Karampatziakis, Paul Mineiro", "title": "Discriminative Features via Generalized Eigenvectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing examples in a way that is compatible with the underlying\nclassifier can greatly enhance the performance of a learning system. In this\npaper we investigate scalable techniques for inducing discriminative features\nby taking advantage of simple second order structure in the data. We focus on\nmulticlass classification and show that features extracted from the generalized\neigenvectors of the class conditional second moments lead to classifiers with\nexcellent empirical performance. Moreover, these features have attractive\ntheoretical properties, such as inducing representations that are invariant to\nlinear transformations of the input. We evaluate classifiers built from these\nfeatures on three different tasks, obtaining state of the art results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:05:52 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Karampatziakis", "Nikos", ""], ["Mineiro", "Paul", ""]]}, {"id": "1310.1947", "submitter": "Frank Hutter", "authors": "Frank Hutter and Holger Hoos and Kevin Leyton-Brown", "title": "Bayesian Optimization With Censored Response Data", "comments": "Extended version of NIPS 2011 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) aims to minimize a given blackbox function using a\nmodel that is updated whenever new evidence about the function becomes\navailable. Here, we address the problem of BO under partially right-censored\nresponse data, where in some evaluations we only obtain a lower bound on the\nfunction value. The ability to handle such response data allows us to\nadaptively censor costly function evaluations in minimization problems where\nthe cost of a function evaluation corresponds to the function value. One\nimportant application giving rise to such censored data is the\nruntime-minimizing variant of the algorithm configuration problem: finding\nsettings of a given parametric algorithm that minimize the runtime required for\nsolving problem instances from a given distribution. We demonstrate that\nterminating slow algorithm runs prematurely and handling the resulting\nright-censored observations can substantially improve the state of the art in\nmodel-based algorithm configuration.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:43:16 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Hutter", "Frank", ""], ["Hoos", "Holger", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1310.1949", "submitter": "Nikos Karampatziakis", "authors": "Alekh Agarwal, Sham M. Kakade, Nikos Karampatziakis, Le Song, Gregory\n  Valiant", "title": "Least Squares Revisited: Scalable Approaches for Multi-class Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides simple algorithms for multi-class (and multi-label)\nprediction in settings where both the number of examples n and the data\ndimension d are relatively large. These robust and parameter free algorithms\nare essentially iterative least-squares updates and very versatile both in\ntheory and in practice. On the theoretical front, we present several variants\nwith convergence guarantees. Owing to their effective use of second-order\nstructure, these algorithms are substantially better than first-order methods\nin many practical scenarios. On the empirical side, we present a scalable\nstagewise variant of our approach, which achieves dramatic computational\nspeedups over popular optimization packages such as Liblinear and Vowpal Wabbit\non standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art\naccuracies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 15:18:37 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Agarwal", "Alekh", ""], ["Kakade", "Sham M.", ""], ["Karampatziakis", "Nikos", ""], ["Song", "Le", ""], ["Valiant", "Gregory", ""]]}, {"id": "1310.2049", "submitter": "Zhi-Hua Zhou", "authors": "Sheng-Jun Huang and Zhi-Hua Zhou", "title": "Fast Multi-Instance Multi-Label Learning", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019, 41(11): 2614-2627", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world tasks, particularly those involving data objects with\ncomplicated semantics such as images and texts, one object can be represented\nby multiple instances and simultaneously be associated with multiple labels.\nSuch tasks can be formulated as multi-instance multi-label learning (MIML)\nproblems, and have been extensively studied during the past few years. Existing\nMIML approaches have been found useful in many applications; however, most of\nthem can only handle moderate-sized data. To efficiently handle large data\nsets, in this paper we propose the MIMLfast approach, which first constructs a\nlow-dimensional subspace shared by all labels, and then trains label specific\nlinear models to optimize approximated ranking loss via stochastic gradient\ndescent. Although the MIML problem is complicated, MIMLfast is able to achieve\nexcellent performance by exploiting label relations with shared space and\ndiscovering sub-concepts for complicated labels. Experiments show that the\nperformance of MIMLfast is highly competitive to state-of-the-art techniques,\nwhereas its time cost is much less; particularly, on a data set with 20K bags\nand 180K instances, MIMLfast is more than 100 times faster than existing MIML\napproaches. On a larger data set where none of existing approaches can return\nresults in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is\nable to identify the most representative instance for each label, and thus\nproviding a chance to understand the relation between input patterns and output\nlabel semantics.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 09:03:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Huang", "Sheng-Jun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1310.2059", "submitter": "Peter Richtarik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Distributed Coordinate Descent Method for Learning with Big Data", "comments": "11 two-column pages, 1 algorithm, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method\nfor solving loss minimization problems with big data. We initially partition\nthe coordinates (features) and assign each partition to a different node of a\ncluster. At every iteration, each node picks a random subset of the coordinates\nfrom those it owns, independently from the other computers, and in parallel\ncomputes and applies updates to the selected coordinates based on a simple\nclosed-form formula. We give bounds on the number of iterations sufficient to\napproximately solve the problem with high probability, and show how it depends\non the data and on the partitioning. We perform numerical experiments with a\nLASSO instance described by a 3TB matrix.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 09:31:27 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1310.2071", "submitter": "Rohit Jha", "authors": "Kalpesh Adhatrao, Aditya Gaykar, Amiraj Dhawan, Rohit Jha and Vipul\n  Honrao", "title": "Predicting Students' Performance Using ID3 And C4.5 Classification\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.5121/ijdkp.2013.3504", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An educational institution needs to have an approximate prior knowledge of\nenrolled students to predict their performance in future academics. This helps\nthem to identify promising students and also provides them an opportunity to\npay attention to and improve those who would probably get lower grades. As a\nsolution, we have developed a system which can predict the performance of\nstudents from their previous performances using concepts of data mining\ntechniques under Classification. We have analyzed the data set containing\ninformation about students, such as gender, marks scored in the board\nexaminations of classes X and XII, marks and rank in entrance examinations and\nresults in first year of the previous batch of students. By applying the ID3\n(Iterative Dichotomiser 3) and C4.5 classification algorithms on this data, we\nhave predicted the general and individual performance of freshly admitted\nstudents in future examinations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:12:15 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Adhatrao", "Kalpesh", ""], ["Gaykar", "Aditya", ""], ["Dhawan", "Amiraj", ""], ["Jha", "Rohit", ""], ["Honrao", "Vipul", ""]]}, {"id": "1310.2273", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Stephen A. Vavasis", "title": "Semidefinite Programming Based Preconditioning for More Robust\n  Near-Separable Nonnegative Matrix Factorization", "comments": "25 pages, 6 figures, 4 tables. New numerical experiments, additional\n  remarks and comments", "journal-ref": "SIAM Journal on Optimization 25 (1), pp. 677-698, 2015", "doi": "10.1137/130940670", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) under the separability assumption can\nprovably be solved efficiently, even in the presence of noise, and has been\nshown to be a powerful technique in document classification and hyperspectral\nunmixing. This problem is referred to as near-separable NMF and requires that\nthere exists a cone spanned by a small subset of the columns of the input\nnonnegative matrix approximately containing all columns. In this paper, we\npropose a preconditioning based on semidefinite programming making the input\nmatrix well-conditioned. This in turn can improve significantly the performance\nof near-separable NMF algorithms which is illustrated on the popular successive\nprojection algorithm (SPA). The new preconditioned SPA is provably more robust\nto noise, and outperforms SPA on several synthetic data sets. We also show how\nan active-set method allow us to apply the preconditioning on large-scale\nreal-world hyperspectral images.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 20:30:38 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 09:11:30 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1310.2408", "submitter": "Jun Zhu", "authors": "Jun Zhu, Xun Zheng, Bo Zhang", "title": "Improved Bayesian Logistic Supervised Topic Models with Data\n  Augmentation", "comments": "9 pages, ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models with a logistic likelihood have two issues that\npotentially limit their practical use: 1) response variables are usually\nover-weighted by document word counts; and 2) existing variational inference\nmethods make strict mean-field assumptions. We address these issues by: 1)\nintroducing a regularization constant to better balance the two parts based on\nan optimization formulation of Bayesian inference; and 2) developing a simple\nGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and\ncollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm\nhas analytical forms of each conditional distribution without making any\nrestricting assumptions and can be easily parallelized. Empirical results\ndemonstrate significant improvements on prediction performance and time\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:23:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhu", "Jun", ""], ["Zheng", "Xun", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2409", "submitter": "Ning Chen", "authors": "Ning Chen, Jun Zhu, Fei Xia, Bo Zhang", "title": "Discriminative Relational Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:32:56 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Xia", "Fei", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2451", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (LIF), Hachem Kadri (LIF)", "title": "M-Power Regularized Least Squares Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is used to find a solution that both fits the data and is\nsufficiently smooth, and thereby is very effective for designing and refining\nlearning algorithms. But the influence of its exponent remains poorly\nunderstood. In particular, it is unclear how the exponent of the reproducing\nkernel Hilbert space~(RKHS) regularization term affects the accuracy and the\nefficiency of kernel-based learning algorithms. Here we consider regularized\nleast squares regression (RLSR) with an RKHS regularization raised to the power\nof m, where m is a variable real exponent. We design an efficient algorithm for\nsolving the associated minimization problem, we provide a theoretical analysis\nof its stability, and we compare its advantage with respect to computational\ncomplexity, speed of convergence and prediction accuracy to the classical\nkernel ridge regression algorithm where the regularization exponent m is fixed\nat 2. Our results show that the m-power RLSR problem can be solved efficiently,\nand support the suggestion that one can use a regularization term that grows\nsignificantly slower than the standard quadratic growth in the RKHS norm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 12:18:29 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 13:45:18 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Audiffren", "Julien", "", "LIF"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1310.2627", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Bryan R. Routledge and Noah A. Smith", "title": "A Sparse and Adaptive Prior for Time-Dependent Model Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the scenario where the parameters of a probabilistic model are\nexpected to vary over time. We construct a novel prior distribution that\npromotes sparsity and adapts the strength of correlation between parameters at\nsuccessive timesteps, based on the data. We derive approximate variational\ninference procedures for learning and prediction with this prior. We test the\napproach on two tasks: forecasting financial quantities from relevant text, and\nmodeling language contingent on time-varying financial measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 20:39:08 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 05:11:48 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Yogatama", "Dani", ""], ["Routledge", "Bryan R.", ""], ["Smith", "Noah A.", ""]]}, {"id": "1310.2646", "submitter": "Akshay Gadde", "authors": "Sunil K. Narang, Akshay Gadde, Eduard Sanou and Antonio Ortega", "title": "Localized Iterative Methods for Interpolation in Graph Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two localized graph filtering based methods for\ninterpolating graph signals defined on the vertices of arbitrary graphs from\nonly a partial set of samples. The first method is an extension of previous\nwork on reconstructing bandlimited graph signals from partially observed\nsamples. The iterative graph filtering approach very closely approximates the\nsolution proposed in the that work, while being computationally more efficient.\nAs an alternative, we propose a regularization based framework in which we\ndefine the cost of reconstruction to be a combination of smoothness of the\ngraph signal and the reconstruction error with respect to the known samples,\nand find solutions that minimize this cost. We provide both a closed form\nsolution and a computationally efficient iterative solution of the optimization\nproblem. The experimental results on the recommendation system datasets\ndemonstrate effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 22:24:28 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Narang", "Sunil K.", ""], ["Gadde", "Akshay", ""], ["Sanou", "Eduard", ""], ["Ortega", "Antonio", ""]]}, {"id": "1310.2700", "submitter": "Marvin Weinstein", "authors": "M. Weinstein, F. Meirer, A. Hume, Ph. Sciau, G. Shaked, R. Hofstetter,\n  E. Persi, A. Mehta, and D. Horn", "title": "Analyzing Big Data with Dynamic Quantum Clustering", "comments": "37 pages, 22 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does one search for a needle in a multi-dimensional haystack without\nknowing what a needle is and without knowing if there is one in the haystack?\nThis kind of problem requires a paradigm shift - away from hypothesis driven\nsearches of the data - towards a methodology that lets the data speak for\nitself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a\npowerful visual method that works with big, high-dimensional data. It exploits\nvariations of the density of the data (in feature space) and unearths subsets\nof the data that exhibit correlations among all the measured variables. The\noutcome of a DQC analysis is a movie that shows how and why sets of data-points\nare eventually classified as members of simple clusters or as members of - what\nwe call - extended structures. This allows DQC to be successfully used in a\nnon-conventional exploratory mode where one searches data for unexpected\ninformation without the need to model the data. We show how this works for big,\ncomplex, real-world datasets that come from five distinct fields: i.e., x-ray\nnano-chemistry, condensed matter, biology, seismology and finance. These\nstudies show how DQC excels at uncovering unexpected, small - but meaningful -\nsubsets of the data that contain important information. We also establish an\nimportant new result: namely, that big, complex datasets often contain\ninteresting structures that will be missed by many conventional clustering\ntechniques. Experience shows that these structures appear frequently enough\nthat it is crucial to know they can exist, and that when they do, they encode\nimportant hidden information. In short, we not only demonstrate that DQC can be\nflexibly applied to datasets that present significantly different challenges,\nwe also show how a simple analysis can be used to look for the needle in the\nhaystack, determine what it is, and find what this means.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 04:00:03 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 21:06:22 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Weinstein", "M.", ""], ["Meirer", "F.", ""], ["Hume", "A.", ""], ["Sciau", "Ph.", ""], ["Shaked", "G.", ""], ["Hofstetter", "R.", ""], ["Persi", "E.", ""], ["Mehta", "A.", ""], ["Horn", "D.", ""]]}, {"id": "1310.2797", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "Lemma Mining over HOL Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large formal mathematical libraries consist of millions of atomic inference\nsteps that give rise to a corresponding number of proved statements (lemmas).\nAnalogously to the informal mathematical practice, only a tiny fraction of such\nstatements is named and re-used in later proofs by formal mathematicians. In\nthis work, we suggest and implement criteria defining the estimated usefulness\nof the HOL Light lemmas for proving further theorems. We use these criteria to\nmine the large inference graph of all lemmas in the core HOL Light library,\nadding thousands of the best lemmas to the pool of named statements that can be\nre-used in later proofs. The usefulness of the new lemmas is then evaluated by\ncomparing the performance of automated proving of the core HOL Light theorems\nwith and without such added lemmas.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 12:53:04 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1310.2805", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "MizAR 40 for Mizar 40", "comments": null, "journal-ref": "J. Automated Reasoning 55(3): 245-256 (2015)", "doi": "10.1007/s10817-015-9330-8", "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a present to Mizar on its 40th anniversary, we develop an AI/ATP system\nthat in 30 seconds of real time on a 14-CPU machine automatically proves 40% of\nthe theorems in the latest official version of the Mizar Mathematical Library\n(MML). This is a considerable improvement over previous performance of large-\ntheory AI/ATP methods measured on the whole MML. To achieve that, a large suite\nof AI/ATP methods is employed and further developed. We implement the most\nuseful methods efficiently, to scale them to the 150000 formulas in MML. This\nreduces the training times over the corpus to 1-3 seconds, allowing a simple\npractical deployment of the methods in the online automated reasoning service\nfor the Mizar users (MizAR).\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:24:07 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1310.2816", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang", "title": "Gibbs Max-margin Topic Models with Data Augmentation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-margin learning is a powerful approach to building classifiers and\nstructured output predictors. Recent work on max-margin supervised topic models\nhas successfully integrated it with Bayesian topic models to discover\ndiscriminative latent semantic structures and make accurate predictions for\nunseen testing data. However, the resulting learning problems are usually hard\nto solve because of the non-smoothness of the margin loss. Existing approaches\nto building max-margin supervised topic models rely on an iterative procedure\nto solve multiple latent SVM subproblems with additional mean-field assumptions\non the desired posterior distributions. This paper presents an alternative\napproach by defining a new max-margin loss. Namely, we present Gibbs max-margin\nsupervised topic models, a latent variable Gibbs classifier to discover hidden\ntopic representations for various tasks, including classification, regression\nand multi-task learning. Gibbs max-margin supervised topic models minimize an\nexpected margin loss, which is an upper bound of the existing margin loss\nderived from an expected prediction rule. By introducing augmented variables\nand integrating out the Dirichlet variables analytically by conjugacy, we\ndevelop simple Gibbs sampling algorithms with no restricting assumptions and no\nneed to solve SVM subproblems. Furthermore, each step of the\n\"augment-and-collapse\" Gibbs sampling algorithms has an analytical conditional\ndistribution, from which samples can be easily drawn. Experimental results\ndemonstrate significant improvements on time efficiency. The classification\nperformance is also significantly improved over competitors on binary,\nmulti-class and multi-label classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:47:40 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Perkins", "Hugh", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2880", "submitter": "Adrian Barbu", "authors": "Adrian Barbu, Yiyuan She, Liangjing Ding, Gary Gramajo", "title": "Feature Selection with Annealing for Computer Vision and Big Data\n  Learning", "comments": "18 pages, 9 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 39, no 2, pp 272 - 286, 2017", "doi": "10.1109/TPAMI.2016.2544315", "report-no": null, "categories": "stat.ML cs.CV cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and medical imaging problems are faced with learning\nfrom large-scale datasets, with millions of observations and features. In this\npaper we propose a novel efficient learning scheme that tightens a sparsity\nconstraint by gradually removing variables based on a criterion and a schedule.\nThe attractive fact that the problem size keeps dropping throughout the\niterations makes it particularly suitable for big data learning. Our approach\napplies generically to the optimization of any differentiable loss function,\nand finds applications in regression, classification and ranking. The resultant\nalgorithms build variable screening into estimation and are extremely simple to\nimplement. We provide theoretical guarantees of convergence and selection\nconsistency. In addition, one dimensional piecewise linear response functions\nare used to account for nonlinearity and a second order prior is imposed on\nthese functions to avoid overfitting. Experiments on real and synthetic data\nshow that the proposed method compares very well with other state of the art\nmethods in regression, classification and ranking while being computationally\nvery efficient and scalable.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 16:47:22 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 22:42:51 GMT"}, {"version": "v3", "created": "Tue, 30 Sep 2014 00:33:36 GMT"}, {"version": "v4", "created": "Wed, 1 Oct 2014 20:03:42 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2015 13:20:26 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 02:02:20 GMT"}, {"version": "v7", "created": "Thu, 17 Mar 2016 14:55:09 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Barbu", "Adrian", ""], ["She", "Yiyuan", ""], ["Ding", "Liangjing", ""], ["Gramajo", "Gary", ""]]}, {"id": "1310.2931", "submitter": "Stefan Wager", "authors": "Stefan Wager, Nick Chamandy, Omkar Muralidharan, and Amir Najmi", "title": "Feedback Detection for Live Predictors", "comments": "Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictor that is deployed in a live production system may perturb the\nfeatures it uses to make predictions. Such a feedback loop can occur, for\nexample, when a model that predicts a certain type of behavior ends up causing\nthe behavior it predicts, thus creating a self-fulfilling prophecy. In this\npaper we analyze predictor feedback detection as a causal inference problem,\nand introduce a local randomization scheme that can be used to detect\nnon-linear feedback in real-world problems. We conduct a pilot study for our\nproposed methodology using a predictive system currently deployed as a part of\na search engine.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 19:57:45 GMT"}, {"version": "v2", "created": "Sat, 1 Nov 2014 01:48:35 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Chamandy", "Nick", ""], ["Muralidharan", "Omkar", ""], ["Najmi", "Amir", ""]]}, {"id": "1310.2955", "submitter": "Marc Pickett", "authors": "Marc Pickett and David W. Aha", "title": "Spontaneous Analogy by Piggybacking on a Perceptual System", "comments": "Proceedings of the 35th Meeting of the Cognitive Science Society,\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computational models of analogy assume they are given a delineated\nsource domain and often a specified target domain. These systems do not address\nhow analogs can be isolated from large domains and spontaneously retrieved from\nlong-term memory, a process we call spontaneous analogy. We present a system\nthat represents relational structures as feature bags. Using this\nrepresentation, our system leverages perceptual algorithms to automatically\ncreate an ontology of relational structures and to efficiently retrieve analogs\nfor new relational structures from long-term memory. We provide a demonstration\nof our approach that takes a set of unsegmented stories, constructs an ontology\nof analogical schemas (corresponding to plot devices), and uses this ontology\nto efficiently find analogs within new stories, yielding significant\ntime-savings over linear analog retrieval at a small accuracy cost.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 20:22:33 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Pickett", "Marc", ""], ["Aha", "David W.", ""]]}, {"id": "1310.2959", "submitter": "Partha Talukdar", "authors": "Partha Pratim Talukdar, William Cohen", "title": "Scaling Graph-based Semi Supervised Learning to Large Number of Labels\n  Using Count-Min Sketch", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based Semi-supervised learning (SSL) algorithms have been successfully\nused in a large number of applications. These methods classify initially\nunlabeled nodes by propagating label information over the structure of graph\nstarting from seed nodes. Graph-based SSL algorithms usually scale linearly\nwith the number of distinct labels (m), and require O(m) space on each node.\nUnfortunately, there exist many applications of practical significance with\nvery large m over large graphs, demanding better space and time complexity. In\nthis paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which\ncompactly stores label distribution on each node using Count-min Sketch, a\nrandomized data structure. We present theoretical analysis showing that under\nmild conditions, MAD-SKETCH can reduce space complexity at each node from O(m)\nto O(log m), and achieve similar savings in time complexity as well. We support\nour analysis through experiments on multiple real world datasets. We observe\nthat MAD-SKETCH achieves similar performance as existing state-of-the-art\ngraph- based SSL algorithms, while requiring smaller memory footprint and at\nthe same time achieving up to 10x speedup. We find that MAD-SKETCH is able to\nscale to datasets with one million labels, which is beyond the scope of\nexisting graph- based SSL algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 20:30:06 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 21:19:41 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Talukdar", "Partha Pratim", ""], ["Cohen", "William", ""]]}, {"id": "1310.2997", "submitter": "Tomer Koren", "authors": "Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres", "title": "Bandits with Switching Costs: T^{2/3} Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adversarial multi-armed bandit problem in a setting where the\nplayer incurs a unit cost each time he switches actions. We prove that the\nplayer's $T$-round minimax regret in this setting is\n$\\widetilde{\\Theta}(T^{2/3})$, thereby closing a fundamental gap in our\nunderstanding of learning with bandit feedback. In the corresponding\nfull-information version of the problem, the minimax regret is known to grow at\na much slower rate of $\\Theta(\\sqrt{T})$. The difference between these two\nrates provides the \\emph{first} indication that learning with bandit feedback\ncan be significantly harder than learning with full-information feedback\n(previous results only showed a different dependence on the number of actions,\nbut not on $T$.)\n  In addition to characterizing the inherent difficulty of the multi-armed\nbandit problem with switching costs, our results also resolve several other\nopen problems in online learning. One direct implication is that learning with\nbandit feedback against bounded-memory adaptive adversaries has a minimax\nregret of $\\widetilde{\\Theta}(T^{2/3})$. Another implication is that the\nminimax regret of online learning in adversarial Markov decision processes\n(MDPs) is $\\widetilde{\\Theta}(T^{2/3})$. The key to all of our results is a new\nrandomized construction of a multi-scale random walk, which is of independent\ninterest and likely to prove useful in additional settings.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 02:01:53 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 07:13:05 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Dekel", "Ofer", ""], ["Ding", "Jian", ""], ["Koren", "Tomer", ""], ["Peres", "Yuval", ""]]}, {"id": "1310.3099", "submitter": "Roland Maas", "authors": "Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann", "title": "A Bayesian Network View on Acoustic Model-Based Techniques for Robust\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a unifying Bayesian network view on various approaches\nfor acoustic model adaptation, missing feature, and uncertainty decoding that\nare well-known in the literature of robust automatic speech recognition. The\nrepresentatives of these classes can often be deduced from a Bayesian network\nthat extends the conventional hidden Markov models used in speech recognition.\nThese extensions, in turn, can in many cases be motivated from an underlying\nobservation model that relates clean and distorted feature vectors. By\nconverting the observation models into a Bayesian network representation, we\nformulate the corresponding compensation rules leading to a unified view on\nknown derivations as well as to new formulations for certain approaches. The\ngeneric Bayesian perspective provided in this contribution thus highlights\nstructural differences and similarities between the analyzed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:07:57 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 13:52:44 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Maas", "Roland", ""], ["Huemmer", "Christian", ""], ["Sehr", "Armin", ""], ["Kellermann", "Walter", ""]]}, {"id": "1310.3101", "submitter": "Eric Strobl", "authors": "Eric Strobl, Shyam Visweswaran", "title": "Deep Multiple Kernel Learning", "comments": "4 pages, 1 figure, 1 table, conference paper", "journal-ref": "IEEE 12th International Conference on Machine Learning and\n  Applications (ICMLA 2013)", "doi": "10.1109/ICMLA.2013.84", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Deep learning methods have predominantly been applied to large artificial\nneural networks. Despite their state-of-the-art performance, these large\nnetworks typically do not generalize well to datasets with limited sample\nsizes. In this paper, we take a different approach by learning multiple layers\nof kernels. We combine kernels at each layer and then optimize over an estimate\nof the support vector machine leave-one-out error rather than the dual\nobjective function. Our experiments on a variety of datasets show that each\nlayer successively increases performance with only a few base kernels.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:14:00 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Strobl", "Eric", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1310.3333", "submitter": "Sriramkumar Balasubramanian", "authors": "Sriramkumar Balasubramanian and Raghuram Reddy Nagireddy", "title": "Visualizing Bags of Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is two-fold - a) to compare between two different\nmodes of visualizing data that exists in a bag of vectors format b) to propose\na theoretical model that supports a new mode of visualizing data. Visualizing\nhigh dimensional data can be achieved using Minimum Volume Embedding, but the\ndata has to exist in a format suitable for computing similarities while\npreserving local distances. This paper compares the visualization between two\nmethods of representing data and also proposes a new method providing sample\nvisualizations for that method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 03:48:38 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Balasubramanian", "Sriramkumar", ""], ["Nagireddy", "Raghuram Reddy", ""]]}, {"id": "1310.3407", "submitter": "Sameh Sorour", "authors": "Sameh Sorour, Yves Lostanlen, Shahrokh Valaee", "title": "Joint Indoor Localization and Radio Map Construction with Limited\n  Deployment Load", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major bottleneck in the practical implementation of received signal\nstrength (RSS) based indoor localization systems is the extensive deployment\nefforts required to construct the radio maps through fingerprinting. In this\npaper, we aim to design an indoor localization scheme that can be directly\nemployed without building a full fingerprinted radio map of the indoor\nenvironment. By accumulating the information of localized RSSs, this scheme can\nalso simultaneously construct the radio map with limited calibration. To design\nthis scheme, we employ a source data set that possesses the same spatial\ncorrelation of the RSSs in the indoor environment under study. The knowledge of\nthis data set is then transferred to a limited number of calibration\nfingerprints and one or several RSS observations with unknown locations, in\norder to perform direct localization of these observations using manifold\nalignment. We test two different source data sets, namely a simulated radio\npropagation map and the environments plan coordinates. For moving users, we\nexploit the correlation of their observations to improve the localization\naccuracy. The online testing in two indoor environments shows that the plan\ncoordinates achieve better results than the simulated radio maps, and a\nnegligible degradation with 70-85% reduction in calibration load.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 17:20:41 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Sorour", "Sameh", ""], ["Lostanlen", "Yves", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "1310.3492", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Xiangnan Kong, Philip S. Yu", "title": "Predicting Social Links for New Users across Aligned Heterogeneous\n  Social Networks", "comments": "11 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks have gained great success in recent years and many of\nthem involve multiple kinds of nodes and complex relationships. Among these\nrelationships, social links among users are of great importance. Many existing\nlink prediction methods focus on predicting social links that will appear in\nthe future among all users based upon a snapshot of the social network. In\nreal-world social networks, many new users are joining in the service every\nday. Predicting links for new users are more important. Different from\nconventional link prediction problems, link prediction for new users are more\nchallenging due to the following reasons: (1) differences in information\ndistributions between new users and the existing active users (i.e., old\nusers); (2) lack of information from the new users in the network. We propose a\nlink prediction method called SCAN-PS (Supervised Cross Aligned Networks link\nprediction with Personalized Sampling), to solve the link prediction problem\nfor new users with information transferred from both the existing active users\nin the target network and other source networks through aligned accounts. We\nproposed a within-target-network personalized sampling method to process the\nexisting active users' information in order to accommodate the differences in\ninformation distributions before the intra-network knowledge transfer. SCAN-PS\ncan also exploit information in other source networks, where the user accounts\nare aligned with the target network. In this way, SCAN-PS could solve the cold\nstart problem when information of these new users is total absent in the target\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 16:35:00 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Zhang", "Jiawei", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1310.3556", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Srinivas Nambirajan, Petros Drineas", "title": "Identifying Influential Entries in a Matrix", "comments": "There is a bug in the proof of Lemma 5, which we are currently\n  working to fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any matrix A in R^(m x n) of rank \\rho, we present a probability\ndistribution over the entries of A (the element-wise leverage scores of\nequation (2)) that reveals the most influential entries in the matrix. From a\ntheoretical perspective, we prove that sampling at most s = O ((m + n) \\rho^2\nln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s)\nwith respect to these scores and solving the nuclear norm minimization problem\non the sampled entries, reconstructs A exactly. To the best of our knowledge,\nthese are the strongest theoretical guarantees on matrix completion without any\nincoherence assumptions on the matrix A. From an experimental perspective, we\nshow that entries corresponding to high element-wise leverage scores reveal\nstructural properties of the data matrix that are of interest to domain\nscientists.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 03:49:02 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 12:13:32 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Kundu", "Abhisek", ""], ["Nambirajan", "Srinivas", ""], ["Drineas", "Petros", ""]]}, {"id": "1310.3567", "submitter": "Adam Vaughan", "authors": "Adam Vaughan and Stanislav V. Bohac", "title": "An Extreme Learning Machine Approach to Predicting Near Chaotic HCCI\n  Combustion Phasing in Real-Time", "comments": "11 pages, 7 figures, minor revision (added implementation details and\n  video link), submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuel efficient Homogeneous Charge Compression Ignition (HCCI) engine\ncombustion timing predictions must contend with non-linear chemistry,\nnon-linear physics, period doubling bifurcation(s), turbulent mixing, model\nparameters that can drift day-to-day, and air-fuel mixture state information\nthat cannot typically be resolved on a cycle-to-cycle basis, especially during\ntransients. In previous work, an abstract cycle-to-cycle mapping function\ncoupled with $\\epsilon$-Support Vector Regression was shown to predict\nexperimentally observed cycle-to-cycle combustion timing over a wide range of\nengine conditions, despite some of the aforementioned difficulties. The main\nlimitation of the previous approach was that a partially acausual randomly\nsampled training dataset was used to train proof of concept offline\npredictions. The objective of this paper is to address this limitation by\nproposing a new online adaptive Extreme Learning Machine (ELM) extension named\nWeighted Ring-ELM. This extension enables fully causal combustion timing\npredictions at randomly chosen engine set points, and is shown to achieve\nresults that are as good as or better than the previous offline method. The\nbroader objective of this approach is to enable a new class of real-time model\npredictive control strategies for high variability HCCI and, ultimately, to\nbring HCCI's low engine-out NOx and reduced CO2 emissions to production\nengines.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 06:00:31 GMT"}, {"version": "v2", "created": "Wed, 24 Sep 2014 16:52:27 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 20:23:49 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Vaughan", "Adam", ""], ["Bohac", "Stanislav V.", ""]]}, {"id": "1310.3607", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann, Sruthi Moorthy and Zifan Shi", "title": "Predicting college basketball match outcomes using machine learning\n  techniques: some results and lessons learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing work on predicting NCAAB matches has been developed in a\nstatistical context. Trusting the capabilities of ML techniques, particularly\nclassification learners, to uncover the importance of features and learn their\nrelationships, we evaluated a number of different paradigms on this task. In\nthis paper, we summarize our work, pointing out that attributes seem to be more\nimportant than models, and that there seems to be an upper limit to predictive\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 09:42:54 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Zimmermann", "Albrecht", ""], ["Moorthy", "Sruthi", ""], ["Shi", "Zifan", ""]]}, {"id": "1310.3609", "submitter": "Sean Sedwards", "authors": "Axel Legay, Sean Sedwards and Louis-Marie Traonouez", "title": "Scalable Verification of Markov Decision Processes", "comments": "V4: FMDS version, 12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes (MDP) are useful to model concurrent process\noptimisation problems, but verifying them with numerical methods is often\nintractable. Existing approximative approaches do not scale well and are\nlimited to memoryless schedulers. Here we present the basis of scalable\nverification for MDPSs, using an O(1) memory representation of\nhistory-dependent schedulers. We thus facilitate scalable learning techniques\nand the use of massively parallel verification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 09:50:49 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 13:17:58 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 07:31:27 GMT"}, {"version": "v4", "created": "Wed, 17 Sep 2014 11:07:09 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Legay", "Axel", ""], ["Sedwards", "Sean", ""], ["Traonouez", "Louis-Marie", ""]]}, {"id": "1310.3697", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Shie Mannor", "title": "Variance Adjusted Actor Critic Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an actor-critic framework for MDPs where the objective is the\nvariance-adjusted expected return. Our critic uses linear function\napproximation, and we extend the concept of compatible features to the\nvariance-adjusted setting. We present an episodic actor-critic algorithm and\nshow that it converges almost surely to a locally optimal point of the\nobjective function.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 14:36:22 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1310.3892", "submitter": "Bradley Price", "authors": "Bradley S. Price, Charles J. Geyer, and Adam J. Rothman", "title": "Ridge Fusion in Statistical Learning", "comments": "24 pages and 9 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to jointly estimate multiple\nprecision matrices for use in quadratic discriminant analysis and model based\nclustering. A ridge penalty and a ridge fusion penalty are used to introduce\nshrinkage and promote similarity between precision matrix estimates. Block-wise\ncoordinate descent is used for optimization, and validation likelihood is used\nfor tuning parameter selection. Our method is applied in quadratic discriminant\nanalysis and semi-supervised model based clustering.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 01:27:14 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 21:36:52 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 13:10:03 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Price", "Bradley S.", ""], ["Geyer", "Charles J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1310.4210", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo", "title": "Demystifying Information-Theoretic Clustering", "comments": "Proceedings of The 31st International Conference on Machine Learning\n  (ICML), 2014. 11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for clustering data which is grounded in\ninformation-theoretic principles and requires no parametric assumptions.\nPrevious attempts to use information theory to define clusters in an\nassumption-free way are based on maximizing mutual information between data and\ncluster labels. We demonstrate that this intuition suffers from a fundamental\nconceptual flaw that causes clustering performance to deteriorate as the amount\nof data increases. Instead, we return to the axiomatic foundations of\ninformation theory to define a meaningful clustering measure based on the\nnotion of consistency under coarse-graining for finite data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 21:19:22 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 22:21:06 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""], ["Sha", "Fei", ""], ["DeDeo", "Simon", ""]]}, {"id": "1310.4223", "submitter": "Hamidreza Chitsaz", "authors": "Hamidreza Chitsaz, Mohammad Aminisharifabad", "title": "Exact Learning of RNA Energy Parameters From Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exact learning of parameters of a linear RNA\nenergy model from secondary structure data. A necessary and sufficient\ncondition for learnability of parameters is derived, which is based on\ncomputing the convex hull of union of translated Newton polytopes of input\nsequences. The set of learned energy parameters is characterized as the convex\ncone generated by the normal vectors to those facets of the resulting polytope\nthat are incident to the origin. In practice, the sufficient condition may not\nbe satisfied by the entire training data set; hence, computing a maximal subset\nof training data for which the sufficient condition is satisfied is often\ndesired. We show that problem is NP-hard in general for an arbitrary\ndimensional feature space. Using a randomized greedy algorithm, we select a\nsubset of RNA STRAND v2.0 database that satisfies the sufficient condition for\nseparate A-U, C-G, G-U base pair counting model. The set of learned energy\nparameters includes experimentally measured energies of A-U, C-G, and G-U\npairs; hence, our parameter set is in agreement with the Turner parameters.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 23:04:00 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Chitsaz", "Hamidreza", ""], ["Aminisharifabad", "Mohammad", ""]]}, {"id": "1310.4227", "submitter": "Francesco Orabona", "authors": "Francesco Orabona, Tamir Hazan, Anand D. Sarwate, Tommi Jaakkola", "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum a-posteriori (MAP) perturbation framework has emerged as a useful\napproach for inference and learning in high dimensional complex models. By\nmaximizing a randomly perturbed potential function, MAP perturbations generate\nunbiased samples from the Gibbs distribution. Unfortunately, the computational\ncost of generating so many high-dimensional random variables can be\nprohibitive. More efficient algorithms use sequential sampling strategies based\non the expected value of low dimensional MAP perturbations. This paper develops\nnew measure concentration inequalities that bound the number of samples needed\nto estimate such expected values. Applying the general result to MAP\nperturbations can yield a more efficient algorithm to approximate sampling from\nthe Gibbs distribution. The measure concentration result is of general interest\nand may be applicable to other areas involving expected estimations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 23:30:52 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Orabona", "Francesco", ""], ["Hazan", "Tamir", ""], ["Sarwate", "Anand D.", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1310.4252", "submitter": "Sihong Xie", "authors": "Sihong Xie and Xiangnan Kong and Jing Gao and Wei Fan and Philip S.Yu", "title": "Multilabel Consensus Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, a large amount of noisy and incomplete data can be\ncollected from multiple sources for prediction tasks. Combining multiple models\nor data sources helps to counteract the effects of low data quality and the\nbias of any single model or data source, and thus can improve the robustness\nand the performance of predictive models. Out of privacy, storage and bandwidth\nconsiderations, in certain circumstances one has to combine the predictions\nfrom multiple models or data sources to obtain the final predictions without\naccessing the raw data. Consensus-based prediction combination algorithms are\neffective for such situations. However, current research on prediction\ncombination focuses on the single label setting, where an instance can have one\nand only one label. Nonetheless, data nowadays are usually multilabeled, such\nthat more than one label have to be predicted at the same time. Direct\napplications of existing prediction combination methods to multilabel settings\ncan lead to degenerated performance. In this paper, we address the challenges\nof combining predictions from multiple multilabel classifiers and propose two\nnovel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and\nMLCM-a (MLCM for microAUC). These algorithms can capture label correlations\nthat are common in multilabel classifications, and optimize corresponding\nperformance metrics. Experimental results on popular multilabel classification\ntasks verify the theoretical analysis and effectiveness of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 03:04:47 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Xie", "Sihong", ""], ["Kong", "Xiangnan", ""], ["Gao", "Jing", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1310.4362", "submitter": "Jussi Gillberg Mr.", "authors": "Jussi Gillberg, Pekka Marttinen, Matti Pirinen, Antti J Kangas, Pasi\n  Soininen, Marjo-Riitta J\\\"arvelin, Mika Ala-Korpela, Samuel Kaski", "title": "Bayesian Information Sharing Between Noise And Regression Models\n  Improves Prediction of Weak Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the prediction of weak effects in a multiple-output regression\nsetup, when covariates are expected to explain a small amount, less than\n$\\approx 1%$, of the variance of the target variables. To facilitate the\nprediction of the weak effects, we constrain our model structure by introducing\na novel Bayesian approach of sharing information between the regression model\nand the noise model. Further reduction of the effective number of parameters is\nachieved by introducing an infinite shrinkage prior and group sparsity in the\ncontext of the Bayesian reduced rank regression, and using the Bayesian\ninfinite factor model as a flexible low-rank noise model. In our experiments\nthe model incorporating the novelties outperformed alternatives in genomic\nprediction of rich phenotype data. In particular, the information sharing\nbetween the noise and regression models led to significant improvement in\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:13:45 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Gillberg", "Jussi", ""], ["Marttinen", "Pekka", ""], ["Pirinen", "Matti", ""], ["Kangas", "Antti J", ""], ["Soininen", "Pasi", ""], ["J\u00e4rvelin", "Marjo-Riitta", ""], ["Ala-Korpela", "Mika", ""], ["Kaski", "Samuel", ""]]}, {"id": "1310.4456", "submitter": "Stefan Webb", "authors": "Stefan Douglas Webb", "title": "Inference, Sampling, and Learning in Copula Cumulative Distribution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative distribution network (CDN) is a recently developed class of\nprobabilistic graphical models (PGMs) permitting a copula factorization, in\nwhich the CDF, rather than the density, is factored. Despite there being much\nrecent interest within the machine learning community about copula\nrepresentations, there has been scarce research into the CDN, its amalgamation\nwith copula theory, and no evaluation of its performance. Algorithms for\ninference, sampling, and learning in these models are underdeveloped compared\nthose of other PGMs, hindering widerspread use.\n  One advantage of the CDN is that it allows the factors to be parameterized as\ncopulae, combining the benefits of graphical models with those of copula\ntheory. In brief, the use of a copula parameterization enables greater\nmodelling flexibility by separating representation of the marginals from the\ndependence structure, permitting more efficient and robust learning. Another\nadvantage is that the CDN permits the representation of implicit latent\nvariables, whose parameterization and connectivity are not required to be\nspecified. Unfortunately, that the model can encode only latent relationships\nbetween variables severely limits its utility.\n  In this thesis, we present inference, learning, and sampling for CDNs, and\nfurther the state-of-the-art. First, we explain the basics of copula theory and\nthe representation of copula CDNs. Then, we discuss inference in the models,\nand develop the first sampling algorithm. We explain standard learning methods,\npropose an algorithm for learning from data missing completely at random\n(MCAR), and develop a novel algorithm for learning models of arbitrary\ntreewidth and size. Properties of the models and algorithms are investigated\nthrough Monte Carlo simulations. We conclude with further discussion of the\nadvantages and limitations of CDNs, and suggest future work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 17:33:34 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Webb", "Stefan Douglas", ""]]}, {"id": "1310.4485", "submitter": "Baochang Zhang", "authors": "Juan Liu, Baochang Zhang, Linlin Shen, Jianzhuang Liu, Jason Zhao", "title": "The BeiHang Keystroke Dynamics Authentication System", "comments": "25 pages,17 figures,5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Keystroke Dynamics is an important biometric solution for person\nauthentication. Based upon keystroke dynamics, this paper designs an embedded\npassword protection device, develops an online system, collects two public\ndatabases for promoting the research on keystroke authentication, exploits the\nGabor filter bank to characterize keystroke dynamics, and provides benchmark\nresults of three popular classification algorithms, one-class support vector\nmachine, Gaussian classifier, and nearest neighbour classifier.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 12:12:44 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Liu", "Juan", ""], ["Zhang", "Baochang", ""], ["Shen", "Linlin", ""], ["Liu", "Jianzhuang", ""], ["Zhao", "Jason", ""]]}, {"id": "1310.4495", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu and SSSN Usha Devi Nedunuri", "title": "Multiple Attractor Cellular Automata (MACA) for Addressing Major\n  Problems in Bioinformatics", "comments": "arXiv admin note: text overlap with arXiv:1310.4342", "journal-ref": "Review of Bioinformatics and Biometrics (RBB) Volume 2 Issue 3,\n  September 2013", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CA has grown as potential classifier for addressing major problems in\nbioinformatics. Lot of bioinformatics problems like predicting the protein\ncoding region, finding the promoter region, predicting the structure of protein\nand many other problems in bioinformatics can be addressed through Cellular\nAutomata. Even though there are some prediction techniques addressing these\nproblems, the approximate accuracy level is very less. An automated procedure\nwas proposed with MACA (Multiple Attractor Cellular Automata) which can address\nall these problems. The genetic algorithm is also used to find rules with good\nfitness values. Extensive experiments are conducted for reporting the accuracy\nof the proposed tool. The average accuracy of MACA when tested with ENCODE,\nBG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 15:01:19 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["Nedunuri", "SSSN Usha Devi", ""]]}, {"id": "1310.4546", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean", "title": "Distributed Representations of Words and Phrases and their\n  Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 23:28:53 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Mikolov", "Tomas", ""], ["Sutskever", "Ilya", ""], ["Chen", "Kai", ""], ["Corrado", "Greg", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1310.4579", "submitter": "Abir De", "authors": "Abir De, Niloy Ganguly, Soumen Chakrabarti", "title": "Discriminative Link Prediction using Local Links, Node Features and\n  Community Structure", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A link prediction (LP) algorithm is given a graph, and has to rank, for each\nnode, other nodes that are candidates for new linkage. LP is strongly motivated\nby social search and recommendation applications. LP techniques often focus on\nglobal properties (graph conductance, hitting or commute times, Katz score) or\nlocal properties (Adamic-Adar and many variations, or node feature vectors),\nbut rarely combine these signals. Furthermore, neither of these extremes\nexploit link densities at the intermediate level of communities. In this paper\nwe describe a discriminative LP algorithm that exploits two new signals. First,\na co-clustering algorithm provides community level link density estimates,\nwhich are used to qualify observed links with a surprise value. Second, links\nin the immediate neighborhood of the link to be predicted are not interpreted\nat face value, but through a local model of node feature similarities. These\nsignals are combined into a discriminative link predictor. We evaluate the new\npredictor using five diverse data sets that are standard in the literature. We\nreport on significant accuracy boosts compared to standard LP methods\n(including Adamic-Adar and random walk). Apart from the new predictor, another\ncontribution is a rigorous protocol for benchmarking and reporting LP\nalgorithms, which reveals the regions of strengths and weaknesses of all the\npredictors studied here, and establishes the new proposal as the most robust.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 04:21:37 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["De", "Abir", ""], ["Ganguly", "Niloy", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1310.4661", "submitter": "Arnak Dalalyan S.", "authors": "Olivier Collier and Arnak S. Dalalyan", "title": "Minimax rates in permutation estimation for feature matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of matching two sets of features appears in various tasks of\ncomputer vision and can be often formalized as a problem of permutation\nestimation. We address this problem from a statistical point of view and\nprovide a theoretical analysis of the accuracy of several natural estimators.\nTo this end, the minimax rate of separation is investigated and its expression\nis obtained as a function of the sample size, noise level and dimension. We\nconsider the cases of homoscedastic and heteroscedastic noise and establish, in\neach case, tight upper bounds on the separation distance of several estimators.\nThese upper bounds are shown to be unimprovable both in the homoscedastic and\nheteroscedastic settings. Interestingly, these bounds demonstrate that a phase\ntransition occurs when the dimension $d$ of the features is of the order of the\nlogarithm of the number of features $n$. For $d=O(\\log n)$, the rate is\ndimension free and equals $\\sigma (\\log n)^{1/2}$, where $\\sigma$ is the noise\nlevel. In contrast, when $d$ is larger than $c\\log n$ for some constant $c>0$,\nthe minimax rate increases with $d$ and is of the order $\\sigma(d\\log\nn)^{1/4}$. We also discuss the computational aspects of the estimators and\nprovide empirical evidence of their consistency on synthetic data. Finally, we\nshow that our results extend to more general matching criteria.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 11:42:07 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 20:11:21 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Collier", "Olivier", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1310.4849", "submitter": "Willem Waegeman", "authors": "Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei\n  Cheng, Eyke Hullermeier", "title": "On the Bayes-optimality of F-measure maximizers", "comments": null, "journal-ref": "JMLR 15 (2014) 3333-3388", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure, which has originally been introduced in information retrieval,\nis nowadays routinely used as a performance metric for problems such as binary\nclassification, multi-label classification, and structured output prediction.\nOptimizing this measure is a statistically and computationally challenging\nproblem, since no closed-form solution exists. Adopting a decision-theoretic\nperspective, this article provides a formal and experimental analysis of\ndifferent approaches for maximizing the F-measure. We start with a Bayes-risk\nanalysis of related loss functions, such as Hamming loss and subset zero-one\nloss, showing that optimizing such losses as a surrogate of the F-measure leads\nto a high worst-case regret. Subsequently, we perform a similar type of\nanalysis for F-measure maximizing algorithms, showing that such algorithms are\napproximate, while relying on additional assumptions regarding the statistical\ndistribution of the binary response variables. Furthermore, we present a new\nalgorithm which is not only computationally efficient but also Bayes-optimal,\nregardless of the underlying distribution. To this end, the algorithm requires\nonly a quadratic (with respect to the number of binary responses) number of\nparameters of the joint distribution. We illustrate the practical performance\nof all analyzed methods by means of experiments with multi-label classification\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 20:34:04 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 15:58:38 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 15:58:09 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Waegeman", "Willem", ""], ["Dembczynski", "Krzysztof", ""], ["Jachnik", "Arkadiusz", ""], ["Cheng", "Weiwei", ""], ["Hullermeier", "Eyke", ""]]}, {"id": "1310.4909", "submitter": "Chinchu Jose", "authors": "M. Sudheep Elayidom, Chinchu Jose, Anitta Puthussery, Neenu K Sasi", "title": "Text Classification For Authorship Attribution Analysis", "comments": "10 pages", "journal-ref": "Advanced Computing: An International Journal (ACIJ), Vol.4, No.5,\n  September 2013", "doi": "10.5121/acij.2013.4501", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution mainly deals with undecided authorship of literary\ntexts. Authorship attribution is useful in resolving issues like uncertain\nauthorship, recognize authorship of unknown texts, spot plagiarism so on.\nStatistical methods can be used to set apart the approach of an author\nnumerically. The basic methodologies that are made use in computational\nstylometry are word length, sentence length, vocabulary affluence, frequencies\netc. Each author has an inborn style of writing, which is particular to\nhimself. Statistical quantitative techniques can be used to differentiate the\napproach of an author in a numerical way. The problem can be broken down into\nthree sub problems as author identification, author characterization and\nsimilarity detection. The steps involved are pre-processing, extracting\nfeatures, classification and author identification. For this different\nclassifiers can be used. Here fuzzy learning classifier and SVM are used. After\nauthor identification the SVM was found to have more accuracy than Fuzzy\nclassifier. Later combined the classifiers to obtain a better accuracy when\ncompared to individual SVM and fuzzy classifier.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 04:18:09 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Elayidom", "M. Sudheep", ""], ["Jose", "Chinchu", ""], ["Puthussery", "Anitta", ""], ["Sasi", "Neenu K", ""]]}, {"id": "1310.4945", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "A novel sparsity and clustering regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel SPARsity and Clustering (SPARC) regularizer, which is a\nmodified version of the previous octagonal shrinkage and clustering algorithm\nfor regression (OSCAR), where, the proposed regularizer consists of a\n$K$-sparse constraint and a pair-wise $\\ell_{\\infty}$ norm restricted on the\n$K$ largest components in magnitude. The proposed regularizer is able to\nseparably enforce $K$-sparsity and encourage the non-zeros to be equal in\nmagnitude. Moreover, it can accurately group the features without shrinking\ntheir magnitude. In fact, SPARC is closely related to OSCAR, so that the\nproximity operator of the former can be efficiently computed based on that of\nthe latter, allowing using proximal splitting algorithms to solve problems with\nSPARC regularization. Experiments on synthetic data and with benchmark breast\ncancer data show that SPARC is a competitive group-sparsity inducing\nregularizer for regression and classification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:31:54 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 16:41:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1310.4977", "submitter": "Marco Signoretto", "authors": "Marco Signoretto and Lieven De Lathauwer and Johan A.K. Suykens", "title": "Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear\n  Spectral Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework to learn functions in tensor product\nreproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a\nnovel representer theorem suitable for existing as well as new spectral\npenalties for tensors. When the functions in the TP-RKHS are defined on the\nCartesian product of finite discrete sets, in particular, our main problem\nformulation admits as a special case existing tensor completion problems. Other\nspecial cases include transfer learning with multimodal side information and\nmultilinear multitask learning. For the latter case, our kernel-based view is\ninstrumental to derive nonlinear extensions of existing model classes. We give\na novel algorithm and show in experiments the usefulness of the proposed\nextensions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 11:37:33 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Signoretto", "Marco", ""], ["De Lathauwer", "Lieven", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1310.5007", "submitter": "Tianbing Xu", "authors": "Tianbing Xu, Jianfeng Gao, Lin Xiao, Amelia Regan", "title": "Online Classification Using a Voted RDA Method", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a voted dual averaging method for online classification problems\nwith explicit regularization. This method employs the update rule of the\nregularized dual averaging (RDA) method, but only on the subsequence of\ntraining examples where a classification error is made. We derive a bound on\nthe number of mistakes made by this method on the training set, as well as its\ngeneralization error rate. We also introduce the concept of relative strength\nof regularization, and show how it affects the mistake bound and generalization\nperformance. We experimented with the method using $\\ell_1$ regularization on a\nlarge-scale natural language processing task, and obtained state-of-the-art\nclassification performance with fairly sparse models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 04:01:25 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Xu", "Tianbing", ""], ["Gao", "Jianfeng", ""], ["Xiao", "Lin", ""], ["Regan", "Amelia", ""]]}, {"id": "1310.5008", "submitter": "Tianbing Xu", "authors": "Tianbing Xu, Yaming Yu, John Turner, Amelia Regan", "title": "Thompson Sampling in Dynamic Systems for Contextual Bandit Problems", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multiarm bandit problems in the timevarying dynamic system\nfor rich structural features. For the nonlinear dynamic model, we propose the\napproximate inference for the posterior distributions based on Laplace\nApproximation. For the context bandit problems, Thompson Sampling is adopted\nbased on the underlying posterior distributions of the parameters. More\nspecifically, we introduce the discount decays on the previous samples impact\nand analyze the different decay rates with the underlying sample dynamics.\nConsequently, the exploration and exploitation is adaptively tradeoff according\nto the dynamics in the system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 04:17:20 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Xu", "Tianbing", ""], ["Yu", "Yaming", ""], ["Turner", "John", ""], ["Regan", "Amelia", ""]]}, {"id": "1310.5034", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Kathrin Bujna, and Daniel Kuntze", "title": "A Theoretical and Experimental Comparison of the EM and SEM Algorithm", "comments": "This paper is a preprint of a paper submitted to and accepted for\n  publication in ICPR 2014 and is subject to IEEE copyright", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a new analysis of the SEM algorithm. Unlike previous\nwork, we focus on the analysis of a single run of the algorithm. First, we\ndiscuss the algorithm for general mixture distributions. Second, we consider\nGaussian mixture models and show that with high probability the update\nequations of the EM algorithm and its stochastic variant are almost the same,\ngiven that the input set is sufficiently large. Our experiments confirm that\nthis still holds for a large number of successive update steps. In particular,\nfor Gaussian mixture models, we show that the stochastic variant runs nearly\ntwice as fast.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:31:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 14:05:49 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Bujna", "Kathrin", ""], ["Kuntze", "Daniel", ""]]}, {"id": "1310.5035", "submitter": "Zhouchen Lin", "authors": "Zhouchen Lin, Risheng Liu, Huan Li", "title": "Linearized Alternating Direction Method with Parallel Splitting and\n  Adaptive Penalty for Separable Convex Programs in Machine Learning", "comments": "Preliminary version published on Asian Conference on Machine Learning\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and other fields can be (re)for-mulated as\nlinearly constrained separable convex programs. In most of the cases, there are\nmultiple blocks of variables. However, the traditional alternating direction\nmethod (ADM) and its linearized version (LADM, obtained by linearizing the\nquadratic penalty term) are for the two-block case and cannot be naively\ngeneralized to solve the multi-block case. So there is great demand on\nextending the ADM based methods for the multi-block case. In this paper, we\npropose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve\nmulti-block separable convex programs efficiently. When all the component\nobjective functions have bounded subgradients, we obtain convergence results\nthat are stronger than those of ADM and LADM, e.g., allowing the penalty\nparameter to be unbounded and proving the sufficient and necessary conditions}\nfor global convergence. We further propose a simple optimality measure and\nreveal the convergence rate of LADMPSAP in an ergodic sense. For programs with\nextra convex set constraints, with refined parameter estimation we devise a\npractical version of LADMPSAP for faster convergence. Finally, we generalize\nLADMPSAP to handle programs with more difficult objective functions by\nlinearizing part of the objective function as well. LADMPSAP is particularly\nsuitable for sparse representation and low-rank recovery problems because its\nsubproblems have closed form solutions and the sparsity and low-rankness of the\niterates can be preserved during the iteration. It is also highly\nparallelizable and hence fits for parallel or distributed computing. Numerical\nexperiments testify to the advantages of LADMPSAP in speed and numerical\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:31:08 GMT"}, {"version": "v2", "created": "Thu, 29 May 2014 02:14:13 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Lin", "Zhouchen", ""], ["Liu", "Risheng", ""], ["Li", "Huan", ""]]}, {"id": "1310.5042", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Distributional semantics beyond words: Supervised learning of analogy\n  and paraphrase", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), (2013), 1, 353-366", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several efforts to extend distributional semantics beyond\nindividual words, to measure the similarity of word pairs, phrases, and\nsentences (briefly, tuples; ordered sets of words, contiguous or\nnoncontiguous). One way to extend beyond words is to compare two tuples using a\nfunction that combines pairwise similarities between the component words in the\ntuples. A strength of this approach is that it works with both relational\nsimilarity (analogy) and compositional similarity (paraphrase). However, past\nwork required hand-coding the combination function for different tasks. The\nmain contribution of this paper is that combination functions are generated by\nsupervised learning. We achieve state-of-the-art results in measuring\nrelational similarity between word pairs (SAT analogies and SemEval~2012 Task\n2) and measuring compositional similarity between noun-modifier phrases and\nunigrams (multiple-choice paraphrase questions).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:50:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1310.5082", "submitter": "Gustavo Camps-Valls", "authors": "Gustavo Camps-Valls, Juan Guti\\'errez, Gabriel G\\'omez-P\\'erez,\n  Jes\\'us Malo", "title": "On the Suitable Domain for SVM Training in Image Coding", "comments": null, "journal-ref": "Journal of Machine Learning Research, JMLR, 9(1), 49-66, 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional SVM-based image coding methods are founded on independently\nrestricting the distortion in every image coefficient at some particular image\nrepresentation. Geometrically, this implies allowing arbitrary signal\ndistortions in an $n$-dimensional rectangle defined by the\n$\\varepsilon$-insensitivity zone in each dimension of the selected image\nrepresentation domain. Unfortunately, not every image representation domain is\nwell-suited for such a simple, scalar-wise, approach because statistical and/or\nperceptual interactions between the coefficients may exist. These interactions\nimply that scalar approaches may induce distortions that do not follow the\nimage statistics and/or are perceptually annoying. Taking into account these\nrelations would imply using non-rectangular $\\varepsilon$-insensitivity regions\n(allowing coupled distortions in different coefficients), which is beyond the\nconventional SVM formulation.\n  In this paper, we report a condition on the suitable domain for developing\nefficient SVM image coding schemes. We analytically demonstrate that no linear\ndomain fulfills this condition because of the statistical and perceptual\ninter-coefficient relations that exist in these domains. This theoretical\nresult is experimentally confirmed by comparing SVM learning in previously\nreported linear domains and in a recently proposed non-linear perceptual domain\nthat simultaneously reduces the statistical and perceptual relations (so it is\ncloser to fulfilling the proposed condition). These results highlight the\nrelevance of an appropriate choice of the image representation before SVM\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 16:34:04 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Camps-Valls", "Gustavo", ""], ["Guti\u00e9rrez", "Juan", ""], ["G\u00f3mez-P\u00e9rez", "Gabriel", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1310.5089", "submitter": "Gustavo Camps-Valls", "authors": "Jer\\'onimo Arenas-Garc\\'ia, Kaare Brandt Petersen, Gustavo\n  Camps-Valls, Lars Kai Hansen", "title": "Kernel Multivariate Analysis Framework for Supervised Subspace Learning:\n  A Tutorial on Linear and Kernel Multivariate Methods", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, 30(4), 16-29, 2013", "doi": "10.1109/MSP.2013.2250591", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and dimensionality reduction are important tasks in many\nfields of science dealing with signal processing and analysis. The relevance of\nthese techniques is increasing as current sensory devices are developed with\never higher resolution, and problems involving multimodal data sources become\nmore common. A plethora of feature extraction methods are available in the\nliterature collectively grouped under the field of Multivariate Analysis (MVA).\nThis paper provides a uniform treatment of several methods: Principal Component\nAnalysis (PCA), Partial Least Squares (PLS), Canonical Correlation Analysis\n(CCA) and Orthonormalized PLS (OPLS), as well as their non-linear extensions\nderived by means of the theory of reproducing kernel Hilbert spaces. We also\nreview their connections to other methods for classification and statistical\ndependence estimation, and introduce some recent developments to deal with the\nextreme cases of large-scale and low-sized problems. To illustrate the wide\napplicability of these methods in both classification and regression problems,\nwe analyze their performance in a benchmark of publicly available data sets,\nand pay special attention to specific real applications involving audio\nprocessing for music genre prediction and hyperspectral satellite images for\nEarth and climate monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 16:44:05 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""], ["Petersen", "Kaare Brandt", ""], ["Camps-Valls", "Gustavo", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1310.5095", "submitter": "Fabrice Rossi", "authors": "Martin Riedel, Marika K\\\"astner, Fabrice Rossi (SAMM), Thomas Villmann", "title": "Regularization in Relevance Learning Vector Quantization Using l one\n  Norms", "comments": null, "journal-ref": "21-th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN 2013), Bruges :\n  Belgium (2013)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this contribution a method for l one regularization in\nprototype based relevance learning vector quantization (LVQ) for sparse\nrelevance profiles. Sparse relevance profiles in hyperspectral data analysis\nfade down those spectral bands which are not necessary for classification. In\nparticular, we consider the sparsity in the relevance profile enforced by LASSO\noptimization. The latter one is obtained by a gradient learning scheme using a\ndifferentiable parametrized approximation of the $l_{1}$-norm, which has an\nupper error bound. We extend this regularization idea also to the matrix\nlearning variant of LVQ as the natural generalization of relevance learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 17:00:34 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Riedel", "Martin", "", "SAMM"], ["K\u00e4stner", "Marika", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Villmann", "Thomas", ""]]}, {"id": "1310.5114", "submitter": "Thomas Gueudr\\'e", "authors": "Thomas Gueudr\\'e and Alexander Dobrinevski and Jean-Philippe Bouchaud", "title": "Explore or exploit? A generic model and an exactly solvable case", "comments": "5 pages 2 figures", "journal-ref": "Phys. Rev. Lett. 112, 050602 (2014)", "doi": "10.1103/PhysRevLett.112.050602", "report-no": null, "categories": "cond-mat.dis-nn cs.LG physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a good compromise between the exploitation of known resources and the\nexploration of unknown, but potentially more profitable choices, is a general\nproblem, which arises in many different scientific disciplines. We propose a\nstylized model for these exploration-exploitation situations, including\npopulation or economic growth, portfolio optimisation, evolutionary dynamics,\nor the problem of optimal pinning of vortices or dislocations in disordered\nmaterials. We find the exact growth rate of this model for tree-like geometries\nand prove the existence of an optimal migration rate in this case. Numerical\nsimulations in the one-dimensional case confirm the generic existence of an\noptimum.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 18:10:01 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 11:42:50 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 15:07:34 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Gueudr\u00e9", "Thomas", ""], ["Dobrinevski", "Alexander", ""], ["Bouchaud", "Jean-Philippe", ""]]}, {"id": "1310.5249", "submitter": "Fabrice Rossi", "authors": "Mohamed Khalil El Mahrsi (LTCI, SAMM), Fabrice Rossi (SAMM)", "title": "Graph-Based Approaches to Clustering Network-Constrained Trajectory Data", "comments": null, "journal-ref": "New Frontiers in Mining Complex Patterns, Appice, Annalisa and\n  Ceci, Michelangelo and Loglisci, Corrado and Manco, Giuseppe and Masciari,\n  Elio and Ras, Zbigniew (Ed.) (2013) 124-137", "doi": "10.1007/978-3-642-37382-4_9", "report-no": "NFMCP2013", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering trajectory data attracted considerable attention in the last few\nyears. Most of prior work assumed that moving objects can move freely in an\neuclidean space and did not consider the eventual presence of an underlying\nroad network and its influence on evaluating the similarity between\ntrajectories. In this paper, we present an approach to clustering such\nnetwork-constrained trajectory data. More precisely we aim at discovering\ngroups of road segments that are often travelled by the same trajectories. To\nachieve this end, we model the interactions between segments w.r.t. their\nsimilarity as a weighted graph to which we apply a community detection\nalgorithm to discover meaningful clusters. We showcase our proposition through\nexperimental results obtained on synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2013 17:24:39 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI, SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1310.5288", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham", "title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian\n  Processes", "comments": "13 Pages, 9 Figures, 1 Table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are typically used for smoothing and interpolation on\nsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --\nenabling automatic pattern extrapolation with Gaussian processes on large\nmultidimensional datasets. GPatt unifies and extends highly expressive kernels\nand fast exact inference techniques. Without human intervention -- no hand\ncrafting of kernel features, and no sophisticated initialisation procedures --\nwe show that GPatt can solve large scale pattern extrapolation, inpainting, and\nkernel discovery problems, including a problem with 383400 training points. We\nfind that GPatt significantly outperforms popular alternative scalable Gaussian\nprocess methods in speed and accuracy. Moreover, we discover profound\ndifferences between each of these methods, suggesting expressive kernels,\nnonparametric representations, and exact inference are useful for modelling\nlarge scale multidimensional patterns.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 01:26:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 16:58:35 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 14:10:34 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Gilboa", "Elad", ""], ["Nehorai", "Arye", ""], ["Cunningham", "John P.", ""]]}, {"id": "1310.5347", "submitter": "Il Memming Park", "authors": "Il Memming Park, Sohan Seth, Steven Van Vaerenbergh", "title": "Bayesian Extensions of Kernel Least Mean Squares", "comments": "7 pages, 4 fiures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel least mean squares (KLMS) algorithm is a computationally efficient\nnonlinear adaptive filtering method that \"kernelizes\" the celebrated (linear)\nleast mean squares algorithm. We demonstrate that the least mean squares\nalgorithm is closely related to the Kalman filtering, and thus, the KLMS can be\ninterpreted as an approximate Bayesian filtering method. This allows us to\nsystematically develop extensions of the KLMS by modifying the underlying\nstate-space and observation models. The resulting extensions introduce many\ndesirable properties such as \"forgetting\", and the ability to learn from\ndiscrete data, while retaining the computational simplicity and time complexity\nof the original algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2013 16:58:57 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Park", "Il Memming", ""], ["Seth", "Sohan", ""], ["Van Vaerenbergh", "Steven", ""]]}, {"id": "1310.5393", "submitter": "Fanyi Xiao", "authors": "Fanyi Xiao, Ruikun Luo, Zhiding Yu", "title": "Multi-Task Regularization with Covariance Dictionary for Linear\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multi-task linear classifier learning problem\ncalled D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance\nshared by all tasks to do multi-task knowledge transfer among different tasks.\nWe formally define the learning problem of D-SVM and show two interpretations\nof this problem, from both the probabilistic and kernel perspectives. From the\nprobabilistic perspective, we show that our learning formulation is actually a\nMAP estimation on all optimization variables. We also show its equivalence to a\nmultiple kernel learning problem in which one is trying to find a re-weighting\nkernel for features from a dictionary of basis (despite the fact that only\nlinear classifiers are learned). Finally, we describe an alternative\noptimization scheme to minimize the objective function and present empirical\nstudies to valid our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 01:06:56 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Xiao", "Fanyi", ""], ["Luo", "Ruikun", ""], ["Yu", "Zhiding", ""]]}, {"id": "1310.5426", "submitter": "Evan Sparks", "authors": "Evan R. Sparks, Ameet Talwalkar, Virginia Smith, Jey Kottalam, Xinghao\n  Pan, Joseph Gonzalez, Michael J. Franklin, Michael I. Jordan, Tim Kraska", "title": "MLI: An API for Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLI is an Application Programming Interface designed to address the\nchallenges of building Machine Learn- ing algorithms in a distributed setting\nbased on data-centric computing. Its primary goal is to simplify the\ndevelopment of high-performance, scalable, distributed algorithms. Our initial\nresults show that, relative to existing systems, this interface can be used to\nbuild distributed implementations of a wide variety of common Machine Learning\nalgorithms with minimal complexity and highly competitive performance and\nscalability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 04:58:11 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 22:08:12 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Sparks", "Evan R.", ""], ["Talwalkar", "Ameet", ""], ["Smith", "Virginia", ""], ["Kottalam", "Jey", ""], ["Pan", "Xinghao", ""], ["Gonzalez", "Joseph", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Kraska", "Tim", ""]]}, {"id": "1310.5665", "submitter": "Andres Munoz", "authors": "Mehryar Mohri and Andres Mu\\~noz Medina", "title": "Learning Theory and Algorithms for Revenue Optimization in Second-Price\n  Auctions with Reserve", "comments": "Accepted at ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-price auctions with reserve play a critical role for modern search\nengine and popular online sites since the revenue of these companies often\ndirectly de- pends on the outcome of such auctions. The choice of the reserve\nprice is the main mechanism through which the auction revenue can be influenced\nin these electronic markets. We cast the problem of selecting the reserve price\nto optimize revenue as a learning problem and present a full theoretical\nanalysis dealing with the complex properties of the corresponding loss\nfunction. We further give novel algorithms for solving this problem and report\nthe results of several experiments in both synthetic and real data\ndemonstrating their effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 18:27:25 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 18:31:04 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 20:42:17 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Mohri", "Mehryar", ""], ["Medina", "Andres Mu\u00f1oz", ""]]}, {"id": "1310.5715", "submitter": "Deanna Needell", "authors": "Deanna Needell, Nathan Srebro, Rachel Ward", "title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized\n  Kaczmarz algorithm", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain an improved finite-sample guarantee on the linear convergence of\nstochastic gradient descent for smooth and strongly convex objectives,\nimproving from a quadratic dependence on the conditioning $(L/\\mu)^2$ (where\n$L$ is a bound on the smoothness and $\\mu$ on the strong convexity) to a linear\ndependence on $L/\\mu$. Furthermore, we show how reweighting the sampling\ndistribution (i.e. importance sampling) is necessary in order to further\nimprove convergence, and obtain a linear dependence in the average smoothness,\ndominating previous results. We also discuss importance sampling for SGD more\nbroadly and show how it can improve convergence also in other scenarios. Our\nresults are based on a connection we make between SGD and the randomized\nKaczmarz algorithm, which allows us to transfer ideas between the separate\nbodies of literature studying each of the two methods. In particular, we recast\nthe randomized Kaczmarz algorithm as an instance of SGD, and apply our results\nto prove its exponential convergence, but to the solution of a weighted least\nsquares problem rather than the original least squares problem. We then present\na modified Kaczmarz algorithm with partially biased sampling which does\nconverge to the original least squares solution with the same exponential\nconvergence rate.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 20:15:44 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2014 01:43:32 GMT"}, {"version": "v3", "created": "Thu, 20 Mar 2014 16:51:23 GMT"}, {"version": "v4", "created": "Thu, 27 Nov 2014 05:10:09 GMT"}, {"version": "v5", "created": "Fri, 16 Jan 2015 17:11:24 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Needell", "Deanna", ""], ["Srebro", "Nathan", ""], ["Ward", "Rachel", ""]]}, {"id": "1310.5738", "submitter": "Frank Hutter", "authors": "Frank Hutter and Michael A. Osborne", "title": "A Kernel for Hierarchical Parameter Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a family of kernels for mixed continuous/discrete hierarchical\nparameter spaces and show that they are positive definite.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 22:02:17 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Hutter", "Frank", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1310.5796", "submitter": "Spencer Greenberg", "authors": "Corinna Cortes, Spencer Greenberg, Mehryar Mohri", "title": "Relative Deviation Learning Bounds and Generalization with Unbounded\n  Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extensive analysis of relative deviation bounds, including\ndetailed proofs of two-sided inequalities and their implications. We also give\ndetailed proofs of two-sided generalization bounds that hold in the general\ncase of unbounded loss functions, under the assumption that a moment of the\nloss is bounded. These bounds are useful in the analysis of importance\nweighting and other learning tasks such as unbounded regression.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 04:28:12 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 02:58:33 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 22:53:30 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 23:35:45 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Cortes", "Corinna", ""], ["Greenberg", "Spencer", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1310.6007", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, Marcus A. Brubaker, David J. Fleet, Aaron Hertzmann", "title": "Efficient Optimization for Sparse Gaussian Process Regression", "comments": "To appear in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient optimization algorithm for selecting a subset of\ntraining data to induce sparsity for Gaussian process regression. The algorithm\nestimates an inducing set and the hyperparameters using a single objective,\neither the marginal likelihood or a variational free energy. The space and time\ncomplexity are linear in training set size, and the algorithm can be applied to\nlarge regression problems on discrete or continuous domains. Empirical\nevaluation shows state-of-art performance in discrete cases and competitive\nresults in the continuous case.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 18:44:29 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 05:13:30 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 08:21:58 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Cao", "Yanshuai", ""], ["Brubaker", "Marcus A.", ""], ["Fleet", "David J.", ""], ["Hertzmann", "Aaron", ""]]}, {"id": "1310.6288", "submitter": "Hao Zhang", "authors": "Hao Zhang and Liqing Zhang", "title": "Spatial-Spectral Boosting Analysis for Stroke Patients' Motor Imagery\n  EEG in Rehabilitation Training", "comments": "10 pages,3 figures", "journal-ref": null, "doi": "10.3233/978-1-61499-419-0-537", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current studies about motor imagery based rehabilitation training systems for\nstroke subjects lack an appropriate analytic method, which can achieve a\nconsiderable classification accuracy, at the same time detects gradual changes\nof imagery patterns during rehabilitation process and disinters potential\nmechanisms about motor function recovery. In this study, we propose an adaptive\nboosting algorithm based on the cortex plasticity and spectral band shifts.\nThis approach models the usually predetermined spatial-spectral configurations\nin EEG study into variable preconditions, and introduces a new heuristic of\nstochastic gradient boost for training base learners under these preconditions.\nWe compare our proposed algorithm with commonly used methods on datasets\ncollected from 2 months' clinical experiments. The simulation results\ndemonstrate the effectiveness of the method in detecting the variations of\nstroke patients' EEG patterns. By chronologically reorganizing the weight\nparameters of the learned additive model, we verify the spatial compensatory\nmechanism on impaired cortex and detect the changes of accentuation bands in\nspectral domain, which may contribute important prior knowledge for\nrehabilitation practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 16:43:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Zhang", "Hao", ""], ["Zhang", "Liqing", ""]]}, {"id": "1310.6304", "submitter": "Nikos Karampatziakis", "authors": "Nikos Karampatziakis, Paul Mineiro", "title": "Combining Structured and Unstructured Randomness in Large Scale PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a ubiquitous tool with many\napplications in machine learning including feature construction, subspace\nembedding, and outlier detection. In this paper, we present an algorithm for\ncomputing the top principal components of a dataset with a large number of rows\n(examples) and columns (features). Our algorithm leverages both structured and\nunstructured random projections to retain good accuracy while being\ncomputationally efficient. We demonstrate the technique on the winning\nsubmission the KDD 2010 Cup.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 17:33:26 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2013 17:36:27 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Karampatziakis", "Nikos", ""], ["Mineiro", "Paul", ""]]}, {"id": "1310.6343", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora and Aditya Bhaskara and Rong Ge and Tengyu Ma", "title": "Provable Bounds for Learning Some Deep Representations", "comments": "The first 18 pages serve as an extended abstract and a 36 pages long\n  technical appendix follows", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms with provable guarantees that learn a class of deep nets\nin the generative model view popularized by Hinton and others. Our generative\nmodel is an $n$ node multilayer neural net that has degree at most $n^{\\gamma}$\nfor some $\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our\nalgorithm learns {\\em almost all} networks in this class with polynomial\nrunning time. The sample complexity is quadratic or cubic depending upon the\ndetails of the model.\n  The algorithm uses layerwise learning. It is based upon a novel idea of\nobserving correlations among features and using these to infer the underlying\nedge structure via a global graph recovery procedure. The analysis of the\nalgorithm reveals interesting structure of neural networks with random edge\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 19:49:32 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Arora", "Sanjeev", ""], ["Bhaskara", "Aditya", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1310.6536", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Randomized co-training: from cortical neurons to machine learning and\n  back again", "comments": "NIPS workshop: Randomized methods for machine learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its size and complexity, the human cortex exhibits striking\nanatomical regularities, suggesting there may simple meta-algorithms underlying\ncortical learning and computation. We expect such meta-algorithms to be of\ninterest since they need to operate quickly, scalably and effectively with\nlittle-to-no specialized assumptions.\n  This note focuses on a specific question: How can neurons use vast quantities\nof unlabeled data to speed up learning from the comparatively rare labels\nprovided by reward systems? As a partial answer, we propose randomized\nco-training as a biologically plausible meta-algorithm satisfying the above\nrequirements. As evidence, we describe a biologically-inspired algorithm,\nCorrelated Nystrom Views (XNV) that achieves state-of-the-art performance in\nsemi-supervised learning, and sketch work in progress on a neuronal\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 09:33:17 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1310.6740", "submitter": "Roman Garnett", "authors": "Roman Garnett and Michael A. Osborne and Philipp Hennig", "title": "Active Learning of Linear Embeddings for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active learning method for discovering low-dimensional\nstructure in high-dimensional Gaussian process (GP) tasks. Such problems are\nincreasingly frequent and important, but have hitherto presented severe\npractical difficulties. We further introduce a novel technique for\napproximately marginalizing GP hyperparameters, yielding marginal predictions\nrobust to hyperparameter mis-specification. Our method offers an efficient\nmeans of performing GP regression, quadrature, or Bayesian optimization in\nhigh-dimensional spaces.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 14:15:39 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Garnett", "Roman", ""], ["Osborne", "Michael A.", ""], ["Hennig", "Philipp", ""]]}, {"id": "1310.6775", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas", "title": "Durkheim Project Data Analysis Report", "comments": "43 pages, to appear as appendix of primary science publication\n  Poulin, et al \"Predicting the risk of suicide by analyzing the text of\n  clinical notes\"", "journal-ref": null, "doi": "10.1371/journal.pone.0085733.s001", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the suicidality prediction models created under the\nDARPA DCAPS program in association with the Durkheim Project\n[http://durkheimproject.org/]. The models were built primarily from\nunstructured text (free-format clinician notes) for several hundred patient\nrecords obtained from the Veterans Health Administration (VHA). The models were\nconstructed using a genetic programming algorithm applied to bag-of-words and\nbag-of-phrases datasets. The influence of additional structured data was\nexplored but was found to be minor. Given the small dataset size,\nclassification between cohorts was high fidelity (98%). Cross-validation\nsuggests these models are reasonably predictive, with an accuracy of 50% to 69%\non five rotating folds, with ensemble averages of 58% to 67%. One particularly\nnoteworthy result is that word-pairs can dramatically improve classification\naccuracy; but this is the case only when one of the words in the pair is\nalready known to have a high predictive value. By contrast, the set of all\npossible word-pairs does not improve on a simple bag-of-words model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 21:10:53 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Vepstas", "Linas", ""]]}, {"id": "1310.6998", "submitter": "Shiladitya Sinha", "authors": "Shiladitya Sinha, Chris Dyer, Kevin Gimpel, and Noah A. Smith", "title": "Predicting the NFL using Twitter", "comments": "Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\n  Mining for Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between social media output and National Football\nLeague (NFL) games, using a dataset containing messages from Twitter and NFL\ngame statistics. Specifically, we consider tweets pertaining to specific teams\nand games in the NFL season and use them alongside statistical game data to\nbuild predictive models for future game outcomes (which team will win?) and\nsports betting outcomes (which team will win with the point spread? will the\ntotal points be over/under the line?). We experiment with several feature sets\nand find that simple features using large volumes of tweets can match or exceed\nthe performance of more traditional features that use game statistics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 18:35:22 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Sinha", "Shiladitya", ""], ["Dyer", "Chris", ""], ["Gimpel", "Kevin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1310.7048", "submitter": "Jie  Wang", "authors": "Jie Wang and Peter Wonka and Jieping Ye", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is a widely used method for classification.\nAlthough many efforts have been devoted to develop efficient solvers, it\nremains challenging to apply SVM to large-scale problems. A nice property of\nSVM is that the non-support vectors have no effect on the resulting classifier.\nMotivated by this observation, we present fast and efficient screening rules to\ndiscard non-support vectors by analyzing the dual problem of SVM via\nvariational inequalities (DVI). As a result, the number of data instances to be\nentered into the optimization can be substantially reduced. Some appealing\nfeatures of our screening method are: (1) DVI is safe in the sense that the\nvectors discarded by DVI are guaranteed to be non-support vectors; (2) the data\nset needs to be scanned only once to run the screening, whose computational\ncost is negligible compared to that of solving the SVM problem; (3) DVI is\nindependent of the solvers and can be integrated with any existing efficient\nsolvers. We also show that the DVI technique can be extended to detect\nnon-support vectors in the least absolute deviations regression (LAD). To the\nbest of our knowledge, there are currently no screening methods for LAD. We\nhave evaluated DVI on both synthetic and real data sets. Experiments indicate\nthat DVI significantly outperforms the existing state-of-the-art screening\nrules for SVM, and is very effective in discarding non-support vectors for LAD.\nThe speedup gained by DVI rules can be up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 23:01:52 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Wang", "Jie", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1310.7163", "submitter": "Lihong Li", "authors": "Lihong Li", "title": "Generalized Thompson Sampling for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling, one of the oldest heuristics for solving multi-armed\nbandits, has recently been shown to demonstrate state-of-the-art performance.\nThe empirical success has led to great interests in theoretical understanding\nof this heuristic. In this paper, we approach this problem in a way very\ndifferent from existing efforts. In particular, motivated by the connection\nbetween Thompson Sampling and exponentiated updates, we propose a new family of\nalgorithms called Generalized Thompson Sampling in the expert-learning\nframework, which includes Thompson Sampling as a special case. Similar to most\nexpert-learning algorithms, Generalized Thompson Sampling uses a loss function\nto adjust the experts' weights. General regret bounds are derived, which are\nalso instantiated to two important loss functions: square loss and logarithmic\nloss. In contrast to existing bounds, our results apply to quite general\ncontextual bandits. More importantly, they quantify the effect of the \"prior\"\ndistribution on the regret bounds.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 06:29:55 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Li", "Lihong", ""]]}, {"id": "1310.7300", "submitter": "Maxim Raginsky", "authors": "Peng Guan, Maxim Raginsky, Rebecca Willett", "title": "Relax but stay in control: from value to algorithms for online Markov\n  decision processes", "comments": "40 pages; additional results in the convex-analytic framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms are designed to perform in non-stationary\nenvironments, but generally there is no notion of a dynamic state to model\nconstraints on current and future actions as a function of past actions.\nState-based models are common in stochastic control settings, but commonly used\nframeworks such as Markov Decision Processes (MDPs) assume a known stationary\nenvironment. In recent years, there has been a growing interest in combining\nthe above two frameworks and considering an MDP setting in which the cost\nfunction is allowed to change arbitrarily after each time step. However, most\nof the work in this area has been algorithmic: given a problem, one would\ndevelop an algorithm almost from scratch. Moreover, the presence of the state\nand the assumption of an arbitrarily varying environment complicate both the\ntheoretical analysis and the development of computationally efficient methods.\nThis paper describes a broad extension of the ideas proposed by Rakhlin et al.\nto give a general framework for deriving algorithms in an MDP setting with\narbitrarily changing costs. This framework leads to a unifying view of existing\nmethods and provides a general procedure for constructing new ones. Several new\nmethods are presented, and one of them is shown to have important advantages\nover a similar method developed from scratch via an online version of\napproximate dynamic programming.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 03:08:48 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 18:14:36 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Guan", "Peng", ""], ["Raginsky", "Maxim", ""], ["Willett", "Rebecca", ""]]}, {"id": "1310.7529", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind\n  Source Separation", "comments": "31 pages, 7 figures, 4 tables. Main changes: new numerical\n  experiments on column-rank-deficient matrices, typos corrected, discussion on\n  the comparison with XRAY", "journal-ref": "SIAM J. on Imaging Sciences 7 (2), pp. 1420-1450, 2014", "doi": "10.1137/130946782", "report-no": null, "categories": "stat.ML cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new fast and robust recursive algorithm for\nnear-separable nonnegative matrix factorization, a particular nonnegative blind\nsource separation problem. This algorithm, which we refer to as the successive\nnonnegative projection algorithm (SNPA), is closely related to the popular\nsuccessive projection algorithm (SPA), but takes advantage of the nonnegativity\nconstraint in the decomposition. We prove that SNPA is more robust than SPA and\ncan be applied to a broader class of nonnegative matrices. This is illustrated\non some synthetic data sets, and on a real-world hyperspectral image.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 18:41:48 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 09:06:07 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 08:47:07 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1310.7780", "submitter": "Garvesh Raskutti", "authors": "Garvesh Raskutti and Sayan Mukherjee", "title": "The Information Geometry of Mirror Descent", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information geometry applies concepts in differential geometry to probability\nand statistics and is especially useful for parameter estimation in exponential\nfamilies where parameters are known to lie on a Riemannian manifold.\nConnections between the geometric properties of the induced manifold and\nstatistical properties of the estimation problem are well-established. However\ndeveloping first-order methods that scale to larger problems has been less of a\nfocus in the information geometry community. The best known algorithm that\nincorporates manifold structure is the second-order natural gradient descent\nalgorithm introduced by Amari. On the other hand, stochastic approximation\nmethods have led to the development of first-order methods for optimizing noisy\nobjective functions. A recent generalization of the Robbins-Monro algorithm\nknown as mirror descent, developed by Nemirovski and Yudin is a first order\nmethod that induces non-Euclidean geometries. However current analysis of\nmirror descent does not precisely characterize the induced non-Euclidean\ngeometry nor does it consider performance in terms of statistical relative\nefficiency. In this paper, we prove that mirror descent induced by Bregman\ndivergences is equivalent to the natural gradient descent algorithm on the dual\nRiemannian manifold. Using this equivalence, it follows that (1) mirror descent\nis the steepest descent direction along the Riemannian manifold of the\nexponential family; (2) mirror descent with log-likelihood loss applied to\nparameter estimation in exponential families asymptotically achieves the\nclassical Cram\\'er-Rao lower bound and (3) natural gradient descent for\nmanifolds corresponding to exponential families can be implemented as a\nfirst-order method through mirror descent.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 12:21:12 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 20:40:42 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1310.7795", "submitter": "Jimmy Ren", "authors": "Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Liao", "title": "An Unsupervised Feature Learning Approach to Improve Automatic Incident\n  Detection", "comments": "The 15th IEEE International Conference on Intelligent Transportation\n  Systems (ITSC 2012)", "journal-ref": null, "doi": "10.1109/ITSC.2012.6338621", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated automatic incident detection (AID) technology plays a key role\nin contemporary transportation systems. Though many papers were devoted to\nstudy incident classification algorithms, few study investigated how to enhance\nfeature representation of incidents to improve AID performance. In this paper,\nwe propose to use an unsupervised feature learning algorithm to generate higher\nlevel features to represent incidents. We used real incident data in the\nexperiments and found that effective feature mapping function can be learnt\nfrom the data crosses the test sites. With the enhanced features, detection\nrate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are\nsignificantly improved in all of the three representative cases. This approach\nalso provides an alternative way to reduce the amount of labeled data, which is\nexpensive to obtain, required in training better incident classifiers since the\nfeature learning is unsupervised.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 13:18:41 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ren", "Jimmy SJ.", ""], ["Wang", "Wei", ""], ["Wang", "Jiawei", ""], ["Liao", "Stephen", ""]]}, {"id": "1310.7868", "submitter": "Karim Pichara Baksai", "authors": "Karim Pichara and Pavlos Protopapas", "title": "Automatic Classification of Variable Stars in Catalogs with missing data", "comments": null, "journal-ref": "2013 ApJ 777 83", "doi": "10.1088/0004-637X/777/2/83", "report-no": null, "categories": "astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automatic classification method for astronomical catalogs with\nmissing data. We use Bayesian networks, a probabilistic graphical model, that\nallows us to perform inference to pre- dict missing values given observed data\nand dependency relationships between variables. To learn a Bayesian network\nfrom incomplete data, we use an iterative algorithm that utilises sampling\nmethods and expectation maximization to estimate the distributions and\nprobabilistic dependencies of variables from data with missing values. To test\nour model we use three catalogs with missing data (SAGE, 2MASS and UBVI) and\none complete catalog (MACHO). We examine how classification accuracy changes\nwhen information from missing data catalogs is included, how our method\ncompares to traditional missing data approaches and at what computational cost.\nIntegrating these catalogs with missing data we find that classification of\nvariable objects improves by few percent and by 15% for quasar detection while\nkeeping the computational cost the same.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 16:37:13 GMT"}], "update_date": "2013-10-30", "authors_parsed": [["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1310.7991", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth\n  Netrapalli", "title": "Learning Sparsely Used Overcomplete Dictionaries via Alternating\n  Minimization", "comments": "Local linear convergence now holds under RIP and also more general\n  restricted eigenvalue conditions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse coding, where each sample consists of a\nsparse linear combination of a set of dictionary atoms, and the task is to\nlearn both the dictionary elements and the mixing coefficients. Alternating\nminimization is a popular heuristic for sparse coding, where the dictionary and\nthe coefficients are estimated in alternate steps, keeping the other fixed.\nTypically, the coefficients are estimated via $\\ell_1$ minimization, keeping\nthe dictionary fixed, and the dictionary is estimated through least squares,\nkeeping the coefficients fixed. In this paper, we establish local linear\nconvergence for this variant of alternating minimization and establish that the\nbasin of attraction for the global optimum (corresponding to the true\ndictionary and the coefficients) is $\\order{1/s^2}$, where $s$ is the sparsity\nlevel in each sample and the dictionary satisfies RIP. Combined with the recent\nresults of approximate dictionary estimation, this yields provable guarantees\nfor exact recovery of both the dictionary elements and the coefficients, when\nthe dictionary elements are incoherent.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 01:12:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 22:55:12 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Agarwal", "Alekh", ""], ["Anandkumar", "Animashree", ""], ["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1310.7994", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions for Novel Word Detection in\n  Separable Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplicial condition and other stronger conditions that imply it have\nrecently played a central role in developing polynomial time algorithms with\nprovable asymptotic consistency and sample complexity guarantees for topic\nestimation in separable topic models. Of these algorithms, those that rely\nsolely on the simplicial condition are impractical while the practical ones\nneed stronger conditions. In this paper, we demonstrate, for the first time,\nthat the simplicial condition is a fundamental, algorithm-independent,\ninformation-theoretic necessary condition for consistent separable topic\nestimation. Furthermore, under solely the simplicial condition, we present a\npractical quadratic-complexity algorithm based on random projections which\nconsistently detects all novel words of all topics using only up to\nsecond-order empirical word moments. This algorithm is amenable to distributed\nimplementation making it attractive for 'big-data' scenarios involving a\nnetwork of large distributed databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 01:19:26 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Rohban", "Mohammad H.", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1310.8004", "submitter": "Boyu Wang", "authors": "Boyu Wang, Joelle Pineau", "title": "Online Ensemble Learning for Imbalanced Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While both cost-sensitive learning and online learning have been studied\nextensively, the effort in simultaneously dealing with these two issues is\nlimited. Aiming at this challenge task, a novel learning framework is proposed\nin this paper. The key idea is based on the fusion of online ensemble\nalgorithms and the state of the art batch mode cost-sensitive bagging/boosting\nalgorithms. Within this framework, two separately developed research areas are\nbridged together, and a batch of theoretically sound online cost-sensitive\nbagging and online cost-sensitive boosting algorithms are first proposed.\nUnlike other online cost-sensitive learning algorithms lacking theoretical\nanalysis of asymptotic properties, the convergence of the proposed algorithms\nis guaranteed under certain conditions, and the experimental evidence with\nbenchmark data sets also validates the effectiveness and efficiency of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 02:11:48 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Wang", "Boyu", ""], ["Pineau", "Joelle", ""]]}, {"id": "1310.8243", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Leon Bottou, Miroslav Dudik, John Langford", "title": "Para-active learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training examples are not all equally informative. Active learning strategies\nleverage this observation in order to massively reduce the number of examples\nthat need to be labeled. We leverage the same observation to build a generic\nstrategy for parallelizing learning algorithms. This strategy is effective\nbecause the search for informative examples is highly parallelizable and\nbecause we show that its performance does not deteriorate when the sifting\nprocess relies on a slightly outdated model. Parallel active learning is\nparticularly attractive to train nonlinear models with non-linear\nrepresentations because there are few practical parallel learning algorithms\nfor such models. We report preliminary experiments using both kernel SVMs and\nSGD-trained neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 17:49:11 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bottou", "Leon", ""], ["Dudik", "Miroslav", ""], ["Langford", "John", ""]]}, {"id": "1310.8320", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Jun Liu", "title": "Safe and Efficient Screening For Sparse Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening is an effective technique for speeding up the training process of a\nsparse learning model by removing the features that are guaranteed to be\ninactive the process. In this paper, we present a efficient screening technique\nfor sparse support vector machine based on variational inequality. The\ntechnique is both efficient and safe.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 20:56:50 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Zhao", "Zheng", ""], ["Liu", "Jun", ""]]}, {"id": "1310.8418", "submitter": "Dhruv Mahajan", "authors": "Dhruv Mahajan, Nikunj Agrawal, S. Sathiya Keerthi, S. Sundararajan,\n  Leon Bottou", "title": "An efficient distributed learning algorithm based on effective local\n  functional approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable machine learning over big data is an important problem that is\nreceiving a lot of attention in recent years. On popular distributed\nenvironments such as Hadoop running on a cluster of commodity machines,\ncommunication costs are substantial and algorithms need to be designed suitably\nconsidering those costs. In this paper we give a novel approach to the\ndistributed training of linear classifiers (involving smooth losses and L2\nregularization) that is designed to reduce the total communication costs. At\neach iteration, the nodes minimize locally formed approximate objective\nfunctions; then the resulting minimizers are combined to form a descent\ndirection to move. Our approach gives a lot of freedom in the formation of the\napproximate objective function as well as in the choice of methods to solve\nthem. The method is shown to have $O(log(1/\\epsilon))$ time convergence. The\nmethod can be viewed as an iterative parameter mixing method. A special\ninstantiation yields a parallel stochastic gradient descent method with strong\nconvergence. When communication times between nodes are large, our method is\nmuch faster than the Terascale method (Agarwal et al., 2011), which is a state\nof the art distributed solver based on the statistical query model (Chuet al.,\n2006) that computes function and gradient values in a distributed fashion. We\nalso evaluate against other recent distributed methods and demonstrate superior\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 08:00:21 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 06:03:55 GMT"}, {"version": "v3", "created": "Sun, 18 May 2014 20:13:22 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2015 21:06:08 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Agrawal", "Nikunj", ""], ["Keerthi", "S. Sathiya", ""], ["Sundararajan", "S.", ""], ["Bottou", "Leon", ""]]}, {"id": "1310.8428", "submitter": "Hongyu Su", "authors": "Hongyu Su, Juho Rousu", "title": "Multilabel Classification through Random Graph Ensembles", "comments": "15 Pages, 1 Figures", "journal-ref": "JMLR: Workshop and Conference Proceedings 29:404--418, 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present new methods for multilabel classification, relying on ensemble\nlearning on a collection of random output graphs imposed on the multilabel and\na kernel-based structured output learner as the base classifier. For ensemble\nlearning, differences among the output graphs provide the required base\nclassifier diversity and lead to improved performance in the increasing size of\nthe ensemble. We study different methods of forming the ensemble prediction,\nincluding majority voting and two methods that perform inferences over the\ngraph structures before or after combining the base models into the ensemble.\nWe compare the methods against the state-of-the-art machine learning approaches\non a set of heterogeneous multilabel benchmark problems, including multilabel\nAdaBoost, convex multitask feature learning, as well as single target learning\napproaches represented by Bagging and SVM. In our experiments, the random graph\nensembles are very competitive and robust, ranking first or second on most of\nthe datasets. Overall, our results show that random graph ensembles are viable\nalternatives to flat multilabel and multitask learners.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 09:00:39 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 04:04:49 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Su", "Hongyu", ""], ["Rousu", "Juho", ""]]}, {"id": "1310.8467", "submitter": "G.Srinivas Rao", "authors": "G.Srinivas Rao, A.V.Ramana", "title": "Reinforcement Learning Framework for Opportunistic Routing in WSNs", "comments": "05 Pages,07 Figures, 01 Table, IJCSN, Volume 2, Issue 5", "journal-ref": "IJCSN - International Journal of Computer Science and Network -\n  October 2013", "doi": null, "report-no": "IJCSN-2013-2-5-22", "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing packets opportunistically is an essential part of multihop ad hoc\nwireless sensor networks. The existing routing techniques are not adaptive\nopportunistic. In this paper we have proposed an adaptive opportunistic routing\nscheme that routes packets opportunistically in order to ensure that packet\nloss is avoided. Learning and routing are combined in the framework that\nexplores the optimal routing possibilities. In this paper we implemented this\nReinforced learning framework using a customer simulator. The experimental\nresults revealed that the scheme is able to exploit the opportunistic to\noptimize routing of packets even though the network structure is unknown.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 11:57:06 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Rao", "G. Srinivas", ""], ["Ramana", "A. V.", ""]]}, {"id": "1310.8499", "submitter": "Karol Gregor", "authors": "Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan\n  Wierstra", "title": "Deep AutoRegressive Networks", "comments": "Appears in Proceedings of the 31st International Conference on\n  Machine Learning (ICML), Beijing, China, 2014", "journal-ref": "Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan\n  Wierstra. Deep AutoRegressive Networks. In Proceedings of the 31st\n  International Conference on Machine Learning (ICML), JMLR: W&CP volume 32,\n  2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep, generative autoencoder capable of learning hierarchies\nof distributed representations from data. Successive deep stochastic hidden\nlayers are equipped with autoregressive connections, which enable the model to\nbe sampled from quickly and exactly via ancestral sampling. We derive an\nefficient approximate parameter estimation method based on the minimum\ndescription length (MDL) principle, which can be seen as maximising a\nvariational lower bound on the log-likelihood, with a feedforward neural\nnetwork implementing approximate inference. We demonstrate state-of-the-art\ngenerative performance on a number of classic data sets: several UCI data sets,\nMNIST and Atari 2600 games.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 13:47:30 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 16:22:43 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Gregor", "Karol", ""], ["Danihelka", "Ivo", ""], ["Mnih", "Andriy", ""], ["Blundell", "Charles", ""], ["Wierstra", "Daan", ""]]}]