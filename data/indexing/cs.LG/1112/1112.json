[{"id": "1112.0826", "submitter": "Yingyu Liang", "authors": "Maria Florina Balcan, Yingyu Liang", "title": "Clustering under Perturbation Resilience", "comments": "54 pages. Appears in SIAM Journal on Computing (SICOMP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fact that distances between data points in many real-world\nclustering instances are often based on heuristic measures, Bilu and\nLinial~\\cite{BL} proposed analyzing objective based clustering problems under\nthe assumption that the optimum clustering to the objective is preserved under\nsmall multiplicative perturbations to distances between points. The hope is\nthat by exploiting the structure in such instances, one can overcome worst case\nhardness results.\n  In this paper, we provide several results within this framework. For\ncenter-based objectives, we present an algorithm that can optimally cluster\ninstances resilient to perturbations of factor $(1 + \\sqrt{2})$, solving an\nopen problem of Awasthi et al.~\\cite{ABS10}. For $k$-median, a center-based\nobjective of special interest, we additionally give algorithms for a more\nrelaxed assumption in which we allow the optimal solution to change in a small\n$\\epsilon$ fraction of the points after perturbation. We give the first bounds\nknown for $k$-median under this more realistic and more general assumption. We\nalso provide positive results for min-sum clustering which is typically a\nharder objective than center-based objectives from approximability standpoint.\nOur algorithms are based on new linkage criteria that may be of independent\ninterest.\n  Additionally, we give sublinear-time algorithms, showing algorithms that can\nreturn an implicit clustering from only access to a small random sample.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 03:42:07 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 19:49:32 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2011 03:37:24 GMT"}, {"version": "v4", "created": "Fri, 8 Aug 2014 02:27:56 GMT"}, {"version": "v5", "created": "Sun, 11 Dec 2016 21:41:33 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Liang", "Yingyu", ""]]}, {"id": "1112.1125", "submitter": "Daniel Little", "authors": "Daniel Y. Little and Friedrich T. Sommer", "title": "Learning in embodied action-perception loops through exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although exploratory behaviors are ubiquitous in the animal kingdom, their\ncomputational underpinnings are still largely unknown. Behavioral Psychology\nhas identified learning as a primary drive underlying many exploratory\nbehaviors. Exploration is seen as a means for an animal to gather sensory data\nuseful for reducing its ignorance about the environment. While related problems\nhave been addressed in Data Mining and Reinforcement Learning, the\ncomputational modeling of learning-driven exploration by embodied agents is\nlargely unrepresented.\n  Here, we propose a computational theory for learning-driven exploration based\non the concept of missing information that allows an agent to identify\ninformative actions using Bayesian inference. We demonstrate that when\nembodiment constraints are high, agents must actively coordinate their actions\nto learn efficiently. Compared to earlier approaches, our exploration policy\nyields more efficient learning across a range of worlds with diverse\nstructures. The improved learning in turn affords greater success in general\ntasks including navigation and reward gathering. We conclude by discussing how\nthe proposed theory relates to previous information-theoretic objectives of\nbehavior, such as predictive information and the free energy principle, and how\nit might contribute to a general theory of exploratory behavior.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 00:13:44 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2011 22:58:58 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Little", "Daniel Y.", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "1112.1133", "submitter": "Richard Sutton", "authors": "Joseph Modayil, Adam White, Richard S. Sutton", "title": "Multi-timescale Nexting in a Reinforcement Learning Robot", "comments": "(11 pages, 5 figures, This version to appear in the Proceedings of\n  the Conference on the Simulation of Adaptive Behavior, 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"nexting\" has been used by psychologists to refer to the propensity\nof people and many other animals to continually predict what will happen next\nin an immediate, local, and personal sense. The ability to \"next\" constitutes a\nbasic kind of awareness and knowledge of one's environment. In this paper we\npresent results with a robot that learns to next in real time, predicting\nthousands of features of the world's state, including all sensory inputs, at\ntimescales from 0.1 to 8 seconds. This was achieved by treating each state\nfeature as a reward-like target and applying temporal-difference methods to\nlearn a corresponding value function with a discount rate corresponding to the\ntimescale. We show that two thousand predictions, each dependent on six\nthousand state features, can be learned and updated online at better than 10Hz\non a laptop computer, using the standard TD(lambda) algorithm with linear\nfunction approximation. We show that this approach is efficient enough to be\npractical, with most of the learning complete within 30 minutes. We also show\nthat a single tile-coded feature representation suffices to accurately predict\nmany different signals at a significant range of timescales. Finally, we show\nthat the accuracy of our learned predictions compares favorably with the\noptimal off-line solution.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 00:45:28 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2011 23:37:45 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2012 20:39:30 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Modayil", "Joseph", ""], ["White", "Adam", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1112.1390", "submitter": "Yuri Kalnishkan", "authors": "Fedor Zhdanov and Yuri Kalnishkan", "title": "An Identity for Kernel Ridge Regression", "comments": "35 pages; extended version of ALT 2010 paper (Proceedings of ALT\n  2010, LNCS 6331, Springer, 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives an identity connecting the square loss of ridge regression\nin on-line mode with the loss of the retrospectively best regressor. Some\ncorollaries about the properties of the cumulative loss of on-line ridge\nregression are also obtained.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 20:15:37 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Zhdanov", "Fedor", ""], ["Kalnishkan", "Yuri", ""]]}, {"id": "1112.1556", "submitter": "Sivan Sabato", "authors": "Alon Gonen, Sivan Sabato and Shai Shalev-Shwartz", "title": "Active Learning of Halfspaces under a Margin Assumption", "comments": "A more detailed exposition; Added a description of a simpler\n  implementation and results of experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and analyze a new, efficient, pool-based active learning algorithm\nfor halfspaces, called ALuMA. Most previous algorithms show exponential\nimprovement in the label complexity assuming that the distribution over the\ninstance space is close to uniform. This assumption rarely holds in practical\napplications. Instead, we study the label complexity under a large-margin\nassumption -- a much more realistic condition, as evident by the success of\nmargin-based algorithms such as SVM. Our algorithm is computationally efficient\nand comes with formal guarantees on its label complexity. It also naturally\nextends to the non-separable case and to non-linear kernels. Experiments\nillustrate the clear advantage of ALuMA over other active learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 13:34:25 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2012 11:48:41 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2012 08:07:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gonen", "Alon", ""], ["Sabato", "Sivan", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1112.1615", "submitter": "Boubkeur Boudaoud", "authors": "Dominique Barth, Boubkeur Boudaoud and Thierry Mautor", "title": "SLA Establishment with Guaranteed QoS in the Interdomain Network: A\n  Stock Model", "comments": "19 pages, 19 figures, IJCNC,\n  http://airccse.org/journal/cnc/0711cnc13.pdf", "journal-ref": "International Journal of Computer Networks & Communications 3.4\n  (July 2011) 188-206", "doi": "10.5121/ijcnc.2011.3413", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new model that we present in this paper is introduced in the context of\nguaranteed QoS and resources management in the inter-domain routing framework.\nThis model, called the stock model, is based on a reverse cascade approach and\nis applied in a distributed context. So transit providers have to learn the\nright capacities to buy and to stock and, therefore learning theory is applied\nthrough an iterative process. We show that transit providers manage to learn\nhow to strategically choose their capacities on each route in order to maximize\ntheir benefits, despite the very incomplete information. Finally, we provide\nand analyse some simulation results given by the application of the model in a\nsimple case where the model quickly converges to a stable state.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 16:34:20 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Barth", "Dominique", ""], ["Boudaoud", "Boubkeur", ""], ["Mautor", "Thierry", ""]]}, {"id": "1112.1734", "submitter": "Marcos Domingues", "authors": "Marcos Aur\\'elio Domingues, Solange Oliveira Rezende", "title": "Using Taxonomies to Facilitate the Analysis of the Association Rules", "comments": "ECML/PKDD'05 The Second International Workshop on Knowledge Discovery\n  and Ontologies (KDO'05)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data Mining process enables the end users to analyze, understand and use\nthe extracted knowledge in an intelligent system or to support in the\ndecision-making processes. However, many algorithms used in the process\nencounter large quantities of patterns, complicating the analysis of the\npatterns. This fact occurs with association rules, a Data Mining technique that\ntries to identify intrinsic patterns in large data sets. A method that can help\nthe analysis of the association rules is the use of taxonomies in the step of\npost-processing knowledge. In this paper, the GART algorithm is proposed, which\nuses taxonomies to generalize association rules, and the RulEE-GAR\ncomputational module, that enables the analysis of the generalized rules.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 23:33:15 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Domingues", "Marcos Aur\u00e9lio", ""], ["Rezende", "Solange Oliveira", ""]]}, {"id": "1112.1757", "submitter": "Soumitra Pal", "authors": "T. S. Jayram, Soumitra Pal, Vijay Arya", "title": "Recovery of a Sparse Integer Solution to an Underdetermined System of\n  Linear Equations", "comments": "4 pages, contributed paper to be published at NIPS 2011 Workshop on\n  Sparse Representation and Low-rank Approximation, 16 December 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a system of m linear equations in n variables Ax=b where A is a\ngiven m x n matrix and b is a given m-vector known to be equal to Ax' for some\nunknown solution x' that is integer and k-sparse: x' in {0,1}^n and exactly k\nentries of x' are 1. We give necessary and sufficient conditions for recovering\nthe solution x exactly using an LP relaxation that minimizes l1 norm of x. When\nA is drawn from a distribution that has exchangeable columns, we show an\ninteresting connection between the recovery probability and a well known\nproblem in geometry, namely the k-set problem. To the best of our knowledge,\nthis connection appears to be new in the compressive sensing literature. We\nempirically show that for large n if the elements of A are drawn i.i.d. from\nthe normal distribution then the performance of the recovery LP exhibits a\nphase transition, i.e., for each k there exists a value m' of m such that the\nrecovery always succeeds if m > m' and always fails if m < m'. Using the\nempirical data we conjecture that m' = nH(k/n)/2 where H(x) = -(x)log_2(x) -\n(1-x)log_2(1-x) is the binary entropy function.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 03:32:39 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2011 05:33:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jayram", "T. S.", ""], ["Pal", "Soumitra", ""], ["Arya", "Vijay", ""]]}, {"id": "1112.1768", "submitter": "Keqin Liu", "authors": "Keqin Liu, Qing Zhao", "title": "Extended UCB Policy for Multi-Armed Bandit with Light-Tailed Reward\n  Distributions", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-armed bandit problems in which a player aims to accrue\nreward by sequentially playing a given set of arms with unknown reward\nstatistics. In the classic work, policies were proposed to achieve the optimal\nlogarithmic regret order for some special classes of light-tailed reward\ndistributions, e.g., Auer et al.'s UCB1 index policy for reward distributions\nwith finite support. In this paper, we extend Auer et al.'s UCB1 index policy\nto achieve the optimal logarithmic regret order for all light-tailed (or\nequivalently, locally sub-Gaussian) reward distributions defined by the (local)\nexistence of the moment-generating function.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 05:53:35 GMT"}], "update_date": "2011-12-09", "authors_parsed": [["Liu", "Keqin", ""], ["Zhao", "Qing", ""]]}, {"id": "1112.1937", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen (INRIA Bordeaux - Sud-Ouest), Adrien Baranes (INRIA\n  Bordeaux - Sud-Ouest), Pierre-Yves Oudeyer (INRIA Bordeaux - Sud-Ouest)", "title": "Bootstrapping Intrinsically Motivated Learning with Human Demonstrations", "comments": "IEEE International Conference on Development and Learning, Frankfurt\n  : Germany (2011)", "journal-ref": "2011 IEEE International Conference on Development and Learning\n  (ICDL)", "doi": "10.1109/DEVLRN.2011.6037329", "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the coupling of internally guided learning and social\ninteraction, and more specifically the improvement owing to demonstrations of\nthe learning by intrinsic motivation. We present Socially Guided Intrinsic\nMotivation by Demonstration (SGIM-D), an algorithm for learning in continuous,\nunbounded and non-preset environments. After introducing social learning and\nintrinsic motivation, we describe the design of our algorithm, before showing\nthrough a fishing experiment that SGIM-D efficiently combines the advantages of\nsocial learning and intrinsic motivation to gain a wide repertoire while being\nspecialised in specific subspaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 20:27:31 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Nguyen", "Sao Mai", "", "INRIA Bordeaux - Sud-Ouest"], ["Baranes", "Adrien", "", "INRIA\n  Bordeaux - Sud-Ouest"], ["Oudeyer", "Pierre-Yves", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1112.1966", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Bipartite ranking algorithm for classification and survival analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1108.2820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised aggregation of independently built univariate predictors is\nexplored as an alternative regularization approach for noisy, sparse datasets.\nBipartite ranking algorithm Smooth Rank implementing this approach is\nintroduced. The advantages of this algorithm are demonstrated on two types of\nproblems. First, Smooth Rank is applied to two-class problems from bio-medical\nfield, where ranking is often preferable to classification. In comparison\nagainst SVMs with radial and linear kernels, Smooth Rank had the best\nperformance on 8 out of 12 benchmark benchmarks. The second area of application\nis survival analysis, which is reduced here to bipartite ranking in a way which\nallows one to use commonly accepted measures of methods performance. In\ncomparison of Smooth Rank with Cox PH regression and CoxPath methods, Smooth\nRank proved to be the best on 9 out of 10 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 21:33:38 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1112.2187", "submitter": "Chih-Yu Wang", "authors": "Chih-Yu Wang and Yan Chen and K. J. Ray Liu", "title": "Chinese Restaurant Game - Part II: Applications to Wireless Networking,\n  Cloud Computing, and Online Social Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part I of this two-part paper [1], we proposed a new game, called Chinese\nrestaurant game, to analyze the social learning problem with negative network\nexternality. The best responses of agents in the Chinese restaurant game with\nimperfect signals are constructed through a recursive method, and the influence\nof both learning and network externality on the utilities of agents is studied.\nIn Part II of this two-part paper, we illustrate three applications of Chinese\nrestaurant game in wireless networking, cloud computing, and online social\nnetworking. For each application, we formulate the corresponding problem as a\nChinese restaurant game and analyze how agents learn and make strategic\ndecisions in the problem. The proposed method is compared with four\ncommon-sense methods in terms of agents' utilities and the overall system\nperformance through simulations. We find that the proposed Chinese restaurant\ngame theoretic approach indeed helps agents make better decisions and improves\nthe overall system performance. Furthermore, agents with different decision\norders have different advantages in terms of their utilities, which also\nverifies the conclusions drawn in Part I of this two-part paper.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 19:31:28 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2011 00:22:51 GMT"}], "update_date": "2011-12-16", "authors_parsed": [["Wang", "Chih-Yu", ""], ["Chen", "Yan", ""], ["Liu", "K. J. Ray", ""]]}, {"id": "1112.2188", "submitter": "Chih-Yu Wang", "authors": "Chih-Yu Wang and Yan Chen and K. J. Ray Liu", "title": "Chinese Restaurant Game - Part I: Theory of Learning with Negative\n  Network Externality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a social network, agents are intelligent and have the capability to make\ndecisions to maximize their utilities. They can either make wise decisions by\ntaking advantages of other agents' experiences through learning, or make\ndecisions earlier to avoid competitions from huge crowds. Both these two\neffects, social learning and negative network externality, play important roles\nin the decision process of an agent. While there are existing works on either\nsocial learning or negative network externality, a general study on considering\nboth these two contradictory effects is still limited. We find that the Chinese\nrestaurant process, a popular random process, provides a well-defined structure\nto model the decision process of an agent under these two effects. By\nintroducing the strategic behavior into the non-strategic Chinese restaurant\nprocess, in Part I of this two-part paper, we propose a new game, called\nChinese Restaurant Game, to formulate the social learning problem with negative\nnetwork externality. Through analyzing the proposed Chinese restaurant game, we\nderive the optimal strategy of each agent and provide a recursive method to\nachieve the optimal strategy. How social learning and negative network\nexternality influence each other under various settings is also studied through\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 19:33:06 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2011 00:21:47 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2012 07:20:48 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Wang", "Chih-Yu", ""], ["Chen", "Yan", ""], ["Liu", "K. J. Ray", ""]]}, {"id": "1112.2315", "submitter": "Michalis Smyrnakis", "authors": "Michalis Smyrnakis and David S. Leslie", "title": "Adaptive Forgetting Factor Fictitious Play", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well known that decentralised optimisation can be formulated as a\npotential game, and game-theoretical learning algorithms can be used to find an\noptimum. One of the most common learning techniques in game theory is\nfictitious play. However fictitious play is founded on an implicit assumption\nthat opponents' strategies are stationary. We present a novel variation of\nfictitious play that allows the use of a more realistic model of opponent\nstrategy. It uses a heuristic approach, from the online streaming data\nliterature, to adaptively update the weights assigned to recently observed\nactions. We compare the results of the proposed algorithm with those of\nstochastic and geometric fictitious play in a simple strategic form game, a\nvehicle target assignment game and a disaster management problem. In all the\ntests the rate of convergence of the proposed algorithm was similar or better\nthan the variations of fictitious play we compared it with. The new algorithm\ntherefore improves the performance of game-theoretical learning in\ndecentralised optimisation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 01:52:50 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Smyrnakis", "Michalis", ""], ["Leslie", "David S.", ""]]}, {"id": "1112.2318", "submitter": "Bamdev Mishra", "authors": "B. Mishra, G. Meyer, F. Bach and R. Sepulchre", "title": "Low-rank optimization with trace norm penalty", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of low-rank trace norm minimization. We\npropose an algorithm that alternates between fixed-rank optimization and\nrank-one updates. The fixed-rank optimization is characterized by an efficient\nfactorization that makes the trace norm differentiable in the search space and\nthe computation of duality gap numerically tractable. The search space is\nnonlinear but is equipped with a particular Riemannian structure that leads to\nefficient computations. We present a second-order trust-region algorithm with a\nguaranteed quadratic rate of convergence. Overall, the proposed optimization\nscheme converges super-linearly to the global solution while maintaining\ncomplexity that is linear in the number of rows and columns of the matrix. To\ncompute a set of solutions efficiently for a grid of regularization parameters\nwe propose a predictor-corrector approach that outperforms the naive\nwarm-restart approach on the fixed-rank quotient manifold. The performance of\nthe proposed algorithm is illustrated on problems of low-rank matrix completion\nand multivariate linear regression.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 04:00:57 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 04:58:28 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Mishra", "B.", ""], ["Meyer", "G.", ""], ["Bach", "F.", ""], ["Sepulchre", "R.", ""]]}, {"id": "1112.2680", "submitter": "Rob Hall", "authors": "Rob Hall, Alessandro Rinaldo, Larry Wasserman", "title": "Random Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a relaxed privacy definition called {\\em random differential\nprivacy} (RDP). Differential privacy requires that adding any new observation\nto a database will have small effect on the output of the data-release\nprocedure. Random differential privacy requires that adding a {\\em randomly\ndrawn new observation} to a database will have small effect on the output. We\nshow an analog of the composition property of differentially private procedures\nwhich applies to our new definition. We show how to release an RDP histogram\nand we show that RDP histograms are much more accurate than histograms obtained\nusing ordinary differential privacy. We finally show an analog of the global\nsensitivity framework for the release of functions under our privacy\ndefinition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 20:16:03 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Hall", "Rob", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1112.2738", "submitter": "Jonas Peters", "authors": "Bernhard Sch\\\"olkopf, Dominik Janzing, Jonas Peters, Kun Zhang", "title": "Robust Learning via Cause-Effect Models", "comments": null, "journal-ref": "A version of this paper has been published as \"On Causal and\n  Anticausal Learning\" in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of function estimation in the case where the data\ndistribution may shift between training and test time, and additional\ninformation about it may be available at test time. This relates to popular\nscenarios such as covariate shift, concept drift, transfer learning and\nsemi-supervised learning. This working paper discusses how these tasks could be\ntackled depending on the kind of changes of the distributions. It argues that\nknowledge of an underlying causal direction can facilitate several of these\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 22:33:55 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""], ["Peters", "Jonas", ""], ["Zhang", "Kun", ""]]}, {"id": "1112.2801", "submitter": "Yohji Akama", "authors": "Yohji Akama", "title": "A new order theory of set systems and better quasi-orderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By reformulating a learning process of a set system L as a game between\nTeacher (presenter of data) and Learner (updater of the abstract independent\nset), we define the order type dim L of L to be the order type of the game\ntree. The theory of this new order type and continuous, monotone function\nbetween set systems corresponds to the theory of well quasi-orderings (WQOs).\nAs Nash-Williams developed the theory of WQOs to the theory of better\nquasi-orderings (BQOs), we introduce a set system that has order type and\ncorresponds to a BQO. We prove that the class of set systems corresponding to\nBQOs is closed by any monotone function. In (Shinohara and Arimura. \"Inductive\ninference of unbounded unions of pattern languages from positive data.\"\nTheoretical Computer Science, pp. 191-209, 2000), for any set system L, they\nconsidered the class of arbitrary (finite) unions of members of L. From\nviewpoint of WQOs and BQOs, we characterize the set systems L such that the\nclass of arbitrary (finite) unions of members of L has order type. The\ncharacterization shows that the order structure of the set system L with\nrespect to the set-inclusion is not important for the resulting set system\nhaving order type. We point out continuous, monotone function of set systems is\nsimilar to positive reduction to Jockusch-Owings' weakly semirecursive sets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 06:04:05 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2012 18:27:57 GMT"}, {"version": "v3", "created": "Wed, 29 Feb 2012 13:14:08 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Akama", "Yohji", ""]]}, {"id": "1112.3712", "submitter": "Youngmin Cho", "authors": "Youngmin Cho and Lawrence K. Saul", "title": "Analysis and Extension of Arc-Cosine Kernels for Large Margin\n  Classification", "comments": "Preprint submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a recently proposed family of positive-definite kernels that\nmimic the computation in large neural networks. We examine the properties of\nthese kernels using tools from differential geometry; specifically, we analyze\nthe geometry of surfaces in Hilbert space that are induced by these kernels.\nWhen this geometry is described by a Riemannian manifold, we derive results for\nthe metric, curvature, and volume element. Interestingly, though, we find that\nthe simplest kernel in this family does not admit such an interpretation. We\nexplore two variations of these kernels that mimic computation in neural\nnetworks with different activation functions. We experiment with these new\nkernels on several data sets and highlight their general trends in performance\nfor classification.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 05:21:10 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Cho", "Youngmin", ""], ["Saul", "Lawrence K.", ""]]}, {"id": "1112.3714", "submitter": "Youngmin Cho", "authors": "Youngmin Cho and Lawrence K. Saul", "title": "Nonnegative Matrix Factorization for Semi-supervised Dimensionality\n  Reduction", "comments": "Preprint submitted to Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to incorporate information from labeled examples into nonnegative\nmatrix factorization (NMF), a popular unsupervised learning algorithm for\ndimensionality reduction. In addition to mapping the data into a space of lower\ndimensionality, our approach aims to preserve the nonnegative components of the\ndata that are important for classification. We identify these components from\nthe support vectors of large-margin classifiers and derive iterative updates to\npreserve them in a semi-supervised version of NMF. These updates have a simple\nmultiplicative form like their unsupervised counterparts; they are also\nguaranteed at each iteration to decrease their loss function---a weighted sum\nof I-divergences that captures the trade-off between unsupervised and\nsupervised learning. We evaluate these updates for dimensionality reduction\nwhen they are used as a precursor to linear classification. In this role, we\nfind that they yield much better performance than their unsupervised\ncounterparts. We also find one unexpected benefit of the low dimensional\nrepresentations discovered by our approach: often they yield more accurate\nclassifiers than both ordinary and transductive SVMs trained in the original\ninput space.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 05:33:59 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Cho", "Youngmin", ""], ["Saul", "Lawrence K.", ""]]}, {"id": "1112.3946", "submitter": "Hui Zhang", "authors": "Hui Zhang, Jian-Feng Cai, Lizhi Cheng, Jubo Zhu", "title": "Strongly Convex Programming for Exact Matrix Completion and Robust\n  Principal Component Analysis", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common task in matrix completion (MC) and robust principle component\nanalysis (RPCA) is to recover a low-rank matrix from a given data matrix. These\nproblems gained great attention from various areas in applied sciences\nrecently, especially after the publication of the pioneering works of Cand`es\net al.. One fundamental result in MC and RPCA is that nuclear norm based convex\noptimizations lead to the exact low-rank matrix recovery under suitable\nconditions. In this paper, we extend this result by showing that strongly\nconvex optimizations can guarantee the exact low-rank matrix recovery as well.\nThe result in this paper not only provides sufficient conditions under which\nthe strongly convex models lead to the exact low-rank matrix recovery, but also\nguides us on how to choose suitable parameters in practical algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 20:40:23 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 04:27:46 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Zhang", "Hui", ""], ["Cai", "Jian-Feng", ""], ["Cheng", "Lizhi", ""], ["Zhu", "Jubo", ""]]}, {"id": "1112.4020", "submitter": "Andri Mirzal", "authors": "Andri Mirzal", "title": "Clustering and Latent Semantic Indexing Aspects of the Nonnegative\n  Matrix Factorization", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper provides a theoretical support for clustering aspect of the\nnonnegative matrix factorization (NMF). By utilizing the Karush-Kuhn-Tucker\noptimality conditions, we show that NMF objective is equivalent to graph\nclustering objective, so clustering aspect of the NMF has a solid\njustification. Different from previous approaches which usually discard the\nnonnegativity constraints, our approach guarantees the stationary point being\nused in deriving the equivalence is located on the feasible region in the\nnonnegative orthant. Additionally, since clustering capability of a matrix\ndecomposition technique can sometimes imply its latent semantic indexing (LSI)\naspect, we will also evaluate LSI aspect of the NMF by showing its capability\nin solving the synonymy and polysemy problems in synthetic datasets. And more\nextensive evaluation will be conducted by comparing LSI performances of the NMF\nand the singular value decomposition (SVD), the standard LSI method, using some\nstandard datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 03:57:06 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Mirzal", "Andri", ""]]}, {"id": "1112.4105", "submitter": "Jeff M Phillips", "authors": "Jeff M. Phillips", "title": "epsilon-Samples of Kernels", "comments": "13 pages, 2 figures. Cleaned up writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the worst case error of kernel density estimates via subset\napproximation. A kernel density estimate of a distribution is the convolution\nof that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset\n(i.e. a point set) of the input distribution, we can compare the kernel density\nestimates of the input distribution with that of the subset and bound the worst\ncase error. If the maximum error is eps, then this subset can be thought of as\nan eps-sample (aka an eps-approximation) of the range space defined with the\ninput distribution as the ground set and the fixed kernel representing the\nfamily of ranges. Interestingly, in this case the ranges are not binary, but\nhave a continuous range (for simplicity we focus on kernels with range of\n[0,1]); these allow for smoother notions of range spaces.\n  It turns out, the use of this smoother family of range spaces has an added\nbenefit of greatly decreasing the size required for eps-samples. For instance,\nin the plane the size is O((1/eps^{4/3}) log^{2/3}(1/eps)) for disks (based on\nVC-dimension arguments) but is only O((1/eps) sqrt{log (1/eps)}) for Gaussian\nkernels and for kernels with bounded slope that only affect a bounded domain.\nThese bounds are accomplished by studying the discrepancy of these \"kernel\"\nrange spaces, and here the improvement in bounds are even more pronounced. In\nthe plane, we show the discrepancy is O(sqrt{log n}) for these kernels, whereas\nfor balls there is a lower bound of Omega(n^{1/4}).\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 01:19:25 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2012 05:35:25 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2012 22:46:53 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Phillips", "Jeff M.", ""]]}, {"id": "1112.4133", "submitter": "Hocine Cherifi", "authors": "Vincent Labatut, Hocine Cherifi (Le2i)", "title": "Evaluation of Performance Measures for Classifiers Comparison", "comments": null, "journal-ref": "Ubiquitous Computing and Communication Journal, 6:21-34, 2011", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of the best classification algorithm for a given dataset is a\nvery widespread problem, occuring each time one has to choose a classifier to\nsolve a real-world problem. It is also a complex task with many important\nmethodological decisions to make. Among those, one of the most crucial is the\nchoice of an appropriate measure in order to properly assess the classification\nperformance and rank the algorithms. In this article, we focus on this specific\ntask. We present the most popular measures and compare their behavior through\ndiscrimination plots. We then discuss their properties from a more theoretical\nperspective. It turns out several of them are equivalent for classifiers\ncomparison purposes. Futhermore. they can also lead to interpretation problems.\nAmong the numerous measures proposed over the years, it appears that the\nclassical overall success rate and marginal rates are the more suitable for\nclassifier comparison task.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 08:02:49 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Labatut", "Vincent", "", "Le2i"], ["Cherifi", "Hocine", "", "Le2i"]]}, {"id": "1112.4243", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Jiqing Han and Tieran Zheng and Shiwen Deng", "title": "Online Learning for Classification of Low-rank Representation Features\n  and Its Applications in Audio Segment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel framework based on trace norm minimization for audio\nsegment is proposed. In this framework, both the feature extraction and\nclassification are obtained by solving corresponding convex optimization\nproblem with trace norm regularization. For feature extraction, robust\nprinciple component analysis (robust PCA) via minimization a combination of the\nnuclear norm and the $\\ell_1$-norm is used to extract low-rank features which\nare robust to white noise and gross corruption for audio segments. These\nlow-rank features are fed to a linear classifier where the weight and bias are\nlearned by solving similar trace norm constrained problems. For this\nclassifier, most methods find the weight and bias in batch-mode learning, which\nmakes them inefficient for large-scale problems. In this paper, we propose an\nonline framework using accelerated proximal gradient method. This framework has\na main advantage in memory cost. In addition, as a result of the regularization\nformulation of matrix classification, the Lipschitz constant was given\nexplicitly, and hence the step size estimation of general proximal gradient\nmethod was omitted in our approach. Experiments on real data sets for\nlaugh/non-laugh and applause/non-applause classification indicate that this\nnovel framework is effective and noise robust.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 05:29:18 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Shi", "Ziqiang", ""], ["Han", "Jiqing", ""], ["Zheng", "Tieran", ""], ["Deng", "Shiwen", ""]]}, {"id": "1112.4258", "submitter": "Mahdi Soltanolkotabi", "authors": "Mahdi Soltanolkotabi, Emmanuel J. Cand\\'es", "title": "A geometric analysis of subspace clustering with outliers", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1034 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2195-2238", "doi": "10.1214/12-AOS1034", "report-no": "IMS-AOS-AOS1034", "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of clustering a collection of unlabeled data\npoints assumed to lie near a union of lower-dimensional planes. As is common in\ncomputer vision or unsupervised learning applications, we do not know in\nadvance how many subspaces there are nor do we have any information about their\ndimensions. We develop a novel geometric analysis of an algorithm named sparse\nsubspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern\nRecognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantly\nbroadens the range of problems where it is provably effective. For instance, we\nshow that SSC can recover multiple subspaces, each of dimension comparable to\nthe ambient dimension. We also prove that SSC can correctly cluster data points\neven when the subspaces of interest intersect. Further, we develop an extension\nof SSC that succeeds when the data set is corrupted with possibly\noverwhelmingly many outliers. Underlying our analysis are clear geometric\ninsights, which may bear on other sparse recovery problems. A numerical study\ncomplements our theoretical analysis and demonstrates the effectiveness of\nthese methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 07:42:21 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 19:01:38 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2012 20:30:16 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2012 14:12:49 GMT"}, {"version": "v5", "created": "Wed, 30 Jan 2013 14:20:53 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""], ["Cand\u00e9s", "Emmanuel J.", ""]]}, {"id": "1112.4261", "submitter": "Chandrasekhar Sekhar t", "authors": "T.Chandrasekhar, K.Thangavel and E.Elayaraja", "title": "Performance Analysis of Enhanced Clustering Algorithm for Gene\n  Expression Data", "comments": "ISSN (Online): 1694-0814 http://www.IJCSI.org", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 3, November 2011", "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Microarrays are made it possible to simultaneously monitor the expression\nprofiles of thousands of genes under various experimental conditions. It is\nused to identify the co-expressed genes in specific cells or tissues that are\nactively used to make proteins. This method is used to analysis the gene\nexpression, an important task in bioinformatics research. Cluster analysis of\ngene expression data has proved to be a useful tool for identifying\nco-expressed genes, biologically relevant groupings of genes and samples. In\nthis paper we applied K-Means with Automatic Generations of Merge Factor for\nISODATA- AGMFI. Though AGMFI has been applied for clustering of Gene Expression\nData, this proposed Enhanced Automatic Generations of Merge Factor for ISODATA-\nEAGMFI Algorithms overcome the drawbacks of AGMFI in terms of specifying the\noptimal number of clusters and initialization of good cluster centroids.\nExperimental results on Gene Expression Data show that the proposed EAGMFI\nalgorithms could identify compact clusters with perform well in terms of the\nSilhouette Coefficients cluster measure.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 08:16:13 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Elayaraja", "E.", ""]]}, {"id": "1112.4344", "submitter": "Giovanni Zappella", "authors": "Giovanni Zappella", "title": "A Scalable Multiclass Algorithm for Node Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a scalable algorithm, MUCCA, for multiclass node classification\nin weighted graphs. Unlike previously proposed methods for the same task, MUCCA\nworks in time linear in the number of nodes. Our approach is based on a\ngame-theoretic formulation of the problem in which the test labels are\nexpressed as a Nash Equilibrium of a certain game. However, in order to achieve\nscalability, we find the equilibrium on a spanning tree of the original graph.\nExperiments on real-world data reveal that MUCCA is much faster than its\ncompetitors while achieving a similar predictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 14:21:00 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Zappella", "Giovanni", ""]]}, {"id": "1112.4394", "submitter": "David Duvenaud", "authors": "David Duvenaud, Hannes Nickisch, Carl Edward Rasmussen", "title": "Additive Gaussian Processes", "comments": "Appearing in Neural Information Processing Systems 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce a Gaussian process model of functions which are additive. An\nadditive function is one which decomposes into a sum of low-dimensional\nfunctions, each depending on only a subset of the input variables. Additive GPs\ngeneralize both Generalized Additive Models, and the standard GP models which\nuse squared-exponential kernels. Hyperparameter learning in this model can be\nseen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive\nbut tractable parameterization of the kernel function, which allows efficient\nevaluation of all input interaction terms, whose number is exponential in the\ninput dimension. The additional structure discoverable by this model results in\nincreased interpretability, as well as state-of-the-art predictive power in\nregression tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 16:22:09 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Duvenaud", "David", ""], ["Nickisch", "Hannes", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1112.4607", "submitter": "Arash Afkanpour", "authors": "Arash Afkanpour and Csaba Szepesvari and Michael Bowling", "title": "Alignment Based Kernel Learning with a Continuous Set of Base Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of kernel-based learning methods depend on the choice of kernel.\nRecently, kernel learning methods have been proposed that use data to select\nthe most appropriate kernel, usually by combining a set of base kernels. We\nintroduce a new algorithm for kernel learning that combines a {\\em continuous\nset of base kernels}, without the common step of discretizing the space of base\nkernels. We demonstrate that our new method achieves state-of-the-art\nperformance across a variety of real-world datasets. Furthermore, we explicitly\ndemonstrate the importance of combining the right dictionary of kernels, which\nis problematic for methods based on a finite set of base kernels chosen a\npriori. Our method is not the first approach to work with continuously\nparameterized kernels. However, we show that our method requires substantially\nless computation than previous such approaches, and so is more amenable to\nmultiple dimensional parameterizations of base kernels, which we demonstrate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 08:52:56 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["Afkanpour", "Arash", ""], ["Szepesvari", "Csaba", ""], ["Bowling", "Michael", ""]]}, {"id": "1112.4628", "submitter": "Habib Shah", "authors": "Habib Shah, Rozaida Ghazali, and Nazri Mohd Nawi", "title": "Using Artificial Bee Colony Algorithm for MLP Training on Earthquake\n  Time Series Data Prediction", "comments": "8 pages,8 figures;\n  http://www.journalofcomputing.org/volume-3-issue-6-june-2011", "journal-ref": "Journal of Computing, 3, 6 (2011), 135-142", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, computer scientists have shown the interest in the study of social\ninsect's behaviour in neural networks area for solving different combinatorial\nand statistical problems. Chief among these is the Artificial Bee Colony (ABC)\nalgorithm. This paper investigates the use of ABC algorithm that simulates the\nintelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron\n(MLP) trained with the standard back propagation algorithm normally utilises\ncomputationally intensive training algorithms. One of the crucial problems with\nthe backpropagation (BP) algorithm is that it can sometimes yield the networks\nwith suboptimal weights because of the presence of many local optima in the\nsolution space. To overcome ABC algorithm used in this work to train MLP\nlearning the complex behaviour of earthquake time series data trained by BP,\nthe performance of MLP-ABC is benchmarked against MLP training with the\nstandard BP. The experimental result shows that MLP-ABC performance is better\nthan MLP-BP for time series data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 09:50:53 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Shah", "Habib", ""], ["Ghazali", "Rozaida", ""], ["Nawi", "Nazri Mohd", ""]]}, {"id": "1112.4722", "submitter": "Steffen Gr\\\"unew\\\"alder", "authors": "Steffen Gr\\\"unew\\\"alder, Luca Baldassarre, Massimiliano Pontil, Arthur\n  Gretton, Guy Lever", "title": "Modeling transition dynamics in MDPs with RKHS embeddings of conditional\n  distributions", "comments": "The article can now be found under arXiv:1206.4655. We combined both\n  versions and are withdrawing this version because of the resulting redundancy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, nonparametric approach to estimating the value function in\nreinforcement learning. This approach makes use of a recently developed\nrepresentation of conditional distributions as functions in a reproducing\nkernel Hilbert space. Such representations bypass the need for estimating\ntransition probabilities, and apply to any domain on which kernels can be\ndefined. Our approach avoids the need to approximate intractable integrals\nsince expectations are represented as RKHS inner products whose computation has\nlinear complexity in the sample size. Thus, we can efficiently perform value\nfunction estimation in a wide variety of settings, including finite state\nspaces, continuous states spaces, and partially observable tasks where only\nsensor measurements are available. A second advantage of the approach is that\nwe learn the conditional distribution representation from a training sample,\nand do not require an exhaustive exploration of the state space. We prove\nconvergence of our approach either to the optimal policy, or to the closest\nprojection of the optimal policy in our model class, under reasonable\nassumptions. In experiments, we demonstrate the performance of our algorithm on\na learning task in a continuous state space (the under-actuated pendulum), and\non a navigation problem where only images from a sensor are observed. We\ncompare with least-squares policy iteration where a Gaussian process is used\nfor value function estimation. Our algorithm achieves better performance in\nboth tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 15:21:26 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2012 15:27:25 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Gr\u00fcnew\u00e4lder", "Steffen", ""], ["Baldassarre", "Luca", ""], ["Pontil", "Massimiliano", ""], ["Gretton", "Arthur", ""], ["Lever", "Guy", ""]]}, {"id": "1112.5246", "submitter": "Eitan Menahem", "authors": "Eitan Menahem, Lior Rokach and Yuval Elovici", "title": "Combining One-Class Classifiers via Meta-Learning", "comments": "To appear in CIKM 2013. Related to both Ensemble learning and\n  one-class learning. Length: 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the best classifier among the available ones is a difficult task,\nespecially when only instances of one class exist. In this work we examine the\nnotion of combining one-class classifiers as an alternative for selecting the\nbest classifier. In particular, we propose two new one-class classification\nperformance measures to weigh classifiers and show that a simple ensemble that\nimplements these measures can outperform the most popular one-class ensembles.\nFurthermore, we propose a new one-class ensemble scheme, TUPSO, which uses\nmeta-learning to combine one-class classifiers. Our experiments demonstrate the\nsuperiority of TUPSO over all other tested ensembles and show that the TUPSO\nperformance is statistically indistinguishable from that of the hypothetical\nbest classifier.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 08:07:56 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2013 14:27:14 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2013 12:08:43 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Menahem", "Eitan", ""], ["Rokach", "Lior", ""], ["Elovici", "Yuval", ""]]}, {"id": "1112.5309", "submitter": "Juergen Schmidhuber", "authors": "J\\\"urgen Schmidhuber", "title": "POWERPLAY: Training an Increasingly General Problem Solver by\n  Continually Searching for the Simplest Still Unsolvable Problem", "comments": "21 pages, additional connections to previous work, references to\n  first experiments with POWERPLAY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of computer science focuses on automatically solving given computational\nproblems. I focus on automatically inventing or discovering problems in a way\ninspired by the playful behavior of animals and humans, to train a more and\nmore general problem solver from scratch in an unsupervised fashion. Consider\nthe infinite set of all computable descriptions of tasks with possibly\ncomputable solutions. The novel algorithmic framework POWERPLAY (2011)\ncontinually searches the space of possible pairs of new tasks and modifications\nof the current problem solver, until it finds a more powerful problem solver\nthat provably solves all previously learned tasks plus the new one, while the\nunmodified predecessor does not. Wow-effects are achieved by continually making\npreviously learned skills more efficient such that they require less time and\nspace. New skills may (partially) re-use previously learned skills. POWERPLAY's\nsearch orders candidate pairs of tasks and solver modifications by their\nconditional computational (time & space) complexity, given the stored\nexperience so far. The new task and its corresponding task-solving skill are\nthose first found and validated. The computational costs of validating new\ntasks need not grow with task repertoire size. POWERPLAY's ongoing search for\nnovelty keeps breaking the generalization abilities of its present solver. This\nis related to Goedel's sequence of increasingly powerful formal theories based\non adding formerly unprovable statements to the axioms without affecting\npreviously provable theorems. The continually increasing repertoire of problem\nsolving procedures can be exploited by a parallel search for solutions to\nadditional externally posed tasks. POWERPLAY may be viewed as a greedy but\npractical implementation of basic principles of creativity. A first\nexperimental analysis can be found in separate papers [53,54].\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 13:50:46 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2012 17:22:46 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1112.5404", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Prateek Jain", "title": "Similarity-based Learning via Data Driven Embeddings", "comments": "To appear in the proceedings of NIPS 2011, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification using similarity/distance functions\nover data. Specifically, we propose a framework for defining the goodness of a\n(dis)similarity function with respect to a given learning task and propose\nalgorithms that have guaranteed generalization properties when working with\nsuch good functions. Our framework unifies and generalizes the frameworks\nproposed by [Balcan-Blum ICML 2006] and [Wang et al ICML 2007]. An attractive\nfeature of our framework is its adaptability to data - we do not promote a\nfixed notion of goodness but rather let data dictate it. We show, by giving\ntheoretical guarantees that the goodness criterion best suited to a problem can\nitself be learned which makes our approach applicable to a variety of domains\nand problems. We propose a landmarking-based approach to obtaining a classifier\nfrom such learned goodness criteria. We then provide a novel diversity based\nheuristic to perform task-driven selection of landmark points instead of random\nselection. We demonstrate the effectiveness of our goodness criteria learning\nmethod as well as the landmark selection heuristic on a variety of\nsimilarity-based learning datasets and benchmark UCI datasets on which our\nmethod consistently outperforms existing approaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 18:08:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1112.5441", "submitter": "John Snyder", "authors": "John C. Snyder, Matthias Rupp, Katja Hansen, Klaus-Robert M\\\"uller,\n  and Kieron Burke", "title": "Finding Density Functionals with Machine Learning", "comments": "4 pages, 4 figures, 1 table. The Supplemental Material is included at\n  the end of the manuscript (2 pages, 3 tables)", "journal-ref": null, "doi": "10.1103/PhysRevLett.108.253002", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is used to approximate density functionals. For the model\nproblem of the kinetic energy of non-interacting fermions in 1d, mean absolute\nerrors below 1 kcal/mol on test densities similar to the training set are\nreached with fewer than 100 training densities. A predictor identifies if a\ntest density is within the interpolation region. Via principal component\nanalysis, a projected functional derivative finds highly accurate\nself-consistent densities. Challenges for application of our method to real\nelectronic structure problems are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 20:29:32 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Snyder", "John C.", ""], ["Rupp", "Matthias", ""], ["Hansen", "Katja", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1112.5505", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji", "title": "A Study on Using Uncertain Time Series Matching Algorithms in MapReduce\n  Applications", "comments": "12 pages a version has been accepted to journal of \"Concurrency and\n  Computation: Practice and Experience\", available online from the University\n  of Sydney at http://www.nicta.com.au/pub?doc=4744", "journal-ref": null, "doi": null, "report-no": "TR672- University of Sydney", "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study CPU utilization time patterns of several Map-Reduce\napplications. After extracting running patterns of several applications, the\npatterns with their statistical information are saved in a reference database\nto be later used to tweak system parameters to efficiently execute unknown\napplications in future. To achieve this goal, CPU utilization patterns of new\napplications along with its statistical information are compared with the\nalready known ones in the reference database to find/predict their most\nprobable execution patterns. Because of different patterns lengths, the Dynamic\nTime Warping (DTW) is utilized for such comparison; a statistical analysis is\nthen applied to DTWs' outcomes to select the most suitable candidates.\nMoreover, under a hypothesis, another algorithm is proposed to classify\napplications under similar CPU utilization patterns. Three widely used text\nprocessing applications (WordCount, Distributed Grep, and Terasort) and another\napplication (Exim Mainlog parsing) are used to evaluate our hypothesis in\ntweaking system parameters in executing similar applications. Results were very\npromising and showed effectiveness of our approach on 5-node Map-Reduce\nplatform\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 02:38:42 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2012 00:30:11 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2012 00:28:52 GMT"}, {"version": "v4", "created": "Wed, 13 Jun 2012 03:33:54 GMT"}, {"version": "v5", "created": "Fri, 18 Jan 2013 03:54:34 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Zomaya", "Albert Y.", ""], ["Moraveji", "Reza", ""]]}, {"id": "1112.5627", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Alessandro Rinaldo, Don Sheehy, Aarti Singh,\n  Larry Wasserman", "title": "Minimax Rates for Homology Inference", "comments": "16 pages, 4 figures. Artificial Intelligence and Statistics, AISTATS\n  2012, Accepted as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, high dimensional data lie close to a low-dimensional submanifold and\nit is of interest to understand the geometry of these submanifolds. The\nhomology groups of a manifold are important topological invariants that provide\nan algebraic summary of the manifold. These groups contain rich topological\ninformation, for instance, about the connected components, holes, tunnels and\nsometimes the dimension of the manifold. In this paper, we consider the\nstatistical problem of estimating the homology of a manifold from noisy samples\nunder several different noise models. We derive upper and lower bounds on the\nminimax risk for this problem. Our upper bounds are based on estimators which\nare constructed from a union of balls of appropriate radius around carefully\nselected points. In each case we establish complementary lower bounds using Le\nCam's lemma.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 18:12:33 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Sheehy", "Don", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1112.5629", "submitter": "Brian Eriksson", "authors": "Brian Eriksson and Laura Balzano and Robert Nowak", "title": "High-Rank Matrix Completion and Subspace Clustering with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of completing a matrix with many missing\nentries under the assumption that the columns of the matrix belong to a union\nof multiple low-rank subspaces. This generalizes the standard low-rank matrix\ncompletion problem to situations in which the matrix rank can be quite high or\neven full rank. Since the columns belong to a union of subspaces, this problem\nmay also be viewed as a missing-data version of the subspace clustering\nproblem. Let X be an n x N matrix whose (complete) columns lie in a union of at\nmost k subspaces, each of rank <= r < n, and assume N >> kn. The main result of\nthe paper shows that under mild assumptions each column of X can be perfectly\nrecovered with high probability from an incomplete version so long as at least\nCrNlog^2(n) entries of X are observed uniformly at random, with C>1 a constant\ndepending on the usual incoherence conditions, the geometrical arrangement of\nsubspaces, and the distribution of columns over the subspaces. The result is\nillustrated with numerical experiments and an application to Internet distance\nmatrix completion and topology identification.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 18:25:17 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2011 15:22:13 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Eriksson", "Brian", ""], ["Balzano", "Laura", ""], ["Nowak", "Robert", ""]]}, {"id": "1112.5745", "submitter": "Neil Houlsby", "authors": "Neil Houlsby, Ferenc Husz\\'ar, Zoubin Ghahramani, M\\'at\\'e Lengyel", "title": "Bayesian Active Learning for Classification and Preference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic active learning has been widely studied for\nprobabilistic models. For simple regression an optimal myopic policy is easily\ntractable. However, for other tasks and with more complex models, such as\nclassification with nonparametric models, the optimal solution is harder to\ncompute. Current approaches make approximations to achieve tractability. We\npropose an approach that expresses information gain in terms of predictive\nentropies, and apply this method to the Gaussian Process Classifier (GPC). Our\napproach makes minimal approximations to the full information theoretic\nobjective. Our experimental performance compares favourably to many popular\nactive learning algorithms, and has equal or lower computational complexity. We\ncompare well to decision theoretic approaches also, which are privy to more\ninformation and require much more computational time. Secondly, by developing\nfurther a reformulation of binary preference learning to a classification\nproblem, we extend our algorithm to Gaussian Process preference learning.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2011 17:53:19 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Houlsby", "Neil", ""], ["Husz\u00e1r", "Ferenc", ""], ["Ghahramani", "Zoubin", ""], ["Lengyel", "M\u00e1t\u00e9", ""]]}, {"id": "1112.6209", "submitter": "Quoc Le", "authors": "Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai\n  Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng", "title": "Building high-level features using large scale unsupervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of building high-level, class-specific feature\ndetectors from only unlabeled data. For example, is it possible to learn a face\ndetector using only unlabeled images? To answer this, we train a 9-layered\nlocally connected sparse autoencoder with pooling and local contrast\nnormalization on a large dataset of images (the model has 1 billion\nconnections, the dataset has 10 million 200x200 pixel images downloaded from\nthe Internet). We train this network using model parallelism and asynchronous\nSGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to\nwhat appears to be a widely-held intuition, our experimental results reveal\nthat it is possible to train a face detector without having to label images as\ncontaining a face or not. Control experiments show that this feature detector\nis robust not only to translation but also to scaling and out-of-plane\nrotation. We also find that the same network is sensitive to other high-level\nconcepts such as cat faces and human bodies. Starting with these learned\nfeatures, we trained our network to obtain 15.8% accuracy in recognizing 20,000\nobject categories from ImageNet, a leap of 70% relative improvement over the\nprevious state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 00:26:54 GMT"}, {"version": "v2", "created": "Tue, 22 May 2012 08:12:49 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2012 05:12:56 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2012 04:40:33 GMT"}, {"version": "v5", "created": "Thu, 12 Jul 2012 04:32:50 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Le", "Quoc V.", ""], ["Ranzato", "Marc'Aurelio", ""], ["Monga", "Rajat", ""], ["Devin", "Matthieu", ""], ["Chen", "Kai", ""], ["Corrado", "Greg S.", ""], ["Dean", "Jeff", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1112.6234", "submitter": "Weiyu Xu", "authors": "Weiyu Xu, Meng Wang, Jianfeng Cai and Ao Tang", "title": "Sparse Recovery from Nonlinear Measurements with Applications in Bad\n  Data Detection for Power Networks", "comments": "journal. arXiv admin note: substantial text overlap with\n  arXiv:1105.0442", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of sparse recovery from nonlinear\nmeasurements, which has applications in state estimation and bad data detection\nfor power networks. An iterative mixed $\\ell_1$ and $\\ell_2$ convex program is\nused to estimate the true state by locally linearizing the nonlinear\nmeasurements. When the measurements are linear, through using the almost\nEuclidean property for a linear subspace, we derive a new performance bound for\nthe state estimation error under sparse bad data and additive observation\nnoise. As a byproduct, in this paper we provide sharp bounds on the almost\nEuclidean property of a linear subspace, using the \"escape-through-the-mesh\"\ntheorem from geometric functional analysis. When the measurements are\nnonlinear, we give conditions under which the solution of the iterative\nalgorithm converges to the true state even though the locally linearized\nmeasurements may not be the actual nonlinear measurements. We numerically\nevaluate our iterative convex programming approach to perform bad data\ndetections in nonlinear electrical power networks problems. We are able to use\nsemidefinite programming to verify the conditions for convergence of the\nproposed iterative sparse recovery algorithms from nonlinear measurements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 06:07:43 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2013 10:41:17 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Xu", "Weiyu", ""], ["Wang", "Meng", ""], ["Cai", "Jianfeng", ""], ["Tang", "Ao", ""]]}, {"id": "1112.6399", "submitter": "Byron Boots", "authors": "Byron Boots and Geoffrey J. Gordon", "title": "Two-Manifold Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been much interest in spectral approaches to learning\nmanifolds---so-called kernel eigenmap methods. These methods have had some\nsuccesses, but their applicability is limited because they are not robust to\nnoise. To address this limitation, we look at two-manifold problems, in which\nwe simultaneously reconstruct two related manifolds, each representing a\ndifferent view of the same data. By solving these interconnected learning\nproblems together and allowing information to flow between them, two-manifold\nalgorithms are able to succeed where a non-integrated approach would fail: each\nview allows us to suppress noise in the other, reducing bias in the same way\nthat an instrumental variable allows us to remove bias in a {linear}\ndimensionality reduction problem. We propose a class of algorithms for\ntwo-manifold problems, based on spectral decomposition of cross-covariance\noperators in Hilbert space. Finally, we discuss situations where two-manifold\nproblems are useful, and demonstrate that solving a two-manifold problem can\naid in learning a nonlinear dynamical system from limited data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 19:52:14 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Boots", "Byron", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1112.6411", "submitter": "Ali Jalali", "authors": "Christopher C. Johnson, Ali Jalali and Pradeep Ravikumar", "title": "High-dimensional Sparse Inverse Covariance Estimation using Greedy\n  Methods", "comments": "Accepted to AI STAT 2012 for Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the task of estimating the non-zero pattern of the\nsparse inverse covariance matrix of a zero-mean Gaussian random vector from a\nset of iid samples. Note that this is also equivalent to recovering the\nunderlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We\npresent two novel greedy approaches to solving this problem. The first\nestimates the non-zero covariates of the overall inverse covariance matrix\nusing a series of global forward and backward greedy steps. The second\nestimates the neighborhood of each node in the graph separately, again using\ngreedy forward and backward steps, and combines the intermediate neighborhoods\nto form an overall estimate. The principal contribution of this paper is a\nrigorous analysis of the sparsistency, or consistency in recovering the\nsparsity pattern of the inverse covariance matrix. Surprisingly, we show that\nboth the local and global greedy methods learn the full structure of the model\nwith high probability given just $O(d\\log(p))$ samples, which is a\n\\emph{significant} improvement over state of the art $\\ell_1$-regularized\nGaussian MLE (Graphical Lasso) that requires $O(d^2\\log(p))$ samples. Moreover,\nthe restricted eigenvalue and smoothness conditions imposed by our greedy\nmethods are much weaker than the strong irrepresentable conditions required by\nthe $\\ell_1$-regularization based methods. We corroborate our results with\nextensive simulations and examples, comparing our local and global greedy\nmethods to the $\\ell_1$-regularized Gaussian MLE as well as the Neighborhood\nGreedy method to that of nodewise $\\ell_1$-regularized linear regression\n(Neighborhood Lasso).\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 20:35:40 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Johnson", "Christopher C.", ""], ["Jalali", "Ali", ""], ["Ravikumar", "Pradeep", ""]]}]