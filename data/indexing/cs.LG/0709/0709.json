[{"id": "0709.0509", "submitter": "Enrique ter Horst Dr", "authors": "Henryk Gzyl and Enrique ter Horst", "title": "Filtering Additive Measurement Noise with Maximum Entropy in the Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GL", "license": null, "abstract": "  The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2007 19:36:22 GMT"}], "update_date": "2007-09-05", "authors_parsed": [["Gzyl", "Henryk", ""], ["ter Horst", "Enrique", ""]]}, {"id": "0709.1516", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "On Universal Prediction and Bayesian Confirmation", "comments": "24 pages", "journal-ref": "Theoretical Computer Science, 384 (2007) pages 33-48", "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": null, "abstract": "  The Bayesian framework is a well-studied and successful framework for\ninductive reasoning, which includes hypothesis testing and confirmation,\nparameter estimation, sequence prediction, classification, and regression. But\nstandard statistical guidelines for choosing the model class and prior are not\nalways available or fail, in particular in complex situations. Solomonoff\ncompleted the Bayesian framework by providing a rigorous, unique, formal, and\nuniversal choice for the model class and the prior. We discuss in breadth how\nand in which sense universal (non-i.i.d.) sequence prediction solves various\n(philosophical) problems of traditional Bayesian sequence prediction. We show\nthat Solomonoff's model possesses many desirable properties: Strong total and\nweak instantaneous bounds, and in contrast to most classical continuous prior\ndensities has no zero p(oste)rior problem, i.e. can confirm universal\nhypotheses, is reparametrization and regrouping invariant, and avoids the\nold-evidence and updating problem. It even performs well (actually better) in\nnon-computable environments.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2007 01:39:20 GMT"}], "update_date": "2008-06-26", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0709.2446", "submitter": "Fangwen Fu", "authors": "Fangwen Fu, Mihaela van der Schaar", "title": "Learning for Dynamic Bidding in Cognitive Radio Resources", "comments": "29pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": null, "abstract": "  In this paper, we model the various wireless users in a cognitive radio\nnetwork as a collection of selfish, autonomous agents that strategically\ninteract in order to acquire the dynamically available spectrum opportunities.\nOur main focus is on developing solutions for wireless users to successfully\ncompete with each other for the limited and time-varying spectrum\nopportunities, given the experienced dynamics in the wireless network. We\ncategorize these dynamics into two types: one is the disturbance due to the\nenvironment (e.g. wireless channel conditions, source traffic characteristics,\netc.) and the other is the impact caused by competing users. To analyze the\ninteractions among users given the environment disturbance, we propose a\ngeneral stochastic framework for modeling how the competition among users for\nspectrum opportunities evolves over time. At each stage of the dynamic resource\nallocation, a central spectrum moderator auctions the available resources and\nthe users strategically bid for the required resources. The joint bid actions\naffect the resource allocation and hence, the rewards and future strategies of\nall users. Based on the observed resource allocation and corresponding rewards\nfrom previous allocations, we propose a best response learning algorithm that\ncan be deployed by wireless users to improve their bidding policy at each\nstage. The simulation results show that by deploying the proposed best response\nlearning algorithm, the wireless users can significantly improve their own\nperformance in terms of both the packet loss rate and the incurred cost for the\nused resources.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2007 20:48:57 GMT"}], "update_date": "2007-09-18", "authors_parsed": [["Fu", "Fangwen", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "0709.3427", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Amaury\n  Lendasse (CIS), Damien Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Michel\n  Verleysen (DICE - MLG)", "title": "Mutual information for the selection of relevant variables in\n  spectrometric nonlinear modelling", "comments": null, "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 80, 2 (2006) 215-226", "doi": "10.1016/j.chemolab.2005.06.010", "report-no": null, "categories": "cs.LG cs.NE stat.AP", "license": null, "abstract": "  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2007 12:49:47 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Lendasse", "Amaury", "", "CIS"], ["Fran\u00e7ois", "Damien", "", "CESAME"], ["Wertz", "Vincent", "", "CESAME"], ["Verleysen", "Michel", "", "DICE - MLG"]]}, {"id": "0709.3461", "submitter": "Fabrice Rossi", "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)", "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps", "comments": null, "journal-ref": "Neural Networks 19, 6-7 (2006) 855-863", "doi": "10.1016/j.neunet.2006.05.002", "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2007 15:20:07 GMT"}], "update_date": "2007-09-24", "authors_parsed": [["Conan-Guez", "Brieuc", "", "LITA"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"], ["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"]]}, {"id": "0709.3586", "submitter": "Fabrice Rossi", "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (LITA), Yves Lechevallier (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)", "title": "Une adaptation des cartes auto-organisatrices pour des donn\\'ees\n  d\\'ecrites par un tableau de dissimilarit\\'es", "comments": null, "journal-ref": "Revue de Statistique Appliqu\\'ee LIV, 3 (2006) 33-64", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2007 15:53:54 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Conan-Guez", "Brieuc", "", "LITA"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"]]}, {"id": "0709.3587", "submitter": "Fabrice Rossi", "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis)", "title": "Self-organizing maps and symbolic data", "comments": null, "journal-ref": "Journal of Symbolic Data Analysis 2, 1 (2004)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": null, "abstract": "  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2007 15:54:37 GMT"}], "update_date": "2007-09-25", "authors_parsed": [["Golli", "A\u00efcha El", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Conan-Guez", "Brieuc", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"]]}, {"id": "0709.3639", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Damien\n  Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Marc Meurens (BNUT), Michel\n  Verleysen (DICE - MLG)", "title": "Fast Selection of Spectral Variables with B-Spline Compression", "comments": null, "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 86, 2 (2007) 208-218", "doi": "10.1016/j.chemolab.2006.06.007", "report-no": null, "categories": "cs.LG stat.AP", "license": null, "abstract": "  The large number of spectral variables in most data sets encountered in\nspectral chemometrics often renders the prediction of a dependent variable\nuneasy. The number of variables hopefully can be reduced, by using either\nprojection techniques or selection methods; the latter allow for the\ninterpretation of the selected variables. Since the optimal approach of testing\nall possible subsets of variables with the prediction model is intractable, an\nincremental selection approach using a nonparametric statistics is a good\noption, as it avoids the computationally intensive use of the model itself. It\nhas two drawbacks however: the number of groups of variables to test is still\nhuge, and colinearities can make the results unstable. To overcome these\nlimitations, this paper presents a method to select groups of spectral\nvariables. It consists in a forward-backward procedure applied to the\ncoefficients of a B-Spline representation of the spectra. The criterion used in\nthe forward-backward procedure is the mutual information, allowing to find\nnonlinear dependencies between variables, on the contrary of the generally used\ncorrelation. The spline representation is used to get interpretability of the\nresults, as groups of consecutive spectral variables will be selected. The\nexperiments conducted on NIR spectra from fescue grass and diesel fuels show\nthat the method provides clearly identified groups of selected variables,\nmaking interpretation easy, while keeping a low computational load. The\nprediction performances obtained using the selected coefficients are higher\nthan those obtained by the same method applied directly to the original\nvariables and similar to those obtained using traditional models, although\nusing significantly less spectral variables.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2007 14:08:51 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Fran\u00e7ois", "Damien", "", "CESAME"], ["Wertz", "Vincent", "", "CESAME"], ["Meurens", "Marc", "", "BNUT"], ["Verleysen", "Michel", "", "DICE - MLG"]]}, {"id": "0709.3640", "submitter": "Fabrice Rossi", "authors": "Damien Fran\\c{c}ois (CESAME), Fabrice Rossi (INRIA Rocquencourt /\n  INRIA Sophia Antipolis), Vincent Wertz (CESAME), Michel Verleysen (DICE -\n  MLG)", "title": "Resampling methods for parameter-free and robust feature selection with\n  mutual information", "comments": null, "journal-ref": "Neurocomputing 70, 7-9 (2007) 1276-1288", "doi": "10.1016/j.neucom.2006.11.019", "report-no": null, "categories": "cs.LG stat.AP", "license": null, "abstract": "  Combining the mutual information criterion with a forward feature selection\nstrategy offers a good trade-off between optimality of the selected feature\nsubset and computation time. However, it requires to set the parameter(s) of\nthe mutual information estimator and to determine when to halt the forward\nprocedure. These two choices are difficult to make because, as the\ndimensionality of the subset increases, the estimation of the mutual\ninformation becomes less and less reliable. This paper proposes to use\nresampling methods, a K-fold cross-validation and the permutation test, to\naddress both issues. The resampling methods bring information about the\nvariance of the estimator, information which can then be used to automatically\nset the parameter and to calculate a threshold to stop the forward procedure.\nThe procedure is illustrated on a synthetic dataset as well as on real-world\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2007 14:09:28 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Fran\u00e7ois", "Damien", "", "CESAME"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt /\n  INRIA Sophia Antipolis"], ["Wertz", "Vincent", "", "CESAME"], ["Verleysen", "Michel", "", "DICE -\n  MLG"]]}, {"id": "0709.3965", "submitter": "Tshilidzi Marwala", "authors": "Greg Hulley and Tshilidzi Marwala", "title": "Evolving Classifiers: Methods for Incremental Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": null, "abstract": "  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2007 14:28:32 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2007 10:37:00 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Hulley", "Greg", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "0709.3967", "submitter": "Tshilidzi Marwala", "authors": "Gidudu Anthony, Hulley Greg and Marwala Tshilidzi", "title": "Classification of Images Using Support Vector Machines", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": null, "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusions that\nultimately the choice of technique adopted boils down to personal preference\nand the uniqueness of the dataset at hand.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2007 14:37:40 GMT"}], "update_date": "2007-09-26", "authors_parsed": [["Anthony", "Gidudu", ""], ["Greg", "Hulley", ""], ["Tshilidzi", "Marwala", ""]]}]