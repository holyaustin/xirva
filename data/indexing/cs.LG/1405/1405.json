[{"id": "1405.0042", "submitter": "Silvia Villa", "authors": "Lorenzo Rosasco, Silvia Villa", "title": "Learning with incremental iterative regularization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a statistical learning setting, we propose and study an iterative\nregularization algorithm for least squares defined by an incremental gradient\nmethod. In particular, we show that, if all other parameters are fixed a\npriori, the number of passes over the data (epochs) acts as a regularization\nparameter, and prove strong universal consistency, i.e. almost sure convergence\nof the risk, as well as sharp finite sample bounds for the iterates. Our\nresults are a step towards understanding the effect of multiple epochs in\nstochastic gradient techniques in machine learning and rely on integrating\nstatistical and optimization results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 21:48:34 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 13:12:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""]]}, {"id": "1405.0099", "submitter": "Max Sklar", "authors": "Max Sklar", "title": "Fast MLE Computation for the Dirichlet Multinomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of categorical data, we want to find the parameters of a\nDirichlet distribution which maximizes the likelihood of that data. Newton's\nmethod is typically used for this purpose but current implementations require\nreading through the entire dataset on each iteration. In this paper, we propose\na modification which requires only a single pass through the dataset and\nsubstantially decreases running time. Furthermore we analyze both theoretically\nand empirically the performance of the proposed algorithm, and provide an open\nsource implementation.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 05:27:51 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Sklar", "Max", ""]]}, {"id": "1405.0133", "submitter": "Binbin Lin", "authors": "Binbin Lin, Ji Yang, Xiaofei He and Jieping Ye", "title": "Geodesic Distance Function Learning via Heat Flow on Vector Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a distance function or metric on a given data manifold is of great\nimportance in machine learning and pattern recognition. Many of the previous\nworks first embed the manifold to Euclidean space and then learn the distance\nfunction. However, such a scheme might not faithfully preserve the distance\nfunction if the original manifold is not Euclidean. Note that the distance\nfunction on a manifold can always be well-defined. In this paper, we propose to\nlearn the distance function directly on the manifold without embedding. We\nfirst provide a theoretical characterization of the distance function by its\ngradient field. Based on our theoretical analysis, we propose to first learn\nthe gradient field of the distance function and then learn the distance\nfunction itself. Specifically, we set the gradient field of a local distance\nfunction as an initial vector field. Then we transport it to the whole manifold\nvia heat flow on vector fields. Finally, the geodesic distance function can be\nobtained by requiring its gradient field to be close to the normalized vector\nfield. Experimental results on both synthetic and real data demonstrate the\neffectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 11:10:36 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 05:07:21 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Lin", "Binbin", ""], ["Yang", "Ji", ""], ["He", "Xiaofei", ""], ["Ye", "Jieping", ""]]}, {"id": "1405.0501", "submitter": "Mathias Niepert", "authors": "Mathias Niepert and Pedro Domingos", "title": "Exchangeable Variable Models", "comments": "ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequence of random variables is exchangeable if its joint distribution is\ninvariant under variable permutations. We introduce exchangeable variable\nmodels (EVMs) as a novel class of probabilistic models whose basic building\nblocks are partially exchangeable sequences, a generalization of exchangeable\nsequences. We prove that a family of tractable EVMs is optimal under zero-one\nloss for a large class of functions, including parity and threshold functions,\nand strictly subsumes existing tractable independence-based model families.\nExtensive experiments show that EVMs outperform state of the art classifiers\nsuch as SVMs and probabilistic models which are solely based on independence\nassumptions.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 20:13:06 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Niepert", "Mathias", ""], ["Domingos", "Pedro", ""]]}, {"id": "1405.0514", "submitter": "Ines Marusic", "authors": "Ines Marusic and James Worrell", "title": "Complexity of Equivalence and Learning for Multiplicity Tree Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of equivalence and learning for multiplicity tree\nautomata, i.e., weighted tree automata over a field. We first show that the\nequivalence problem is logspace equivalent to polynomial identity testing, the\ncomplexity of which is a longstanding open problem. Secondly, we derive lower\nbounds on the number of queries needed to learn multiplicity tree automata in\nAngluin's exact learning model, over both arbitrary and fixed fields.\n  Habrard and Oncina (2006) give an exact learning algorithm for multiplicity\ntree automata, in which the number of queries is proportional to the size of\nthe target automaton and the size of a largest counterexample, represented as a\ntree, that is returned by the Teacher. However, the smallest\ntree-counterexample may be exponential in the size of the target automaton.\nThus the above algorithm does not run in time polynomial in the size of the\ntarget automaton, and has query complexity exponential in the lower bound.\n  Assuming a Teacher that returns minimal DAG representations of\ncounterexamples, we give a new exact learning algorithm whose query complexity\nis quadratic in the target automaton size, almost matching the lower bound, and\nimproving the best previously-known algorithm by an exponential factor.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 20:58:39 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 18:59:45 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Marusic", "Ines", ""], ["Worrell", "James", ""]]}, {"id": "1405.0586", "submitter": "Ambuj Tewari", "authors": "Ambuj Tewari and Sougata Chaudhuri", "title": "On Lipschitz Continuity and Smoothness of Loss Functions in Learning to\n  Rank", "comments": "This paper has been withdrawn as it was superseded by an ICML 2015\n  paper \"Generalization error bounds for learning to rank: Does the length of\n  document lists matter?\" available as arXiv:1603.01860", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary classification and regression problems, it is well understood that\nLipschitz continuity and smoothness of the loss function play key roles in\ngoverning generalization error bounds for empirical risk minimization\nalgorithms. In this paper, we show how these two properties affect\ngeneralization error bounds in the learning to rank problem. The learning to\nrank problem involves vector valued predictions and therefore the choice of the\nnorm with respect to which Lipschitz continuity and smoothness are defined\nbecomes crucial. Choosing the $\\ell_\\infty$ norm in our definition of Lipschitz\ncontinuity allows us to improve existing bounds. Furthermore, under smoothness\nassumptions, our choice enables us to prove rates that interpolate between\n$1/\\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular\nlearning to rank method, gives state-of-the-art performance guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 13:36:59 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 14:53:40 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 18:06:14 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Tewari", "Ambuj", ""], ["Chaudhuri", "Sougata", ""]]}, {"id": "1405.0591", "submitter": "Ambuj  Tewari", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Perceptron-like Algorithms and Generalization Bounds for Learning to\n  Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is a supervised learning problem where the output space is\nthe space of rankings but the supervision space is the space of relevance\nscores. We make theoretical contributions to the learning to rank problem both\nin the online and batch settings. First, we propose a perceptron-like algorithm\nfor learning a ranking function in an online setting. Our algorithm is an\nextension of the classic perceptron algorithm for the classification problem.\nSecond, in the setting of batch learning, we introduce a sufficient condition\nfor convex ranking surrogates to ensure a generalization bound that is\nindependent of number of objects per query. Our bound holds when linear ranking\nfunctions are used: a common practice in many learning to rank algorithms. En\nroute to developing the online algorithm and generalization bound, we propose a\nnovel family of listwise large margin ranking surrogates. Our novel surrogate\nfamily is obtained by modifying a well-known pairwise large margin ranking\nsurrogate and is distinct from the listwise large margin surrogates developed\nusing the structured prediction framework. Using the proposed family, we\nprovide a guaranteed upper bound on the cumulative NDCG (or MAP) induced loss\nunder the perceptron-like algorithm. We also show that the novel surrogates\nsatisfy the generalization bound condition.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 14:38:47 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1405.0782", "submitter": "John Duchi", "authors": "John C. Duchi and Michael I. Jordan and Martin J. Wainwright and\n  Yuchen Zhang", "title": "Optimality guarantees for distributed statistical estimation", "comments": "34 pages, 1 figure. Preliminary version appearing in Neural\n  Information Processing Systems 2013\n  (http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large data sets often require performing distributed statistical estimation,\nwith a full data set split across multiple machines and limited communication\nbetween machines. To study such scenarios, we define and study some refinements\nof the classical minimax risk that apply to distributed settings, comparing to\nthe performance of estimators with access to the entire data. Lower bounds on\nthese quantities provide a precise characterization of the minimum amount of\ncommunication required to achieve the centralized minimax risk. We study two\nclasses of distributed protocols: one in which machines send messages\nindependently over channels without feedback, and a second allowing for\ninteractive communication, in which a central server broadcasts the messages\nfrom a given machine to all other machines. We establish lower bounds for a\nvariety of problems, including location estimation in several families and\nparameter estimation in different types of regression models. Our results\ninclude a novel class of quantitative data-processing inequalities used to\ncharacterize the effects of limited communication.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 05:23:30 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 01:08:17 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Duchi", "John C.", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""], ["Zhang", "Yuchen", ""]]}, {"id": "1405.0792", "submitter": "Nader Bshouty", "authors": "Hasan Abasi and Nader H. Bshouty and Hanna Mazzawi", "title": "On Exact Learning Monotone DNF from Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning a monotone DNF with at most\n$s$ terms of size (number of variables in each term) at most $r$ ($s$ term\n$r$-MDNF) from membership queries. This problem is equivalent to the problem of\nlearning a general hypergraph using hyperedge-detecting queries, a problem\nmotivated by applications arising in chemical reactions and genome sequencing.\n  We first present new lower bounds for this problem and then present\ndeterministic and randomized adaptive algorithms with query complexities that\nare almost optimal. All the algorithms we present in this paper run in time\nlinear in the query complexity and the number of variables $n$. In addition,\nall of the algorithms we present in this paper are asymptotically tight for\nfixed $r$ and/or $s$.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 06:49:05 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Abasi", "Hasan", ""], ["Bshouty", "Nader H.", ""], ["Mazzawi", "Hanna", ""]]}, {"id": "1405.0833", "submitter": "Alexander Zimin", "authors": "Alexander Zimin and Rasmus Ibsen-Jensen and Krishnendu Chatterjee", "title": "Generalized Risk-Aversion in Stochastic Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the regret in stochastic multi-armed\nbandit, when the measure of goodness of an arm is not the mean return, but some\ngeneral function of the mean and the variance.We characterize the conditions\nunder which learning is possible and present examples for which no natural\nalgorithm can achieve sublinear regret.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 09:29:17 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Zimin", "Alexander", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Chatterjee", "Krishnendu", ""]]}, {"id": "1405.0869", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "Robust Subspace Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Rare data in a large-scale database are called outliers that reveal\nsignificant information in the real world. The subspace-based outlier detection\nis regarded as a feasible approach in very high dimensional space. However, the\noutliers found in subspaces are only part of the true outliers in high\ndimensional space, indeed. The outliers hidden in normal-clustered points are\nsometimes neglected in the projected dimensional subspace. In this paper, we\npropose a robust subspace method for detecting such inner outliers in a given\ndataset, which uses two dimensional-projections: detecting outliers in\nsubspaces with local density ratio in the first projected dimensions; finding\noutliers by comparing neighbor's positions in the second projected dimensions.\nEach point's weight is calculated by summing up all related values got in the\ntwo steps projected dimensions, and then the points scoring the largest weight\nvalues are taken as outliers. By taking a series of experiments with the number\nof dimensions from 10 to 10000, the results show that our proposed method\nachieves high precision in the case of extremely high dimensional space, and\nworks well in low dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:01:24 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.1005", "submitter": "Mohammad Rastegari", "authors": "Mohammad Rastegari, Shobeir Fakhraei, Jonghyun Choi, David Jacobs,\n  Larry S. Davis", "title": "Comparing apples to apples in the evaluation of binary coding methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss methodological issues related to the evaluation of unsupervised\nbinary code construction methods for nearest neighbor search. These issues have\nbeen widely ignored in literature. These coding methods attempt to preserve\neither Euclidean distance or angular (cosine) distance in the binary embedding\nspace. We explain why when comparing a method whose goal is preserving cosine\nsimilarity to one designed for preserving Euclidean distance, the original\nfeatures should be normalized by mapping them to the unit hypersphere before\nlearning the binary mapping functions. To compare a method whose goal is to\npreserves Euclidean distance to one that preserves cosine similarity, the\noriginal feature data must be mapped to a higher dimension by including a bias\nterm in binary mapping functions. These conditions ensure the fair comparison\nbetween different binary code methods for the task of nearest neighbor search.\nOur experiments show under these conditions the very simple methods (e.g. LSH\nand ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and\nOK-means).\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 19:26:58 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 18:35:35 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Rastegari", "Mohammad", ""], ["Fakhraei", "Shobeir", ""], ["Choi", "Jonghyun", ""], ["Jacobs", "David", ""], ["Davis", "Larry S.", ""]]}, {"id": "1405.1027", "submitter": "Zhana Bao", "authors": "Zhana Bao", "title": "K-NS: Section-Based Outlier Detection in High Dimensional Space", "comments": "10 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1405.0869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Finding rare information hidden in a huge amount of data from the Internet is\na necessary but complex issue. Many researchers have studied this issue and\nhave found effective methods to detect anomaly data in low dimensional space.\nHowever, as the dimension increases, most of these existing methods perform\npoorly in detecting outliers because of \"high dimensional curse\". Even though\nsome approaches aim to solve this problem in high dimensional space, they can\nonly detect some anomaly data appearing in low dimensional space and cannot\ndetect all of anomaly data which appear differently in high dimensional space.\nTo cope with this problem, we propose a new k-nearest section-based method\n(k-NS) in a section-based space. Our proposed approach not only detects\noutliers in low dimensional space with section-density ratio but also detects\noutliers in high dimensional space with the ratio of k-nearest section against\naverage value. After taking a series of experiments with the dimension from 10\nto 10000, the experiment results show that our proposed method achieves 100%\nprecision and 100% recall result in the case of extremely high dimensional\nspace, and better improvement in low dimensional space compared to our\npreviously proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 12:06:06 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Bao", "Zhana", ""]]}, {"id": "1405.1119", "submitter": "Yishi Zhang", "authors": "Yishi Zhang, Chao Yang, Anrong Yang, Chan Xiong, Xingchi Zhou, Zigang\n  Zhang", "title": "Feature selection for classification with class-separability strategy\n  and data envelopment analysis", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel feature selection method is presented, which is based\non Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). To\nbetter capture the relationship between features and the class, class labels\nare separated into individual variables and relevance and redundancy are\nexplicitly handled on each class label. Super-efficiency DEA is employed to\nevaluate and rank features via their conditional dependence scores on all class\nlabels, and the feature with maximum super-efficiency score is then added in\nthe conditioning set for conditional dependence estimation in the next\niteration, in such a way as to iteratively select features and get the final\nselected features. Eventually, experiments are conducted to evaluate the\neffectiveness of proposed method comparing with four state-of-the-art methods\nfrom the viewpoint of classification accuracy. Empirical results verify the\nfeasibility and the superiority of proposed feature selection method.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 01:17:26 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 12:00:07 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zhang", "Yishi", ""], ["Yang", "Chao", ""], ["Yang", "Anrong", ""], ["Xiong", "Chan", ""], ["Zhou", "Xingchi", ""], ["Zhang", "Zigang", ""]]}, {"id": "1405.1297", "submitter": "Dong Huang", "authors": "Dong Huang and Jian-Huang Lai and Chang-Dong Wang", "title": "Combining Multiple Clusterings via Crowd Agreement Estimation and\n  Multi-Granularity Link Analysis", "comments": "The MATLAB source code of this work is available at:\n  https://www.researchgate.net/publication/281970316", "journal-ref": "Neurocomputing, 2015, vol.170, pp.240-250", "doi": "10.1016/j.neucom.2014.05.094", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering ensemble technique aims to combine multiple clusterings into a\nprobably better and more robust clustering and has been receiving an increasing\nattention in recent years. There are mainly two aspects of limitations in the\nexisting clustering ensemble approaches. Firstly, many approaches lack the\nability to weight the base clusterings without access to the original data and\ncan be affected significantly by the low-quality, or even ill clusterings.\nSecondly, they generally focus on the instance level or cluster level in the\nensemble system and fail to integrate multi-granularity cues into a unified\nmodel. To address these two limitations, this paper proposes to solve the\nclustering ensemble problem via crowd agreement estimation and\nmulti-granularity link analysis. We present the normalized crowd agreement\nindex (NCAI) to evaluate the quality of base clusterings in an unsupervised\nmanner and thus weight the base clusterings in accordance with their clustering\nvalidity. To explore the relationship between clusters, the source aware\nconnected triple (SACT) similarity is introduced with regard to their common\nneighbors and the source reliability. Based on NCAI and multi-granularity\ninformation collected among base clusterings, clusters, and data instances, we\nfurther propose two novel consensus functions, termed weighted evidence\naccumulation clustering (WEAC) and graph partitioning with multi-granularity\nlink analysis (GP-MGLA) respectively. The experiments are conducted on eight\nreal-world datasets. The experimental results demonstrate the effectiveness and\nrobustness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 15:05:02 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 16:10:19 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Huang", "Dong", ""], ["Lai", "Jian-Huang", ""], ["Wang", "Chang-Dong", ""]]}, {"id": "1405.1304", "submitter": "Sumaira Tasnim", "authors": "Akhlaqur Rahman and Sumaira Tasnim", "title": "Application of Machine Learning Techniques in Aquaculture", "comments": "2 pages", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V10(3):214-215 Apr 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V10P137", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present applications of different machine learning\nalgorithms in aquaculture. Machine learning algorithms learn models from\nhistorical data. In aquaculture historical data are obtained from farm\npractices, yields, and environmental data sources. Associations between these\ndifferent variables can be obtained by applying machine learning algorithms to\nhistorical data. In this paper we present applications of different machine\nlearning algorithms in aquaculture applications.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 14:26:42 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Rahman", "Akhlaqur", ""], ["Tasnim", "Sumaira", ""]]}, {"id": "1405.1380", "submitter": "Yingbo Zhou", "authors": "Yingbo Zhou, Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju", "title": "Is Joint Training Better for Deep Auto-Encoders?", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, when generative models of data are developed via deep\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\nmodel, the lower layer of the architecture models the data distribution\nconditional upon the hidden variables, while the higher layers model the hidden\ndistribution prior. But due to the greedy scheme of the layerwise training\ntechnique, the parameters of lower layers are fixed when training higher\nlayers. This makes it extremely challenging for the model to learn the hidden\ndistribution prior, which in turn leads to a suboptimal model for the data\ndistribution. We therefore investigate joint training of deep autoencoders,\nwhere the architecture is viewed as one stack of two or more single-layer\nautoencoders. A single global reconstruction objective is jointly optimized,\nsuch that the objective for the single autoencoders at each layer acts as a\nlocal, layer-level regularizer. We empirically evaluate the performance of this\njoint training scheme and observe that it not only learns a better data model,\nbut also learns better higher layer representations, which highlights its\npotential for unsupervised feature learning. In addition, we find that the\nusage of regularizations in the joint training scheme is crucial in achieving\ngood performance. In the supervised setting, joint training also shows superior\nperformance when training deeper models. The joint training framework can thus\nprovide a platform for investigating more efficient usage of different types of\nregularizers, especially in light of the growing volumes of available unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 17:41:33 GMT"}, {"version": "v2", "created": "Sat, 14 Jun 2014 15:48:53 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 18:02:06 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 23:52:59 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhou", "Yingbo", ""], ["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1405.1436", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Russell Greiner, Brendan Frey", "title": "Training Restricted Boltzmann Machine by Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to maximum likelihood learning of discrete graphical models\nand RBM in particular is introduced. Our method, Perturb and Descend (PD) is\ninspired by two ideas (I) perturb and MAP method for sampling (II) learning by\nContrastive Divergence minimization. In contrast to perturb and MAP, PD\nleverages training data to learn the models that do not allow efficient MAP\nestimation. During the learning, to produce a sample from the current model, we\nstart from a training data and descend in the energy landscape of the\n\"perturbed model\", for a fixed number of steps, or until a local optima is\nreached. For RBM, this involves linear calculations and thresholding which can\nbe very fast. Furthermore we show that the amount of perturbation is closely\nrelated to the temperature parameter and it can regularize the model by\nproducing robust features resulting in sparse hidden layer activation.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:02:46 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Greiner", "Russell", ""], ["Frey", "Brendan", ""]]}, {"id": "1405.1503", "submitter": "Andres Munoz", "authors": "Corinna Cortes and Mehryar Mohri and Andres Mu\\~noz Medina", "title": "Adaptation Algorithm and Theory Based on Generalized Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for domain adaptation improving upon a discrepancy\nminimization algorithm previously shown to outperform a number of algorithms\nfor this task. Unlike many previous algorithms for domain adaptation, our\nalgorithm does not consist of a fixed reweighting of the losses over the\ntraining sample. We show that our algorithm benefits from a solid theoretical\nfoundation and more favorable learning bounds than discrepancy minimization. We\npresent a detailed description of our algorithm and give several efficient\nsolutions for solving its optimization problem. We also report the results of\nseveral experiments showing that it outperforms discrepancy minimization.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 04:39:01 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 23:35:31 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2015 02:23:24 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Medina", "Andres Mu\u00f1oz", ""]]}, {"id": "1405.1513", "submitter": "Ibrahim Alabdulmohsin", "authors": "Ibrahim Alabdulmohsin", "title": "A Mathematical Theory of Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a mathematical theory of learning is proposed that has many\nparallels with information theory. We consider Vapnik's General Setting of\nLearning in which the learning process is defined to be the act of selecting a\nhypothesis in response to a given training set. Such hypothesis can, for\nexample, be a decision boundary in classification, a set of centroids in\nclustering, or a set of frequent item-sets in association rule mining.\nDepending on the hypothesis space and how the final hypothesis is selected, we\nshow that a learning process can be assigned a numeric score, called learning\ncapacity, which is analogous to Shannon's channel capacity and satisfies\nsimilar interesting properties as well such as the data-processing inequality\nand the information-cannot-hurt inequality. In addition, learning capacity\nprovides the tightest possible bound on the difference between true risk and\nempirical risk of the learning process for all loss functions that are\nparametrized by the chosen hypothesis. It is also shown that the notion of\nlearning capacity equivalently quantifies how sensitive the choice of the final\nhypothesis is to a small perturbation in the training set. Consequently,\nalgorithmic stability is both necessary and sufficient for generalization.\nWhile the theory does not rely on concentration inequalities, we finally show\nthat analogs to classical results in learning theory using the Probably\nApproximately Correct (PAC) model can be immediately deduced using this theory,\nand conclude with information-theoretic bounds to learning capacity.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 06:10:47 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""]]}, {"id": "1405.1533", "submitter": "Pierre Gaillard", "authors": "Pierre Gaillard (GREGH), Paul Baudin (INRIA Rocquencourt)", "title": "A consistent deterministic regression tree for non-parametric prediction\n  of time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online prediction of bounded stationary ergodic processes. To do so,\nwe consider the setting of prediction of individual sequences and build a\ndeterministic regression tree that performs asymptotically as well as the best\nL-Lipschitz constant predictors. Then, we show why the obtained regret bound\nentails the asymptotical optimality with respect to the class of bounded\nstationary ergodic processes.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 08:33:41 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 20:12:02 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Gaillard", "Pierre", "", "GREGH"], ["Baudin", "Paul", "", "INRIA Rocquencourt"]]}, {"id": "1405.1535", "submitter": "Nader Bshouty", "authors": "Hasan Abasi and Ali Z. Abdi and Nader H. Bshouty", "title": "Learning Boolean Halfspaces with Small Weights from Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of proper learning a Boolean Halfspace with integer\nweights $\\{0,1,\\ldots,t\\}$ from membership queries only. The best known\nalgorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$\nmembership queries where the best lower bound for the number of membership\nqueries is $n^t$ [Learning Threshold Functions with Small Weights Using\nMembership Queries. COLT 1999]\n  In this paper we close this gap and give an adaptive proper learning\nalgorithm with two rounds that asks $n^{O(t)}$ membership queries. We also give\na non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membership\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 09:06:28 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Abasi", "Hasan", ""], ["Abdi", "Ali Z.", ""], ["Bshouty", "Nader H.", ""]]}, {"id": "1405.1665", "submitter": "Tengyu Ma", "authors": "Ankit Garg and Tengyu Ma and Huy L. Nguyen", "title": "On Communication Cost of Distributed Statistical Estimation and\n  Dimensionality", "comments": "to appear at NIPS'14 with oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the connection between dimensionality and communication cost in\ndistributed learning problems. Specifically we study the problem of estimating\nthe mean $\\vec{\\theta}$ of an unknown $d$ dimensional gaussian distribution in\nthe distributed setting. In this problem, the samples from the unknown\ndistribution are distributed among $m$ different machines. The goal is to\nestimate the mean $\\vec{\\theta}$ at the optimal minimax rate while\ncommunicating as few bits as possible. We show that in this setting, the\ncommunication cost scales linearly in the number of dimensions i.e. one needs\nto deal with different dimensions individually. Applying this result to\nprevious lower bounds for one dimension in the interactive setting\n\\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we prove\nnew lower bounds of $\\Omega(md/\\log(m))$ and $\\Omega(md)$ for the bits of\ncommunication needed to achieve the minimax squared loss, in the interactive\nand simultaneous settings respectively. To complement, we also demonstrate an\ninteractive protocol achieving the minimax squared loss with $O(md)$ bits of\ncommunication, which improves upon the simple simultaneous protocol by a\nlogarithmic factor. Given the strong lower bounds in the general setting, we\ninitiate the study of the distributed parameter estimation problems with\nstructured parameters. Specifically, when the parameter is promised to be\n$s$-sparse, we show a simple thresholding based protocol that achieves the same\nsquared loss while saving a $d/s$ factor of communication. We conjecture that\nthe tradeoff between communication and squared loss demonstrated by this\nprotocol is essentially optimal up to logarithmic factor.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 16:44:21 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 03:06:04 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Garg", "Ankit", ""], ["Ma", "Tengyu", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1405.1966", "submitter": "Rajalakshmi  M", "authors": "M.Rajalakshmi and Dr. P.Subashini", "title": "Texture Based Image Segmentation of Chili Pepper X-Ray Images Using\n  Gabor Filter", "comments": "7 pages, 2 figures, 8 tables", "journal-ref": "IJASCSE, Volume 3, Issue 3, 2014, pg 44-51", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture segmentation is the process of partitioning an image into regions\nwith different textures containing a similar group of pixels. Detecting the\ndiscontinuity of the filter's output and their statistical properties help in\nsegmenting and classifying a given image with different texture regions. In\nthis proposed paper, chili x-ray image texture segmentation is performed by\nusing Gabor filter. The texture segmented result obtained from Gabor filter fed\ninto three texture filters, namely Entropy, Standard Deviation and Range\nfilter. After performing texture analysis, features can be extracted by using\nStatistical methods. In this paper Gray Level Co-occurrence Matrices and First\norder statistics are used as feature extraction methods. Features extracted\nfrom statistical methods are given to Support Vector Machine (SVM) classifier.\nUsing this methodology, it is found that texture segmentation is followed by\nthe Gray Level Co-occurrence Matrix feature extraction method gives a higher\naccuracy rate of 84% when compared with First order feature extraction method.\n  Key Words: Texture segmentation, Texture filter, Gabor filter, Feature\nextraction methods, SVM classifier.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 16:52:38 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Rajalakshmi", "M.", ""], ["Subashini", "Dr. P.", ""]]}, {"id": "1405.2102", "submitter": "Anna Ma", "authors": "Anna Ma, Arjuna Flenner, Deanna Needell, Allon G. Percus", "title": "Improving Image Clustering using Sparse Text and the Wisdom of the\n  Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to improve image clustering using sparse text and the\nwisdom of the crowds. In particular, we present a method to fuse two different\nkinds of document features, image and text features, and use a common\ndictionary or \"wisdom of the crowds\" as the connection between the two\ndifferent kinds of documents. With the proposed fusion matrix, we use topic\nmodeling via non-negative matrix factorization to cluster documents.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 21:29:04 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Ma", "Anna", ""], ["Flenner", "Arjuna", ""], ["Needell", "Deanna", ""], ["Percus", "Allon G.", ""]]}, {"id": "1405.2262", "submitter": "Michael S. Gashler Ph.D.", "authors": "Michael S. Gashler and Stephen C. Ashmore", "title": "Training Deep Fourier Neural Networks To Fit Time-Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training a deep neural network containing sinusoidal\nactivation functions to fit to time-series data. Weights are initialized using\na fast Fourier transform, then trained with regularization to improve\ngeneralization. A simple dynamic parameter tuning method is employed to adjust\nboth the learning rate and regularization term, such that stability and\nefficient training are both achieved. We show how deeper layers can be utilized\nto model the observed sequence using a sparser set of sinusoid units, and how\nnon-uniform regularization can improve generalization by promoting the shifting\nof weight toward simpler units. The method is demonstrated with time-series\nproblems to show that it leads to effective extrapolation of nonlinear trends.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 15:23:06 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Gashler", "Michael S.", ""], ["Ashmore", "Stephen C.", ""]]}, {"id": "1405.2278", "submitter": "Robert Lyon", "authors": "R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers", "title": "Hellinger Distance Trees for Imbalanced Streams", "comments": "6 Pages, 2 figures, to be published in Proceedings 22nd International\n  Conference on Pattern Recognition (ICPR) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers trained on data sets possessing an imbalanced class distribution\nare known to exhibit poor generalisation performance. This is known as the\nimbalanced learning problem. The problem becomes particularly acute when we\nconsider incremental classifiers operating on imbalanced data streams,\nespecially when the learning objective is rare class identification. As\naccuracy may provide a misleading impression of performance on imbalanced data,\nexisting stream classifiers based on accuracy can suffer poor minority class\nperformance on imbalanced streams, with the result being low minority class\nrecall rates. In this paper we address this deficiency by proposing the use of\nthe Hellinger distance measure, as a very fast decision tree split criterion.\nWe demonstrate that by using Hellinger a statistically significant improvement\nin recall rates on imbalanced data streams can be achieved, with an acceptable\nincrease in the false positive rate.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 16:14:47 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Lyon", "R. J.", ""], ["Brooke", "J. M.", ""], ["Knowles", "J. D.", ""], ["Stappers", "B. W.", ""]]}, {"id": "1405.2294", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Yingbin Liang, H. Vincent Poor, Xinghua Shi", "title": "Nonparametric Detection of Anomalous Data Streams", "comments": "Submitted to IEEE Transactions on Signal Processing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric anomalous hypothesis testing problem is investigated, in\nwhich there are totally n sequences with s anomalous sequences to be detected.\nEach typical sequence contains m independent and identically distributed\n(i.i.d.) samples drawn from a distribution p, whereas each anomalous sequence\ncontains m i.i.d. samples drawn from a distribution q that is distinct from p.\nThe distributions p and q are assumed to be unknown in advance.\nDistribution-free tests are constructed using maximum mean discrepancy as the\nmetric, which is based on mean embeddings of distributions into a reproducing\nkernel Hilbert space. The probability of error is bounded as a function of the\nsample size m, the number s of anomalous sequences and the number n of\nsequences. It is then shown that with s known, the constructed test is\nexponentially consistent if m is greater than a constant factor of log n, for\nany p and q, whereas with s unknown, m should has an order strictly greater\nthan log n. Furthermore, it is shown that no test can be consistent for\narbitrary p and q if m is less than a constant factor of log n, thus the\norder-level optimality of the proposed test is established. Numerical results\nare provided to demonstrate that our tests outperform (or perform as well as)\nthe tests based on other competitive approaches under various cases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 15:52:47 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 02:06:49 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Zou", "Shaofeng", ""], ["Liang", "Yingbin", ""], ["Poor", "H. Vincent", ""], ["Shi", "Xinghua", ""]]}, {"id": "1405.2377", "submitter": "James Brofos", "authors": "James Brofos", "title": "A Hybrid Monte Carlo Architecture for Parameter Optimization", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent research has been conducted in the area of Bayesian learning,\nparticularly with regard to the optimization of hyper-parameters via Gaussian\nprocess regression. The methodologies rely chiefly on the method of maximizing\nthe expected improvement of a score function with respect to adjustments in the\nhyper-parameters. In this work, we present a novel algorithm that exploits\nnotions of confidence intervals and uncertainties to enable the discovery of\nthe best optimal within a targeted region of the parameter space. We\ndemonstrate the efficacy of our algorithm with respect to machine learning\nproblems and show cases where our algorithm is competitive with the method of\nmaximizing expected improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 02:03:22 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Brofos", "James", ""]]}, {"id": "1405.2420", "submitter": "Amit Daniely", "authors": "Amit Daniely and Shai Shalev-Shwartz", "title": "Optimal Learners for Multiclass Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental theorem of statistical learning states that for binary\nclassification problems, any Empirical Risk Minimization (ERM) learning rule\nhas close to optimal sample complexity. In this paper we seek for a generic\noptimal learner for multiclass prediction. We start by proving a surprising\nresult: a generic optimal multiclass learner must be improper, namely, it must\nhave the ability to output hypotheses which do not belong to the hypothesis\nclass, even though it knows that all the labels are generated by some\nhypothesis from the class. In particular, no ERM learner is optimal. This\nbrings back the fundmamental question of \"how to learn\"? We give a complete\nanswer to this question by giving a new analysis of the one-inclusion\nmulticlass learner of Rubinstein et al (2006) showing that its sample\ncomplexity is essentially optimal. Then, we turn to study the popular\nhypothesis class of generalized linear classifiers. We derive optimal learners\nthat, unlike the one-inclusion algorithm, are computationally efficient.\nFurthermore, we show that the sample complexity of these learners is better\nthan the sample complexity of the ERM rule, thus settling in negative an open\nquestion due to Collins (2005).\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 11:23:08 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Daniely", "Amit", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1405.2432", "submitter": "Jia Yuan Yu", "authors": "Long Tran-Thanh and Jia Yuan Yu", "title": "Functional Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functional bandit problem, where the objective is to find an\narm that optimises a known functional of the unknown arm-reward distributions.\nThese problems arise in many settings such as maximum entropy methods in\nnatural language processing, and risk-averse decision-making, but current\nbest-arm identification techniques fail in these domains. We propose a new\napproach, that combines functional estimation and arm elimination, to tackle\nthis problem. This method achieves provably efficient performance guarantees.\nIn addition, we illustrate this method on a number of important functionals in\nrisk management and information theory, and refine our generic theoretical\nresults in those cases.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 13:34:22 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Tran-Thanh", "Long", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1405.2476", "submitter": "Achilles Beros", "authors": "Achilles Beros, Colin de la Higuera", "title": "A Canonical Semi-Deterministic Transducer", "comments": "A shorter version has been published in the proceedings of ICGI 2014.\n  This version will appear in an issue of Fundamenta Informaticae", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the existence of a canonical form for semi-deterministic transducers\nwith incomparable sets of output strings. Based on this, we develop an\nalgorithm which learns semi-deterministic transducers given access to\ntranslation queries. We also prove that there is no learning algorithm for\nsemi-deterministic transducers that uses only domain knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 22:30:38 GMT"}, {"version": "v2", "created": "Mon, 19 May 2014 08:47:45 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2015 16:45:05 GMT"}, {"version": "v4", "created": "Mon, 10 Oct 2016 20:56:01 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Beros", "Achilles", ""], ["de la Higuera", "Colin", ""]]}, {"id": "1405.2600", "submitter": "Yuyi Wang", "authors": "Yuyi Wang and Jan Ramon and Zheng-Chu Guo", "title": "Learning from networked examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are based on the assumption that training\nexamples are drawn independently. However, this assumption does not hold\nanymore when learning from a networked sample because two or more training\nexamples may share some common objects, and hence share the features of these\nshared objects. We show that the classic approach of ignoring this problem\npotentially can have a harmful effect on the accuracy of statistics, and then\nconsider alternatives. One of these is to only use independent examples,\ndiscarding other information. However, this is clearly suboptimal. We analyze\nsample error bounds in this networked setting, providing significantly improved\nresults. An important component of our approach is formed by efficient sample\nweighting schemes, which leads to novel concentration inequalities.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 23:11:52 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:24:18 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 00:23:06 GMT"}, {"version": "v4", "created": "Sat, 3 Jun 2017 12:03:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Yuyi", ""], ["Ramon", "Jan", ""], ["Guo", "Zheng-Chu", ""]]}, {"id": "1405.2606", "submitter": "Joshua Joseph", "authors": "Joshua Joseph, Javier Velez, Nicholas Roy", "title": "Structural Return Maximization for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from\na designer-provided class of policies given a fixed set of training data.\nChoosing the policy which maximizes an estimate of return often leads to\nover-fitting when only limited data is available, due to the size of the policy\nclass in relation to the amount of data available. In this work, we focus on\nlearning policy classes that are appropriately sized to the amount of data\navailable. We accomplish this by using the principle of Structural Risk\nMinimization, from Statistical Learning Theory, which uses Rademacher\ncomplexity to identify a policy class that maximizes a bound on the return of\nthe best policy in the chosen policy class, given the available data. Unlike\nsimilar batch RL approaches, our bound on return requires only extremely weak\nassumptions on the true system.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 00:26:12 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Joseph", "Joshua", ""], ["Velez", "Javier", ""], ["Roy", "Nicholas", ""]]}, {"id": "1405.2639", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Sharp Finite-Time Iterated-Logarithm Martingale Concentration", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give concentration bounds for martingales that are uniform over finite\ntimes and extend classical Hoeffding and Bernstein inequalities. We also\ndemonstrate our concentration bounds to be optimal with a matching\nanti-concentration inequality, proved using the same method. Together these\nconstitute a finite-time version of the law of the iterated logarithm, and shed\nlight on the relationship between it and the central limit theorem.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 06:32:49 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 21:19:07 GMT"}, {"version": "v3", "created": "Wed, 7 Jan 2015 20:27:27 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 21:15:23 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1405.2652", "submitter": "Ronald Ortner", "authors": "Ronald Ortner, Odalric-Ambrym Maillard, Daniil Ryabko", "title": "Selecting Near-Optimal Approximate State Representations in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a reinforcement learning setting introduced in (Maillard et al.,\nNIPS 2011) where the learner does not have explicit access to the states of the\nunderlying Markov decision process (MDP). Instead, she has access to several\nmodels that map histories of past interactions to states. Here we improve over\nknown regret bounds in this setting, and more importantly generalize to the\ncase where the models given to the learner do not contain a true model\nresulting in an MDP representation but only approximations of it. We also give\nimproved error bounds for state aggregation.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 07:45:54 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 12:43:36 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 14:40:20 GMT"}, {"version": "v4", "created": "Mon, 21 Jul 2014 11:52:37 GMT"}, {"version": "v5", "created": "Tue, 12 Aug 2014 12:19:55 GMT"}, {"version": "v6", "created": "Mon, 15 Sep 2014 08:32:45 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Ortner", "Ronald", ""], ["Maillard", "Odalric-Ambrym", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1405.2664", "submitter": "Ji Zhao", "authors": "Ji Zhao, Deyu Meng", "title": "FastMMD: Ensemble of Circular Discrepancy for Efficient Two-Sample Test", "comments": null, "journal-ref": "Neural Computation, 2015 June, Vol. 27, No. 6, Pages 1345-1372", "doi": "10.1162/NECO_a_00732", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum mean discrepancy (MMD) is a recently proposed test statistic for\ntwo-sample test. Its quadratic time complexity, however, greatly hampers its\navailability to large-scale applications. To accelerate the MMD calculation, in\nthis study we propose an efficient method called FastMMD. The core idea of\nFastMMD is to equivalently transform the MMD with shift-invariant kernels into\nthe amplitude expectation of a linear combination of sinusoid components based\non Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking\nadvantage of sampling of Fourier transform, FastMMD decreases the time\ncomplexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$\nare the size and dimension of the sample set, respectively. Here $L$ is the\nnumber of basis functions for approximating kernels which determines the\napproximation accuracy. For kernels that are spherically invariant, the\ncomputation can be further accelerated to $O(L N \\log d)$ by using the Fastfood\ntechnique (Le et al., 2013). The uniform convergence of our method has also\nbeen theoretically proved in both unbiased and biased estimates. We have\nfurther provided a geometric explanation for our method, namely ensemble of\ncircular discrepancy, which facilitates us to understand the insight of MMD,\nand is hopeful to help arouse more extensive metrics for assessing two-sample\ntest. Experimental results substantiate that FastMMD is with similar accuracy\nas exact MMD, while with faster computation speed and lower variance than the\nexisting MMD approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 08:20:21 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 12:06:14 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Zhao", "Ji", ""], ["Meng", "Deyu", ""]]}, {"id": "1405.2690", "submitter": "Prashanth L.A.", "authors": "Prashanth L.A.", "title": "Policy Gradients for CVaR-Constrained MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a risk-constrained version of the stochastic shortest path (SSP)\nproblem, where the risk measure considered is Conditional Value-at-Risk (CVaR).\nWe propose two algorithms that obtain a locally risk-optimal policy by\nemploying four tools: stochastic approximation, mini batches, policy gradients\nand importance sampling. Both the algorithms incorporate a CVaR estimation\nprocedure, along the lines of Bardou et al. [2009], which in turn is based on\nRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio\nprinciple for estimating the gradient of the sum of one cost function\n(objective of the SSP) and the gradient of the CVaR of the sum of another cost\nfunction (in the constraint of SSP). The algorithms differ in the manner in\nwhich they approximate the CVaR estimates/necessary gradients - the first\nalgorithm uses stochastic approximation, while the second employ mini-batches\nin the spirit of Monte Carlo methods. We establish asymptotic convergence of\nboth the algorithms. Further, since estimating CVaR is related to rare-event\nsimulation, we incorporate an importance sampling based variance reduction\nscheme into our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 09:59:59 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["A.", "Prashanth L.", ""]]}, {"id": "1405.2798", "submitter": "Jun Wang", "authors": "Jun Wang, Ke Sun, Fei Sha, Stephane Marchand-Maillet, Alexandros\n  Kalousis", "title": "Two-Stage Metric Learning", "comments": "Accepted for publication in ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel two-stage metric learning algorithm. We\nfirst map each learning instance to a probability distribution by computing its\nsimilarities to a set of fixed anchor points. Then, we define the distance in\nthe input data space as the Fisher information distance on the associated\nstatistical manifold. This induces in the input data space a new family of\ndistance metric with unique properties. Unlike kernelized metric learning, we\ndo not require the similarity measure to be positive semi-definite. Moreover,\nit can also be interpreted as a local metric learning algorithm with well\ndefined distance approximation. We evaluate its performance on a number of\ndatasets. It outperforms significantly other metric learning methods and SVM.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 15:18:15 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Wang", "Jun", ""], ["Sun", "Ke", ""], ["Sha", "Fei", ""], ["Marchand-Maillet", "Stephane", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1405.2875", "submitter": "Chien-Ju Ho", "authors": "Chien-Ju Ho, Aleksandrs Slivkins, Jennifer Wortman Vaughan", "title": "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms\n  for Repeated Principal-Agent Problems", "comments": "This is the full version of a paper in the ACM Conference on\n  Economics and Computation (ACM-EC), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing markets have emerged as a popular platform for matching\navailable workers with tasks to complete. The payment for a particular task is\ntypically set by the task's requester, and may be adjusted based on the quality\nof the completed work, for example, through the use of \"bonus\" payments. In\nthis paper, we study the requester's problem of dynamically adjusting\nquality-contingent payments for tasks. We consider a multi-round version of the\nwell-known principal-agent model, whereby in each round a worker makes a\nstrategic choice of the effort level which is not directly observable by the\nrequester. In particular, our formulation significantly generalizes the\nbudget-free online task pricing problems studied in prior work.\n  We treat this problem as a multi-armed bandit problem, with each \"arm\"\nrepresenting a potential contract. To cope with the large (and in fact,\ninfinite) number of arms, we propose a new algorithm, AgnosticZooming, which\ndiscretizes the contract space into a finite number of regions, effectively\ntreating each region as a single arm. This discretization is adaptively\nrefined, so that more promising regions of the contract space are eventually\ndiscretized more finely. We analyze this algorithm, showing that it achieves\nregret sublinear in the time horizon and substantially improves over\nnon-adaptive discretization (which is the only competing approach in the\nliterature).\n  Our results advance the state of art on several different topics: the theory\nof crowdsourcing markets, principal-agent problems, multi-armed bandits, and\ndynamic pricing.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 18:52:28 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 04:21:07 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ho", "Chien-Ju", ""], ["Slivkins", "Aleksandrs", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1405.2878", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA)", "title": "Approximate Policy Iteration Schemes: A Comparison", "comments": "ICML (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the infinite-horizon discounted optimal control problem\nformalized by Markov Decision Processes. We focus on several approximate\nvariations of the Policy Iteration algorithm: Approximate Policy Iteration,\nConservative Policy Iteration (CPI), a natural adaptation of the Policy Search\nby Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$),\nand the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all\nalgorithms, we describe performance bounds, and make a comparison by paying a\nparticular attention to the concentrability constants involved, the number of\niterations and the memory required. Our analysis highlights the following\npoints: 1) The performance guarantee of CPI can be arbitrarily better than that\nof API/API($\\alpha$), but this comes at the cost of a relative---exponential in\n$\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$\nenjoys the best of both worlds: its performance guarantee is similar to that of\nCPI, but within a number of iterations similar to that of API. 3) Contrary to\nAPI that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$\nis proportional to their number of iterations, which may be problematic when\nthe discount factor $\\gamma$ is close to 1 or the approximation error\n$\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make\nan overall trade-off between memory and performance. Simulations with these\nschemes confirm our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 19:11:03 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1405.3080", "submitter": "Tong Zhang", "authors": "Peilin Zhao, Tong Zhang", "title": "Accelerating Minibatch Stochastic Gradient Descent using Stratified\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a popular optimization method which has\nbeen applied to many important machine learning tasks such as Support Vector\nMachines and Deep Neural Networks. In order to parallelize SGD, minibatch\ntraining is often employed. The standard approach is to uniformly sample a\nminibatch at each step, which often leads to high variance. In this paper we\npropose a stratified sampling strategy, which divides the whole dataset into\nclusters with low within-cluster variance; we then take examples from these\nclusters using a stratified sampling technique. It is shown that the\nconvergence rate can be significantly improved by the algorithm. Encouraging\nexperimental results confirm the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 09:45:49 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Zhao", "Peilin", ""], ["Zhang", "Tong", ""]]}, {"id": "1405.3162", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang", "title": "Circulant Binary Embedding", "comments": "ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embedding of high-dimensional data requires long codes to preserve the\ndiscriminative power of the input space. Traditional binary coding methods\noften suffer from very high computation and storage costs in such a scenario.\nTo address this problem, we propose Circulant Binary Embedding (CBE) which\ngenerates binary codes by projecting the data with a circulant matrix. The\ncirculant structure enables the use of Fast Fourier Transformation to speed up\nthe computation. Compared to methods that use unstructured matrices, the\nproposed method improves the time complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d\\log{d})$, and the space complexity from $\\mathcal{O}(d^2)$ to\n$\\mathcal{O}(d)$ where $d$ is the input dimensionality. We also propose a novel\ntime-frequency alternating optimization to learn data-dependent circulant\nprojections, which alternatively minimizes the objective in original and\nFourier domains. We show by extensive experiments that the proposed approach\ngives much better performance than the state-of-the-art approaches for fixed\ntime, and provides much faster computation with no performance degradation for\nfixed number of bits.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 14:17:11 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Yu", "Felix X.", ""], ["Kumar", "Sanjiv", ""], ["Gong", "Yunchao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1405.3167", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Yury Makarychev, Nathan Srebro", "title": "Clustering, Hamming Embedding, Generalized LSH and the Max Norm", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convex relaxation of clustering and hamming embedding, focusing\non the asymmetric case (co-clustering and asymmetric hamming embedding),\nunderstanding their relationship to LSH as studied by (Charikar 2002) and to\nthe max-norm ball, and the differences between their symmetric and asymmetric\nversions.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 14:36:59 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Makarychev", "Yury", ""], ["Srebro", "Nathan", ""]]}, {"id": "1405.3210", "submitter": "Jeremy Kun", "authors": "Jeremy Kun, Rajmonda Caceres, Kevin Carter", "title": "Locally Boosted Graph Aggregation for Community Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:1401.3258", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the right graph representation from noisy, multi-source data has\ngarnered significant interest in recent years. A central tenet of this problem\nis relational learning. Here the objective is to incorporate the partial\ninformation each data source gives us in a way that captures the true\nunderlying relationships. To address this challenge, we present a general,\nboosting-inspired framework for combining weak evidence of entity associations\ninto a robust similarity metric. Building on previous work, we explore the\nextent to which different local quality measurements yield graph\nrepresentations that are suitable for community detection. We present empirical\nresults on a variety of datasets demonstrating the utility of this framework,\nespecially with respect to real datasets where noise and scale present serious\nchallenges. Finally, we prove a convergence theorem in an ideal setting and\noutline future research into other application domains.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:08:55 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Kun", "Jeremy", ""], ["Caceres", "Rajmonda", ""], ["Carter", "Kevin", ""]]}, {"id": "1405.3222", "submitter": "Taylor Arnold", "authors": "Taylor Arnold and Ryan Tibshirani", "title": "Efficient Implementations of the Generalized Lasso Dual Path Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficient implementations of the generalized lasso dual path\nalgorithm of Tibshirani and Taylor (2011). We first describe a generic approach\nthat covers any penalty matrix D and any (full column rank) matrix X of\npredictor variables. We then describe fast implementations for the special\ncases of trend filtering problems, fused lasso problems, and sparse fused lasso\nproblems, both with X=I and a general matrix X. These specialized\nimplementations offer a considerable improvement over the generic\nimplementation, both in terms of numerical stability and efficiency of the\nsolution path computation. These algorithms are all available for use in the\ngenlasso R package, which can be found in the CRAN repository.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:42:45 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:44:50 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Arnold", "Taylor", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1405.3224", "submitter": "Aurelien Garivier", "authors": "Emilie Kaufmann (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (IMT)", "title": "On the Complexity of A/B Testing", "comments": null, "journal-ref": "Conference on Learning Theory, Jun 2014, Barcelona, Spain. JMLR:\n  Workshop and Conference Proceedings, 35, pp.461-481", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing refers to the task of determining the best option among two\nalternatives that yield random outcomes. We provide distribution-dependent\nlower bounds for the performance of A/B testing that improve over the results\ncurrently available both in the fixed-confidence (or delta-PAC) and\nfixed-budget settings. When the distribution of the outcomes are Gaussian, we\nprove that the complexity of the fixed-confidence and fixed-budget settings are\nequivalent, and that uniform sampling of both alternatives is optimal only in\nthe case of equal variances. In the common variance case, we also provide a\nstopping rule that terminates faster than existing fixed-confidence algorithms.\nIn the case of Bernoulli distributions, we show that the complexity of\nfixed-budget setting is smaller than that of fixed-confidence setting and that\nuniform sampling of both alternatives -though not optimal- is advisable in\npractice when combined with an appropriate stopping criterion.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:47:17 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 08:55:57 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Kaufmann", "Emilie", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1405.3229", "submitter": "Bruno Scherrer", "authors": "Manel Tagorti (INRIA Nancy - Grand Est / LORIA), Bruno Scherrer (INRIA\n  Nancy - Grand Est / LORIA)", "title": "Rate of Convergence and Error Bounds for LSTD($\\lambda$)", "comments": "(2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider LSTD($\\lambda$), the least-squares temporal-difference algorithm\nwith eligibility traces algorithm proposed by Boyan (2002). It computes a\nlinear approximation of the value function of a fixed policy in a large Markov\nDecision Process. Under a $\\beta$-mixing assumption, we derive, for any value\nof $\\lambda \\in (0,1)$, a high-probability estimate of the rate of convergence\nof this algorithm to its limit. We deduce a high-probability bound on the error\nof this algorithm, that extends (and slightly improves) that derived by Lazaric\net al. (2012) in the specific case where $\\lambda=0$. In particular, our\nanalysis sheds some light on the choice of $\\lambda$ with respect to the\nquality of the chosen linear space and the number of samples, that complies\nwith simulations.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 16:51:54 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Tagorti", "Manel", "", "INRIA Nancy - Grand Est / LORIA"], ["Scherrer", "Bruno", "", "INRIA\n  Nancy - Grand Est / LORIA"]]}, {"id": "1405.3292", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki, Rafael Bassi Stern", "title": "Learning with many experts: model selection and sparsity", "comments": "This is the pre-peer reviewed version", "journal-ref": "Izbicki, R., Stern, R. B. \"Learning with many experts: Model\n  selection and sparsity.\" Statistical Analysis and Data Mining 6.6 (2013):\n  565-577", "doi": "10.1002/sam.11206", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experts classifying data are often imprecise. Recently, several models have\nbeen proposed to train classifiers using the noisy labels generated by these\nexperts. How to choose between these models? In such situations, the true\nlabels are unavailable. Thus, one cannot perform model selection using the\nstandard versions of methods such as empirical risk minimization and cross\nvalidation. In order to allow model selection, we present a surrogate loss and\nprovide theoretical guarantees that assure its consistency. Next, we discuss\nhow this loss can be used to tune a penalization which introduces sparsity in\nthe parameters of a traditional class of models. Sparsity provides more\nparsimonious models and can avoid overfitting. Nevertheless, it has seldom been\ndiscussed in the context of noisy labels due to the difficulty in model\nselection and, therefore, in choosing tuning parameters. We apply these\ntechniques to several sets of simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:03:14 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Izbicki", "Rafael", ""], ["Stern", "Rafael Bassi", ""]]}, {"id": "1405.3295", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter and Christoph Waldhauser", "title": "Effects of Sampling Methods on Prediction Quality. The Case of\n  Classifying Land Cover Using Decision Trees", "comments": null, "journal-ref": "Proceedings of COMPSTAT 2014: 585-592. 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clever sampling methods can be used to improve the handling of big data and\nincrease its usefulness. The subject of this study is remote sensing,\nspecifically airborne laser scanning point clouds representing different\nclasses of ground cover. The aim is to derive a supervised learning model for\nthe classification using CARTs. In order to measure the effect of different\nsampling methods on the classification accuracy, various experiments with\nvarying types of sampling methods, sample sizes, and accuracy metrics have been\ndesigned. Numerical results for a subset of a large surveying project covering\nthe lower Rhine area in Germany are shown. General conclusions regarding\nsampling design are drawn and presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:07:09 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hochreiter", "Ronald", ""], ["Waldhauser", "Christoph", ""]]}, {"id": "1405.3316", "submitter": "Yonatan Gur", "authors": "Omar Besbes, Yonatan Gur, Assaf Zeevi", "title": "Optimal Exploration-Exploitation in a Multi-Armed-Bandit Problem with\n  Non-stationary Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit (MAB) problem a gambler needs to choose at each round\nof play one of K arms, each characterized by an unknown reward distribution.\nReward realizations are only observed when an arm is selected, and the\ngambler's objective is to maximize his cumulative expected earnings over some\ngiven horizon of play T. To do this, the gambler needs to acquire information\nabout arms (exploration) while simultaneously optimizing immediate rewards\n(exploitation); the price paid due to this trade off is often referred to as\nthe regret, and the main question is how small can this price be as a function\nof the horizon length T. This problem has been studied extensively when the\nreward distributions do not change over time; an assumption that supports a\nsharp characterization of the regret, yet is often violated in practical\nsettings. In this paper, we focus on a MAB formulation which allows for a broad\nrange of temporal uncertainties in the rewards, while still maintaining\nmathematical tractability. We fully characterize the (regret) complexity of\nthis class of MAB problems by establishing a direct link between the extent of\nallowable reward \"variation\" and the minimal achievable regret. Our analysis\ndraws some connections between two rather disparate strands of literature: the\nadversarial and the stochastic MAB frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:15:06 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 16:42:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Besbes", "Omar", ""], ["Gur", "Yonatan", ""], ["Zeevi", "Assaf", ""]]}, {"id": "1405.3318", "submitter": "James Neufeld", "authors": "James Neufeld, Andr\\'as Gy\\\"orgy, Dale Schuurmans, Csaba Szepesv\\'ari", "title": "Adaptive Monte Carlo via Bandit Allocation", "comments": "The 31st International Conference on Machine Learning (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequentially choosing between a set of unbiased\nMonte Carlo estimators to minimize the mean-squared-error (MSE) of a final\ncombined estimate. By reducing this task to a stochastic multi-armed bandit\nproblem, we show that well developed allocation strategies can be used to\nachieve an MSE that approaches that of the best estimator chosen in retrospect.\nWe then extend these developments to a scenario where alternative estimators\nhave different, possibly stochastic costs. The outcome is a new set of adaptive\nMonte Carlo strategies that provide stronger guarantees than previous\napproaches while offering practical advantages.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:29:14 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Neufeld", "James", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Schuurmans", "Dale", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1405.3382", "submitter": "Samaneh Khoshrou", "authors": "Samaneh Khoshrou, Jaime S. Cardoso, Luis F. Teixeira", "title": "Active Mining of Parallel Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practicality of a video surveillance system is adversely limited by the\namount of queries that can be placed on human resources and their vigilance in\nresponse. To transcend this limitation, a major effort under way is to include\nsoftware that (fully or at least semi) automatically mines video footage,\nreducing the burden imposed to the system. Herein, we propose a semi-supervised\nincremental learning framework for evolving visual streams in order to develop\na robust and flexible track classification system. Our proposed method learns\nfrom consecutive batches by updating an ensemble in each time. It tries to\nstrike a balance between performance of the system and amount of data which\nneeds to be labelled. As no restriction is considered, the system can address\nmany practical problems in an evolving multi-camera scenario, such as concept\ndrift, class evolution and various length of video streams which have not been\naddressed before. Experiments were performed on synthetic as well as real-world\nvisual data in non-stationary environments, showing high accuracy with fairly\nlittle human collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 07:00:38 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Khoshrou", "Samaneh", ""], ["Cardoso", "Jaime S.", ""], ["Teixeira", "Luis F.", ""]]}, {"id": "1405.3396", "submitter": "Nir Ailon", "authors": "Nir Ailon and Thorsten Joachims and Zohar Karnin", "title": "Reducing Dueling Bandits to Cardinal Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for reducing the Dueling Bandits problem to the\nconventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits\nproblem is an online model of learning with ordinal feedback of the form \"A is\npreferred to B\" (as opposed to cardinal feedback like \"A has value 2.5\"),\ngiving it wide applicability in learning from implicit user feedback and\nrevealed and stated preferences. In contrast to existing algorithms for the\nDueling Bandits problem, our reductions -- named $\\Doubler$, $\\MultiSbm$ and\n$\\DoubleSbm$ -- provide a generic schema for translating the extensive body of\nknown results about conventional Multi-Armed Bandit algorithms to the Dueling\nBandits setting. For $\\Doubler$ and $\\MultiSbm$ we prove regret upper bounds in\nboth finite and infinite settings, and conjecture about the performance of\n$\\DoubleSbm$ which empirically outperforms the other two as well as previous\nalgorithms in our experiments. In addition, we provide the first almost optimal\nregret bound in terms of second order terms, such as the differences between\nthe values of the arms.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 08:03:08 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Ailon", "Nir", ""], ["Joachims", "Thorsten", ""], ["Karnin", "Zohar", ""]]}, {"id": "1405.3410", "submitter": "Tieming Chen", "authors": "Tieming Chen, Xu Zhang, Shichao Jin, Okhee Kim", "title": "Efficient classification using parallel and scalable compressed model\n  and Its application on intrusion detection", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2014.04.009", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve high efficiency of classification in intrusion detection,\na compressed model is proposed in this paper which combines horizontal\ncompression with vertical compression. OneR is utilized as horizontal\ncom-pression for attribute reduction, and affinity propagation is employed as\nvertical compression to select small representative exemplars from large\ntraining data. As to be able to computationally compress the larger volume of\ntraining data with scalability, MapReduce based parallelization approach is\nthen implemented and evaluated for each step of the model compression process\nabovementioned, on which common but efficient classification methods can be\ndirectly used. Experimental application study on two publicly available\ndatasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the\nclassification using the compressed model proposed can effectively speed up the\ndetection procedure at up to 184 times, most importantly at the cost of a\nminimal accuracy difference with less than 1% on average.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 08:47:31 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Chen", "Tieming", ""], ["Zhang", "Xu", ""], ["Jin", "Shichao", ""], ["Kim", "Okhee", ""]]}, {"id": "1405.3536", "submitter": "Preux Philippe", "authors": "Olivier Nicol (INRIA Lille - Nord Europe, LIFL), J\\'er\\'emie Mary\n  (INRIA Lille - Nord Europe, LIFL), Philippe Preux (INRIA Lille - Nord Europe,\n  LIFL)", "title": "Improving offline evaluation of contextual bandit algorithms via\n  bootstrapping techniques", "comments": null, "journal-ref": "International Conference on Machine Learning 32 (2014)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many recommendation applications such as news recommendation, the items\nthat can be rec- ommended come and go at a very fast pace. This is a challenge\nfor recommender systems (RS) to face this setting. Online learning algorithms\nseem to be the most straight forward solution. The contextual bandit framework\nwas introduced for that very purpose. In general the evaluation of a RS is a\ncritical issue. Live evaluation is of- ten avoided due to the potential loss of\nrevenue, hence the need for offline evaluation methods. Two options are\navailable. Model based meth- ods are biased by nature and are thus difficult to\ntrust when used alone. Data driven methods are therefore what we consider here.\nEvaluat- ing online learning algorithms with past data is not simple but some\nmethods exist in the litera- ture. Nonetheless their accuracy is not satisfac-\ntory mainly due to their mechanism of data re- jection that only allow the\nexploitation of a small fraction of the data. We precisely address this issue\nin this paper. After highlighting the limita- tions of the previous methods, we\npresent a new method, based on bootstrapping techniques. This new method comes\nwith two important improve- ments: it is much more accurate and it provides a\nmeasure of quality of its estimation. The latter is a highly desirable property\nin order to minimize the risks entailed by putting online a RS for the first\ntime. We provide both theoretical and ex- perimental proofs of its superiority\ncompared to state-of-the-art methods, as well as an analysis of the convergence\nof the measure of quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 15:29:02 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Nicol", "Olivier", "", "INRIA Lille - Nord Europe, LIFL"], ["Mary", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe, LIFL"], ["Preux", "Philippe", "", "INRIA Lille - Nord Europe,\n  LIFL"]]}, {"id": "1405.3612", "submitter": "Reid Priedhorsky", "authors": "Nicholas Generous (1), Geoffrey Fairchild (1), Alina Deshpande (1),\n  Sara Y. Del Valle (1), Reid Priedhorsky (1) ((1) Los Alamos National\n  Laboratory, Los Alamos, NM)", "title": "Global disease monitoring and forecasting with Wikipedia", "comments": "27 pages; 4 figures; 4 tables. Version 2: Cite McIver & Brownstein\n  and adjust novelty claims accordingly; revise title; various revisions for\n  clarity", "journal-ref": "PLOS Comput. Biol., vol. 10, no. 11, p. e1003892, Nov. 2014", "doi": "10.1371/journal.pcbi.1003892", "report-no": "LA-UR 14-22535", "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious disease is a leading threat to public health, economic stability,\nand other key social structures. Efforts to mitigate these impacts depend on\naccurate and timely monitoring to measure the risk and progress of disease.\nTraditional, biologically-focused monitoring techniques are accurate but costly\nand slow; in response, new techniques based on social internet data such as\nsocial media and search queries are emerging. These efforts are promising, but\nimportant challenges in the areas of scientific peer review, breadth of\ndiseases and countries, and forecasting hamper their operational usefulness.\n  We examine a freely available, open data source for this use: access logs\nfrom the online encyclopedia Wikipedia. Using linear models, language as a\nproxy for location, and a systematic yet simple article selection procedure, we\ntested 14 location-disease combinations and demonstrate that these data\nfeasibly support an approach that overcomes these challenges. Specifically, our\nproof-of-concept yields models with $r^2$ up to 0.92, forecasting value up to\nthe 28 days tested, and several pairs of models similar enough to suggest that\ntransferring models from one location to another without re-training is\nfeasible.\n  Based on these preliminary results, we close with a research agenda designed\nto overcome these challenges and produce a disease monitoring and forecasting\nsystem that is significantly more effective, robust, and globally comprehensive\nthan the current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 18:26:23 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 16:11:43 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Generous", "Nicholas", ""], ["Fairchild", "Geoffrey", ""], ["Deshpande", "Alina", ""], ["Del Valle", "Sara Y.", ""], ["Priedhorsky", "Reid", ""]]}, {"id": "1405.3726", "submitter": "Xi Qiu", "authors": "Xi Qiu and Christopher Stewart", "title": "Topic words analysis based on LDA model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 02:15:01 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Qiu", "Xi", ""], ["Stewart", "Christopher", ""]]}, {"id": "1405.3843", "submitter": "Tomer Koren", "authors": "Elad Hazan, Tomer Koren, Kfir Y. Levy", "title": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logistic loss function is often advocated in machine learning and\nstatistics as a smooth and strictly convex surrogate for the 0-1 loss. In this\npaper we investigate the question of whether these smoothness and convexity\nproperties make the logistic loss preferable to other widely considered options\nsuch as the hinge loss. We show that in contrast to known asymptotic bounds, as\nlong as the number of prediction/optimization iterations is sub exponential,\nthe logistic loss provides no improvement over a generic non-smooth loss\nfunction such as the hinge loss. In particular we show that the convergence\nrate of stochastic logistic optimization is bounded from below by a polynomial\nin the diameter of the decision set and the number of prediction iterations,\nand provide a matching tight upper bound. This resolves the COLT open problem\nof McMahan and Streeter (2012).\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 13:29:27 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Hazan", "Elad", ""], ["Koren", "Tomer", ""], ["Levy", "Kfir Y.", ""]]}, {"id": "1405.4047", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Methods and Models for Interpretable Linear Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integer programming framework to build accurate and\ninterpretable discrete linear classification models. Unlike existing\napproaches, our framework is designed to provide practitioners with the control\nand flexibility they need to tailor accurate and interpretable models for a\ndomain of choice. To this end, our framework can produce models that are fully\noptimized for accuracy, by minimizing the 0--1 classification loss, and that\naddress multiple aspects of interpretability, by incorporating a range of\ndiscrete constraints and penalty functions. We use our framework to produce\nmodels that are difficult to create with existing methods, such as scoring\nsystems and M-of-N rule tables. In addition, we propose specially designed\noptimization methods to improve the scalability of our framework through\ndecomposition and data reduction. We show that discrete linear classifiers can\nattain the training accuracy of any other linear classifier, and provide an\nOccam's Razor type argument as to why the use of small discrete coefficients\ncan provide better generalization. We demonstrate the performance and\nflexibility of our framework through numerical experiments and a case study in\nwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 01:30:11 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 23:33:31 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1405.4053", "submitter": "Quoc Le", "authors": "Quoc V. Le and Tomas Mikolov", "title": "Distributed Representations of Sentences and Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 07:12:16 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 23:23:19 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Le", "Quoc V.", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1405.4324", "submitter": "Akshay Gadde", "authors": "Akshay Gadde, Aamir Anis and Antonio Ortega", "title": "Active Semi-Supervised Learning Using Sampling Theory for Graph Signals", "comments": "10 pages, 6 figures, To appear in KDD'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of offline, pool-based active semi-supervised\nlearning on graphs. This problem is important when the labeled data is scarce\nand expensive whereas unlabeled data is easily available. The data points are\nrepresented by the vertices of an undirected graph with the similarity between\nthem captured by the edge weights. Given a target number of nodes to label, the\ngoal is to choose those nodes that are most informative and then predict the\nunknown labels. We propose a novel framework for this problem based on our\nrecent results on sampling theory for graph signals. A graph signal is a\nreal-valued function defined on each node of the graph. A notion of frequency\nfor such signals can be defined using the spectrum of the graph Laplacian\nmatrix. The sampling theory for graph signals aims to extend the traditional\nNyquist-Shannon sampling theory by allowing us to identify the class of graph\nsignals that can be reconstructed from their values on a subset of vertices.\nThis approach allows us to define a criterion for active learning based on\nsampling set selection which aims at maximizing the frequency of the signals\nthat can be reconstructed from their samples on the set. Experiments show the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 22:31:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Gadde", "Akshay", ""], ["Anis", "Aamir", ""], ["Ortega", "Antonio", ""]]}, {"id": "1405.4394", "submitter": "Tapio Pahikkala", "authors": "Michiel Stock, Thomas Fober, Eyke H\\\"ullermeier, Serghei Glinca,\n  Gerhard Klebe, Tapio Pahikkala, Antti Airola, Bernard De Baets, Willem\n  Waegeman", "title": "Identification of functionally related enzymes by learning-to-rank\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enzyme sequences and structures are routinely used in the biological sciences\nas queries to search for functionally related enzymes in online databases. To\nthis end, one usually departs from some notion of similarity, comparing two\nenzymes by looking for correspondences in their sequences, structures or\nsurfaces. For a given query, the search operation results in a ranking of the\nenzymes in the database, from very similar to dissimilar enzymes, while\ninformation about the biological function of annotated database enzymes is\nignored.\n  In this work we show that rankings of that kind can be substantially improved\nby applying kernel-based learning algorithms. This approach enables the\ndetection of statistical dependencies between similarities of the active cleft\nand the biological function of annotated enzymes. This is in contrast to\nsearch-based approaches, which do not take annotated training data into\naccount. Similarity measures based on the active cleft are known to outperform\nsequence-based or structure-based measures under certain conditions. We\nconsider the Enzyme Commission (EC) classification hierarchy for obtaining\nannotated enzymes during the training phase. The results of a set of sizeable\nexperiments indicate a consistent and significant improvement for a set of\nsimilarity measures that exploit information about small cavities in the\nsurface of enzymes.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 13:51:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Stock", "Michiel", ""], ["Fober", "Thomas", ""], ["H\u00fcllermeier", "Eyke", ""], ["Glinca", "Serghei", ""], ["Klebe", "Gerhard", ""], ["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1405.4423", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala, Michiel Stock, Antti Airola, Tero Aittokallio,\n  Bernard De Baets, Willem Waegeman", "title": "A two-step learning approach for solving full and almost full cold start\n  problems in dyadic prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic prediction methods operate on pairs of objects (dyads), aiming to\ninfer labels for out-of-sample dyads. We consider the full and almost full cold\nstart problem in dyadic prediction, a setting that occurs when both objects in\nan out-of-sample dyad have not been observed during training, or if one of them\nhas been observed, but very few times. A popular approach for addressing this\nproblem is to train a model that makes predictions based on a pairwise feature\nrepresentation of the dyads, or, in case of kernel methods, based on a tensor\nproduct pairwise kernel. As an alternative to such a kernel approach, we\nintroduce a novel two-step learning algorithm that borrows ideas from the\nfields of pairwise learning and spectral filtering. We show theoretically that\nthe two-step method is very closely related to the tensor product kernel\napproach, and experimentally that it yields a slightly better predictive\nperformance. Moreover, unlike existing tensor product kernel methods, the\ntwo-step method allows closed-form solutions for training and parameter\nselection via cross-validation estimates both in the full and almost full cold\nstart settings, making the approach much more efficient and straightforward to\nimplement.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 18:20:13 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Stock", "Michiel", ""], ["Airola", "Antti", ""], ["Aittokallio", "Tero", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1405.4463", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Shaowei Lin, Dusit Niyato and Hwee-Pink Tan", "title": "Machine Learning in Wireless Sensor Networks: Algorithms, Strategies,\n  and Applications", "comments": "Accepted for publication in IEEE Communications Surveys and Tutorials", "journal-ref": "IEEE Communications Surveys & Tutorials, vol. 16, no. 4, pp.\n  1996-2018, Fourthquarter 2014", "doi": "10.1109/COMST.2014.2320099", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor networks monitor dynamic environments that change rapidly\nover time. This dynamic behavior is either caused by external factors or\ninitiated by the system designers themselves. To adapt to such conditions,\nsensor networks often adopt machine learning techniques to eliminate the need\nfor unnecessary redesign. Machine learning also inspires many practical\nsolutions that maximize resource utilization and prolong the lifespan of the\nnetwork. In this paper, we present an extensive literature review over the\nperiod 2002-2013 of machine learning methods that were used to address common\nissues in wireless sensor networks (WSNs). The advantages and disadvantages of\neach proposed algorithm are evaluated against the corresponding problem. We\nalso provide a comparative guide to aid WSN designers in developing suitable\nmachine learning solutions for their specific application challenges.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 06:28:47 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 15:15:04 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Lin", "Shaowei", ""], ["Niyato", "Dusit", ""], ["Tan", "Hwee-Pink", ""]]}, {"id": "1405.4471", "submitter": "Tomer Koren", "authors": "Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres", "title": "Online Learning with Composite Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new class of online learning problems where each of the online\nalgorithm's actions is assigned an adversarial value, and the loss of the\nalgorithm at each step is a known and deterministic function of the values\nassigned to its recent actions. This class includes problems where the\nalgorithm's loss is the minimum over the recent adversarial values, the maximum\nover the recent values, or a linear combination of the recent values. We\nanalyze the minimax regret of this class of problems when the algorithm\nreceives bandit feedback, and prove that when the minimum or maximum functions\nare used, the minimax regret is $\\tilde \\Omega(T^{2/3})$ (so called hard online\nlearning problems), and when a linear function is used, the minimax regret is\n$\\tilde O(\\sqrt{T})$ (so called easy learning problems). Previously, the only\nonline learning problem that was known to be provably hard was the multi-armed\nbandit with switching costs.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 08:47:58 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Dekel", "Ofer", ""], ["Ding", "Jian", ""], ["Koren", "Tomer", ""], ["Peres", "Yuval", ""]]}, {"id": "1405.4543", "submitter": "Dhruv Mahajan", "authors": "Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan", "title": "A Distributed Algorithm for Training Nonlinear Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the distributed training of nonlinear kernel machines on\nMap-Reduce. We show that a re-formulation of Nystr\\\"om approximation based\nsolution which is solved using gradient based techniques is well suited for\nthis, especially when it is necessary to work with a large number of basis\npoints. The main advantages of this approach are: avoidance of computing the\npseudo-inverse of the kernel sub-matrix corresponding to the basis points;\nsimplicity and efficiency of the distributed part of the computations; and,\nfriendliness to stage-wise addition of basis points. We implement the method\nusing an AllReduce tree on Hadoop and demonstrate its value on a few large\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 19:54:18 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Keerthi", "S. Sathiya", ""], ["Sundararajan", "S.", ""]]}, {"id": "1405.4544", "submitter": "Dhruv Mahajan", "authors": "Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan", "title": "A distributed block coordinate descent method for training $l_1$\n  regularized linear classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of $l_1$ regularized classifiers has received great\nattention recently. Most existing methods approach this problem by taking steps\nobtained from approximating the objective by a quadratic approximation that is\ndecoupled at the individual variable level. These methods are designed for\nmulticore and MPI platforms where communication costs are low. They are\ninefficient on systems such as Hadoop running on a cluster of commodity\nmachines where communication costs are substantial. In this paper we design a\ndistributed algorithm for $l_1$ regularization that is much better suited for\nsuch systems than existing algorithms. A careful cost analysis is used to\nsupport these points and motivate our method. The main idea of our algorithm is\nto do block optimization of many variables on the actual objective function\nwithin each computing node; this increases the computational cost per step that\nis matched with the communication cost, and decreases the number of outer\niterations, thus yielding a faster overall method. Distributed Gauss-Seidel and\nGauss-Southwell greedy schemes are used for choosing variables to update in\neach step. We establish global convergence theory for our algorithm, including\nQ-linear rate of convergence. Experiments on two benchmark problems show our\nmethod to be much faster than existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 20:07:41 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 21:31:59 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Keerthi", "S. Sathiya", ""], ["Sundararajan", "S.", ""]]}, {"id": "1405.4583", "submitter": "Wei Feng", "authors": "Wei Feng and Jiaya Jia and Zhi-Qiang Liu", "title": "ESSP: An Efficient Approach to Minimizing Dense and Nonsubmodular Energy\n  Functions", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in computer vision have demonstrated the impressive\npower of dense and nonsubmodular energy functions in solving visual labeling\nproblems. However, minimizing such energies is challenging. None of existing\ntechniques (such as s-t graph cut, QPBO, BP and TRW-S) can individually do this\nwell. In this paper, we present an efficient method, namely ESSP, to optimize\nbinary MRFs with arbitrary pairwise potentials, which could be nonsubmodular\nand with dense connectivity. We also provide a comparative study of our\napproach and several recent promising methods. From our study, we make some\nreasonable recommendations of combining existing methods that perform the best\nin different situations for this challenging problem. Experimental results\nvalidate that for dense and nonsubmodular energy functions, the proposed\napproach can usually obtain lower energies than the best combination of other\ntechniques using comparably reasonable time.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 03:06:14 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Feng", "Wei", ""], ["Jia", "Jiaya", ""], ["Liu", "Zhi-Qiang", ""]]}, {"id": "1405.4589", "submitter": "Chao Zhang", "authors": "Chao Zhang, Hong-cen Mei, Hao Yang", "title": "A Parallel Way to Select the Parameters of SVM Based on the Ant\n  Optimization Algorithm", "comments": "3 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of experimental data shows that Support Vector Machine (SVM)\nalgorithm has obvious advantages in text classification, handwriting\nrecognition, image classification, bioinformatics, and some other fields. To\nsome degree, the optimization of SVM depends on its kernel function and Slack\nvariable, the determinant of which is its parameters $\\delta$ and c in the\nclassification function. That is to say,to optimize the SVM algorithm, the\noptimization of the two parameters play a huge role. Ant Colony Optimization\n(ACO) is optimization algorithm which simulate ants to find the optimal path.In\nthe available literature, we mix the ACO algorithm and Parallel algorithm\ntogether to find a well parameters.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 03:50:21 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 11:53:39 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Zhang", "Chao", ""], ["Mei", "Hong-cen", ""], ["Yang", "Hao", ""]]}, {"id": "1405.4599", "submitter": "Dalei Wu", "authors": "Dalei Wu and Haiqing Wu", "title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for\n  Multivariate Gaussian Mixture Models with an Application to Noisy Speech\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The trimming scheme with a prefixed cutoff portion is known as a method of\nimproving the robustness of statistical models such as multivariate Gaussian\nmixture models (MG- MMs) in small scale tests by alleviating the impacts of\noutliers. However, when this method is applied to real- world data, such as\nnoisy speech processing, it is hard to know the optimal cut-off portion to\nremove the outliers and sometimes removes useful data samples as well. In this\npaper, we propose a new method based on measuring the dispersion degree (DD) of\nthe training data to avoid this problem, so as to realise automatic robust\nestimation for MGMMs. The DD model is studied by using two different measures.\nFor each one, we theoretically prove that the DD of the data samples in a\ncontext of MGMMs approximately obeys a specific (chi or chi-square)\ndistribution. The proposed method is evaluated on a real-world application with\na moderately-sized speaker recognition task. Experiments show that the proposed\nmethod can significantly improve the robustness of the conventional training\nmethod of GMMs for speaker recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 04:36:38 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Wu", "Dalei", ""], ["Wu", "Haiqing", ""]]}, {"id": "1405.4604", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu, Yann N. Dauphin, Surya Ganguli and Yoshua Bengio", "title": "On the saddle point problem for non-convex optimization", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge to many fields of science and engineering involves\nminimizing non-convex error functions over continuous, high dimensional spaces.\nGradient descent or quasi-Newton methods are almost ubiquitously used to\nperform such minimizations, and it is often thought that a main source of\ndifficulty for the ability of these local methods to find the global minimum is\nthe proliferation of local minima with much higher error than the global\nminimum. Here we argue, based on results from statistical physics, random\nmatrix theory, and neural network theory, that a deeper and more profound\ndifficulty originates from the proliferation of saddle points, not local\nminima, especially in high dimensional problems of practical interest. Such\nsaddle points are surrounded by high error plateaus that can dramatically slow\ndown learning, and give the illusory impression of the existence of a local\nminimum. Motivated by these arguments, we propose a new algorithm, the\nsaddle-free Newton method, that can rapidly escape high dimensional saddle\npoints, unlike gradient descent and quasi-Newton methods. We apply this\nalgorithm to deep neural network training, and provide preliminary numerical\nevidence for its superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 04:56:30 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 03:05:00 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Pascanu", "Razvan", ""], ["Dauphin", "Yann N.", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1405.4758", "submitter": "Richard Combes", "authors": "Stefan Magureanu and Richard Combes and Alexandre Proutiere", "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms", "comments": "COLT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandit problems where the expected reward\nis a Lipschitz function of the arm, and where the set of arms is either\ndiscrete or continuous. For discrete Lipschitz bandits, we derive asymptotic\nproblem specific lower bounds for the regret satisfied by any algorithm, and\npropose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz\nstructure of the problem. In fact, we prove that OSLB is asymptotically\noptimal, as its asymptotic regret matches the lower bound. The regret analysis\nof our algorithms relies on a new concentration inequality for weighted sums of\nKL divergences between the empirical distributions of rewards and their true\ndistributions. For continuous Lipschitz bandits, we propose to first discretize\nthe action space, and then apply OSLB or CKL-UCB, algorithms that provably\nexploit the structure efficiently. This approach is shown, through numerical\nexperiments, to significantly outperform existing algorithms that directly deal\nwith the continuous set of arms. Finally the results and algorithms are\nextended to contextual bandits with similarities.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 14:56:51 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Magureanu", "Stefan", ""], ["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1405.4807", "submitter": "Yuxin Chen", "authors": "Qixing Huang, Yuxin Chen, and Leonidas Guibas", "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation", "comments": "accepted to International Conference on Machine Learning (ICML 2014)", "journal-ref": "International Conference on Machine Learning (ICML), vol. 32, pp.\n  64-72, June 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum a posteriori (MAP) inference over discrete Markov random fields is a\nfundamental task spanning a wide spectrum of real-world applications, which is\nknown to be NP-hard for general graphs. In this paper, we propose a novel\nsemidefinite relaxation formulation (referred to as SDR) to estimate the MAP\nassignment. Algorithmically, we develop an accelerated variant of the\nalternating direction method of multipliers (referred to as SDPAD-LR) that can\neffectively exploit the special structure of the new relaxation. Encouragingly,\nthe proposed procedure allows solving SDR for large-scale problems, e.g.,\nproblems on a grid graph comprising hundreds of thousands of variables with\nmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable\nof attaining comparable accuracy while exhibiting remarkably improved\nscalability, in contrast to the commonly held belief that semidefinite\nrelaxation can only been applied on small-scale MRF problems. We have evaluated\nthe performance of SDR on various benchmark datasets including OPENGM2 and PIC\nin terms of both the quality of the solutions and computation time.\nExperimental results demonstrate that for a broad class of problems, SDPAD-LR\noutperforms state-of-the-art algorithms in producing better MAP assignment in\nan efficient manner.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 16:58:24 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Huang", "Qixing", ""], ["Chen", "Yuxin", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1405.4897", "submitter": "Yun Wang", "authors": "Zhen James Xiang, Yun Wang and Peter J. Ramadge", "title": "Screening Tests for Lasso Problems", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2568185", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a survey of dictionary screening for the lasso problem. The\nlasso problem seeks a sparse linear combination of the columns of a dictionary\nto best match a given target vector. This sparse representation has proven\nuseful in a variety of subsequent processing and decision tasks. For a given\ntarget vector, dictionary screening quickly identifies a subset of dictionary\ncolumns that will receive zero weight in a solution of the corresponding lasso\nproblem. These columns can be removed from the dictionary prior to solving the\nlasso problem without impacting the optimality of the solution obtained. This\nhas two potential advantages: it reduces the size of the dictionary, allowing\nthe lasso problem to be solved with less resources, and it may speed up\nobtaining a solution. Using a geometrically intuitive framework, we provide\nbasic insights for understanding useful lasso screening tests and their\nlimitations. We also provide illustrative numerical studies on several\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 21:07:08 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 22:04:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Xiang", "Zhen James", ""], ["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1405.4980", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck", "title": "Convex Optimization: Algorithms and Complexity", "comments": "A previous version of the manuscript was titled \"Theory of Convex\n  Optimization for Machine Learning\"", "journal-ref": "In Foundations and Trends in Machine Learning, Vol. 8: No. 3-4, pp\n  231-357, 2015", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph presents the main complexity theorems in convex optimization\nand their corresponding algorithms. Starting from the fundamental theory of\nblack-box optimization, the material progresses towards recent advances in\nstructural optimization and stochastic optimization. Our presentation of\nblack-box optimization, strongly influenced by Nesterov's seminal book and\nNemirovski's lecture notes, includes the analysis of cutting plane methods, as\nwell as (accelerated) gradient descent schemes. We also pay special attention\nto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror\ndescent, and dual averaging) and discuss their relevance in machine learning.\nWe provide a gentle introduction to structural optimization with FISTA (to\noptimize a sum of a smooth and a simple non-smooth term), saddle-point mirror\nprox (Nemirovski's alternative to Nesterov's smoothing), and a concise\ndescription of interior point methods. In stochastic optimization we discuss\nstochastic gradient descent, mini-batches, random coordinate descent, and\nsublinear algorithms. We also briefly touch upon convex relaxation of\ncombinatorial problems and the use of randomness to round solutions, as well as\nrandom walks based methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:50:56 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 18:52:04 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1405.5096", "submitter": "Richard Combes", "authors": "Richard Combes and Alexandre Proutiere", "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms", "comments": "ICML 2014 (technical report). arXiv admin note: text overlap with\n  arXiv:1307.7309", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandits where the expected reward is a\nunimodal function over partially ordered arms. This important class of problems\nhas been recently investigated in (Cope 2009, Yu 2011). The set of arms is\neither discrete, in which case arms correspond to the vertices of a finite\ngraph whose structure represents similarity in rewards, or continuous, in which\ncase arms belong to a bounded interval. For discrete unimodal bandits, we\nderive asymptotic lower bounds for the regret achieved under any algorithm, and\npropose OSUB, an algorithm whose regret matches this lower bound. Our algorithm\noptimally exploits the unimodal structure of the problem, and surprisingly, its\nasymptotic regret does not depend on the number of arms. We also provide a\nregret upper bound for OSUB in non-stationary environments where the expected\nrewards smoothly evolve over time. The analytical results are supported by\nnumerical experiments showing that OSUB performs significantly better than the\nstate-of-the-art algorithms. For continuous sets of arms, we provide a brief\ndiscussion. We show that combining an appropriate discretization of the set of\narms with the UCB algorithm yields an order-optimal regret, and in practice,\noutperforms recently proposed algorithms designed to exploit the unimodal\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 14:15:54 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1405.5147", "submitter": "Everaldo Aguiar", "authors": "Everaldo Aguiar, Saurabh Nagrecha, Nitesh V. Chawla", "title": "Predicting Online Video Engagement Using Clickstreams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the nascent days of e-content delivery, having a superior product was\nenough to give companies an edge against the competition. With today's fiercely\ncompetitive market, one needs to be multiple steps ahead, especially when it\ncomes to understanding consumers. Focusing on a large set of web portals owned\nand managed by a private communications company, we propose methods by which\nthese sites' clickstream data can be used to provide a deep understanding of\ntheir visitors, as well as their interests and preferences. We further expand\nthe use of this data to show that it can be effectively used to predict user\nengagement to video streams.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 16:32:59 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Aguiar", "Everaldo", ""], ["Nagrecha", "Saurabh", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1405.5156", "submitter": "Liping Liu", "authors": "Li-Ping Liu, Daniel Sheldon, Thomas G. Dietterich", "title": "Gaussian Approximation of Collective Graphical Models", "comments": "Accepted by ICML 2014. 10 page version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Collective Graphical Model (CGM) models a population of independent and\nidentically distributed individuals when only collective statistics (i.e.,\ncounts of individuals) are observed. Exact inference in CGMs is intractable,\nand previous work has explored Markov Chain Monte Carlo (MCMC) and MAP\napproximations for learning and inference. This paper studies Gaussian\napproximations to the CGM. As the population grows large, we show that the CGM\ndistribution converges to a multivariate Gaussian distribution (GCGM) that\nmaintains the conditional independence properties of the original CGM. If the\nobservations are exact marginals of the CGM or marginals that are corrupted by\nGaussian noise, inference in the GCGM approximation can be computed efficiently\nin closed form. If the observations follow a different noise model (e.g.,\nPoisson), then expectation propagation provides efficient and accurate\napproximate inference. The accuracy and speed of GCGM inference is compared to\nthe MCMC and MAP methods on a simulated bird migration problem. The GCGM\nmatches or exceeds the accuracy of the MAP method while being significantly\nfaster.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 17:12:56 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Liu", "Li-Ping", ""], ["Sheldon", "Daniel", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1405.5268", "submitter": "Andrew Wan", "authors": "Dana Dachman-Soled and Vitaly Feldman and Li-Yang Tan and Andrew Wan\n  and Karl Wimmer", "title": "Approximate resilience, monotonicity, and the complexity of agnostic\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f$ is $d$-resilient if all its Fourier coefficients of degree at\nmost $d$ are zero, i.e., $f$ is uncorrelated with all low-degree parities. We\nstudy the notion of $\\mathit{approximate}$ $\\mathit{resilience}$ of Boolean\nfunctions, where we say that $f$ is $\\alpha$-approximately $d$-resilient if $f$\nis $\\alpha$-close to a $[-1,1]$-valued $d$-resilient function in $\\ell_1$\ndistance. We show that approximate resilience essentially characterizes the\ncomplexity of agnostic learning of a concept class $C$ over the uniform\ndistribution. Roughly speaking, if all functions in a class $C$ are far from\nbeing $d$-resilient then $C$ can be learned agnostically in time $n^{O(d)}$ and\nconversely, if $C$ contains a function close to being $d$-resilient then\nagnostic learning of $C$ in the statistical query (SQ) framework of Kearns has\ncomplexity of at least $n^{\\Omega(d)}$. This characterization is based on the\nduality between $\\ell_1$ approximation by degree-$d$ polynomials and\napproximate $d$-resilience that we establish. In particular, it implies that\n$\\ell_1$ approximation by low-degree polynomials, known to be sufficient for\nagnostic learning over product distributions, is in fact necessary.\n  Focusing on monotone Boolean functions, we exhibit the existence of\nnear-optimal $\\alpha$-approximately\n$\\widetilde{\\Omega}(\\alpha\\sqrt{n})$-resilient monotone functions for all\n$\\alpha>0$. Prior to our work, it was conceivable even that every monotone\nfunction is $\\Omega(1)$-far from any $1$-resilient function. Furthermore, we\nconstruct simple, explicit monotone functions based on ${\\sf Tribes}$ and ${\\sf\nCycleRun}$ that are close to highly resilient functions. Our constructions are\nbased on a fairly general resilience analysis and amplification. These\nstructural results, together with the characterization, imply nearly optimal\nlower bounds for agnostic learning of monotone juntas.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 00:06:02 GMT"}, {"version": "v2", "created": "Wed, 9 Jul 2014 19:16:57 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Dachman-Soled", "Dana", ""], ["Feldman", "Vitaly", ""], ["Tan", "Li-Yang", ""], ["Wan", "Andrew", ""], ["Wimmer", "Karl", ""]]}, {"id": "1405.5300", "submitter": "Martin Takac", "authors": "Olivier Fercoq and Zheng Qu and Peter Richt\\'arik and Martin\n  Tak\\'a\\v{c}", "title": "Fast Distributed Coordinate Descent for Non-Strongly Convex Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient distributed randomized coordinate descent method for\nminimizing regularized non-strongly convex loss functions. The method attains\nthe optimal $O(1/k^2)$ convergence rate, where $k$ is the iteration counter.\nThe core of the work is the theoretical study of stepsize parameters. We have\nimplemented the method on Archer - the largest supercomputer in the UK - and\nshow that the method is capable of solving a (synthetic) LASSO optimization\nproblem with 50 billion variables.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 05:12:55 GMT"}, {"version": "v2", "created": "Sun, 27 Jul 2014 12:22:28 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Fercoq", "Olivier", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1405.5311", "submitter": "Atanu Ghosh KUMAR", "authors": "Atanu Kumar Ghosh, Arnab Chakraborty", "title": "Compressive Sampling Using EM Algorithm", "comments": "9 pages, 4 figures. This paper has been published as a technical\n  report in Applied Statistics Unit in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": "Technical Report No: ASU/2014/4", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches of sampling signals follow the celebrated theorem of\nNyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and\nTao, is a new paradigm that goes against the conventional methods in data\nacquisition and provides a way of recovering signals using fewer samples than\nthe traditional methods use. Here we suggest an alternative way of\nreconstructing the original signals in compressive sampling using EM algorithm.\nWe first propose a naive approach which has certain computational difficulties\nand subsequently modify it to a new approach which performs better than the\nconventional methods of compressive sampling. The comparison of the different\napproaches and the performance of the new approach has been studied using\nsimulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 06:53:16 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Ghosh", "Atanu Kumar", ""], ["Chakraborty", "Arnab", ""]]}, {"id": "1405.5358", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan and Tim Brys and Peter Vrancx and Ann Nowe", "title": "Off-Policy Shaping Ensembles in Reinforcement Learning", "comments": "Full version of the paper to appear in Proc. ECAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of gradient temporal-difference methods allow to learn\noff-policy multiple value functions in parallel with- out sacrificing\nconvergence guarantees or computational efficiency. This opens up new\npossibilities for sound ensemble techniques in reinforcement learning. In this\nwork we propose learning an ensemble of policies related through\npotential-based shaping rewards. The ensemble induces a combination policy by\nusing a voting mechanism on its components. Learning happens in real time, and\nwe empirically show the combination policy to outperform the individual\npolicies of the ensemble.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 10:20:15 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Brys", "Tim", ""], ["Vrancx", "Peter", ""], ["Nowe", "Ann", ""]]}, {"id": "1405.5488", "submitter": "Marc'Aurelio Ranzato", "authors": "Marc'Aurelio Ranzato", "title": "On Learning Where To Look", "comments": "deep learning, vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current automatic vision systems face two major challenges: scalability and\nextreme variability of appearance. First, the computational time required to\nprocess an image typically scales linearly with the number of pixels in the\nimage, therefore limiting the resolution of input images to thumbnail size.\nSecond, variability in appearance and pose of the objects constitute a major\nhurdle for robust recognition and detection. In this work, we propose a model\nthat makes baby steps towards addressing these challenges. We describe a\nlearning based method that recognizes objects through a series of glimpses.\nThis system performs an amount of computation that scales with the complexity\nof the input rather than its number of pixels. Moreover, the proposed method is\npotentially more robust to changes in appearance since its parameters are\nlearned in a data driven manner. Preliminary experiments on a handwritten\ndataset of digits demonstrate the computational advantages of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 02:29:19 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1405.5505", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur\n  Gretton, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Shrinkage Estimators", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel\nmean, is central to kernel methods in that it is used by many classical\nalgorithms such as kernel principal component analysis, and it also forms the\ncore inference step of modern kernel methods that rely on embedding probability\ndistributions in RKHSs. Given a finite sample, an empirical average has been\nused commonly as a standard estimator of the true kernel mean. Despite a\nwidespread use of this estimator, we show that it can be improved thanks to the\nwell-known Stein phenomenon. We propose a new family of estimators called\nkernel mean shrinkage estimators (KMSEs), which benefit from both theoretical\njustifications and good empirical performance. The results demonstrate that the\nproposed estimators outperform the standard one, especially in a \"large d,\nsmall n\" paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 18:17:37 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 13:01:18 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 09:28:14 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Muandet", "Krikamol", ""], ["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1405.5769", "submitter": "Philipp Fischer", "authors": "Philipp Fischer, Alexey Dosovitskiy, Thomas Brox", "title": "Descriptor Matching with Convolutional Neural Networks: a Comparison to\n  SIFT", "comments": "This paper has been merged with arXiv:1406.6909", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest results indicate that features learned via convolutional neural\nnetworks outperform previous descriptors on classification tasks by a large\nmargin. It has been shown that these networks still work well when they are\napplied to datasets or recognition tasks different from those they were trained\non. However, descriptors like SIFT are not only used in recognition but also\nfor many correspondence problems that rely on descriptor matching. In this\npaper we compare features from various layers of convolutional neural nets to\nstandard SIFT descriptors. We consider a network that was trained on ImageNet\nand another one that was trained without supervision. Surprisingly,\nconvolutional neural networks clearly outperform SIFT on descriptor matching.\nThis paper has been merged with arXiv:1406.6909\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 14:35:52 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 09:16:28 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Fischer", "Philipp", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1405.5829", "submitter": "Michele Dallachiesa", "authors": "Michele Dallachiesa and Charu Aggarwal and Themis Palpanas", "title": "Node Classification in Uncertain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real applications that use and analyze networked data, the links in\nthe network graph may be erroneous, or derived from probabilistic techniques.\nIn such cases, the node classification problem can be challenging, since the\nunreliability of the links may affect the final results of the classification\nprocess. If the information about link reliability is not used explicitly, the\nclassification accuracy in the underlying network may be affected adversely. In\nthis paper, we focus on situations that require the analysis of the uncertainty\nthat is present in the graph structure. We study the novel problem of node\nclassification in uncertain graphs, by treating uncertainty as a first-class\ncitizen. We propose two techniques based on a Bayes model and automatic\nparameter selection, and show that the incorporation of uncertainty in the\nclassification process as a first-class citizen is beneficial. We\nexperimentally evaluate the proposed approach using different real data sets,\nand study the behavior of the algorithms under different conditions. The\nresults demonstrate the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 17:13:00 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Dallachiesa", "Michele", ""], ["Aggarwal", "Charu", ""], ["Palpanas", "Themis", ""]]}, {"id": "1405.5868", "submitter": "James Atwood", "authors": "James Atwood, Don Towsley, Krista Gile, and David Jensen", "title": "Learning to Generate Networks", "comments": "Neural Information Processing Systems 2014 Workshop on Networks: From\n  Graphs to Rich Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning to generate complex networks from\ndata. Specifically, we consider whether deep belief networks, dependency\nnetworks, and members of the exponential random graph family can learn to\ngenerate networks whose complex behavior is consistent with a set of input\nexamples. We find that the deep model is able to capture the complex behavior\nof small networks, but that no model is able capture this behavior for networks\nwith more than a handful of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 19:41:51 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 18:11:10 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Atwood", "James", ""], ["Towsley", "Don", ""], ["Gile", "Krista", ""], ["Jensen", "David", ""]]}, {"id": "1405.5869", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 19:42:57 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1405.5960", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "LASS: a simple assignment model with Laplacian smoothing", "comments": "20 pages, 4 figures. A shorter version appears in AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning soft assignments of $N$ items to $K$\ncategories given two sources of information: an item-category similarity\nmatrix, which encourages items to be assigned to categories they are similar to\n(and to not be assigned to categories they are dissimilar to), and an item-item\nsimilarity matrix, which encourages similar items to have similar assignments.\nWe propose a simple quadratic programming model that captures this intuition.\nWe give necessary conditions for its solution to be unique, define an\nout-of-sample mapping, and derive a simple, effective training algorithm based\non the alternating direction method of multipliers. The model predicts\nreasonable assignments from even a few similarity values, and can be seen as a\ngeneralization of semisupervised learning. It is particularly useful when items\nnaturally belong to multiple categories, as for example when annotating\ndocuments with keywords or pictures with tags, with partially tagged items, or\nwhen the categories have complex interrelations (e.g. hierarchical) that are\nunknown.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 04:28:29 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1405.6012", "submitter": "Qi Xie", "authors": "Qi Xie, Deyu Meng, Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng\n  and Zongben Xu", "title": "On the Optimal Solution of Weighted Nuclear Norm Minimization", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the nuclear norm minimization (NNM) problem has been\nattracting much attention in computer vision and machine learning. The NNM\nproblem is capitalized on its convexity and it can be solved efficiently. The\nstandard nuclear norm regularizes all singular values equally, which is however\nnot flexible enough to fit real scenarios. Weighted nuclear norm minimization\n(WNNM) is a natural extension and generalization of NNM. By assigning properly\ndifferent weights to different singular values, WNNM can lead to\nstate-of-the-art results in applications such as image denoising. Nevertheless,\nso far the global optimal solution of WNNM problem is not completely solved yet\ndue to its non-convexity in general cases. In this article, we study the\ntheoretical properties of WNNM and prove that WNNM can be equivalently\ntransformed into a quadratic programming problem with linear constraints. This\nimplies that WNNM is equivalent to a convex problem and its global optimum can\nbe readily achieved by off-the-shelf convex optimization solvers. We further\nshow that when the weights are non-descending, the globally optimal solution of\nWNNM can be obtained in closed-form.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 10:15:04 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Xie", "Qi", ""], ["Meng", "Deyu", ""], ["Gu", "Shuhang", ""], ["Zhang", "Lei", ""], ["Zuo", "Wangmeng", ""], ["Feng", "Xiangchu", ""], ["Xu", "Zongben", ""]]}, {"id": "1405.6076", "submitter": "Chansoo Lee", "authors": "Jacob Abernethy, Chansoo Lee, Abhinav Sinha, Ambuj Tewari", "title": "Online Linear Optimization via Smoothing", "comments": "COLT 2014", "journal-ref": "JMLR 2014 W&CP", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new optimization-theoretic approach to analyzing\nFollow-the-Leader style algorithms, particularly in the setting where\nperturbations are used as a tool for regularization. We show that adding a\nstrongly convex penalty function to the decision rule and adding stochastic\nperturbations to data correspond to deterministic and stochastic smoothing\noperations, respectively. We establish an equivalence between \"Follow the\nRegularized Leader\" and \"Follow the Perturbed Leader\" up to the smoothness\nproperties. This intuition leads to a new generic analysis framework that\nrecovers and improves the previous known regret bounds of the class of\nalgorithms commonly known as Follow the Perturbed Leader.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 14:33:48 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Abernethy", "Jacob", ""], ["Lee", "Chansoo", ""], ["Sinha", "Abhinav", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1405.6137", "submitter": "Arun P V", "authors": "S.K. Katiyar and P.V. Arun", "title": "An enhanced neural network based approach towards object extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvements in spectral and spatial resolution of the satellite images\nhave facilitated the automatic extraction and identification of the features\nfrom satellite images and aerial photographs. An automatic object extraction\nmethod is presented for extracting and identifying the various objects from\nsatellite images and the accuracy of the system is verified with regard to IRS\nsatellite images. The system is based on neural network and simulates the\nprocess of visual interpretation from remote sensing images and hence increases\nthe efficiency of image analysis. This approach obtains the basic\ncharacteristics of the various features and the performance is enhanced by the\nautomatic learning approach, intelligent interpretation, and intelligent\ninterpolation. The major advantage of the method is its simplicity and that the\nsystem identifies the features not only based on pixel value but also based on\nthe shape, haralick features etc of the objects. Further the system allows\nflexibility for identifying the features within the same category based on size\nand shape. The successful application of the system verified its effectiveness\nand the accuracy of the system were assessed by ground truth verification.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 20:05:34 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Katiyar", "S. K.", ""], ["Arun", "P. V.", ""]]}, {"id": "1405.6159", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "A Bi-clustering Framework for Consensus Problems", "comments": null, "journal-ref": null, "doi": "10.1137/140967325", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider grouping as a general characterization for problems such as\nclustering, community detection in networks, and multiple parametric model\nestimation. We are interested in merging solutions from different grouping\nalgorithms, distilling all their good qualities into a consensus solution. In\nthis paper, we propose a bi-clustering framework and perspective for reaching\nconsensus in such grouping problems. In particular, this is the first time that\nthe task of finding/fitting multiple parametric models to a dataset is formally\nposed as a consensus problem. We highlight the equivalence of these tasks and\nestablish the connection with the computational Gestalt program, that seeks to\nprovide a psychologically-inspired detection theory for visual events. We also\npresent a simple but powerful bi-clustering algorithm, specially tuned to the\nnature of the problem we address, though general enough to handle many\ndifferent instances inscribed within our characterization. The presentation is\naccompanied with diverse and extensive experimental results in clustering,\ncommunity detection, and multiple parametric model estimation in image\nprocessing applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 21:58:10 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 17:44:55 GMT"}, {"version": "v3", "created": "Wed, 20 Aug 2014 22:12:15 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1405.6177", "submitter": "Md. Tarek Habib", "authors": "Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman, Farruk Ahmed", "title": "Automated Fabric Defect Inspection: A Survey of Classifiers", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.5121/ijfcst.2014.4102", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control at each stage of production in textile industry has become a\nkey factor to retaining the existence in the highly competitive global market.\nProblems of manual fabric defect inspection are lack of accuracy and high time\nconsumption, where early and accurate fabric defect detection is a significant\nphase of quality control. Computer vision based, i.e. automated fabric defect\ninspection systems are thought by many researchers of different countries to be\nvery useful to resolve these problems. There are two major challenges to be\nresolved to attain a successful automated fabric defect inspection system. They\nare defect detection and defect classification. In this work, we discuss\ndifferent techniques used for automated fabric defect classification, then show\na survey of classifiers used in automated fabric defect inspection systems, and\nfinally, compare these classifiers by using performance metrics. This work is\nexpected to be very useful for the researchers in the area of automated fabric\ndefect inspection to understand and evaluate the many potential options in this\nfield.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 20:53:43 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Habib", "Md. Tarek", ""], ["Faisal", "Rahat Hossain", ""], ["Rokonuzzaman", "M.", ""], ["Ahmed", "Farruk", ""]]}, {"id": "1405.6223", "submitter": "Fangfang Li", "authors": "Fangfang Li, Guandong Xu, Longbing Cao", "title": "Coupled Item-based Matrix Factorization", "comments": "7 pages submitted to AAAI2014. arXiv admin note: substantial text\n  overlap with arXiv:1404.7467", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essence of the challenges cold start and sparsity in Recommender Systems\n(RS) is that the extant techniques, such as Collaborative Filtering (CF) and\nMatrix Factorization (MF), mainly rely on the user-item rating matrix, which\nsometimes is not informative enough for predicting recommendations. To solve\nthese challenges, the objective item attributes are incorporated as\ncomplementary information. However, most of the existing methods for inferring\nthe relationships between items assume that the attributes are \"independently\nand identically distributed (iid)\", which does not always hold in reality. In\nfact, the attributes are more or less coupled with each other by some implicit\nrelationships. Therefore, in this pa-per we propose an attribute-based coupled\nsimilarity measure to capture the implicit relationships between items. We then\nintegrate the implicit item coupling into MF to form the Coupled Item-based\nMatrix Factorization (CIMF) model. Experimental results on two open data sets\ndemonstrate that CIMF outperforms the benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 00:42:16 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Li", "Fangfang", ""], ["Xu", "Guandong", ""], ["Cao", "Longbing", ""]]}, {"id": "1405.6341", "submitter": "Stefanos Nikolaidis", "authors": "Stefanos Nikolaidis, Keren Gu, Ramya Ramakrishnan, and Julie Shah", "title": "Efficient Model Learning for Human-Robot Collaborative Tasks", "comments": null, "journal-ref": "Proceedings of the Tenth Annual ACM/IEEE International Conference\n  on Human-Robot Interaction (HRI 2015)", "doi": "10.1145/2696454.2696455", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning human user models from joint-action\ndemonstrations that enables the robot to compute a robust policy for a\ncollaborative task with a human. The learning takes place completely\nautomatically, without any human intervention. First, we describe the\nclustering of demonstrated action sequences into different human types using an\nunsupervised learning algorithm. These demonstrated sequences are also used by\nthe robot to learn a reward function that is representative for each type,\nthrough the employment of an inverse reinforcement learning algorithm. The\nlearned model is then used as part of a Mixed Observability Markov Decision\nProcess formulation, wherein the human type is a partially observable variable.\nWith this framework, we can infer, either offline or online, the human type of\na new user that was not included in the training set, and can compute a policy\nfor the robot that will be aligned to the preference of this new user and will\nbe robust to deviations of the human actions from prior demonstrations. Finally\nwe validate the approach using data collected in human subject experiments, and\nconduct proof-of-concept demonstrations in which a person performs a\ncollaborative task with a small industrial robot.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 20:44:26 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Nikolaidis", "Stefanos", ""], ["Gu", "Keren", ""], ["Ramakrishnan", "Ramya", ""], ["Shah", "Julie", ""]]}, {"id": "1405.6434", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Lingbo Wang, Yanwen Guo", "title": "Multi-view Metric Learning for Multi-view Video Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods on video summarization are designed to generate summaries\nfor single-view video records; and thus they cannot fully exploit the\nredundancy in multi-view video records. In this paper, we present a multi-view\nmetric learning framework for multi-view video summarization that combines the\nadvantages of maximum margin clustering with the disagreement minimization\ncriterion. The learning framework thus has the ability to find a metric that\nbest separates the data, and meanwhile to force the learned metric to maintain\noriginal intrinsic information between data points, for example geometric\ninformation. Facilitated by such a framework, a systematic solution to the\nmulti-view video summarization problem is developed. To the best of our\nknowledge, it is the first time to address multi-view video summarization from\nthe viewpoint of metric learning. The effectiveness of the proposed method is\ndemonstrated by experiments.\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 22:35:19 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 22:56:21 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Wang", "Lingbo", ""], ["Guo", "Yanwen", ""]]}, {"id": "1405.6444", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Weiran Wang and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "The role of dimensionality reduction in linear classification", "comments": "15 pages, 6 figures. A shorter version appears in AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) is often used as a preprocessing step in\nclassification, but usually one first fixes the DR mapping, possibly using\nlabel information, and then learns a classifier (a filter approach). Best\nperformance would be obtained by optimizing the classification error jointly\nover DR mapping and classifier (a wrapper approach), but this is a difficult\nnonconvex problem, particularly with nonlinear DR. Using the method of\nauxiliary coordinates, we give a simple, efficient algorithm to train a\ncombination of nonlinear DR and a classifier, and apply it to a RBF mapping\nwith a linear SVM. This alternates steps where we train the RBF mapping and a\nlinear SVM as usual regression and classification, respectively, with a\nclosed-form step that coordinates both. The resulting nonlinear low-dimensional\nclassifier achieves classification errors competitive with the state-of-the-art\nbut is fast at training and testing, and allows the user to trade off runtime\nfor classification accuracy easily. We then study the role of nonlinear DR in\nlinear classification, and the interplay between the DR mapping, the number of\nlatent dimensions and the number of classes. When trained jointly, the DR\nmapping takes an extreme role in eliminating variation: it tends to collapse\nclasses in latent space, erasing all manifold structure, and lay out class\ncentroids so they are linearly separable with maximum margin.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 01:15:44 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1405.6472", "submitter": "Julien Mairal", "authors": "Yuansi Chen (EECS, INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann)", "title": "Fast and Robust Archetypal Analysis for Representation Learning", "comments": null, "journal-ref": "CVPR 2014 - IEEE Conference on Computer Vision \\& Pattern\n  Recognition (2014)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a pioneer unsupervised learning technique called archetypal\nanalysis, which is related to successful data analysis methods such as sparse\ncoding and non-negative matrix factorization. Since it was proposed, archetypal\nanalysis did not gain a lot of popularity even though it produces more\ninterpretable models than other alternatives. Because no efficient\nimplementation has ever been made publicly available, its application to\nimportant scientific problems may have been severely limited. Our goal is to\nbring back into favour archetypal analysis. We propose a fast optimization\nscheme using an active-set strategy, and provide an efficient open-source\nimplementation interfaced with Matlab, R, and Python. Then, we demonstrate the\nusefulness of archetypal analysis for computer vision tasks, such as codebook\nlearning, signal classification, and large image collection visualization.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 06:25:18 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Chen", "Yuansi", "", "EECS, INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"]]}, {"id": "1405.6524", "submitter": "Dan Stowell", "authors": "Dan Stowell and Mark D. Plumbley", "title": "Automatic large-scale classification of bird sounds is strongly improved\n  by unsupervised feature learning", "comments": null, "journal-ref": "PeerJ 2:e488, 2014", "doi": "10.7717/peerj.488", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automatic species classification of birds from their sound is a computational\ntool of increasing importance in ecology, conservation monitoring and vocal\ncommunication studies. To make classification useful in practice, it is crucial\nto improve its accuracy while ensuring that it can run at big data scales. Many\napproaches use acoustic measures based on spectrogram-type data, such as the\nMel-frequency cepstral coefficient (MFCC) features which represent a\nmanually-designed summary of spectral information. However, recent work in\nmachine learning has demonstrated that features learnt automatically from data\ncan often outperform manually-designed feature transforms. Feature learning can\nbe performed at large scale and \"unsupervised\", meaning it requires no manual\ndata labelling, yet it can improve performance on \"supervised\" tasks such as\nclassification. In this work we introduce a technique for feature learning from\nlarge volumes of bird sound recordings, inspired by techniques that have proven\nuseful in other domains. We experimentally compare twelve different feature\nrepresentations derived from the Mel spectrum (of which six use this\ntechnique), using four large and diverse databases of bird vocalisations, with\na random forest classifier. We demonstrate that MFCCs are of limited power in\nthis context, leading to worse performance than the raw Mel spectral data.\nConversely, we demonstrate that unsupervised feature learning provides a\nsubstantial boost over MFCCs and Mel spectra without adding computational\ncomplexity after the model has been trained. The boost is particularly notable\nfor single-label classification tasks at large scale. The spectro-temporal\nactivations learned through our procedure resemble spectro-temporal receptive\nfields calculated from avian primary auditory forebrain.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 09:58:20 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Stowell", "Dan", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1405.6563", "submitter": "Radu Horaud P", "authors": "Fabio Cuzzolin, Diana Mateus and Radu Horaud", "title": "Robust Temporally Coherent Laplacian Protrusion Segmentation of 3D\n  Articulated Bodies", "comments": "31 pages, 26 figures", "journal-ref": "International Journal of Computer Vision 112(1), 43-70, 2015", "doi": "10.1007/s11263-014-0754-0", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In motion analysis and understanding it is important to be able to fit a\nsuitable model or structure to the temporal series of observed data, in order\nto describe motion patterns in a compact way, and to discriminate between them.\nIn an unsupervised context, i.e., no prior model of the moving object(s) is\navailable, such a structure has to be learned from the data in a bottom-up\nfashion. In recent times, volumetric approaches in which the motion is captured\nfrom a number of cameras and a voxel-set representation of the body is built\nfrom the camera views, have gained ground due to attractive features such as\ninherent view-invariance and robustness to occlusions. Automatic, unsupervised\nsegmentation of moving bodies along entire sequences, in a temporally-coherent\nand robust way, has the potential to provide a means of constructing a\nbottom-up model of the moving body, and track motion cues that may be later\nexploited for motion classification. Spectral methods such as locally linear\nembedding (LLE) can be useful in this context, as they preserve \"protrusions\",\ni.e., high-curvature regions of the 3D volume, of articulated shapes, while\nimproving their separation in a lower dimensional space, making them in this\nway easier to cluster. In this paper we therefore propose a spectral approach\nto unsupervised and temporally-coherent body-protrusion segmentation along time\nsequences. Volumetric shapes are clustered in an embedding space, clusters are\npropagated in time to ensure coherence, and merged or split to accommodate\nchanges in the body's topology. Experiments on both synthetic and real\nsequences of dense voxel-set data are shown. This supports the ability of the\nproposed method to cluster body-parts consistently over time in a totally\nunsupervised fashion, its robustness to sampling density and shape quality, and\nits potential for bottom-up model construction\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 13:12:05 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Cuzzolin", "Fabio", ""], ["Mateus", "Diana", ""], ["Horaud", "Radu", ""]]}, {"id": "1405.6642", "submitter": "Guang Cheng", "authors": "Wei Sun (Yahoo Labs), Xingye Qiao (Binghamton) and Guang Cheng\n  (Purdue)", "title": "Stabilized Nearest Neighbor Classifier and Its Statistical Properties", "comments": "48 Pages, 11 Figures. To Appear in JASA--T&M", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stability of statistical analysis is an important indicator for\nreproducibility, which is one main principle of scientific method. It entails\nthat similar statistical conclusions can be reached based on independent\nsamples from the same underlying population. In this paper, we introduce a\ngeneral measure of classification instability (CIS) to quantify the sampling\nvariability of the prediction made by a classification method. Interestingly,\nthe asymptotic CIS of any weighted nearest neighbor classifier turns out to be\nproportional to the Euclidean norm of its weight vector. Based on this concise\nform, we propose a stabilized nearest neighbor (SNN) classifier, which\ndistinguishes itself from other nearest neighbor classifiers, by taking the\nstability into consideration. In theory, we prove that SNN attains the minimax\noptimal convergence rate in risk, and a sharp convergence rate in CIS. The\nlatter rate result is established for general plug-in classifiers under a\nlow-noise condition. Extensive simulated and real examples demonstrate that SNN\nachieves a considerable improvement in CIS over existing nearest neighbor\nclassifiers, with comparable classification accuracy. We implement the\nalgorithm in a publicly available R package snn.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 17:07:10 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 18:56:05 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Sun", "Wei", "", "Yahoo Labs"], ["Qiao", "Xingye", "", "Binghamton"], ["Cheng", "Guang", "", "Purdue"]]}, {"id": "1405.6664", "submitter": "Andreas Tillmann", "authors": "Andreas M. Tillmann", "title": "On the Computational Intractability of Exact and Approximate Dictionary\n  Learning", "comments": "5 pages; accepted for publication", "journal-ref": null, "doi": "10.1109/LSP.2014.2345761", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient sparse coding and reconstruction of signal vectors via linear\nobservations has received a tremendous amount of attention over the last\ndecade. In this context, the automated learning of a suitable basis or\novercomplete dictionary from training data sets of certain signal classes for\nuse in sparse representations has turned out to be of particular importance\nregarding practical signal processing applications. Most popular dictionary\nlearning algorithms involve NP-hard sparse recovery problems in each iteration,\nwhich may give some indication about the complexity of dictionary learning but\ndoes not constitute an actual proof of computational intractability. In this\ntechnical note, we show that learning a dictionary with which a given set of\ntraining signals can be represented as sparsely as possible is indeed NP-hard.\nMoreover, we also establish hardness of approximating the solution to within\nlarge factors of the optimal sparsity level. Furthermore, we give NP-hardness\nand non-approximability results for a recent dictionary learning variation\ncalled the sensor permutation problem. Along the way, we also obtain a new\nnon-approximability result for the classical sparse recovery problem from\ncompressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:05:18 GMT"}, {"version": "v2", "created": "Sun, 3 Aug 2014 23:00:22 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tillmann", "Andreas M.", ""]]}, {"id": "1405.6676", "submitter": "Philippe Besse", "authors": "Philippe Besse (IMT), Nathalie Villa-Vialaneix (MIAT INRA)", "title": "Statistique et Big Data Analytics; Volum\\'etrie, L'Attaque des Clones", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article assumes acquired the skills and expertise of a statistician in\nunsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,\nrandom forest). What skills and knowledge do a statistician must acquire to\nreach the \"Volume\" scale of big data? After a quick overview of the different\nstrategies available and especially of those imposed by Hadoop, the algorithms\nof some available learning methods are outlined in order to understand how they\nare adapted to the strong stresses of the Map-Reduce functionalities\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:44:11 GMT"}, {"version": "v2", "created": "Sun, 5 Oct 2014 06:28:45 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Besse", "Philippe", "", "IMT"], ["Villa-Vialaneix", "Nathalie", "", "MIAT INRA"]]}, {"id": "1405.6684", "submitter": "Piotr Plonski", "authors": "Piotr P{\\l}o\\'nski and Krzysztof Zaremba", "title": "Visualizing Random Forest with Self-Organising Map", "comments": null, "journal-ref": "Lecture Notes in Computer Science Volume 8468, 2014, pp 63-71", "doi": "10.1007/978-3-319-07176-3_6", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest (RF) is a powerful ensemble method for classification and\nregression tasks. It consists of decision trees set. Although, a single tree is\nwell interpretable for human, the ensemble of trees is a black-box model. The\npopular technique to look inside the RF model is to visualize a RF proximity\nmatrix obtained on data samples with Multidimensional Scaling (MDS) method.\nHerein, we present a novel method based on Self-Organising Maps (SOM) for\nrevealing intrinsic relationships in data that lay inside the RF used for\nclassification tasks. We propose an algorithm to learn the SOM with the\nproximity matrix obtained from the RF. The visualization of RF proximity matrix\nwith MDS and SOM is compared. What is more, the SOM learned with the RF\nproximity matrix has better classification accuracy in comparison to SOM\nlearned with Euclidean distance. Presented approach enables better\nunderstanding of the RF and additionally improves accuracy of the SOM.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 19:00:15 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["P\u0142o\u0144ski", "Piotr", ""], ["Zaremba", "Krzysztof", ""]]}, {"id": "1405.6757", "submitter": "Sridhar Mahadevan", "authors": "Sridhar Mahadevan, Bo Liu, Philip Thomas, Will Dabney, Steve Giguere,\n  Nicholas Jacek, Ian Gemp, Ji Liu", "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision\n  Making in Primal-Dual Spaces", "comments": "121 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we set forth a new vision of reinforcement learning developed\nby us over the past few years, one that yields mathematically rigorous\nsolutions to longstanding important questions that have remained unresolved:\n(i) how to design reliable, convergent, and robust reinforcement learning\nalgorithms (ii) how to guarantee that reinforcement learning satisfies\npre-specified \"safety\" guarantees, and remains in a stable region of the\nparameter space (iii) how to design \"off-policy\" temporal difference learning\nalgorithms in a reliable and stable manner, and finally (iv) how to integrate\nthe study of reinforcement learning into the rich theory of stochastic\noptimization. In this paper, we provide detailed answers to all these questions\nusing the powerful framework of proximal operators.\n  The key idea that emerges is the use of primal dual spaces connected through\nthe use of a Legendre transform. This allows temporal difference updates to\noccur in dual spaces, allowing a variety of important technical advantages. The\nLegendre transform elegantly generalizes past algorithms for solving\nreinforcement learning problems, such as natural gradient methods, which we\nshow relate closely to the previously unconnected framework of mirror descent\nmethods. Equally importantly, proximal operator theory enables the systematic\ndevelopment of operator splitting methods that show how to safely and reliably\ndecompose complex products of gradients that occur in recent variants of\ngradient-based temporal difference learning. This key technical innovation\nmakes it possible to finally design \"true\" stochastic gradient methods for\nreinforcement learning. Finally, Legendre transforms enable a variety of other\nbenefits, including modeling sparsity and domain geometry. Our work builds\nextensively on recent work on the convergence of saddle-point algorithms, and\non the theory of monotone operators.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 23:11:40 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Mahadevan", "Sridhar", ""], ["Liu", "Bo", ""], ["Thomas", "Philip", ""], ["Dabney", "Will", ""], ["Giguere", "Steve", ""], ["Jacek", "Nicholas", ""], ["Gemp", "Ian", ""], ["Liu", "Ji", ""]]}, {"id": "1405.6791", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Pravesh Kothari", "title": "Agnostic Learning of Disjunctions on Symmetric Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 05:33:19 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 21:58:56 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""]]}, {"id": "1405.6804", "submitter": "Zhuowen Tu", "authors": "Zhuowen Tu and Piotr Dollar and Yingnian Wu", "title": "Layered Logic Classifiers: Exploring the `And' and `Or' Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective and efficient classifier for pattern analysis is a key\nproblem in machine learning and computer vision. Many the solutions to the\nproblem require to perform logic operations such as `and', `or', and `not'.\nClassification and regression tree (CART) include these operations explicitly.\nOther methods such as neural networks, SVM, and boosting learn/compute a\nweighted sum on features (weak classifiers), which weakly perform the 'and' and\n'or' operations. However, it is hard for these classifiers to deal with the\n'xor' pattern directly. In this paper, we propose layered logic classifiers for\npatterns of complicated distributions by combining the `and', `or', and `not'\noperations. The proposed algorithm is very general and easy to implement. We\ntest the classifiers on several typical datasets from the Irvine repository and\ntwo challenging vision applications, object segmentation and pedestrian\ndetection. We observe significant improvements on all the datasets over the\nwidely used decision stump based AdaBoost algorithm. The resulting classifiers\nhave much less training complexity than decision tree based AdaBoost, and can\nbe applied in a wide range of domains.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:29:01 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 00:51:08 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Tu", "Zhuowen", ""], ["Dollar", "Piotr", ""], ["Wu", "Yingnian", ""]]}, {"id": "1405.6914", "submitter": "Ivan Ivek", "authors": "Ivan Ivek", "title": "Supervised Dictionary Learning by a Variational Bayesian Group Sparse\n  Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) with group sparsity constraints is\nformulated as a probabilistic graphical model and, assuming some observed data\nhave been generated by the model, a feasible variational Bayesian algorithm is\nderived for learning model parameters. When used in a supervised learning\nscenario, NMF is most often utilized as an unsupervised feature extractor\nfollowed by classification in the obtained feature subspace. Having mapped the\nclass labels to a more general concept of groups which underlie sparsity of the\ncoefficients, what the proposed group sparse NMF model allows is incorporating\nclass label information to find low dimensional label-driven dictionaries which\nnot only aim to represent the data faithfully, but are also suitable for class\ndiscrimination. Experiments performed in face recognition and facial expression\nrecognition domains point to advantages of classification in such label-driven\nfeature subspaces over classification in feature subspaces obtained in an\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 14:02:45 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Ivek", "Ivan", ""]]}, {"id": "1405.6922", "submitter": "Omid Aghazadeh", "authors": "Omid Aghazadeh and Stefan Carlsson", "title": "Large Scale, Large Margin Classification using Indefinite Similarity\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of the popular kernelized support vector machines, they\nhave two major limitations: they are restricted to Positive Semi-Definite (PSD)\nkernels, and their training complexity scales at least quadratically with the\nsize of the data. Many natural measures of similarity between pairs of samples\nare not PSD e.g. invariant kernels, and those that are implicitly or explicitly\ndefined by latent variable models. In this paper, we investigate scalable\napproaches for using indefinite similarity measures in large margin frameworks.\nIn particular we show that a normalization of similarity to a subset of the\ndata points constitutes a representation suitable for linear classifiers. The\nresult is a classifier which is competitive to kernelized SVM in terms of\naccuracy, despite having better training and test time complexities.\nExperimental results demonstrate that on CIFAR-10 dataset, the model equipped\nwith similarity measures invariant to rigid and non-rigid deformations, can be\nmade more than 5 times sparser while being more accurate than kernelized SVM\nusing RBF kernels.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 14:18:26 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Aghazadeh", "Omid", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1405.6974", "submitter": "Max Kuhn", "authors": "Max Kuhn", "title": "Futility Analysis in the Cross-Validation of Machine Learning Models", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning models have important structural tuning parameters that\ncannot be directly estimated from the data. The common tactic for setting these\nparameters is to use resampling methods, such as cross--validation or the\nbootstrap, to evaluate a candidate set of values and choose the best based on\nsome pre--defined criterion. Unfortunately, this process can be time consuming.\nHowever, the model tuning process can be streamlined by adaptively resampling\ncandidate values so that settings that are clearly sub-optimal can be\ndiscarded. The notion of futility analysis is introduced in this context. An\nexample is shown that illustrates how adaptive resampling can be used to reduce\ntraining time. Simulation studies are used to understand how the potential\nspeed--up is affected by parallel processing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 16:52:49 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Kuhn", "Max", ""]]}, {"id": "1405.7085", "submitter": "Raef Bassily", "authors": "Raef Bassily, Adam Smith, Abhradeep Thakurta", "title": "Differentially Private Empirical Risk Minimization: Efficient Algorithms\n  and Tight Error Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a systematic investigation of differentially\nprivate algorithms for convex empirical risk minimization. Various\ninstantiations of this problem have been studied before. We provide new\nalgorithms and matching lower bounds for private ERM assuming only that each\ndata point's contribution to the loss function is Lipschitz bounded and that\nthe domain of optimization is bounded. We provide a separate set of algorithms\nand matching lower bounds for the setting in which the loss functions are known\nto also be strongly convex.\n  Our algorithms run in polynomial time, and in some cases even match the\noptimal non-private running time (as measured by oracle complexity). We give\nseparate algorithms (and lower bounds) for $(\\epsilon,0)$- and\n$(\\epsilon,\\delta)$-differential privacy; perhaps surprisingly, the techniques\nused for designing optimal algorithms in the two cases are completely\ndifferent.\n  Our lower bounds apply even to very simple, smooth function families, such as\nlinear and quadratic functions. This implies that algorithms from previous work\ncan be used to obtain optimal error rates, under the additional assumption that\nthe contributions of each data point to the loss function is smooth. We show\nthat simple approaches to smoothing arbitrary loss functions (in order to apply\nprevious techniques) do not yield optimal error rates. In particular, optimal\nalgorithms were not previously known for problems such as training support\nvector machines and the high-dimensional median.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 22:58:26 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 23:49:13 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1405.7292", "submitter": "Michael Smith", "authors": "Michael R. Smith and Andrew White and Christophe Giraud-Carrier and\n  Tony Martinez", "title": "An Easy to Use Repository for Comparing and Improving Machine Learning\n  Algorithm Usage", "comments": "7 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results from most machine learning experiments are used for a specific\npurpose and then discarded. This results in a significant loss of information\nand requires rerunning experiments to compare learning algorithms. This also\nrequires implementation of another algorithm for comparison, that may not\nalways be correctly implemented. By storing the results from previous\nexperiments, machine learning algorithms can be compared easily and the\nknowledge gained from them can be used to improve their performance. The\npurpose of this work is to provide easy access to previous experimental results\nfor learning and comparison. These stored results are comprehensive -- storing\nthe prediction for each test instance as well as the learning algorithm,\nhyperparameters, and training set that were used. Previous results are\nparticularly important for meta-learning, which, in a broad sense, is the\nprocess of learning from previous machine learning results such that the\nlearning process is improved. While other experiment databases do exist, one of\nour focuses is on easy access to the data. We provide meta-learning data sets\nthat are ready to be downloaded for meta-learning experiments. In addition,\nqueries to the underlying database can be made if specific information is\ndesired. We also differ from previous experiment databases in that our\ndatabases is designed at the instance level, where an instance is an example in\na data set. We store the predictions of a learning algorithm trained on a\nspecific training set for each instance in the test set. Data set level\ninformation can then be obtained by aggregating the results from the instances.\nThe instance level information can be used for many tasks such as determining\nthe diversity of a classifier or algorithmically determining the optimal subset\nof training instances for a learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 16:08:32 GMT"}, {"version": "v2", "created": "Thu, 5 Jun 2014 15:47:26 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Smith", "Michael R.", ""], ["White", "Andrew", ""], ["Giraud-Carrier", "Christophe", ""], ["Martinez", "Tony", ""]]}, {"id": "1405.7430", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization,\n  Experimental Design and Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BayesOpt is a library with state-of-the-art Bayesian optimization methods to\nsolve nonlinear optimization, stochastic bandits or sequential experimental\ndesign problems. Bayesian optimization is sample efficient by building a\nposterior distribution to capture the evidence and prior knowledge for the\ntarget function. Built in standard C++, the library is extremely efficient\nwhile being portable and flexible. It includes a common interface for C, C++,\nPython, Matlab and Octave.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 00:37:28 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1405.7460", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya and Ashkan Jafarpour and Alon Orlitsky and Ananda\n  Theertha Suresh", "title": "Universal Compression of Envelope Classes: Tight Characterization via\n  Poisson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson-sampling technique eliminates dependencies among symbol\nappearances in a random sequence. It has been used to simplify the analysis and\nstrengthen the performance guarantees of randomized algorithms. Applying this\nmethod to universal compression, we relate the redundancies of fixed-length and\nPoisson-sampled sequences, use the relation to derive a simple single-letter\nformula that approximates the redundancy of any envelope class to within an\nadditive logarithmic term. As a first application, we consider i.i.d.\ndistributions over a small alphabet as a step-envelope class, and provide a\nshort proof that determines the redundancy of discrete distributions over a\nsmall al- phabet up to the first order terms. We then show the strength of our\nmethod by applying the formula to tighten the existing bounds on the redundancy\nof exponential and power-law classes, in particular answering a question posed\nby Boucheron, Garivier and Gassiat.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 04:35:51 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Acharya", "Jayadev", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1405.7471", "submitter": "Dibya Jyoti Bora", "authors": "Mr. Dibya Jyoti Bora, Dr. Anil Kumar Gupta", "title": "Effect of Different Distance Measures on the Performance of K-Means\n  Algorithm: An Experimental Study in Matlab", "comments": "6 pages, 11 figures, Clustering, K Means", "journal-ref": "International Journal of Computer Science and Information\n  Technologies,(IJCSIT), Vol. 5 (2) , 2014, 2501-2506,ISSN 0975-9646", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means algorithm is a very popular clustering algorithm which is famous for\nits simplicity. Distance measure plays a very important rule on the performance\nof this algorithm. We have different distance measure techniques available. But\nchoosing a proper technique for distance calculation is totally dependent on\nthe type of the data that we are going to cluster. In this paper an\nexperimental study is done in Matlab to cluster the iris and wine data sets\nwith different distance measures and thereby observing the variation of the\nperformances shown.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 05:59:26 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Bora", "Mr. Dibya Jyoti", ""], ["Gupta", "Dr. Anil Kumar", ""]]}, {"id": "1405.7624", "submitter": "Billy Peralta", "authors": "Billy Peralta", "title": "Simultaneous Feature and Expert Selection within Mixture of Experts", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful strategy to deal with complex classification scenarios is the\n\"divide and conquer\" approach. The mixture of experts (MOE) technique makes use\nof this strategy by joinly training a set of classifiers, or experts, that are\nspecialized in different regions of the input space. A global model, or gate\nfunction, complements the experts by learning a function that weights their\nrelevance in different parts of the input space. Local feature selection\nappears as an attractive alternative to improve the specialization of experts\nand gate function, particularly, for the case of high dimensional data. Our\nmain intuition is that particular subsets of dimensions, or subspaces, are\nusually more appropriate to classify instances located in different regions of\nthe input space. Accordingly, this work contributes with a regularized variant\nof MoE that incorporates an embedded process for local feature selection using\n$L1$ regularization, with a simultaneous expert selection. The experiments are\nstill pending.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 17:32:29 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Peralta", "Billy", ""]]}, {"id": "1405.7713", "submitter": "Sophia Katrenko", "authors": "Sophia Katrenko, Pieter Adriaans, Maarten van Someren", "title": "Using Local Alignments for Relation Recognition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  1-48, 2010", "doi": "10.1613/jair.2964", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of marrying structural similarity with\nsemantic relatedness for Information Extraction from text. Aiming at accurate\nrecognition of relations, we introduce local alignment kernels and explore\nvarious possibilities of using them for this task. We give a definition of a\nlocal alignment (LA) kernel based on the Smith-Waterman score as a sequence\nsimilarity measure and proceed with a range of possibilities for computing\nsimilarity between elements of sequences. We show how distributional similarity\nmeasures obtained from unlabeled data can be incorporated into the learning\ntask as semantic knowledge. Our experiments suggest that the LA kernel yields\npromising results on various biomedical corpora outperforming two baselines by\na large margin. Additional series of experiments have been conducted on the\ndata sets of seven general relation types, where the performance of the LA\nkernel is comparable to the current state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:51:47 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Katrenko", "Sophia", ""], ["Adriaans", "Pieter", ""], ["van Someren", "Maarten", ""]]}, {"id": "1405.7752", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Michal Valko", "title": "Learning to Act Greedily: Polymatroid Semi-Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important optimization problems, such as the minimum spanning tree and\nminimum-cost flow, can be solved optimally by a greedy method. In this work, we\nstudy a learning variant of these problems, where the model of the problem is\nunknown and has to be learned by interacting repeatedly with the environment in\nthe bandit setting. We formalize our learning problem quite generally, as\nlearning how to maximize an unknown modular function on a known polymatroid. We\npropose a computationally efficient algorithm for solving our problem and bound\nits expected cumulative regret. Our gap-dependent upper bound is tight up to a\nconstant and our gap-free upper bound is tight up to polylogarithmic factors.\nFinally, we evaluate our method on three problems and demonstrate that it is\npractical.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 00:35:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 21:26:40 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 10:13:34 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Valko", "Michal", ""]]}, {"id": "1405.7764", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula and Cynthia Rudin", "title": "Generalization Bounds for Learning with Linear, Polygonal, Quadratic and\n  Conic Side Knowledge", "comments": "37 pages, 3 figures, a shorter version appeared in ISAIM 2014 (new\n  additions include a reference change and a new figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a supervised learning setting where side knowledge\nis provided about the labels of unlabeled examples. The side knowledge has the\neffect of reducing the hypothesis space, leading to tighter generalization\nbounds, and thus possibly better generalization. We consider several types of\nside knowledge, the first leading to linear and polygonal constraints on the\nhypothesis space, the second leading to quadratic constraints, and the last\nleading to conic constraints. We show how different types of domain knowledge\ncan lead directly to these kinds of side knowledge. We prove bounds on\ncomplexity measures of the hypothesis space for quadratic and conic side\nknowledge, and show that these bounds are tight in a specific sense for the\nquadratic case.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 02:05:37 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 00:58:03 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 16:45:06 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1405.7897", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain", "title": "Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1", "comments": "Supplementary material for B. Jain. Flip-Flop Sublinear Models for\n  Graphs, S+SSPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that there is no class-dual for almost all sublinear models on\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 15:50:28 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Jain", "Brijnesh", ""]]}, {"id": "1405.7908", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Semantic Composition and Decomposition: From Recognition to Generation", "comments": "National Research Council Canada - Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic composition is the task of understanding the meaning of text by\ncomposing the meanings of the individual words in the text. Semantic\ndecomposition is the task of understanding the meaning of an individual word by\ndecomposing it into various aspects (factors, constituents, components) that\nare latent in the meaning of the word. We take a distributional approach to\nsemantics, in which a word is represented by a context vector. Much recent work\nhas considered the problem of recognizing compositions and decompositions, but\nwe tackle the more difficult generation problem. For simplicity, we focus on\nnoun-modifier bigrams and noun unigrams. A test for semantic composition is,\ngiven context vectors for the noun and modifier in a noun-modifier bigram (\"red\nsalmon\"), generate a noun unigram that is synonymous with the given bigram\n(\"sockeye\"). A test for semantic decomposition is, given a context vector for a\nnoun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous\nwith the given unigram (\"brandy glass\"). With a vocabulary of about 73,000\nunigrams from WordNet, there are 73,000 candidate unigram compositions for a\nbigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a\nunigram. We generate ranked lists of potential solutions in two passes. A fast\nunsupervised learning algorithm generates an initial list of candidates and\nthen a slower supervised learning algorithm refines the list. We evaluate the\ncandidate solutions by comparing them to WordNet synonym sets. For\ndecomposition (unigram to bigram), the top 100 most highly ranked bigrams\ninclude a WordNet synonym of the given unigram 50.7% of the time. For\ncomposition (bigram to unigram), the top 100 most highly ranked unigrams\ninclude a WordNet synonym of the given bigram 77.8% of the time.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 16:36:07 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1405.7910", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis and David P. Woodruff", "title": "Optimal CUR Matrix Decompositions", "comments": "small revision in lemma 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR decomposition of an $m \\times n$ matrix $A$ finds an $m \\times c$\nmatrix $C$ with a subset of $c < n$ columns of $A,$ together with an $r \\times\nn$ matrix $R$ with a subset of $r < m$ rows of $A,$ as well as a $c \\times r$\nlow-rank matrix $U$ such that the matrix $C U R$ approximates the matrix $A,$\nthat is, $ || A - CUR ||_F^2 \\le (1+\\epsilon) || A - A_k||_F^2$, where\n$||.||_F$ denotes the Frobenius norm and $A_k$ is the best $m \\times n$ matrix\nof rank $k$ constructed via the SVD. We present input-sparsity-time and\ndeterministic algorithms for constructing such a CUR decomposition where\n$c=O(k/\\epsilon)$ and $r=O(k/\\epsilon)$ and rank$(U) = k$. Up to constant\nfactors, our algorithms are simultaneously optimal in $c, r,$ and rank$(U)$.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 16:44:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 14:53:44 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Boutsidis", "Christos", ""], ["Woodruff", "David P.", ""]]}]