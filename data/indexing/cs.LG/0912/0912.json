[{"id": "0912.0071", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate", "title": "Differentially Private Empirical Risk Minimization", "comments": "40 pages, 7 figures, accepted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving machine learning algorithms are crucial for the\nincreasingly common setting in which personal data, such as medical or\nfinancial records, are analyzed. We provide general techniques to produce\nprivacy-preserving approximations of classifiers learned via (regularized)\nempirical risk minimization (ERM). These algorithms are private under the\n$\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we\napply the output perturbation ideas of Dwork et al. (2006), to ERM\nclassification. Then we propose a new method, objective perturbation, for\nprivacy-preserving machine learning algorithm design. This method entails\nperturbing the objective function before optimizing over classifiers. If the\nloss and regularizer satisfy certain convexity and differentiability criteria,\nwe prove theoretical results showing that our algorithms preserve privacy, and\nprovide generalization bounds for linear and nonlinear kernels. We further\npresent a privacy-preserving technique for tuning the parameters in general\nmachine learning algorithms, thereby providing end-to-end privacy guarantees\nfor the training process. We apply these results to produce privacy-preserving\nanalogues of regularized logistic regression and support vector machines. We\nobtain encouraging results from evaluating their performance on real\ndemographic and benchmark data sets. Our results show that both theoretically\nand empirically, objective perturbation is superior to the previous\nstate-of-the-art, output perturbation, in managing the inherent tradeoff\nbetween privacy and learning performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2009 04:35:44 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2010 19:33:44 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2010 05:26:22 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2010 23:16:36 GMT"}, {"version": "v5", "created": "Wed, 16 Feb 2011 22:35:55 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Monteleoni", "Claire", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "0912.0086", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri, Sanjoy Dasgupta, Andrea Vattani", "title": "Learning Mixtures of Gaussians using the k-means Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most popular algorithms for clustering in Euclidean space is the\n$k$-means algorithm; $k$-means is difficult to analyze mathematically, and few\ntheoretical guarantees are known about it, particularly when the data is {\\em\nwell-clustered}. In this paper, we attempt to fill this gap in the literature\nby analyzing the behavior of $k$-means on well-clustered data. In particular,\nwe study the case when each cluster is distributed as a different Gaussian --\nor, in other words, when the input comes from a mixture of Gaussians.\n  We analyze three aspects of the $k$-means algorithm under this assumption.\nFirst, we show that when the input comes from a mixture of two spherical\nGaussians, a variant of the 2-means algorithm successfully isolates the\nsubspace containing the means of the mixture components. Second, we show an\nexact expression for the convergence of our variant of the 2-means algorithm,\nwhen the input is a very large number of samples from a mixture of spherical\nGaussians. Our analysis does not require any lower bound on the separation\nbetween the mixture components.\n  Finally, we study the sample requirement of $k$-means; for a mixture of 2\nspherical Gaussians, we show an upper bound on the number of samples required\nby a variant of 2-means to get close to the true solution. The sample\nrequirement grows with increasing dimensionality of the data, and decreasing\nseparation between the means of the Gaussians. To match our upper bound, we\nshow an information-theoretic lower bound on any algorithm that learns mixtures\nof two spherical Gaussians; our lower bound indicates that in the case when the\noverlap between the probability masses of the two distributions is small, the\nsample requirement of $k$-means is {\\em near-optimal}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2009 19:10:46 GMT"}], "update_date": "2009-12-02", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Dasgupta", "Sanjoy", ""], ["Vattani", "Andrea", ""]]}, {"id": "0912.0572", "submitter": "Mingyu Fan", "authors": "Mingyu Fan, Hong Qiao, and Bo Zhang", "title": "Isometric Multi-Manifolds Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isometric feature mapping (Isomap) is a promising manifold learning method.\nHowever, Isomap fails to work on data which distribute on clusters in a single\nmanifold or manifolds. Many works have been done on extending Isomap to\nmulti-manifolds learning. In this paper, we first proposed a new\nmulti-manifolds learning algorithm (M-Isomap) with help of a general procedure.\nThe new algorithm preserves intra-manifold geodesics and multiple\ninter-manifolds edges precisely. Compared with previous methods, this algorithm\ncan isometrically learn data distributed on several manifolds. Secondly, the\noriginal multi-cluster manifold learning algorithm first proposed in\n\\cite{DCIsomap} and called D-C Isomap has been revised so that the revised D-C\nIsomap can learn multi-manifolds data. Finally, the features and effectiveness\nof the proposed multi-manifolds learning algorithms are demonstrated and\ncompared through experiments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2009 03:05:59 GMT"}], "update_date": "2009-12-04", "authors_parsed": [["Fan", "Mingyu", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""]]}, {"id": "0912.0779", "submitter": "Hartmut Neven", "authors": "Hartmut Neven, Vasil S. Denchev, Geordie Rose, William G. Macready", "title": "Training a Large Scale Classifier with the Quantum Adiabatic Algorithm", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous publication we proposed discrete global optimization as a\nmethod to train a strong binary classifier constructed as a thresholded sum\nover weak classifiers. Our motivation was to cast the training of a classifier\ninto a format amenable to solution by the quantum adiabatic algorithm. Applying\nadiabatic quantum computing (AQC) promises to yield solutions that are superior\nto those which can be achieved with classical heuristic solvers. Interestingly\nwe found that by using heuristic solvers to obtain approximate solutions we\ncould already gain an advantage over the standard method AdaBoost. In this\ncommunication we generalize the baseline method to large scale classifier\ntraining. By large scale we mean that either the cardinality of the dictionary\nof candidate weak classifiers or the number of weak learners used in the strong\nclassifier exceed the number of variables that can be handled effectively in a\nsingle global optimization. For such situations we propose an iterative and\npiecewise approach in which a subset of weak classifiers is selected in each\niteration via global optimization. The strong classifier is then constructed by\nconcatenating the subsets of weak classifiers. We show in numerical studies\nthat the generalized method again successfully competes with AdaBoost. We also\nprovide theoretical arguments as to why the proposed optimization method, which\ndoes not only minimize the empirical loss but also adds L0-norm regularization,\nis superior to versions of boosting that only minimize the empirical loss. By\nconducting a Quantum Monte Carlo simulation we gather evidence that the quantum\nadiabatic algorithm is able to handle a generic training problem efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2009 06:30:27 GMT"}], "update_date": "2009-12-07", "authors_parsed": [["Neven", "Hartmut", ""], ["Denchev", "Vasil S.", ""], ["Rose", "Geordie", ""], ["Macready", "William G.", ""]]}, {"id": "0912.1007", "submitter": "Rdv Ijcsis", "authors": "Mehdi Salkhordeh Haghighi, Hadi Sadoghi Yazdi, Abedin Vahedian, Hamed\n  Modaghegh", "title": "Designing Kernel Scheme for Classifiers Fusion", "comments": "7 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 239-248, November 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a special fusion method for combining ensembles of\nbase classifiers utilizing new neural networks in order to improve overall\nefficiency of classification. While ensembles are designed such that each\nclassifier is trained independently while the decision fusion is performed as a\nfinal procedure, in this method, we would be interested in making the fusion\nprocess more adaptive and efficient. This new combiner, called Neural Network\nKernel Least Mean Square1, attempts to fuse outputs of the ensembles of\nclassifiers. The proposed Neural Network has some special properties such as\nKernel abilities,Least Mean Square features, easy learning over variants of\npatterns and traditional neuron capabilities. Neural Network Kernel Least Mean\nSquare is a special neuron which is trained with Kernel Least Mean Square\nproperties. This new neuron is used as a classifiers combiner to fuse outputs\nof base neural network classifiers. Performance of this method is analyzed and\ncompared with other fusion methods. The analysis represents higher performance\nof our new method as opposed to others.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2009 12:41:40 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Haghighi", "Mehdi Salkhordeh", ""], ["Yazdi", "Hadi Sadoghi", ""], ["Vahedian", "Abedin", ""], ["Modaghegh", "Hamed", ""]]}, {"id": "0912.1009", "submitter": "Rdv Ijcsis", "authors": "V.K.Panchal, Parminder Singh, Navdeep Kaur, Harish Kundra", "title": "Biogeography based Satellite Image Classification", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 269-274, November 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biogeography is the study of the geographical distribution of biological\norganisms. The mindset of the engineer is that we can learn from nature.\nBiogeography Based Optimization is a burgeoning nature inspired technique to\nfind the optimal solution of the problem. Satellite image classification is an\nimportant task because it is the only way we can know about the land cover map\nof inaccessible areas. Though satellite images have been classified in past by\nusing various techniques, the researchers are always finding alternative\nstrategies for satellite image classification so that they may be prepared to\nselect the most appropriate technique for the feature extraction task in hand.\nThis paper is focused on classification of the satellite image of a particular\nland cover using the theory of Biogeography based Optimization. The original\nBBO algorithm does not have the inbuilt property of clustering which is\nrequired during image classification. Hence modifications have been proposed to\nthe original algorithm and the modified algorithm is used to classify the\nsatellite image of a given region. The results indicate that highly accurate\nland cover features can be extracted effectively when the proposed algorithm is\nused.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2009 12:54:24 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Panchal", "V. K.", ""], ["Singh", "Parminder", ""], ["Kaur", "Navdeep", ""], ["Kundra", "Harish", ""]]}, {"id": "0912.1014", "submitter": "Rdv Ijcsis", "authors": "Shailendra Singh, Sanjay Silakari", "title": "An ensemble approach for feature selection of Cyber Attack Dataset", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 297-302, November 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an indispensable preprocessing step when mining huge\ndatasets that can significantly improve the overall system performance.\nTherefore in this paper we focus on a hybrid approach of feature selection.\nThis method falls into two phases. The filter phase select the features with\nhighest information gain and guides the initialization of search process for\nwrapper phase whose output the final feature subset. The final feature subsets\nare passed through the Knearest neighbor classifier for classification of\nattacks. The effectiveness of this algorithm is demonstrated on DARPA KDDCUP99\ncyber attack dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2009 13:15:08 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Singh", "Shailendra", ""], ["Silakari", "Sanjay", ""]]}, {"id": "0912.1128", "submitter": "Timon Schroeter", "authors": "David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe,\n  Katja Hansen, Klaus-Robert Mueller", "title": "How to Explain Individual Classification Decisions", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After building a classifier with modern tools of machine learning we\ntypically have a black box at hand that is able to predict well for unseen\ndata. Thus, we get an answer to the question what is the most likely label of a\ngiven unseen data point. However, most methods will provide no answer why the\nmodel predicted the particular label for a single instance and what features\nwere most influential for that particular instance. The only method that is\ncurrently able to provide such explanations are decision trees. This paper\nproposes a procedure which (based on a set of assumptions) allows to explain\nthe decisions of any classification method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 19:29:04 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Baehrens", "David", ""], ["Schroeter", "Timon", ""], ["Harmeling", "Stefan", ""], ["Kawanabe", "Motoaki", ""], ["Hansen", "Katja", ""], ["Mueller", "Klaus-Robert", ""]]}, {"id": "0912.1155", "submitter": "Benjamin Rubinstein", "authors": "Adam Barth, Benjamin I. P. Rubinstein, Mukund Sundararajan, John C.\n  Mitchell, Dawn Song, Peter L. Bartlett", "title": "A Learning-Based Approach to Reactive Security", "comments": "22 pages, 4 figures; full version of paper to be published in\n  Financial Cryptography and Data Security 2010 (FC'10)", "journal-ref": null, "doi": "10.1007/978-3-642-14577-3_16", "report-no": null, "categories": "cs.CR cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the conventional wisdom that proactive security is superior to\nreactive security, we show that reactive security can be competitive with\nproactive security as long as the reactive defender learns from past attacks\ninstead of myopically overreacting to the last attack. Our game-theoretic model\nfollows common practice in the security literature by making worst-case\nassumptions about the attacker: we grant the attacker complete knowledge of the\ndefender's strategy and do not require the attacker to act rationally. In this\nmodel, we bound the competitive ratio between a reactive defense algorithm\n(which is inspired by online learning theory) and the best fixed proactive\ndefense. Additionally, we show that, unlike proactive defenses, this reactive\nstrategy is robust to a lack of information about the attacker's incentives and\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2009 01:45:32 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2009 04:36:44 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Barth", "Adam", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Sundararajan", "Mukund", ""], ["Mitchell", "John C.", ""], ["Song", "Dawn", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "0912.1198", "submitter": "Ying Cui", "authors": "Vincent K.N.Lau and Ying Cui", "title": "Delay-Optimal Power and Subcarrier Allocation for OFDMA Systems via\n  Stochastic Approximation", "comments": "11 pages, 7 figures, TWC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider delay-optimal power and subcarrier allocation\ndesign for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base\nstation. There are $K$ queues at the base station for the downlink traffic to\nthe $K$ mobiles with heterogeneous packet arrivals and delay requirements. We\nshall model the problem as a $K$-dimensional infinite horizon average reward\nMarkov Decision Problem (MDP) where the control actions are assumed to be a\nfunction of the instantaneous Channel State Information (CSI) as well as the\njoint Queue State Information (QSI). This problem is challenging because it\ncorresponds to a stochastic Network Utility Maximization (NUM) problem where\ngeneral solution is still unknown. We propose an {\\em online stochastic value\niteration} solution using {\\em stochastic approximation}. The proposed power\ncontrol algorithm, which is a function of both the CSI and the QSI, takes the\nform of multi-level water-filling. We prove that under two mild conditions in\nTheorem 1 (One is the stepsize condition. The other is the condition on\naccessibility of the Markov Chain, which can be easily satisfied in most of the\ncases we are interested.), the proposed solution converges to the optimal\nsolution almost surely (with probability 1) and the proposed framework offers a\npossible solution to the general stochastic NUM problem. By exploiting the\nbirth-death structure of the queue dynamics, we obtain a reduced complexity\ndecomposed solution with linear $\\mathcal{O}(KN_F)$ complexity and\n$\\mathcal{O}(K)$ memory requirement.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2009 10:35:56 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Lau", "Vincent K. N.", ""], ["Cui", "Ying", ""]]}, {"id": "0912.1822", "submitter": "Vishal Goyal", "authors": "S.Kannan and R.Bhaskaran", "title": "Association Rule Pruning based on Interestingness Measures with\n  Clustering", "comments": "International Journal of Computer Science Issues, IJCSI Volume 6,\n  Issue 1, pp35-43, November 2009", "journal-ref": "S.Kannan and R.Bhaskaran, \"Association Rule Pruning based on\n  Interestingness Measures with Clustering\", International Journal of Computer\n  Science Issues, IJCSI, Volume 6, Issue 1, pp35-43, November 2009", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining plays vital part in knowledge mining. The difficult\ntask is discovering knowledge or useful rules from the large number of rules\ngenerated for reduced support. For pruning or grouping rules, several\ntechniques are used such as rule structure cover methods, informative cover\nmethods, rule clustering, etc. Another way of selecting association rules is\nbased on interestingness measures such as support, confidence, correlation, and\nso on. In this paper, we study how rule clusters of the pattern Xi - Y are\ndistributed over different interestingness measures.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2009 18:11:11 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Kannan", "S.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "0912.1830", "submitter": "Vishal Goyal", "authors": "Kazumoto Tanaka", "title": "Gesture Recognition with a Focus on Important Actions by Using a Path\n  Searching Method in Weighted Graph", "comments": "International Journal of Computer Science Issues, IJCSI Volume 6,\n  Issue 2, pp14-19, November 2009", "journal-ref": "K. TANAKA, \"Gesture Recognition with a Focus on Important Actions\n  by Using a Path Searching Method in Weighted Graph\", International Journal of\n  Computer Science Issues, IJCSI, Volume 6, Issue 2, pp14-19, November 2009", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method of gesture recognition with a focus on important\nactions for distinguishing similar gestures. The method generates a partial\naction sequence by using optical flow images, expresses the sequence in the\neigenspace, and checks the feature vector sequence by applying an optimum\npath-searching method of weighted graph to focus the important actions. Also\npresented are the results of an experiment on the recognition of similar sign\nlanguage words.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2009 18:41:49 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Tanaka", "Kazumoto", ""]]}, {"id": "0912.2302", "submitter": "Kadirvelu SivaKumar", "authors": "Ali Douik, Mourad Moussa Jlassi", "title": "Synthesis of supervised classification algorithm using intelligent and\n  statistical tools", "comments": null, "journal-ref": "IJCSE Volume 1 Issue 2 2009 89-97", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in detecting foreground objects in both static and dynamic\nscenes is to take the best choice of color system representation and the\nefficient technique for background modeling. We propose in this paper a\nnon-parametric algorithm dedicated to segment and to detect objects in color\nimages issued from a football sports meeting. Indeed segmentation by pixel\nconcern many applications and revealed how the method is robust to detect\nobjects, even in presence of strong shadows and highlights. In the other hand\nto refine their playing strategy such as in football, handball, volley ball,\nRugby..., the coach need to have a maximum of technical-tactics information\nabout the on-going of the game and the players. We propose in this paper a\nrange of algorithms allowing the resolution of many problems appearing in the\nautomated process of team identification, where each player is affected to his\ncorresponding team relying on visual data. The developed system was tested on a\nmatch of the Tunisian national competition. This work is prominent for many\nnext computer vision studies as it's detailed in this study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2009 18:14:29 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Douik", "Ali", ""], ["Jlassi", "Mourad Moussa", ""]]}, {"id": "0912.2314", "submitter": "Kadirvelu SivaKumar", "authors": "Y.Ireaneus Anna Rejani, S.Thamarai Selvi", "title": "Early Detection of Breast Cancer using SVM Classifier Technique", "comments": null, "journal-ref": "IJCSE Volume 1 Issue 3 2009 127-130", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tumor detection algorithm from mammogram. The proposed\nsystem focuses on the solution of two problems. One is how to detect tumors as\nsuspicious regions with a very weak contrast to their background and another is\nhow to extract features which categorize tumors. The tumor detection method\nfollows the scheme of (a) mammogram enhancement. (b) The segmentation of the\ntumor area. (c) The extraction of features from the segmented tumor area. (d)\nThe use of SVM classifier. The enhancement can be defined as conversion of the\nimage quality to a better and more understandable level. The mammogram\nenhancement procedure includes filtering, top hat operation, DWT. Then the\ncontrast stretching is used to increase the contrast of the image. The\nsegmentation of mammogram images has been playing an important role to improve\nthe detection and diagnosis of breast cancer. The most common segmentation\nmethod used is thresholding. The features are extracted from the segmented\nbreast area. Next stage include, which classifies the regions using the SVM\nclassifier. The method was tested on 75 mammographic images, from the mini-MIAS\ndatabase. The methodology achieved a sensitivity of 88.75%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2009 18:50:46 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Rejani", "Y. Ireaneus Anna", ""], ["Selvi", "S. Thamarai", ""]]}, {"id": "0912.2385", "submitter": "Byron Boots", "authors": "Byron Boots, Sajid M. Siddiqi, Geoffrey J. Gordon", "title": "Closing the Learning-Planning Loop with Predictive State Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in artificial intelligence is that of planning to maximize\nfuture reward under uncertainty in a partially observable environment. In this\npaper we propose and demonstrate a novel algorithm which accurately learns a\nmodel of such an environment directly from sequences of action-observation\npairs. We then close the loop from observations to actions by planning in the\nlearned model and recovering a policy which is near-optimal in the original\nenvironment. Specifically, we present an efficient and statistically consistent\nspectral algorithm for learning the parameters of a Predictive State\nRepresentation (PSR). We demonstrate the algorithm by learning a model of a\nsimulated high-dimensional, vision-based mobile robot planning task, and then\nperform approximate point-based planning in the learned PSR. Analysis of our\nresults shows that the algorithm learns a state space which efficiently\ncaptures the essential features of the environment. This representation allows\naccurate prediction with a small number of parameters, and enables successful\nand efficient planning.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2009 00:59:26 GMT"}], "update_date": "2009-12-15", "authors_parsed": [["Boots", "Byron", ""], ["Siddiqi", "Sajid M.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "0912.2709", "submitter": "Daniel Kane", "authors": "Daniel M. Kane", "title": "The Gaussian Surface Area and Noise Sensitivity of Degree-$d$\n  Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide asymptotically sharp bounds for the Gaussian surface area and the\nGaussian noise sensitivity of polynomial threshold functions. In particular we\nshow that if $f$ is a degree-$d$ polynomial threshold function, then its\nGaussian sensitivity at noise rate $\\epsilon$ is less than some quantity\nasymptotic to $\\frac{d\\sqrt{2\\epsilon}}{\\pi}$ and the Gaussian surface area is\nat most $\\frac{d}{\\sqrt{2\\pi}}$. Furthermore these bounds are asymptotically\ntight as $\\epsilon\\to 0$ and $f$ the threshold function of a product of $d$\ndistinct homogeneous linear functions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2009 19:14:03 GMT"}], "update_date": "2009-12-15", "authors_parsed": [["Kane", "Daniel M.", ""]]}, {"id": "0912.2843", "submitter": "Nallusamy R", "authors": "R.Nallusamy, K.Jayarajan, K.Duraiswamy", "title": "Intrusion Detection In Mobile Ad Hoc Networks Using GA Based Feature\n  Selection", "comments": "This paper has been withdrawn by the authors. To improve the quality\n  of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile ad hoc networking (MANET) has become an exciting and important\ntechnology in recent years because of the rapid proliferation of wireless\ndevices. MANETs are highly vulnerable to attacks due to the open medium,\ndynamically changing network topology and lack of centralized monitoring point.\nIt is important to search new architecture and mechanisms to protect the\nwireless networks and mobile computing application. IDS analyze the network\nactivities by means of audit data and use patterns of well-known attacks or\nnormal profile to detect potential attacks. There are two methods to analyze:\nmisuse detection and anomaly detection. Misuse detection is not effective\nagainst unknown attacks and therefore, anomaly detection method is used. In\nthis approach, the audit data is collected from each mobile node after\nsimulating the attack and compared with the normal behavior of the system. If\nthere is any deviation from normal behavior then the event is considered as an\nattack. Some of the features of collected audit data may be redundant or\ncontribute little to the detection process. So it is essential to select the\nimportant features to increase the detection rate. This paper focuses on\nimplementing two feature selection methods namely, markov blanket discovery and\ngenetic algorithm. In genetic algorithm, bayesian network is constructed over\nthe collected features and fitness function is calculated. Based on the fitness\nvalue the features are selected. Markov blanket discovery also uses bayesian\nnetwork and the features are selected depending on the minimum description\nlength. During the evaluation phase, the performances of both approaches are\ncompared based on detection rate and false alarm rate.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2009 10:57:58 GMT"}, {"version": "v2", "created": "Sun, 30 May 2010 09:00:50 GMT"}], "update_date": "2010-06-01", "authors_parsed": [["Nallusamy", "R.", ""], ["Jayarajan", "K.", ""], ["Duraiswamy", "K.", ""]]}, {"id": "0912.3983", "submitter": "William Jackson", "authors": "Samarjeet Borah, Mrinal Kanti Ghose", "title": "Performance Analysis of AIM-K-means & K-means in Quality Cluster\n  Generation", "comments": null, "journal-ref": "Journal of Computing, Volume 1, Issue 1, pp 175-178, December 2009", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among all the partition based clustering algorithms K-means is the most\npopular and well known method. It generally shows impressive results even in\nconsiderably large data sets. The computational complexity of K-means does not\nsuffer from the size of the data set. The main disadvantage faced in performing\nthis clustering is that the selection of initial means. If the user does not\nhave adequate knowledge about the data set, it may lead to erroneous results.\nThe algorithm Automatic Initialization of Means (AIM), which is an extension to\nK-means, has been proposed to overcome the problem of initial mean generation.\nIn this paper an attempt has been made to compare the performance of the\nalgorithms through implementation\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2009 05:21:45 GMT"}], "update_date": "2009-12-22", "authors_parsed": [["Borah", "Samarjeet", ""], ["Ghose", "Mrinal Kanti", ""]]}, {"id": "0912.3995", "submitter": "Niranjan Srinivas", "authors": "Niranjan Srinivas, Andreas Krause, Sham M. Kakade and Matthias Seeger", "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and\n  Experimental Design", "comments": null, "journal-ref": null, "doi": "10.1109/TIT.2011.2182033", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require optimizing an unknown, noisy function that is\nexpensive to evaluate. We formalize this task as a multi-armed bandit problem,\nwhere the payoff function is either sampled from a Gaussian process (GP) or has\nlow RKHS norm. We resolve the important open problem of deriving regret bounds\nfor this setting, which imply novel convergence rates for GP optimization. We\nanalyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its\ncumulative regret in terms of maximal information gain, establishing a novel\nconnection between GP optimization and experimental design. Moreover, by\nbounding the latter in terms of operator spectra, we obtain explicit sublinear\nregret bounds for many commonly used covariance functions. In some important\ncases, our bounds have surprisingly weak dependence on the dimensionality. In\nour experiments on real sensor data, GP-UCB compares favorably with other\nheuristical GP optimization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2009 00:08:19 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2010 06:15:15 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2010 18:24:43 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2010 23:24:13 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Srinivas", "Niranjan", ""], ["Krause", "Andreas", ""], ["Kakade", "Sham M.", ""], ["Seeger", "Matthias", ""]]}, {"id": "0912.4473", "submitter": "Shankar Vembu", "authors": "Shankar Vembu", "title": "Learning to Predict Combinatorial Structures", "comments": "PhD thesis, Department of Computer Science, University of Bonn\n  (submitted, December 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major challenge in designing a discriminative learning algorithm for\npredicting structured data is to address the computational issues arising from\nthe exponential size of the output space. Existing algorithms make different\nassumptions to ensure efficient, polynomial time estimation of model\nparameters. For several combinatorial structures, including cycles, partially\nordered sets, permutations and other graph classes, these assumptions do not\nhold. In this thesis, we address the problem of designing learning algorithms\nfor predicting combinatorial structures by introducing two new assumptions: (i)\nThe first assumption is that a particular counting problem can be solved\nefficiently. The consequence is a generalisation of the classical ridge\nregression for structured prediction. (ii) The second assumption is that a\nparticular sampling problem can be solved efficiently. The consequence is a new\ntechnique for designing and analysing probabilistic structured prediction\nmodels. These results can be applied to solve several complex learning problems\nincluding but not limited to multi-label classification, multi-category\nhierarchical classification, and label ranking.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2009 18:03:55 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2010 22:47:44 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Vembu", "Shankar", ""]]}, {"id": "0912.4883", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Futurs, Lifl)", "title": "On Finding Predictors for Arbitrary Families of Processes", "comments": null, "journal-ref": "Journal of Machine Learning Research 11 (2010) 581-602", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is sequence prediction in the following setting. A sequence\n$x_1,...,x_n,...$ of discrete-valued observations is generated according to\nsome unknown probabilistic law (measure) $\\mu$. After observing each outcome,\nit is required to give the conditional probabilities of the next observation.\nThe measure $\\mu$ belongs to an arbitrary but known class $C$ of stochastic\nprocess measures. We are interested in predictors $\\rho$ whose conditional\nprobabilities converge (in some sense) to the \"true\" $\\mu$-conditional\nprobabilities if any $\\mu\\in C$ is chosen to generate the sequence. The\ncontribution of this work is in characterizing the families $C$ for which such\npredictors exist, and in providing a specific and simple form in which to look\nfor a solution. We show that if any predictor works, then there exists a\nBayesian predictor, whose prior is discrete, and which works too. We also find\nseveral sufficient and necessary conditions for the existence of a predictor,\nin terms of topological characterizations of the family $C$, as well as in\nterms of local behaviour of the measures in $C$, which in some cases lead to\nprocedures for constructing such predictors. It should be emphasized that the\nframework is completely general: the stochastic processes considered are not\nrequired to be i.i.d., stationary, or to belong to any parametric or countable\nfamily.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2009 15:29:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Futurs, Lifl"]]}, {"id": "0912.4884", "submitter": "Raghu Meka", "authors": "Prahladh Harsha, Adam Klivans and Raghu Meka", "title": "An Invariance Principle for Polytopes", "comments": "Added a lowerbound and minor corrections", "journal-ref": "JACM, 59(6):29, 2012", "doi": "10.1145/2395116.2395118", "report-no": null, "categories": "cs.CC cs.CG cs.DM cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X be randomly chosen from {-1,1}^n, and let Y be randomly chosen from the\nstandard spherical Gaussian on R^n. For any (possibly unbounded) polytope P\nformed by the intersection of k halfspaces, we prove that\n  |Pr [X belongs to P] - Pr [Y belongs to P]| < log^{8/5}k * Delta, where Delta\nis a parameter that is small for polytopes formed by the intersection of\n\"regular\" halfspaces (i.e., halfspaces with low influence). The novelty of our\ninvariance principle is the polylogarithmic dependence on k. Previously, only\nbounds that were at least linear in k were known. We give two important\napplications of our main result: (1) A polylogarithmic in k bound on the\nBoolean noise sensitivity of intersections of k \"regular\" halfspaces (previous\nwork gave bounds linear in k). (2) A pseudorandom generator (PRG) with seed\nlength O((log n)*poly(log k,1/delta)) that delta-fools all polytopes with k\nfaces with respect to the Gaussian distribution. We also obtain PRGs with\nsimilar parameters that fool polytopes formed by intersection of regular\nhalfspaces over the hypercube. Using our PRG constructions, we obtain the first\ndeterministic quasi-polynomial time algorithms for approximately counting the\nnumber of solutions to a broad class of integer programs, including dense\ncovering problems and contingency tables.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2009 15:35:56 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 23:33:10 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Harsha", "Prahladh", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "0912.5029", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Complexity of stochastic branch and bound methods for belief tree search\n  in Bayesian reinforcement learning", "comments": "13 pages, 1 figure, ICAART 2010", "journal-ref": null, "doi": null, "report-no": "TR-UVA-09-01", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent work on Bayesian methods for reinforcement\nlearning exhibiting near-optimal online performance. The main obstacle facing\nsuch methods is that in most problems of interest, the optimal solution\ninvolves planning in an infinitely large tree. However, it is possible to\nobtain stochastic lower and upper bounds on the value of each tree node. This\nenables us to use stochastic branch and bound algorithms to search the tree\nefficiently. This paper proposes two such algorithms and examines their\ncomplexity in this setting.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2009 16:32:46 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "0912.5193", "submitter": "Ricardo Silva", "authors": "Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi", "title": "Ranking relations using analogies in biological and information networks", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS321 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 615-644", "doi": "10.1214/09-AOAS321", "report-no": "IMS-AOAS-AOAS321", "categories": "stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogical reasoning depends fundamentally on the ability to learn and\ngeneralize about relations between objects. We develop an approach to\nrelational learning which, given a set of pairs of objects\n$\\mathbf{S}=\\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\\ldots,A^{(N)}:B ^{(N)}\\}$,\nmeasures how well other pairs A:B fit in with the set $\\mathbf{S}$. Our work\naddresses the following question: is the relation between objects A and B\nanalogous to those relations found in $\\mathbf{S}$? Such questions are\nparticularly relevant in information retrieval, where an investigator might\nwant to search for analogous pairs of objects that match the query set of\ninterest. There are many ways in which objects can be related, making the task\nof measuring analogies very challenging. Our approach combines a similarity\nmeasure on function spaces with Bayesian analysis to produce a ranking. It\nrequires data containing features of the objects of interest and a link matrix\nspecifying which relationships exist; no further attributes of such\nrelationships are necessary. We illustrate the potential of our method on text\nanalysis and information networks. An application on discovering functional\ninteractions between pairs of proteins is discussed in detail, where we show\nthat our approach can work in practice even if a small set of protein pairs is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2009 17:56:50 GMT"}, {"version": "v2", "created": "Mon, 8 Nov 2010 11:52:09 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 06:50:07 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Silva", "Ricardo", ""], ["Heller", "Katherine", ""], ["Ghahramani", "Zoubin", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "0912.5410", "submitter": "Edoardo Airoldi", "authors": "Anna Goldenberg, Alice X Zheng, Stephen E Fienberg, Edoardo M Airoldi", "title": "A survey of statistical network models", "comments": "96 pages, 14 figures, 333 references", "journal-ref": "Foundations and Trends in Machine Learning, 2(2):1-117, 2009", "doi": null, "report-no": null, "categories": "stat.ME cs.LG physics.soc-ph q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in science and have become a focal point for\ndiscussion in everyday life. Formal statistical models for the analysis of\nnetwork data have emerged as a major topic of interest in diverse areas of\nstudy, and most of these involve a form of graphical representation.\nProbability models on graphs date back to 1959. Along with empirical studies in\nsocial psychology and sociology from the 1960s, these early works generated an\nactive network community and a substantial literature in the 1970s. This effort\nmoved into the statistical literature in the late 1970s and 1980s, and the past\ndecade has seen a burgeoning network literature in statistical physics and\ncomputer science. The growth of the World Wide Web and the emergence of online\nnetworking communities such as Facebook, MySpace, and LinkedIn, and a host of\nmore specialized professional network communities has intensified interest in\nthe study of networks and network data. Our goal in this review is to provide\nthe reader with an entry point to this burgeoning literature. We begin with an\noverview of the historical development of statistical network modeling and then\nwe introduce a number of examples that have been studied in the network\nliterature. Our subsequent discussion focuses on a number of prominent static\nand dynamic network models and their interconnections. We emphasize formal\nmodel descriptions, and pay special attention to the interpretation of\nparameters and their estimation. We end with a description of some open\nproblems and challenges for machine learning and statistics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 17:53:13 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Goldenberg", "Anna", ""], ["Zheng", "Alice X", ""], ["Fienberg", "Stephen E", ""], ["Airoldi", "Edoardo M", ""]]}]