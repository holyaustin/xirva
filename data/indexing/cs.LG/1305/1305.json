[{"id": "1305.0015", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan and Yee Whye Teh", "title": "Inferring ground truth from multi-annotator ordinal data: a\n  probabilistic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for large scale data annotation tasks is crowdsourcing,\nwherein each data point is labeled by multiple noisy annotators. We consider\nthe problem of inferring ground truth from noisy ordinal labels obtained from\nmultiple annotators of varying and unknown expertise levels. Annotation models\nfor ordinal data have been proposed mostly as extensions of their\nbinary/categorical counterparts and have received little attention in the\ncrowdsourcing literature. We propose a new model for crowdsourced ordinal data\nthat accounts for instance difficulty as well as annotator expertise, and\nderive a variational Bayesian inference algorithm for parameter estimation. We\nanalyze the ordinal extensions of several state-of-the-art annotator models for\nbinary/categorical labels and evaluate the performance of all the models on two\nreal world datasets containing ordinal query-URL relevance scores, collected\nthrough Amazon's Mechanical Turk. Our results indicate that the proposed model\nperforms better or as well as existing state-of-the-art methods and is more\nresistant to `spammy' annotators (i.e., annotators who assign labels randomly\nwithout actually looking at the instance) than popular baselines such as mean,\nmedian, and majority vote which do not account for annotator expertise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 20:12:01 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1305.0051", "submitter": "Kevin Xu", "authors": "Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, and Alfred O.\n  Hero III", "title": "Revealing social networks of spammers through spectral clustering", "comments": "Source code and data available at\n  http://tbayes.eecs.umich.edu/xukevin/spam_icc09 Proceedings of the IEEE\n  International Conference on Communications (2009)", "journal-ref": null, "doi": "10.1109/ICC.2009.5199418", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most studies on spam have focused only on the spamming phase of the\nspam cycle and have ignored the harvesting phase, which consists of the mass\nacquisition of email addresses. It has been observed that spammers conceal\ntheir identity to a lesser degree in the harvesting phase, so it may be\npossible to gain new insights into spammers' behavior by studying the behavior\nof harvesters, which are individuals or bots that collect email addresses. In\nthis paper, we reveal social networks of spammers by identifying communities of\nharvesters with high behavioral similarity using spectral clustering. The data\nanalyzed was collected through Project Honey Pot, a distributed system for\nmonitoring harvesting and spamming. Our main findings are (1) that most\nspammers either send only phishing emails or no phishing emails at all, (2)\nthat most communities of spammers also send only phishing emails or no phishing\nemails at all, and (3) that several groups of spammers within communities\nexhibit coherent temporal behavior and have similar IP addresses. Our findings\nreveal some previously unknown behavior of spammers and suggest that there is\nindeed social structure between spammers to be discovered.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 22:57:12 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Xu", "Kevin S.", ""], ["Kliger", "Mark", ""], ["Chen", "Yilun", ""], ["Woolf", "Peter J.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1305.0103", "submitter": "Marthinus Christoffel du Plessis Marthinus Christoffel du Plessi", "authors": "Marthinus Christoffel du Plessis and Masashi Sugiyama", "title": "Clustering Unclustered Data: Unsupervised Binary Labeling of Two\n  Datasets Having Different Class Balances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unsupervised learning problem of assigning labels to\nunlabeled data. A naive approach is to use clustering methods, but this works\nwell only when data is properly clustered and each cluster corresponds to an\nunderlying class. In this paper, we first show that this unsupervised labeling\nproblem in balanced binary cases can be solved if two unlabeled datasets having\ndifferent class balances are available. More specifically, estimation of the\nsign of the difference between probability densities of two unlabeled datasets\ngives the solution. We then introduce a new method to directly estimate the\nsign of the density difference without density estimation. Finally, we\ndemonstrate the usefulness of the proposed method against several clustering\nmethods on various toy problems and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 06:32:12 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Plessis", "Marthinus Christoffel du", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1305.0208", "submitter": "Afshin Rostamizadeh", "authors": "Mehryar Mohri, Afshin Rostamizadeh", "title": "Perceptron Mistake Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a brief survey of existing mistake bounds and introduce novel\nbounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds\ngeneralize beyond standard margin-loss type bounds, allow for any convex and\nLipschitz loss function, and admit a very simple proof.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 15:45:34 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 02:13:57 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1305.0355", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Model Selection for High-Dimensional Regression under the Generalized\n  Irrepresentability Condition", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-dimensional regression model a response variable is linearly\nrelated to $p$ covariates, but the sample size $n$ is smaller than $p$. We\nassume that only a small subset of covariates is `active' (i.e., the\ncorresponding coefficients are non-zero), and consider the model-selection\nproblem of identifying the active covariates. A popular approach is to estimate\nthe regression coefficients through the Lasso ($\\ell_1$-regularized least\nsquares). This is known to correctly identify the active set only if the\nirrelevant covariates are roughly orthogonal to the relevant ones, as\nquantified through the so called `irrepresentability' condition. In this paper\nwe study the `Gauss-Lasso' selector, a simple two-stage method that first\nsolves the Lasso, and then performs ordinary least squares restricted to the\nLasso active set. We formulate `generalized irrepresentability condition'\n(GIC), an assumption that is substantially weaker than irrepresentability. We\nprove that, under GIC, the Gauss-Lasso correctly recovers the active set.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 07:25:52 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1305.0395", "submitter": "Andrzej  Cichocki", "authors": "Andrzej Cichocki", "title": "Tensor Decompositions: A New Concept in Brain Data Analysis?", "comments": null, "journal-ref": "Control Measurement, and System Integration (SICE), special issue;\n  Measurement of Brain Functions and Bio-Signals, 7, 507-517, (2011)", "doi": null, "report-no": null, "categories": "cs.NA cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorizations and their extensions to tensor factorizations and\ndecompositions have become prominent techniques for linear and multilinear\nblind source separation (BSS), especially multiway Independent Component\nAnalysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), Smooth\nComponent Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover,\ntensor decompositions have many other potential applications beyond multilinear\nBSS, especially feature extraction, classification, dimensionality reduction\nand multiway clustering. In this paper, we briefly overview new and emerging\nmodels and approaches for tensor decompositions in applications to group and\nlinked multiway BSS/ICA, feature extraction, classification andMultiway Partial\nLeast Squares (MPLS) regression problems. Keywords: Multilinear BSS, linked\nmultiway BSS/ICA, tensor factorizations and decompositions, constrained Tucker\nand CP models, Penalized Tensor Decompositions (PTD), feature extraction,\nclassification, multiway PLS and CCA.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 11:17:47 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Cichocki", "Andrzej", ""]]}, {"id": "1305.0423", "submitter": "Somayeh Danafar", "authors": "Somayeh Danafar, Paola M.V. Rancoita, Tobias Glasmachers, Kevin\n  Whittingstall, Juergen Schmidhuber", "title": "Testing Hypotheses by Regularized Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do two data samples come from different distributions? Recent studies of this\nfundamental problem focused on embedding probability distributions into\nsufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to\ncompare distributions by the distance between their embeddings. We show that\nRegularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based\nhypothesis testing, yields substantial improvements even when sample sizes are\nsmall, and excels at hypothesis tests involving multiple comparisons with power\ncontrol. We derive asymptotic distributions under the null and alternative\nhypotheses, and assess power control. Outstanding results are obtained on:\nchallenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 13:03:53 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Danafar", "Somayeh", ""], ["Rancoita", "Paola M. V.", ""], ["Glasmachers", "Tobias", ""], ["Whittingstall", "Kevin", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1305.0445", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "Deep Learning of Representations: Looking Forward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning research aims at discovering learning algorithms that discover\nmultiple levels of distributed representations, with higher levels representing\nmore abstract concepts. Although the study of deep learning has already led to\nimpressive theoretical results, learning algorithms and breakthrough\nexperiments, several challenges lie ahead. This paper proposes to examine some\nof these challenges, centering on the questions of scaling deep learning\nalgorithms to much larger models and datasets, reducing optimization\ndifficulties due to ill-conditioning or local minima, designing more efficient\nand powerful inference and sampling procedures, and learning to disentangle the\nfactors of variation underlying the observed data. It also proposes a few\nforward-looking research directions aimed at overcoming these challenges.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 14:33:28 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 02:35:21 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1305.0626", "submitter": "Fuqiang Chen", "authors": "Fuqiang Chen", "title": "An Improved EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we firstly give a brief introduction of expectation\nmaximization (EM) algorithm, and then discuss the initial value sensitivity of\nexpectation maximization algorithm. Subsequently, we give a short proof of EM's\nconvergence. Then, we implement experiments with the expectation maximization\nalgorithm (We implement all the experiments on Gaussion mixture model (GMM)).\nOur experiment with expectation maximization is performed in the following\nthree cases: initialize randomly; initialize with result of K-means; initialize\nwith result of K-medoids. The experiment result shows that expectation\nmaximization algorithm depend on its initial state or parameters. And we found\nthat EM initialized with K-medoids performed better than both the one\ninitialized with K-means and the one initialized randomly.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 06:25:41 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Chen", "Fuqiang", ""]]}, {"id": "1305.0638", "submitter": "Deqing Wang", "authors": "Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv", "title": "Feature Selection Based on Term Frequency and T-Test for Text\n  Categorization", "comments": "5pages 9 figures CIKM2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Much work has been done on feature selection. Existing methods are based on\ndocument frequency, such as Chi-Square Statistic, Information Gain etc.\nHowever, these methods have two shortcomings: one is that they are not reliable\nfor low-frequency terms, and the other is that they only count whether one term\noccurs in a document and ignore the term frequency. Actually, high-frequency\nterms within a specific category are often regards as discriminators.\n  This paper focuses on how to construct the feature selection function based\non term frequency, and proposes a new approach based on $t$-test, which is used\nto measure the diversity of the distributions of a term between the specific\ncategory and the entire corpus. Extensive comparative experiments on two text\ncorpora using three classifiers show that our new approach is comparable to or\nor slightly better than the state-of-the-art feature selection methods (i.e.,\n$\\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 08:26:05 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Wang", "Deqing", ""], ["Zhang", "Hui", ""], ["Liu", "Rui", ""], ["Lv", "Weifeng", ""]]}, {"id": "1305.0665", "submitter": "Fuqiang Chen", "authors": "Fuqiang Chen, Yan Wu, Yude Bu, Guodong Zhao", "title": "Spectral Classification Using Restricted Boltzmann Machine", "comments": "8 pages, 2 figures, Accepted in PASA for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel machine learning algorithm, restricted Boltzmann\nmachine (RBM), is introduced. The algorithm is applied for the spectral\nclassification in astronomy. RBM is a bipartite generative graphical model with\ntwo separate layers (one visible layer and one hidden layer), which can extract\nhigher level features to represent the original data. Despite generative, RBM\ncan be used for classification when modified with a free energy and a soft-max\nfunction. Before spectral classification, the original data is binarized\naccording to some rule. Then we resort to the binary RBM to classify\ncataclysmic variables (CVs) and non-CVs (one half of all the given data for\ntraining and the other half for testing). The experiment result shows\nstate-of-the-art accuracy of 100%, which indicates the efficiency of the binary\nRBM algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 10:20:02 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 01:03:56 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Chen", "Fuqiang", ""], ["Wu", "Yan", ""], ["Bu", "Yude", ""], ["Zhao", "Guodong", ""]]}, {"id": "1305.0698", "submitter": "Eyke H\\\"ullermeier", "authors": "Eyke H\\\"ullermeier", "title": "Learning from Imprecise and Fuzzy Observations: Data Disambiguation\n  through Generalized Loss Minimization", "comments": null, "journal-ref": "International Journal of Approximate Reasoning, 55(7):1519-1534,\n  2014", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for analyzing or learning from \"fuzzy data\" have attracted increasing\nattention in recent years. In many cases, however, existing methods (for\nprecise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner,\nand without carefully considering the interpretation of a fuzzy set when being\nused for modeling data. Distinguishing between an ontic and an epistemic\ninterpretation of fuzzy set-valued data, and focusing on the latter, we argue\nthat a \"fuzzification\" of learning algorithms based on an application of the\ngeneric extension principle is not appropriate. In fact, the extension\nprinciple fails to properly exploit the inductive bias underlying statistical\nand machine learning methods, although this bias, at least in principle, offers\na means for \"disambiguating\" the fuzzy data. Alternatively, we therefore\npropose a method which is based on the generalization of loss functions in\nempirical risk minimization, and which performs model identification and data\ndisambiguation simultaneously. Elaborating on the fuzzification of specific\ntypes of losses, we establish connections to well-known loss functions in\nregression and classification. We compare our approach with related methods and\nillustrate its use in logistic regression for binary classification.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 13:26:24 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1305.0922", "submitter": "M.M.A. Hashem", "authors": "M.A. Khayer Azad, Md. Shafiqul Islam and M.M.A. Hashem", "title": "On Comparison between Evolutionary Programming Network-based Learning\n  and Novel Evolution Strategy Algorithm-based Learning", "comments": null, "journal-ref": "Procs. of the 3rd International Conference on Electrical,\n  Electronics and Computer Engineering (ICEECE 2003), pp. 213-218, Dhaka,\n  Bangladesh, December 22-24, (2003)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two different evolutionary systems - Evolutionary\nProgramming Network (EPNet) and Novel Evolutions Strategy (NES) Algorithm.\nEPNet does both training and architecture evolution simultaneously, whereas NES\ndoes a fixed network and only trains the network. Five mutation operators\nproposed in EPNet to reflect the emphasis on evolving ANNs behaviors. Close\nbehavioral links between parents and their offspring are maintained by various\nmutations, such as partial training and node splitting. On the other hand, NES\nuses two new genetic operators - subpopulation-based max-mean arithmetical\ncrossover and time-variant mutation. The above-mentioned two algorithms have\nbeen tested on a number of benchmark problems, such as the medical diagnosis\nproblems (breast cancer, diabetes, and heart disease). The results and the\ncomparison between them are also presented in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2013 14:06:48 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Azad", "M. A. Khayer", ""], ["Islam", "Md. Shafiqul", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1305.1002", "submitter": "Ji Won Yoon", "authors": "Ji Won Yoon and Nial Friel", "title": "Efficient Estimation of the number of neighbours in Probabilistic K\n  Nearest Neighbour Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic k-nearest neighbour (PKNN) classification has been introduced\nto improve the performance of original k-nearest neighbour (KNN) classification\nalgorithm by explicitly modelling uncertainty in the classification of each\nfeature vector. However, an issue common to both KNN and PKNN is to select the\noptimal number of neighbours, $k$. The contribution of this paper is to\nincorporate the uncertainty in $k$ into the decision making, and in so doing\nuse Bayesian model averaging to provide improved classification. Indeed the\nproblem of assessing the uncertainty in $k$ can be viewed as one of statistical\nmodel selection which is one of the most important technical issues in the\nstatistics and machine learning domain. In this paper, a new functional\napproximation algorithm is proposed to reconstruct the density of the model\n(order) without relying on time consuming Monte Carlo simulations. In addition,\nthis algorithm avoids cross validation by adopting Bayesian framework. The\nperformance of this algorithm yielded very good performance on several real\nexperimental datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 09:44:08 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Yoon", "Ji Won", ""], ["Friel", "Nial", ""]]}, {"id": "1305.1019", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang, Ji Wu", "title": "Simple Deep Random Model Ensemble", "comments": "This paper has been withdrawn by the author due to a lack of full\n  empirical evaluation. More advanced method has been developed. This method\n  has been fully out of date", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning and unsupervised learning are two central topics of\nmachine learning and signal processing. Deep learning is one of the most\neffective unsupervised representation learning approach. The main contributions\nof this paper to the topics are as follows. (i) We propose to view the\nrepresentative deep learning approaches as special cases of the knowledge reuse\nframework of clustering ensemble. (ii) We propose to view sparse coding when\nused as a feature encoder as the consensus function of clustering ensemble, and\nview dictionary learning as the training process of the base clusterings of\nclustering ensemble. (ii) Based on the above two views, we propose a very\nsimple deep learning algorithm, named deep random model ensemble (DRME). It is\na stack of random model ensembles. Each random model ensemble is a special\nk-means ensemble that discards the expectation-maximization optimization of\neach base k-means but only preserves the default initialization method of the\nbase k-means. (iv) We propose to select the most powerful representation among\nthe layers by applying DRME to clustering where the single-linkage is used as\nthe clustering algorithm. Moreover, the DRME based clustering can also detect\nthe number of the natural clusters accurately. Extensive experimental\ncomparisons with 5 representation learning methods on 19 benchmark data sets\ndemonstrate the effectiveness of DRME.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 14:58:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 23:37:26 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Zhang", "Xiao-Lei", ""], ["Wu", "Ji", ""]]}, {"id": "1305.1027", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Alessandro Lazaric and Emma Brunskill", "title": "Regret Bounds for Reinforcement Learning with Policy Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some reinforcement learning problems an agent may be provided with a set\nof input policies, perhaps learned from prior experience or provided by\nadvisors. We present a reinforcement learning with policy advice (RLPA)\nalgorithm which leverages this input set and learns to use the best policy in\nthe set for the reinforcement learning task at hand. We prove that RLPA has a\nsub-linear regret of \\tilde O(\\sqrt{T}) relative to the best input policy, and\nthat both this regret and its computational complexity are independent of the\nsize of the state and action space. Our empirical simulations support our\ntheoretical analysis. This suggests RLPA may offer significant advantages in\nlarge domains where some prior good policies are provided.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 16:59:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 21:07:37 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1305.1040", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen", "title": "On the Convergence and Consistency of the Blurring Mean-Shift Process", "comments": "arXiv admin note: text overlap with arXiv:1201.1979", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean-shift algorithm is a popular algorithm in computer vision and image\nprocessing. It can also be cast as a minimum gamma-divergence estimation. In\nthis paper we focus on the \"blurring\" mean shift algorithm, which is one\nversion of the mean-shift process that successively blurs the dataset. The\nanalysis of the blurring mean-shift is relatively more complicated compared to\nthe nonblurring version, yet the algorithm convergence and the estimation\nconsistency have not been well studied in the literature. In this paper we\nprove both the convergence and the consistency of the blurring mean-shift. We\nalso perform simulation studies to compare the efficiency of the blurring and\nthe nonblurring versions of the mean-shift algorithms. Our results show that\nthe blurring mean-shift has more efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 18:51:24 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Chen", "Ting-Li", ""]]}, {"id": "1305.1172", "submitter": "Frederic Chazal", "authors": "Fr\\'ed\\'eric Chazal (INRIA Sophia Antipolis / INRIA Saclay - Ile de\n  France), Jian Sun", "title": "Gromov-Hausdorff Approximation of Metric Spaces with Linear Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications data come as discrete metric spaces sampled\naround 1-dimensional filamentary structures that can be seen as metric graphs.\nIn this paper we address the metric reconstruction problem of such filamentary\nstructures from data sampled around them. We prove that they can be\napproximated, with respect to the Gromov-Hausdorff distance by well-chosen Reeb\ngraphs (and some of their variants) and we provide an efficient and easy to\nimplement algorithm to compute such approximations in almost linear time. We\nillustrate the performances of our algorithm on a few synthetic and real data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 12:57:24 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", "", "INRIA Sophia Antipolis / INRIA Saclay - Ile de\n  France"], ["Sun", "Jian", ""]]}, {"id": "1305.1359", "submitter": "Panigrahy Rina", "authors": "Alexandr Andoni and Rina Panigrahy", "title": "A Differential Equations Approach to Optimizing Regret Trade-offs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical question of predicting binary sequences and study\nthe {\\em optimal} algorithms for obtaining the best possible regret and payoff\nfunctions for this problem. The question turns out to be also equivalent to the\nproblem of optimal trade-offs between the regrets of two experts in an \"experts\nproblem\", studied before by \\cite{kearns-regret}. While, say, a regret of\n$\\Theta(\\sqrt{T})$ is known, we argue that it important to ask what is the\nprovably optimal algorithm for this problem --- both because it leads to\nnatural algorithms, as well as because regret is in fact often comparable in\nmagnitude to the final payoffs and hence is a non-negligible term.\n  In the basic setting, the result essentially follows from a classical result\nof Cover from '65. Here instead, we focus on another standard setting, of\ntime-discounted payoffs, where the final \"stopping time\" is not specified. We\nexhibit an explicit characterization of the optimal regret for this setting.\n  To obtain our main result, we show that the optimal payoff functions have to\nsatisfy the Hermite differential equation, and hence are given by the solutions\nto this equation. It turns out that characterization of the payoff function is\nqualitatively different from the classical (non-discounted) setting, and,\nnamely, there's essentially a unique optimal solution.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 00:02:51 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Andoni", "Alexandr", ""], ["Panigrahy", "Rina", ""]]}, {"id": "1305.1363", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Rong Jin and Shenghuo Zhu and Zhi-Hua Zhou", "title": "One-Pass AUC Optimization", "comments": "Proceeding of 30th International Conference on Machine Learning", "journal-ref": "Artificial Intelligence, 2016, 236: 1-29", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC is an important performance measure and many algorithms have been devoted\nto AUC optimization, mostly by minimizing a surrogate convex loss on a training\ndata set. In this work, we focus on one-pass AUC optimization that requires\nonly going through the training data once without storing the entire training\ndataset, where conventional online learning algorithms cannot be applied\ndirectly because AUC is measured by a sum of losses defined over pairs of\ninstances from different classes. We develop a regression-based algorithm which\nonly needs to maintain the first and second order statistics of training data\nin memory, resulting a storage requirement independent from the size of\ntraining data. To efficiently handle high dimensional data, we develop a\nrandomized algorithm that approximates the covariance matrices by low rank\nmatrices. We verify, both theoretically and empirically, the effectiveness of\nthe proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 00:30:32 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 13:24:37 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gao", "Wei", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1305.1396", "submitter": "Marcelo Fiori", "authors": "Mat\\'ias Di Martino, Guzman Hern\\'andez, Marcelo Fiori, Alicia\n  Fern\\'andez", "title": "A new framework for optimal classifier design", "comments": null, "journal-ref": "Pattern Recognition, Volume 46, Issue 8, August 2013, Pages\n  2249-2255", "doi": "10.1016/j.patcog.2013.01.006", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of alternative measures to evaluate classifier performance is gaining\nattention, specially for imbalanced problems. However, the use of these\nmeasures in the classifier design process is still unsolved. In this work we\npropose a classifier designed specifically to optimize one of these alternative\nmeasures, namely, the so-called F-measure. Nevertheless, the technique is\ngeneral, and it can be used to optimize other evaluation measures. An algorithm\nto train the novel classifier is proposed, and the numerical scheme is tested\nwith several databases, showing the optimality and robustness of the presented\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 04:05:24 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2013 16:09:55 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Di Martino", "Mat\u00edas", ""], ["Hern\u00e1ndez", "Guzman", ""], ["Fiori", "Marcelo", ""], ["Fern\u00e1ndez", "Alicia", ""]]}, {"id": "1305.1679", "submitter": "Thiago Christiano Silva", "authors": "Thiago Christiano Silva and Liang Zhao", "title": "High Level Pattern Classification via Tourist Walks in Networks", "comments": "Submitted to the IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks refer to large-scale graphs with nontrivial connection\npatterns. The salient and interesting features that the complex network study\noffer in comparison to graph theory are the emphasis on the dynamical\nproperties of the networks and the ability of inherently uncovering pattern\nformation of the vertices. In this paper, we present a hybrid data\nclassification technique combining a low level and a high level classifier. The\nlow level term can be equipped with any traditional classification techniques,\nwhich realize the classification task considering only physical features (e.g.,\ngeometrical or statistical features) of the input data. On the other hand, the\nhigh level term has the ability of detecting data patterns with semantic\nmeanings. In this way, the classification is realized by means of the\nextraction of the underlying network's features constructed from the input\ndata. As a result, the high level classification process measures the\ncompliance of the test instances with the pattern formation of the training\ndata. Out of various high level perspectives that can be utilized to capture\nsemantic meaning, we utilize the dynamical features that are generated from a\ntourist walker in a networked environment. Specifically, a weighted combination\nof transient and cycle lengths generated by the tourist walk is employed for\nthat end. Interestingly, our study shows that the proposed technique is able to\nfurther improve the already optimized performance of traditional classification\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 23:40:08 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Silva", "Thiago Christiano", ""], ["Zhao", "Liang", ""]]}, {"id": "1305.1707", "submitter": "Rushi Longadge", "authors": "Rushi Longadge and Snehalata Dongre", "title": "Class Imbalance Problem in Data Mining Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In last few years there are major changes and evolution has been done on\nclassification of data. As the application area of technology is increases the\nsize of data also increases. Classification of data becomes difficult because\nof unbounded size and imbalance nature of data. Class imbalance problem become\ngreatest issue in data mining. Imbalance problem occur where one of the two\nclasses having more sample than other classes. The most of algorithm are more\nfocusing on classification of major sample while ignoring or misclassifying\nminority sample. The minority samples are those that rarely occur but very\nimportant. There are different methods available for classification of\nimbalance data set which is divided into three main categories, the algorithmic\napproach, data-preprocessing approach and feature selection approach. Each of\nthis technique has their own advantages and disadvantages. In this paper\nsystematic study of each approach is define which gives the right direction for\nresearch in class imbalance problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 03:39:17 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Longadge", "Rushi", ""], ["Dongre", "Snehalata", ""]]}, {"id": "1305.1809", "submitter": "Christos Dimitrakakis", "authors": "Nikolaos Tziortziotis and Christos Dimitrakakis and Konstantinos\n  Blekas", "title": "Cover Tree Bayesian Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an online tree-based Bayesian approach for reinforcement\nlearning. For inference, we employ a generalised context tree model. This\ndefines a distribution on multivariate Gaussian piecewise-linear models, which\ncan be updated in closed form. The tree structure itself is constructed using\nthe cover tree method, which remains efficient in high dimensional spaces. We\ncombine the model with Thompson sampling and approximate dynamic programming to\nobtain effective exploration policies in unknown environments. The flexibility\nand computational simplicity of the model render it suitable for many\nreinforcement learning problems in continuous state spaces. We demonstrate this\nin an experimental comparison with least squares policy iteration.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 13:11:52 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 09:44:45 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Tziortziotis", "Nikolaos", ""], ["Dimitrakakis", "Christos", ""], ["Blekas", "Konstantinos", ""]]}, {"id": "1305.1956", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Joint Topic Modeling and Factor Analysis of Textual Information and\n  Graded Response Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning methods are critical to the development of\nlarge-scale personalized learning systems that cater directly to the needs of\nindividual learners. The recently developed SPARse Factor Analysis (SPARFA)\nframework provides a new statistical model and algorithms for machine\nlearning-based learning analytics, which estimate a learner's knowledge of the\nlatent concepts underlying a domain, and content analytics, which estimate the\nrelationships among a collection of questions and the latent concepts. SPARFA\nestimates these quantities given only the binary-valued graded responses to a\ncollection of questions. In order to better interpret the estimated latent\nconcepts, SPARFA relies on a post-processing step that utilizes user-defined\ntags (e.g., topics or keywords) available for each question. In this paper, we\nrelax the need for user-defined tags by extending SPARFA to jointly process\nboth graded learner responses and the text of each question and its associated\nanswer(s) or other feedback. Our purely data-driven approach (i) enhances the\ninterpretability of the estimated latent concepts without the need of\nexplicitly generating a set of tags or performing a post-processing step, (ii)\nimproves the prediction performance of SPARFA, and (iii) scales to large\ntest/assessments where human annotation would prove burdensome. We demonstrate\nthe efficacy of the proposed approach on two real educational datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 20:44:55 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 01:05:09 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1305.2218", "submitter": "Shenghuo Zhu", "authors": "Shenghuo Zhu", "title": "Stochastic gradient descent algorithms for strongly convex functions at\n  O(1/T) convergence rates", "comments": null, "journal-ref": null, "doi": null, "report-no": "2013-TR053", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a weighting scheme proportional to t, a traditional stochastic gradient\ndescent (SGD) algorithm achieves a high probability convergence rate of\nO({\\kappa}/T) for strongly convex functions, instead of O({\\kappa} ln(T)/T). We\nalso prove that an accelerated SGD algorithm also achieves a rate of\nO({\\kappa}/T).\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 21:31:47 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Zhu", "Shenghuo", ""]]}, {"id": "1305.2238", "submitter": "Tuo Zhao", "authors": "Han Liu and Lie Wang and Tuo Zhao", "title": "Calibrated Multivariate Regression with Application to Neural Semantic\n  Basis Discovery", "comments": "Journal of Machine Learning Research, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a calibrated multivariate regression method named CMR for fitting\nhigh dimensional multivariate regression models. Compared with existing\nmethods, CMR calibrates regularization for each regression task with respect to\nits noise level so that it simultaneously attains improved finite-sample\nperformance and tuning insensitiveness. Theoretically, we provide sufficient\nconditions under which CMR achieves the optimal rate of convergence in\nparameter estimation. Computationally, we propose an efficient smoothed\nproximal gradient algorithm with a worst-case numerical rate of convergence\n$\\cO(1/\\epsilon)$, where $\\epsilon$ is a pre-specified accuracy of the\nobjective function value. We conduct thorough numerical simulations to\nillustrate that CMR consistently outperforms other high dimensional\nmultivariate regression methods. We also apply CMR to solve a brain activity\nprediction problem and find that it is as competitive as a handcrafted model\ncreated by human experts. The R package \\texttt{camel} implementing the\nproposed method is available on the Comprehensive R Archive Network\n\\url{http://cran.r-project.org/web/packages/camel/}.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 01:08:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 05:05:18 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Liu", "Han", ""], ["Wang", "Lie", ""], ["Zhao", "Tuo", ""]]}, {"id": "1305.2362", "submitter": "David Wipf", "authors": "David Wipf and Haichao Zhang", "title": "Revisiting Bayesian Blind Deconvolution", "comments": "This paper has been submitted to JMLR. A conference version will\n  appear at EMMCVPR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution involves the estimation of a sharp signal or image given\nonly a blurry observation. Because this problem is fundamentally ill-posed,\nstrong priors on both the sharp image and blur kernel are required to\nregularize the solution space. While this naturally leads to a standard MAP\nestimation framework, performance is compromised by unknown trade-off parameter\nsettings, optimization heuristics, and convergence issues stemming from\nnon-convexity and/or poor prior selections. To mitigate some of these problems,\na number of authors have recently proposed substituting a variational Bayesian\n(VB) strategy that marginalizes over the high-dimensional image space leading\nto better estimates of the blur kernel. However, the underlying cost function\nnow involves both integrals with no closed-form solution and complex,\nfunction-valued arguments, thus losing the transparency of MAP. Beyond standard\nBayesian-inspired intuitions, it thus remains unclear by exactly what mechanism\nthese methods are able to operate, rendering understanding, improvements and\nextensions more difficult. To elucidate these issues, we demonstrate that the\nVB methodology can be recast as an unconventional MAP problem with a very\nparticular penalty/prior that couples the image, blur kernel, and noise level\nin a principled way. This unique penalty has a number of useful characteristics\npertaining to relative concavity, local minima avoidance, and scale-invariance\nthat allow us to rigorously explain the success of VB including its existing\nimplementational heuristics and approximations. It also provides strict\ncriteria for choosing the optimal image prior that, perhaps\ncounter-intuitively, need not reflect the statistics of natural scenes. In so\ndoing we challenge the prevailing notion of why VB is successful for blind\ndeconvolution while providing a transparent platform for introducing\nenhancements.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 15:09:11 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Wipf", "David", ""], ["Zhang", "Haichao", ""]]}, {"id": "1305.2388", "submitter": "Ehsan Saboori Mr.", "authors": "Shafigh Parsazad, Ehsan Saboori, Amin Allahyar", "title": "Fast Feature Reduction in intrusion detection datasets", "comments": null, "journal-ref": "Parsazad, Shafigh; Saboori, Ehsan; Allahyar, Amin; , \"Fast Feature\n  Reduction in intrusion detection datasets,\" MIPRO, 2012 Proceedings of the\n  35th International Convention , vol., no., pp.1023-1029, 21-25 May 2012", "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the most intrusion detection systems (IDS), a system tries to learn\ncharacteristics of different type of attacks by analyzing packets that sent or\nreceived in network. These packets have a lot of features. But not all of them\nis required to be analyzed to detect that specific type of attack. Detection\nspeed and computational cost is another vital matter here, because in these\ntypes of problems, datasets are very huge regularly. In this paper we tried to\npropose a very simple and fast feature selection method to eliminate features\nwith no helpful information on them. Result faster learning in process of\nredundant feature omission. We compared our proposed method with three most\nsuccessful similarity based feature selection algorithm including Correlation\nCoefficient, Least Square Regression Error and Maximal Information Compression\nIndex. After that we used recommended features by each of these algorithms in\ntwo popular classifiers including: Bayes and KNN classifier to measure the\nquality of the recommendations. Experimental result shows that although the\nproposed method can't outperform evaluated algorithms with high differences in\naccuracy, but in computational cost it has huge superiority over them.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 05:27:47 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Parsazad", "Shafigh", ""], ["Saboori", "Ehsan", ""], ["Allahyar", "Amin", ""]]}, {"id": "1305.2452", "submitter": "James Foulds", "authors": "James Foulds, Levi Boyles, Christopher Dubois, Padhraic Smyth, Max\n  Welling", "title": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the internet era there has been an explosion in the amount of digital text\ninformation available, leading to difficulties of scale for traditional\ninference algorithms for topic models. Recent advances in stochastic\nvariational inference algorithms for latent Dirichlet allocation (LDA) have\nmade it feasible to learn topic models on large-scale corpora, but these\nmethods do not currently take full advantage of the collapsed representation of\nthe model. We propose a stochastic algorithm for collapsed variational Bayesian\ninference for LDA, which is simpler and more efficient than the state of the\nart method. We show connections between collapsed variational Bayesian\ninference and MAP estimation for LDA, and leverage these connections to prove\nconvergence properties of the proposed algorithm. In experiments on large-scale\ntext corpora, the algorithm was found to converge faster and often to a better\nsolution than the previous method. Human-subject experiments also demonstrated\nthat the method can learn coherent topics in seconds on small corpora,\nfacilitating the use of topic models in interactive document analysis software.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 23:06:47 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Foulds", "James", ""], ["Boyles", "Levi", ""], ["Dubois", "Christopher", ""], ["Smyth", "Padhraic", ""], ["Welling", "Max", ""]]}, {"id": "1305.2505", "submitter": "Purushottam Kar", "authors": "Purushottam Kar, Bharath K Sriperumbudur, Prateek Jain and Harish C\n  Karnick", "title": "On the Generalization Ability of Online Learning Algorithms for Pairwise\n  Loss Functions", "comments": "To appear in proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013)", "journal-ref": "Journal of Machine Learning Research, W&CP 28(3) (2013)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization properties of online learning\nbased stochastic methods for supervised learning problems where the loss\nfunction is dependent on more than one training sample (e.g., metric learning,\nranking). We present a generic decoupling technique that enables us to provide\nRademacher complexity-based generalization error bounds. Our bounds are in\ngeneral tighter than those obtained by Wang et al (COLT 2012) for the same\nproblem. Using our decoupling technique, we are further able to obtain fast\nconvergence rates for strongly convex pairwise loss functions. We are also able\nto analyze a class of memory efficient online learning algorithms for pairwise\nlearning problems that use only a bounded subset of past training samples to\nupdate the hypothesis at each step. Finally, in order to complement our\ngeneralization bounds, we propose a novel memory efficient online learning\nalgorithm for higher order learning problems with bounded regret guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 13:52:37 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kar", "Purushottam", ""], ["Sriperumbudur", "Bharath K", ""], ["Jain", "Prateek", ""], ["Karnick", "Harish C", ""]]}, {"id": "1305.2532", "submitter": "Stephane Ross", "authors": "Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, J. Andrew\n  Bagnell", "title": "Learning Policies for Contextual Submodular Prediction", "comments": "13 pages. To appear in proceedings of the International Conference on\n  Machine Learning (ICML), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction domains, such as ad placement, recommendation, trajectory\nprediction, and document summarization, require predicting a set or list of\noptions. Such lists are often evaluated using submodular reward functions that\nmeasure both quality and diversity. We propose a simple, efficient, and\nprovably near-optimal approach to optimizing such prediction problems based on\nno-regret learning. Our method leverages a surprising result from online\nsubmodular optimization: a single no-regret online learner can compete with an\noptimal sequence of predictions. Compared to previous work, which either learn\na sequence of classifiers or rely on stronger assumptions such as\nrealizability, we ensure both data-efficiency as well as performance guarantees\nin the fully agnostic setting. Experiments validate the efficiency and\napplicability of the approach on a wide range of problems including manipulator\ntrajectory optimization, news recommendation and document summarization.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 18:09:52 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Ross", "Stephane", ""], ["Zhou", "Jiaji", ""], ["Yue", "Yisong", ""], ["Dey", "Debadeepta", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1305.2545", "submitter": "Aleksandrs Slivkins", "authors": "Ashwinkumar Badanidiyuru, Robert Kleinberg and Aleksandrs Slivkins", "title": "Bandits with Knapsacks", "comments": "An extended abstract of this work has appeared in the 54th IEEE\n  Symposium on Foundations of Computer Science (FOCS 2013). 55 pages. Compared\n  to the initial \"full version\" from May'13, this version has a significantly\n  revised presentation and reflects the current status of the follow-up work.\n  Also, this version contains a stronger regret bound in one of the main\n  results", "journal-ref": null, "doi": "10.1109/FOCS.2013.30", "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are the predominant theoretical model of\nexploration-exploitation tradeoffs in learning, and they have countless\napplications ranging from medical trials, to communication networks, to Web\nsearch and advertising. In many of these application domains the learner may be\nconstrained by one or more supply (or budget) limits, in addition to the\ncustomary limitation on the time horizon. The literature lacks a general model\nencompassing these sorts of problems. We introduce such a model, called\n\"bandits with knapsacks\", that combines aspects of stochastic integer\nprogramming with online learning. A distinctive feature of our problem, in\ncomparison to the existing regret-minimization literature, is that the optimal\npolicy for a given latent distribution may significantly outperform the policy\nthat plays the optimal fixed arm. Consequently, achieving sublinear regret in\nthe bandits-with-knapsacks problem is significantly more challenging than in\nconventional bandit problems.\n  We present two algorithms whose reward is close to the information-theoretic\noptimum: one is based on a novel \"balanced exploration\" paradigm, while the\nother is a primal-dual algorithm that uses multiplicative updates. Further, we\nprove that the regret achieved by both algorithms is optimal up to\npolylogarithmic factors. We illustrate the generality of the problem by\npresenting applications in a number of different domains including electronic\ncommerce, routing, and scheduling. As one example of a concrete application, we\nconsider the problem of dynamic posted pricing with limited supply and obtain\nthe first algorithm whose regret, with respect to the optimal dynamic policy,\nis sublinear in the supply.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 21:50:46 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 21:13:51 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2013 13:52:46 GMT"}, {"version": "v4", "created": "Mon, 20 Oct 2014 19:24:09 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 16:43:30 GMT"}, {"version": "v6", "created": "Fri, 31 Jul 2015 18:36:51 GMT"}, {"version": "v7", "created": "Sat, 17 Jun 2017 18:59:24 GMT"}, {"version": "v8", "created": "Tue, 5 Sep 2017 14:00:33 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1305.2581", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic dual coordinate ascent (SDCA) is an effective technique for\nsolving regularized loss minimization problems in machine learning. This paper\nconsiders an extension of SDCA under the mini-batch setting that is often used\nin practice. Our main contribution is to introduce an accelerated mini-batch\nversion of SDCA and prove a fast convergence rate for this method. We discuss\nan implementation of our method over a parallel computing system, and compare\nthe results to both the vanilla stochastic dual coordinate ascent and to the\naccelerated deterministic gradient descent method of\n\\cite{nesterov2007gradient}.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 12:46:25 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1305.2648", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Boosting with the Logistic Loss is Consistent", "comments": "To appear, COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript provides optimization guarantees, generalization bounds, and\nstatistical consistency results for AdaBoost variants which replace the\nexponential loss with the logistic and similar losses (specifically, twice\ndifferentiable convex losses which are Lipschitz and tend to zero on one side).\n  The heart of the analysis is to show that, in lieu of explicit regularization\nand constraints, the structure of the problem is fairly rigidly controlled by\nthe source distribution itself. The first control of this type is in the\nseparable case, where a distribution-dependent relaxed weak learning rate\ninduces speedy convergence with high probability over any sample. Otherwise, in\nthe nonseparable case, the convex surrogate risk itself exhibits\ndistribution-dependent levels of curvature, and consequently the algorithm's\noutput has small norm with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 00:15:14 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1305.2732", "submitter": "Gergely Neu", "authors": "Gergely Neu and G\\'abor Bart\\'ok", "title": "An efficient algorithm for learning with semi-bandit feedback", "comments": "submitted to ALT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online combinatorial optimization under\nsemi-bandit feedback. The goal of the learner is to sequentially select its\nactions from a combinatorial decision set so as to minimize its cumulative\nloss. We propose a learning algorithm for this problem based on combining the\nFollow-the-Perturbed-Leader (FPL) prediction method with a novel loss\nestimation procedure called Geometric Resampling (GR). Contrary to previous\nsolutions, the resulting algorithm can be efficiently implemented for any\ndecision set where efficient offline combinatorial optimization is possible at\nall. Assuming that the elements of the decision set can be described with\nd-dimensional binary vectors with at most m non-zero entries, we show that the\nexpected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As a\nside result, we also improve the best known regret bounds for FPL in the full\ninformation setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m)\nover previous bounds for this algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 10:39:47 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Neu", "Gergely", ""], ["Bart\u00f3k", "G\u00e1bor", ""]]}, {"id": "1305.2788", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France), Michael Eickenberg (INRIA Saclay - Ile de France, LNAO), Bertrand\n  Thirion (INRIA Saclay - Ile de France, LNAO), Alexandre Gramfort (LTCI)", "title": "HRF estimation improves sensitivity of fMRI encoding and decoding models", "comments": "3nd International Workshop on Pattern Recognition in NeuroImaging\n  (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting activation patterns from functional Magnetic Resonance Images\n(fMRI) datasets remains challenging in rapid-event designs due to the inherent\ndelay of blood oxygen level-dependent (BOLD) signal. The general linear model\n(GLM) allows to estimate the activation from a design matrix and a fixed\nhemodynamic response function (HRF). However, the HRF is known to vary\nsubstantially between subjects and brain regions. In this paper, we propose a\nmodel for jointly estimating the hemodynamic response function (HRF) and the\nactivation patterns via a low-rank representation of task effects.This model is\nbased on the linearity assumption behind the GLM and can be computed using\nstandard gradient-based solvers. We use the activation patterns computed by our\nmodel as input data for encoding and decoding studies and report performance\nimprovement in both settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 14:19:24 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France"], ["Eickenberg", "Michael", "", "INRIA Saclay - Ile de France, LNAO"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, LNAO"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1305.2982", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic neurons can be useful for a number of reasons in deep learning\nmodels, but in many cases they pose a challenging problem: how to estimate the\ngradient of a loss function with respect to the input of such stochastic\nneurons, i.e., can we \"back-propagate\" through these stochastic neurons? We\nexamine this question, existing approaches, and present two novel families of\nsolutions, applicable in different settings. In particular, it is demonstrated\nthat a simple biologically plausible formula gives rise to an an unbiased (but\nnoisy) estimator of the gradient with respect to a binary stochastic neuron\nfiring probability. Unlike other estimators which view the noise as a small\nperturbation in order to estimate gradients by finite differences, this\nestimator is unbiased even without assuming that the stochastic perturbation is\nsmall. This estimator is also interesting because it can be applied in very\ngeneral settings which do not allow gradient back-propagation, including the\nestimation of the gradient with respect to future rewards, as required in\nreinforcement learning setups. We also propose an approach to approximating\nthis unbiased but high-variance estimator by learning to predict it using a\nbiased estimator. The second approach we propose assumes that an estimator of\nthe gradient can be back-propagated and it provides an unbiased estimator of\nthe gradient, but can only work with non-linearities unlike the hard threshold,\nbut like the rectifier, that are not flat for all of their range. This is\nsimilar to traditional sigmoidal units but has the advantage that for many\ninputs, a hard decision (e.g., a 0 output) can be produced, which would be\nconvenient for conditional computation and achieving sparse representations and\nsparse gradients.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 00:29:42 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1305.3011", "submitter": "Ali Jalali", "authors": "Kuang-Chih Lee, Ali Jalali and Ali Dasdan", "title": "Real Time Bid Optimization with Smooth Budget Delivery in Online\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, billions of display ad impressions are purchased on a daily basis\nthrough a public auction hosted by real time bidding (RTB) exchanges. A\ndecision has to be made for advertisers to submit a bid for each selected RTB\nad request in milliseconds. Restricted by the budget, the goal is to buy a set\nof ad impressions to reach as many targeted users as possible. A desired action\n(conversion), advertiser specific, includes purchasing a product, filling out a\nform, signing up for emails, etc. In addition, advertisers typically prefer to\nspend their budget smoothly over the time in order to reach a wider range of\naudience accessible throughout a day and have a sustainable impact. However,\nsince the conversions occur rarely and the occurrence feedback is normally\ndelayed, it is very challenging to achieve both budget and performance goals at\nthe same time. In this paper, we present an online approach to the smooth\nbudget delivery while optimizing for the conversion performance. Our algorithm\ntries to select high quality impressions and adjust the bid price based on the\nprior performance distribution in an adaptive manner by distributing the budget\noptimally across time. Our experimental results from real advertising campaigns\ndemonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 03:39:45 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Lee", "Kuang-Chih", ""], ["Jalali", "Ali", ""], ["Dasdan", "Ali", ""]]}, {"id": "1305.3014", "submitter": "Ali Jalali", "authors": "Ali Jalali, Santanu Kolay, Peter Foldes and Ali Dasdan", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising has been introduced as one of the most efficient methods\nof advertising throughout the recent years. Yet, advertisers are concerned\nabout the efficiency of their online advertising campaigns and consequently,\nwould like to restrict their ad impressions to certain websites and/or certain\ngroups of audience. These restrictions, known as targeting criteria, limit the\nreachability for better performance. This trade-off between reachability and\nperformance illustrates a need for a forecasting system that can quickly\npredict/estimate (with good accuracy) this trade-off. Designing such a system\nis challenging due to (a) the huge amount of data to process, and, (b) the need\nfor fast and accurate estimates. In this paper, we propose a distributed fault\ntolerant system that can generate such estimates fast with good accuracy. The\nmain idea is to keep a small representative sample in memory across multiple\nmachines and formulate the forecasting problem as queries against the sample.\nThe key challenge is to find the best strata across the past data, perform\nmultivariate stratified sampling while ensuring fuzzy fall-back to cover the\nsmall minorities. Our results show a significant improvement over the uniform\nand simple stratified sampling strategies which are currently widely used in\nthe industry.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 03:48:09 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Jalali", "Ali", ""], ["Kolay", "Santanu", ""], ["Foldes", "Peter", ""], ["Dasdan", "Ali", ""]]}, {"id": "1305.3120", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Optimization with First-Order Surrogate Functions", "comments": "to appear in the proceedings of ICML 2013; the arxiv paper contains\n  the 9 pages main text followed by 26 pages of supplemental material.\n  International Conference on Machine Learning (ICML 2013) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study optimization methods consisting of iteratively\nminimizing surrogates of an objective function. By proposing several\nalgorithmic variants and simple convergence analyses, we make two main\ncontributions. First, we provide a unified viewpoint for several first-order\noptimization techniques such as accelerated proximal gradient, block coordinate\ndescent, or Frank-Wolfe algorithms. Second, we introduce a new incremental\nscheme that experimentally matches or outperforms state-of-the-art solvers for\nlarge-scale optimization problems typically arising in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 11:49:34 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1305.3149", "submitter": "Xiao-Bo Jin", "authors": "Xiao-Bo Jin, Qiang Lu, Feng Wang, Quan-gong Huo", "title": "Qualitative detection of oil adulteration with machine learning\n  approaches", "comments": "18 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The study focused on the machine learning analysis approaches to identify the\nadulteration of 9 kinds of edible oil qualitatively and answered the following\nthree questions: Is the oil sample adulterant? How does it constitute? What is\nthe main ingredient of the adulteration oil? After extracting the\nhigh-performance liquid chromatography (HPLC) data on triglyceride from 370 oil\nsamples, we applied the adaptive boosting with multi-class Hamming loss\n(AdaBoost.MH) to distinguish the oil adulteration in contrast with the support\nvector machine (SVM). Further, we regarded the adulterant oil and the pure oil\nsamples as ones with multiple labels and with only one label, respectively.\nThen multi-label AdaBoost.MH and multi-label learning vector quantization\n(ML-LVQ) model were built to determine the ingredients and their relative ratio\nin the adulteration oil. The experimental results on six measures show that\nML-LVQ achieves better performance than multi-label AdaBoost.MH.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 13:23:19 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Lu", "Qiang", ""], ["Wang", "Feng", ""], ["Huo", "Quan-gong", ""]]}, {"id": "1305.3207", "submitter": "Ilias Diakonikolas", "authors": "Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Efficient Density Estimation via Piecewise Polynomial Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a highly efficient \"semi-agnostic\" algorithm for learning univariate\nprobability distributions that are well approximated by piecewise polynomial\ndensity functions. Let $p$ be an arbitrary distribution over an interval $I$\nwhich is $\\tau$-close (in total variation distance) to an unknown probability\ndistribution $q$ that is defined by an unknown partition of $I$ into $t$\nintervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each of\nthe intervals. We give an algorithm that draws $\\tilde{O}(t\\new{(d+1)}/\\eps^2)$\nsamples from $p$, runs in time $\\poly(t,d,1/\\eps)$, and with high probability\noutputs a piecewise polynomial hypothesis distribution $h$ that is\n$(O(\\tau)+\\eps)$-close (in total variation distance) to $p$. This sample\ncomplexity is essentially optimal; we show that even for $\\tau=0$, any\nalgorithm that learns an unknown $t$-piecewise degree-$d$ probability\ndistribution over $I$ to accuracy $\\eps$ must use $\\Omega({\\frac {t(d+1)}\n{\\poly(1 + \\log(d+1))}} \\cdot {\\frac 1 {\\eps^2}})$ samples from the\ndistribution, regardless of its running time. Our algorithm combines tools from\napproximation theory, uniform convergence, linear programming, and dynamic\nprogramming.\n  We apply this general algorithm to obtain a wide range of results for many\nnatural problems in density estimation over both continuous and discrete\ndomains. These include state-of-the-art results for learning mixtures of\nlog-concave distributions; mixtures of $t$-modal distributions; mixtures of\nMonotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions;\nmixtures of Gaussians; and mixtures of $k$-monotone densities. Our general\ntechnique yields computationally efficient algorithms for all these problems,\nin many cases with provably optimal sample complexities (up to logarithmic\nfactors) in all parameters.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 16:54:10 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1305.3334", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mingyan Liu", "title": "Online Learning in a Contract Selection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an online contract selection problem there is a seller which offers a set\nof contracts to sequentially arriving buyers whose types are drawn from an\nunknown distribution. If there exists a profitable contract for the buyer in\nthe offered set, i.e., a contract with payoff higher than the payoff of not\naccepting any contracts, the buyer chooses the contract that maximizes its\npayoff. In this paper we consider the online contract selection problem to\nmaximize the sellers profit. Assuming that a structural property called ordered\npreferences holds for the buyer's payoff function, we propose online learning\nalgorithms that have sub-linear regret with respect to the best set of\ncontracts given the distribution over the buyer's type. This problem has many\napplications including spectrum contracts, wireless service provider data plans\nand recommendation systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 01:22:34 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1305.3384", "submitter": "Lior Rokach", "authors": "Naseem Biadsy, Lior Rokach, Armin Shmilovici", "title": "Transfer Learning for Content-Based Recommender Systems using Tree\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to content-based transfer learning\nfor solving the data sparsity problem in cases when the users' preferences in\nthe target domain are either scarce or unavailable, but the necessary\ninformation on the preferences exists in another domain. We show that training\na system to use such information across domains can produce better performance.\nSpecifically, we represent users' behavior patterns based on topological graph\nstructures. Each behavior pattern represents the behavior of a set of users,\nwhen the users' behavior is defined as the items they rated and the items'\nrating values. In the next step we find a correlation between behavior patterns\nin the source domain and behavior patterns in the target domain. This mapping\nis considered a bridge between the two domains. Based on the correlation and\ncontent-attributes of the items, we train a machine learning model to predict\nusers' ratings in the target domain. When we compare our approach to the\npopularity approach and KNN-cross-domain on a real world dataset, the results\nshow that on an average of 83$%$ of the cases our approach outperforms both\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 08:00:54 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Biadsy", "Naseem", ""], ["Rokach", "Lior", ""], ["Shmilovici", "Armin", ""]]}, {"id": "1305.3486", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Noisy Subspace Clustering via Thresholding", "comments": "Presented at the IEEE Int. Symp. Inf. Theory (ISIT) 2013, Istanbul,\n  Turkey. The version posted here corrects a minor error in the published\n  version. Specifically, the exponent -c n_l in the success probability of\n  Theorem 1 and in the corresponding proof outline has been corrected to\n  -c(n_l-1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy high-dimensional data points into\na union of low-dimensional subspaces and a set of outliers. The number of\nsubspaces, their dimensions, and their orientations are unknown. A\nprobabilistic performance analysis of the thresholding-based subspace\nclustering (TSC) algorithm introduced recently in [1] shows that TSC succeeds\nin the noisy case, even when the subspaces intersect. Our results reveal an\nexplicit tradeoff between the allowed noise level and the affinity of the\nsubspaces. We furthermore find that the simple outlier detection scheme\nintroduced in [1] provably succeeds in the noisy case.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 14:12:50 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 11:04:58 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1305.3794", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger and Michael Kommenda", "title": "Evolution of Covariance Functions for Gaussian Process Regression using\n  Genetic Programming", "comments": "Presented at the Workshop \"Theory and Applications of Metaheuristic\n  Algorithms\", EUROCAST2013. To appear in selected papers of Computer Aided\n  Systems Theory - EUROCAST 2013; Volumes Editors: Roberto Moreno-D\\'iaz, Franz\n  R. Pichler, Alexis Quesada-Arencibia; LNCS Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we describe an approach to evolve composite covariance\nfunctions for Gaussian processes using genetic programming. A critical aspect\nof Gaussian processes and similar kernel-based models such as SVM is, that the\ncovariance function should be adapted to the modeled data. Frequently, the\nsquared exponential covariance function is used as a default. However, this can\nlead to a misspecified model, which does not fit the data well. In the proposed\napproach we use a grammar for the composition of covariance functions and\ngenetic programming to search over the space of sentences that can be derived\nfrom the grammar. We tested the proposed approach on synthetic data from\ntwo-dimensional test functions, and on the Mauna Loa CO2 time series. The\nresults show, that our approach is feasible, finding covariance functions that\nperform much better than a default covariance function. For the CO2 data set a\ncomposite covariance function is found, that matches the performance of a\nhand-tuned covariance function.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 13:25:20 GMT"}, {"version": "v2", "created": "Wed, 22 May 2013 09:28:04 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Kronberger", "Gabriel", ""], ["Kommenda", "Michael", ""]]}, {"id": "1305.3814", "submitter": "Ali Hadian", "authors": "Ali Hadian, Behrouz Minaei-Bidgoli", "title": "Multi-View Learning for Web Spam Detection", "comments": "I want to upload a major revision in a couple of months", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spam pages are designed to maliciously appear among the top search results by\nexcessive usage of popular terms. Therefore, spam pages should be removed using\nan effective and efficient spam detection system. Previous methods for web spam\nclassification used several features from various information sources (page\ncontents, web graph, access logs, etc.) to detect web spam. In this paper, we\nfollow page-level classification approach to build fast and scalable spam\nfilters. We show that each web page can be classified with satisfiable accuracy\nusing only its own HTML content. In order to design a multi-view classification\nsystem, we used state-of-the-art spam classification methods with distinct\nfeature sets (views) as the base classifiers. Then, a fusion model is learned\nto combine the output of the base classifiers and make final prediction.\nResults show that multi-view learning significantly improves the classification\nperformance, namely AUC by 22%, while providing linear speedup for parallel\nexecution.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 14:11:02 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 05:21:16 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Hadian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1305.3932", "submitter": "Reid Priedhorsky", "authors": "Reid Priedhorsky (1), Aron Culotta (2), Sara Y. Del Valle (1) ((1) Los\n  Alamos National Laboratory, (2) Illinois Institute of Technology)", "title": "Inferring the Origin Locations of Tweets with Quantitative Confidence", "comments": "14 pages, 6 figures. Version 2: Move mathematics to appendix, 2 new\n  references, various other presentation improvements. Version 3: Various\n  presentation improvements, accepted at ACM CSCW 2014", "journal-ref": null, "doi": "10.1145/2531602.2531607", "report-no": "LA-UR 13-23557", "categories": "cs.SI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Internet content plays an increasingly critical role in many domains,\nincluding public health, disaster management, and politics. However, its\nutility is limited by missing geographic information; for example, fewer than\n1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable,\ncontent-based approach to estimate the location of tweets using a novel yet\nsimple variant of gaussian mixture models. Further, because real-world\napplications depend on quantified uncertainty for such estimates, we propose\nnovel metrics of accuracy, precision, and calibration, and we evaluate our\napproach accordingly. Experiments on 13 million global, comprehensively\nmulti-lingual tweets show that our approach yields reliable, well-calibrated\nresults competitive with previous computationally intensive methods. We also\nshow that a relatively small number of training data are required for good\nestimates (roughly 30,000 tweets) and models are quite time-invariant\n(effective on tweets many weeks newer than the training set). Finally, we show\nthat toponyms and languages with small geographic footprint provide the most\nuseful location signals.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 20:47:05 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2013 22:48:26 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2013 00:06:38 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Priedhorsky", "Reid", ""], ["Culotta", "Aron", ""], ["Del Valle", "Sara Y.", ""]]}, {"id": "1305.4076", "submitter": "Fuqiang Chen", "authors": "Fu-qiang Chen, Yan Wu, Guo-dong Zhao, Jun-ming Zhang, Ming Zhu, Jing\n  Bai", "title": "Contractive De-noising Auto-encoder", "comments": "Figures edited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoder is a special kind of neural network based on reconstruction.\nDe-noising auto-encoder (DAE) is an improved auto-encoder which is robust to\nthe input by corrupting the original data first and then reconstructing the\noriginal input by minimizing the reconstruction error function. And contractive\nauto-encoder (CAE) is another kind of improved auto-encoder to learn robust\nfeature by introducing the Frobenius norm of the Jacobean matrix of the learned\nfeature with respect to the original input. In this paper, we combine\nde-noising auto-encoder and contractive auto- encoder, and propose another\nimproved auto-encoder, contractive de-noising auto- encoder (CDAE), which is\nrobust to both the original input and the learned feature. We stack CDAE to\nextract more abstract features and apply SVM for classification. The experiment\nresult on benchmark dataset MNIST shows that our proposed CDAE performed better\nthan both DAE and CAE, proving the effective of our method.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 13:42:49 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 04:22:44 GMT"}, {"version": "v3", "created": "Thu, 30 May 2013 00:01:45 GMT"}, {"version": "v4", "created": "Mon, 10 Mar 2014 13:41:32 GMT"}, {"version": "v5", "created": "Wed, 23 Apr 2014 11:40:12 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Chen", "Fu-qiang", ""], ["Wu", "Yan", ""], ["Zhao", "Guo-dong", ""], ["Zhang", "Jun-ming", ""], ["Zhu", "Ming", ""], ["Bai", "Jing", ""]]}, {"id": "1305.4081", "submitter": "Patrick Hop", "authors": "Patrick Hop, Xinghao Pan", "title": "Conditions for Convergence in Regularized Machine Learning Objectives", "comments": "3 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of the convergence rates of modern convex optimization algorithms\ncan be achived through binary means: analysis of emperical convergence, or\nanalysis of theoretical convergence. These two pathways of capturing\ninformation diverge in efficacy when moving to the world of distributed\ncomputing, due to the introduction of non-intuitive, non-linear slowdowns\nassociated with broadcasting, and in some cases, gathering operations. Despite\nthese nuances in the rates of convergence, we can still show the existence of\nconvergence, and lower bounds for the rates. This paper will serve as a helpful\ncheat-sheet for machine learning practitioners encountering this problem class\nin the field.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 13:53:17 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Hop", "Patrick", ""], ["Pan", "Xinghao", ""]]}, {"id": "1305.4204", "submitter": "Joel Ratsaby", "authors": "Uzi Chester, Joel Ratsaby", "title": "Machine learning on images using a string-distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for image feature-extraction which is based on\nrepresenting an image by a finite-dimensional vector of distances that measure\nhow different the image is from a set of image prototypes. We use the recently\nintroduced Universal Image Distance (UID) \\cite{RatsabyChesterIEEE2012} to\ncompare the similarity between an image and a prototype image. The advantage in\nusing the UID is the fact that no domain knowledge nor any image analysis need\nto be done. Each image is represented by a finite dimensional feature vector\nwhose components are the UID values between the image and a finite set of image\nprototypes from each of the feature categories. The method is automatic since\nonce the user selects the prototype images, the feature vectors are\nautomatically calculated without the need to do any image analysis. The\nprototype images can be of different size, in particular, different than the\nimage size. Based on a collection of such cases any supervised or unsupervised\nlearning algorithm can be used to train and produce an image classifier or\nimage cluster analysis. In this paper we present the image feature-extraction\nmethod and use it on several supervised and unsupervised learning experiments\nfor satellite image data.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 22:40:14 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Chester", "Uzi", ""], ["Ratsaby", "Joel", ""]]}, {"id": "1305.4324", "submitter": "Fares Hedayati Fares Hedayati", "authors": "Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati,\n  Wojciech Kotlowski", "title": "Horizon-Independent Optimal Prediction with Log-Loss in Exponential\n  Families", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning under logarithmic loss with regular parametric\nmodels. Hedayati and Bartlett (2012b) showed that a Bayesian prediction\nstrategy with Jeffreys prior and sequential normalized maximum likelihood\n(SNML) coincide and are optimal if and only if the latter is exchangeable, and\nif and only if the optimal strategy can be calculated without knowing the time\nhorizon in advance. They put forward the question what families have\nexchangeable SNML strategies. This paper fully answers this open problem for\none-dimensional exponential families. The exchangeability can happen only for\nthree classes of natural exponential family distributions, namely the Gaussian,\nGamma, and the Tweedie exponential family of order 3/2. Keywords: SNML\nExchangeability, Exponential Family, Online Learning, Logarithmic Loss,\nBayesian Strategy, Jeffreys Prior, Fisher Information1\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 04:56:05 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Bartlett", "Peter", ""], ["Grunwald", "Peter", ""], ["Harremoes", "Peter", ""], ["Hedayati", "Fares", ""], ["Kotlowski", "Wojciech", ""]]}, {"id": "1305.4339", "submitter": "Michiaki Hamada", "authors": "Michiaki Hamada, Hisanori Kiryu, Wataru Iwasaki and Kiyoshi Asai", "title": "Generalized Centroid Estimators in Bioinformatics", "comments": "35 pages. This is a corrected version of the published paper: PLoS\n  ONE 6(2):e16450, 2011. The original version is available from\n  http://www.plosone.org/article/info:doi/10.1371/journal.pone.0016450", "journal-ref": "PLoS ONE 6(2):e16450, 2011", "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a number of estimation problems in bioinformatics, accuracy measures of\nthe target problem are usually given, and it is important to design estimators\nthat are suitable to those accuracy measures. However, there is often a\ndiscrepancy between an employed estimator and a given accuracy measure of the\nproblem. In this study, we introduce a general class of efficient estimators\nfor estimation problems on high-dimensional binary spaces, which representmany\nfundamental problems in bioinformatics. Theoretical analysis reveals that the\nproposed estimators generally fit with commonly-used accuracy measures (e.g.\nsensitivity, PPV, MCC and F-score) as well as it can be computed efficiently in\nmany cases, and cover a wide range of problems in bioinformatics from the\nviewpoint of the principle of maximum expected accuracy (MEA). It is also shown\nthat some important algorithms in bioinformatics can be interpreted in a\nunified manner. Not only the concept presented in this paper gives a useful\nframework to design MEA-based estimators but also it is highly extendable and\nsheds new light on many problems in bioinformatics.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 07:50:14 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Hamada", "Michiaki", ""], ["Kiryu", "Hisanori", ""], ["Iwasaki", "Wataru", ""], ["Asai", "Kiyoshi", ""]]}, {"id": "1305.4345", "submitter": "Alon Schclar", "authors": "Alon Schclar and Lior Rokach and Amir Amit", "title": "Ensembles of Classifiers based on Dimensionality Reduction", "comments": "31 pages, 4 figures, 4 tables, Submitted to Pattern Analysis and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the construction of ensemble classifiers\nbased on dimensionality reduction. Dimensionality reduction methods represent\ndatasets using a small number of attributes while preserving the information\nconveyed by the original dataset. The ensemble members are trained based on\ndimension-reduced versions of the training set. These versions are obtained by\napplying dimensionality reduction to the original training set using different\nvalues of the input parameters. This construction meets both the diversity and\naccuracy criteria which are required to construct an ensemble classifier where\nthe former criterion is obtained by the various input parameter values and the\nlatter is achieved due to the decorrelation and noise reduction properties of\ndimensionality reduction. In order to classify a test sample, it is first\nembedded into the dimension reduced space of each individual classifier by\nusing an out-of-sample extension algorithm. Each classifier is then applied to\nthe embedded sample and the classification is obtained via a voting scheme. We\npresent three variations of the proposed approach based on the Random\nProjections, the Diffusion Maps and the Random Subspaces dimensionality\nreduction algorithms. We also present a multi-strategy ensemble which combines\nAdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost,\nRotation Forest ensemble classifiers and also with the base classifier which\ndoes not incorporate dimensionality reduction. Our experiments used seventeen\nbenchmark datasets from the UCI repository. The results obtained by the\nproposed algorithms were superior in many cases to other algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 10:24:06 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Schclar", "Alon", ""], ["Rokach", "Lior", ""], ["Amit", "Amir", ""]]}, {"id": "1305.4433", "submitter": "Xiangnan Kong", "authors": "Xiangnan Kong, Bokai Cao, Philip S. Yu, Ying Ding and David J. Wild", "title": "Meta Path-Based Collective Classification in Heterogeneous Information\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective classification has been intensively studied due to its impact in\nmany important applications, such as web mining, bioinformatics and citation\nanalysis. Collective classification approaches exploit the dependencies of a\ngroup of linked objects whose class labels are correlated and need to be\npredicted simultaneously. In this paper, we focus on studying the collective\nclassification problem in heterogeneous networks, which involves multiple types\nof data objects interconnected by multiple types of links. Intuitively, two\nobjects are correlated if they are linked by many paths in the network.\nHowever, most existing approaches measure the dependencies among objects\nthrough directly links or indirect links without considering the different\nsemantic meanings behind different paths. In this paper, we study the\ncollective classification problem taht is defined among the same type of\nobjects in heterogenous networks. Moreover, by considering different linkage\npaths in the network, one can capture the subtlety of different types of\ndependencies among objects. We introduce the concept of meta-path based\ndependencies among objects, where a meta path is a path consisting a certain\nsequence of linke types. We show that the quality of collective classification\nresults strongly depends upon the meta paths used. To accommodate the large\nnetwork size, a novel solution, called HCC (meta-path based Heterogenous\nCollective Classification), is developed to effectively assign labels to a\ngroup of instances that are interconnected through different meta-paths. The\nproposed HCC model can capture different types of dependencies among objects\nwith respect to different meta paths. Empirical studies on real-world networks\ndemonstrate that effectiveness of the proposed meta path-based collective\nclassification approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 04:05:23 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Kong", "Xiangnan", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""], ["Ding", "Ying", ""], ["Wild", "David J.", ""]]}, {"id": "1305.4525", "submitter": "Miron Kursa", "authors": "Miron B. Kursa", "title": "Robustness of Random Forest-based gene selection methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene selection is an important part of microarray data analysis because it\nprovides information that can lead to a better mechanistic understanding of an\ninvestigated phenomenon. At the same time, gene selection is very difficult\nbecause of the noisy nature of microarray data. As a consequence, gene\nselection is often performed with machine learning methods. The Random Forest\nmethod is particularly well suited for this purpose. In this work, four\nstate-of-the-art Random Forest-based feature selection methods were compared in\na gene selection context. The analysis focused on the stability of selection\nbecause, although it is necessary for determining the significance of results,\nit is often ignored in similar studies.\n  The comparison of post-selection accuracy in the validation of Random Forest\nclassifiers revealed that all investigated methods were equivalent in this\ncontext. However, the methods substantially differed with respect to the number\nof selected genes and the stability of selection. Of the analysed methods, the\nBoruta algorithm predicted the most genes as potentially important.\n  The post-selection classifier error rate, which is a frequently used measure,\nwas found to be a potentially deceptive measure of gene selection quality. When\nthe number of consistently selected genes was considered, the Boruta algorithm\nwas clearly the best. Although it was also the most computationally intensive\nmethod, the Boruta algorithm's computational demands could be reduced to levels\ncomparable to those of other algorithms by replacing the Random Forest\nimportance with a comparable measure from Random Ferns (a similar but\nsimplified classifier). Despite their design assumptions, the minimal optimal\nselection methods, were found to select a high fraction of false positives.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 13:39:03 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 23:12:50 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 15:30:45 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Kursa", "Miron B.", ""]]}, {"id": "1305.4723", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Lin Xiao", "title": "On the Complexity Analysis of Randomized Block-Coordinate Descent\n  Methods", "comments": "26 pages (submitted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the randomized block-coordinate descent (RBCD)\nmethods proposed in [8,11] for minimizing the sum of a smooth convex function\nand a block-separable convex function. In particular, we extend Nesterov's\ntechnique developed in [8] for analyzing the RBCD method for minimizing a\nsmooth convex function over a block-separable closed convex set to the\naforementioned more general problem and obtain a sharper expected-value type of\nconvergence rate than the one implied in [11]. Also, we obtain a better\nhigh-probability type of iteration complexity, which improves upon the one in\n[11] by at least the amount $O(n/\\epsilon)$, where $\\epsilon$ is the target\nsolution accuracy and $n$ is the number of problem blocks. In addition, for\nunconstrained smooth convex minimization, we develop a new technique called\n{\\it randomized estimate sequence} to analyze the accelerated RBCD method\nproposed by Nesterov [11] and establish a sharper expected-value type of\nconvergence rate than the one given in [11].\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 06:12:42 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Lu", "Zhaosong", ""], ["Xiao", "Lin", ""]]}, {"id": "1305.4757", "submitter": "Parasaran Raman", "authors": "Parasaran Raman and Suresh Venkatasubramanian", "title": "Power to the Points: Validating Data Memberships in Clusterings", "comments": "18 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clustering is an implicit assignment of labels of points, based on\nproximity to other points. It is these labels that are then used for downstream\nanalysis (either focusing on individual clusters, or identifying\nrepresentatives of clusters and so on). Thus, in order to trust a clustering as\na first step in exploratory data analysis, we must trust the labels assigned to\nindividual data. Without supervision, how can we validate this assignment? In\nthis paper, we present a method to attach affinity scores to the implicit\nlabels of individual points in a clustering. The affinity scores capture the\nconfidence level of the cluster that claims to \"own\" the point. This method is\nvery general: it can be used with clusterings derived from Euclidean data,\nkernelized data, or even data derived from information spaces. It smoothly\nincorporates importance functions on clusters, allowing us to eight different\nclusters differently. It is also efficient: assigning an affinity score to a\npoint depends only polynomially on the number of clusters and is independent of\nthe number of points in the data. The dimensionality of the underlying space\nonly appears in preprocessing. We demonstrate the value of our approach with an\nexperimental study that illustrates the use of these scores in different data\nanalysis tasks, as well as the efficiency and flexibility of the method. We\nalso demonstrate useful visualizations of these scores; these might prove\nuseful within an interactive analytics framework.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 08:51:30 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Raman", "Parasaran", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1305.4778", "submitter": "Bruno Ziliotto", "authors": "Bruno Ziliotto", "title": "Zero-sum repeated games: Counterexamples to the existence of the\n  asymptotic value and the conjecture\n  $\\operatorname{maxmin}=\\operatorname{lim}v_n$", "comments": "Published at http://dx.doi.org/10.1214/14-AOP997 in the Annals of\n  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Probability 2016, Vol. 44, No. 2, 1107-1133", "doi": "10.1214/14-AOP997", "report-no": "IMS-AOP-AOP997", "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mertens [In Proceedings of the International Congress of Mathematicians\n(Berkeley, Calif., 1986) (1987) 1528-1577 Amer. Math. Soc.] proposed two\ngeneral conjectures about repeated games: the first one is that, in any\ntwo-person zero-sum repeated game, the asymptotic value exists, and the second\none is that, when Player 1 is more informed than Player 2, in the long run\nPlayer 1 is able to guarantee the asymptotic value. We disprove these two\nlong-standing conjectures by providing an example of a zero-sum repeated game\nwith public signals and perfect observation of the actions, where the value of\nthe $\\lambda$-discounted game does not converge when $\\lambda$ goes to 0. The\naforementioned example involves seven states, two actions and two signals for\neach player. Remarkably, players observe the payoffs, and play in turn.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 10:32:29 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 10:31:22 GMT"}, {"version": "v3", "created": "Thu, 11 Dec 2014 12:48:54 GMT"}, {"version": "v4", "created": "Tue, 15 Mar 2016 10:55:10 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Ziliotto", "Bruno", ""]]}, {"id": "1305.4955", "submitter": "Arthur Carvalho", "authors": "Renato Oliveira and Paulo Adeodato and Arthur Carvalho and Icamaan\n  Viegas and Christian Diego and Tsang Ing-Ren", "title": "A Data Mining Approach to Solve the Goal Scoring Problem", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2009.5178616", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In soccer, scoring goals is a fundamental objective which depends on many\nconditions and constraints. Considering the RoboCup soccer 2D-simulator, this\npaper presents a data mining-based decision system to identify the best time\nand direction to kick the ball towards the goal to maximize the overall chances\nof scoring during a simulated soccer match. Following the CRISP-DM methodology,\ndata for modeling were extracted from matches of major international\ntournaments (10691 kicks), knowledge about soccer was embedded via\ntransformation of variables and a Multilayer Perceptron was used to estimate\nthe scoring chance. Experimental performance assessment to compare this\napproach against previous LDA-based approach was conducted from 100 matches.\nSeveral statistical metrics were used to analyze the performance of the system\nand the results showed an increase of 7.7% in the number of kicks, producing an\noverall increase of 78% in the number of goals scored.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 20:29:02 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 21:59:35 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Oliveira", "Renato", ""], ["Adeodato", "Paulo", ""], ["Carvalho", "Arthur", ""], ["Viegas", "Icamaan", ""], ["Diego", "Christian", ""], ["Ing-Ren", "Tsang", ""]]}, {"id": "1305.4987", "submitter": "Julie Tibshirani", "authors": "Julie Tibshirani and Christopher D. Manning", "title": "Robust Logistic Regression using Shift Parameters (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation errors can significantly hurt classifier performance, yet datasets\nare only growing noisier with the increased use of Amazon Mechanical Turk and\ntechniques like distant supervision that automatically generate labels. In this\npaper, we present a robust extension of logistic regression that incorporates\nthe possibility of mislabelling directly into the objective. Our model can be\ntrained through nearly the same means as logistic regression, and retains its\nefficiency on high-dimensional datasets. Through named entity recognition\nexperiments, we demonstrate that our approach can provide a significant\nimprovement over the standard model when annotation errors are present.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:36:18 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 07:32:58 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Tibshirani", "Julie", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1305.5029", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang and John C. Duchi and Martin J. Wainwright", "title": "Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with\n  Minimax Optimal Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish optimal convergence rates for a decomposition-based scalable\napproach to kernel ridge regression. The method is simple to describe: it\nrandomly partitions a dataset of size N into m subsets of equal size, computes\nan independent kernel ridge regression estimator for each subset, then averages\nthe local solutions into a global predictor. This partitioning leads to a\nsubstantial reduction in computation time versus the standard approach of\nperforming kernel ridge regression on all N samples. Our two main theorems\nestablish that despite the computational speed-up, statistical optimality is\nretained: as long as m is not too large, the partition-based estimator achieves\nthe statistical minimax rate over all estimators using the set of N samples. As\nconcrete examples, our theory guarantees that the number of processors m may\ngrow nearly linearly for finite-rank kernels and Gaussian kernels and\npolynomially in N for Sobolev spaces, which in turn allows for substantial\nreductions in computational cost. We conclude with experiments on both\nsimulated data and a music-prediction task that complement our theoretical\nresults, exhibiting the computational and statistical benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 06:30:46 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 22:02:35 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zhang", "Yuchen", ""], ["Duchi", "John C.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1305.5078", "submitter": "Miron Kursa", "authors": "Alicja A. Wieczorkowska, Miron B. Kursa", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments\n  in Jazz Recordings", "comments": null, "journal-ref": "Foundations of Intelligent Systems, Lecture Notes in Computer\n  Science Volume 7661, 2012, pp 208-217", "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first apply random ferns for classification of real music\nrecordings of a jazz band. No initial segmentation of audio data is assumed,\ni.e., no onset, offset, nor pitch data are needed. The notion of random ferns\nis described in the paper, to familiarize the reader with this classification\nalgorithm, which was introduced quite recently and applied so far in image\nrecognition tasks. The performance of random ferns is compared with random\nforests for the same data. The results of experiments are presented in the\npaper, and conclusions are drawn.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 10:43:25 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Wieczorkowska", "Alicja A.", ""], ["Kursa", "Miron B.", ""]]}, {"id": "1305.5306", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n  Classification and Annotation", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to perform scene recognition and annotation. Recently, a\nnew type of topic model called the Document Neural Autoregressive Distribution\nEstimator (DocNADE) was proposed and demonstrated state-of-the-art performance\nfor document modeling. In this work, we show how to successfully apply and\nextend this model to the context of visual scene modeling. Specifically, we\npropose SupDocNADE, a supervised extension of DocNADE, that increases the\ndiscriminative power of the hidden topic features by incorporating label\ninformation into the training objective of the model. We also describe how to\nleverage information about the spatial position of the visual words and how to\nembed additional image annotations, so as to simultaneously perform image\nclassification and annotation. We test our model on the Scene15, LabelMe and\nUIUC-Sports datasets and show that it compares favorably to other topic models\nsuch as the supervised variant of LDA.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 03:35:31 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1305.5399", "submitter": "Gilles Stoltz", "authors": "Shie Mannor (EE-Technion), Vianney Perchet (LPMA), Gilles Stoltz\n  (INRIA Paris - Rocquencourt, DMA, GREGH)", "title": "A Primal Condition for Approachability with Partial Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In approachability with full monitoring there are two types of conditions\nthat are known to be equivalent for convex sets: a primal and a dual condition.\nThe primal one is of the form: a set C is approachable if and only all\ncontaining half-spaces are approachable in the one-shot game; while the dual\none is of the form: a convex set C is approachable if and only if it intersects\nall payoff sets of a certain form. We consider approachability in games with\npartial monitoring. In previous works (Perchet 2011; Mannor et al. 2011) we\nprovided a dual characterization of approachable convex sets; we also exhibited\nefficient strategies in the case where C is a polytope. In this paper we\nprovide primal conditions on a convex set to be approachable with partial\nmonitoring. They depend on a modified reward function and lead to\napproachability strategies, based on modified payoff functions, that proceed by\nprojections similarly to Blackwell's (1956) strategy; this is in contrast with\npreviously studied strategies in this context that relied mostly on the\nsignaling structure and aimed at estimating well the distributions of the\nsignals received. Our results generalize classical results by Kohlberg 1975\n(see also Mertens et al. 1994) and apply to games with arbitrary signaling\nstructure as well as to arbitrary convex sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 12:44:29 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Mannor", "Shie", "", "EE-Technion"], ["Perchet", "Vianney", "", "LPMA"], ["Stoltz", "Gilles", "", "INRIA Paris - Rocquencourt, DMA, GREGH"]]}, {"id": "1305.5734", "submitter": "Yin Song", "authors": "Yin Song, Longbing Cao, Xuhui Fan, Wei Cao and Jian Zhang", "title": "Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative model, the latent Dirichlet hidden Markov\nmodels (LDHMM), for characterizing a database of sequential behaviors\n(sequences). LDHMMs posit that each sequence is generated by an underlying\nMarkov chain process, which are controlled by the corresponding parameters\n(i.e., the initial state vector, transition matrix and the emission matrix).\nThese sequence-level latent parameters for each sequence are modeled as latent\nDirichlet random variables and parameterized by a set of deterministic\ndatabase-level hyper-parameters. Through this way, we expect to model the\nsequence in two levels: the database level by deterministic hyper-parameters\nand the sequence-level by latent parameters. To learn the deterministic\nhyper-parameters and approximate posteriors of parameters in LDHMMs, we propose\nan iterative algorithm under the variational EM framework, which consists of E\nand M steps. We examine two different schemes, the fully-factorized and\npartially-factorized forms, for the framework, based on different assumptions.\nWe present empirical results of behavior modeling and sequence classification\non three real-world data sets, and compare them to other related models. The\nexperimental results prove that the proposed LDHMMs produce better\ngeneralization performance in terms of log-likelihood and deliver competitive\nresults on the sequence classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 13:51:20 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Song", "Yin", ""], ["Cao", "Longbing", ""], ["Fan", "Xuhui", ""], ["Cao", "Wei", ""], ["Zhang", "Jian", ""]]}, {"id": "1305.5782", "submitter": "Christopher Aicher", "authors": "Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset", "title": "Adapting the Stochastic Block Model to Edge-Weighted Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the stochastic block model to the important case in which edges\nare annotated with weights drawn from an exponential family distribution. This\ngeneralization introduces several technical difficulties for model estimation,\nwhich we solve using a Bayesian approach. We introduce a variational algorithm\nthat efficiently approximates the model's posterior distribution for dense\ngraphs. In specific numerical experiments on edge-weighted networks, this\nweighted stochastic block model outperforms the common approach of first\napplying a single threshold to all weights and then applying the classic\nstochastic block model, which can obscure latent block structure in networks.\nThis model will enable the recovery of latent structure in a broader range of\nnetwork data than was previously possible.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 16:32:10 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Aicher", "Christopher", ""], ["Jacobs", "Abigail Z.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1305.5826", "submitter": "Kian Hsiang Low", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "29th Conference on Uncertainty in Artificial Intelligence (UAI 2013),\n  Extended version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:00:28 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1305.5829", "submitter": "Li Hou-biao", "authors": "Shu-Zhen Lai, Hou-Biao Li, Zu-Tao Zhang", "title": "A Symmetric Rank-one Quasi Newton Method for Non-negative Matrix\n  Factorization", "comments": "19 pages, 13 figures, Submitted to PP on Feb. 5, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we all known, the nonnegative matrix factorization (NMF) is a dimension\nreduction method that has been widely used in image processing, text\ncompressing and signal processing etc. In this paper, an algorithm for\nnonnegative matrix approximation is proposed. This method mainly bases on the\nactive set and the quasi-Newton type algorithm, by using the symmetric rank-one\nand negative curvature direction technologies to approximate the Hessian\nmatrix. Our method improves the recent results of those methods in [Pattern\nRecognition, 45(2012)3557-3565; SIAM J. Sci. Comput., 33(6)(2011)3261-3281;\nNeural Computation, 19(10)(2007)2756-2779, etc.]. Moreover, the object function\ndecreases faster than many other NMF methods. In addition, some numerical\nexperiments are presented in the synthetic data, imaging processing and text\nclustering. By comparing with the other six nonnegative matrix approximation\nmethods, our experiments confirm to our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:09:02 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Lai", "Shu-Zhen", ""], ["Li", "Hou-Biao", ""], ["Zhang", "Zu-Tao", ""]]}, {"id": "1305.6046", "submitter": "Sidahmed Mokeddem", "authors": "Sidahmed Mokeddem, Baghdad Atmani and Mostefa Mokaddem", "title": "Supervised Feature Selection for Diagnosis of Coronary Artery Disease\n  Based on Genetic Algorithm", "comments": "First International Conference on Computational Science and\n  Engineering (CSE-2013), May 18 ~ 19, 2013, Dubai, UAE. Volume Editors:\n  Sundarapandian Vaidyanathan, Dhinaharan Nagamalai", "journal-ref": null, "doi": "10.5121/csit.2013.3305", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Selection (FS) has become the focus of much research on decision\nsupport systems areas for which data sets with tremendous number of variables\nare analyzed. In this paper we present a new method for the diagnosis of\nCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes\nNaive (BN) based FS. Basically, CAD dataset contains two classes defined with\n13 features. In GA BN algorithm, GA generates in each iteration a subset of\nattributes that will be evaluated using the BN in the second step of the\nselection procedure. The final set of attribute contains the most relevant\nfeature model that increases the accuracy. The algorithm in this case produces\n85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the\nAlgorithm is then compared with the use of Support Vector Machine (SVM),\nMultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of\nclassification accuracy for those algorithms are respectively 83.5%, 83.16% and\n80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared\nwith other FS algorithms. The Obtained results have shown very promising\noutcomes for the diagnosis of CAD.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 18:16:52 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Mokeddem", "Sidahmed", ""], ["Atmani", "Baghdad", ""], ["Mokaddem", "Mostefa", ""]]}, {"id": "1305.6129", "submitter": "Kian Hsiang Low", "authors": "Kian Hsiang Low, John M. Dolan, Pradeep Khosla", "title": "Information-Theoretic Approach to Efficient Adaptive Path Planning for\n  Mobile Robotic Environmental Sensing", "comments": "19th International Conference on Automated Planning and Scheduling\n  (ICAPS 2009), Extended version with proofs, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in robot exploration and mapping has focused on sampling\nenvironmental hotspot fields. This exploration task is formalized by Low,\nDolan, and Khosla (2008) in a sequential decision-theoretic planning under\nuncertainty framework called MASP. The time complexity of solving MASP\napproximately depends on the map resolution, which limits its use in\nlarge-scale, high-resolution exploration and mapping. To alleviate this\ncomputational difficulty, this paper presents an information-theoretic approach\nto MASP (iMASP) for efficient adaptive path planning; by reformulating the\ncost-minimizing iMASP as a reward-maximizing problem, its time complexity\nbecomes independent of map resolution and is less sensitive to increasing robot\nteam size as demonstrated both theoretically and empirically. Using the\nreward-maximizing dual, we derive a novel adaptive variant of maximum entropy\nsampling, thus improving the induced exploration policy performance. It also\nallows us to establish theoretical bounds quantifying the performance advantage\nof optimal adaptive over non-adaptive policies and the performance quality of\napproximately optimal vs. optimal adaptive policies. We show analytically and\nempirically the superior performance of iMASP-based policies for sampling the\nlog-Gaussian process to that of policies for the widely-used Gaussian process\nin mapping the hotspot field. Lastly, we provide sufficient conditions that,\nwhen met, guarantee adaptivity has no benefit under an assumed environment\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 07:28:05 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Low", "Kian Hsiang", ""], ["Dolan", "John M.", ""], ["Khosla", "Pradeep", ""]]}, {"id": "1305.6143", "submitter": "Vivek Narayanan", "authors": "Vivek Narayanan, Ishan Arora, Arjun Bhatia", "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model", "comments": "8 pages, 2 figures", "journal-ref": "Intelligent Data Engineering and Automated Learning IDEAL 2013\n  Lecture Notes in Computer Science Volume 8206, 2013, pp 194-201", "doi": "10.1007/978-3-642-41278-3_24", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 08:37:26 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 05:36:29 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Narayanan", "Vivek", ""], ["Arora", "Ishan", ""], ["Bhatia", "Arjun", ""]]}, {"id": "1305.6239", "submitter": "Frederic Chazal", "authors": "Fr\\'ed\\'eric Chazal and Marc Glisse and Catherine Labru\\`ere and\n  Bertrand Michel", "title": "Optimal rates of convergence for persistence diagrams in Topological\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG cs.LG math.GT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational topology has recently known an important development toward\ndata analysis, giving birth to the field of topological data analysis.\nTopological persistence, or persistent homology, appears as a fundamental tool\nin this field. In this paper, we study topological persistence in general\nmetric spaces, with a statistical approach. We show that the use of persistent\nhomology can be naturally considered in general statistical frameworks and\npersistence diagrams can be used as statistics with interesting convergence\nproperties. Some numerical experiments are performed in various contexts to\nillustrate our results.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 14:37:29 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Glisse", "Marc", ""], ["Labru\u00e8re", "Catherine", ""], ["Michel", "Bertrand", ""]]}, {"id": "1305.6568", "submitter": "Arthur Carvalho", "authors": "Arthur Carvalho and Renato Oliveira", "title": "Reinforcement Learning for the Soccer Dribbling Task", "comments": null, "journal-ref": null, "doi": "10.1109/CIG.2011.6031994", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning solution to the \\emph{soccer dribbling\ntask}, a scenario in which a soccer agent has to go from the beginning to the\nend of a region keeping possession of the ball, as an adversary attempts to\ngain possession. While the adversary uses a stationary policy, the dribbler\nlearns the best action to take at each decision point. After defining\nmeaningful variables to represent the state space, and high-level macro-actions\nto incorporate domain knowledge, we describe our application of the\nreinforcement learning algorithm \\emph{Sarsa} with CMAC for function\napproximation. Our experiments show that, after the training period, the\ndribbler is able to accomplish its task against a strong adversary around 58%\nof the time.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 17:47:08 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Carvalho", "Arthur", ""], ["Oliveira", "Renato", ""]]}, {"id": "1305.6646", "submitter": "Paul Mineiro", "authors": "Stephane Ross and Paul Mineiro and John Langford", "title": "Normalized Online Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online learning algorithms which are independent of feature\nscales, proving regret bounds dependent on the ratio of scales existent in the\ndata rather than the absolute scale. This has several useful effects: there is\nno need to pre-normalize data, the test-time and test-space complexity are\nreduced, and the algorithms are more robust.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 22:12:59 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Ross", "Stephane", ""], ["Mineiro", "Paul", ""], ["Langford", "John", ""]]}, {"id": "1305.6659", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence\n  Carin", "title": "Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process\n  Mixture", "comments": "This paper is from NIPS 2013. Please use the following BibTeX\n  citation: @inproceedings{Campbell13_NIPS, Author = {Trevor Campbell and Miao\n  Liu and Brian Kulis and Jonathan P. How and Lawrence Carin}, Title = {Dynamic\n  Clustering via Asymptotics of the Dependent Dirichlet Process}, Booktitle =\n  {Advances in Neural Information Processing Systems (NIPS)}, Year = {2013}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm, based upon the dependent Dirichlet\nprocess mixture model (DDPMM), for clustering batch-sequential data containing\nan unknown number of evolving clusters. The algorithm is derived via a\nlow-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM,\nand provides a hard clustering with convergence guarantees similar to those of\nthe k-means algorithm. Empirical results from a synthetic test with moving\nGaussian clusters and a test with real ADS-B aircraft trajectory data\ndemonstrate that the algorithm requires orders of magnitude less computational\ntime than contemporary probabilistic and hard clustering algorithms, while\nproviding higher accuracy on the examined datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 23:59:16 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 18:25:39 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Campbell", "Trevor", ""], ["Liu", "Miao", ""], ["Kulis", "Brian", ""], ["How", "Jonathan P.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1305.6663", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent", "title": "Generalized Denoising Auto-Encoders as Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown how denoising and contractive autoencoders implicitly\ncapture the structure of the data-generating density, in the case where the\ncorruption noise is Gaussian, the reconstruction error is the squared error,\nand the data is continuous-valued. This has led to various proposals for\nsampling from this implicitly learned density function, using Langevin and\nMetropolis-Hastings MCMC. However, it remained unclear how to connect the\ntraining procedure of regularized auto-encoders to the implicit estimation of\nthe underlying data-generating distribution when the data are discrete, or\nusing other forms of corruption process and reconstruction errors. Another\nissue is the mathematical justification which is only valid in the limit of\nsmall corruption noise. We propose here a different attack on the problem,\nwhich deals with all these issues: arbitrary (but noisy enough) corruption,\narbitrary reconstruction loss (seen as a log-likelihood), handling both\ndiscrete and continuous-valued variables, and removing the bias due to\nnon-infinitesimal corruption noise (or non-infinitesimal contractive penalty).\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 00:25:54 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2013 00:03:48 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2013 16:46:15 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 02:27:55 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Bengio", "Yoshua", ""], ["Yao", "Li", ""], ["Alain", "Guillaume", ""], ["Vincent", "Pascal", ""]]}, {"id": "1305.7057", "submitter": "Sahar Mokhtar", "authors": "Sahar A. Mokhtar and Alaa. M. Elsayad", "title": "Predicting the Severity of Breast Masses with Data Mining Methods", "comments": null, "journal-ref": "International Journal of Computer Science Issues (IJCSI) March\n  2013 Issue (Volume 10, Issue 2)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is the most effective and available tool for breast cancer\nscreening. However, the low positive predictive value of breast biopsy\nresulting from mammogram interpretation leads to approximately 70% unnecessary\nbiopsies with benign outcomes. Data mining algorithms could be used to help\nphysicians in their decisions to perform a breast biopsy on a suspicious lesion\nseen in a mammogram image or to perform a short term follow-up examination\ninstead. In this research paper data mining classification algorithms; Decision\nTree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM)\nare analyzed on mammographic masses data set. The purpose of this study is to\nincrease the ability of physicians to determine the severity (benign or\nmalignant) of a mammographic mass lesion from BI-RADS attributes and the\npatient,s age. The whole data set is divided for training the models and test\nthem by the ratio of 70:30% respectively and the performances of classification\nalgorithms are compared through three statistical measures; sensitivity,\nspecificity, and classification accuracy. Accuracy of DT, ANN and SVM are\n78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows that\nout of these three classification models SVM predicts severity of breast cancer\nwith least error rate and highest accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 10:44:41 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Mokhtar", "Sahar A.", ""], ["Elsayad", "Alaa. M.", ""]]}, {"id": "1305.7111", "submitter": "Jose Hernandez-Orallo", "authors": "Celestine Periale Maguedong-Djoumessi, Jos\\'e Hern\\'andez-Orallo", "title": "Test cost and misclassification cost trade-off using reframing", "comments": "Keywords: test cost, misclassification cost, missing values,\n  reframing, ROC analysis, operating context, feature configuration, feature\n  selection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many solutions to cost-sensitive classification (and regression) rely on some\nor all of the following assumptions: we have complete knowledge about the cost\ncontext at training time, we can easily re-train whenever the cost context\nchanges, and we have technique-specific methods (such as cost-sensitive\ndecision trees) that can take advantage of that information. In this paper we\naddress the problem of selecting models and minimising joint cost (integrating\nboth misclassification cost and test costs) without any of the above\nassumptions. We introduce methods and plots (such as the so-called JROC plots)\nthat can work with any off-the-shelf predictive technique, including ensembles,\nsuch that we reframe the model to use the appropriate subset of attributes (the\nfeature configuration) during deployment time. In other words, models are\ntrained with the available attributes (once and for all) and then deployed by\nsetting missing values on the attributes that are deemed ineffective for\nreducing the joint cost. As the number of feature configuration combinations\ngrows exponentially with the number of features we introduce quadratic methods\nthat are able to approximate the optimal configuration and model choices, as\nshown by the experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 13:52:32 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Maguedong-Djoumessi", "Celestine Periale", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""]]}, {"id": "1305.7331", "submitter": "Naresh Kumar Mallenahalli Prof. Dr.", "authors": "M. Naresh Kumar", "title": "Alternating Decision trees for early diagnosis of dengue fever", "comments": "13 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dengue fever is a flu-like illness spread by the bite of an infected mosquito\nwhich is fast emerging as a major health problem. Timely and cost effective\ndiagnosis using clinical and laboratory features would reduce the mortality\nrates besides providing better grounds for clinical management and disease\nsurveillance. We wish to develop a robust and effective decision tree based\napproach for predicting dengue disease. Our analysis is based on the clinical\ncharacteristics and laboratory measurements of the diseased individuals. We\nhave developed and trained an alternating decision tree with boosting and\ncompared its performance with C4.5 algorithm for dengue disease diagnosis. Of\nthe 65 patient records a diagnosis establishes that 53 individuals have been\nconfirmed to have dengue fever. An alternating decision tree based algorithm\nwas able to differentiate the dengue fever using the clinical and laboratory\ndata with number of correctly classified instances as 89%, F-measure of 0.86\nand receiver operator characteristics (ROC) of 0.826 as compared to C4.5 having\ncorrectly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617\nrespectively. Alternating decision tree based approach with boosting has been\nable to predict dengue fever with a higher degree of accuracy than C4.5 based\ndecision tree using simple clinical and laboratory features. Further analysis\non larger data sets is required to improve the sensitivity and specificity of\nthe alternating decision trees.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 09:15:47 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 04:56:15 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Kumar", "M. Naresh", ""]]}, {"id": "1305.7454", "submitter": "Uwe Aickelin", "authors": "Jan Feyereisl, Uwe Aickelin", "title": "Privileged Information for Data Clustering", "comments": "Information Sciences 194, 4-23, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms assume that all input samples are\nindependently and identically distributed from some common distribution on\neither the input space X, in the case of unsupervised learning, or the input\nand output space X x Y in the case of supervised and semi-supervised learning.\nIn the last number of years the relaxation of this assumption has been explored\nand the importance of incorporation of additional information within machine\nlearning algorithms became more apparent. Traditionally such fusion of\ninformation was the domain of semi-supervised learning. More recently the\ninclusion of knowledge from separate hypothetical spaces has been proposed by\nVapnik as part of the supervised setting. In this work we are interested in\nexploring Vapnik's idea of master-class learning and the associated learning\nusing privileged information, however within the unsupervised setting. Adoption\nof the advanced supervised learning paradigm for the unsupervised setting\ninstigates investigation into the difference between privileged and technical\ndata. By means of our proposed aRi-MAX method stability of the KMeans algorithm\nis improved and identification of the best clustering solution is achieved on\nan artificial dataset. Subsequently an information theoretic dot product based\nalgorithm called P-Dot is proposed. This method has the ability to utilize a\nwide variety of clustering techniques, individually or in combination, while\nfusing privileged and technical data for improved clustering. Application of\nthe P-Dot method to the task of digit recognition confirms our findings in a\nreal-world scenario.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:28:44 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7477", "submitter": "Jason Lee", "authors": "Jason D. Lee, Yuekai Sun, Jonathan E. Taylor", "title": "On model selection consistency of regularized M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized M-estimators are used in diverse areas of science and engineering\nto fit high-dimensional models with some low-dimensional structure. Usually the\nlow-dimensional structure is encoded by the presence of the (unknown)\nparameters in some low-dimensional model subspace. In such settings, it is\ndesirable for estimates of the model parameters to be \\emph{model selection\nconsistent}: the estimates also fall in the model subspace. We develop a\ngeneral framework for establishing consistency and model selection consistency\nof regularized M-estimators and show how it applies to some special cases of\ninterest in statistical learning. Our analysis identifies two key properties of\nregularized M-estimators, referred to as geometric decomposability and\nirrepresentability, that ensure the estimators are consistent and model\nselection consistent.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:24:17 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 16:59:21 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2013 23:47:12 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2013 19:44:45 GMT"}, {"version": "v5", "created": "Wed, 23 Oct 2013 19:34:33 GMT"}, {"version": "v6", "created": "Tue, 10 Dec 2013 17:52:09 GMT"}, {"version": "v7", "created": "Wed, 12 Feb 2014 08:52:10 GMT"}, {"version": "v8", "created": "Sat, 11 Oct 2014 05:54:58 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}]