[{"id": "1507.00039", "submitter": "Jason Lee", "authors": "Jason D. Lee", "title": "Selective Inference and Learning Mixed Graphical Models", "comments": "Jason D. Lee PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis studies two problems in modern statistics. First, we study\nselective inference, or inference for hypothesis that are chosen after looking\nat the data. The motiving application is inference for regression coefficients\nselected by the lasso. We present the Condition-on-Selection method that allows\nfor valid selective inference, and study its application to the lasso, and\nseveral other selection algorithms.\n  In the second part, we consider the problem of learning the structure of a\npairwise graphical model over continuous and discrete variables. We present a\nnew pairwise model for graphical models with both continuous and discrete\nvariables that is amenable to structure learning. In previous work, authors\nhave considered structure learning of Gaussian graphical models and structure\nlearning of discrete models. Our approach is a natural generalization of these\ntwo lines of work to the mixed case. The penalization scheme involves a novel\nsymmetric use of the group-lasso norm and follows naturally from a particular\nparametrization of the model. We provide conditions under which our estimator\nis model selection consistent in the high-dimensional regime.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 21:10:18 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Lee", "Jason D.", ""]]}, {"id": "1507.00066", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Fast Cross-Validation for Incremental Learning", "comments": "Appearing in the International Joint Conference on Artificial\n  Intelligence (IJCAI-2015), Buenos Aires, Argentina, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is one of the main tools for performance estimation and\nparameter tuning in machine learning. The general recipe for computing CV\nestimate is to run a learning algorithm separately for each CV fold, a\ncomputationally expensive process. In this paper, we propose a new approach to\nreduce the computational burden of CV-based performance estimation. As opposed\nto all previous attempts, which are specific to a particular learning model or\nproblem domain, we propose a general method applicable to a large class of\nincremental learning algorithms, which are uniquely fitted to big data\nproblems. In particular, our method applies to a wide range of supervised and\nunsupervised learning tasks with different performance criteria, as long as the\nbase learning algorithm is incremental. We show that the running time of the\nalgorithm scales logarithmically, rather than linearly, in the number of CV\nfolds. Furthermore, the algorithm has favorable properties for parallel and\ndistributed implementation. Experiments with state-of-the-art incremental\nlearning algorithms confirm the practicality of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 23:30:28 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1507.00093", "submitter": "Prasad H L", "authors": "H. L. Prasad and Shalabh Bhatnagar", "title": "A Study of Gradient Descent Schemes for General-Sum Stochastic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-sum stochastic games are easy to solve as they can be cast as simple\nMarkov decision processes. This is however not the case with general-sum\nstochastic games. A fairly general optimization problem formulation is\navailable for general-sum stochastic games by Filar and Vrieze [2004]. However,\nthe optimization problem there has a non-linear objective and non-linear\nconstraints with special structure. Since gradients of both the objective as\nwell as constraints of this optimization problem are well defined, gradient\nbased schemes seem to be a natural choice. We discuss a gradient scheme tuned\nfor two-player stochastic games. We show in simulations that this scheme indeed\nconverges to a Nash equilibrium, for a simple terrain exploration problem\nmodelled as a general-sum stochastic game. However, it turns out that only\nglobal minima of the optimization problem correspond to Nash equilibria of the\nunderlying general-sum stochastic game, while gradient schemes only guarantee\nconvergence to local minima. We then provide important necessary conditions for\ngradient schemes to converge to Nash equilibria in general-sum stochastic\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 02:37:33 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Prasad", "H. L.", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1507.00210", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray\n  Kavukcuoglu", "title": "Natural Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:42:01 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Simonyan", "Karen", ""], ["Pascanu", "Razvan", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1507.00220", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger, Ronald R. Coifman, Nicholas Downing, Harlan M.\n  Krumholz", "title": "Bigeometric Organization of Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build an organization of high-dimensional datasets that\ncannot be cleanly embedded into a low-dimensional representation due to missing\nentries and a subset of the features being irrelevant to modeling functions of\ninterest. Our algorithm begins by defining coarse neighborhoods of the points\nand defining an expected empirical function value on these neighborhoods. We\nthen generate new non-linear features with deep net representations tuned to\nmodel the approximate function, and re-organize the geometry of the points with\nrespect to the new representation. Finally, the points are locally z-scored to\ncreate an intrinsic geometric organization which is independent of the\nparameters of the deep net, a geometry designed to assure smoothness with\nrespect to the empirical function. We examine this approach on data from the\nCenter for Medicare and Medicaid Services Hospital Quality Initiative, and\ngenerate an intrinsic low-dimensional organization of the hospitals that is\nsmooth with respect to an expert driven function of quality.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 13:18:53 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""], ["Downing", "Nicholas", ""], ["Krumholz", "Harlan M.", ""]]}, {"id": "1507.00300", "submitter": "Ian Osband", "authors": "Ian Osband and Benjamin Van Roy", "title": "Bootstrapped Thompson Sampling and Deep Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note presents a new approach to carrying out the kind of\nexploration achieved by Thompson sampling, but without explicitly maintaining\nor sampling from posterior distributions. The approach is based on a bootstrap\ntechnique that uses a combination of observed and artificially generated data.\nThe latter serves to induce a prior distribution which, as we will demonstrate,\nis critical to effective exploration. We explain how the approach can be\napplied to multi-armed bandit and reinforcement learning problems and how it\nrelates to Thompson sampling. The approach is particularly well-suited for\ncontexts in which exploration is coupled with deep learning, since in these\nsettings, maintaining or generating samples from a posterior distribution\nbecomes computationally infeasible.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 17:47:01 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1507.00333", "submitter": "Jie Yang", "authors": "Yuan Lu and Jie Yang", "title": "Notes on Low-rank Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix factorization (MF) is an important technique in data science.\nThe key idea of MF is that there exists latent structures in the data, by\nuncovering which we could obtain a compressed representation of the data. By\nfactorizing an original matrix to low-rank matrices, MF provides a unified\nmethod for dimension reduction, clustering, and matrix completion. In this\narticle we review several important variants of MF, including: Basic MF,\nNon-negative MF, Orthogonal non-negative MF. As can be told from their names,\nnon-negative MF and orthogonal non-negative MF are variants of basic MF with\nnon-negativity and/or orthogonality constraints. Such constraints are useful in\nspecific senarios. In the first part of this article, we introduce, for each of\nthese models, the application scenarios, the distinctive properties, and the\noptimizing method. By properly adapting MF, we can go beyond the problem of\nclustering and matrix completion. In the second part of this article, we will\nextend MF to sparse matrix compeletion, enhance matrix compeletion using\nvarious regularization methods, and make use of MF for (semi-)supervised\nlearning by introducing latent space reinforcement and transformation. We will\nsee that MF is not only a useful model but also as a flexible framework that is\napplicable for various prediction problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:47:34 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 20:44:46 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 10:35:36 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Lu", "Yuan", ""], ["Yang", "Jie", ""]]}, {"id": "1507.00353", "submitter": "Harm van Seijen", "authors": "Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S.\n  Sutton", "title": "An Empirical Evaluation of True Online TD({\\lambda})", "comments": "European Workshop on Reinforcement Learning (EWRL) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The true online TD({\\lambda}) algorithm has recently been proposed (van\nSeijen and Sutton, 2014) as a universal replacement for the popular\nTD({\\lambda}) algorithm, in temporal-difference learning and reinforcement\nlearning. True online TD({\\lambda}) has better theoretical properties than\nconventional TD({\\lambda}), and the expectation is that it also results in\nfaster learning. In this paper, we put this hypothesis to the test.\nSpecifically, we compare the performance of true online TD({\\lambda}) with that\nof TD({\\lambda}) on challenging examples, random Markov reward processes, and a\nreal-world myoelectric prosthetic arm. We use linear function approximation\nwith tabular, binary, and non-binary features. We assess the algorithms along\nthree dimensions: computational cost, learning speed, and ease of use. Our\nresults confirm the strength of true online TD({\\lambda}): 1) for sparse\nfeature vectors, the computational overhead with respect to TD({\\lambda}) is\nminimal; for non-sparse features the computation time is at most twice that of\nTD({\\lambda}), 2) across all domains/representations the learning speed of true\nonline TD({\\lambda}) is often better, but never worse than that of\nTD({\\lambda}), and 3) true online TD({\\lambda}) is easier to use, because it\ndoes not require choosing between trace types, and it is generally more stable\nwith respect to the step-size. Overall, our results suggest that true online\nTD({\\lambda}) should be the first choice when looking for an efficient,\ngeneral-purpose TD method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 20:03:49 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["van Seijen", "Harm", ""], ["Mahmood", "A. Rupam", ""], ["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1507.00407", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire", "title": "Fast Convergence of Regularized Learning in Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that natural classes of regularized learning algorithms with a form\nof recency bias achieve faster convergence rates to approximate efficiency and\nto coarse correlated equilibria in multiplayer normal form games. When each\nplayer in a game uses an algorithm from our class, their individual regret\ndecays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate\noptimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.\nWe show a black-box reduction for any algorithm in the class to achieve\n$\\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster\nrates against algorithms in the class. Our results extend those of [Rakhlin and\nShridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player\nzero-sum games for specific algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 01:55:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 21:58:16 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 14:51:56 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2015 17:22:43 GMT"}, {"version": "v5", "created": "Thu, 10 Dec 2015 21:52:29 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Agarwal", "Alekh", ""], ["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1507.00418", "submitter": "Vasilis Syrgkanis", "authors": "Jason Hartline, Vasilis Syrgkanis, Eva Tardos", "title": "No-Regret Learning in Bayesian Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent price-of-anarchy analyses of games of complete information suggest\nthat coarse correlated equilibria, which characterize outcomes resulting from\nno-regret learning dynamics, have near-optimal welfare. This work provides two\nmain technical results that lift this conclusion to games of incomplete\ninformation, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian\ngames follows directly from the smoothness-based proof of near-optimal welfare\nin the same game when the private information is public. Second, no-regret\nlearning dynamics converge to Bayesian coarse correlated equilibrium in these\nincomplete information games. These results are enabled by interpretation of a\nBayesian game as a stochastic game of complete information.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:27:32 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:45:14 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Hartline", "Jason", ""], ["Syrgkanis", "Vasilis", ""], ["Tardos", "Eva", ""]]}, {"id": "1507.00421", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie", "title": "Categorical Matrix Completion", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of completing a matrix with categorical-valued\nentries from partial observations. This is achieved by extending the\nformulation and theory of one-bit matrix completion. We recover a low-rank\nmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear\nnorm of $X$, and the observations are mapped from entries of $X$ through\nmultiple link functions. We establish theoretical upper and lower bounds on the\nrecovery error, which meet up to a constant factor $\\mathcal{O}(K^{3/2})$ where\n$K$ is the fixed number of categories. The upper bound in our case depends on\nthe number of categories implicitly through a maximization of terms that\ninvolve the smoothness of the link functions. In contrast to one-bit matrix\ncompletion, our bounds for categorical matrix completion are optimal up to a\nfactor on the order of the square root of the number of categories, which is\nconsistent with an intuition that the problem becomes harder when the number of\ncategories increases. By comparing the performance of our method with the\nconventional matrix completion method on the MovieLens dataset, we demonstrate\nthe advantage of our method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:58:47 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1507.00436", "submitter": "Yusen Zhan", "authors": "Yusen Zhan and Matthew E. Taylor", "title": "Online Transfer Learning in Reinforcement Learning Domains", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes an online transfer framework to capture the interaction\namong agents and shows that current transfer learning in reinforcement learning\nis a special case of online transfer. Furthermore, this paper re-characterizes\nexisting agents-teaching-agents methods as online transfer and analyze one such\nteaching method in three ways. First, the convergence of Q-learning and Sarsa\nwith tabular representation with a finite budget is proven. Second, the\nconvergence of Q-learning and Sarsa with linear function approximation is\nestablished. Third, the we show the asymptotic performance cannot be hurt\nthrough teaching. Additionally, all theoretical results are empirically\nvalidated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 06:05:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 00:12:10 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Zhan", "Yusen", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "1507.00438", "submitter": "Alain Rakotomamonjy", "authors": "Alain Rakotomamonjy (LITIS), Remi Flamary (LAGRANGE, OCA), Gilles\n  Gasso (LITIS)", "title": "DC Proximal Newton for Non-Convex Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm for solving learning problems where both the\nloss function and the regularizer are non-convex but belong to the class of\ndifference of convex (DC) functions. Our contribution is a new general purpose\nproximal Newton algorithm that is able to deal with such a situation. The\nalgorithm consists in obtaining a descent direction from an approximation of\nthe loss function and then in performing a line search to ensure sufficient\ndescent. A theoretical analysis is provided showing that the iterates of the\nproposed algorithm {admit} as limit points stationary points of the DC\nobjective function. Numerical experiments show that our approach is more\nefficient than current state of the art for a problem with a convex loss\nfunctions and non-convex regularizer. We have also illustrated the benefit of\nour algorithm in high-dimensional transductive learning problem where both loss\nfunction and regularizers are non-convex.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 06:41:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Rakotomamonjy", "Alain", "", "LITIS"], ["Flamary", "Remi", "", "LAGRANGE, OCA"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1507.00473", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "The Optimal Sample Complexity of PAC Learning", "comments": null, "journal-ref": "Journal of Machine Learning Research, Vol. 17 (2016), No. 38, pp.\n  1-15", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes a new upper bound on the number of samples sufficient\nfor PAC learning in the realizable case. The bound matches known lower bounds\nup to numerical constant factors. This solves a long-standing open problem on\nthe sample complexity of PAC learning. The technique and analysis build on a\nrecent breakthrough by Hans Simon.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 08:43:21 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 12:23:27 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 04:08:28 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2016 16:35:39 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1507.00500", "submitter": "Remi Flamary", "authors": "L\\'ea Laporte (IRIT), R\\'emi Flamary (OCA, LAGRANGE), Stephane Canu\n  (LITIS), S\\'ebastien D\\'ejean (IMT), Josiane Mothe (IRIT)", "title": "Non-convex Regularizations for Feature Selection in Ranking With Sparse\n  SVM", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, IEEE,\n  2013, pp.1,1", "doi": "10.1109/TNNLS.2013.2286696", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection in learning to rank has recently emerged as a crucial\nissue. Whereas several preprocessing approaches have been proposed, only a few\nworks have been focused on integrating the feature selection into the learning\nprocess. In this work, we propose a general framework for feature selection in\nlearning to rank using SVM with a sparse regularization term. We investigate\nboth classical convex regularizations such as $\\ell\\_1$ or weighted $\\ell\\_1$\nand non-convex regularization terms such as log penalty, Minimax Concave\nPenalty (MCP) or $\\ell\\_p$ pseudo norm with $p\\textless{}1$. Two algorithms are\nproposed, first an accelerated proximal approach for solving the convex\nproblems, second a reweighted $\\ell\\_1$ scheme to address the non-convex\nregularizations. We conduct intensive experiments on nine datasets from Letor\n3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convex\nregularizations we propose leads to more sparsity in the resulting models while\nprediction performance is preserved. The number of features is decreased by up\nto a factor of six compared to the $\\ell\\_1$ regularization. In addition, the\nsoftware is publicly available on the web.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:06:02 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Laporte", "L\u00e9a", "", "IRIT"], ["Flamary", "R\u00e9mi", "", "OCA, LAGRANGE"], ["Canu", "Stephane", "", "LITIS"], ["D\u00e9jean", "S\u00e9bastien", "", "IMT"], ["Mothe", "Josiane", "", "IRIT"]]}, {"id": "1507.00504", "submitter": "Remi Flamary", "authors": "Nicolas Courty (OBELIX), R\\'emi Flamary (LAGRANGE, OCA), Devis Tuia\n  (LASIG), Alain Rakotomamonjy (LITIS)", "title": "Optimal Transport for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation from one data space (or domain) to another is one of the\nmost challenging tasks of modern data analytics. If the adaptation is done\ncorrectly, models built on a specific data space become more robust when\nconfronted to data depicting the same semantic concepts (the classes), but\nobserved by another observation system with its own specificities. Among the\nmany strategies proposed to adapt a domain to another, finding a common\nrepresentation has shown excellent properties: by finding a common\nrepresentation for both domains, a single classifier can be effective in both\nand use labelled samples from the source domain to predict the unlabelled\nsamples of the target domain. In this paper, we propose a regularized\nunsupervised optimal transportation model to perform the alignment of the\nrepresentations in the source and target domains. We learn a transportation\nplan matching both PDFs, which constrains labelled samples in the source domain\nto remain close during transport. This way, we exploit at the same time the few\nlabeled information in the source and the unlabelled distributions observed in\nboth domains. Experiments in toy and challenging real visual adaptation\nexamples show the interest of the method, that consistently outperforms state\nof the art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:15:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 13:24:01 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Courty", "Nicolas", "", "OBELIX"], ["Flamary", "R\u00e9mi", "", "LAGRANGE, OCA"], ["Tuia", "Devis", "", "LASIG"], ["Rakotomamonjy", "Alain", "", "LITIS"]]}, {"id": "1507.00564", "submitter": "Alessandro Chiuso", "authors": "Gianluigi Pillonetto, Tianshi Chen, Alessandro Chiuso, Giuseppe De\n  Nicolao, Lennart Ljung", "title": "Regularized linear system identification using atomic, nuclear and\n  kernel-based norms: the role of the stability constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by ideas taken from the machine learning literature, new\nregularization techniques have been recently introduced in linear system\nidentification. In particular, all the adopted estimators solve a regularized\nleast squares problem, differing in the nature of the penalty term assigned to\nthe impulse response. Popular choices include atomic and nuclear norms (applied\nto Hankel matrices) as well as norms induced by the so called stable spline\nkernels. In this paper, a comparative study of estimators based on these\ndifferent types of regularizers is reported. Our findings reveal that stable\nspline kernels outperform approaches based on atomic and nuclear norms since\nthey suitably embed information on impulse response stability and smoothness.\nThis point is illustrated using the Bayesian interpretation of regularization.\nWe also design a new class of regularizers defined by \"integral\" versions of\nstable spline/TC kernels. Under quite realistic experimental conditions, the\nnew estimators outperform classical prediction error methods also when the\nlatter are equipped with an oracle for model order selection.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 13:07:17 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Pillonetto", "Gianluigi", ""], ["Chen", "Tianshi", ""], ["Chiuso", "Alessandro", ""], ["De Nicolao", "Giuseppe", ""], ["Ljung", "Lennart", ""]]}, {"id": "1507.00567", "submitter": "Pooyan Jamshidi", "authors": "Pooyan Jamshidi, Amir Sharifloo, Claus Pahl, Andreas Metzger, Giovani\n  Estrada", "title": "Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge\n  Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.DC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud controllers aim at responding to application demands by automatically\nscaling the compute resources at runtime to meet performance guarantees and\nminimize resource costs. Existing cloud controllers often resort to scaling\nstrategies that are codified as a set of adaptation rules. However, for a cloud\nprovider, applications running on top of the cloud infrastructure are more or\nless black-boxes, making it difficult at design time to define optimal or\npre-emptive adaptation rules. Thus, the burden of taking adaptation decisions\noften is delegated to the cloud application. Yet, in most cases, application\ndevelopers in turn have limited knowledge of the cloud infrastructure. In this\npaper, we propose learning adaptation rules during runtime. To this end, we\nintroduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE\nlearns and modifies fuzzy rules at runtime. The benefit is that for designing\ncloud controllers, we do not have to rely solely on precise design-time\nknowledge, which may be difficult to acquire. FQL4KE empowers users to specify\ncloud controllers by simply adjusting weights representing priorities in system\ngoals instead of specifying complex adaptation rules. The applicability of\nFQL4KE has been experimentally assessed as part of the cloud application\nframework ElasticBench. The experimental results indicate that FQL4KE\noutperforms our previously developed fuzzy controller without learning\nmechanisms and the native Azure auto-scaling.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 13:11:22 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Jamshidi", "Pooyan", ""], ["Sharifloo", "Amir", ""], ["Pahl", "Claus", ""], ["Metzger", "Andreas", ""], ["Estrada", "Giovani", ""]]}, {"id": "1507.00646", "submitter": "Zhensong Qian", "authors": "Oliver Schulte and Zhensong Qian", "title": "SQL for SRL: Structure Learning Inside a Database System", "comments": "3 pages, 1 figure, Position Paper of the Fifth International Workshop\n  on Statistical Relational AI at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The position we advocate in this paper is that relational algebra can provide\na unified language for both representing and computing with\nstatistical-relational objects, much as linear algebra does for traditional\nsingle-table machine learning. Relational algebra is implemented in the\nStructured Query Language (SQL), which is the basis of relational database\nmanagement systems. To support our position, we have developed the FACTORBASE\nsystem, which uses SQL as a high-level scripting language for\nstatistical-relational learning of a graphical model structure. The design\nphilosophy of FACTORBASE is to manage statistical models as first-class\ncitizens inside a database. Our implementation shows how our SQL constructs in\nFACTORBASE facilitate fast, modular, and reliable program development.\nEmpirical evidence from six benchmark databases indicates that leveraging\ndatabase system capabilities achieves scalable model structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 16:07:48 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Schulte", "Oliver", ""], ["Qian", "Zhensong", ""]]}, {"id": "1507.00677", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii", "title": "Distributional Smoothing with Virtual Adversarial Training", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local distributional smoothness (LDS), a new notion of smoothness\nfor statistical model that can be used as a regularization term to promote the\nsmoothness of the model distribution. We named the LDS based regularization as\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\ndefined as the KL-divergence based robustness of the model distribution against\nlocal perturbation around the datapoint. VAT resembles adversarial training,\nbut distinguishes itself in that it determines the adversarial direction from\nthe model distribution alone without using the label information, making it\napplicable to semi-supervised learning. The computational cost for VAT is\nrelatively low. For neural network, the approximated gradient of the LDS can be\ncomputed with no more than three pairs of forward and back propagations. When\nwe applied our technique to supervised and semi-supervised learning for the\nMNIST dataset, it outperformed all the training methods other than the current\nstate of the art method, which is based on a highly advanced generative model.\nWe also applied our method to SVHN and NORB, and confirmed our method's\nsuperior performance over the current state of the art semi-supervised method\napplied to these datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 18:01:23 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 19:59:36 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 09:19:40 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2015 12:20:05 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2015 18:47:51 GMT"}, {"version": "v6", "created": "Wed, 25 Nov 2015 13:31:07 GMT"}, {"version": "v7", "created": "Sat, 9 Jan 2016 23:53:05 GMT"}, {"version": "v8", "created": "Mon, 29 Feb 2016 15:39:55 GMT"}, {"version": "v9", "created": "Sat, 11 Jun 2016 18:22:33 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Miyato", "Takeru", ""], ["Maeda", "Shin-ichi", ""], ["Koyama", "Masanori", ""], ["Nakae", "Ken", ""], ["Ishii", "Shin", ""]]}, {"id": "1507.00710", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng and Anup Rao and Sushant Sachdeva", "title": "Fast, Provable Algorithms for Isotonic Regression in all\n  $\\ell_{p}$-norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,\nthe Isotonic Regression of $y$ is a vector $x$ that respects the partial order\ndescribed by $G,$ and minimizes $||x-y||,$ for a specified norm. This paper\ngives improved algorithms for computing the Isotonic Regression for all\nweighted $\\ell_{p}$-norms with rigorous performance guarantees. Our algorithms\nare quite practical, and their variants can be implemented to run fast in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:42:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:14:21 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Kyng", "Rasmus", ""], ["Rao", "Anup", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1507.00814", "submitter": "Bradly Stadie", "authors": "Bradly C. Stadie, Sergey Levine, Pieter Abbeel", "title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving efficient and scalable exploration in complex domains poses a major\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\nthe exploration problem offer strong formal guarantees, they are often\nimpractical in higher dimensions due to their reliance on enumerating the\nstate-action space. Hence, exploration in complex domains is often performed\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\nAtari games domain, which requires processing raw pixel inputs and delayed\nrewards. We evaluate several more sophisticated exploration strategies,\nincluding Thompson sampling and Boltzman exploration, and propose a new\nexploration method based on assigning exploration bonuses from a concurrently\nlearned model of the system dynamics. By parameterizing our learned model with\na neural network, we are able to develop a scalable and efficient approach to\nexploration bonuses that can be applied to tasks with complex, high-dimensional\nstate spaces. In the Atari domain, our method provides the most consistent\nimprovement across a range of games that pose a major challenge for prior\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\nthe Atari Learning domain to evaluate the impact of exploration on this\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 04:11:15 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 23:05:34 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 22:40:30 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Stadie", "Bradly C.", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1507.00824", "submitter": "Behnam Babagholami-Mohamadabadi", "authors": "Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic", "title": "D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models provide a framework for probabilistic modelling of complex\ndatasets. However, many of such models are computationally demanding especially\nin the presence of large datasets. On the other hand, in sensor network\napplications, statistical (Bayesian) parameter estimation usually needs\ndistributed algorithms, in which both data and computation are distributed\nacross the nodes of the network. In this paper we propose a general framework\nfor distributed Bayesian learning using Bregman Alternating Direction Method of\nMultipliers (B-ADMM). We demonstrate the utility of our framework, with Mean\nField Variational Bayes (MFVB) as the primitive for distributed Matrix\nFactorization (MF) and distributed affine structure from motion (SfM).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:14:26 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Babagholami-Mohamadabadi", "Behnam", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1507.00825", "submitter": "Yutaro Shigeto", "authors": "Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji\n  Matsumoto", "title": "Ridge Regression, Hubness, and Zero-Shot Learning", "comments": "To be presented at ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the effect of hubness in zero-shot learning, when ridge\nregression is used to find a mapping between the example space to the label\nspace. Contrary to the existing approach, which attempts to find a mapping from\nthe example space to the label space, we show that mapping labels into the\nexample space is desirable to suppress the emergence of hubs in the subsequent\nnearest neighbor search step. Assuming a simple data model, we prove that the\nproposed approach indeed reduces hubness. This was verified empirically on the\ntasks of bilingual lexicon extraction and image labeling: hubness was reduced\nwith both of these tasks and the accuracy was improved accordingly.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:18:36 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Shigeto", "Yutaro", ""], ["Suzuki", "Ikumi", ""], ["Hara", "Kazuo", ""], ["Shimbo", "Masashi", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1507.00908", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Jie Cheng and Qiang Chen", "title": "LogDet Rank Minimization with Application to Subspace Clustering", "comments": "10 pages, 4 figures", "journal-ref": "Computational Intelligence and Neuroscience, Volume 2015, Article\n  ID 824289", "doi": "10.1155/2015/824289", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix is desired in many machine learning and computer vision\nproblems. Most of the recent studies use the nuclear norm as a convex surrogate\nof the rank operator. However, all singular values are simply added together by\nthe nuclear norm, and thus the rank may not be well approximated in practical\nproblems. In this paper, we propose to use a log-determinant (LogDet) function\nas a smooth and closer, though non-convex, approximation to rank for obtaining\na low-rank representation in subspace clustering. Augmented Lagrange\nmultipliers strategy is applied to iteratively optimize the LogDet-based\nnon-convex objective function on potentially large-scale data. By making use of\nthe angular information of principal directions of the resultant low-rank\nrepresentation, an affinity graph matrix is constructed for spectral\nclustering. Experimental results on motion segmentation and face clustering\ndata demonstrate that the proposed method often outperforms state-of-the-art\nsubspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 13:30:41 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Jie", ""], ["Chen", "Qiang", ""]]}, {"id": "1507.00955", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "comments": "32 pages, 5 figures", "journal-ref": "Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu, X.\n  (Eds.). (2016). ISBN 1910571571", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:24:18 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 11:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Tharsis T. P.", ""], ["Treleaven", "Philip", ""], ["Aste", "Tomaso", ""]]}, {"id": "1507.01053", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Aaron Courville, Yoshua Bengio", "title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "comments": "Submitted to IEEE Transactions on Multimedia Special Issue on Deep\n  Learning for Multimedia Computing", "journal-ref": null, "doi": "10.1109/TMM.2015.2477044", "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 01:06:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.01073", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Kishan\n  Wimalawarne, Suleiman A Khan, Samuel Kaski, Hiroshi Mamitsuka, Yi Chang", "title": "Convex Factorization Machine for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the convex factorization machine (CFM), which is a convex variant\nof the widely used Factorization Machines (FMs). Specifically, we employ a\nlinear+quadratic model and regularize the linear term with the\n$\\ell_2$-regularizer and the quadratic term with the trace norm regularizer.\nThen, we formulate the CFM optimization as a semidefinite programming problem\nand propose an efficient optimization procedure with Hazan's algorithm. A key\nadvantage of CFM over existing FMs is that it can find a globally optimal\nsolution, while FMs may get a poor locally optimal solution since the objective\nfunction of FMs is non-convex. In addition, the proposed algorithm is simple\nyet effective and can be implemented easily. Finally, CFM is a general\nfactorization method and can also be used for other factorization problems\nincluding including multi-view matrix factorization and tensor completion\nproblems. Through synthetic and movielens datasets, we first show that the\nproposed CFM achieves results competitive to FMs. Furthermore, in a\ntoxicogenomics prediction task, we show that CFM outperforms a state-of-the-art\ntensor factorization method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 05:54:29 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 17:17:17 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2015 08:52:42 GMT"}, {"version": "v4", "created": "Mon, 8 Aug 2016 14:55:49 GMT"}, {"version": "v5", "created": "Wed, 10 Aug 2016 01:23:56 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Yamada", "Makoto", ""], ["Lian", "Wenzhao", ""], ["Goyal", "Amit", ""], ["Chen", "Jianhui", ""], ["Wimalawarne", "Kishan", ""], ["Khan", "Suleiman A", ""], ["Kaski", "Samuel", ""], ["Mamitsuka", "Hiroshi", ""], ["Chang", "Yi", ""]]}, {"id": "1507.01160", "submitter": "Vaibhav Srivastava", "authors": "Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard", "title": "Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the correlated multiarmed bandit (MAB) problem in which the\nrewards associated with each arm are modeled by a multivariate Gaussian random\nvariable, and we investigate the influence of the assumptions in the Bayesian\nprior on the performance of the upper credible limit (UCL) algorithm and a new\ncorrelated UCL algorithm. We rigorously characterize the influence of accuracy,\nconfidence, and correlation scale in the prior on the decision-making\nperformance of the algorithms. Our results show how priors and correlation\nstructure can be leveraged to improve performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 02:16:25 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 22:27:35 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Srivastava", "Vaibhav", ""], ["Reverdy", "Paul", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1507.01193", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Andreas Vlachos", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on language modelling has shifted focus from count-based models\nto neural models. In these works, the words in each sentence are always\nconsidered in a left-to-right order. In this paper we show how we can improve\nthe performance of the recurrent neural network (RNN) language model by\nincorporating the syntactic dependencies of a sentence, which have the effect\nof bringing relevant contexts closer to the word being predicted. We evaluate\nour approach on the Microsoft Research Sentence Completion Challenge and show\nthat the dependency RNN proposed improves over the RNN by about 10 points in\naccuracy. Furthermore, we achieve results comparable with the state-of-the-art\nmodels on this task.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 11:10:24 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mirowski", "Piotr", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1507.01215", "submitter": "Ziyuan Gao", "authors": "Ziyuan Gao, Frank Stephan and Sandra Zilles", "title": "Combining Models of Approximation with Partial Learning", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Gold's framework of inductive inference, the model of partial learning\nrequires the learner to output exactly one correct index for the target object\nand only the target object infinitely often. Since infinitely many of the\nlearner's hypotheses may be incorrect, it is not obvious whether a partial\nlearner can be modifed to \"approximate\" the target object.\n  Fulk and Jain (Approximate inference and scientific method. Information and\nComputation 114(2):179--191, 1994) introduced a model of approximate learning\nof recursive functions. The present work extends their research and solves an\nopen problem of Fulk and Jain by showing that there is a learner which\napproximates and partially identifies every recursive function by outputting a\nsequence of hypotheses which, in addition, are also almost all finite variants\nof the target function.\n  The subsequent study is dedicated to the question how these findings\ngeneralise to the learning of r.e. languages from positive data. Here three\nvariants of approximate learning will be introduced and investigated with\nrespect to the question whether they can be combined with partial learning.\nFollowing the line of Fulk and Jain's research, further investigations provide\nconditions under which partial language learners can eventually output only\nfinite variants of the target language. The combinabilities of other partial\nlearning criteria will also be briefly studied.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 12:49:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 03:07:24 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Gao", "Ziyuan", ""], ["Stephan", "Frank", ""], ["Zilles", "Sandra", ""]]}, {"id": "1507.01238", "submitter": "Chong You", "authors": "Chong You, Daniel P. Robinson, Rene Vidal", "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit", "comments": "13 pages, 1 figure, 2 tables. Accepted to CVPR 2016 as an oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods based on $\\ell_1$, $\\ell_2$ or nuclear norm\nregularization have become very popular due to their simplicity, theoretical\nguarantees and empirical success. However, the choice of the regularizer can\ngreatly impact both theory and practice. For instance, $\\ell_1$ regularization\nis guaranteed to give a subspace-preserving affinity (i.e., there are no\nconnections between points from different subspaces) under broad conditions\n(e.g., arbitrary subspaces and corrupted data). However, it requires solving a\nlarge scale convex optimization problem. On the other hand, $\\ell_2$ and\nnuclear norm regularization provide efficient closed form solutions, but\nrequire very strong assumptions to guarantee a subspace-preserving affinity,\ne.g., independent subspaces and uncorrupted data. In this paper we study a\nsubspace clustering method based on orthogonal matching pursuit. We show that\nthe method is both computationally efficient and guaranteed to give a\nsubspace-preserving affinity under broad conditions. Experiments on synthetic\ndata verify our theoretical analysis, and applications in handwritten digit and\nface clustering show that our approach achieves the best trade off between\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:29:31 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 04:25:27 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 20:16:54 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["You", "Chong", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "1507.01239", "submitter": "Hang Su", "authors": "Hang Su, Haoyu Chen", "title": "Experiments on Parallel Training of Deep Neural Network using Model\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply model averaging to parallel training of deep neural\nnetwork (DNN). Parallelization is done in a model averaging manner. Data is\npartitioned and distributed to different nodes for local model updates, and\nmodel averaging across nodes is done every few minibatches. We use multiple\nGPUs for data parallelization, and Message Passing Interface (MPI) for\ncommunication between nodes, which allows us to perform model averaging\nfrequently without losing much time on communication. We investigate the\neffectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and\nRestricted Boltzmann Machine (RBM) pretraining for parallel training in\nmodel-averaging framework, and explore the best setups in term of different\nlearning rate schedules, averaging frequencies and minibatch sizes. It is shown\nthat NG-SGD and RBM pretraining benefits parameter-averaging based model\ntraining. On the 300h Switchboard dataset, a 9.3 times speedup is achieved\nusing 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:29:33 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 22:04:07 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 00:21:59 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Su", "Hang", ""], ["Chen", "Haoyu", ""]]}, {"id": "1507.01269", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser M. Nasrabadi and Alfred O. Hero III", "title": "Semi-supervised Multi-sensor Classification via Consensus-based\n  Multi-View Maximum Entropy Discrimination", "comments": "5 pages, 4 figures, Accepted in 40th IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP 15)", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178308", "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multi-sensor classification when there is a large\nnumber of unlabeled samples. The problem is formulated under the multi-view\nlearning framework and a Consensus-based Multi-View Maximum Entropy\nDiscrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the\nstochastic agreement between multiple classifiers on the unlabeled dataset, the\nalgorithm simultaneously learns multiple high accuracy classifiers. We\ndemonstrate that our proposed method can yield improved performance over\nprevious multi-view learning approaches by comparing performance on three real\nmulti-sensor data sets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 20:23:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xie", "Tianpei", ""], ["Nasrabadi", "Nasser M.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1507.01273", "submitter": "Sergey Levine", "authors": "Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel", "title": "Learning Deep Neural Network Policies with Continuous Memory States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy learning for partially observed control tasks requires policies that\ncan remember salient information from past observations. In this paper, we\npresent a method for learning policies with internal memory for\nhigh-dimensional, continuous systems, such as robotic manipulators. Our\napproach consists of augmenting the state and action space of the system with\ncontinuous-valued memory states that the policy can read from and write to.\nLearning general-purpose policies with this type of memory representation\ndirectly is difficult, because the policy must automatically figure out the\nmost salient information to memorize at each time step. We show that, by\ndecomposing this policy search problem into a trajectory optimization phase and\na supervised learning phase through a method called guided policy search, we\ncan acquire policies with effective memorization and recall strategies.\nIntuitively, the trajectory optimization phase chooses the values of the memory\nstates that will make it easier for the policy to produce the right action in\nfuture states, while the supervised learning phase encourages the policy to use\nmemorization actions to produce those memory states. We evaluate our method on\ntasks involving continuous control in manipulation and navigation settings, and\nshow that our method can learn complex policies that successfully complete a\nrange of tasks that require memory.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 20:54:57 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 04:59:46 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Zhang", "Marvin", ""], ["McCarthy", "Zoe", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1507.01279", "submitter": "Shuang Li", "authors": "Shuang Li, Yao Xie, Hanjun Dai, and Le Song", "title": "Scan $B$-Statistic for Kernel Change-Point Detection", "comments": "Submitted for journal publication. Partial results appeared in NIPS\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the emergence of an abrupt change-point is a classic problem in\nstatistics and machine learning. Kernel-based nonparametric statistics have\nbeen used for this task which enjoy fewer assumptions on the distributions than\nthe parametric approach and can handle high-dimensional data. In this paper we\nfocus on the scenario when the amount of background data is large, and propose\ntwo related computationally efficient kernel-based statistics for change-point\ndetection, which are inspired by the recently developed $B$-statistics. A novel\ntheoretical result of the paper is the characterization of the tail probability\nof these statistics using the change-of-measure technique, which focuses on\ncharacterizing the tail of the detection statistics rather than obtaining its\nasymptotic distribution under the null distribution. Such approximations are\ncrucial to control the false alarm rate, which corresponds to the significance\nlevel in offline change-point detection and the average-run-length in online\nchange-point detection. Our approximations are shown to be highly accurate.\nThus, they provide a convenient way to find detection thresholds for both\noffline and online cases without the need to resort to the more expensive\nsimulations or bootstrapping. We show that our methods perform well on both\nsynthetic data and real data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 21:46:03 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 18:41:51 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 14:36:53 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 04:22:51 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2018 20:57:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Dai", "Hanjun", ""], ["Song", "Le", ""]]}, {"id": "1507.01422", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan and Xavier Gir\\'o-i-Nieto", "title": "End-to-end Convolutional Network for Saliency Prediction", "comments": "Winner of the saliency prediction challenge in the Large-scale Scene\n  Understanding (LSUN) Challenge in the associated workshop of the IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of saliency areas in images has been traditionally addressed\nwith hand crafted features based on neuroscience principles. This paper however\naddresses the problem with a completely data-driven approach by training a\nconvolutional network. The learning process is formulated as a minimization of\na loss function that measures the Euclidean distance of the predicted saliency\nmap with the provided ground truth. The recent publication of large datasets of\nsaliency prediction has provided enough data to train a not very deep\narchitecture which is both fast and accurate. The convolutional network in this\npaper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction\nwith a superior performance in all considered metrics.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:43:26 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Pan", "Junting", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1507.01461", "submitter": "Radu Cristian Ionescu", "authors": "Radu Cristian Ionescu", "title": "Revisiting Large Scale Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the widespread of smartphones and other portable gadgets\nequipped with a variety of sensors, data is ubiquitous available and the focus\nof machine learning has shifted from being able to infer from small training\nsamples to dealing with large scale high-dimensional data. In domains such as\npersonal healthcare applications, which motivates this survey, distributed\nmachine learning is a promising line of research, both for scaling up learning\nalgorithms, but mostly for dealing with data which is inherently produced at\ndifferent locations. This report offers a thorough overview of and\nstate-of-the-art algorithms for distributed machine learning, for both\nsupervised and unsupervised learning, ranging from simple linear logistic\nregression to graphical models and clustering. We propose future directions for\nmost categories, specific to the potential personal healthcare applications.\nWith this in mind, the report focuses on how security and low communication\noverhead can be assured in the specific case of a strictly client-server\narchitectural model. As particular directions we provides an exhaustive\npresentation of an empirical clustering algorithm, k-windows, and proposed an\nasynchronous distributed machine learning algorithm that would scale well and\nalso would be computationally cheap and easy to implement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:50:26 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Ionescu", "Radu Cristian", ""]]}, {"id": "1507.01476", "submitter": "Niao He", "authors": "Niao He and Zaid Harchaoui", "title": "Semi-proximal Mirror-Prox for Nonsmooth Composite Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new first-order optimisation algorithm to solve high-dimensional\nnon-smooth composite minimisation problems. Typical examples of such problems\nhave an objective that decomposes into a non-smooth empirical risk part and a\nnon-smooth regularisation penalty. The proposed algorithm, called Semi-Proximal\nMirror-Prox, leverages the Fenchel-type representation of one part of the\nobjective while handling the other part of the objective via linear\nminimization over the domain. The algorithm stands in contrast with more\nclassical proximal gradient algorithms with smoothing, which require the\ncomputation of proximal operators at each iteration and can therefore be\nimpractical for high-dimensional problems. We establish the theoretical\nconvergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal\ncomplexity bounds, i.e. $O(1/\\epsilon^2)$, for the number of calls to linear\nminimization oracle. We present promising experimental results showing the\ninterest of the approach in comparison to competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 14:21:21 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["He", "Niao", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1507.01526", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Ivo Danihelka, Alex Graves", "title": "Grid Long Short-Term Memory", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 16:30:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 17:40:17 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:39:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""]]}, {"id": "1507.01563", "submitter": "Sariel Har-Peled", "authors": "Sariel Har-Peled", "title": "A Simple Algorithm for Maximum Margin Classification, Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] for\ncomputing a linear maximum margin classifier. Our presentation is self\ncontained, and the algorithm itself is slightly simpler than the original\nalgorithm. The algorithm itself is a simple Perceptron like iterative\nalgorithm. For more details and background, the reader is referred to the\noriginal paper.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 18:53:09 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Har-Peled", "Sariel", ""]]}, {"id": "1507.01569", "submitter": "Ashique Rupam Mahmood", "authors": "A. Rupam Mahmood, Huizhen Yu, Martha White, Richard S. Sutton", "title": "Emphatic Temporal-Difference Learning", "comments": "9 pages, accepted for presentation at European Workshop on\n  Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emphatic algorithms are temporal-difference learning algorithms that change\ntheir effective state distribution by selectively emphasizing and\nde-emphasizing their updates on different time steps. Recent works by Sutton,\nMahmood and White (2015), and Yu (2015) show that by varying the emphasis in a\nparticular way, these algorithms become stable and convergent under off-policy\ntraining with linear function approximation. This paper serves as a unified\nsummary of the available results from both works. In addition, we demonstrate\nthe empirical benefits from the flexibility of emphatic algorithms, including\nstate-dependent discounting, state-dependent bootstrapping, and the\nuser-specified allocation of function approximation resources.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 19:28:36 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mahmood", "A. Rupam", ""], ["Yu", "Huizhen", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1507.01698", "submitter": "Aniruddh Nath", "authors": "Aniruddh Nath and Pedro Domingos", "title": "Learning Tractable Probabilistic Models for Fault Localization", "comments": "Fifth International Workshop on Statistical Relational AI (StaR-AI\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several probabilistic techniques have been applied to\nvarious debugging problems. However, most existing probabilistic debugging\nsystems use relatively simple statistical models, and fail to generalize across\nmultiple programs. In this work, we propose Tractable Fault Localization Models\n(TFLMs) that can be learned from data, and probabilistically infer the location\nof the bug. While most previous statistical debugging methods generalize over\nmany executions of a single program, TFLMs are trained on a corpus of\npreviously seen buggy programs, and learn to identify recurring patterns of\nbugs. Widely-used fault localization techniques such as TARANTULA evaluate the\nsuspiciousness of each line in isolation; in contrast, a TFLM defines a joint\nprobability distribution over buggy indicator variables for each line. Joint\ndistributions with rich dependency structure are often computationally\nintractable; TFLMs avoid this by exploiting recent developments in tractable\nprobabilistic models (specifically, Relational SPNs). Further, TFLMs can\nincorporate additional sources of information, including coverage-based\nfeatures such as TARANTULA. We evaluate the fault localization performance of\nTFLMs that include TARANTULA scores as features in the probabilistic model. Our\nstudy shows that the learned TFLMs isolate bugs more effectively than previous\nstatistical methods or using TARANTULA directly.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 08:04:56 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Nath", "Aniruddh", ""], ["Domingos", "Pedro", ""]]}, {"id": "1507.01784", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Francis Bach, and Simon Lacoste-Julien", "title": "Rethinking LDA: moment matching for discrete ICA", "comments": "30 pages; added plate diagrams and clarifications, changed style,\n  corrected typos, updated figures. in Proceedings of the 29-th Conference on\n  Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider moment matching techniques for estimation in Latent Dirichlet\nAllocation (LDA). By drawing explicit links between LDA and discrete versions\nof independent component analysis (ICA), we first derive a new set of\ncumulant-based tensors, with an improved sample complexity. Moreover, we reuse\nstandard ICA techniques such as joint diagonalization of tensors to improve\nover existing methods based on the tensor power method. In an extensive set of\nexperiments on both synthetic and real datasets, we show that our new\ncombination of tensors and orthogonal joint diagonalization techniques\noutperforms existing moment matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 12:48:30 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 20:16:04 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1507.01839", "submitter": "Mingbo Ma", "authors": "Mingbo Ma and Liang Huang and Bing Xiang and Bowen Zhou", "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "comments": "this paper has been accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sentence modeling and classification, convolutional neural network\napproaches have recently achieved state-of-the-art results, but all such\nefforts process word vectors sequentially and neglect long-distance\ndependencies. To exploit both deep learning and linguistic structures, we\npropose a tree-based convolutional neural network model which exploit various\nlong-distance relationships between words. Our model improves the sequential\nbaselines on all three sentiment and question classification tasks, and\nachieves the highest published accuracy on TREC.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 15:20:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 15:36:45 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ma", "Mingbo", ""], ["Huang", "Liang", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1507.01892", "submitter": "Alessandro Montalto", "authors": "Alessandro Montalto, Giovanni Tessitore, Roberto Prevete", "title": "A linear approach for sparse coding by a two-layer neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches to transform classification problems from non-linear to\nlinear by feature transformation have been recently presented in the\nliterature. These notably include sparse coding methods and deep neural\nnetworks. However, many of these approaches require the repeated application of\na learning process upon the presentation of unseen data input vectors, or else\ninvolve the use of large numbers of parameters and hyper-parameters, which must\nbe chosen through cross-validation, thus increasing running time dramatically.\nIn this paper, we propose and experimentally investigate a new approach for the\npurpose of overcoming limitations of both kinds. The proposed approach makes\nuse of a linear auto-associative network (called SCNN) with just one hidden\nlayer. The combination of this architecture with a specific error function to\nbe minimized enables one to learn a linear encoder computing a sparse code\nwhich turns out to be as similar as possible to the sparse coding that one\nobtains by re-training the neural network. Importantly, the linearity of SCNN\nand the choice of the error function allow one to achieve reduced running time\nin the learning phase. The proposed architecture is evaluated on the basis of\ntwo standard machine learning tasks. Its performances are compared with those\nof recently proposed non-linear auto-associative neural networks. The overall\nresults suggest that linear encoders can be profitably used to obtain sparse\ndata representations in the context of machine learning problems, provided that\nan appropriate error function is used during the learning phase.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 11:42:16 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Montalto", "Alessandro", ""], ["Tessitore", "Giovanni", ""], ["Prevete", "Roberto", ""]]}, {"id": "1507.01972", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon, Klaus-Robert M\\\"uller, Marco Cuturi", "title": "Wasserstein Training of Boltzmann Machines", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Boltzmann machine provides a useful framework to learn highly complex,\nmultimodal and multiscale data distributions that occur in the real world. The\ndefault method to learn its parameters consists of minimizing the\nKullback-Leibler (KL) divergence from training samples to the Boltzmann model.\nWe propose in this work a novel approach for Boltzmann training which assumes\nthat a meaningful metric between observations is given. This metric can be\nrepresented by the Wasserstein distance between distributions, for which we\nderive a gradient with respect to the model parameters. Minimization of this\nnew Wasserstein objective leads to generative models that are better when\nconsidering the metric and that have a cluster-like structure. We demonstrate\nthe practical potential of these models for data completion and denoising, for\nwhich the metric between observations plays a crucial role.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 21:30:36 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Cuturi", "Marco", ""]]}, {"id": "1507.01978", "submitter": "Magda Gregorova", "authors": "Magda Gregorova, Alexandros Kalousis, St\\'ephane Marchand-Maillet", "title": "Learning Leading Indicators for Time Series Predictions", "comments": "Changed title plus minor updates in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning models for forecasting multiple\ntime-series systems together with discovering the leading indicators that serve\nas good predictors for the system. We model the systems by linear vector\nautoregressive models (VAR) and link the discovery of leading indicators to\ninferring sparse graphs of Granger-causality. We propose new problem\nformulations and develop two new methods to learn such models, gradually\nincreasing the complexity of assumptions and approaches. While the first method\nassumes common structures across the whole system, our second method uncovers\nmodel clusters based on the Granger-causality and leading indicators together\nwith learning the model parameters. We study the performance of our methods on\na comprehensive set of experiments and confirm their efficacy and their\nadvantages over state-of-the-art sparse VAR and graphical Granger learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 22:18:43 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 12:54:19 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 18:52:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Gregorova", "Magda", ""], ["Kalousis", "Alexandros", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1507.02011", "submitter": "Qinxun Bai", "authors": "Qinxun Bai, Henry Lam, Stan Sclaroff", "title": "A Bayesian Approach for Online Classifier Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach for recursively estimating the classifier\nweights in online learning of a classifier ensemble. In contrast with past\nmethods, such as stochastic gradient descent or online boosting, our approach\nestimates the weights by recursively updating its posterior distribution. For a\nspecified class of loss functions, we show that it is possible to formulate a\nsuitably defined likelihood function and hence use the posterior distribution\nas an approximation to the global empirical loss minimizer. If the stream of\ntraining data is sampled from a stationary process, we can also show that our\napproach admits a superior rate of convergence to the expected loss minimizer\nthan is possible with standard stochastic gradient descent. In experiments with\nreal-world datasets, our formulation often performs better than\nstate-of-the-art stochastic gradient descent and online boosting algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 03:35:58 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Bai", "Qinxun", ""], ["Lam", "Henry", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1507.02030", "submitter": "Kfir Levy Yehuda", "authors": "Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz", "title": "Beyond Convexity: Stochastic Quasi-Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic convex optimization is a basic and well studied primitive in\nmachine learning. It is well known that convex and Lipschitz functions can be\nminimized efficiently using Stochastic Gradient Descent (SGD). The Normalized\nGradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which\nupdates according to the direction of the gradients, rather than the gradients\nthemselves. In this paper we analyze a stochastic version of NGD and prove its\nconvergence to a global minimum for a wider class of functions: we require the\nfunctions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens\nthe con- cept of unimodality to multidimensions and allows for certain types of\nsaddle points, which are a known hurdle for first-order optimization methods\nsuch as gradient descent. Locally-Lipschitz functions are only required to be\nLipschitz in a small region around the optimum. This assumption circumvents\ngradient explosion, which is another known hurdle for gradient descent\nvariants. Interestingly, unlike the vanilla SGD algorithm, the stochastic\nnormalized gradient descent algorithm provably requires a minimal minibatch\nsize.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 05:47:42 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 08:14:31 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2015 07:00:56 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Hazan", "Elad", ""], ["Levy", "Kfir Y.", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1507.02084", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost", "comments": null, "journal-ref": "Pattern Recognition Letters 33 (2012) 247-255", "doi": "10.1016/j.patrec.2011.10.022", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a different insight to analyze AdaBoost. This\nanalysis reveals that, beyond some preconceptions, AdaBoost can be directly\nused as an asymmetric learning algorithm, preserving all its theoretical\nproperties. A novel class-conditional description of AdaBoost, which models the\nactual asymmetric behavior of the algorithm, is presented.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:58:06 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02154", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Double-Base Asymmetric AdaBoost", "comments": null, "journal-ref": "Neurocomputing 118 (2013) 101-114", "doi": "10.1016/j.neucom.2013.02.019", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the use of different exponential bases to define class-dependent\nerror bounds, a new and highly efficient asymmetric boosting scheme, coined as\nAdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical\nderivation procedure, unlike most of the other approaches in the literature,\nour algorithm preserves all the formal guarantees and properties of original\n(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive\nAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel\nderivation scheme enables an extremely efficient conditional search procedure,\ndramatically improving and simplifying the training phase of the algorithm.\nExperiments, both over synthetic and real datasets, reveal that AdaBoostDB is\nable to save over 99% training time with regard to Cost-Sensitive AdaBoost,\nproviding the same cost-sensitive results. This computational advantage of\nAdaBoostDB can make a difference in problems managing huge pools of weak\nclassifiers in which boosting techniques are commonly used.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:44:34 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02158", "submitter": "Nicol\\`o Navarin", "authors": "Giovanni Da San Martino, Nicol\\`o Navarin, Alessandro Sperduti", "title": "An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams\n  of Graphs", "comments": "Author's version of the manuscript, to appear in Neurocomputing\n  (ELSEVIER)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are considered an effective technique for on-line learning.\nMany approaches have been developed for compactly representing the dual\nsolution of a kernel method when the problem imposes memory constraints.\nHowever, in literature no work is specifically tailored to streams of graphs.\nMotivated by the fact that the size of the feature space representation of many\nstate-of-the-art graph kernels is relatively small and thus it is explicitly\ncomputable, we study whether executing kernel algorithms in the feature space\ncan be more effective than the classical dual approach. We study three\ndifferent algorithms and various strategies for managing the budget. Efficiency\nand efficacy of the proposed approaches are experimentally assessed on\nrelatively large graph streams exhibiting concept drift. It turns out that,\nwhen strict memory budget constraints have to be enforced, working in feature\nspace, given the current state of the art on graph kernels, is more than a\nviable alternative to dual approaches, both in terms of speed and\nclassification performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:58:19 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 11:02:46 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1507.02186", "submitter": "Nicol\\`o Navarin", "authors": "Nicol\\`o Navarin, Alessandro Sperduti, Riccardo Tesselli", "title": "Extending local features with contextual information in graph kernels", "comments": "To appear in ICONIP 2015", "journal-ref": "Lecture Notes in Computer Science, Neural Information Processing,\n  22nd International Conference, ICONIP 2015, November 9-12, 2015, Proceedings,\n  Part IV", "doi": "10.1007/978-3-319-26561-2_33", "report-no": "9492, pp 271-279", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph kernels are usually defined in terms of simpler kernels over local\nsubstructures of the original graphs. Different kernels consider different\ntypes of substructures. However, in some cases they have similar predictive\nperformances, probably because the substructures can be interpreted as\napproximations of the subgraphs they induce. In this paper, we propose to\nassociate to each feature a piece of information about the context in which the\nfeature appears in the graph. A substructure appearing in two different graphs\nwill match only if it appears with the same context in both graphs. We propose\na kernel based on this idea that considers trees as substructures, and where\nthe contexts are features too. The kernel is inspired from the framework in\n[6], even if it is not part of it. We give an efficient algorithm for computing\nthe kernel and show promising results on real-world graph classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:58:49 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 10:23:38 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""], ["Tesselli", "Riccardo", ""]]}, {"id": "1507.02188", "submitter": "Abhishek Thakur", "authors": "Abhishek Thakur and Artus Krohn-Grimberghe", "title": "AutoCompete: A Framework for Machine Learning Competition", "comments": "Paper at AutoML workshop in ICML, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 15:07:39 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Thakur", "Abhishek", ""], ["Krohn-Grimberghe", "Artus", ""]]}, {"id": "1507.02189", "submitter": "Rong Ge", "authors": "Rong Ge and James Zou", "title": "Intersecting Faces: Non-negative Matrix Factorization With New\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a natural model of admixture and\nis widely used in science and engineering. A plethora of algorithms have been\ndeveloped to tackle NMF, but due to the non-convex nature of the problem, there\nis little guarantee on how well these methods work. Recently a surge of\nresearch have focused on a very restricted class of NMFs, called separable NMF,\nwhere provably correct algorithms have been developed. In this paper, we\npropose the notion of subset-separable NMF, which substantially generalizes the\nproperty of separability. We show that subset-separability is a natural\nnecessary condition for the factorization to be unique or to have minimum\nvolume. We developed the Face-Intersect algorithm which provably and\nefficiently solves subset-separable NMF under natural conditions, and we prove\nthat our algorithm is robust to small noise. We explored the performance of\nFace-Intersect on simulations and discuss settings where it empirically\noutperformed the state-of-art methods. Our work is a step towards finding\nprovably correct algorithms that solve large classes of NMF problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 15:07:40 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Ge", "Rong", ""], ["Zou", "James", ""]]}, {"id": "1507.02216", "submitter": "Cecile Chenot", "authors": "Cecile Chenot, Jerome Bobin and Jeremy Rapin", "title": "Robust Sparse Blind Source Separation", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2015.2463232", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind Source Separation is a widely used technique to analyze multichannel\ndata. In many real-world applications, its results can be significantly\nhampered by the presence of unknown outliers. In this paper, a novel algorithm\ncoined rGMCA (robust Generalized Morphological Component Analysis) is\nintroduced to retrieve sparse sources in the presence of outliers. It\nexplicitly estimates the sources, the mixing matrix, and the outliers. It also\ntakes advantage of the estimation of the outliers to further implement a\nweighting scheme, which provides a highly robust separation procedure.\nNumerical experiments demonstrate the efficiency of rGMCA to estimate the\nmixing matrix in comparison with standard BSS techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 16:50:26 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 12:06:29 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Chenot", "Cecile", ""], ["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""]]}, {"id": "1507.02268", "submitter": "Jelani Nelson", "authors": "Michael B. Cohen, Jelani Nelson, David P. Woodruff", "title": "Optimal approximate matrix product in terms of stable rank", "comments": "v3: minor edits; v2: fixed one step in proof of Theorem 9 which was\n  wrong by a constant factor (see the new Lemma 5 and its use; final theorem\n  unaffected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove, using the subspace embedding guarantee in a black box way, that one\ncan achieve the spectral norm guarantee for approximate matrix multiplication\nwith a dimensionality-reducing map having $m = O(\\tilde{r}/\\varepsilon^2)$\nrows. Here $\\tilde{r}$ is the maximum stable rank, i.e. squared ratio of\nFrobenius and operator norms, of the two matrices being multiplied. This is a\nquantitative improvement over previous work of [MZ11, KVZ14], and is also\noptimal for any oblivious dimensionality-reducing map. Furthermore, due to the\nblack box reliance on the subspace embedding property in our proofs, our\ntheorem can be applied to a much more general class of sketching matrices than\nwhat was known before, in addition to achieving better bounds. For example, one\ncan apply our theorem to efficient subspace embeddings such as the Subsampled\nRandomized Hadamard Transform or sparse subspace embeddings, or even with\nsubspace embedding constructions that may be developed in the future.\n  Our main theorem, via connections with spectral error matrix multiplication\nshown in prior work, implies quantitative improvements for approximate least\nsquares regression and low rank approximation. Our main result has also already\nbeen applied to improve dimensionality reduction guarantees for $k$-means\nclustering [CEMMP14], and implies new results for nonparametric regression\n[YPW15].\n  We also separately point out that the proof of the \"BSS\" deterministic\nrow-sampling result of [BSS12] can be modified to show that for any matrices\n$A, B$ of stable rank at most $\\tilde{r}$, one can achieve the spectral norm\nguarantee for approximate matrix multiplication of $A^T B$ by deterministically\nsampling $O(\\tilde{r}/\\varepsilon^2)$ rows that can be found in polynomial\ntime. The original result of [BSS12] was for rank instead of stable rank. Our\nobservation leads to a stronger version of a main theorem of [KMST10].\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 19:45:21 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 22:33:26 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 12:58:32 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Cohen", "Michael B.", ""], ["Nelson", "Jelani", ""], ["Woodruff", "David P.", ""]]}, {"id": "1507.02284", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "The Information Sieve", "comments": "Appearing in Proceedings of the International Conference on Machine\n  Learning (ICML), 2016. Updated reference to continuous version:\n  http://arxiv.org/abs/1606.02307", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for unsupervised learning of representations\nbased on a novel hierarchical decomposition of information. Intuitively, data\nis passed through a series of progressively fine-grained sieves. Each layer of\nthe sieve recovers a single latent factor that is maximally informative about\nmultivariate dependence in the data. The data is transformed after each pass so\nthat the remaining unexplained information trickles down to the next layer.\nUltimately, we are left with a set of latent factors explaining all the\ndependence in the original data and remainder information consisting of\nindependent noise. We present a practical implementation of this framework for\ndiscrete variables and apply it to a variety of fundamental tasks in\nunsupervised learning including independent component analysis, lossy and\nlossless compression, and predicting missing values in data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 20:00:42 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 20:29:36 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 00:12:24 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1507.02293", "submitter": "Manuel Gomez Rodriguez", "authors": "Mehrdad Farajtabar and Yichen Wang and Manuel Gomez Rodriguez and\n  Shuang Li and Hongyuan Zha and Le Song", "title": "COEVOLVE: A Joint Point Process Model for Information Diffusion and\n  Network Co-evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information diffusion in online social networks is affected by the underlying\nnetwork topology, but it also has the power to change it. Online users are\nconstantly creating new links when exposed to new information sources, and in\nturn these links are alternating the way information spreads. However, these\ntwo highly intertwined stochastic processes, information diffusion and network\nevolution, have been predominantly studied separately, ignoring their\nco-evolutionary dynamics.\n  We propose a temporal point process model, COEVOLVE, for such joint dynamics,\nallowing the intensity of one process to be modulated by that of the other.\nThis model allows us to efficiently simulate interleaved diffusion and network\nevents, and generate traces obeying common diffusion and network patterns\nobserved in real-world networks. Furthermore, we also develop a convex\noptimization framework to learn the parameters of the model from historical\ndiffusion and network evolution traces. We experimented with both synthetic\ndata and data gathered from Twitter, and show that our model provides a good\nfit to the data as well as more accurate predictions than alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 20:01:32 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 13:51:32 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Farajtabar", "Mehrdad", ""], ["Wang", "Yichen", ""], ["Rodriguez", "Manuel Gomez", ""], ["Li", "Shuang", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""]]}, {"id": "1507.02347", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang, Minju Jung, Naveen Madapana, Jinhyung Kim, Minkyu Choi\n  and Jun Tani", "title": "Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning\n  of Dynamic Visuo-Motor-Attentional Coordination", "comments": "submitted to 2015 IEEE-RAS International Conference on Humanoid\n  Robots", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2015.7363448", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study examines how adequate coordination among different\ncognitive processes including visual recognition, attention switching, action\npreparation and generation can be developed via learning of robots by\nintroducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).\nThe proposed model is built on coupling of a dynamic vision network, a motor\ngeneration network, and a higher level network allocated on top of these two.\nThe simulation experiments using the iCub simulator were conducted for\ncognitive tasks including visual object manipulation responding to human\ngestures. The results showed that synergetic coordination can be developed via\niterative learning through the whole network when spatio-temporal hierarchy and\ntemporal one can be self-organized in the visual pathway and in the motor\npathway, respectively, such that the higher level can manipulate them with\nabstraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:10:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hwang", "Jungsik", ""], ["Jung", "Minju", ""], ["Madapana", "Naveen", ""], ["Kim", "Jinhyung", ""], ["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1507.02356", "submitter": "Chintan Dalal", "authors": "Chintan A. Dalal, Vladimir Pavlovic, Robert E. Kopp", "title": "Intrinsic Non-stationary Covariance Function for Climate Modeling", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a covariance function that represents the underlying correlation is\na crucial step in modeling complex natural systems, such as climate models.\nGeospatial datasets at a global scale usually suffer from non-stationarity and\nnon-uniformly smooth spatial boundaries. A Gaussian process regression using a\nnon-stationary covariance function has shown promise for this task, as this\ncovariance function adapts to the variable correlation structure of the\nunderlying distribution. In this paper, we generalize the non-stationary\ncovariance function to address the aforementioned global scale geospatial\nissues. We define this generalized covariance function as an intrinsic\nnon-stationary covariance function, because it uses intrinsic statistics of the\nsymmetric positive definite matrices to represent the characteristic length\nscale and, thereby, models the local stochastic process. Experiments on a\nsynthetic and real dataset of relative sea level changes across the world\ndemonstrate improvements in the error metrics for the regression estimates\nusing our newly proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:52:19 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Dalal", "Chintan A.", ""], ["Pavlovic", "Vladimir", ""], ["Kopp", "Robert E.", ""]]}, {"id": "1507.02387", "submitter": "Saurabh Khanna", "authors": "Saurabh Khanna, Chandra R. Murthy", "title": "Decentralized Joint-Sparse Signal Recovery: A Sparse Bayesian Learning\n  Approach", "comments": "Submitted to IEEE Transactions on Signal and Information Processing\n  over Networks, 15 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2612120", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a decentralized, iterative, Bayesian algorithm called\nCB-DSBL for in-network estimation of multiple jointly sparse vectors by a\nnetwork of nodes, using noisy and underdetermined linear measurements. The\nproposed algorithm exploits the network wide joint sparsity of the un- known\nsparse vectors to recover them from significantly fewer number of local\nmeasurements compared to standalone sparse signal recovery schemes. To reduce\nthe amount of inter-node communication and the associated overheads, the nodes\nexchange messages with only a small subset of their single hop neighbors. Under\nthis communication scheme, we separately analyze the convergence of the\nunderlying Alternating Directions Method of Multipliers (ADMM) iterations used\nin our proposed algorithm and establish its linear convergence rate. The\nfindings from the convergence analysis of decentralized ADMM are used to\naccelerate the convergence of the proposed CB-DSBL algorithm. Using Monte Carlo\nsimulations, we demonstrate the superior signal reconstruction as well as\nsupport recovery performance of our proposed algorithm compared to existing\ndecentralized algorithms: DRL-1, DCOMP and DCSP.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 06:21:57 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 10:58:08 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Khanna", "Saurabh", ""], ["Murthy", "Chandra R.", ""]]}, {"id": "1507.02482", "submitter": "Or Sheffet", "authors": "Or Sheffet", "title": "Differentially Private Ordinary Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is one of the most prevalent techniques in machine\nlearning, however, it is also common to use linear regression for its\n\\emph{explanatory} capabilities rather than label prediction. Ordinary Least\nSquares (OLS) is often used in statistics to establish a correlation between an\nattribute (e.g. gender) and a label (e.g. income) in the presence of other\n(potentially correlated) features. OLS assumes a particular model that randomly\ngenerates the data, and derives \\emph{$t$-values} --- representing the\nlikelihood of each real value to be the true correlation. Using $t$-values, OLS\ncan release a \\emph{confidence interval}, which is an interval on the reals\nthat is likely to contain the true correlation, and when this interval does not\nintersect the origin, we can \\emph{reject the null hypothesis} as it is likely\nthat the true correlation is non-zero. Our work aims at achieving similar\nguarantees on data under differentially private estimators. First, we show that\nfor well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives\na very good approximation of $t$-values, secondly, when JLT approximates Ridge\nregression (linear regression with $l_2$-regularization) we derive, under\ncertain conditions, confidence intervals using the projected data, lastly, we\nderive, under different conditions, confidence intervals for the \"Analyze\nGauss\" algorithm (Dwork et al, STOC 2014).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 12:32:19 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 02:19:03 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 00:24:42 GMT"}, {"version": "v4", "created": "Mon, 21 Aug 2017 21:30:27 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Sheffet", "Or", ""]]}, {"id": "1507.02528", "submitter": "Elad Hazan", "authors": "Jacob Abernethy, Elad Hazan", "title": "Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 14:32:55 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 16:50:41 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Abernethy", "Jacob", ""], ["Hazan", "Elad", ""]]}, {"id": "1507.02564", "submitter": "Ronen Eldan", "authors": "S\\'ebastien Bubeck, Ronen Eldan, Joseph Lehec", "title": "Sampling from a log-concave distribution with Projected Langevin Monte\n  Carlo", "comments": "Preliminary version; 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Langevin Monte Carlo (LMC) algorithm to compactly supported\nmeasures via a projection step, akin to projected Stochastic Gradient Descent\n(SGD). We show that (projected) LMC allows to sample in polynomial time from a\nlog-concave distribution with smooth potential. This gives a new Markov chain\nto sample from a log-concave distribution. Our main result shows in particular\nthat when the target distribution is uniform, LMC mixes in $\\tilde{O}(n^7)$\nsteps (where $n$ is the dimension). We also provide preliminary experimental\nevidence that LMC performs at least as well as hit-and-run, for which a better\nmixing time of $\\tilde{O}(n^4)$ was proved by Lov{\\'a}sz and Vempala.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 15:44:57 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""], ["Lehec", "Joseph", ""]]}, {"id": "1507.02574", "submitter": "Sariel Har-Peled", "authors": "Avrim Blum, Sariel Har-Peled and Benjamin Raichel", "title": "Sparse Approximation via Generating Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ \\newcommand{\\kalg}{{k_{\\mathrm{alg}}}}\n  \\newcommand{\\kopt}{{k_{\\mathrm{opt}}}}\n  \\newcommand{\\algset}{{T}} \\renewcommand{\\Re}{\\mathbb{R}}\n  \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\pth}[2][\\!]{#1\\left({#2}\\right)}\n\\newcommand{\\npoints}{n} \\newcommand{\\ballD}{\\mathsf{b}}\n\\newcommand{\\dataset}{{P}} $ For a set $\\dataset$ of $\\npoints$ points in the\nunit ball $\\ballD \\subseteq \\Re^d$, consider the problem of finding a small\nsubset $\\algset \\subseteq \\dataset$ such that its convex-hull\n$\\eps$-approximates the convex-hull of the original set. We present an\nefficient algorithm to compute such a $\\eps'$-approximation of size $\\kalg$,\nwhere $\\eps'$ is function of $\\eps$, and $\\kalg$ is a function of the minimum\nsize $\\kopt$ of such an $\\eps$-approximation. Surprisingly, there is no\ndependency on the dimension $d$ in both bounds. Furthermore, every point of\n$\\dataset$ can be $\\eps$-approximated by a convex-combination of points of\n$\\algset$ that is $O(1/\\eps^2)$-sparse.\n  Our result can be viewed as a method for sparse, convex autoencoding:\napproximately representing the data in a compact way using sparse combinations\nof a small subset $\\algset$ of the original data. The new algorithm can be\nkernelized, and it preserves sparsity in the original input.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 16:02:54 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 20:58:22 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Blum", "Avrim", ""], ["Har-Peled", "Sariel", ""], ["Raichel", "Benjamin", ""]]}, {"id": "1507.02592", "submitter": "Nishant Mehta", "authors": "Tim van Erven and Peter D. Gr\\\"unwald and Nishant A. Mehta and Mark D.\n  Reid and Robert C. Williamson", "title": "Fast rates in statistical and online learning", "comments": "69 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speed with which a learning algorithm converges as it is presented with\nmore data is a central problem in machine learning --- a fast rate of\nconvergence means less data is needed for the same level of performance. The\npursuit of fast rates in online and statistical learning has led to the\ndiscovery of many conditions in learning theory under which fast learning is\npossible. We show that most of these conditions are special cases of a single,\nunifying condition, that comes in two forms: the central condition for 'proper'\nlearning algorithms that always output a hypothesis in the given model, and\nstochastic mixability for online algorithms that may make predictions outside\nof the model. We show that under surprisingly weak assumptions both conditions\nare, in a certain sense, equivalent. The central condition has a\nre-interpretation in terms of convexity of a set of pseudoprobabilities,\nlinking it to density estimation under misspecification. For bounded losses, we\nshow how the central condition enables a direct proof of fast rates and we\nprove its equivalence to the Bernstein condition, itself a generalization of\nthe Tsybakov margin condition, both of which have played a central role in\nobtaining fast rates in statistical learning. Yet, while the Bernstein\ncondition is two-sided, the central condition is one-sided, making it more\nsuitable to deal with unbounded losses. In its stochastic mixability form, our\ncondition generalizes both a stochastic exp-concavity condition identified by\nJuditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying\nconditions thus provide a substantial step towards a characterization of fast\nrates in statistical learning, similar to how classical mixability\ncharacterizes constant regret in the sequential prediction with expert advice\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 16:53:30 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 09:38:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["van Erven", "Tim", ""], ["Gr\u00fcnwald", "Peter D.", ""], ["Mehta", "Nishant A.", ""], ["Reid", "Mark D.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1507.02642", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Ashish Kapoor, Christopher Granade, Krysta M Svore", "title": "Quantum Inspired Training for Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient classical algorithm for training deep Boltzmann\nmachines (DBMs) that uses rejection sampling in concert with variational\napproximations to estimate the gradients of the training objective function.\nOur algorithm is inspired by a recent quantum algorithm for training DBMs. We\nobtain rigorous bounds on the errors in the approximate gradients; in turn, we\nfind that choosing the instrumental distribution to minimize the alpha=2\ndivergence with the Gibbs state minimizes the asymptotic algorithmic\ncomplexity. Our rejection sampling approach can yield more accurate gradients\nthan low-order contrastive divergence training and the costs incurred in\nfinding increasingly accurate gradients can be easily parallelized. Finally our\nalgorithm can train full Boltzmann machines and scales more favorably with the\nnumber of layers in a DBM than greedy contrastive divergence training.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 18:32:19 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Wiebe", "Nathan", ""], ["Kapoor", "Ashish", ""], ["Granade", "Christopher", ""], ["Svore", "Krysta M", ""]]}, {"id": "1507.02672", "submitter": "Mathias Berglund", "authors": "Antti Rasmus and Harri Valpola and Mikko Honkala and Mathias Berglund\n  and Tapani Raiko", "title": "Semi-Supervised Learning with Ladder Networks", "comments": "Revised denoising function, updated results, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 19:52:19 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 09:22:23 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Rasmus", "Antti", ""], ["Valpola", "Harri", ""], ["Honkala", "Mikko", ""], ["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""]]}, {"id": "1507.02743", "submitter": "Prateek Jain", "authors": "Kush Bhatia and Himanshu Jain and Purushottam Kar and Prateek Jain and\n  Manik Varma", "title": "Locally Non-linear Embeddings for Extreme Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective in extreme multi-label learning is to train a classifier that\ncan automatically tag a novel data point with the most relevant subset of\nlabels from an extremely large label set. Embedding based approaches make\ntraining and prediction tractable by assuming that the training label matrix is\nlow-rank and hence the effective number of labels can be reduced by projecting\nthe high dimensional label vectors onto a low dimensional linear subspace.\nStill, leading embedding approaches have been unable to deliver high prediction\naccuracies or scale to large problems as the low rank assumption is violated in\nmost real world applications.\n  This paper develops the X-One classifier to address both limitations. The\nmain technical contribution in X-One is a formulation for learning a small\nensemble of local distance preserving embeddings which can accurately predict\ninfrequently occurring (tail) labels. This allows X-One to break free of the\ntraditional low-rank assumption and boost classification accuracy by learning\nembeddings which preserve pairwise distances between only the nearest label\nvectors.\n  We conducted extensive experiments on several real-world as well as benchmark\ndata sets and compared our method against state-of-the-art methods for extreme\nmulti-label classification. Experiments reveal that X-One can make\nsignificantly more accurate predictions then the state-of-the-art methods\nincluding both embeddings (by as much as 35%) as well as trees (by as much as\n6%). X-One can also scale efficiently to data sets with a million labels which\nare beyond the pale of leading embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:29:10 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Himanshu", ""], ["Kar", "Purushottam", ""], ["Jain", "Prateek", ""], ["Varma", "Manik", ""]]}, {"id": "1507.02750", "submitter": "Pratik Gajane", "authors": "Pratik Gajane and Tanguy Urvoy", "title": "Utility-based Dueling Bandits as a Partial Monitoring Game", "comments": "Accepted at the 12th European Workshop on Reinforcement Learning\n  (EWRL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial monitoring is a generic framework for sequential decision-making with\nincomplete feedback. It encompasses a wide class of problems such as dueling\nbandits, learning with expect advice, dynamic pricing, dark pools, and label\nefficient prediction. We study the utility-based dueling bandit problem as an\ninstance of partial monitoring problem and prove that it fits the time-regret\npartial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We\nsurvey some partial monitoring algorithms and see how they could be used to\nsolve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,\nPartial Monitoring, Partial Feedback, Multiarmed Bandits\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 00:05:38 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 11:47:38 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Gajane", "Pratik", ""], ["Urvoy", "Tanguy", ""]]}, {"id": "1507.02801", "submitter": "Heysem Kaya Dr", "authors": "Heysem Kaya and Albert Ali Salah", "title": "Adaptive Mixtures of Factor Analyzers", "comments": "Pre-print has 30 pages including the appendix and references. A\n  MATLAB tool of the proposed method is available (see the conclusions section)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of factor analyzers is a semi-parametric density estimator that\ngeneralizes the well-known mixtures of Gaussians model by allowing each\nGaussian in the mixture to be represented in a different lower-dimensional\nmanifold. This paper presents a robust and parsimonious model selection\nalgorithm for training a mixture of factor analyzers, carrying out simultaneous\nclustering and locally linear, globally nonlinear dimensionality reduction.\nPermitting different number of factors per mixture component, the algorithm\nadapts the model complexity to the data complexity. We compare the proposed\nalgorithm with related automatic model selection algorithms on a number of\nbenchmarks. The results indicate the effectiveness of this fast and robust\napproach in clustering, manifold learning and class-conditional modeling.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 08:13:02 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 19:50:02 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Kaya", "Heysem", ""], ["Salah", "Albert Ali", ""]]}, {"id": "1507.03032", "submitter": "Chansoo Lee", "authors": "Jacob Abernethy, Chansoo Lee, Ambuj Tewari", "title": "Spectral Smoothing via Random Matrix Perturbations", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  Theorem 6.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic smoothing of spectral functions of matrices using\nperturbations commonly studied in random matrix theory. We show that a spectral\nfunction remains spectral when smoothed using a unitarily invariant\nperturbation distribution. We then derive state-of-the-art smoothing bounds for\nthe maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).\nSmoothing the maximum eigenvalue function is important for applications in\nsemidefinite optimization and online learning. As a direct consequence of our\nGOE smoothing results, we obtain an $O((N \\log N)^{1/4} \\sqrt{T})$ expected\nregret bound for the online variance minimization problem using an algorithm\nthat performs only a single maximum eigenvector computation per time step. Here\n$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and\nits analysis also extend to the more general online PCA problem where the\nlearner has to output a rank $k$ subspace. The algorithm just requires\ncomputing $k$ maximum eigenvectors per step and enjoys an $O(k (N \\log N)^{1/4}\n\\sqrt{T})$ expected regret bound.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 20:52:35 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 01:58:25 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Abernethy", "Jacob", ""], ["Lee", "Chansoo", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1507.03040", "submitter": "Yury Maximov", "authors": "Yury Maximov, Daria Reshetova", "title": "Tight Risk Bounds for Multi-Class Margin Classifiers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of risk estimation for large-margin multi-class\nclassifiers. We propose a novel risk bound for the multi-class classification\nproblem. The bound involves the marginal distribution of the classifier and the\nRademacher complexity of the hypothesis class. We prove that our bound is tight\nin the number of classes. Finally, we compare our bound with the related ones\nand provide a simplified version of the bound for the multi-class\nclassification with kernel based hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 22:19:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 15:22:50 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 22:07:43 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Maximov", "Yury", ""], ["Reshetova", "Daria", ""]]}, {"id": "1507.03125", "submitter": "Nan Wang", "authors": "Nan Wang", "title": "A new boosting algorithm based on dual averaging scheme", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fields of machine learning and mathematical optimization increasingly\nintertwined. The special topic on supervised learning and convex optimization\nexamines this interplay. The training part of most supervised learning\nalgorithms can usually be reduced to an optimization problem that minimizes a\nloss between model predictions and training data. While most optimization\ntechniques focus on accuracy and speed of convergence, the qualities of good\noptimization algorithm from the machine learning perspective can be quite\ndifferent since machine learning is more than fitting the data. Better\noptimization algorithms that minimize the training loss can possibly give very\npoor generalization performance. In this paper, we examine a particular kind of\nmachine learning algorithm, boosting, whose training process can be viewed as\nfunctional coordinate descent on the exponential loss. We study the relation\nbetween optimization techniques and machine learning by implementing a new\nboosting algorithm. DABoost, based on dual-averaging scheme and study its\ngeneralization performance. We show that DABoost, although slower in reducing\nthe training error, in general enjoys a better generalization error than\nAdaBoost.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 16:46:37 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Wang", "Nan", ""]]}, {"id": "1507.03194", "submitter": "Ali Caner T\\\"urkmen", "authors": "Ali Caner T\\\"urkmen", "title": "A Review of Nonnegative Matrix Factorization Methods for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank\nmatrix approximation technique, and has enjoyed a wide area of applications.\nAlthough NMF does not seem related to the clustering problem at first, it was\nshown that they are closely linked. In this report, we provide a gentle\nintroduction to clustering and NMF before reviewing the theoretical\nrelationship between them. We then explore several NMF variants, namely Sparse\nNMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along\nwith their clustering interpretations.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 07:14:16 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 13:43:28 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["T\u00fcrkmen", "Ali Caner", ""]]}, {"id": "1507.03229", "submitter": "Ichiro Takeuchi Prof.", "authors": "Shinya Suzumura, Kohei Ogawa, Masashi Sugiyama, Masayuki Karasuyama,\n  Ichiro Takeuchi", "title": "Homotopy Continuation Approaches for Robust SV Classification and\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support vector machine (SVM) applications with unreliable data that\ncontains a portion of outliers, non-robustness of SVMs often causes\nconsiderable performance deterioration. Although many approaches for improving\nthe robustness of SVMs have been studied, two major challenges remain in robust\nSVM learning. First, robust learning algorithms are essentially formulated as\nnon-convex optimization problems. It is thus important to develop a non-convex\noptimization method for robust SVM that can find a good local optimal solution.\nThe second practical issue is how one can tune the hyperparameter that controls\nthe balance between robustness and efficiency. Unfortunately, due to the\nnon-convexity, robust SVM solutions with slightly different hyper-parameter\nvalues can be significantly different, which makes model selection highly\nunstable. In this paper, we address these two issues simultaneously by\nintroducing a novel homotopy approach to non-convex robust SVM learning. Our\nbasic idea is to introduce parametrized formulations of robust SVM which bridge\nthe standard SVM and fully robust SVM via the parameter that represents the\ninfluence of outliers. We characterize the necessary and sufficient conditions\nof the local optimal solutions of robust SVM, and develop an algorithm that can\ntrace a path of local optimal solutions when the influence of outliers is\ngradually decreased. An advantage of our homotopy approach is that it can be\ninterpreted as simulated annealing, a common approach for finding a good local\noptimal solution in non-convex optimization problems. In addition, our homotopy\nmethod allows stable and efficient model selection based on the path of local\noptimal solutions. Empirical performances of the proposed approach are\ndemonstrated through intensive numerical experiments both on robust\nclassification and regression problems.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 13:07:26 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Suzumura", "Shinya", ""], ["Ogawa", "Kohei", ""], ["Sugiyama", "Masashi", ""], ["Karasuyama", "Masayuki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1507.03269", "submitter": "David Steurer", "authors": "Samuel B. Hopkins and Jonathan Shi and David Steurer", "title": "Tensor principal component analysis via sum-of-squares proofs", "comments": "published in Conference on Learning Theory (COLT) 2015 (submitted\n  February 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 20:30:09 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1507.03292", "submitter": "Jaeseong Jeong", "authors": "Jaeseong Jeong, Mathieu Leconte and Alexandre Proutiere", "title": "Cluster-Aided Mobility Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future location of users in wireless net- works has numerous\napplications, and can help service providers to improve the quality of service\nperceived by their clients. The location predictors proposed so far estimate\nthe next location of a specific user by inspecting the past individual\ntrajectories of this user. As a consequence, when the training data collected\nfor a given user is limited, the resulting prediction is inaccurate. In this\npaper, we develop cluster-aided predictors that exploit past trajectories\ncollected from all users to predict the next location of a given user. These\npredictors rely on clustering techniques and extract from the training data\nsimilarities among the mobility patterns of the various users to improve the\nprediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility\nPredictor), a cluster-aided predictor whose design is based on recent\nnon-parametric bayesian statistical tools. CAMP is robust and adaptive in the\nsense that it exploits similarities in users' mobility only if such\nsimilarities are really present in the training data. We analytically prove the\nconsistency of the predictions provided by CAMP, and investigate its\nperformance using two large-scale datasets. CAMP significantly outperforms\nexisting predictors, and in particular those that only exploit individual past\ntrajectories.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 23:27:50 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 18:35:18 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 23:09:58 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2016 21:44:54 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Jeong", "Jaeseong", ""], ["Leconte", "Mathieu", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1507.03340", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, M.F. Azeem, Waseem Ahmed and A.Vinaya Babu", "title": "Quantitative Evaluation of Performance and Validity Indices for\n  Clustering the Web Navigational Sessions", "comments": null, "journal-ref": "World of Computer Science and Information Technology Journal pp.\n  217-226, Vol. 1, No. 5, June 2011. (ISSN: 2221- 0741, WCSIT Publisher, Unites\n  States)", "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering techniques are widely used in Web Usage Mining to capture similar\ninterests and trends among users accessing a Web site. For this purpose, web\naccess logs generated at a particular web site are preprocessed to discover the\nuser navigational sessions. Clustering techniques are then applied to group the\nuser session data into user session clusters, where intercluster similarities\nare minimized while the intra cluster similarities are maximized. Since the\napplication of different clustering algorithms generally results in different\nsets of cluster formation, it is important to evaluate the performance of these\nmethods in terms of accuracy and validity of the clusters, and also the time\nrequired to generate them, using appropriate performance measures. This paper\ndescribes various validity and accuracy measures including Dunn's Index, Davies\nBouldin Index, C Index, Rand Index, Jaccard Index, Silhouette Index, Fowlkes\nMallows and Sum of the Squared Error (SSE). We conducted the performance\nevaluation of the following clustering techniques: k-Means, k-Medoids, Leader,\nSingle Link Agglomerative Hierarchical and DBSCAN. These techniques are\nimplemented and tested against the Web user navigational data. Finally their\nperformance results are presented and compared.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 07:15:06 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Ansari", "Zahid", ""], ["Azeem", "M. F.", ""], ["Ahmed", "Waseem", ""], ["Babu", "A. Vinaya", ""]]}, {"id": "1507.03372", "submitter": "Nicol\\`o Navarin", "authors": "Giovanni Da San Martino, Nicol\\`o Navarin, Alessandro Sperduti", "title": "Ordered Decompositional DAG Kernels Enhancements", "comments": "Paper accepted for publication in Neurocomputing", "journal-ref": "Neurocomputing, Volume 192, 5 June 2016, Pages 92--103", "doi": "10.1016/j.neucom.2015.12.110", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how the Ordered Decomposition DAGs (ODD) kernel\nframework, a framework that allows the definition of graph kernels from tree\nkernels, allows to easily define new state-of-the-art graph kernels. Here we\nconsider a fast graph kernel based on the Subtree kernel (ST), and we propose\nvarious enhancements to increase its expressiveness. The proposed DAG kernel\nhas the same worst-case complexity as the one based on ST, but an improved\nexpressivity due to an augmented set of features. Moreover, we propose a novel\nweighting scheme for the features, which can be applied to other kernels of the\nODD framework. These improvements allow the proposed kernels to improve on the\nclassification performances of the ST-based kernel for several real-world\ndatasets, reaching state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 09:50:41 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 14:03:57 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1507.03707", "submitter": "Weiyu Xu", "authors": "Jian-Feng Cai, Suhui Liu, and Weiyu Xu", "title": "Projected Wirtinger Gradient Descent for Low-Rank Hankel Matrix\n  Completion in Spectral Compressed Sensing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers reconstructing a spectrally sparse signal from a small\nnumber of randomly observed time-domain samples. The signal of interest is a\nlinear combination of complex sinusoids at $R$ distinct frequencies. The\nfrequencies can assume any continuous values in the normalized frequency domain\n$[0,1)$. After converting the spectrally sparse signal recovery into a low rank\nstructured matrix completion problem, we propose an efficient feasible point\napproach, named projected Wirtinger gradient descent (PWGD) algorithm, to\nefficiently solve this structured matrix completion problem. We further\naccelerate our proposed algorithm by a scheme inspired by FISTA. We give the\nconvergence analysis of our proposed algorithms. Extensive numerical\nexperiments are provided to illustrate the efficiency of our proposed\nalgorithm. Different from earlier approaches, our algorithm can solve problems\nof very large dimensions very efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 02:48:09 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Liu", "Suhui", ""], ["Xu", "Weiyu", ""]]}, {"id": "1507.03719", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward", "title": "A New Framework for Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. A lot of recent effort has been\ndevoted to developing distributed algorithms for these problems. However, these\nresults suffer from high number of rounds, suboptimal approximation ratios, or\nboth. We develop a framework for bringing existing algorithms in the sequential\nsetting to the distributed setting, achieving near optimal approximation ratios\nfor many settings in only a constant number of MapReduce rounds. Our techniques\nalso give a fast sequential algorithm for non-monotone maximization subject to\na matroid constraint.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 04:46:01 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 21:20:02 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1507.03751", "submitter": "Manfred Harringer", "authors": "Manfred Harringer", "title": "Closed Curves and Elementary Visual Object Identification", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two closed curves on a plane (discrete version) and local criteria for\nsimilarity of points on the curves one gets a potential, which describes the\nsimilarity between curve points. This is the base for a global similarity\nmeasure of closed curves (Fr\\'echet distance). I use borderlines of handwritten\ndigits to demonstrate an area of application. I imagine, measuring the\nsimilarity of closed curves is an essential and elementary task performed by a\nvisual system. This approach to similarity measures may be used by visual\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 07:57:39 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Harringer", "Manfred", ""]]}, {"id": "1507.03867", "submitter": "Rong Ge", "authors": "Rong Ge and James Zou", "title": "Rich Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings, we have multiple data sets (also called views) that capture\ndifferent and overlapping aspects of the same phenomenon. We are often\ninterested in finding patterns that are unique to one or to a subset of the\nviews. For example, we might have one set of molecular observations and one set\nof physiological observations on the same group of individuals, and we want to\nquantify molecular patterns that are uncorrelated with physiology. Despite\nbeing a common problem, this is highly challenging when the correlations come\nfrom complex distributions. In this paper, we develop the general framework of\nRich Component Analysis (RCA) to model settings where the observations from\ndifferent views are driven by different sets of latent components, and each\ncomponent can be a complex, high-dimensional distribution. We introduce\nalgorithms based on cumulant extraction that provably learn each of the\ncomponents without having to model the other components. We show how to\nintegrate RCA with stochastic gradient descent into a meta-algorithm for\nlearning general models, and demonstrate substantial improvement in accuracy on\nseveral synthetic and real datasets in both supervised and unsupervised tasks.\nOur method makes it possible to learn latent variable models when we don't have\nsamples from the true model but only samples after complex perturbations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 14:38:23 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ge", "Rong", ""], ["Zou", "James", ""]]}, {"id": "1507.04029", "submitter": "Tom Portegys", "authors": "Thomas E. Portegys", "title": "Training artificial neural networks to learn a nondeterministic game", "comments": "ICAI'15: The 2015 International Conference on Artificial\n  Intelligence, Las Vegas, NV, USA, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that artificial neural networks (ANNs) can learn\ndeterministic automata. Learning nondeterministic automata is another matter.\nThis is important because much of the world is nondeterministic, taking the\nform of unpredictable or probabilistic events that must be acted upon. If ANNs\nare to engage such phenomena, then they must be able to learn how to deal with\nnondeterminism. In this project the game of Pong poses a nondeterministic\nenvironment. The learner is given an incomplete view of the game state and\nunderlying deterministic physics, resulting in a nondeterministic game. Three\nmodels were trained and tested on the game: Mona, Elman, and Numenta's NuPIC.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 21:16:23 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Portegys", "Thomas E.", ""]]}, {"id": "1507.04121", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Solomonoff Induction Violates Nicod's Criterion", "comments": "ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nicod's criterion states that observing a black raven is evidence for the\nhypothesis H that all ravens are black. We show that Solomonoff induction does\nnot satisfy Nicod's criterion: there are time steps in which observing black\nravens decreases the belief in H. Moreover, while observing any computable\ninfinite string compatible with H, the belief in H decreases infinitely often\nwhen using the unnormalized Solomonoff prior, but only finitely often when\nusing the normalized Solomonoff prior. We argue that the fault is not with\nSolomonoff induction; instead we should reject Nicod's criterion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:37:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1507.04124", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "On the Computability of Solomonoff Induction and Knowledge-Seeking", "comments": "ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solomonoff induction is held as a gold standard for learning, but it is known\nto be incomputable. We quantify its incomputability by placing various flavors\nof Solomonoff's prior M in the arithmetical hierarchy. We also derive\ncomputability bounds for knowledge-seeking agents, and give a limit-computable\nweakly asymptotically optimal reinforcement learning agent.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:46:06 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1507.04125", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part I:\n  Theoretical Perspective", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms have been widely used to tackle a plethora of problems.\nIn the last few years, a lot of approaches have been proposed to provide\nstandard AdaBoost with cost-sensitive capabilities, each with a different\nfocus. However, for the researcher, these algorithms shape a tangled set with\ndiffuse differences and properties, lacking a unifying analysis to jointly\ncompare, classify, evaluate and discuss those approaches on a common basis. In\nthis series of two papers we aim to revisit the various proposals, both from\ntheoretical (Part I) and practical (Part II) perspectives, in order to analyze\ntheir specific properties and behavior, with the final goal of identifying the\nalgorithm providing the best and soundest results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:50:09 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:11 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04126", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part II:\n  Empirical Analysis", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of approaches, each following a different strategy, have been proposed\nin the literature to provide AdaBoost with cost-sensitive properties. In the\nfirst part of this series of two papers, we have presented these algorithms in\na homogeneous notational framework, proposed a clustering scheme for them and\nperformed a thorough theoretical analysis of those approaches with a fully\ntheoretical foundation. The present paper, in order to complete our analysis,\nis focused on the empirical study of all the algorithms previously presented\nover a wide range of heterogeneous classification problems. The results of our\nexperiments, confirming the theoretical conclusions, seem to reveal that the\nsimplest approach, just based on cost-sensitive weight initialization, is the\none showing the best and soundest results, despite having been recurrently\noverlooked in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:51:18 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:33 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04155", "submitter": "Cem Orhan", "authors": "Cem Orhan and \\\"Oznur Ta\\c{s}tan", "title": "ALEVS: Active Learning by Statistical Leverage Sampling", "comments": "4 pages, presented as contributed talk in ICML 2015 Active Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to obtain a classifier of high accuracy by using fewer\nlabel requests in comparison to passive learning by selecting effective\nqueries. Many active learning methods have been developed in the past two\ndecades, which sample queries based on informativeness or representativeness of\nunlabeled data points. In this work, we explore a novel querying criterion\nbased on statistical leverage scores. The statistical leverage scores of a row\nin a matrix are the squared row-norms of the matrix containing its (top) left\nsingular vectors and is a measure of influence of the row on the matrix.\nLeverage scores have been used for detecting high influential points in\nregression diagnostics and have been recently shown to be useful for data\nanalysis and randomized low-rank matrix approximation algorithms. We explore\nhow sampling data instances with high statistical leverage scores perform in\nactive learning. Our empirical comparison on several binary classification\ndatasets indicate that querying high leverage points is an effective strategy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 10:31:00 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Orhan", "Cem", ""], ["Ta\u015ftan", "\u00d6znur", ""]]}, {"id": "1507.04201", "submitter": "Nicos Pavlidis", "authors": "Nicos G. Pavlidis, David P. Hofmeyr, Sotiris K. Tasoulis", "title": "Minimum Density Hyperplanes", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(156): 1-33, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating distinct groups of objects (clusters) with contiguous regions of\nhigh probability density (high-density clusters), is central to many\nstatistical and machine learning approaches to the classification of unlabelled\ndata. We propose a novel hyperplane classifier for clustering and\nsemi-supervised classification which is motivated by this objective. The\nproposed minimum density hyperplane minimises the integral of the empirical\nprobability density function along it, thereby avoiding intersection with high\ndensity clusters. We show that the minimum density and the maximum margin\nhyperplanes are asymptotically equivalent, thus linking this approach to\nmaximum margin clustering and semi-supervised support vector classifiers. We\npropose a projection pursuit formulation of the associated optimisation problem\nwhich allows us to find minimum density hyperplanes efficiently in practice,\nand evaluate its performance on a range of benchmark datasets. The proposed\napproach is found to be very competitive with state of the art methods for\nclustering and semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:08:11 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 14:11:27 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 17:19:48 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pavlidis", "Nicos G.", ""], ["Hofmeyr", "David P.", ""], ["Tasoulis", "Sotiris K.", ""]]}, {"id": "1507.04208", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari", "title": "Combinatorial Cascading Bandits", "comments": "Advances in Neural Information Processing Systems 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose combinatorial cascading bandits, a class of partial monitoring\nproblems where at each step a learning agent chooses a tuple of ground items\nsubject to constraints and receives a reward if and only if the weights of all\nchosen items are one. The weights of the items are binary, stochastic, and\ndrawn independently of each other. The agent observes the index of the first\nchosen item whose weight is zero. This observation model arises in network\nrouting, for instance, where the learning agent may only observe the first link\nin the routing path which is down, and blocks the path. We propose a UCB-like\nalgorithm for solving our problems, CombCascade; and prove gap-dependent and\ngap-free upper bounds on its $n$-step regret. Our proofs build on recent work\nin stochastic combinatorial semi-bandits but also address two novel challenges\nof our setting, a non-linear reward function and partial observability. We\nevaluate CombCascade on two real-world problems and show that it performs well\neven when our modeling assumptions are violated. We also demonstrate that our\nsetting requires a new learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:30:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 19:34:21 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 20:27:44 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1507.04230", "submitter": "Jiaji Huang", "authors": "Jiaji Huang and Qiang Qiu and Robert Calderbank", "title": "The Role of Principal Angles in Subspace Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2500889", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace models play an important role in a wide range of signal processing\ntasks, and this paper explores how the pairwise geometry of subspaces\ninfluences the probability of misclassification. When the mismatch between the\nsignal and the model is vanishingly small, the probability of misclassification\nis determined by the product of the sines of the principal angles between\nsubspaces. When the mismatch is more significant, the probability of\nmisclassification is determined by the sum of the squares of the sines of the\nprincipal angles. Reliability of classification is derived in terms of the\ndistribution of signal energy across principal vectors. Larger principal angles\nlead to smaller classification error, motivating a linear transform that\noptimizes principal angles. The transform presented here (TRAIT) preserves some\nspecific characteristic of each individual class, and this approach is shown to\nbe complementary to a previously developed transform (LRT) that enlarges\ninter-class distance while suppressing intra-class dispersion. Theoretical\nresults are supported by demonstration of superior classification accuracy on\nsynthetic and measured data even in the presence of significant model mismatch.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 14:24:24 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""]]}, {"id": "1507.04285", "submitter": "Nina Gierasimczuk", "authors": "Thomas Bolander and Nina Gierasimczuk", "title": "Learning Action Models: Qualitative Approach", "comments": "18 pages, accepted for LORI-V: The Fifth International Conference on\n  Logic, Rationality and Interaction, October 28-31, 2015, National Taiwan\n  University, Taipei, Taiwan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic epistemic logic, actions are described using action models. In\nthis paper we introduce a framework for studying learnability of action models\nfrom observations. We present first results concerning propositional action\nmodels. First we check two basic learnability criteria: finite identifiability\n(conclusively inferring the appropriate action model in finite time) and\nidentifiability in the limit (inconclusive convergence to the right action\nmodel). We show that deterministic actions are finitely identifiable, while\nnon-deterministic actions require more learning power-they are identifiable in\nthe limit. We then move on to a particular learning method, which proceeds via\nrestriction of a space of events within a learning-specific action model. This\nway of learning closely resembles the well-known update method from dynamic\nepistemic logic. We introduce several different learning methods suited for\nfinite identifiability of particular types of deterministic actions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 16:32:03 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Bolander", "Thomas", ""], ["Gierasimczuk", "Nina", ""]]}, {"id": "1507.04296", "submitter": "Arun Nair", "authors": "Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory\n  Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman,\n  Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray\n  Kavukcuoglu, David Silver", "title": "Massively Parallel Methods for Deep Reinforcement Learning", "comments": "Presented at the Deep Learning Workshop, International Conference on\n  Machine Learning, Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first massively distributed architecture for deep\nreinforcement learning. This architecture uses four main components: parallel\nactors that generate new behaviour; parallel learners that are trained from\nstored experience; a distributed neural network to represent the value function\nor behaviour policy; and a distributed store of experience. We used our\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\nLearning Environment, using identical hyperparameters. Our performance\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\nwall-time required to achieve these results by an order of magnitude on most\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 16:56:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 09:27:06 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Nair", "Arun", ""], ["Srinivasan", "Praveen", ""], ["Blackwell", "Sam", ""], ["Alcicek", "Cagdas", ""], ["Fearon", "Rory", ""], ["De Maria", "Alessandro", ""], ["Panneershelvam", "Vedavyas", ""], ["Suleyman", "Mustafa", ""], ["Beattie", "Charles", ""], ["Petersen", "Stig", ""], ["Legg", "Shane", ""], ["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""], ["Silver", "David", ""]]}, {"id": "1507.04319", "submitter": "Dustin Mixon", "authors": "Dustin G. Mixon, Jesse Peterson", "title": "Learning Boolean functions with concentrated spectra", "comments": null, "journal-ref": null, "doi": "10.1117/12.2189112", "report-no": null, "categories": "cs.LG cs.IT math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the theory and application of learning Boolean functions\nthat are concentrated in the Fourier domain. We first estimate the VC dimension\nof this function class in order to establish a small sample complexity of\nlearning in this case. Next, we propose a computationally efficient method of\nempirical risk minimization, and we apply this method to the MNIST database of\nhandwritten digits. These results demonstrate the effectiveness of our model\nfor modern classification tasks. We conclude with a list of open problems for\nfuture investigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 18:38:00 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""]]}, {"id": "1507.04396", "submitter": "Risi Kondor", "authors": "Risi Kondor, Nedelina Teneva, Pramod K. Mudrakarta", "title": "Parallel MMF: a Multiresolution Approach to Matrix Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Matrix Factorization (MMF) was recently introduced as a\nmethod for finding multiscale structure and defining wavelets on\ngraphs/matrices. In this paper we derive pMMF, a parallel algorithm for\ncomputing the MMF factorization. Empirically, the running time of pMMF scales\nlinearly in the dimension for sparse matrices. We argue that this makes pMMF a\nvaluable new computational primitive in its own right, and present experiments\non using pMMF for two distinct purposes: compressing matrices and\npreconditioning large sparse linear systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 21:19:25 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Kondor", "Risi", ""], ["Teneva", "Nedelina", ""], ["Mudrakarta", "Pramod K.", ""]]}, {"id": "1507.04457", "submitter": "Dohyung Park", "authors": "Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, Inderjit S.\n  Dhillon", "title": "Preference Completion: Large-scale Collaborative Ranking from Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the collaborative ranking setting: a pool of users\neach provides a small number of pairwise preferences between $d$ possible\nitems; from these we need to predict preferences of the users for items they\nhave not yet seen. We do so by fitting a rank $r$ score matrix to the pairwise\ndata, and provide two main contributions: (a) we show that an algorithm based\non convex optimization provides good generalization guarantees once each user\nprovides as few as $O(r\\log^2 d)$ pairwise comparisons -- essentially matching\nthe sample complexity required in the related matrix completion setting (which\nuses actual numerical as opposed to pairwise information), and (b) we develop a\nlarge-scale non-convex implementation, which we call AltSVM, that trains a\nfactored form of the matrix via alternating minimization (which we show reduces\nto alternating SVM problems), and scales and parallelizes very well to large\nproblem settings. It also outperforms common baselines on many moderately large\npopular collaborative filtering datasets in both NDCG and in other measures of\nranking performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 06:00:51 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Park", "Dohyung", ""], ["Neeman", "Joe", ""], ["Zhang", "Jin", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1507.04502", "submitter": "Nicholas H. Kirk", "authors": "Nicholas H. Kirk and Ilya Dianov", "title": "Towards Predicting First Daily Departure Times: a Gaussian Modeling\n  Approach for Load Shift Forecasting", "comments": "2015 IEEE International Conference on Systems, Man and Cybernetics\n  [accepted]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides two statistical Gaussian forecasting methods for\npredicting First Daily Departure Times (FDDTs) of everyday use electric\nvehicles. This is important in smart grid applications to understand\ndisconnection times of such mobile storage units, for instance to forecast\nstorage of non dispatchable loads (e.g. wind and solar power). We provide a\nreview of the relevant state-of-the-art driving behavior features towards FDDT\nprediction, to then propose an approximated Gaussian method which qualitatively\nforecasts how many vehicles will depart within a given time frame, by assuming\nthat departure times follow a normal distribution. This method considers\nsampling sessions as Poisson distributions which are superimposed to obtain a\nsingle approximated Gaussian model. Given the Gaussian distribution assumption\nof the departure times, we also model the problem with Gaussian Mixture Models\n(GMM), in which the priorly set number of clusters represents the desired time\ngranularity. Evaluation has proven that for the dataset tested, low error and\nhigh confidence ($\\approx 95\\%$) is possible for 15 and 10 minute intervals,\nand that GMM outperforms traditional modeling but is less generalizable across\ndatasets, as it is a closer fit to the sampling data. Conclusively we discuss\nfuture possibilities and practical applications of the discussed model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 09:28:27 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Kirk", "Nicholas H.", ""], ["Dianov", "Ilya", ""]]}, {"id": "1507.04523", "submitter": "Andr\\'as Antos", "authors": "Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, R\\'emi\n  Munos, Peter Auer, Andr\\'as Antos", "title": "Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed\n  Bandits", "comments": "30 pages, 2 Postscript figures, uses elsarticle.cls, earlier, shorter\n  version published in Proceedings of the 22nd International Conference,\n  Algorithmic Learning Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of estimating uniformly well the mean\nvalues of several distributions given a finite budget of samples. If the\nvariance of the distributions were known, one could design an optimal sampling\nstrategy by collecting a number of independent samples per distribution that is\nproportional to their variance. However, in the more realistic case where the\ndistributions are not known in advance, one needs to design adaptive sampling\nstrategies in order to select which distribution to sample from according to\nthe previously observed samples. We describe two strategies based on pulling\nthe distributions a number of times that is proportional to a high-probability\nupper-confidence-bound on their variance (built from previous observed samples)\nand report a finite-sample performance analysis on the excess estimation error\ncompared to the optimal allocation. We show that the performance of these\nallocation strategies depends not only on the variances but also on the full\nshape of the distributions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 11:02:13 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Lazaric", "Alessandro", ""], ["Ghavamzadeh", "Mohammad", ""], ["Munos", "R\u00e9mi", ""], ["Auer", "Peter", ""], ["Antos", "Andr\u00e1s", ""]]}, {"id": "1507.04540", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser M. Nasrabadi and Alfred O. Hero", "title": "Learning to classify with possible sensor failures", "comments": "13 pages, submitted to IEEE Transaction of Signal Processing, Feb\n  2016", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854029", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to learn a robust large-margin\nbinary classifier when corrupt measurements, called anomalies, caused by sensor\nfailure might be present in the training set. The goal is to minimize the\ngeneralization error of the classifier on non-corrupted measurements while\ncontrolling the false alarm rate associated with anomalous samples. By\nincorporating a non-parametric regularizer based on an empirical entropy\nestimator, we propose a Geometric-Entropy-Minimization regularized Maximum\nEntropy Discrimination (GEM-MED) method to learn to classify and detect\nanomalies in a joint manner. We demonstrate using simulated data and a real\nmultimodal data set. Our GEM-MED method can yield improved performance over\nprevious robust classification methods in terms of both classification accuracy\nand anomaly detection rate.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 12:16:02 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 02:19:49 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 18:18:15 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xie", "Tianpei", ""], ["Nasrabadi", "Nasser M.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1507.04646", "submitter": "Yang Liu", "authors": "Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, Houfeng Wang", "title": "A Dependency-Based Neural Network for Relation Classification", "comments": "This preprint is the full version of a short paper accepted in the\n  annual meeting of the Association for Computational Linguistics (ACL) 2015\n  (Beijing, China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on relation classification has verified the effectiveness\nof using dependency shortest paths or subtrees. In this paper, we further\nexplore how to make full use of the combination of these dependency\ninformation. We first propose a new structure, termed augmented dependency path\n(ADP), which is composed of the shortest dependency path between two entities\nand the subtrees attached to the shortest path. To exploit the semantic\nrepresentation behind the ADP structure, we develop dependency-based neural\nnetworks (DepNN): a recursive neural network designed to model the subtrees,\nand a convolutional neural network to capture the most important features on\nthe shortest path. Experiments on the SemEval-2010 dataset show that our\nproposed method achieves state-of-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:43:55 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Liu", "Yang", ""], ["Wei", "Furu", ""], ["Li", "Sujian", ""], ["Ji", "Heng", ""], ["Zhou", "Ming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1507.04717", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Raffaello Camoriano, Lorenzo Rosasco", "title": "Less is More: Nystr\\\"om Computational Regularization", "comments": "updated version of NIPS 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Nystr\\\"om type subsampling approaches to large scale kernel methods,\nand prove learning bounds in the statistical learning setting, where random\nsampling and high probability estimates are considered. In particular, we prove\nthat these approaches can achieve optimal learning bounds, provided the\nsubsampling level is suitably chosen. These results suggest a simple\nincremental variant of Nystr\\\"om Kernel Regularized Least Squares, where the\nsubsampling level implements a form of computational regularization, in the\nsense that it controls at the same time regularization and computations.\nExtensive experimental analysis shows that the considered approach achieves\nstate of the art performances on benchmark large scale datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:26:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 15:37:29 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2015 21:34:59 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2015 15:16:59 GMT"}, {"version": "v5", "created": "Mon, 7 Mar 2016 17:34:28 GMT"}, {"version": "v6", "created": "Thu, 17 Mar 2016 16:27:36 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Rudi", "Alessandro", ""], ["Camoriano", "Raffaello", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1507.04734", "submitter": "Amin Jalali", "authors": "Amin Jalali, Maryam Fazel, Lin Xiao", "title": "Variational Gram Functions: Convex Analysis and Optimization", "comments": "26 pages, 5 figures, additional revisions to text, under revision in\n  SIOPT, An earlier version of this work has appeared as Chapter 3 in reference\n  [21]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of convex penalty functions, called \\emph{variational\nGram functions} (VGFs), that can promote pairwise relations, such as\northogonality, among a set of vectors in a vector space. These functions can\nserve as regularizers in convex optimization problems arising from hierarchical\nclassification, multitask learning, and estimating vectors with disjoint\nsupports, among other applications. We study convexity for VGFs, and give\nefficient characterizations for their convex conjugates, subdifferentials, and\nproximal operators. We discuss efficient optimization algorithms for\nregularized loss minimization problems where the loss admits a common, yet\nsimple, variational representation and the regularizer is a VGF. These\nalgorithms enjoy a simple kernel trick, an efficient line search, as well as\ncomputational advantages over first order methods based on the subdifferential\nor proximal maps. We also establish a general representer theorem for such\nlearning problems. Lastly, numerical experiments on a hierarchical\nclassification problem are presented to demonstrate the effectiveness of VGFs\nand the associated optimization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 19:51:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 18:08:03 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 03:45:29 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Jalali", "Amin", ""], ["Fazel", "Maryam", ""], ["Xiao", "Lin", ""]]}, {"id": "1507.04761", "submitter": "Bob Sturm", "authors": "Corey Kereliuk and Bob L. Sturm and Jan Larsen", "title": "Deep Learning and Music Adversaries", "comments": "13 pages, 6 figures, 3 tables, 6 sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversary is essentially an algorithm intent on making a classification\nsystem perform in some particular way given an input, e.g., increase the\nprobability of a false negative. Recent work builds adversaries for deep\nlearning systems applied to image object recognition, which exploits the\nparameters of the system to find the minimal perturbation of the input image\nsuch that the network misclassifies it with high confidence. We adapt this\napproach to construct and deploy an adversary of deep learning systems applied\nto music content analysis. In our case, however, the input to the systems is\nmagnitude spectral frames, which requires special care in order to produce\nvalid input audio signals from network-derived perturbations. For two different\ntrain-test partitionings of two benchmark datasets, and two different deep\narchitectures, we find that this adversary is very effective in defeating the\nresulting systems. We find the convolutional networks are more robust, however,\ncompared with systems based on a majority vote over individually classified\naudio frames. Furthermore, we integrate the adversary into the training of new\ndeep systems, but do not find that this improves their resilience against the\nsame adversary.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 20:24:18 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Kereliuk", "Corey", ""], ["Sturm", "Bob L.", ""], ["Larsen", "Jan", ""]]}, {"id": "1507.04777", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Florian Wenzel, Shinichi Nakajima, John P. Cunningham,\n  Christoph Lippert, and Marius Kloft", "title": "Sparse Probit Linear Mixed Model", "comments": "Published version, 21 pages, 6 figures", "journal-ref": "Machine Learning, 106(9), 1621-1642 (2017)", "doi": "10.1007/s10994-017-5652-6", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Mixed Models (LMMs) are important tools in statistical genetics. When\nused for feature selection, they allow to find a sparse set of genetic traits\nthat best predict a continuous phenotype of interest, while simultaneously\ncorrecting for various confounding factors such as age, ethnicity and\npopulation structure. Formulated as models for linear regression, LMMs have\nbeen restricted to continuous phenotypes. We introduce the Sparse Probit Linear\nMixed Model (Probit-LMM), where we generalize the LMM modeling paradigm to\nbinary phenotypes. As a technical challenge, the model no longer possesses a\nclosed-form likelihood function. In this paper, we present a scalable\napproximate inference algorithm that lets us fit the model to high-dimensional\ndata sets. We show on three real-world examples from different domains that in\nthe setup of binary labels, our algorithm leads to better prediction accuracies\nand also selects features which show less correlation with the confounding\nfactors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 21:33:48 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 16:01:49 GMT"}, {"version": "v3", "created": "Sat, 11 Feb 2017 23:12:43 GMT"}, {"version": "v4", "created": "Mon, 17 Jul 2017 21:10:27 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mandt", "Stephan", ""], ["Wenzel", "Florian", ""], ["Nakajima", "Shinichi", ""], ["Cunningham", "John P.", ""], ["Lippert", "Christoph", ""], ["Kloft", "Marius", ""]]}, {"id": "1507.04793", "submitter": "Mahdi Soltanolkotabi", "authors": "Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi", "title": "Sharp Time--Data Tradeoffs for Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we characterize sharp time-data tradeoffs for optimization\nproblems used for solving linear inverse problems. We focus on the minimization\nof a least-squares objective subject to a constraint defined as the sub-level\nset of a penalty function. We present a unified convergence analysis of the\ngradient projection algorithm applied to such problems. We sharply characterize\nthe convergence rate associated with a wide variety of random measurement\nensembles in terms of the number of measurements and structural complexity of\nthe signal with respect to the chosen penalty function. The results apply to\nboth convex and nonconvex constraints, demonstrating that a linear convergence\nrate is attainable even though the least squares objective is not strongly\nconvex in these settings. When specialized to Gaussian measurements our results\nshow that such linear convergence occurs when the number of measurements is\nmerely 4 times the minimal number required to recover the desired signal at all\n(a.k.a. the phase transition). We also achieve a slower but geometric rate of\nconvergence precisely above the phase transition point. Extensive numerical\nresults suggest that the derived rates exactly match the empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 23:03:00 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 06:04:43 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1507.04798", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist", "title": "Exploratory topic modeling with distributional semantics", "comments": "Conference: The Fourteenth International Symposium on Intelligent\n  Data Analysis (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As we continue to collect and store textual data in a multitude of domains,\nwe are regularly confronted with material whose largely unknown thematic\nstructure we want to uncover. With unsupervised, exploratory analysis, no prior\nknowledge about the content is required and highly open-ended tasks can be\nsupported. In the past few years, probabilistic topic modeling has emerged as a\npopular approach to this problem. Nevertheless, the representation of the\nlatent topics as aggregations of semi-coherent terms limits their\ninterpretability and level of detail.\n  This paper presents an alternative approach to topic modeling that maps\ntopics as a network for exploration, based on distributional semantics using\nlearned word vectors. From the granular level of terms and their semantic\nsimilarity relations global topic structures emerge as clustered regions and\ngradients of concepts. Moreover, the paper discusses the visual interactive\nrepresentation of the topic map, which plays an important role in supporting\nits exploration.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 23:11:45 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""]]}, {"id": "1507.04808", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville\n  and Joelle Pineau", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models", "comments": "8 pages with references; Published in AAAI 2016 (Special Track on\n  Cognitive Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 00:21:39 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 19:49:39 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:20:41 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pineau", "Joelle", ""]]}, {"id": "1507.04831", "submitter": "Yongtao Hu", "authors": "Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, and Wenping\n  Wang", "title": "Deep Multimodal Speaker Naming", "comments": null, "journal-ref": null, "doi": "10.1145/2733373.2806293", "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker naming is the problem of localizing as well as identifying\neach speaking character in a TV/movie/live show video. This is a challenging\nproblem mainly attributes to its multimodal nature, namely face cue alone is\ninsufficient to achieve good performance. Previous multimodal approaches to\nthis problem usually process the data of different modalities individually and\nmerge them using handcrafted heuristics. Such approaches work well for simple\nscenes, but fail to achieve high performance for speakers with large appearance\nvariations. In this paper, we propose a novel convolutional neural networks\n(CNN) based learning framework to automatically learn the fusion function of\nboth face and audio cues. We show that without using face tracking, facial\nlandmark localization or subtitle/transcript, our system with robust multimodal\nfeature extraction is able to achieve state-of-the-art speaker naming\nperformance evaluated on two diverse TV series. The dataset and implementation\nof our algorithm are publicly available online.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 04:13:12 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Hu", "Yongtao", ""], ["Ren", "Jimmy", ""], ["Dai", "Jingwen", ""], ["Yuan", "Chang", ""], ["Xu", "Li", ""], ["Wang", "Wenping", ""]]}, {"id": "1507.04888", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Peter Ondruska, Ingmar Posner", "title": "Maximum Entropy Deep Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general framework for exploiting the representational\ncapacity of neural networks to approximate complex, nonlinear reward functions\nin the context of solving the inverse reinforcement learning (IRL) problem. We\nshow in this context that the Maximum Entropy paradigm for IRL lends itself\nnaturally to the efficient training of deep architectures. At test time, the\napproach leads to a computational complexity independent of the number of\ndemonstrations, which makes it especially well-suited for applications in\nlife-long learning scenarios. Our approach achieves performance commensurate to\nthe state-of-the-art on existing benchmarks while exceeding on an alternative\nbenchmark based on highly varying reward structures. Finally, we extend the\nbasic architecture - which is equivalent to a simplified subclass of Fully\nConvolutional Neural Networks (FCNNs) with width one - to include larger\nconvolutions in order to eliminate dependency on precomputed spatial features\nand work on raw input representations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 09:30:03 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 17:48:12 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 11:02:00 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Ondruska", "Peter", ""], ["Posner", "Ingmar", ""]]}, {"id": "1507.04910", "submitter": "Aleksandr Vorobev", "authors": "Aleksandr Vorobev and Gleb Gusev", "title": "Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit problem with non-equivalent\nmultiple plays where, at each step, an agent chooses not only a set of arms,\nbut also their order, which influences reward distribution. In several problem\nformulations with different assumptions, we provide lower bounds for regret\nwith standard asymptotics $O(\\log{t})$ but novel coefficients and provide\noptimal algorithms, thus proving that these bounds cannot be improved.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:39:52 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Vorobev", "Aleksandr", ""], ["Gusev", "Gleb", ""]]}, {"id": "1507.04997", "submitter": "Ismael Rodr\\'iguez-Fdez M.Sc", "authors": "I. Rodr\\'iguez-Fdez, M. Mucientes, A. Bugar\\'in", "title": "FRULER: Fuzzy Rule Learning through Evolution for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression problems, the use of TSK fuzzy systems is widely extended due\nto the precision of the obtained models. Moreover, the use of simple linear TSK\nmodels is a good choice in many real problems due to the easy understanding of\nthe relationship between the output and input variables. In this paper we\npresent FRULER, a new genetic fuzzy system for automatically learning accurate\nand simple linguistic TSK fuzzy rule bases for regression problems. In order to\nreduce the complexity of the learned models while keeping a high accuracy, the\nalgorithm consists of three stages: instance selection, multi-granularity fuzzy\ndiscretization of the input variables, and the evolutionary learning of the\nrule base that uses the Elastic Net regularization to obtain the consequents of\nthe rules. Each stage was validated using 28 real-world datasets and FRULER was\ncompared with three state of the art enetic fuzzy systems. Experimental results\nshow that FRULER achieves the most accurate and simple models compared even\nwith approximative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 15:26:06 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Rodr\u00edguez-Fdez", "I.", ""], ["Mucientes", "M.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1507.05053", "submitter": "Keiron O'Shea Mr", "authors": "Keiron O'Shea", "title": "Massively Deep Artificial Neural Networks for Handwritten Digit\n  Recognition", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on\nthe famous MNIST database of handwritten digits. All that was required to\nachieve this result was a high number of hidden layers consisting of many\nneurons, and a graphics card to greatly speed up the rate of learning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 17:48:49 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["O'Shea", "Keiron", ""]]}, {"id": "1507.05087", "submitter": "Ritwik Giri", "authors": "Ritwik Giri, Bhaskar D. Rao", "title": "Type I and Type II Bayesian Methods for Sparse Signal Recovery using\n  Scale Mixtures", "comments": "Under Review", "journal-ref": null, "doi": "10.1109/TSP.2016.2546231", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalized scale mixture family of\ndistributions, namely the Power Exponential Scale Mixture (PESM) family, to\nmodel the sparsity inducing priors currently in use for sparse signal recovery\n(SSR). We show that the successful and popular methods such as LASSO,\nReweighted $\\ell_1$ and Reweighted $\\ell_2$ methods can be formulated in an\nunified manner in a maximum a posteriori (MAP) or Type I Bayesian framework\nusing an appropriate member of the PESM family as the sparsity inducing prior.\nIn addition, exploiting the natural hierarchical framework induced by the PESM\nfamily, we utilize these priors in a Type II framework and develop the\ncorresponding EM based estimation algorithms. Some insight into the differences\nbetween Type I and Type II methods is provided and of particular interest in\nthe algorithmic development is the Type II variant of the popular and\nsuccessful reweighted $\\ell_1$ method. Extensive empirical results are provided\nand they show that the Type II methods exhibit better support recovery than the\ncorresponding Type I methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 19:57:38 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Giri", "Ritwik", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1507.05181", "submitter": "Matej Balog", "authors": "Matej Balog and Yee Whye Teh", "title": "The Mondrian Process for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is concerned with the Mondrian process and its applications in\nmachine learning. The Mondrian process is a guillotine-partition-valued\nstochastic process that possesses an elegant self-consistency property. The\nfirst part of the report uses simple concepts from applied probability to\ndefine the Mondrian process and explore its properties.\n  The Mondrian process has been used as the main building block of a clever\nonline random forest classification algorithm that turns out to be equivalent\nto its batch counterpart. We outline a slight adaptation of this algorithm to\nregression, as the remainder of the report uses regression as a case study of\nhow Mondrian processes can be utilized in machine learning. In particular, the\nMondrian process will be used to construct a fast approximation to the\ncomputationally expensive kernel ridge regression problem with a Laplace\nkernel.\n  The complexity of random guillotine partitions generated by a Mondrian\nprocess and hence the complexity of the resulting regression models is\ncontrolled by a lifetime hyperparameter. It turns out that these models can be\nefficiently trained and evaluated for all lifetimes in a given range at once,\nwithout needing to retrain them from scratch for each lifetime value. This\nleads to an efficient procedure for determining the right model complexity for\na dataset at hand.\n  The limitation of having a single lifetime hyperparameter will motivate the\nfinal Mondrian grid model, in which each input dimension is endowed with its\nown lifetime parameter. In this model we preserve the property that its\nhyperparameters can be tweaked without needing to retrain the modified model\nfrom scratch.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 12:58:11 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Balog", "Matej", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1507.05259", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez Rodriguez and\n  Krishna P. Gummadi", "title": "Fairness Constraints: Mechanisms for Fair Classification", "comments": "To appear in Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS). Open-source code\n  implementation of our scheme is available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision making systems are ubiquitous across a wide variety of\nonline as well as offline services. These systems rely on complex learning\nmethods and vast amounts of data to optimize the service functionality,\nsatisfaction of the end user and profitability. However, there is a growing\nconcern that these automated decisions can lead, even in the absence of intent,\nto a lack of fairness, i.e., their outcomes can disproportionately hurt (or,\nbenefit) particular groups of people sharing one or more sensitive attributes\n(e.g., race, sex). In this paper, we introduce a flexible mechanism to design\nfair classifiers by leveraging a novel intuitive measure of decision boundary\n(un)fairness. We instantiate this mechanism with two well-known classifiers,\nlogistic regression and support vector machines, and show on real-world data\nthat our mechanism allows for a fine-grained control on the degree of fairness,\noften at a small cost in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 07:34:25 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 16:20:40 GMT"}, {"version": "v3", "created": "Fri, 18 Mar 2016 16:56:26 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 16:21:12 GMT"}, {"version": "v5", "created": "Thu, 23 Mar 2017 18:10:34 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "1507.05307", "submitter": "Shai Ben-David", "authors": "Shai Ben-David", "title": "2 Notes on Classes with Vapnik-Chervonenkis Dimension 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vapnik-Chervonenkis dimension is a combinatorial parameter that reflects\nthe \"complexity\" of a set of sets (a.k.a. concept classes). It has been\nintroduced by Vapnik and Chervonenkis in their seminal 1971 paper and has since\nfound many applications, most notably in machine learning theory and in\ncomputational geometry. Arguably the most influential consequence of the VC\nanalysis is the fundamental theorem of statistical machine learning, stating\nthat a concept class is learnable (in some precise sense) if and only if its\nVC-dimension is finite. Furthermore, for such classes a most simple learning\nrule - empirical risk minimization (ERM) - is guaranteed to succeed.\n  The simplest non-trivial structures, in terms of the VC-dimension, are the\nclasses (i.e., sets of subsets) for which that dimension is 1.\n  In this note we show a couple of curious results concerning such classes. The\nfirst result shows that such classes share a very simple structure, and, as a\ncorollary, the labeling information contained in any sample labeled by such a\nclass can be compressed into a single instance.\n  The second result shows that due to some subtle measurability issues, in\nspite of the above mentioned fundamental theorem, there are classes of\ndimension 1 for which an ERM learning rule fails miserably.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 16:55:08 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ben-David", "Shai", ""]]}, {"id": "1507.05331", "submitter": "Justin Bayer", "authors": "Justin Bayer and Maximilian Karl and Daniela Korhammer and Patrick van\n  der Smagt", "title": "Fast Adaptive Weight Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginalising out uncertain quantities within the internal representations or\nparameters of neural networks is of central importance for a wide range of\nlearning techniques, such as empirical, variational or full Bayesian methods.\nWe set out to generalise fast dropout (Wang & Manning, 2013) to cover a wider\nvariety of noise processes in neural networks. This leads to an efficient\ncalculation of the marginal likelihood and predictive distribution which evades\nsampling and the consequential increase in training time due to highly variant\ngradient estimates. This allows us to approximate variational Bayes for the\nparameters of feed-forward neural networks. Inspired by the minimum description\nlength principle, we also propose and experimentally verify the direct\noptimisation of the regularised predictive distribution. The methods yield\nresults competitive with previous neural network based approaches and Gaussian\nprocesses on a wide range of regression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 20:30:10 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Bayer", "Justin", ""], ["Karl", "Maximilian", ""], ["Korhammer", "Daniela", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1507.05371", "submitter": "Luis F Voloch", "authors": "Guy Bresler, Devavrat Shah, and Luis F. Voloch", "title": "Regret Guarantees for Item-Item Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much empirical evidence that item-item collaborative filtering works\nwell in practice. Motivated to understand this, we provide a framework to\ndesign and analyze various recommendation algorithms. The setup amounts to\nonline binary matrix completion, where at each time a random user requests a\nrecommendation and the algorithm chooses an entry to reveal in the user's row.\nThe goal is to minimize regret, or equivalently to maximize the number of +1\nentries revealed at any time. We analyze an item-item collaborative filtering\nalgorithm that can achieve fundamentally better performance compared to\nuser-user collaborative filtering. The algorithm achieves good \"cold-start\"\nperformance (appropriately defined) by quickly making good recommendations to\nnew users about whom there is little information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:45:01 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 15:28:40 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bresler", "Guy", ""], ["Shah", "Devavrat", ""], ["Voloch", "Luis F.", ""]]}, {"id": "1507.05444", "submitter": "Tom Rainforth", "authors": "Tom Rainforth and Frank Wood", "title": "Canonical Correlation Forests", "comments": "Substantial update: longer journal format version which now covers\n  regression and multiple output prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce canonical correlation forests (CCFs), a new decision tree\nensemble method for classification and regression. Individual canonical\ncorrelation trees are binary decision trees with hyperplane splits based on\nlocal canonical correlation coefficients calculated during training. Unlike\naxis-aligned alternatives, the decision surfaces of CCFs are not restricted to\nthe coordinate system of the inputs features and therefore more naturally\nrepresent data with correlated inputs. CCFs naturally accommodate multiple\noutputs, provide a similar computational complexity to random forests, and\ninherit their impressive robustness to the choice of input parameters. As part\nof the CCF training algorithm, we also introduce projection bootstrapping, a\nnovel alternative to bagging for oblique decision tree ensembles which\nmaintains use of the full dataset in selecting split points, often leading to\nimprovements in predictive accuracy. Our experiments show that, even without\nparameter tuning, CCFs out-perform axis-aligned random forests and other\nstate-of-the-art tree ensemble methods on both classification and regression\nproblems, delivering both improved predictive accuracy and faster training\ntimes. We further show that they outperform all of the 179 classifiers\nconsidered in a recent extensive survey.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 10:51:02 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 09:59:49 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 15:17:25 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2015 10:54:55 GMT"}, {"version": "v5", "created": "Sat, 5 Dec 2015 18:48:19 GMT"}, {"version": "v6", "created": "Wed, 9 Aug 2017 16:55:56 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Rainforth", "Tom", ""], ["Wood", "Frank", ""]]}, {"id": "1507.05455", "submitter": "Duncan Barrack S", "authors": "Duncan Barrack, James Goulding, Keith Hopcraft, Simon Preston and\n  Gavin Smith", "title": "AMP: a new time-frequency feature extraction method for intermittent\n  time-series data", "comments": "Paper accepted in workshop on mining and learning from time series\n  (MiLeTS) to be held in conjunction with KDD 2015 August 10- 13 2015, Sydney,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterisation of time-series data via their most salient features is\nextremely important in a range of machine learning task, not least of all with\nregards to classification and clustering. While there exist many feature\nextraction techniques suitable for non-intermittent time-series data, these\napproaches are not always appropriate for intermittent time-series data, where\nintermittency is characterized by constant values for large periods of time\npunctuated by sharp and transient increases or decreases in value.\n  Motivated by this, we present aggregation, mode decomposition and projection\n(AMP) a feature extraction technique particularly suited to intermittent\ntime-series data which contain time-frequency patterns. For our method all\nindividual time-series within a set are combined to form a non-intermittent\naggregate. This is decomposed into a set of components which represent the\nintrinsic time-frequency signals within the data set. Individual time-series\ncan then be fit to these components to obtain a set of numerical features that\nrepresent their intrinsic time-frequency patterns. To demonstrate the\neffectiveness of AMP, we evaluate against the real word task of clustering\nintermittent time-series data. Using synthetically generated data we show that\na clustering approach which uses the features derived from AMP significantly\noutperforms traditional clustering methods. Our technique is further\nexemplified on a real world data set where AMP can be used to discover\ngroupings of individuals which correspond to real world sub-populations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 11:48:01 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 18:16:15 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Barrack", "Duncan", ""], ["Goulding", "James", ""], ["Hopcraft", "Keith", ""], ["Preston", "Simon", ""], ["Smith", "Gavin", ""]]}, {"id": "1507.05498", "submitter": "Alexander Jung", "authors": "Alexander Jung, Yonina C. Eldar, Norbert G\\\"ortz", "title": "On the Minimax Risk of Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a dictionary matrix from a number of\nobserved signals, which are assumed to be generated via a linear model with a\ncommon underlying dictionary. In particular, we derive lower bounds on the\nminimum achievable worst case mean squared error (MSE), regardless of\ncomputational complexity of the dictionary learning (DL) schemes. By casting DL\nas a classical (or frequentist) estimation problem, the lower bounds on the\nworst case MSE are derived by following an established information-theoretic\napproach to minimax estimation. The main conceptual contribution of this paper\nis the adaption of the information-theoretic approach to minimax estimation for\nthe DL problem in order to derive lower bounds on the worst case MSE of any DL\nscheme. We derive three different lower bounds applying to different generative\nmodels for the observed signals. The first bound applies to a wide range of\nmodels, it only requires the existence of a covariance matrix of the (unknown)\nunderlying coefficient vector. By specializing this bound to the case of sparse\ncoefficient distributions, and assuming the true dictionary satisfies the\nrestricted isometry property, we obtain a lower bound on the worst case MSE of\nDL schemes in terms of a signal to noise ratio (SNR). The third bound applies\nto a more restrictive subclass of coefficient distributions by requiring the\nnon-zero coefficients to be Gaussian. While, compared with the previous two\nbounds, the applicability of this final bound is the most limited it is the\ntightest of the three bounds in the low SNR regime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:58:49 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Jung", "Alexander", ""], ["Eldar", "Yonina C.", ""], ["G\u00f6rtz", "Norbert", ""]]}, {"id": "1507.05532", "submitter": "Hongyu Miao", "authors": "Na Lu, Hongyu Miao", "title": "Clustering Tree-structured Data on Manifold", "comments": "14 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured data usually contain both topological and geometrical\ninformation, and are necessarily considered on manifold instead of Euclidean\nspace for appropriate data parameterization and analysis. In this study, we\npropose a novel tree-structured data parameterization, called\nTopology-Attribute matrix (T-A matrix), so the data clustering task can be\nconducted on matrix manifold. We incorporate the structure constraints embedded\nin data into the negative matrix factorization method to determine meta-trees\nfrom the T-A matrix, and the signature vector of each single tree can then be\nextracted by meta-tree decomposition. The meta-tree space turns out to be a\ncone space, in which we explore the distance metric and implement the\nclustering algorithm based on the concepts like Fr\\'echet mean. Finally, the\nT-A matrix based clustering (TAMBAC) framework is evaluated and compared using\nboth simulated data and real retinal images to illustrate its efficiency and\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 15:23:00 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 02:32:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lu", "Na", ""], ["Miao", "Hongyu", ""]]}, {"id": "1507.05670", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Ce Zhang, Christopher R\\'e and Li Fei-Fei", "title": "Building a Large-scale Multimodal Knowledge Base System for Answering\n  Visual Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of the visual world creates significant challenges for\ncomprehensive visual understanding. In spite of recent successes in visual\nrecognition, today's vision systems would still struggle to deal with visual\nqueries that require a deeper reasoning. We propose a knowledge base (KB)\nframework to handle an assortment of visual queries, without the need to train\nnew classifiers for new tasks. Building such a large-scale multimodal KB\npresents a major challenge of scalability. We cast a large-scale MRF into a KB\nrepresentation, incorporating visual, textual and structured data, as well as\ntheir diverse relations. We introduce a scalable knowledge base construction\nsystem that is capable of building a KB with half billion variables and\nmillions of parameters in a few hours. Our system achieves competitive results\ncompared to purpose-built models on standard recognition and retrieval tasks,\nwhile exhibiting greater flexibility in answering richer visual queries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 22:43:51 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 22:52:22 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Zhu", "Yuke", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1507.05775", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Jia-Nan Wu", "title": "Compression of Fully-Connected Layer in Neural Network by Kronecker\n  Product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a technique to reduce the number of\nparameters and computation time in fully-connected layers of neural networks\nusing Kronecker product, at a mild cost of the prediction quality. The\ntechnique proceeds by replacing Fully-Connected layers with so-called Kronecker\nFully-Connected layers, where the weight matrices of the FC layers are\napproximated by linear combinations of multiple Kronecker products of smaller\nmatrices. In particular, given a model trained on SVHN dataset, we are able to\nconstruct a new KFC model with 73\\% reduction in total number of parameters,\nwhile the error only rises mildly. In contrast, using low-rank method can only\nachieve 35\\% reduction in total number of parameters given similar quality\ndegradation allowance. If we only compare the KFC layer with its counterpart\nfully-connected layer, the reduction in the number of parameters exceeds 99\\%.\nThe amount of computation is also reduced as we replace matrix product of the\nlarge matrices in FC layers with matrix products of a few smaller matrices in\nKFC layers. Further experiments on MNIST, SVHN and some Chinese Character\nrecognition models also demonstrate effectiveness of our technique.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:29:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 11:59:08 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Jia-Nan", ""]]}, {"id": "1507.05800", "submitter": "Hao Zhang", "authors": "Hao Zhang, Yao Ma, Masashi Sugiyama", "title": "Bandit-Based Task Assignment for Heterogeneous Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a task assignment problem in crowdsourcing, which is aimed at\ncollecting as many reliable labels as possible within a limited budget. A\nchallenge in this scenario is how to cope with the diversity of tasks and the\ntask-dependent reliability of workers, e.g., a worker may be good at\nrecognizing the name of sports teams, but not be familiar with cosmetics\nbrands. We refer to this practical setting as heterogeneous crowdsourcing. In\nthis paper, we propose a contextual bandit formulation for task assignment in\nheterogeneous crowdsourcing, which is able to deal with the\nexploration-exploitation trade-off in worker selection. We also theoretically\ninvestigate the regret bounds for the proposed method, and demonstrate its\npractical usefulness experimentally.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 12:17:58 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Zhang", "Hao", ""], ["Ma", "Yao", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1507.05819", "submitter": "Joss Wright", "authors": "Joss Wright, Alexander Darer, Oliver Farnan", "title": "On Identifying Anomalies in Tor Usage with Applications in Detecting\n  Internet Censorship", "comments": "To appear in ACM WebSci 2018", "journal-ref": null, "doi": "10.1145/3201064.3201093", "report-no": null, "categories": "cs.CY cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop a means to detect ongoing per-country anomalies in the daily usage\nmetrics of the Tor anonymous communication network, and demonstrate the\napplicability of this technique to identifying likely periods of internet\ncensorship and related events. The presented approach identifies contiguous\nanomalous periods, rather than daily spikes or drops, and allows anomalies to\nbe ranked according to deviation from expected behaviour.\n  The developed method is implemented as a running tool, with outputs published\ndaily by mailing list. This list highlights per-country anomalous Tor usage,\nand produces a daily ranking of countries according to the level of detected\nanomalous behaviour. This list has been active since August 2016, and is in use\nby a number of individuals, academics, and NGOs as an early warning system for\npotential censorship events.\n  We focus on Tor, however the presented approach is more generally applicable\nto usage data of other services, both individually and in combination. We\ndemonstrate that combining multiple data sources allows more specific\nidentification of likely Tor blocking events. We demonstrate the our approach\nin comparison to existing anomaly detection tools, and against both known\nhistorical internet censorship events and synthetic datasets. Finally, we\ndetail a number of significant recent anomalous events and behaviours\nidentified by our tool.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 13:17:24 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 22:02:03 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 15:54:04 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wright", "Joss", ""], ["Darer", "Alexander", ""], ["Farnan", "Oliver", ""]]}, {"id": "1507.05880", "submitter": "Elif Vural", "authors": "Elif Vural and Christine Guillemot", "title": "A study of the classification of low-dimensional data with supervised\n  manifold learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised manifold learning methods learn data representations by preserving\nthe geometric structure of data while enhancing the separation between data\nsamples from different classes. In this work, we propose a theoretical study of\nsupervised manifold learning for classification. We consider nonlinear\ndimensionality reduction algorithms that yield linearly separable embeddings of\ntraining data and present generalization bounds for this type of algorithms. A\nnecessary condition for satisfactory generalization performance is that the\nembedding allow the construction of a sufficiently regular interpolation\nfunction in relation with the separation margin of the embedding. We show that\nfor supervised embeddings satisfying this condition, the classification error\ndecays at an exponential rate with the number of training samples. Finally, we\nexamine the separability of supervised nonlinear embeddings that aim to\npreserve the low-dimensional geometric structure of data based on graph\nrepresentations. The proposed analysis is supported by experiments on several\nreal data sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 15:55:46 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 11:59:45 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 15:59:18 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Vural", "Elif", ""], ["Guillemot", "Christine", ""]]}, {"id": "1507.05910", "submitter": "Sarath Chandar", "authors": "Alex Auvolat, Sarath Chandar, Pascal Vincent, Hugo Larochelle, Yoshua\n  Bengio", "title": "Clustering is Efficient for Approximate Maximum Inner Product Search", "comments": "10 pages, Under review at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:53:12 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 16:36:09 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 02:26:44 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Auvolat", "Alex", ""], ["Chandar", "Sarath", ""], ["Vincent", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.05950", "submitter": "Dimitris S. Papailiopoulos", "authors": "Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein", "title": "On the Worst-Case Approximability of Sparse PCA", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:34:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Chan", "Siu On", ""], ["Papailiopoulos", "Dimitris", ""], ["Rubinstein", "Aviad", ""]]}, {"id": "1507.05952", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath", "title": "Optimal Testing for Properties of Distributions", "comments": "31 pages, extended abstract appeared as a spotlight in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown distribution $p$, is it possible to distinguish\nwhether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$\nbeing far from every distribution in $\\mathcal{C}$? This fundamental question\nhas received tremendous attention in statistics, focusing primarily on\nasymptotic analysis, and more recently in information theory and theoretical\ncomputer science, where the emphasis has been on small sample size and\ncomputational complexity. Nevertheless, even for basic properties of\ndistributions such as monotonicity, log-concavity, unimodality, independence,\nand monotone-hazard rate, the optimal sample complexity is unknown.\n  We provide a general approach via which we obtain sample-optimal and\ncomputationally efficient testers for all these distribution families. At the\ncore of our approach is an algorithm which solves the following problem: Given\nsamples from an unknown distribution $p$, and a known distribution $q$, are $p$\nand $q$ close in $\\chi^2$-distance, or far in total variation distance?\n  The optimality of our testers is established by providing matching lower\nbounds with respect to both $n$ and $\\varepsilon$. Finally, a necessary\nbuilding block for our testers and an important byproduct of our work are the\nfirst known computationally efficient proper learners for discrete log-concave\nand monotone hazard rate distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:52:56 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 14:12:33 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 20:00:11 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Acharya", "Jayadev", ""], ["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1507.06020", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "Practical Selection of SVM Supervised Parameters with Different Feature\n  Representations for Vowel Recognition", "comments": "07 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the classification performance of Support Vector Machine\n(SVM) can be conveniently affected by the different parameters of the kernel\ntricks and the regularization parameter, C. Thus, in this article, we propose a\nstudy in order to find the suitable kernel with which SVM may achieve good\ngeneralization performance as well as the parameters to use. We need to analyze\nthe behavior of the SVM classifier when these parameters take very small or\nvery large values. The study is conducted for a multi-class vowel recognition\nusing the TIMIT corpus. Furthermore, for the experiments, we used different\nfeature representations such as MFCC and PLP. Finally, a comparative study was\ndone to point out the impact of the choice of the parameters, kernel trick and\nfeature representations on the performance of the SVM classifier\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:27:11 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06021", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "An Empirical Comparison of SVM and Some Supervised Learning Algorithms\n  for Vowel recognition", "comments": "08 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we conduct a study on the performance of some supervised\nlearning algorithms for vowel recognition. This study aims to compare the\naccuracy of each algorithm. Thus, we present an empirical comparison between\nfive supervised learning classifiers and two combined classifiers: SVM, KNN,\nNaive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms\nwere tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral\ncoefficients (MFCCs).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:34:15 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06023", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Ghaith Manita, Abir Smiti", "title": "Robust speech recognition using consensus function based on multi-layer\n  networks", "comments": "06 pages", "journal-ref": "9th Iberian Conference on Information Systems and Technologies\n  (CISTI), Barcelona 18-21 June, 2014, pgs 1-6", "doi": "10.1109/CISTI.2014.6877093", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering ensembles mingle numerous partitions of a specified data into\na single clustering solution. Clustering ensemble has emerged as a potent\napproach for ameliorating both the forcefulness and the stability of\nunsupervised classification results. One of the major problems in clustering\nensembles is to find the best consensus function. Finding final partition from\ndifferent clustering results requires skillfulness and robustness of the\nclassification algorithm. In addition, the major problem with the consensus\nfunction is its sensitivity to the used data sets quality. This limitation is\ndue to the existence of noisy, silence or redundant data. This paper proposes a\nnovel consensus function of cluster ensembles based on Multilayer networks\ntechnique and a maintenance database method. This maintenance database approach\nis used in order to handle any given noisy speech and, thus, to guarantee the\nquality of databases. This can generates good results and efficient data\npartitions. To show its effectiveness, we support our strategy with empirical\nevaluation using distorted speech from Aurora speech databases.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:49:48 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Manita", "Ghaith", ""], ["Smiti", "Abir", ""]]}, {"id": "1507.06025", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Nouerddine Ellouze", "title": "Incorporating Belief Function in SVM for Phoneme Recognition", "comments": "9th International Conference, Hybrid Artificial Intelligence Systems,\n  Salamanca, Spain, June 11-13, 2014", "journal-ref": "Lecture Notes in Computer Science Volume 8480, 2014, pp 191-199", "doi": "10.1007/978-3-319-07617-1_17", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine (SVM) method has been widely used in numerous\nclassification tasks. The main idea of this algorithm is based on the principle\nof the margin maximization to find an hyperplane which separates the data into\ntwo different classes.In this paper, SVM is applied to phoneme recognition\ntask. However, in many real-world problems, each phoneme in the data set for\nrecognition problems may differ in the degree of significance due to noise,\ninaccuracies, or abnormal characteristics; All those problems can lead to the\ninaccuracies in the prediction phase. Unfortunately, the standard formulation\nof SVM does not take into account all those problems and, in particular, the\nvariation in the speech input. This paper presents a new formulation of SVM\n(B-SVM) that attributes to each phoneme a confidence degree computed based on\nits geometric position in the space. Then, this degree is used in order to\nstrengthen the class membership of the tested phoneme. Hence, we introduce a\nreformulation of the standard SVM that incorporates the degree of belief.\nExperimental performance on TIMIT database shows the effectiveness of the\nproposed method B-SVM on a phoneme recognition problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 01:03:28 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Nouerddine", ""]]}, {"id": "1507.06028", "submitter": "Rimah Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "The challenges of SVM optimization using Adaboost on a phoneme\n  recognition problem", "comments": null, "journal-ref": "IEEE 4th International Conference on Cognitive Infocommunications\n  (CogInfoCom), Budapest 2-5 Dec. 2013, pgs 463-468", "doi": "10.1109/CogInfoCom.2013.6719292", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of digital technology is growing at a very fast pace which led to the\nemergence of systems based on the cognitive infocommunications. The expansion\nof this sector impose the use of combining methods in order to ensure the\nrobustness in cognitive systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 01:27:05 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06065", "submitter": "Reshad Hosseini", "authors": "Reshad Hosseini and Mohamadreza Mash'al", "title": "MixEst: An Estimation Toolbox for Mixture Models", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are powerful statistical models used in many applications\nranging from density estimation to clustering and classification. When dealing\nwith mixture models, there are many issues that the experimenter should be\naware of and needs to solve. The MixEst toolbox is a powerful and user-friendly\npackage for MATLAB that implements several state-of-the-art approaches to\naddress these problems. Additionally, MixEst gives the possibility of using\nmanifold optimization for fitting the density model, a feature specific to this\ntoolbox. MixEst simplifies using and integration of mixture models in\nstatistical models and applications. For developing mixture models of new\ndensities, the user just needs to provide a few functions for that statistical\ndistribution and the toolbox takes care of all the issues regarding mixture\nmodels. MixEst is available at visionlab.ut.ac.ir/mixest and is fully\ndocumented and is licensed under GPL.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 05:23:14 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Hosseini", "Reshad", ""], ["Mash'al", "Mohamadreza", ""]]}, {"id": "1507.06105", "submitter": "Jianyuan Sun", "authors": "Jianyuan Sun and Guoqiang Zhong and Junyu Dong and Yajuan Cai", "title": "Banzhaf Random Forests", "comments": "arXiv admin note: text overlap with arXiv:1302.4853 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a type of ensemble method which makes predictions by\ncombining the results of several independent trees. However, the theory of\nrandom forests has long been outpaced by their application. In this paper, we\npropose a novel random forests algorithm based on cooperative game theory.\nBanzhaf power index is employed to evaluate the power of each feature by\ntraversing possible feature coalitions. Unlike the previously used information\ngain rate of information theory, which simply chooses the most informative\nfeature, the Banzhaf power index can be considered as a metric of the\nimportance of each feature on the dependency among a group of features. More\nimportantly, we have proved the consistency of the proposed algorithm, named\nBanzhaf random forests (BRF). This theoretical analysis takes a step towards\nnarrowing the gap between the theory and practice of random forests for\nclassification problems. Experiments on several UCI benchmark data sets show\nthat BRF is competitive with state-of-the-art classifiers and dramatically\noutperforms previous consistent random forests. Particularly, it is much more\nefficient than previous consistent random forests.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:10:15 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Sun", "Jianyuan", ""], ["Zhong", "Guoqiang", ""], ["Dong", "Junyu", ""], ["Cai", "Yajuan", ""]]}, {"id": "1507.06228", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Klaus Greff, J\\\"urgen Schmidhuber", "title": "Training Very Deep Networks", "comments": "11 pages. Extends arXiv:1505.00387. Project webpage is at\n  http://people.idsia.ch/~rupesh/very_deep_learning/. in Advances in Neural\n  Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical and empirical evidence indicates that the depth of neural\nnetworks is crucial for their success. However, training becomes more difficult\nas depth increases, and training of very deep networks remains an open problem.\nHere we introduce a new architecture designed to overcome this. Our so-called\nhighway networks allow unimpeded information flow across many layers on\ninformation highways. They are inspired by Long Short-Term Memory recurrent\nnetworks and use adaptive gating units to regulate the information flow. Even\nwith hundreds of layers, highway networks can be trained directly through\nsimple gradient descent. This enables the study of extremely deep and efficient\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 15:29:14 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 16:25:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Greff", "Klaus", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1507.06346", "submitter": "Robert Mattila", "authors": "Robert Mattila, Cristian R. Rojas, Bo Wahlberg", "title": "Evaluation of Spectral Learning for the Identification of Hidden Markov\n  Models", "comments": "This paper is accepted and will be published in The Proceedings of\n  the 17th IFAC Symposium on System Identification (SYSID 2015), Beijing,\n  China, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models have successfully been applied as models of discrete\ntime series in many fields. Often, when applied in practice, the parameters of\nthese models have to be estimated. The currently predominating identification\nmethods, such as maximum-likelihood estimation and especially\nexpectation-maximization, are iterative and prone to have problems with local\nminima. A non-iterative method employing a spectral subspace-like approach has\nrecently been proposed in the machine learning literature. This paper evaluates\nthe performance of this algorithm, and compares it to the performance of the\nexpectation-maximization algorithm, on a number of numerical examples. We find\nthat the performance is mixed; it successfully identifies some systems with\nrelatively few available observations, but fails completely for some systems\neven when a large amount of observations is available. An open question is how\nthis discrepancy can be explained. We provide some indications that it could be\nrelated to how well-conditioned some system parameters are.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 21:49:19 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Mattila", "Robert", ""], ["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1507.06370", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Avi Wigderson", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 05:50:16 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ma", "Tengyu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1507.06452", "submitter": "Amin Mantrach", "authors": "Robin Devooght and Nicolas Kourtellis and Amin Mantrach", "title": "Dynamic Matrix Factorization with Priors on Unknown Values", "comments": "in the Proceedings of 21st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced and effective collaborative filtering methods based on explicit\nfeedback assume that unknown ratings do not follow the same model as the\nobserved ones (\\emph{not missing at random}). In this work, we build on this\nassumption, and introduce a novel dynamic matrix factorization framework that\nallows to set an explicit prior on unknown values. When new ratings, users, or\nitems enter the system, we can update the factorization in time independent of\nthe size of data (number of users, items and ratings). Hence, we can quickly\nrecommend items even to very recent users. We test our methods on three large\ndatasets, including two very sparse ones, in static and dynamic conditions. In\neach case, we outrank state-of-the-art matrix factorization methods that do not\nuse a prior on unknown ratings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 11:39:58 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Devooght", "Robin", ""], ["Kourtellis", "Nicolas", ""], ["Mantrach", "Amin", ""]]}, {"id": "1507.06527", "submitter": "Matthew Hausknecht", "authors": "Matthew Hausknecht and Peter Stone", "title": "Deep Recurrent Q-Learning for Partially Observable MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning has yielded proficient controllers for complex\ntasks. However, these controllers have limited memory and rely on being able to\nperceive the complete game screen at each decision point. To address these\nshortcomings, this article investigates the effects of adding recurrency to a\nDeep Q-Network (DQN) by replacing the first post-convolutional fully-connected\nlayer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network}\n(DRQN), although capable of seeing only a single frame at each timestep,\nsuccessfully integrates information through time and replicates DQN's\nperformance on standard Atari games and partially observed equivalents\nfeaturing flickering game screens. Additionally, when trained with partial\nobservations and evaluated with incrementally more complete observations,\nDRQN's performance scales as a function of observability. Conversely, when\ntrained with full observations and evaluated with partial observations, DRQN's\nperformance degrades less than DQN's. Thus, given the same length of history,\nrecurrency is a viable alternative to stacking a history of frames in the DQN's\ninput layer and while recurrency confers no systematic advantage when learning\nto play the game, the recurrent net can better adapt at evaluation time if the\nquality of observations changes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 15:16:46 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 21:17:47 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2015 20:17:22 GMT"}, {"version": "v4", "created": "Wed, 11 Jan 2017 20:25:54 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Hausknecht", "Matthew", ""], ["Stone", "Peter", ""]]}, {"id": "1507.06535", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Pascal Frossard", "title": "Manitest: Are classifiers really invariant?", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance to geometric transformations is a highly desirable property of\nautomatic classifiers in many image recognition tasks. Nevertheless, it is\nunclear to which extent state-of-the-art classifiers are invariant to basic\ntransformations such as rotations and translations. This is mainly due to the\nlack of general methods that properly measure such an invariance. In this\npaper, we propose a rigorous and systematic approach for quantifying the\ninvariance to geometric transformations of any classifier. Our key idea is to\ncast the problem of assessing a classifier's invariance as the computation of\ngeodesics along the manifold of transformed images. We propose the Manitest\nmethod, built on the efficient Fast Marching algorithm to compute the\ninvariance of classifiers. Our new method quantifies in particular the\nimportance of data augmentation for learning invariance from data, and the\nincreased invariance of convolutional neural networks with depth. We foresee\nthat the proposed generic tool for measuring invariance to a large class of\ngeometric transformations and arbitrary classifiers will have many applications\nfor evaluating and comparing classifiers based on their invariance, and help\nimproving the invariance of existing classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 15:36:50 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Frossard", "Pascal", ""]]}, {"id": "1507.06550", "submitter": "Joao Carreira", "authors": "Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, Jitendra Malik", "title": "Human Pose Estimation with Iterative Error Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical feature extractors such as Convolutional Networks (ConvNets)\nhave achieved impressive performance on a variety of classification tasks using\npurely feedforward processing. Feedforward architectures can learn rich\nrepresentations of the input space but do not explicitly model dependencies in\nthe output spaces, that are quite structured for tasks such as articulated\nhuman pose estimation or object segmentation. Here we propose a framework that\nexpands the expressive power of hierarchical feature extractors to encompass\nboth input and output spaces, by introducing top-down feedback. Instead of\ndirectly predicting the outputs in one go, we use a self-correcting model that\nprogressively changes an initial solution by feeding back error predictions, in\na process we call Iterative Error Feedback (IEF). IEF shows excellent\nperformance on the task of articulated pose estimation in the challenging MPII\nand LSP benchmarks, matching the state-of-the-art without requiring ground\ntruth scale annotation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 16:20:57 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 12:37:48 GMT"}, {"version": "v3", "created": "Sun, 12 Jun 2016 19:10:55 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Carreira", "Joao", ""], ["Agrawal", "Pulkit", ""], ["Fragkiadaki", "Katerina", ""], ["Malik", "Jitendra", ""]]}, {"id": "1507.06580", "submitter": "Ronen Eldan", "authors": "S\\'ebastien Bubeck and Ronen Eldan", "title": "Multi-scale exploration of convex functions and bandit convex\n  optimization", "comments": "Preliminary version; 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a new map from a convex function to a distribution on its\ndomain, with the property that this distribution is a multi-scale exploration\nof the function. We use this map to solve a decade-old open problem in\nadversarial bandit convex optimization by showing that the minimax regret for\nthis problem is $\\tilde{O}(\\mathrm{poly}(n) \\sqrt{T})$, where $n$ is the\ndimension and $T$ the number of rounds. This bound is obtained by studying the\ndual Bayesian maximin regret via the information ratio analysis of Russo and\nVan Roy, and then using the multi-scale exploration to solve the Bayesian\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 17:32:49 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""]]}, {"id": "1507.06682", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Chia-Wei Lien, Fu-Jen Chu, Pai-Shun Ting, Shin-Ming Cheng", "title": "Supervised Collective Classification for Crowdsourcing", "comments": "to appear in IEEE Global Communications Conference (GLOBECOM)\n  Workshop on Networking and Collaboration Issues for the Internet of\n  Everything", "journal-ref": null, "doi": "10.1109/GLOCOMW.2015.7414077", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing utilizes the wisdom of crowds for collective classification via\ninformation (e.g., labels of an item) provided by labelers. Current\ncrowdsourcing algorithms are mainly unsupervised methods that are unaware of\nthe quality of crowdsourced data. In this paper, we propose a supervised\ncollective classification algorithm that aims to identify reliable labelers\nfrom the training data (e.g., items with known labels). The reliability (i.e.,\nweighting factor) of each labeler is determined via a saddle point algorithm.\nThe results on several crowdsourced data show that supervised methods can\nachieve better classification accuracy than unsupervised methods, and our\nproposed method outperforms other algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 21:02:33 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:13:46 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Lien", "Chia-Wei", ""], ["Chu", "Fu-Jen", ""], ["Ting", "Pai-Shun", ""], ["Cheng", "Shin-Ming", ""]]}, {"id": "1507.06738", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur", "title": "Linear Contextual Bandits with Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear contextual bandit problem with resource consumption,\nin addition to reward generation. In each round, the outcome of pulling an arm\nis a reward as well as a vector of resource consumptions. The expected values\nof these outcomes depend linearly on the context of that arm. The\nbudget/capacity constraints require that the total consumption doesn't exceed\nthe budget for each resource. The objective is once again to maximize the total\nreward. This problem turns out to be a common generalization of classic linear\ncontextual bandits (linContextual), bandits with knapsacks (BwK), and the\nonline stochastic packing problem (OSPP). We present algorithms with\nnear-optimal regret bounds for this problem. Our bounds compare favorably to\nresults on the unstructured version of the problem where the relation between\nthe contexts and the outcomes could be arbitrary, but the algorithm only\ncompetes against a fixed set of policies accessible through an optimization\noracle. We combine techniques from the work on linContextual, BwK, and OSPP in\na nontrivial manner while also tackling new difficulties that are not present\nin any of these special cases.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 04:24:22 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 06:29:22 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""]]}, {"id": "1507.06763", "submitter": "Rina Okada", "authors": "Rina Okada, Kazuto Fukuchi, Kazuya Kakizaki and Jun Sakuma", "title": "Differentially Private Analysis of Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates differentially private analysis of distance-based\noutliers. The problem of outlier detection is to find a small number of\ninstances that are apparently distant from the remaining instances. On the\nother hand, the objective of differential privacy is to conceal presence (or\nabsence) of any particular instance. Outlier detection and privacy protection\nare thus intrinsically conflicting tasks. In this paper, instead of reporting\noutliers detected, we present two types of differentially private queries that\nhelp to understand behavior of outliers. One is the query to count outliers,\nwhich reports the number of outliers that appear in a given subspace. Our\nformal analysis on the exact global sensitivity of outlier counts reveals that\nregular global sensitivity based method can make the outputs too noisy,\nparticularly when the dimensionality of the given subspace is high. Noting that\nthe counts of outliers are typically expected to be relatively small compared\nto the number of data, we introduce a mechanism based on the smooth upper bound\nof the local sensitivity. The other is the query to discovery top-$h$ subspaces\ncontaining a large number of outliers. This task can be naively achieved by\nissuing count queries to each subspace in turn. However, the variation of\nsubspaces can grow exponentially in the data dimensionality. This can cause\nserious consumption of the privacy budget. For this task, we propose an\nexponential mechanism with a customized score function for subspace discovery.\nTo the best of our knowledge, this study is the first trial to ensure\ndifferential privacy for distance-based outlier analysis. We demonstrated our\nmethods with synthesized datasets and real datasets. The experimental results\nshow that out method achieve better utility compared to the global sensitivity\nbased methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 07:30:49 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 02:19:15 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Okada", "Rina", ""], ["Fukuchi", "Kazuto", ""], ["Kakizaki", "Kazuya", ""], ["Sakuma", "Jun", ""]]}, {"id": "1507.06802", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Implicitly Constrained Semi-Supervised Least Squares Classification", "comments": "12 pages, 2 figures, 1 table. The Fourteenth International Symposium\n  on Intelligent Data Analysis (2015), Saint-Etienne, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel semi-supervised version of the least squares classifier.\nThis implicitly constrained least squares (ICLS) classifier minimizes the\nsquared loss on the labeled data among the set of parameters implied by all\npossible labelings of the unlabeled data. Unlike other discriminative\nsemi-supervised methods, our approach does not introduce explicit additional\nassumptions into the objective function, but leverages implicit assumptions\nalready present in the choice of the supervised least squares classifier. We\nshow this approach can be formulated as a quadratic programming problem and its\nsolution can be found using a simple gradient descent procedure. We prove that,\nin a certain way, our method never leads to performance worse than the\nsupervised classifier. Experimental results corroborate this theoretical result\nin the multidimensional case on benchmark datasets, also in terms of the error\nrate.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:39:44 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1507.06803", "submitter": "Ferran Mazzanti", "authors": "E. Romero, F. Mazzanti, J. Delgado", "title": "A Neighbourhood-Based Stopping Criterion for Contrastive Divergence\n  Learning", "comments": "7 pages. arXiv admin note: substantial text overlap with\n  arXiv:1312.6062", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are general unsupervised learning\ndevices to ascertain generative models of data distributions. RBMs are often\ntrained using the Contrastive Divergence learning algorithm (CD), an\napproximation to the gradient of the data log-likelihood. A simple\nreconstruction error is often used as a stopping criterion for CD, although\nseveral authors\n\\cite{schulz-et-al-Convergence-Contrastive-Divergence-2010-NIPSw,\nfischer-igel-Divergence-Contrastive-Divergence-2010-ICANN} have raised doubts\nconcerning the feasibility of this procedure. In many cases the evolution curve\nof the reconstruction error is monotonic while the log-likelihood is not, thus\nindicating that the former is not a good estimator of the optimal stopping\npoint for learning. However, not many alternatives to the reconstruction error\nhave been discussed in the literature. In this manuscript we investigate simple\nalternatives to the reconstruction error, based on the inclusion of information\ncontained in neighboring states to the training set, as a stopping criterion\nfor CD learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 10:45:19 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Romero", "E.", ""], ["Mazzanti", "F.", ""], ["Delgado", "J.", ""]]}, {"id": "1507.06821", "submitter": "Andreas Eitel", "authors": "Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin\n  Riedmiller, Wolfram Burgard", "title": "Multimodal Deep Learning for Robust RGB-D Object Recognition", "comments": "Final version submitted to IROS'2015, results unchanged,\n  reformulation of some text passages in abstract and introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object recognition is a crucial ingredient of many, if not all,\nreal-world robotics applications. This paper leverages recent progress on\nConvolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture\nfor object recognition. Our architecture is composed of two separate CNN\nprocessing streams - one for each modality - which are consecutively combined\nwith a late fusion network. We focus on learning with imperfect sensor data, a\ntypical problem in real-world robotics tasks. For accurate learning, we\nintroduce a multi-stage training methodology and two crucial ingredients for\nhandling depth data with CNNs. The first, an effective encoding of depth\ninformation for CNNs that enables learning without the need for large depth\ndatasets. The second, a data augmentation scheme for robust learning with depth\nimages by corrupting them with realistic noise patterns. We present\nstate-of-the-art results on the RGB-D object dataset and show recognition in\nchallenging RGB-D real-world noisy settings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 12:20:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 13:04:29 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Eitel", "Andreas", ""], ["Springenberg", "Jost Tobias", ""], ["Spinello", "Luciano", ""], ["Riedmiller", "Martin", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1507.06829", "submitter": "Lisa Posch", "authors": "Lisa Posch, Arnim Bleier, Philipp Schaer, Markus Strohmaier", "title": "The Polylingual Labeled Topic Model", "comments": "Accepted for publication at KI 2015 (38th edition of the German\n  Conference on Artificial Intelligence)", "journal-ref": null, "doi": "10.1007/978-3-319-24489-1_26", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Polylingual Labeled Topic Model, a model which\ncombines the characteristics of the existing Polylingual Topic Model and\nLabeled LDA. The model accounts for multiple languages with separate topic\ndistributions for each language while restricting the permitted topics of a\ndocument to a set of predefined labels. We explore the properties of the model\nin a two-language setting on a dataset from the social science domain. Our\nexperiments show that our model outperforms LDA and Labeled LDA in terms of\ntheir held-out perplexity and that it produces semantically coherent topics\nwhich are well interpretable by human subjects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:01:20 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Posch", "Lisa", ""], ["Bleier", "Arnim", ""], ["Schaer", "Philipp", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1507.06923", "submitter": "Abhinav Garlapati", "authors": "Abhinav Garlapati, Aditi Raghunathan, Vaishnavh Nagarajan and\n  Balaraman Ravindran", "title": "A Reinforcement Learning Approach to Online Learning of Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online decision tree learning algorithms typically examine all features of a\nnew data point to update model parameters. We propose a novel alternative,\nReinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement\nLearning (RL) to actively examine a minimal number of features of a data point\nto classify it with high accuracy. Furthermore, RLDT optimizes a long term\nreturn, providing a better alternative to the traditional myopic greedy\napproach to growing decision trees. We demonstrate that this approach performs\nas well as batch learning algorithms and other online decision tree learning\nalgorithms, while making significantly fewer queries about the features of the\ndata points. We also show that RLDT can effectively handle concept drift.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 17:22:17 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Garlapati", "Abhinav", ""], ["Raghunathan", "Aditi", ""], ["Nagarajan", "Vaishnavh", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1507.06947", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Kanishka Rao, Fran\\c{c}oise Beaufays", "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\n  Recognition", "comments": "To be published in the INTERSPEECH 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\nacoustic models for speech recognition. More recently, we have shown that the\nperformance of sequence trained context dependent (CD) hidden Markov model\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\nphone models initialized with connectionist temporal classification (CTC). In\nthis paper, we present techniques that further improve performance of LSTM RNN\nacoustic models for large vocabulary speech recognition. We show that frame\nstacking and reduced frame rate lead to more accurate models and faster\ndecoding. CD phone modeling leads to further improvements. We also present\ninitial results for LSTM RNN models outputting words directly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:28:32 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Rao", "Kanishka", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1507.06970", "submitter": "Horia Mania", "authors": "Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht,\n  Kannan Ramchandran, Michael I. Jordan", "title": "Perturbed Iterate Analysis for Asynchronous Stochastic Optimization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 19:36:13 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 20:00:45 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Mania", "Horia", ""], ["Pan", "Xinghao", ""], ["Papailiopoulos", "Dimitris", ""], ["Recht", "Benjamin", ""], ["Ramchandran", "Kannan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1507.07105", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel, Michael Tschannen, and Helmut B\\\"olcskei", "title": "Dimensionality-reduced subspace clustering", "comments": "new results for the noisy case, additional simulation work,\n  additional discussions in the main body", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering unlabeled\nhigh-dimensional data points into a union of low-dimensional linear subspaces,\nwhose number, orientations, and dimensions are all unknown. In practice one may\nhave access to dimensionality-reduced observations of the data only, resulting,\ne.g., from undersampling due to complexity and speed constraints on the\nacquisition device or mechanism. More pertinently, even if the high-dimensional\ndata set is available it is often desirable to first project the data points\ninto a lower-dimensional space and to perform clustering there; this reduces\nstorage requirements and computational cost. The purpose of this paper is to\nquantify the impact of dimensionality reduction through random projection on\nthe performance of three subspace clustering algorithms, all of which are based\non principles from sparse signal recovery. Specifically, we analyze the\nthresholding based subspace clustering (TSC) algorithm, the sparse subspace\nclustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof\n(SSC-OMP). We find, for all three algorithms, that dimensionality reduction\ndown to the order of the subspace dimensions is possible without incurring\nsignificant performance degradation. Moreover, these results are order-wise\noptimal in the sense that reducing the dimensionality further leads to a\nfundamentally ill-posed clustering problem. Our findings carry over to the\nnoisy case as illustrated through analytical results for TSC and simulations\nfor SSC and SSC-OMP. Extensive experiments on synthetic and real data\ncomplement our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 14:49:01 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 20:50:49 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Heckel", "Reinhard", ""], ["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1507.07146", "submitter": "Dayong Wang", "authors": "Dayong Wang and Pengcheng Wu and Peilin Zhao and Steven C.H. Hoi", "title": "A Framework of Sparse Online Learning and Its Applications", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data in our society has been exploding in the era of big data\ntoday. In this paper, we address several open challenges of big data stream\nclassification, including high volume, high velocity, high dimensionality, high\nsparsity, and high class-imbalance. Many existing studies in data mining\nliterature solve data stream classification tasks in a batch learning setting,\nwhich suffers from poor efficiency and scalability when dealing with big data.\nTo overcome the limitations, this paper investigates an online learning\nframework for big data stream classification tasks. Unlike some existing online\ndata stream classification techniques that are often based on first-order\nonline learning, we propose a framework of Sparse Online Classification (SOC)\nfor data stream classification, which includes some state-of-the-art\nfirst-order sparse online learning algorithms as special cases and allows us to\nderive a new effective second-order online learning algorithm for data stream\nclassification. In addition, we also propose a new cost-sensitive sparse online\nlearning algorithm by extending the framework with application to tackle online\nanomaly detection tasks where class distribution of data could be very\nimbalanced. We also analyze the theoretical bounds of the proposed method, and\nfinally conduct an extensive set of experiments, in which encouraging results\nvalidate the efficacy of the proposed algorithms in comparison to a family of\nstate-of-the-art techniques on a variety of data stream classification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 22:53:31 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Wang", "Dayong", ""], ["Wu", "Pengcheng", ""], ["Zhao", "Peilin", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1507.07147", "submitter": "Richard Sutton", "authors": "Richard S. Sutton", "title": "True Online Emphatic TD($\\lambda$): Quick Reference and Implementation\n  Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is a guide to the implementation of true online emphatic\nTD($\\lambda$), a model-free temporal-difference algorithm for learning to make\nlong-term predictions which combines the emphasis idea (Sutton, Mahmood & White\n2015) and the true-online idea (van Seijen & Sutton 2014). The setting used\nhere includes linear function approximation, the possibility of off-policy\ntraining, and all the generality of general value functions, as well as the\nemphasis algorithm's notion of \"interest\".\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 22:56:29 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Sutton", "Richard S.", ""]]}, {"id": "1507.07199", "submitter": "Hao Zhang", "authors": "Hao Zhang, Masashi Sugiyama", "title": "Task Selection for Bandit-Based Task Assignment in Heterogeneous\n  Crowdsourcing", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.05800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task selection (picking an appropriate labeling task) and worker selection\n(assigning the labeling task to a suitable worker) are two major challenges in\ntask assignment for crowdsourcing. Recently, worker selection has been\nsuccessfully addressed by the bandit-based task assignment (BBTA) method, while\ntask selection has not been thoroughly investigated yet. In this paper, we\nexperimentally compare several task selection strategies borrowed from active\nlearning literature, and show that the least confidence strategy significantly\nimproves the performance of task assignment in crowdsourcing.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 13:26:57 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Zhang", "Hao", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1507.07260", "submitter": "Hassan Kingravi", "authors": "Hassan A. Kingravi, Patricio A. Vela, Alexandar Gray", "title": "Reduced-Set Kernel Principal Components Analysis for Improving the\n  Training and Execution Speed of Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical, and theoretically well-founded, approach to\nimprove the speed of kernel manifold learning algorithms relying on spectral\ndecomposition. Utilizing recent insights in kernel smoothing and learning with\nintegral operators, we propose Reduced Set KPCA (RSKPCA), which also suggests\nan easy-to-implement method to remove or replace samples with minimal effect on\nthe empirical operator. A simple data point selection procedure is given to\ngenerate a substitute density for the data, with accuracy that is governed by a\nuser-tunable parameter . The effect of the approximation on the quality of the\nKPCA solution, in terms of spectral and operator errors, can be shown directly\nin terms of the density estimate error and as a function of the parameter . We\nshow in experiments that RSKPCA can improve both training and evaluation time\nof KPCA by up to an order of magnitude, and compares favorably to the\nwidely-used Nystrom and density-weighted Nystrom methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 22:28:34 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Kingravi", "Hassan A.", ""], ["Vela", "Patricio A.", ""], ["Gray", "Alexandar", ""]]}, {"id": "1507.07374", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Andrey Ustyuzhanin", "title": "A genetic algorithm for autonomous navigation in partially observable\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of autonomous navigation is one of the basic problems for\nrobotics. Although, in general, it may be challenging when an autonomous\nvehicle is placed into partially observable domain. In this paper we consider\nsimplistic environment model and introduce a navigation algorithm based on\nLearning Classifier System.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 11:50:44 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Borisyak", "Maxim", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1507.07495", "submitter": "Asif Shakeel", "authors": "David A. Meyer and Asif Shakeel", "title": "Estimating an Activity Driven Hidden Markov Model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 17:37:27 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Meyer", "David A.", ""], ["Shakeel", "Asif", ""]]}, {"id": "1507.07595", "submitter": "Tengyu Ma", "authors": "Jason D. Lee, Qihang Lin, Tengyu Ma, Tianbao Yang", "title": "Distributed Stochastic Variance Reduced Gradient Methods and A Lower\n  Bound for Communication Complexity", "comments": "significant addition to both theory and experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed optimization algorithms for minimizing the average of\nconvex functions. The applications include empirical risk minimization problems\nin statistical machine learning where the datasets are large and have to be\nstored on different machines. We design a distributed stochastic variance\nreduced gradient algorithm that, under certain conditions on the condition\nnumber, simultaneously achieves the optimal parallel runtime, amount of\ncommunication and rounds of communication among all distributed first-order\nmethods up to constant factors. Our method and its accelerated extension also\noutperform existing distributed algorithms in terms of the rounds of\ncommunication as long as the condition number is not too large compared to the\nsize of data in each machine. We also prove a lower bound for the number of\nrounds of communication for a broad class of distributed first-order methods\nincluding the proposed algorithms in this paper. We show that our accelerated\ndistributed stochastic variance reduced gradient algorithm achieves this lower\nbound so that it uses the fewest rounds of communication among all distributed\nfirst-order algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 22:09:57 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 19:26:31 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Lee", "Jason D.", ""], ["Lin", "Qihang", ""], ["Ma", "Tengyu", ""], ["Yang", "Tianbao", ""]]}, {"id": "1507.07680", "submitter": "Yann Ollivier", "authors": "Yann Ollivier, Corentin Tallec, Guillaume Charpiat", "title": "Training recurrent networks online without backtracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical\nsystems such as recurrent neural networks. This algorithm works in an online,\nmemoryless setting, thus requiring no backpropagation through time, and is\nscalable, avoiding the large computational and memory cost of maintaining the\nfull gradient of the current state with respect to the parameters.\n  The algorithm essentially maintains, at each time, a single search direction\nin parameter space. The evolution of this search direction is partly stochastic\nand is constructed in such a way to provide, at every time, an unbiased random\nestimate of the gradient of the loss function with respect to the parameters.\nBecause the gradient estimate is unbiased, on average over time the parameter\nis updated as it should.\n  The resulting gradient estimate can then be fed to a lightweight Kalman-like\nfilter to yield an improved algorithm. For recurrent neural networks, the\nresulting algorithms scale linearly with the number of parameters.\n  Small-scale experiments confirm the suitability of the approach, showing that\nthe stochastic approximation of the gradient introduced in the algorithm is not\ndetrimental to learning. In particular, the Kalman-like version of NoBackTrack\nis superior to backpropagation through time (BPTT) when the time span of\ndependencies in the data is longer than the truncation span for BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 08:26:50 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 22:29:38 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ollivier", "Yann", ""], ["Tallec", "Corentin", ""], ["Charpiat", "Guillaume", ""]]}, {"id": "1507.07830", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Timothy Hospedales", "title": "Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian", "comments": "Accepted to BMVC 2015 Workshop on Differential Geometry in Computer\n  Vision (DIFF-CV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most visual recognition methods implicitly assume the data distribution\nremains unchanged from training to testing. However, in practice domain shift\noften exists, where real-world factors such as lighting and sensor type change\nbetween train and test, and classifiers do not generalise from source to target\ndomains. It is impractical to train separate models for all possible situations\nbecause collecting and labelling the data is expensive. Domain adaptation\nalgorithms aim to ameliorate domain shift, allowing a model trained on a source\nto perform well on a different target domain. However, even for the setting of\nunsupervised domain adaptation, where the target domain is unlabelled,\ncollecting data for every possible target domain is still costly. In this\npaper, we propose a new domain adaptation method that has no need to access\neither data or labels of the target domain when it can be described by a\nparametrised vector and there exits several related source domains within the\nsame parametric space. It greatly reduces the burden of data collection and\nannotation, and our experiments show some promising results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 16:13:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 17:53:59 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1507.07870", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist and Peter Sarlin", "title": "Detect & Describe: Deep learning of bank stress in the news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.AI cs.LG cs.NE q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News is a pertinent source of information on financial risks and stress\nfactors, which nevertheless is challenging to harness due to the sparse and\nunstructured nature of natural text. We propose an approach based on\ndistributional semantics and deep learning with neural networks to model and\nlink text to a scarce set of bank distress events. Through unsupervised\ntraining, we learn semantic vector representations of news articles as\npredictors of distress events. The predictive model that we learn can signal\ncoinciding stress with an aggregated index at bank or European level, while\ncrucially allowing for automatic extraction of text descriptions of the events,\nbased on passages with high stress levels. The method offers insight that\nmodels based on other types of data cannot provide, while offering a general\nmeans for interpreting this type of semantic-predictive model. We model bank\ndistress with data on 243 events and 6.6M news articles for 101 large European\nbanks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 18:47:09 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1507.07880", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "Optimally Confident UCB: Improved Regret for Finite-Armed Bandits", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present the first algorithm for stochastic finite-armed bandits that\nsimultaneously enjoys order-optimal problem-dependent regret and worst-case\nregret. Besides the theoretical results, the new algorithm is simple, efficient\nand empirically superb. The approach is based on UCB, but with a carefully\nchosen confidence parameter that optimally balances the risk of failing\nconfidence intervals against the cost of excessive optimism.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 18:09:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 19:42:13 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2016 17:10:17 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1507.07955", "submitter": "Eric Heim", "authors": "Eric Heim and Milos Hauskrecht (University of Pittsburgh)", "title": "Sparse Multidimensional Patient Modeling using Auxiliary Confidence\n  Labels", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on the problem of learning a classification model that\nperforms inference on patient Electronic Health Records (EHRs). Often, a large\namount of costly expert supervision is required to learn such a model. To\nreduce this cost, we obtain confidence labels that indicate how sure an expert\nis in the class labels she provides. If meaningful confidence information can\nbe incorporated into a learning method, fewer patient instances may need to be\nlabeled to learn an accurate model. In addition, while accuracy of predictions\nis important for any inference model, a model of patients must be interpretable\nso that clinicians can understand how the model is making decisions. To these\nends, we develop a novel metric learning method called Confidence bAsed MEtric\nLearning (CAMEL) that supports inclusion of confidence labels, but also\nemphasizes interpretability in three ways. First, our method induces sparsity,\nthus producing simple models that use only a few features from patient EHRs.\nSecond, CAMEL naturally produces confidence scores that can be taken into\nconsideration when clinicians make treatment decisions. Third, the metrics\nlearned by CAMEL induce multidimensional spaces where each dimension represents\na different \"factor\" that clinicians can use to assess patients. In our\nexperimental evaluation, we show on a real-world clinical data set that our\nCAMEL methods are able to learn models that are as or more accurate as other\nmethods that use the same supervision. Furthermore, we show that when CAMEL\nuses confidence scores it is able to learn models as or more accurate as others\nwe tested while using only 10% of the training instances. Finally, we perform\nqualitative assessments on the metrics learned by CAMEL and show that they\nidentify and clearly articulate important factors in how the model performs\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 20:54:56 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Heim", "Eric", "", "University of Pittsburgh"], ["Hauskrecht", "Milos", "", "University of Pittsburgh"]]}, {"id": "1507.07974", "submitter": "Shuchin Aeron", "authors": "John Pothier, Josh Girson, Shuchin Aeron", "title": "An algorithm for online tensor prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for online prediction and learning of tensors\n($N$-way arrays, $N >2$) from sequential measurements. We focus on the specific\ncase of 3-D tensors and exploit a recently developed framework of structured\ntensor decompositions proposed in [1]. In this framework it is possible to\ntreat 3-D tensors as linear operators and appropriately generalize notions of\nrank and positive definiteness to tensors in a natural way. Using these notions\nwe propose a generalization of the matrix exponentiated gradient descent\nalgorithm [2] to a tensor exponentiated gradient descent algorithm using an\nextension of the notion of von-Neumann divergence to tensors. Then following a\nsimilar construction as in [3], we exploit this algorithm to propose an online\nalgorithm for learning and prediction of tensors with provable regret\nguarantees. Simulations results are presented on semi-synthetic data sets of\nratings evolving in time under local influence over a social network. The\nresult indicate superior performance compared to other (online) convex tensor\ncompletion methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 22:09:36 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Pothier", "John", ""], ["Girson", "Josh", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1507.07984", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A., H.L. Prasad, Shalabh Bhatnagar and Prakash Chandra", "title": "A constrained optimization perspective on actor critic algorithms and\n  application to network routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel actor-critic algorithm with guaranteed convergence to an\noptimal policy for a discounted reward Markov decision process. The actor\nincorporates a descent direction that is motivated by the solution of a certain\nnon-linear optimization problem. We also discuss an extension to incorporate\nfunction approximation and demonstrate the practicality of our algorithms on a\nnetwork routing application.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 23:25:30 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["A.", "Prashanth L.", ""], ["Prasad", "H. L.", ""], ["Bhatnagar", "Shalabh", ""], ["Chandra", "Prakash", ""]]}, {"id": "1507.07998", "submitter": "Andrew Dai", "authors": "Andrew M. Dai and Christopher Olah and Quoc V. Le", "title": "Document Embedding with Paragraph Vectors", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 01:04:28 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Dai", "Andrew M.", ""], ["Olah", "Christopher", ""], ["Le", "Quoc V.", ""]]}, {"id": "1507.08074", "submitter": "Sergey Novoselov", "authors": "Sergey Novoselov, Alexandr Kozlov, Galina Lavrentyeva, Konstantin\n  Simonchik, Vadim Shchemelinin", "title": "STC Anti-spoofing Systems for the ASVspoof 2015 Challenge", "comments": "5 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Speech Technology Center (STC) systems submitted to\nAutomatic Speaker Verification Spoofing and Countermeasures (ASVspoof)\nChallenge 2015. In this work we investigate different acoustic feature spaces\nto determine reliable and robust countermeasures against spoofing attacks. In\naddition to the commonly used front-end MFCC features we explored features\nderived from phase spectrum and features based on applying the multiresolution\nwavelet transform. Similar to state-of-the-art ASV systems, we used the\nstandard TV-JFA approach for probability modelling in spoofing detection\nsystems. Experiments performed on the development and evaluation datasets of\nthe Challenge demonstrate that the use of phase-related and wavelet-based\nfeatures provides a substantial input into the efficiency of the resulting STC\nsystems. In our research we also focused on the comparison of the linear (SVM)\nand nonlinear (DBN) classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:22:58 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Novoselov", "Sergey", ""], ["Kozlov", "Alexandr", ""], ["Lavrentyeva", "Galina", ""], ["Simonchik", "Konstantin", ""], ["Shchemelinin", "Vadim", ""]]}, {"id": "1507.08104", "submitter": "Brian McWilliams", "authors": "Barbora Micenkov\\'a, Brian McWilliams, Ira Assent", "title": "Learning Representations for Outlier Detection on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting a small number of outliers in a large dataset is an\nimportant task in many fields from fraud detection to high-energy physics. Two\napproaches have emerged to tackle this problem: unsupervised and supervised.\nSupervised approaches require a sufficient amount of labeled data and are\nchallenged by novel types of outliers and inherent class imbalance, whereas\nunsupervised methods do not take advantage of available labeled training\nexamples and often exhibit poorer predictive performance. We propose BORE (a\nBagged Outlier Representation Ensemble) which uses unsupervised outlier scoring\nfunctions (OSFs) as features in a supervised learning framework. BORE is able\nto adapt to arbitrary OSF feature representations, to the imbalance in labeled\ndata as well as to prediction-time constraints on computational cost. We\ndemonstrate the good performance of BORE compared to a variety of competing\nmethods in the non-budgeted and the budgeted outlier detection problem on 12\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 11:28:41 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Micenkov\u00e1", "Barbora", ""], ["McWilliams", "Brian", ""], ["Assent", "Ira", ""]]}, {"id": "1507.08155", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family", "comments": "13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize\n  the In-Tree structure by Dendrogram", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously, we proposed a physically-inspired method to construct data points\ninto an effective in-tree (IT) structure, in which the underlying cluster\nstructure in the dataset is well revealed. Although there are some edges in the\nIT structure requiring to be removed, such undesired edges are generally\ndistinguishable from other edges and thus are easy to be determined. For\ninstance, when the IT structures for the 2-dimensional (2D) datasets are\ngraphically presented, those undesired edges can be easily spotted and\ninteractively determined. However, in practice, there are many datasets that do\nnot lie in the 2D Euclidean space, thus their IT structures cannot be\ngraphically presented. But if we can effectively map those IT structures into a\nvisualized space in which the salient features of those undesired edges are\npreserved, then the undesired edges in the IT structures can still be visually\ndetermined in a visualization environment. Previously, this purpose was reached\nby our method called IT-map. The outstanding advantage of IT-map is that\nclusters can still be found even with the so-called crowding problem in the\nembedding.\n  In this paper, we propose another method, called IT-Dendrogram, to achieve\nthe same goal through an effective combination of the IT structure and the\nsingle link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram\ncan also effectively represent the IT structures in a visualization\nenvironment, whereas using another form, called the Dendrogram. IT-Dendrogram\ncan serve as another visualization method to determine the undesired edges in\nthe IT structures and thus benefit the IT-based clustering analysis. This was\ndemonstrated on several datasets with different shapes, dimensions, and\nattributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,\nwhich could help users make more reliable cluster analysis in certain problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:22:13 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1507.08240", "submitter": "Yajie Miao", "authors": "Yajie Miao, Mohammad Gowayyed, Florian Metze", "title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and\n  WFST-based Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of automatic speech recognition (ASR) has improved\ntremendously due to the application of deep neural networks (DNNs). Despite\nthis progress, building a new ASR system remains a challenging task, requiring\nvarious resources, multiple training stages and significant expertise. This\npaper presents our Eesen framework which drastically simplifies the existing\npipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen\ninvolves learning a single recurrent neural network (RNN) predicting\ncontext-independent targets (phonemes or characters). To remove the need for\npre-generated frame labels, we adopt the connectionist temporal classification\n(CTC) objective function to infer the alignments between speech and label\nsequences. A distinctive feature of Eesen is a generalized decoding approach\nbased on weighted finite-state transducers (WFSTs), which enables the efficient\nincorporation of lexicons and language models into CTC decoding. Experiments\nshow that compared with the standard hybrid DNN systems, Eesen achieves\ncomparable word error rates (WERs), while at the same time speeding up decoding\nsignificantly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 17:53:50 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 21:03:34 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 20:35:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Miao", "Yajie", ""], ["Gowayyed", "Mohammad", ""], ["Metze", "Florian", ""]]}, {"id": "1507.08271", "submitter": "Guy Lever Dr", "authors": "Thomas Furmston and Guy Lever", "title": "A Gauss-Newton Method for Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Newton methods are a standard optimization tool which aim to\nmaintain the benefits of Newton's method, such as a fast rate of convergence,\nwhilst alleviating its drawbacks, such as computationally expensive calculation\nor estimation of the inverse Hessian. In this work we investigate approximate\nNewton methods for policy optimization in Markov Decision Processes (MDPs). We\nfirst analyse the structure of the Hessian of the objective function for MDPs.\nWe show that, like the gradient, the Hessian exhibits useful structure in the\ncontext of MDPs and we use this analysis to motivate two Gauss-Newton Methods\nfor MDPs. Like the Gauss-Newton method for non-linear least squares, these\nmethods involve approximating the Hessian by ignoring certain terms in the\nHessian which are difficult to estimate. The approximate Hessians possess\ndesirable properties, such as negative definiteness, and we demonstrate several\nimportant performance guarantees including guaranteed ascent directions,\ninvariance to affine transformation of the parameter space, and convergence\nguarantees. We finally provide a unifying perspective of key policy search\nalgorithms, demonstrating that our second Gauss-Newton algorithm is closely\nrelated to both the EM-algorithm and natural gradient ascent applied to MDPs,\nbut performs significantly better in practice on a range of challenging\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 19:37:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 19:08:37 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2015 17:33:39 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2015 14:02:01 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Furmston", "Thomas", ""], ["Lever", "Guy", ""]]}, {"id": "1507.08286", "submitter": "David Held", "authors": "David Held, Sebastian Thrun, Silvio Savarese", "title": "Deep Learning for Single-View Instance Recognition", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have typically been trained on large datasets in which\nmany training examples are available. However, many real-world product datasets\nhave only a small number of images available for each product. We explore the\nuse of deep learning methods for recognizing object instances when we have only\na single training example per class. We show that feedforward neural networks\noutperform state-of-the-art methods for recognizing objects from novel\nviewpoints even when trained from just a single image per object. To further\nimprove our performance on this task, we propose to take advantage of a\nsupplementary dataset in which we observe a separate set of objects from\nmultiple viewpoints. We introduce a new approach for training deep learning\nmethods for instance recognition with limited training data, in which we use an\nauxiliary multi-view dataset to train our network to be robust to viewpoint\nchanges. We find that this approach leads to a more robust classifier for\nrecognizing objects from novel viewpoints, outperforming previous\nstate-of-the-art approaches including keypoint-matching, template-based\ntechniques, and sparse coding.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 20:11:12 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Held", "David", ""], ["Thrun", "Sebastian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1507.08322", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Martin Tak\\'a\\v{c} and Peter Richt\\'arik and Nathan Srebro", "title": "Distributed Mini-Batch SDCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved analysis of mini-batched stochastic dual coordinate\nascent for regularized empirical loss minimization (i.e. SVM and SVM-type\nobjectives). Our analysis allows for flexible sampling schemes, including where\ndata is distribute across machines, and combines a dependence on the smoothness\nof the loss and/or the data spread (measured through the spectral norm).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 21:15:31 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Tak\u00e1\u010d", "Martin", ""], ["Richt\u00e1rik", "Peter", ""], ["Srebro", "Nathan", ""]]}, {"id": "1507.08379", "submitter": "Dong Wang", "authors": "Mian Wang, Dong Wang", "title": "VMF-SNE: Embedding for Spherical Data", "comments": "5 pages", "journal-ref": null, "doi": "10.1007/s11040-015-9171-z", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  T-SNE is a well-known approach to embedding high-dimensional data and has\nbeen widely used in data visualization. The basic assumption of t-SNE is that\nthe data are non-constrained in the Euclidean space and the local proximity can\nbe modelled by Gaussian distributions. This assumption does not hold for a wide\nrange of data types in practical applications, for instance spherical data for\nwhich the local proximity is better modelled by the von Mises-Fisher (vMF)\ndistribution instead of the Gaussian. This paper presents a vMF-SNE embedding\nalgorithm to embed spherical data. An iterative process is derived to produce\nan efficient embedding. The results on a simulation data set demonstrated that\nvMF-SNE produces better embeddings than t-SNE for spherical data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 05:09:03 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Wang", "Mian", ""], ["Wang", "Dong", ""]]}, {"id": "1507.08396", "submitter": "Shuangyin Li", "authors": "Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan", "title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 06:44:37 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Li", "Shuangyin", ""], ["Li", "Jiefei", ""], ["Huang", "Guan", ""], ["Tan", "Ruiyang", ""], ["Pan", "Rong", ""]]}, {"id": "1507.08482", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko, Jacob M. Taylor and Hans J. Briegel", "title": "Framework for learning agents in quantum environments", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a broad framework for describing learning agents in\ngeneral quantum environments. We analyze the types of classically specified\nenvironments which allow for quantum enhancements in learning, by contrasting\nenvironments to quantum oracles. We show that whether or not quantum\nimprovements are at all possible depends on the internal structure of the\nquantum environment. If the environments are constructed and the internal\nstructure is appropriately chosen, or if the agent has limited capacities to\ninfluence the internal states of the environment, we show that improvements in\nlearning times are possible in a broad range of scenarios. Such scenarios we\ncall luck-favoring settings. The case of constructed environments is\nparticularly relevant for the class of model-based learning agents, where our\nresults imply a near-generic improvement.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 12:58:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Dunjko", "Vedran", ""], ["Taylor", "Jacob M.", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1507.08726", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Robustness in sparse linear models: relative efficiency based on robust\n  approximate message passing", "comments": "49 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3894-3944", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding efficiency in high dimensional linear models is a longstanding\nproblem of interest. Classical work with smaller dimensional problems dating\nback to Huber and Bickel has illustrated the benefits of efficient loss\nfunctions. When the number of parameters $p$ is of the same order as the sample\nsize $n$, $p \\approx n$, an efficiency pattern different from the one of Huber\nwas recently established. In this work, we consider the effects of model\nselection on the estimation efficiency of penalized methods. In particular, we\nexplore whether sparsity, results in new efficiency patterns when $p > n$. In\nthe interest of deriving the asymptotic mean squared error for regularized\nM-estimators, we use the powerful framework of approximate message passing. We\npropose a novel, robust and sparse approximate message passing algorithm\n(RAMP), that is adaptive to the error distribution. Our algorithm includes many\nnon-quadratic and non-differentiable loss functions. We derive its asymptotic\nmean squared error and show its convergence, while allowing $p, n, s \\to\n\\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new\npatterns of relative efficiency regarding a number of penalized $M$ estimators,\nwhen $p$ is much larger than $n$. We show that the classical information bound\nis no longer reachable, even for light--tailed error distributions. We show\nthat the penalized least absolute deviation estimator dominates the penalized\nleast square estimator, in cases of heavy--tailed distributions. We observe\nthis pattern for all choices of the number of non-zero parameters $s$, both $s\n\\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$,\nthe opposite regime holds. Therefore, we discover that the presence of model\nselection significantly changes the efficiency patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:31:24 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 00:27:46 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1507.08750", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "comments": "Published at NIPS 2015 (Advances in Neural Information Processing\n  Systems 28)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision-based reinforcement learning (RL) problems, in particular\nAtari games from the recent benchmark Aracade Learning Environment (ALE), we\nconsider spatio-temporal prediction problems where future (image-)frames are\ndependent on control variables or actions as well as previous frames. While not\ncomposed of natural scenes, frames in Atari games are high-dimensional in size,\ncan involve tens of objects with one or more objects being controlled by the\nactions directly and many other objects being influenced indirectly, can\ninvolve entry and departure of objects, and can involve deep partial\nobservability. We propose and evaluate two deep neural network architectures\nthat consist of encoding, action-conditional transformation, and decoding\nlayers based on convolutional neural networks and recurrent neural networks.\nExperimental results show that the proposed architectures are able to generate\nvisually-realistic frames that are also useful for control over approximately\n100-step action-conditional futures in some games. To the best of our\nknowledge, this paper is the first to make and evaluate long-term predictions\non high-dimensional video conditioned by control inputs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:43:30 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:26:54 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Oh", "Junhyuk", ""], ["Guo", "Xiaoxiao", ""], ["Lee", "Honglak", ""], ["Lewis", "Richard", ""], ["Singh", "Satinder", ""]]}, {"id": "1507.08752", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with\n  Two-Point Feedback", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the closely related problems of bandit convex optimization with\ntwo-point feedback, and zero-order stochastic convex optimization with two\nfunction evaluations per round. We provide a simple algorithm and analysis\nwhich is optimal for convex Lipschitz functions. This improves on\n\\cite{dujww13}, which only provides an optimal result for smooth functions;\nMoreover, the algorithm and analysis are simpler, and readily extend to\nnon-Euclidean problems. The algorithm is based on a small but surprisingly\npowerful modification of the gradient estimator.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:48:58 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1507.08788", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and\n  Convexity", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of the VR-PCA algorithm introduced by\n\\cite{shamir2015stochastic} for fast computation of leading singular vectors.\nWe prove several new results, including a formal analysis of a block version of\nthe algorithm, and convergence from random initialization. We also make a few\nobservations of independent interest, such as how pre-initializing with just a\nsingle exact power iteration can significantly improve the runtime of\nstochastic methods, and what are the convexity and non-convexity properties of\nthe underlying optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 07:57:18 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1507.08818", "submitter": "Dario Garcia-Gasulla", "authors": "D. Garcia-Gasulla, J. B\\'ejar, U. Cort\\'es, E. Ayguad\\'e, J. Labarta,\n  T. Suzumura and R. Chen", "title": "A Visual Embedding for the Unsupervised Extraction of Abstract Semantics", "comments": "14 pages, 5 figures, accepted at Cognitive Systems Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-space word representations obtained from neural network models have\nbeen shown to enable semantic operations based on vector arithmetic. In this\npaper, we explore the existence of similar information on vector\nrepresentations of images. For that purpose we define a methodology to obtain\nlarge, sparse vector representations of image classes, and generate vectors\nthrough the state-of-the-art deep learning architecture GoogLeNet for 20K\nimages obtained from ImageNet. We first evaluate the resultant vector-space\nsemantics through its correlation with WordNet distances, and find vector\ndistances to be strongly correlated with linguistic semantics. We then explore\nthe location of images within the vector space, finding elements close in\nWordNet to be clustered together, regardless of significant visual variances\n(e.g. 118 dog types). More surprisingly, we find that the space unsupervisedly\nseparates complex classes without prior knowledge (e.g. living things).\nAfterwards, we consider vector arithmetics. Although we are unable to obtain\nmeaningful results on this regard, we discuss the various problem we\nencountered, and how we consider to solve them. Finally, we discuss the impact\nof our research for cognitive systems, focusing on the role of the architecture\nbeing used.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 10:16:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 17:27:56 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 17:03:54 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 14:37:13 GMT"}, {"version": "v5", "created": "Fri, 25 Nov 2016 09:05:50 GMT"}, {"version": "v6", "created": "Fri, 16 Dec 2016 13:58:59 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Garcia-Gasulla", "D.", ""], ["B\u00e9jar", "J.", ""], ["Cort\u00e9s", "U.", ""], ["Ayguad\u00e9", "E.", ""], ["Labarta", "J.", ""], ["Suzumura", "T.", ""], ["Chen", "R.", ""]]}, {"id": "1507.08847", "submitter": "Fei  Guo", "authors": "Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb", "title": "A novel multivariate performance optimization method based on sparse\n  coding and hyper-predictor learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.011", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of optimization multivariate\nperformance measures, and propose a novel algorithm for it. Different from\ntraditional machine learning methods which optimize simple loss functions to\nlearn prediction function, the problem studied in this paper is how to learn\neffective hyper-predictor for a tuple of data points, so that a complex loss\nfunction corresponding to a multivariate performance measure can be minimized.\nWe propose to present the tuple of data points to a tuple of sparse codes via a\ndictionary, and then apply a linear function to compare a sparse code against a\ngive candidate class label. To learn the dictionary, sparse codes, and\nparameter of the linear function, we propose a joint optimization problem. In\nthis problem, the both the reconstruction error and sparsity of sparse code,\nand the upper bound of the complex loss function are minimized. Moreover, the\nupper bound of the loss function is approximated by the sparse codes and the\nlinear function parameter. To optimize this problem, we develop an iterative\nalgorithm based on descent gradient methods to learn the sparse codes and\nhyper-predictor parameter alternately. Experiment results on some benchmark\ndata sets show the advantage of the proposed methods over other\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 12:14:35 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Yanga", "Jiachen", ""], ["Dinga", "Zhiyong", ""], ["Guoa", "Fei", ""], ["Wanga", "Huogen", ""], ["Hughesb", "Nick", ""]]}]