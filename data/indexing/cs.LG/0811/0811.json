[{"id": "0811.0139", "submitter": "Stefan Jaeger", "authors": "Stefan Jaeger", "title": "Entropy, Perception, and Relativity", "comments": null, "journal-ref": null, "doi": null, "report-no": "LAMP-TR-131/CAR-TR-1012/CS-TR-4799/UMIACS-TR-2006-20", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I expand Shannon's definition of entropy into a new form of\nentropy that allows integration of information from different random events.\nShannon's notion of entropy is a special case of my more general definition of\nentropy. I define probability using a so-called performance function, which is\nde facto an exponential distribution. Assuming that my general notion of\nentropy reflects the true uncertainty about a probabilistic event, I understand\nthat our perceived uncertainty differs. I claim that our perception is the\nresult of two opposing forces similar to the two famous antagonists in Chinese\nphilosophy: Yin and Yang. Based on this idea, I show that our perceived\nuncertainty matches the true uncertainty in points determined by the golden\nratio. I demonstrate that the well-known sigmoid function, which we typically\nemploy in artificial neural networks as a non-linear threshold function,\ndescribes the actual performance. Furthermore, I provide a motivation for the\ntime dilation in Einstein's Special Relativity, basically claiming that\nalthough time dilation conforms with our perception, it does not correspond to\nreality. At the end of the paper, I show how to apply this theoretical\nframework to practical applications. I present recognition rates for a pattern\nrecognition problem, and also propose a network architecture that can take\nadvantage of general entropy to solve complex decision problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 08:02:43 GMT"}], "update_date": "2008-11-04", "authors_parsed": [["Jaeger", "Stefan", ""]]}, {"id": "0811.0146", "submitter": "Alain Lifchitz", "authors": "Alain Lifchitz (LIP6), Sandra Jhean-Larose (LPC), Guy Denhi\\`ere (LPC)", "title": "Effect of Tuned Parameters on a LSA MCQ Answering Model", "comments": "9 pages", "journal-ref": "Behavior Research Methods, 41 (4), p. 1201-1209, November 2009", "doi": "10.3758/BRM.41.4.1201", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the current state of a work in progress, whose objective\nis to better understand the effects of factors that significantly influence the\nperformance of Latent Semantic Analysis (LSA). A difficult task, which consists\nin answering (French) biology Multiple Choice Questions, is used to test the\nsemantic properties of the truncated singular space and to study the relative\ninfluence of main parameters. A dedicated software has been designed to fine\ntune the LSA semantic space for the Multiple Choice Questions task. With\noptimal parameters, the performances of our simple model are quite surprisingly\nequal or superior to those of 7th and 8th grades students. This indicates that\nsemantic spaces were quite good despite their low dimensions and the small\nsizes of training data sets. Besides, we present an original entropy global\nweighting of answers' terms of each question of the Multiple Choice Questions\nwhich was necessary to achieve the model's success.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 09:21:40 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2008 19:54:50 GMT"}, {"version": "v3", "created": "Thu, 14 May 2009 12:51:44 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Lifchitz", "Alain", "", "LIP6"], ["Jhean-Larose", "Sandra", "", "LPC"], ["Denhi\u00e8re", "Guy", "", "LPC"]]}, {"id": "0811.1250", "submitter": "Ping Li", "authors": "Ping Li", "title": "Adaptive Base Class Boost for Multi-class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the concept of ABC-Boost (Adaptive Base Class Boost) for\nmulti-class classification and present ABC-MART, a concrete implementation of\nABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has\nbeen very successful in large-scale applications. For binary classification,\nABC-MART recovers MART. For multi-class classification, ABC-MART considerably\nimproves MART, as evaluated on several public data sets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2008 23:23:08 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0811.1629", "submitter": "Afshin Rostamizadeh", "authors": "Mehryar Mohri and Afshin Rostamizadeh", "title": "Stability Bound for Stationary Phi-mixing and Beta-mixing Processes", "comments": "23 pages, 1 figure, submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generalization bounds in learning theory are based on some measure of\nthe complexity of the hypothesis class used, independently of any algorithm. In\ncontrast, the notion of algorithmic stability can be used to derive tight\ngeneralization bounds that are tailored to specific learning algorithms by\nexploiting their particular properties. However, as in much of learning theory,\nexisting stability analyses and bounds apply only in the scenario where the\nsamples are independently and identically distributed. In many machine learning\napplications, however, this assumption does not hold. The observations received\nby the learning algorithm often have some inherent temporal dependence.\n  This paper studies the scenario where the observations are drawn from a\nstationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\nthe study of non-i.i.d. processes that implies a dependence between\nobservations weakening over time. We prove novel and distinct stability-based\ngeneralization bounds for stationary phi-mixing and beta-mixing sequences.\nThese bounds strictly generalize the bounds given in the i.i.d. case and apply\nto all stable learning algorithms, thereby extending the use of\nstability-bounds to non-i.i.d. scenarios.\n  We also illustrate the application of our phi-mixing generalization bounds to\ngeneral classes of learning algorithms, including Support Vector Regression,\nKernel Ridge Regression, and Support Vector Machines, and many other kernel\nregularization-based and relative entropy-based regularization algorithms.\nThese novel bounds can thus be viewed as the first theoretical basis for the\nuse of these algorithms in non-i.i.d. scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 05:09:08 GMT"}], "update_date": "2008-11-12", "authors_parsed": [["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "0811.1790", "submitter": "Huan Xu Mr.", "authors": "Huan Xu, Constantine Caramanis and Shie Mannor", "title": "Robust Regression and Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lasso, or $\\ell^1$ regularized least squares, has been explored extensively\nfor its remarkable sparsity properties. It is shown in this paper that the\nsolution to Lasso, in addition to its sparsity, has robustness properties: it\nis the solution to a robust optimization problem. This has two important\nconsequences. First, robustness provides a connection of the regularizer to a\nphysical property, namely, protection from noise. This allows a principled\nselection of the regularizer, and in particular, generalizations of Lasso that\nalso yield convex optimization problems are obtained by considering different\nuncertainty sets.\n  Secondly, robustness can itself be used as an avenue to exploring different\nproperties of the solution. In particular, it is shown that robustness of the\nsolution explains why the solution is sparse. The analysis as well as the\nspecific results obtained differ from standard sparsity results, providing\ndifferent geometric intuition. Furthermore, it is shown that the robust\noptimization formulation is related to kernel density estimation, and based on\nthis approach, a proof that Lasso is consistent is given using robustness\ndirectly. Finally, a theorem saying that sparsity and algorithmic stability\ncontradict each other, and hence Lasso is not stable, is presented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 22:46:10 GMT"}], "update_date": "2008-11-13", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""]]}, {"id": "0811.2016", "submitter": "Tshilidzi Marwala", "authors": "A. Gidudu, B. Abe and T. Marwala", "title": "Land Cover Mapping Using Ensemble Feature Selection Methods", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble classification is an emerging approach to land cover mapping whereby\nthe final classification output is a result of a consensus of classifiers.\nIntuitively, an ensemble system should consist of base classifiers which are\ndiverse i.e. classifiers whose decision boundaries err differently. In this\npaper ensemble feature selection is used to impose diversity in ensembles. The\nfeatures of the constituent base classifiers for each ensemble were created\nthrough an exhaustive search algorithm using different separability indices.\nFor each ensemble, the classification accuracy was derived as well as a\ndiversity measure purported to give a measure of the inensemble diversity. The\ncorrelation between ensemble classification accuracy and diversity measure was\ndetermined to establish the interplay between the two variables. From the\nfindings of this paper, diversity measures as currently formulated do not\nprovide an adequate means upon which to constitute ensembles for land cover\nmapping.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 01:23:47 GMT"}], "update_date": "2008-11-14", "authors_parsed": [["Gidudu", "A.", ""], ["Abe", "B.", ""], ["Marwala", "T.", ""]]}, {"id": "0811.4413", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade, Tong Zhang", "title": "A Spectral Algorithm for Learning Hidden Markov Models", "comments": "Published in JCSS Special Issue \"Learning Theory 2009\"", "journal-ref": "Journal of Computer and System Sciences, 78(5):1460-1480, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) are one of the most fundamental and widely used\nstatistical tools for modeling discrete time series. In general, learning HMMs\nfrom data is computationally hard (under cryptographic assumptions), and\npractitioners typically resort to search heuristics which suffer from the usual\nlocal optima issues. We prove that under a natural separation condition (bounds\non the smallest singular value of the HMM parameters), there is an efficient\nand provably correct algorithm for learning HMMs. The sample complexity of the\nalgorithm does not explicitly depend on the number of distinct (discrete)\nobservations---it implicitly depends on this quantity through spectral\nproperties of the underlying HMM. This makes the algorithm particularly\napplicable to settings with a large number of observations, such as those in\nnatural language processing where the space of observation is sometimes the\nwords in a language. The algorithm is also simple, employing only a singular\nvalue decomposition and matrix multiplications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 20:22:51 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2008 06:17:59 GMT"}, {"version": "v3", "created": "Wed, 11 Feb 2009 05:47:49 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2009 16:15:49 GMT"}, {"version": "v5", "created": "Wed, 19 Aug 2009 23:43:59 GMT"}, {"version": "v6", "created": "Fri, 6 Jul 2012 23:29:02 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "0811.4458", "submitter": "Oliver Schulte", "authors": "Oliver Schulte, Hassan Khosravi, Flavia Moser, Martin Ester", "title": "Learning Class-Level Bayes Nets for Relational Data", "comments": "14 pages (2 column)", "journal-ref": null, "doi": null, "report-no": "TR 2008-17, School of Computing Science, Simon Fraser University", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many databases store data in relational format, with different types of\nentities and information about links between the entities. The field of\nstatistical-relational learning (SRL) has developed a number of new statistical\nmodels for such data. In this paper we focus on learning class-level or\nfirst-order dependencies, which model the general database statistics over\nattributes of linked objects and links (e.g., the percentage of A grades given\nin computer science classes). Class-level statistical relationships are\nimportant in themselves, and they support applications like policy making,\nstrategic planning, and query optimization. Most current SRL methods find\nclass-level dependencies, but their main task is to support instance-level\npredictions about the attributes or links of specific entities. We focus only\non class-level prediction, and describe algorithms for learning class-level\nmodels that are orders of magnitude faster for this task. Our algorithms learn\nBayes nets with relational structure, leveraging the efficiency of single-table\nnonrelational Bayes net learners. An evaluation of our methods on three data\nsets shows that they are computationally feasible for realistic table sizes,\nand that the learned structures represent the statistical information in the\ndatabases well. After learning compiles the database statistics into a Bayes\nnet, querying these statistics via Bayes net inference is faster than with SQL\nqueries, and does not depend on the size of the database.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2008 01:02:33 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2009 18:58:20 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Schulte", "Oliver", ""], ["Khosravi", "Hassan", ""], ["Moser", "Flavia", ""], ["Ester", "Martin", ""]]}]