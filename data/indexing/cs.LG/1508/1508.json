[{"id": "1508.00021", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, \\'Etienne Simon, Alex Auvolat, Pascal\n  Vincent, Yoshua Bengio", "title": "Artificial Neural Networks Applied to Taxi Destination Prediction", "comments": "ECML/PKDD discovery challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our first-place solution to the ECML/PKDD discovery challenge on\ntaxi destination prediction. The task consisted in predicting the destination\nof a taxi based on the beginning of its trajectory, represented as a\nvariable-length sequence of GPS points, and diverse associated\nmeta-information, such as the departure time, the driver id and client\ninformation. Contrary to most published competitor approaches, we used an\nalmost fully automated approach based on neural networks and we ranked first\nout of 381 teams. The architectures we tried use multi-layer perceptrons,\nbidirectional recurrent neural networks and models inspired from recently\nintroduced memory networks. Our approach could easily be adapted to other\napplications in which the goal is to predict a fixed-length output from a\nvariable-length sequence.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 20:24:20 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 15:09:35 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Simon", "\u00c9tienne", ""], ["Auvolat", "Alex", ""], ["Vincent", "Pascal", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.00088", "submitter": "Shashaank Sivakumar", "authors": "D.S. Shashaank, V. Sruthi, M.L.S Vijayalakshimi and Jacob Shomona\n  Garcia", "title": "Turnover Prediction Of Shares using Data Mining techniques : A Case\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the turnover of a company in the ever fluctuating Stock market has\nalways proved to be a precarious situation and most certainly a difficult task\nin hand. Data mining is a well-known sphere of Computer Science that aims on\nextracting meaningful information from large databases. However, despite the\nexistence of many algorithms for the purpose of predicting the future trends,\ntheir efficiency is questionable as their predictions suffer from a high error\nrate. The objective of this paper is to investigate various classification\nalgorithms to predict the turnover of different companies based on the Stock\nprice. The authorized dataset for predicting the turnover was taken from\nwww.bsc.com and included the stock market values of various companies over the\npast 10 years. The algorithms were investigated using the \"R\" tool. The feature\nselection algorithm, Boruta, was run on this dataset to extract the important\nand influential features for classification. With these extracted features, the\nTotal Turnover of the company was predicted using various classification\nalgorithms like Random Forest, Decision Tree, SVM and Multinomial Regression.\nThis prediction mechanism was implemented to predict the turnover of a company\non an everyday basis and hence could help navigate through dubious stock market\ntrades. An accuracy rate of 95% was achieved by the above prediction process.\nMoreover, the importance of stock market attributes was established as well.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 06:50:01 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Shashaank", "D. S.", ""], ["Sruthi", "V.", ""], ["Vijayalakshimi", "M. L. S", ""], ["Garcia", "Jacob Shomona", ""]]}, {"id": "1508.00181", "submitter": "Hamed Yaghoubi Shahir", "authors": "Hamed Yaghoubi Shahir, Uwe Gl\\\"asser, Amir Yaghoubi Shahir, Hans Wehn", "title": "An Analytic Framework for Maritime Situation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maritime domain awareness is critical for protecting sea lanes, ports,\nharbors, offshore structures and critical infrastructures against common\nthreats and illegal activities. Limited surveillance resources constrain\nmaritime domain awareness and compromise full security coverage at all times.\nThis situation calls for innovative intelligent systems for interactive\nsituation analysis to assist marine authorities and security personal in their\nroutine surveillance operations. In this article, we propose a novel situation\nanalysis framework to analyze marine traffic data and differentiate various\nscenarios of vessel engagement for the purpose of detecting anomalies of\ninterest for marine vessels that operate over some period of time in relative\nproximity to each other. The proposed framework views vessel behavior as\nprobabilistic processes and uses machine learning to model common vessel\ninteraction patterns. We represent patterns of interest as left-to-right Hidden\nMarkov Models and classify such patterns using Support Vector Machines.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 01:36:37 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Shahir", "Hamed Yaghoubi", ""], ["Gl\u00e4sser", "Uwe", ""], ["Shahir", "Amir Yaghoubi", ""], ["Wehn", "Hans", ""]]}, {"id": "1508.00200", "submitter": "Jian Tang", "authors": "Jian Tang, Meng Qu, Qiaozhu Mei", "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks", "comments": "KDD 2015", "journal-ref": null, "doi": "10.1145/2783258.2783307", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 06:18:10 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Tang", "Jian", ""], ["Qu", "Meng", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1508.00230", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Shaowei Lin, Hwee-Pink Tan, and Dusit Niyato", "title": "Toward a Robust Sparse Data Representation for Wireless Sensor Networks", "comments": "8 pages", "journal-ref": "IEEE 40th Conference on Local Computer Networks (LCN), Clearwater\n  Beach, FL, 2015, pp. 117-124", "doi": "10.1109/LCN.2015.7366290", "report-no": null, "categories": "cs.NI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing has been successfully used for optimized operations in\nwireless sensor networks. However, raw data collected by sensors may be neither\noriginally sparse nor easily transformed into a sparse data representation.\nThis paper addresses the problem of transforming source data collected by\nsensor nodes into a sparse representation with a few nonzero elements. Our\ncontributions that address three major issues include: 1) an effective method\nthat extracts population sparsity of the data, 2) a sparsity ratio guarantee\nscheme, and 3) a customized learning algorithm of the sparsifying dictionary.\nWe introduce an unsupervised neural network to extract an intrinsic sparse\ncoding of the data. The sparse codes are generated at the activation of the\nhidden layer using a sparsity nomination constraint and a shrinking mechanism.\nOur analysis using real data samples shows that the proposed method outperforms\nconventional sparsity-inducing methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 13:12:50 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Lin", "Shaowei", ""], ["Tan", "Hwee-Pink", ""], ["Niyato", "Dusit", ""]]}, {"id": "1508.00285", "submitter": "Zhenhua Zou", "authors": "Zhenhua Zou and Anders Gidmark and Themistoklis Charalambous and\n  Mikael Johansson", "title": "Optimal Radio Frequency Energy Harvesting with Limited Energy Arrival\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop optimal policies for deciding when a wireless node\nwith radio frequency (RF) energy harvesting (EH) capabilities should try and\nharvest ambient RF energy. While the idea of RF-EH is appealing, it is not\nalways beneficial to attempt to harvest energy; in environments where the\nambient energy is low, nodes could consume more energy being awake with their\nharvesting circuits turned on than what they can extract from the ambient radio\nsignals; it is then better to enter a sleep mode until the ambient RF energy\nincreases. Towards this end, we consider a scenario with intermittent energy\narrivals and a wireless node that wakes up for a period of time (herein called\nthe time-slot) and harvests energy. If enough energy is harvested during the\ntime-slot, then the harvesting is successful and excess energy is stored;\nhowever, if there does not exist enough energy the harvesting is unsuccessful\nand energy is lost.\n  We assume that the ambient energy level is constant during the time-slot, and\nchanges at slot boundaries. The energy level dynamics are described by a\ntwo-state Gilbert-Elliott Markov chain model, where the state of the Markov\nchain can only be observed during the harvesting action, and not when in sleep\nmode. Two scenarios are studied under this model. In the first scenario, we\nassume that we have knowledge of the transition probabilities of the Markov\nchain and formulate the problem as a Partially Observable Markov Decision\nProcess (POMDP), where we find a threshold-based optimal policy. In the second\nscenario, we assume that we don't have any knowledge about these parameters and\nformulate the problem as a Bayesian adaptive POMDP; to reduce the complexity of\nthe computations we also propose a heuristic posterior sampling algorithm. The\nperformance of our approaches is demonstrated via numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 21:00:52 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Zou", "Zhenhua", ""], ["Gidmark", "Anders", ""], ["Charalambous", "Themistoklis", ""], ["Johansson", "Mikael", ""]]}, {"id": "1508.00317", "submitter": "Roni Mittelman Roni Mittelman", "authors": "Roni Mittelman", "title": "Time-series modeling with undecimated fully convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new convolutional neural network-based time-series model.\nTypical convolutional neural network (CNN) architectures rely on the use of\nmax-pooling operators in between layers, which leads to reduced resolution at\nthe top layers. Instead, in this work we consider a fully convolutional network\n(FCN) architecture that uses causal filtering operations, and allows for the\nrate of the output signal to be the same as that of the input signal. We\nfurthermore propose an undecimated version of the FCN, which we refer to as the\nundecimated fully convolutional neural network (UFCNN), and is motivated by the\nundecimated wavelet transform. Our experimental results verify that using the\nundecimated version of the FCN is necessary in order to allow for effective\ntime-series modeling. The UFCNN has several advantages compared to other\ntime-series models such as the recurrent neural network (RNN) and long\nshort-term memory (LSTM), since it does not suffer from either the vanishing or\nexploding gradients problems, and is therefore easier to train. Convolution\noperations can also be implemented more efficiently compared to the recursion\nthat is involved in RNN-based models. We evaluate the performance of our model\nin a synthetic target tracking task using bearing only measurements generated\nfrom a state-space model, a probabilistic modeling of polyphonic music\nsequences problem, and a high frequency trading task using a time-series of\nask/bid quotes and their corresponding volumes. Our experimental results using\nsynthetic and real datasets verify the significant advantages of the UFCNN\ncompared to the RNN and LSTM baselines.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 05:58:52 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Mittelman", "Roni", ""]]}, {"id": "1508.00330", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Gustavo Carneiro", "title": "On the Importance of Normalisation Layers in Deep Learning with\n  Piecewise Linear Activation Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feedforward neural networks with piecewise linear activations are\ncurrently producing the state-of-the-art results in several public datasets.\nThe combination of deep learning models and piecewise linear activation\nfunctions allows for the estimation of exponentially complex functions with the\nuse of a large number of subnetworks specialized in the classification of\nsimilar input examples. During the training process, these subnetworks avoid\noverfitting with an implicit regularization scheme based on the fact that they\nmust share their parameters with other subnetworks. Using this framework, we\nhave made an empirical observation that can improve even more the performance\nof such models. We notice that these models assume a balanced initial\ndistribution of data points with respect to the domain of the piecewise linear\nactivation function. If that assumption is violated, then the piecewise linear\nactivation units can degenerate into purely linear activation units, which can\nresult in a significant reduction of their capacity to learn complex functions.\nFurthermore, as the number of model layers increases, this unbalanced initial\ndistribution makes the model ill-conditioned. Therefore, we propose the\nintroduction of batch normalisation units into deep feedforward neural networks\nwith piecewise linear activations, which drives a more balanced use of these\nactivation units, where each region of the activation function is trained with\na relatively large proportion of training samples. Also, this batch\nnormalisation promotes the pre-conditioning of very deep learning models. We\nshow that by introducing maxout and batch normalisation units to the network in\nnetwork model results in a model that produces classification results that are\nbetter than or comparable to the current state of the art in CIFAR-10,\nCIFAR-100, MNIST, and SVHN datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 07:24:07 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 06:44:10 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Liao", "Zhibin", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1508.00451", "submitter": "Rein Houthooft", "authors": "Rein Houthooft, Filip De Turck", "title": "Integrated Inference and Learning of Neural Factors in Structural\n  Support Vector Machines", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.03.014", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling pattern recognition problems in areas such as computer vision,\nbioinformatics, speech or text recognition is often done best by taking into\naccount task-specific statistical relations between output variables. In\nstructured prediction, this internal structure is used to predict multiple\noutputs simultaneously, leading to more accurate and coherent predictions.\nStructural support vector machines (SSVMs) are nonprobabilistic models that\noptimize a joint input-output function through margin-based learning. Because\nSSVMs generally disregard the interplay between unary and interaction factors\nduring the training phase, final parameters are suboptimal. Moreover, its\nfactors are often restricted to linear combinations of input features, limiting\nits generalization power. To improve prediction accuracy, this paper proposes:\n(i) Joint inference and learning by integration of back-propagation and\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\nfactors to neural networks that form highly nonlinear functions of input\nfeatures. Image segmentation benchmark results demonstrate improvements over\nconventional SSVM training methods in terms of accuracy, highlighting the\nfeasibility of end-to-end SSVM training with neural factors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:29:57 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 12:41:52 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 16:02:00 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 22:46:17 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Houthooft", "Rein", ""], ["De Turck", "Filip", ""]]}, {"id": "1508.00506", "submitter": "Tobias Sutter", "authors": "Tobias Sutter, Arnab Ganguly, Heinz Koeppl", "title": "A variational approach to path estimation and parameter inference of\n  hidden diffusion processes", "comments": "37 pages, 2 figures, revised", "journal-ref": "JMLR, volume 17, number 190, year 2016", "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hidden Markov model, where the signal process, given by a\ndiffusion, is only indirectly observed through some noisy measurements. The\narticle develops a variational method for approximating the hidden states of\nthe signal process given the full set of observations. This, in particular,\nleads to systematic approximations of the smoothing densities of the signal\nprocess. The paper then demonstrates how an efficient inference scheme, based\non this variational approach to the approximation of the hidden states, can be\ndesigned to estimate the unknown parameters of stochastic differential\nequations. Two examples at the end illustrate the efficacy and the accuracy of\nthe presented method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:01:13 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 06:39:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 12:11:46 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 09:15:29 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Sutter", "Tobias", ""], ["Ganguly", "Arnab", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1508.00507", "submitter": "Tameem Adel", "authors": "Tameem Adel, Alexander Wong, Daniel Stashuk", "title": "A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic\n  Grouping", "comments": "Submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a spectral graph-theoretic grouping strategy for weakly\nsupervised classification is introduced, where a limited number of labelled\nsamples and a larger set of unlabelled samples are used to construct a larger\nannotated training set composed of strongly labelled and weakly labelled\nsamples. The inherent relationship between the set of strongly labelled samples\nand the set of unlabelled samples is established via spectral grouping, with\nthe unlabelled samples subsequently weakly annotated based on the strongly\nlabelled samples within the associated spectral groups. A number of similarity\ngraph models for spectral grouping, including two new similarity graph models\nintroduced in this study, are explored to investigate their performance in the\ncontext of weakly supervised classification in handling different types of\ndata. Experimental results using benchmark datasets as well as real EMG\ndatasets demonstrate that the proposed approach to weakly supervised\nclassification can provide noticeable improvements in classification\nperformance, and that the proposed similarity graph models can lead to ultimate\nlearning results that are either better than or on a par with existing\nsimilarity graph models in the context of spectral grouping for weakly\nsupervised classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:08:04 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Adel", "Tameem", ""], ["Wong", "Alexander", ""], ["Stashuk", "Daniel", ""]]}, {"id": "1508.00509", "submitter": "Christoph Jahnz", "authors": "Christoph Jahnz", "title": "Maintaining prediction quality under the condition of a growing\n  knowledge space", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence can be understood as an agent's ability to predict its\nenvironment's dynamic by a level of precision which allows it to effectively\nforesee opportunities and threats. Under the assumption that such intelligence\nrelies on a knowledge space any effective reasoning would benefit from a\nmaximum portion of useful and a minimum portion of misleading knowledge\nfragments. It begs the question of how the quality of such knowledge space can\nbe kept high as the amount of knowledge keeps growing. This article proposes a\nmathematical model to describe general principles of how quality of a growing\nknowledge space evolves depending on error rate, error propagation and\ncountermeasures. There is also shown to which extend the quality of a knowledge\nspace collapses as removal of low quality knowledge fragments occurs too slowly\nfor a given knowledge space's growth rate.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:19:41 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 18:50:57 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Jahnz", "Christoph", ""]]}, {"id": "1508.00625", "submitter": "Megasthenis Asteris", "authors": "Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis,\n  Alexandros G. Dimakis", "title": "Sparse PCA via Bipartite Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following multi-component sparse PCA problem: given a set of\ndata points, we seek to extract a small number of sparse components with\ndisjoint supports that jointly capture the maximum possible variance. These\ncomponents can be computed one by one, repeatedly solving the single-component\nproblem and deflating the input data matrix, but as we show this greedy\nprocedure is suboptimal. We present a novel algorithm for sparse PCA that\njointly optimizes multiple disjoint components. The extracted features capture\nvariance that lies within a multiplicative factor arbitrarily close to 1 from\nthe optimal. Our algorithm is combinatorial and computes the desired components\nby solving multiple instances of the bipartite maximum weight matching problem.\nIts complexity grows as a low order polynomial in the ambient dimension of the\ninput data matrix, but exponentially in its rank. However, it can be\neffectively applied on a low-dimensional sketch of the data; this allows us to\nobtain polynomial-time approximation guarantees via spectral bounds. We\nevaluate our algorithm on real data-sets and empirically demonstrate that in\nmany cases it outperforms existing, deflation-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 00:12:35 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1508.00635", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Bayesian mixtures of spatial spline regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work relates the framework of model-based clustering for spatial\nfunctional data where the data are surfaces. We first introduce a Bayesian\nspatial spline regression model with mixed-effects (BSSR) for modeling spatial\nfunction data. The BSSR model is based on Nodal basis functions for spatial\nregression and accommodates both common mean behavior for the data through a\nfixed-effects part, and variability inter-individuals thanks to a\nrandom-effects part. Then, in order to model populations of spatial functional\ndata issued from heterogeneous groups, we integrate the BSSR model into a\nmixture framework. The resulting model is a Bayesian mixture of spatial spline\nregressions with mixed-effects (BMSSR) used for density estimation and\nmodel-based surface clustering. The models, through their Bayesian formulation,\nallow to integrate possible prior knowledge on the data structure and\nconstitute a good alternative to recent mixture of spatial spline regressions\nmodel estimated in a maximum likelihood framework via the\nexpectation-maximization (EM) algorithm. The Bayesian model inference is\nperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs\nsampler to infer the BSSR and the BMSSR models and apply them on simulated\nsurfaces and a real problem of handwritten digit recognition using the MNIST\ndata set. The obtained results highlight the potential benefit of the proposed\nBayesian approaches for modeling surfaces possibly dispersed in particular in\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:29:49 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1508.00641", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Episodic Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of reinforcement learning methods referred to as\n{\\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\\em\nepisodes}, each composed of several {\\em steps}, in which it chooses an action\nand observes a feedback signal. Moreover, in each step, it can take a special\naction, called the $stop$ action, that ends the current episode. After the\n$stop$ action is taken, the learner collects a terminal reward, and observes\nthe costs and terminal rewards associated with each step of the episode. The\ngoal of the learner is to maximize its cumulative gain (i.e., the terminal\nreward minus costs) over all episodes by learning to choose the best sequence\nof actions based on the feedback. First, we define an {\\em oracle} benchmark,\nwhich sequentially selects the actions that maximize the expected immediate\ngain. Then, we propose our online learning algorithm, named {\\em FeedBack\nAdaptive Learning} (FeedBAL), and prove that its regret with respect to the\nbenchmark is bounded with high probability and increases logarithmically in\nexpectation. Moreover, the regret only has polynomial dependence on the number\nof steps, actions and states. eMAB can be used to model applications that\ninvolve humans in the loop, ranging from personalized medical screening to\npersonalized web-based education, where sequences of actions are taken in each\nepisode, and optimal behavior requires adapting the chosen actions based on the\nfeedback.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 01:52:42 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2015 02:53:20 GMT"}, {"version": "v3", "created": "Thu, 4 May 2017 18:16:26 GMT"}, {"version": "v4", "created": "Sun, 11 Mar 2018 20:17:39 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1508.00655", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance\n  based High Dimensional Two Sample Testing", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing is a decision theoretic problem that\ninvolves identifying differences between two random variables without making\nparametric assumptions about their underlying distributions. We refer to the\nmost common settings as mean difference alternatives (MDA), for testing\ndifferences only in first moments, and general difference alternatives (GDA),\nwhich is about testing for any difference in distributions. A large number of\ntest statistics have been proposed for both these settings. This paper connects\nthree classes of statistics - high dimensional variants of Hotelling's t-test,\nstatistics based on Reproducing Kernel Hilbert Spaces, and energy statistics\nbased on pairwise distances. We ask the question: how much statistical power do\npopular kernel and distance based tests for GDA have when the unknown\ndistributions differ in their means, compared to specialized tests for MDA?\n  We formally characterize the power of popular tests for GDA like the Maximum\nMean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent\nvariants of the Energy Distance with the Euclidean norm (eED) in the\nhigh-dimensional MDA regime. Some practically important properties include (a)\neED and gMMD have asymptotically equal power; furthermore they enjoy a free\nlunch because, while they are additionally consistent for GDA, they also have\nthe same power as specialized high-dimensional t-test variants for MDA. All\nthese tests are asymptotically optimal (including matching constants) under MDA\nfor spherical covariances, according to simple lower bounds, (b) The power of\ngMMD is independent of the kernel bandwidth, as long as it is larger than the\nchoice made by the median heuristic, (c) There is a clear and smooth\ncomputation-statistics tradeoff for linear-time, subquadratic-time and\nquadratic-time versions of these tests, with more computation resulting in\nhigher power.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 04:10:05 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1508.00703", "submitter": "Naman Goel", "authors": "Naman Goel, Divyakant Agrawal, Sanjay Chawla, Ahmed Elmagarmid", "title": "Parameter Database : Data-centric Synchronization for Scalable Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "QCRI-TR-2015-003", "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-centric synchronization framework for carrying out of\nmachine learning (ML) tasks in a distributed environment. Our framework\nexploits the iterative nature of ML algorithms and relaxes the application\nagnostic bulk synchronization parallel (BSP) paradigm that has previously been\nused for distributed machine learning. Data-centric synchronization complements\nfunction-centric synchronization based on using stale updates to increase the\nthroughput of distributed ML computations. Experiments to validate our\nframework suggest that we can attain substantial improvement over BSP while\nguaranteeing sequential correctness of ML tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 08:42:41 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Goel", "Naman", ""], ["Agrawal", "Divyakant", ""], ["Chawla", "Sanjay", ""], ["Elmagarmid", "Ahmed", ""]]}, {"id": "1508.00722", "submitter": "Zhi-Hua Zhou", "authors": "Shao-Yuan Li, Yuan Jiang, Zhi-Hua Zhou", "title": "Multi-Label Active Learning from Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label active learning is a hot topic in reducing the label cost by\noptimally choosing the most valuable instance to query its label from an\noracle. In this paper, we consider the poolbased multi-label active learning\nunder the crowdsourcing setting, where during the active query process, instead\nof resorting to a high cost oracle for the ground-truth, multiple low cost\nimperfect annotators with various expertise are available for labeling. To deal\nwith this problem, we propose the MAC (Multi-label Active learning from Crowds)\napproach which incorporate the local influence of label correlations to build a\nprobabilistic model over the multi-label classifier and annotators. Based on\nthis model, we can estimate the labels for instances as well as the expertise\nof each annotator. Then we propose the instance selection and annotator\nselection criteria that consider the uncertainty/diversity of instances and the\nreliability of annotators, such that the most reliable annotator will be\nqueried for the most valuable instances. Experimental results demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 10:11:23 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Li", "Shao-Yuan", ""], ["Jiang", "Yuan", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1508.00792", "submitter": "Zelda Mariet", "authors": "Zelda Mariet, Suvrit Sra", "title": "Fixed-point algorithms for learning determinantal point processes", "comments": "ICML, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Determinantal point processes (DPPs) offer an elegant tool for encoding\nprobabilities over subsets of a ground set. Discrete DPPs are parametrized by a\npositive semidefinite matrix (called the DPP kernel), and estimating this\nkernel is key to learning DPPs from observed data. We consider the task of\nlearning the DPP kernel, and develop for it a surprisingly simple yet effective\nnew algorithm. Our algorithm offers the following benefits over previous\napproaches: (a) it is much simpler; (b) it yields equally good and sometimes\neven better local maxima; and (c) it runs an order of magnitude faster on large\nproblems. We present experimental results on both real and simulated data to\nillustrate the numerical performance of our technique.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 14:58:45 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 20:56:34 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Mariet", "Zelda", ""], ["Sra", "Suvrit", ""]]}, {"id": "1508.00842", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Perceptron like Algorithms for Online Learning to Rank", "comments": "Under review in Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptron is a classic online algorithm for learning a classification\nfunction. In this paper, we provide a novel extension of the perceptron\nalgorithm to the learning to rank problem in information retrieval. We consider\npopular listwise performance measures such as Normalized Discounted Cumulative\nGain (NDCG) and Average Precision (AP). A modern perspective on perceptron for\nclassification is that it is simply an instance of online gradient descent\n(OGD), during mistake rounds, using the hinge loss function. Motivated by this\ninterpretation, we propose a novel family of listwise, large margin ranking\nsurrogates. Members of this family can be thought of as analogs of the hinge\nloss. Exploiting a certain self-bounding property of the proposed family, we\nprovide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our\nperceptron-like algorithm. We show that, if there exists a perfect oracle\nranker which can correctly rank each instance in an online sequence of ranking\ndata, with some margin, the cumulative loss of perceptron algorithm on that\nsequence is bounded by a constant, irrespective of the length of the sequence.\nThis result is reminiscent of Novikoff's convergence theorem for the\nclassification perceptron. Moreover, we prove a lower bound on the cumulative\nloss achievable by any deterministic algorithm, under the assumption of\nexistence of perfect oracle ranker. The lower bound shows that our perceptron\nbound is not tight, and we propose another, \\emph{purely online}, algorithm\nwhich achieves the lower bound. We provide empirical results on simulated and\nlarge commercial datasets to corroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 17:23:46 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 20:32:04 GMT"}, {"version": "v3", "created": "Sun, 27 Mar 2016 00:29:55 GMT"}, {"version": "v4", "created": "Tue, 23 Aug 2016 06:52:04 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1508.00945", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi Jaakkola", "title": "Structured Prediction: From Gaussian Perturbations to Linear-Time\n  Principled Algorithms", "comments": "Uncertainty in Artificial Intelligence (UAI) 2016", "journal-ref": "Uncertainty in Artificial Intelligence (UAI), 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin-based structured prediction commonly uses a maximum loss over all\npossible structured outputs \\cite{Altun03,Collins04b,Taskar03}. In natural\nlanguage processing, recent work \\cite{Zhang14,Zhang15} has proposed the use of\nthe maximum loss over random structured outputs sampled independently from some\nproposal distribution. This method is linear-time in the number of random\nstructured outputs and trivially parallelizable. We study this family of loss\nfunctions in the PAC-Bayes framework under Gaussian perturbations\n\\cite{McAllester07}. Under some technical conditions and up to statistical\naccuracy, we show that this family of loss functions produces a tighter upper\nbound of the Gibbs decoder distortion than commonly used methods. Thus, using\nthe maximum loss over random structured outputs is a principled way of learning\nthe parameter of structured prediction models. Besides explaining the\nexperimental success of \\cite{Zhang14,Zhang15}, our theoretical results show\nthat more general techniques are possible.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 00:22:39 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 16:00:05 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 13:36:25 GMT"}, {"version": "v4", "created": "Sat, 4 Jun 2016 00:19:13 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.00964", "submitter": "Namyoon Lee", "authors": "Namyoon Lee", "title": "MAP Support Detection for Greedy Sparse Signal Recovery Algorithms in\n  Compressive Sensing", "comments": "12 pages, Submitted to IEEE TSP", "journal-ref": null, "doi": "10.1109/TSP.2016.2580527", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable support detection is essential for a greedy algorithm to\nreconstruct a sparse signal accurately from compressed and noisy measurements.\nThis paper proposes a novel support detection method for greedy algorithms,\nwhich is referred to as \"\\textit{maximum a posteriori (MAP) support\ndetection}\". Unlike existing support detection methods that identify support\nindices with the largest correlation value in magnitude per iteration, the\nproposed method selects them with the largest likelihood ratios computed under\nthe true and null support hypotheses by simultaneously exploiting the\ndistributions of sensing matrix, sparse signal, and noise. Leveraging this\ntechnique, MAP-Matching Pursuit (MAP-MP) is first presented to show the\nadvantages of exploiting the proposed support detection method, and a\nsufficient condition for perfect signal recovery is derived for the case when\nthe sparse signal is binary. Subsequently, a set of iterative greedy\nalgorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP),\nMAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace\nPursuit (MAP-SP) are presented to demonstrate the applicability of the proposed\nsupport detection method to existing greedy algorithms. From empirical results,\nit is shown that the proposed greedy algorithms with highly reliable support\ndetection can be better, faster, and easier to implement than basis pursuit via\nlinear programming.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 03:23:34 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 05:50:42 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Lee", "Namyoon", ""]]}, {"id": "1508.00973", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Leonard K.M. Poon, Zhourong Chen", "title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical latent tree analysis (HLTA) is recently proposed as a new method\nfor topic detection. It differs fundamentally from the LDA-based methods in\nterms of topic definition, topic-document relationship, and learning method. It\nhas been shown to discover significantly more coherent topics and better topic\nhierarchies. However, HLTA relies on the Expectation-Maximization (EM)\nalgorithm for parameter estimation and hence is not efficient enough to deal\nwith large datasets. In this paper, we propose a method to drastically speed up\nHLTA using a technique inspired by recent advances in the moments method.\nEmpirical experiments show that our method greatly improves the efficiency of\nHLTA. It is as efficient as the state-of-the-art LDA-based method for\nhierarchical topic detection and finds substantially better topics and topic\nhierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 05:00:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""]]}, {"id": "1508.00984", "submitter": "Pitoyo Hartono", "authors": "Pitoyo Hartono", "title": "Dimension Reduction with Non-degrading Generalization", "comments": null, "journal-ref": "Neural Computing and Applications 30 (2018) 905-915", "doi": "10.1007/s00521-016-2726-5", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing high dimensional data by projecting them into two or three\ndimensional space is one of the most effective ways to intuitively understand\nthe data's underlying characteristics, for example their class neighborhood\nstructure. While data visualization in low dimensional space can be efficient\nfor revealing the data's underlying characteristics, classifying a new sample\nin the reduced-dimensional space is not always beneficial because of the loss\nof information in expressing the data. It is possible to classify the data in\nthe high dimensional space, while visualizing them in the low dimensional\nspace, but in this case, the visualization is often meaningless because it\nfails to illustrate the underlying characteristics that are crucial for the\nclassification process.\n  In this paper, the performance-preserving property of the previously proposed\nRestricted Radial Basis Function Network in reducing the dimension of labeled\ndata is explained. Here, it is argued through empirical experiments that the\ninternal representation of the Restricted Radial Basis Function Network, which\nduring the supervised learning process organizes a visualizable two dimensional\nmap, does not only preserve the topographical structure of high dimensional\ndata but also captures their class neighborhood structures that are important\nfor classifying them. Hence, unlike many of the existing dimension reduction\nmethods, the Restricted Radial Basis Function Network offers two dimensional\nvisualization that is strongly correlated with the classification process.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 06:32:01 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hartono", "Pitoyo", ""]]}, {"id": "1508.01006", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang and Dong Wang", "title": "Relation Classification via Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:03:46 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 03:51:00 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Zhang", "Dongxu", ""], ["Wang", "Dong", ""]]}, {"id": "1508.01011", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang, Tianyi Luo, Dong Wang and Rong Liu", "title": "Learning from LDA using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:22:25 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Zhang", "Dongxu", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Liu", "Rong", ""]]}, {"id": "1508.01023", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Philip S. Yu", "title": "A review of heterogeneous data mining for brain disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in neuroimaging techniques, the research on brain\ndisorder identification has become an emerging area in the data mining\ncommunity. Brain disorder data poses many unique challenges for data mining\nresearch. For example, the raw data generated by neuroimaging experiments is in\ntensor representations, with typical characteristics of high dimensionality,\nstructural complexity and nonlinear separability. Furthermore, brain\nconnectivity networks can be constructed from the tensor data, embedding subtle\ninteractions between brain regions. Other clinical measures are usually\navailable reflecting the disease status from different perspectives. It is\nexpected that integrating complementary information in the tensor data and the\nbrain network data, and incorporating other clinical parameters will be\npotentially transformative for investigating disease mechanisms and for\ninforming therapeutic interventions. Many research efforts have been devoted to\nthis area. They have achieved great success in various applications, such as\ntensor-based modeling, subgraph pattern mining, multi-view feature analysis. In\nthis paper, we review some recent data mining methods that are used for\nanalyzing brain disorders.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:57:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1508.01084", "submitter": "Fabio Anselmi", "authors": "Fabio Anselmi, Lorenzo Rosasco, Cheston Tan, Tomaso Poggio", "title": "Deep Convolutional Networks are Hierarchical Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In i-theory a typical layer of a hierarchical architecture consists of HW\nmodules pooling the dot products of the inputs to the layer with the\ntransformations of a few templates under a group. Such layers include as\nspecial cases the convolutional layers of Deep Convolutional Networks (DCNs) as\nwell as the non-convolutional layers (when the group contains only the\nidentity). Rectifying nonlinearities -- which are used by present-day DCNs --\nare one of the several nonlinearities admitted by i-theory for the HW module.\nWe discuss here the equivalence between group averages of linear combinations\nof rectifying nonlinearities and an associated kernel. This property implies\nthat present-day DCNs can be exactly equivalent to a hierarchy of kernel\nmachines with pooling and non-pooling layers. Finally, we describe a conjecture\nfor theoretically understanding hierarchies of such modules. A main consequence\nof the conjecture is that hierarchies of trained HW modules minimize memory\nrequirements while computing a selective and invariant representation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:18:17 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Anselmi", "Fabio", ""], ["Rosasco", "Lorenzo", ""], ["Tan", "Cheston", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1508.01211", "submitter": "Navdeep Jaitly", "authors": "William Chan and Navdeep Jaitly and Quoc V. Le and Oriol Vinyals", "title": "Listen, Attend and Spell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:17:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 00:38:43 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Chan", "William", ""], ["Jaitly", "Navdeep", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1508.01235", "submitter": "Arash Pourhabib", "authors": "Arash Pourhabib", "title": "Empirical Similarity for Absent Data Generation in Imbalanced\n  Classification", "comments": null, "journal-ref": "Advances in Information and Communication. FICC 2019. 69 (2020)\n  1010-1030", "doi": "10.1007/978-3-030-12388-8_70", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the training data in a two-class classification problem is overwhelmed\nby one class, most classification techniques fail to correctly identify the\ndata points belonging to the underrepresented class. We propose\nSimilarity-based Imbalanced Classification (SBIC) that learns patterns in the\ntraining data based on an empirical similarity function. To take the imbalanced\nstructure of the training data into account, SBIC utilizes the concept of\nabsent data, i.e. data from the minority class which can help better find the\nboundary between the two classes. SBIC simultaneously optimizes the weights of\nthe empirical similarity function and finds the locations of absent data\npoints. As such, SBIC uses an embedded mechanism for synthetic data generation\nwhich does not modify the training dataset, but alters the algorithm to suit\nimbalanced datasets. Therefore, SBIC uses the ideas of both major schools of\nthoughts in imbalanced classification: Like cost-sensitive approaches SBIC\noperates on an algorithm level to handle imbalanced structures; and similar to\nsynthetic data generation approaches, it utilizes the properties of unobserved\ndata points from the minority class. The application of SBIC to imbalanced\ndatasets suggests it is comparable to, and in some cases outperforms, other\ncommonly used classification techniques for imbalanced datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 21:43:32 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 18:37:10 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Pourhabib", "Arash", ""]]}, {"id": "1508.01534", "submitter": "Jundong Liu", "authors": "Bibo Shi, Jundong Liu", "title": "Nonlinear Metric Learning for kNN and SVMs through Geometric\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research efforts to extend linear metric learning models to\nhandle nonlinear structures have attracted great interests. In this paper, we\npropose a novel nonlinear solution through the utilization of deformable\ngeometric models to learn spatially varying metrics, and apply the strategy to\nboost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS)\nare chosen as the geometric model due to their remarkable versatility and\nrepresentation power in accounting for high-order deformations. By transforming\nthe input space through TPS, we can pull same-class neighbors closer while\npushing different-class points farther away in kNN, as well as make the input\ndata points more linearly separable in SVMs. Improvements in the performance of\nkNN classification are demonstrated through experiments on synthetic and real\nworld datasets, with comparisons made with several state-of-the-art metric\nlearning solutions. Our SVM-based models also achieve significant improvements\nover traditional linear and kernel SVMs with the same datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 20:29:28 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Shi", "Bibo", ""], ["Liu", "Jundong", ""]]}, {"id": "1508.01549", "submitter": "Uday Kamath Dr.", "authors": "Uday Kamath, Carlotta Domeniconi and Kenneth De Jong", "title": "Theoretical and Empirical Analysis of a Parallel Boosting Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many real-world problems involve massive amounts of data. Under these\ncircumstances learning algorithms often become prohibitively expensive, making\nscalability a pressing issue to be addressed. A common approach is to perform\nsampling to reduce the size of the dataset and enable efficient learning.\nAlternatively, one customizes learning algorithms to achieve scalability. In\neither case, the key challenge is to obtain algorithmic efficiency without\ncompromising the quality of the results. In this paper we discuss a\nmeta-learning algorithm (PSBML) which combines features of parallel algorithms\nwith concepts from ensemble and boosting methodologies to achieve the desired\nscalability property. We present both theoretical and empirical analyses which\nshow that PSBML preserves a critical property of boosting, specifically,\nconvergence to a distribution centered around the margin. We then present\nadditional empirical analyses showing that this meta-level algorithm provides a\ngeneral and effective framework that can be used in combination with a variety\nof learning classifiers. We perform extensive experiments to investigate the\ntradeoff achieved between scalability and accuracy, and robustness to noise, on\nboth synthetic and real-world data. These empirical results corroborate our\ntheoretical analysis, and demonstrate the potential of PSBML in achieving\nscalability without sacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 21:54:34 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Kamath", "Uday", ""], ["Domeniconi", "Carlotta", ""], ["De Jong", "Kenneth", ""]]}, {"id": "1508.01585", "submitter": "Minwei Feng", "authors": "Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou", "title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "comments": "To appear in the proceedings of ASRU 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We apply a general deep learning framework to address the non-factoid\nquestion answering task. Our approach does not rely on any linguistic tools and\ncan be applied to different languages or domains. Various architectures are\npresented and compared. We create and release a QA corpus and setup a new QA\ntask in the insurance domain. Experimental results demonstrate superior\nperformance compared to the baseline methods and various technologies give\nfurther improvements. For this highly challenging task, the top-1 accuracy can\nreach up to 65.3% on a test set, which indicates a great potential for\npractical use.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 01:54:04 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 18:23:16 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Feng", "Minwei", ""], ["Xiang", "Bing", ""], ["Glass", "Michael R.", ""], ["Wang", "Lidan", ""], ["Zhou", "Bowen", ""]]}, {"id": "1508.01596", "submitter": "Pushpendre Rastogi", "authors": "Pushpendre Rastogi and Benjamin Van Durme", "title": "Sublinear Partition Estimation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output scores of a neural network classifier are converted to\nprobabilities via normalizing over the scores of all competing categories.\nComputing this partition function, $Z$, is then linear in the number of\ncategories, which is problematic as real-world problem sets continue to grow in\ncategorical types, such as in visual object recognition or discriminative\nlanguage modeling. We propose three approaches for sublinear estimation of the\npartition function, based on approximate nearest neighbor search and kernel\nfeature maps and compare the performance of the proposed approaches\nempirically.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 03:45:21 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Rastogi", "Pushpendre", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1508.01633", "submitter": "Ruiliang Zhang", "authors": "Ruiliang Zhang, Shuai Zheng, James T. Kwok", "title": "Asynchronous Distributed Semi-Stochastic Gradient Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent proliferation of large-scale learning problems,there have\nbeen a lot of interest on distributed machine learning algorithms, particularly\nthose that are based on stochastic gradient descent (SGD) and its variants.\nHowever, existing algorithms either suffer from slow convergence due to the\ninherent variance of stochastic gradients, or have a fast linear convergence\nrate but at the expense of poorer solution quality. In this paper, we combine\ntheir merits by proposing a fast distributed asynchronous SGD-based algorithm\nwith variance reduction. A constant learning rate can be used, and it is also\nguaranteed to converge linearly to the optimal solution. Experiments on the\nGoogle Cloud Computing Platform demonstrate that the proposed algorithm\noutperforms state-of-the-art distributed asynchronous algorithms in terms of\nboth wall clock time and solution quality.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 07:54:47 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 06:33:34 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Zhang", "Ruiliang", ""], ["Zheng", "Shuai", ""], ["Kwok", "James T.", ""]]}, {"id": "1508.01746", "submitter": "Alan Godoy", "authors": "Alan Godoy, Fl\\'avio Sim\\~oes, Jos\\'e Augusto Stuchi, Marcus de Assis\n  Angeloni, M\\'ario Uliani, Ricardo Violato", "title": "Using Deep Learning for Detecting Spoofing Attacks on Speech Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:20:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:27:49 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Godoy", "Alan", ""], ["Sim\u00f5es", "Fl\u00e1vio", ""], ["Stuchi", "Jos\u00e9 Augusto", ""], ["Angeloni", "Marcus de Assis", ""], ["Uliani", "M\u00e1rio", ""], ["Violato", "Ricardo", ""]]}, {"id": "1508.01774", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, Emmanouil Benetos, Simon Dixon", "title": "An End-to-End Neural Network for Polyphonic Piano Music Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised neural network model for polyphonic piano music\ntranscription. The architecture of the proposed model is analogous to speech\nrecognition systems and comprises an acoustic model and a music language model.\nThe acoustic model is a neural network used for estimating the probabilities of\npitches in a frame of audio. The language model is a recurrent neural network\nthat models the correlations between pitch combinations over time. The proposed\nmodel is general and can be used to transcribe polyphonic music without\nimposing any constraints on the polyphony. The acoustic and language model\npredictions are combined using a probabilistic graphical model. Inference over\nthe output variables is performed using the beam search algorithm. We perform\ntwo sets of experiments. We investigate various neural network architectures\nfor the acoustic models and also investigate the effect of combining acoustic\nand music language model predictions using the proposed architecture. We\ncompare performance of the neural network based acoustic models with two\npopular unsupervised acoustic models. Results show that convolutional neural\nnetwork acoustic models yields the best performance across all evaluation\nmetrics. We also observe improved performance with the application of the music\nlanguage models. Finally, we present an efficient variant of beam search that\nimproves performance and reduces run-times by an order of magnitude, making the\nmodel suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 18:16:32 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 12:59:35 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Benetos", "Emmanouil", ""], ["Dixon", "Simon", ""]]}, {"id": "1508.01887", "submitter": "Zhanglin Peng", "authors": "Zhanglin Peng, Ya Li, Zhaoquan Cai and Liang Lin", "title": "Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning\n  in Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how the traditional image classification pipelines can\nbe extended into a deep architecture, inspired by recent successes of deep\nneural networks. We propose a deep boosting framework based on layer-by-layer\njoint feature boosting and dictionary learning. In each layer, we construct a\ndictionary of filters by combining the filters from the lower layer, and\niteratively optimize the image representation with a joint\ndiscriminative-generative formulation, i.e. minimization of empirical\nclassification error plus regularization of analysis image generation over\ntraining images. For optimization, we perform two iterating steps: i) to\nminimize the classification error, select the most discriminative features\nusing the gentle adaboost algorithm; ii) according to the feature selection,\nupdate the filters to minimize the regularization on analysis image\nrepresentation using the gradient descent method. Once the optimization is\nconverged, we learn the higher layer representation in the same way. Our model\ndelivers several distinct advantages. First, our layer-wise optimization\nprovides the potential to build very deep architectures. Second, the generated\nimage representation is compact and meaningful. In several visual recognition\ntasks, our framework outperforms existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 11:42:21 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 09:45:07 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Peng", "Zhanglin", ""], ["Li", "Ya", ""], ["Cai", "Zhaoquan", ""], ["Lin", "Liang", ""]]}, {"id": "1508.01903", "submitter": "Badong Chen", "authors": "Wentao Ma, Badong Chen, Jiandong Duan, Haiquan Zhao", "title": "Diffusion Maximum Correntropy Criterion Algorithms for Robust\n  Distributed Estimation", "comments": "17 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust diffusion adaptive estimation algorithms based on the maximum\ncorrentropy criterion (MCC), including adaptation to combination MCC and\ncombination to adaptation MCC, are developed to deal with the distributed\nestimation over network in impulsive (long-tailed) noise environments. The cost\nfunctions used in distributed estimation are in general based on the mean\nsquare error (MSE) criterion, which is desirable when the measurement noise is\nGaussian. In non-Gaussian situations, such as the impulsive-noise case, MCC\nbased methods may achieve much better performance than the MSE methods as they\ntake into account higher order statistics of error distribution. The proposed\nmethods can also outperform the robust diffusion least mean p-power(DLMP) and\ndiffusion minimum error entropy (DMEE) algorithms. The mean and mean square\nconvergence analysis of the new algorithms are also carried out.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 13:38:41 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 12:30:28 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Ma", "Wentao", ""], ["Chen", "Badong", ""], ["Duan", "Jiandong", ""], ["Zhao", "Haiquan", ""]]}, {"id": "1508.01928", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicol\\'as Garc\\'ia Trillos and Dejan Slep\\v{c}ev", "title": "A variational approach to the consistency of spectral clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 17:14:51 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Trillos", "Nicol\u00e1s Garc\u00eda", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1508.01951", "submitter": "Besmira Nushi", "authors": "Besmira Nushi, Adish Singla, Anja Gruenheid, Erfan Zamanian, Andreas\n  Krause, Donald Kossmann", "title": "Crowd Access Path Optimization: Diversity Matters", "comments": "10 pages, 3rd AAAI Conference on Human Computation and Crowdsourcing\n  (HCOMP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assurance is one the most important challenges in crowdsourcing.\nAssigning tasks to several workers to increase quality through redundant\nanswers can be expensive if asking homogeneous sources. This limitation has\nbeen overlooked by current crowdsourcing platforms resulting therefore in\ncostly solutions. In order to achieve desirable cost-quality tradeoffs it is\nessential to apply efficient crowd access optimization techniques. Our work\nargues that optimization needs to be aware of diversity and correlation of\ninformation within groups of individuals so that crowdsourcing redundancy can\nbe adequately planned beforehand. Based on this intuitive idea, we introduce\nthe Access Path Model (APM), a novel crowd model that leverages the notion of\naccess paths as an alternative way of retrieving information. APM aggregates\nanswers ensuring high quality and meaningful confidence. Moreover, we devise a\ngreedy optimization algorithm for this model that finds a provably good\napproximate plan to access the crowd. We evaluate our approach on three\ncrowdsourced datasets that illustrate various aspects of the problem. Our\nresults show that the Access Path Model combined with greedy optimization is\ncost-efficient and practical to overcome common difficulties in large-scale\ncrowdsourcing like data sparsity and anonymity.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 20:36:54 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 07:21:57 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Nushi", "Besmira", ""], ["Singla", "Adish", ""], ["Gruenheid", "Anja", ""], ["Zamanian", "Erfan", ""], ["Krause", "Andreas", ""], ["Kossmann", "Donald", ""]]}, {"id": "1508.01993", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Ralph Fehrer", "title": "Improving Decision Analytics with Deep Learning: The Case of Financial\n  Disclosures", "comments": null, "journal-ref": "Twenty-Fourth European Conference on Information Systems (ECIS\n  2016), Istanbul, Turkey, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision analytics commonly focuses on the text mining of financial news\nsources in order to provide managerial decision support and to predict stock\nmarket movements. Existing predictive frameworks almost exclusively apply\ntraditional machine learning methods, whereas recent research indicates that\ntraditional machine learning methods are not sufficiently capable of extracting\nsuitable features and capturing the non-linear nature of complex tasks. As a\nremedy, novel deep learning models aim to overcome this issue by extending\ntraditional neural network models with additional hidden layers. Indeed, deep\nlearning has been shown to outperform traditional methods in terms of\npredictive performance. In this paper, we adapt the novel deep learning\ntechnique to financial decision support. In this instance, we aim to predict\nthe direction of stock movements following financial disclosures. As a result,\nwe show how deep learning can outperform the accuracy of random forests as a\nbenchmark for machine learning by 5.66%.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 07:39:24 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 09:32:57 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Fehrer", "Ralph", ""]]}, {"id": "1508.02064", "submitter": "Antonis Mytidis", "authors": "Antonis Mytidis, Athanasios Aris Panagopoulos, Orestis P.\n  Panagopoulos, Andrew Miller, Bernard Whiting", "title": "Sensitivity study using machine learning algorithms on simulated r-mode\n  gravitational wave signals from newborn neutron stars", "comments": "Accepted for publication in Physical Review D", "journal-ref": "Phys. Rev. D 99, 024024 (2019)", "doi": "10.1103/PhysRevD.99.024024", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a follow-up sensitivity study on r-mode gravitational wave signals\nfrom newborn neutron stars illustrating the applicability of machine learning\nalgorithms for the detection of long-lived gravitational-wave transients. In\nthis sensitivity study we examine three machine learning algorithms (MLAs):\nartificial neural networks (ANNs), support vector machines (SVMs) and\nconstrained subspace classifiers (CSCs). The objective of this study is to\ncompare the detection efficiency that MLAs can achieve with the efficiency of\nconventional detection algorithms discussed in an earlier paper. Comparisons\nare made using 2 distinct r-mode waveforms. For the training of the MLAs we\nassumed that some information about the distance to the source is given so that\nthe training was performed over distance ranges not wider than half an order of\nmagnitude. The results of this study suggest that machine learning algorithms\nare suitable for the detection of long-lived gravitational-wave transients and\nthat when assuming knowledge of the distance to the source, MLAs are at least\nas efficient as conventional methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 18:23:39 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 04:00:13 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Mytidis", "Antonis", ""], ["Panagopoulos", "Athanasios Aris", ""], ["Panagopoulos", "Orestis P.", ""], ["Miller", "Andrew", ""], ["Whiting", "Bernard", ""]]}, {"id": "1508.02087", "submitter": "Philipp Moritz", "authors": "Philipp Moritz, Robert Nishihara, Michael I. Jordan", "title": "A Linearly-Convergent Stochastic L-BFGS Algorithm", "comments": "10 pages, 3 figures in International Conference on Artificial\n  Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic L-BFGS algorithm and prove a linear convergence\nrate for strongly convex and smooth functions. Our algorithm draws heavily from\na recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as\na recent approach to variance reduction for stochastic gradient descent from\nJohnson and Zhang (2013). We demonstrate experimentally that our algorithm\nperforms well on large-scale convex and non-convex optimization problems,\nexhibiting linear convergence and rapidly solving the optimization problems to\nhigh levels of precision. Furthermore, we show that our algorithm performs well\nfor a wide-range of step sizes, often differing by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 21:40:33 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 23:36:06 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Moritz", "Philipp", ""], ["Nishihara", "Robert", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1508.02103", "submitter": "Sanghack Lee", "authors": "Sanghack Lee and Vasant Honavar", "title": "Lifted Representation of Relational Causal Models Revisited:\n  Implications for Reasoning and Structure Learning", "comments": "Workshop on Advances in Causal Inference, Conference on Uncertainty\n  in Artificial Intelligence, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maier et al. (2010) introduced the relational causal model (RCM) for\nrepresenting and inferring causal relationships in relational data. A lifted\nrepresentation, called abstract ground graph (AGG), plays a central role in\nreasoning with and learning of RCM. The correctness of the algorithm proposed\nby Maier et al. (2013a) for learning RCM from data relies on the soundness and\ncompleteness of AGG for relational d-separation to reduce the learning of an\nRCM to learning of an AGG. We revisit the definition of AGG and show that AGG,\nas defined in Maier et al. (2013b), does not correctly abstract all ground\ngraphs. We revise the definition of AGG to ensure that it correctly abstracts\nall ground graphs. We further show that AGG representation is not complete for\nrelational d-separation, that is, there can exist conditional independence\nrelations in an RCM that are not entailed by AGG. A careful examination of the\nrelationship between the lack of completeness of AGG for relational\nd-separation and faithfulness conditions suggests that weaker notions of\ncompleteness, namely adjacency faithfulness and orientation faithfulness\nbetween an RCM and its AGG, can be used to learn an RCM from data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 00:52:14 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 11:56:30 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Lee", "Sanghack", ""], ["Honavar", "Vasant", ""]]}, {"id": "1508.02131", "submitter": "Trevor Cohn", "authors": "Daniel Beck, Trevor Cohn, Christian Hardmeier, Lucia Specia", "title": "Learning Structural Kernels for Natural Language Processing", "comments": "Transactions of the Association for Computational Linguistics, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural kernels are a flexible learning paradigm that has been widely used\nin Natural Language Processing. However, the problem of model selection in\nkernel-based methods is usually overlooked. Previous approaches mostly rely on\nsetting default values for kernel hyperparameters or using grid search, which\nis slow and coarse-grained. In contrast, Bayesian methods allow efficient model\nselection by maximizing the evidence on the training data through\ngradient-based methods. In this paper we show how to perform this in the\ncontext of structural kernels by using Gaussian Processes. Experimental results\non tree kernels show that this procedure results in better prediction\nperformance compared to hyperparameter optimization via grid search. The\nframework proposed in this paper can be adapted to other structures besides\ntrees, e.g., strings and graphs, thereby extending the utility of kernel-based\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 05:57:14 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Beck", "Daniel", ""], ["Cohn", "Trevor", ""], ["Hardmeier", "Christian", ""], ["Specia", "Lucia", ""]]}, {"id": "1508.02268", "submitter": "Ning Chen", "authors": "Ning Chen and Jun Zhu and Jianfei Chen and Ting Chen", "title": "Dropout Training for SVMs with Data Augmentation", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1404.4171", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and other feature noising schemes have shown promising results in\ncontrolling over-fitting by artificially corrupting the training data. Though\nextensive theoretical and empirical studies have been performed for generalized\nlinear models, little work has been done for support vector machines (SVMs),\none of the most successful approaches for supervised learning. This paper\npresents dropout training for both linear SVMs and the nonlinear extension with\nlatent representation learning. For linear SVMs, to deal with the intractable\nexpectation of the non-smooth hinge loss under corrupting distributions, we\ndevelop an iteratively re-weighted least square (IRLS) algorithm by exploring\ndata augmentation techniques. Our algorithm iteratively minimizes the\nexpectation of a re-weighted least square problem, where the re-weights are\nanalytically updated. For nonlinear latent SVMs, we consider learning one layer\nof latent representations in SVMs and extend the data augmentation technique in\nconjunction with first-order Taylor-expansion to deal with the intractable\nexpected non-smooth hinge loss and the nonlinearity of latent representations.\nFinally, we apply the similar data augmentation ideas to develop a new IRLS\nalgorithm for the expected logistic loss under corrupting distributions, and we\nfurther develop a non-linear extension of logistic regression by incorporating\none layer of latent representations. Our algorithms offer insights on the\nconnection and difference between the hinge loss and logistic loss in dropout\ntraining. Empirical results on several real datasets demonstrate the\neffectiveness of dropout training on significantly boosting the classification\naccuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs\nfurther improve the prediction performance on several image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 14:57:30 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Chen", "Ting", ""]]}, {"id": "1508.02373", "submitter": "Yuan Cao", "authors": "Yuan Cao", "title": "Training Conditional Random Fields with Natural Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel parameter estimation procedure that works efficiently for\nconditional random fields (CRF). This algorithm is an extension to the maximum\nlikelihood estimation (MLE), using loss functions defined by Bregman\ndivergences which measure the proximity between the model expectation and the\nempirical mean of the feature vectors. This leads to a flexible training\nframework from which multiple update strategies can be derived using natural\ngradient descent (NGD). We carefully choose the convex function inducing the\nBregman divergence so that the types of updates are reduced, while making the\noptimization procedure more effective by transforming the gradients of the\nlog-likelihood loss function. The derived algorithms are very simple and can be\neasily implemented on top of the existing stochastic gradient descent (SGD)\noptimization procedure, yet it is very effective as illustrated by experimental\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:46:50 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Cao", "Yuan", ""]]}, {"id": "1508.02375", "submitter": "Matthew Gormley", "authors": "Matthew R. Gormley, Mark Dredze, Jason Eisner", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train the fast dependency parser of Smith and Eisner (2008)\nfor improved accuracy. This parser can consider higher-order interactions among\nedges while retaining O(n^3) runtime. It outputs the parse with maximum\nexpected recall -- but for speed, this expectation is taken under a posterior\ndistribution that is constructed only approximately, using loopy belief\npropagation through structured factors. We show how to adjust the model\nparameters to compensate for the errors introduced by this approximation, by\nfollowing the gradient of the actual loss on training data. We find this\ngradient by back-propagation. That is, we treat the entire parser\n(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)\nand Domke (2010) did for loopy CRFs. The resulting trained parser obtains\nhigher accuracy with fewer iterations of belief propagation than one trained by\nconditional log-likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:48:33 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Gormley", "Matthew R.", ""], ["Dredze", "Mark", ""], ["Eisner", "Jason", ""]]}, {"id": "1508.02428", "submitter": "Zhensong Qian", "authors": "Oliver Schulte and Zhensong Qian", "title": "FactorBase: SQL for Learning A Multi-Relational Graphical Model", "comments": "14 pages, 10 figures, 10 tables, Published on 2015 IEEE International\n  Conference on Data Science and Advanced Analytics (IEEE DSAA'2015), Oct\n  19-21, 2015, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe FactorBase, a new SQL-based framework that leverages a relational\ndatabase management system to support multi-relational model discovery. A\nmulti-relational statistical model provides an integrated analysis of the\nheterogeneous and interdependent data resources in the database. We adopt the\nBayesStore design philosophy: statistical models are stored and managed as\nfirst-class citizens inside a database. Whereas previous systems like\nBayesStore support multi-relational inference, FactorBase supports\nmulti-relational learning. A case study on six benchmark databases evaluates\nhow our system supports a challenging machine learning application, namely\nlearning a first-order Bayesian network model for an entire database. Model\nlearning in this setting has to examine a large number of potential statistical\nassociations across data tables. Our implementation shows how the SQL\nconstructs in FactorBase facilitate the fast, modular, and reliable development\nof highly scalable model learning systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 21:15:43 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Schulte", "Oliver", ""], ["Qian", "Zhensong", ""]]}, {"id": "1508.02449", "submitter": "Houman Owhadi", "authors": "Houman Owhadi and Clint Scovel", "title": "Towards Machine Wald", "comments": "37 pages", "journal-ref": null, "doi": "10.1007/978-3-319-11259-6_3-1", "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past century has seen a steady increase in the need of estimating and\npredicting complex systems and making (possibly critical) decisions with\nlimited information. Although computers have made possible the numerical\nevaluation of sophisticated statistical models, these models are still designed\n\\emph{by humans} because there is currently no known recipe or algorithm for\ndividing the design of a statistical model into a sequence of arithmetic\noperations. Indeed enabling computers to \\emph{think} as \\emph{humans} have the\nability to do when faced with uncertainty is challenging in several major ways:\n(1) Finding optimal statistical models remains to be formulated as a well posed\nproblem when information on the system of interest is incomplete and comes in\nthe form of a complex combination of sample data, partial knowledge of\nconstitutive relations and a limited description of the distribution of input\nrandom variables. (2) The space of admissible scenarios along with the space of\nrelevant information, assumptions, and/or beliefs, tend to be infinite\ndimensional, whereas calculus on a computer is necessarily discrete and finite.\nWith this purpose, this paper explores the foundations of a rigorous framework\nfor the scientific computation of optimal statistical estimators/models and\nreviews their connections with Decision Theory, Machine Learning, Bayesian\nInference, Stochastic Optimization, Robust Optimization, Optimal Uncertainty\nQuantification and Information Based Complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 22:48:51 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 22:41:50 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Owhadi", "Houman", ""], ["Scovel", "Clint", ""]]}, {"id": "1508.02452", "submitter": "Zheng Han", "authors": "Zheng Han and Frank E. Curtis", "title": "Primal-Dual Active-Set Methods for Isotonic Regression and Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic regression (IR) is a non-parametric calibration method used in\nsupervised learning. For performing large-scale IR, we propose a primal-dual\nactive-set (PDAS) algorithm which, in contrast to the state-of-the-art Pool\nAdjacent Violators (PAV) algorithm, can be parallized and is easily\nwarm-started thus well-suited in the online settings. We prove that, like the\nPAV algorithm, our PDAS algorithm for IR is convergent and has a work\ncomplexity of O(n), though our numerical experiments suggest that our PDAS\nalgorithm is often faster than PAV. In addition, we propose PDAS variants (with\nsafeguarding to ensure convergence) for solving related trend filtering (TF)\nproblems, providing the results of experiments to illustrate their\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 23:23:25 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 16:57:55 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Han", "Zheng", ""], ["Curtis", "Frank E.", ""]]}, {"id": "1508.02479", "submitter": "Heejin Choi", "authors": "Heejin Choi, Yutaka Sasaki, Nathan Srebro", "title": "Normalized Hierarchical SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved methods of using structured SVMs in a large-scale\nhierarchical classification problem, that is when labels are leaves, or sets of\nleaves, in a tree or a DAG. We examine the need to normalize both the\nregularization and the margin and show how doing so significantly improves\nperformance, including allowing achieving state-of-the-art results where\nunnormalized structured SVMs do not perform better than flat models. We also\ndescribe a further extension of hierarchical SVMs that highlight the connection\nbetween hierarchical SVMs and matrix factorization models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 03:34:33 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 18:53:19 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Choi", "Heejin", ""], ["Sasaki", "Yutaka", ""], ["Srebro", "Nathan", ""]]}, {"id": "1508.02593", "submitter": "Denis Krompass", "authors": "Denis Krompa{\\ss} and Stephan Baier and Volker Tresp", "title": "Type-Constrained Representation Learning in Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large knowledge graphs increasingly add value to various applications that\nrequire machines to recognize and understand queries and their semantics, as in\nsearch or question answering systems. Latent variable models have increasingly\ngained attention for the statistical modeling of knowledge graphs, showing\npromising results in tasks related to knowledge graph completion and cleaning.\nBesides storing facts about the world, schema-based knowledge graphs are backed\nby rich semantic descriptions of entities and relation-types that allow\nmachines to understand the notion of things and their semantic relationships.\nIn this work, we study how type-constraints can generally support the\nstatistical modeling with latent variable models. More precisely, we integrated\nprior knowledge in form of type-constraints in various state of the art latent\nvariable approaches. Our experimental results show that prior knowledge on\nrelation-types significantly improves these models up to 77% in link-prediction\ntasks. The achieved improvements are especially prominent when a low model\ncomplexity is enforced, a crucial requirement when these models are applied to\nvery large datasets. Unfortunately, type-constraints are neither always\navailable nor always complete e.g., they can become fuzzy when entities lack\nproper typing. We show that in these cases, it can be beneficial to apply a\nlocal closed-world assumption that approximates the semantics of relation-types\nbased on observations made in the data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 13:49:07 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 09:00:31 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Krompa\u00df", "Denis", ""], ["Baier", "Stephan", ""], ["Tresp", "Volker", ""]]}, {"id": "1508.02681", "submitter": "Fatemeh Jahedpari", "authors": "Fatemeh Jahedpari, Marina De Vos, Sattar Hashemi, Benjamin Hirsch,\n  Julian Padget", "title": "Artificial Prediction Markets for Online Prediction of Continuous\n  Variables-A Preliminary Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Artificial Continuous Prediction Market (ACPM) as a means to\npredict a continuous real value, by integrating a range of data sources and\naggregating the results of different machine learning (ML) algorithms. ACPM\nadapts the concept of the (physical) prediction market to address the\nprediction of real values instead of discrete events. Each ACPM participant has\na data source, a ML algorithm and a local decision-making procedure that\ndetermines what to bid on what value. The contributions of ACPM are: (i)\nadaptation to changes in data quality by the use of learning in: (a) the\nmarket, which weights each market participant to adjust the influence of each\non the market prediction and (b) the participants, which use a Q-learning based\ntrading strategy to incorporate the market prediction into their subsequent\npredictions, (ii) resilience to a changing population of low- and\nhigh-performing participants. We demonstrate the effectiveness of ACPM by\napplication to an influenza-like illnesses data set, showing ACPM out-performs\na range of well-known regression models and is resilient to variation in data\nsource quality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 18:37:29 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Jahedpari", "Fatemeh", ""], ["De Vos", "Marina", ""], ["Hashemi", "Sattar", ""], ["Hirsch", "Benjamin", ""], ["Padget", "Julian", ""]]}, {"id": "1508.02788", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "The Effects of Hyperparameters on SGD Training of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of neural network classifiers is determined by a number of\nhyperparameters, including learning rate, batch size, and depth. A number of\nattempts have been made to explore these parameters in the literature, and at\ntimes, to develop methods for optimizing them. However, exploration of\nparameter spaces has often been limited. In this note, I report the results of\nlarge scale experiments exploring these different parameters and their\ninteractions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:01:11 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.02790", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "On the Convergence of SGD Training of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are usually trained by some form of stochastic gradient\ndescent (SGD)). A number of strategies are in common use intended to improve\nSGD optimization, such as learning rate schedules, momentum, and batching.\nThese are motivated by ideas about the occurrence of local minima at different\nscales, valleys, and other phenomena in the objective function. Empirical\nresults presented here suggest that these phenomena are not significant factors\nin SGD optimization of MLP-related objective functions, and that the behavior\nof stochastic gradient descent in these problems is better described as the\nsimultaneous convergence at different rates of many, largely non-interacting\nsubproblems\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:11:47 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.02823", "submitter": "Adish Singla", "authors": "Adish Singla, Eric Horvitz, Pushmeet Kohli, Andreas Krause", "title": "Learning to Hire Teams", "comments": "Short version of this paper will appear in HCOMP'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing and human computation has been employed in increasingly\nsophisticated projects that require the solution of a heterogeneous set of\ntasks. We explore the challenge of building or hiring an effective team, for\nperforming tasks required for such projects on an ongoing basis, from an\navailable pool of applicants or workers who have bid for the tasks. The\nrecruiter needs to learn workers' skills and expertise by performing online\ntests and interviews, and would like to minimize the amount of budget or time\nspent in this process before committing to hiring the team. How can one\noptimally spend budget to learn the expertise of workers as part of recruiting\na team? How can one exploit the similarities among tasks as well as underlying\nsocial ties or commonalities among the workers for faster learning? We tackle\nthese decision-theoretic challenges by casting them as an instance of online\nlearning for best action selection. We present algorithms with PAC bounds on\nthe required budget to hire a near-optimal team with high confidence.\nFurthermore, we consider an embedding of the tasks and workers in an underlying\ngraph that may arise from task similarities or social ties, and that can\nprovide additional side-observations for faster learning. We then quantify the\nimprovement in the bounds that we can achieve depending on the characteristic\nproperties of this graph structure. We evaluate our methodology on simulated\nproblem instances as well as on real-world crowdsourcing data collected from\nthe oDesk platform. Our methodology and results present an interesting\ndirection of research to tackle the challenges faced by a recruiter for\ncontract-based crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 06:35:38 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Singla", "Adish", ""], ["Horvitz", "Eric", ""], ["Kohli", "Pushmeet", ""], ["Krause", "Andreas", ""]]}, {"id": "1508.02849", "submitter": "Xiaobao Sheng", "authors": "Fei Jiang, Lili Jia, Xiaobao Sheng, Riley LeMieux", "title": "Manifold regularization in structured output space for semi-supervised\n  structured output prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured output prediction aims to learn a predictor to predict a\nstructured output from a input data vector. The structured outputs include\nvector, tree, sequence, etc. We usually assume that we have a training set of\ninput-output pairs to train the predictor. However, in many real-world appli-\ncations, it is difficult to obtain the output for a input, thus for many\ntraining input data points, the structured outputs are missing. In this paper,\nwe dis- cuss how to learn from a training set composed of some input-output\npairs, and some input data points without outputs. This problem is called semi-\nsupervised structured output prediction. We propose a novel method for this\nproblem by constructing a nearest neighbor graph from the input space to\npresent the manifold structure, and using it to regularize the structured out-\nput space directly. We define a slack structured output for each training data\npoint, and proposed to predict it by learning a structured output predictor.\nThe learning of both slack structured outputs and the predictor are unified\nwithin one single minimization problem. In this problem, we propose to mini-\nmize the structured loss between the slack structured outputs of neighboring\ndata points, and the prediction error measured by the structured loss. The\nproblem is optimized by an iterative algorithm. Experiment results over three\nbenchmark data sets show its advantage.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 08:44:47 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Jiang", "Fei", ""], ["Jia", "Lili", ""], ["Sheng", "Xiaobao", ""], ["LeMieux", "Riley", ""]]}, {"id": "1508.02933", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, David Lopez-Paz, L\\'eon Bottou", "title": "No Regret Bound for Extreme Bandits", "comments": "11 pages, International Conference on Artificial Intelligence and\n  Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:31:49 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 21:45:51 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 18:20:50 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Nishihara", "Robert", ""], ["Lopez-Paz", "David", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1508.02986", "submitter": "Ugo Louche", "authors": "Liva Ralaivola, Ugo Louche", "title": "From Cutting Planes Algorithms to Compression Schemes and Active\n  Learning", "comments": "IJCNN 2015, Jul 2015, Killarney, Ireland. 2015,\n  \\&lt;http://www.ijcnn.org/\\&gt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting-plane methods are well-studied localization(and optimization)\nalgorithms. We show that they provide a natural framework to perform\nmachinelearning ---and not just to solve optimization problems posed by\nmachinelearning--- in addition to their intended optimization use. In\nparticular, theyallow one to learn sparse classifiers and provide good\ncompression schemes.Moreover, we show that very little effort is required to\nturn them intoeffective active learning methods. This last property provides a\ngeneric way todesign a whole family of active learning algorithms from existing\npassivemethods. We present numerical simulations testifying of the relevance\nofcutting-plane methods for passive and active learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:46:29 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Ralaivola", "Liva", ""], ["Louche", "Ugo", ""]]}, {"id": "1508.03117", "submitter": "Canyi Lu", "authors": "Canyi Lu, Huan Li, Zhouchen Lin", "title": "Optimized Projections for Compressed Sensing via Direct Mutual Coherence\n  Minimization", "comments": null, "journal-ref": "Signal Processing, 2018", "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing (CS) is a novel technique for simultaneous signal sampling\nand compression based on the existence of a sparse representation of signal and\na projected dictionary $PD$, where $P\\in\\mathbb{R}^{m\\times d}$ is the\nprojection matrix and $D\\in\\mathbb{R}^{d\\times n}$ is the dictionary. To\nexactly recover the signal with a small number of measurements $m$, the\nprojected dictionary $PD$ is expected to be of low mutual coherence. Several\nprevious methods attempt to find the projection $P$ such that the mutual\ncoherence of $PD$ can be as low as possible. However, they do not minimize the\nmutual coherence directly and thus their methods are far from optimal. Also the\nsolvers they used lack of the convergence guarantee and thus there has no\nguarantee on the quality of their obtained solutions. This work aims to address\nthese issues. We propose to find an optimal projection by minimizing the mutual\ncoherence of $PD$ directly. This leads to a nonconvex nonsmooth minimization\nproblem. We then approximate it by smoothing and solve it by alternate\nminimization. We further prove the convergence of our algorithm. To the best of\nour knowledge, this is the first work which directly minimizes the mutual\ncoherence of the projected dictionary with a convergence guarantee. Numerical\nexperiments demonstrate that the proposed method can recover sparse signals\nbetter than existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 04:41:40 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 03:00:34 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 19:05:03 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Lu", "Canyi", ""], ["Li", "Huan", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1508.03130", "submitter": "Aditi Raghunathan", "authors": "Narayanan U. Edakunni, Aditi Raghunathan, Abhishek Tripathi, John\n  Handley, Fredric Roulland", "title": "Probabilistic Dependency Networks for Prediction and Diagnostics", "comments": "Presented at the Transportation Research Board Annual Meeting 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in transportation frequently involve modelling and predicting\nattributes of events that occur at regular intervals. The event could be\narrival of a bus at a bus stop, the volume of a traffic at a particular point,\nthe demand at a particular bus stop etc. In this work, we propose a specific\nimplementation of probabilistic graphical models to learn the probabilistic\ndependency between the events that occur in a network. A dependency graph is\nbuilt from the past observed instances of the event and we use the graph to\nunderstand the causal effects of some events on others in the system. The\ndependency graph is also used to predict the attributes of future events and is\nshown to have a good prediction accuracy compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 06:42:25 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Edakunni", "Narayanan U.", ""], ["Raghunathan", "Aditi", ""], ["Tripathi", "Abhishek", ""], ["Handley", "John", ""], ["Roulland", "Fredric", ""]]}, {"id": "1508.03285", "submitter": "Yinjie Huang", "authors": "Yinjie Huang and Michael Georgiopoulos and Georgios C. Anagnostopoulos", "title": "Hash Function Learning via Codewords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel hash learning framework that has two main\ndistinguishing features, when compared to past approaches. First, it utilizes\ncodewords in the Hamming space as ancillary means to accomplish its hash\nlearning task. These codewords, which are inferred from the data, attempt to\ncapture similarity aspects of the data's hash codes. Secondly and more\nimportantly, the same framework is capable of addressing supervised,\nunsupervised and, even, semi-supervised hash learning tasks in a natural\nmanner. A series of comparative experiments focused on content-based image\nretrieval highlights its performance advantages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:55:57 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 18:29:16 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Huang", "Yinjie", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1508.03326", "submitter": "Li Zhou", "authors": "Li Zhou", "title": "A Survey on Contextual Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey we cover a few stochastic and adversarial contextual bandit\nalgorithms. We analyze each algorithm's assumption and regret bound.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 19:49:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2016 07:33:08 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Zhou", "Li", ""]]}, {"id": "1508.03329", "submitter": "Niloofar Yousefi", "authors": "Niloofar Yousefi, Michael Georgiopoulos and Georgios C.\n  Anagnostopoulos", "title": "Multi-Task Learning with Group-Specific Feature Space Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with learning a set of inter-related tasks from a limited amount\nof usable data, learning each task independently may lead to poor\ngeneralization performance. Multi-Task Learning (MTL) exploits the latent\nrelations between tasks and overcomes data scarcity limitations by co-learning\nall these tasks simultaneously to offer improved performance. We propose a\nnovel Multi-Task Multiple Kernel Learning framework based on Support Vector\nMachines for binary classification tasks. By considering pair-wise task\naffinity in terms of similarity between a pair's respective feature spaces, the\nnew framework, compared to other similar MTL approaches, offers a high degree\nof flexibility in determining how similar feature spaces should be, as well as\nwhich pairs of tasks should share a common feature space in order to benefit\noverall performance. The associated optimization problem is solved via a block\ncoordinate descent, which employs a consensus-form Alternating Direction Method\nof Multipliers algorithm to optimize the Multiple Kernel Learning weights and,\nhence, to determine task affinities. Empirical evaluation on seven data sets\nexhibits a statistically significant improvement of our framework's results\ncompared to the ones of several other Clustered Multi-Task Learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 19:58:59 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Yousefi", "Niloofar", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1508.03332", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Sachit Butail, Maurizio Porfiri, Erik M. Bollt", "title": "Dimensionality Reduction of Collective Motion by Principal Manifolds", "comments": "19 pages, 13 figures, journal article", "journal-ref": "Physica-D : Nonlinear Phenomena, Volume 291, 15 January 2015,\n  Pages 62-73", "doi": "10.1016/j.physd.2014.09.009", "report-no": null, "categories": "math.NA cs.LG cs.MA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the existence of low-dimensional embedding manifolds has been shown in\npatterns of collective motion, the current battery of nonlinear dimensionality\nreduction methods are not amenable to the analysis of such manifolds. This is\nmainly due to the necessary spectral decomposition step, which limits control\nover the mapping from the original high-dimensional space to the embedding\nspace. Here, we propose an alternative approach that demands a two-dimensional\nembedding which topologically summarizes the high-dimensional data. In this\nsense, our approach is closely related to the construction of one-dimensional\nprincipal curves that minimize orthogonal error to data points subject to\nsmoothness constraints. Specifically, we construct a two-dimensional principal\nmanifold directly in the high-dimensional space using cubic smoothing splines,\nand define the embedding coordinates in terms of geodesic distances. Thus, the\nmapping from the high-dimensional data to the manifold is defined in terms of\nlocal coordinates. Through representative examples, we show that compared to\nexisting nonlinear dimensionality reduction methods, the principal manifold\nretains the original structure even in noisy and sparse datasets. The principal\nmanifold finding algorithm is applied to configurations obtained from a\ndynamical system of multiple agents simulating a complex maneuver called\npredator mobbing, and the resulting two-dimensional embedding is compared with\nthat of a well-established nonlinear dimensionality reduction method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 21:07:51 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Butail", "Sachit", ""], ["Porfiri", "Maurizio", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1508.03337", "submitter": "Kimon Fountoulakis", "authors": "Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou and\n  Petros Drineas", "title": "A Randomized Rounding Algorithm for Sparse PCA", "comments": "28 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze a simple, two-step algorithm to approximate the\noptimal solution of the sparse PCA problem. Our approach first solves a L1\npenalized version of the NP-hard sparse PCA optimization problem and then uses\na randomized rounding strategy to sparsify the resulting dense solution. Our\nmain theoretical result guarantees an additive error approximation and provides\na tradeoff between sparsity and accuracy. Our experimental evaluation indicates\nthat our approach is competitive in practice, even compared to state-of-the-art\ntoolboxes such as Spasm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 20:06:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 08:38:17 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 01:34:47 GMT"}, {"version": "v4", "created": "Sat, 26 Mar 2016 21:09:19 GMT"}, {"version": "v5", "created": "Tue, 22 Nov 2016 20:05:10 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Fountoulakis", "Kimon", ""], ["Kundu", "Abhisek", ""], ["Kontopoulou", "Eugenia-Maria", ""], ["Drineas", "Petros", ""]]}, {"id": "1508.03386", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, David Vandyke, Milica Gasic, Dongho Kim, Nikola Mrksic,\n  Tsung-Hsien Wen, Steve Young", "title": "Learning from Real Users: Rating Dialogue Success with Neural Networks\n  for Reinforcement Learning in Spoken Dialogue Systems", "comments": "Accepted for publication in INTERSPEECH 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train a statistical spoken dialogue system (SDS) it is essential that an\naccurate method for measuring task success is available. To date training has\nrelied on presenting a task to either simulated or paid users and inferring the\ndialogue's success by observing whether this presented task was achieved or\nnot. Our aim however is to be able to learn from real users acting under their\nown volition, in which case it is non-trivial to rate the success as any prior\nknowledge of the task is simply unavailable. User feedback may be utilised but\nhas been found to be inconsistent. Hence, here we present two neural network\nmodels that evaluate a sequence of turn-level features to rate the success of a\ndialogue. Importantly these models make no use of any prior knowledge of the\nuser's task. The models are trained on dialogues generated by a simulated user\nand the best model is then used to train a policy on-line which is shown to\nperform at least as well as a baseline system using prior knowledge of the\nuser's task. We note that the models should also be of interest for evaluating\nSDS and for monitoring a dialogue in rule-based SDS.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 23:44:03 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Gasic", "Milica", ""], ["Kim", "Dongho", ""], ["Mrksic", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1508.03390", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu, Qihang Lin, Tianbao Yang", "title": "Doubly Stochastic Primal-Dual Coordinate Method for Bilinear\n  Saddle-Point Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a doubly stochastic primal-dual coordinate optimization algorithm\nfor empirical risk minimization, which can be formulated as a bilinear\nsaddle-point problem. In each iteration, our method randomly samples a block of\ncoordinates of the primal and dual solutions to update. The linear convergence\nof our method could be established in terms of 1) the distance from the current\niterate to the optimal solution and 2) the primal-dual objective gap. We show\nthat the proposed method has a lower overall complexity than existing\ncoordinate methods when either the data matrix has a factorized structure or\nthe proximal mapping on each block is computationally expensive, e.g.,\ninvolving an eigenvalue decomposition. The efficiency of the proposed method is\nconfirmed by empirical studies on several real applications, such as the\nmulti-task large margin nearest neighbor problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 00:22:45 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 16:24:20 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 05:24:19 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Yu", "Adams Wei", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1508.03391", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, David Vandyke, Milica Gasic, Nikola Mrksic, Tsung-Hsien\n  Wen, Steve Young", "title": "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line\n  Policy Learning in Spoken Dialogue Systems", "comments": "Accepted for publication in SigDial 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical spoken dialogue systems have the attractive property of being\nable to be optimised from data via interactions with real users. However in the\nreinforcement learning paradigm the dialogue manager (agent) often requires\nsignificant time to explore the state-action space to learn to behave in a\ndesirable manner. This is a critical issue when the system is trained on-line\nwith real users where learning costs are expensive. Reward shaping is one\npromising technique for addressing these concerns. Here we examine three\nrecurrent neural network (RNN) approaches for providing reward shaping\ninformation in addition to the primary (task-orientated) environmental\nfeedback. These RNNs are trained on returns from dialogues generated by a\nsimulated user and attempt to diffuse the overall evaluation of the dialogue\nback down to the turn level to guide the agent towards good behaviour faster.\nIn both simulated and real user scenarios these RNNs are shown to increase\npolicy learning speed. Importantly, they do not require prior knowledge of the\nuser's goal.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 00:41:12 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 12:42:42 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1508.03398", "submitter": "Jianshu Chen", "authors": "Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao,\n  Xinying Song, Li Deng", "title": "End-to-end Learning of LDA by Mirror-Descent Back Propagation over a\n  Deep Architecture", "comments": "Proc. NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully discriminative learning approach for supervised Latent\nDirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which\nmaximizes the posterior probability of the prediction variable given the input\ndocument. Different from traditional variational learning or Gibbs sampling\napproaches, the proposed learning method applies (i) the mirror descent\nalgorithm for maximum a posterior inference and (ii) back propagation over a\ndeep architecture together with stochastic gradient/mirror descent for model\nparameter estimation, leading to scalable and end-to-end discriminative\nlearning of the model. As a byproduct, we also apply this technique to develop\na new learning method for the traditional unsupervised LDA model (i.e.,\nBP-LDA). Experimental results on three real-world regression and classification\ntasks show that the proposed methods significantly outperform the previous\nsupervised topic models, neural networks, and is on par with deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 01:32:27 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 08:11:14 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Chen", "Jianshu", ""], ["He", "Ji", ""], ["Shen", "Yelong", ""], ["Xiao", "Lin", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Song", "Xinying", ""], ["Deng", "Li", ""]]}, {"id": "1508.03411", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Aviv Tamar and Shie Mannor", "title": "Emphatic TD Bellman Operator is a Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, \\citet{SuttonMW15} introduced the emphatic temporal differences\n(ETD) algorithm for off-policy evaluation in Markov decision processes. In this\nshort note, we show that the projected fixed-point equation that underlies ETD\ninvolves a contraction operator, with a $\\sqrt{\\gamma}$-contraction modulus\n(where $\\gamma$ is the discount factor). This allows us to provide error bounds\non the approximation error of ETD. To our knowledge, these are the first error\nbounds for an off-policy evaluation algorithm under general target and behavior\npolicies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 03:34:10 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 13:07:35 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Hallak", "Assaf", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1508.03606", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar and Johannes Rauh", "title": "Hierarchical Models as Marginals of Hierarchical Models", "comments": "18 pages, 4 figures, 2 tables, WUPES'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the representation of hierarchical models in terms of\nmarginals of other hierarchical models with smaller interactions. We focus on\nbinary variables and marginals of pairwise interaction models whose hidden\nvariables are conditionally independent given the visible variables. In this\ncase the problem is equivalent to the representation of linear subspaces of\npolynomials by feedforward neural networks with soft-plus computational units.\nWe show that every hidden variable can freely model multiple interactions among\nthe visible variables, which allows us to generalize and improve previous\nresults. In particular, we show that a restricted Boltzmann machine with less\nthan $[ 2(\\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximate\nevery distribution of $v$ visible binary variables arbitrarily well, compared\nto $2^{v-1}-1$ from the best previously known result.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:56:00 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 19:48:07 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""]]}, {"id": "1508.03712", "submitter": "Ingo Steinwart", "authors": "Philipp Thomann, Ingo Steinwart, Nico Schmid", "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures", "comments": null, "journal-ref": "Journal of Machine Learning Research. 16(Sep):1949-2002, 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some axioms for hierarchical clustering of probability measures\nand investigate their ramifications. The basic idea is to let the user\nstipulate the clusters for some elementary measures. This is done without the\nneed of any notion of metric, similarity or dissimilarity. Our main results\nthen show that for each suitable choice of user-defined clustering on\nelementary measures we obtain a unique notion of clustering on a large set of\ndistributions satisfying a set of additivity and continuity axioms. We\nillustrate the developed theory by numerous examples including some with and\nsome without a density.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 09:07:01 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Thomann", "Philipp", ""], ["Steinwart", "Ingo", ""], ["Schmid", "Nico", ""]]}, {"id": "1508.03720", "submitter": "Lili Mou", "authors": "Xu Yan, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, Zhi Jin", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest\n  Dependency Path", "comments": "EMNLP '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is an important research arena in the field of\nnatural language processing (NLP). In this paper, we present SDP-LSTM, a novel\nneural network to classify the relation of two entities in a sentence. Our\nneural architecture leverages the shortest dependency path (SDP) between two\nentities; multichannel recurrent neural networks, with long short term memory\n(LSTM) units, pick up heterogeneous information along the SDP. Our proposed\nmodel has several distinct features: (1) The shortest dependency paths retain\nmost relevant information (to relation classification), while eliminating\nirrelevant words in the sentence. (2) The multichannel LSTM networks allow\neffective information integration from heterogeneous sources over the\ndependency paths. (3) A customized dropout strategy regularizes the neural\nnetwork to alleviate overfitting. We test our model on the SemEval 2010\nrelation classification task, and achieve an $F_1$-score of 83.7\\%, higher than\ncompeting methods in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 11:15:32 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Yan", "Xu", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Peng", "Hao", ""], ["Jin", "Zhi", ""]]}, {"id": "1508.03721", "submitter": "Lili Mou", "authors": "Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin", "title": "A Comparative Study on Regularization Strategies for Embedding-based\n  Neural Networks", "comments": "EMNLP '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to compare different regularization strategies to address a\ncommon phenomenon, severe overfitting, in embedding-based neural networks for\nNLP. We chose two widely studied neural models and tasks as our testbed. We\ntried several frequently applied or newly proposed regularization strategies,\nincluding penalizing weights (embeddings excluded), penalizing embeddings,\nre-embedding words, and dropout. We also emphasized on incremental\nhyperparameter tuning, and combining different regularizations. The results\nprovide a picture on tuning hyperparameters for neural NLP models.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 11:16:39 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Peng", "Hao", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Lu", "Yangyang", ""], ["Jin", "Zhi", ""]]}, {"id": "1508.03826", "submitter": "Shaohua Li", "authors": "Shaohua Li, Jun Zhu, Chunyan Miao", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite\n  Solution", "comments": "Proceedings of the Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing word embedding methods can be categorized into Neural Embedding\nModels and Matrix Factorization (MF)-based methods. However some models are\nopaque to probabilistic interpretation, and MF-based methods, typically solved\nusing Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as\ntopics, sentiments or writing styles, into the word embedding model. Since\ngenerative models provide a principled way to incorporate latent factors, we\npropose a generative word embedding model, which is easy to interpret, and can\nserve as a basis of more sophisticated latent factor models. The model\ninference reduces to a low rank weighted positive semidefinite approximation\nproblem. Its optimization is approached by eigendecomposition on a submatrix,\nfollowed by online blockwise regression, which is scalable and avoids the\ninformation loss in SVD. In experiments on 7 common benchmark datasets, our\nvectors are competitive to word2vec, and better than other MF-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 14:12:17 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Li", "Shaohua", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1508.03846", "submitter": "Jose Picado", "authors": "Jose Picado, Arash Termehchy, Alan Fern, Parisa Ataei", "title": "Schema Independent Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning novel concepts and relations from relational databases is an\nimportant problem with many applications in database systems and machine\nlearning. Relational learning algorithms learn the definition of a new relation\nin terms of existing relations in the database. Nevertheless, the same data set\nmay be represented under different schemas for various reasons, such as\nefficiency, data quality, and usability. Unfortunately, the output of current\nrelational learning algorithms tends to vary quite substantially over the\nchoice of schema, both in terms of learning accuracy and efficiency. This\nvariation complicates their off-the-shelf application. In this paper, we\nintroduce and formalize the property of schema independence of relational\nlearning algorithms, and study both the theoretical and empirical dependence of\nexisting algorithms on the common class of (de) composition schema\ntransformations. We study both sample-based learning algorithms, which learn\nfrom sets of labeled examples, and query-based algorithms, which learn by\nasking queries to an oracle. We prove that current relational learning\nalgorithms are generally not schema independent. For query-based learning\nalgorithms we show that the (de) composition transformations influence their\nquery complexity. We propose Castor, a sample-based relational learning\nalgorithm that achieves schema independence by leveraging data dependencies. We\nsupport the theoretical results with an empirical study that demonstrates the\nschema dependence/independence of several algorithms on existing benchmark and\nreal-world datasets under (de) compositions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 16:57:20 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:35:32 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Picado", "Jose", ""], ["Termehchy", "Arash", ""], ["Fern", "Alan", ""], ["Ataei", "Parisa", ""]]}, {"id": "1508.03854", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Online Representation Learning in Recurrent Neural Language Models", "comments": "In Proceedings of EMNLP 2015", "journal-ref": null, "doi": "10.18653/v1/D15-1026", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an extension of continuous online learning in recurrent neural\nnetwork language models. The model keeps a separate vector representation of\nthe current unit of text being processed and adaptively adjusts it after each\nprediction. The initial experiments give promising results, indicating that the\nmethod is able to increase language modelling accuracy, while also decreasing\nthe parameters needed to store the model along with the computation required at\neach step.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 18:27:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1508.03856", "submitter": "Sheikh Muhammad Sarwar", "authors": "Sheikh Muhammad Sarwar, Mahamudul Hasan, Dmitry I. Ignatov", "title": "Two-stage Cascaded Classifier for Purchase Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our machine learning solution for the RecSys\nChallenge, 2015. We have proposed a time efficient two-stage cascaded\nclassifier for the prediction of buy sessions and purchased items within such\nsessions. Based on the model, several interesting features found, and formation\nof our own test bed, we have achieved a reasonable score. Usage of Random\nForests helps us to cope with the effect of the multiplicity of good models\ndepending on varying subsets of features in the purchased items prediction and,\nin its turn, boosting is used as a suitable technique to overcome severe class\nimbalance of the buy-session prediction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 19:27:35 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Hasan", "Mahamudul", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1508.03865", "submitter": "Yannick Meier", "authors": "Yannick Meier, Jie Xu, Onur Atan and Mihaela van der Schaar", "title": "Predicting Grades", "comments": "15 pages, 15 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 64, no. 4, pp.\n  959-972, Feb.15, 2016", "doi": "10.1109/TSP.2015.2496278", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To increase efficacy in traditional classroom courses as well as in Massive\nOpen Online Courses (MOOCs), automated systems supporting the instructor are\nneeded. One important problem is to automatically detect students that are\ngoing to do poorly in a course early enough to be able to take remedial\nactions. Existing grade prediction systems focus on maximizing the accuracy of\nthe prediction while overseeing the importance of issuing timely and\npersonalized predictions. This paper proposes an algorithm that predicts the\nfinal grade of each student in a class. It issues a prediction for each student\nindividually, when the expected accuracy of the prediction is sufficient. The\nalgorithm learns online what is the optimal prediction and time to issue a\nprediction based on past history of students' performance in a course. We\nderive a confidence estimate for the prediction accuracy and demonstrate the\nperformance of our algorithm on a dataset obtained based on the performance of\napproximately 700 UCLA undergraduate students who have taken an introductory\ndigital signal processing over the past 7 years. We demonstrate that for 85% of\nthe students we can predict with 76% accuracy whether they are going do well or\npoorly in the class after the 4th course week. Using data obtained from a pilot\ncourse, our methodology suggests that it is effective to perform early in-class\nassessments such as quizzes, which result in timely performance prediction for\neach student, thereby enabling timely interventions by the instructor (at the\nstudent or class level) when necessary.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 20:53:09 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 15:52:33 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Meier", "Yannick", ""], ["Xu", "Jie", ""], ["Atan", "Onur", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1508.03906", "submitter": "EPTCS", "authors": "Davide Bacciu (Dipartimento di Informatica, Universit\\`a di Pisa),\n  Stefania Gnesi (Istituto di Scienza e Tecnologie dell'Informazione, CNR),\n  Laura Semini (Dipartimento di Informatica, Universit\\`a di Pisa)", "title": "Using a Machine Learning Approach to Implement and Evaluate Product Line\n  Features", "comments": "In Proceedings WWV 2015, arXiv:1508.03389", "journal-ref": "EPTCS 188, 2015, pp. 75-83", "doi": "10.4204/EPTCS.188.8", "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bike-sharing systems are a means of smart transportation in urban\nenvironments with the benefit of a positive impact on urban mobility. In this\npaper we are interested in studying and modeling the behavior of features that\npermit the end user to access, with her/his web browser, the status of the\nBike-Sharing system. In particular, we address features able to make a\nprediction on the system state. We propose to use a machine learning approach\nto analyze usage patterns and learn computational models of such features from\nlogs of system usage.\n  On the one hand, machine learning methodologies provide a powerful and\ngeneral means to implement a wide choice of predictive features. On the other\nhand, trained machine learning models are provided with a measure of predictive\nperformance that can be used as a metric to assess the cost-performance\ntrade-off of the feature. This provides a principled way to assess the runtime\nbehavior of different components before putting them into operation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 01:51:36 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Bacciu", "Davide", "", "Dipartimento di Informatica, Universit\u00e0 di Pisa"], ["Gnesi", "Stefania", "", "Istituto di Scienza e Tecnologie dell'Informazione, CNR"], ["Semini", "Laura", "", "Dipartimento di Informatica, Universit\u00e0 di Pisa"]]}, {"id": "1508.04028", "submitter": "Lex Fridman", "authors": "Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor", "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze\n  Classification", "comments": "Accepted for Publication in IET Computer Vision. arXiv admin note:\n  text overlap with arXiv:1507.04760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate, robust, inexpensive gaze tracking in the car can help keep a driver\nsafe by facilitating the more effective study of how to improve (1) vehicle\ninterfaces and (2) the design of future Advanced Driver Assistance Systems. In\nthis paper, we estimate head pose and eye pose from monocular video using\nmethods developed extensively in prior work and ask two new interesting\nquestions. First, how much better can we classify driver gaze using head and\neye pose versus just using head pose? Second, are there individual-specific\ngaze strategies that strongly correlate with how much gaze classification\nimproves with the addition of eye pose information? We answer these questions\nby evaluating data drawn from an on-road study of 40 drivers. The main insight\nof the paper is conveyed through the analogy of an \"owl\" and \"lizard\" which\ndescribes the degree to which the eyes and the head move when shifting gaze.\nWhen the head moves a lot (\"owl\"), not much classification improvement is\nattained by estimating eye pose on top of head pose. On the other hand, when\nthe head stays still and only the eyes move (\"lizard\"), classification accuracy\nincreases significantly from adding in eye pose. We characterize how that\naccuracy varies between people, gaze strategies, and gaze regions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 13:54:05 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 18:10:49 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 04:46:26 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Fridman", "Lex", ""], ["Lee", "Joonbum", ""], ["Reimer", "Bryan", ""], ["Victor", "Trent", ""]]}, {"id": "1508.04035", "submitter": "Emmanuel Osegi", "authors": "Emmanuel N. Osegi", "title": "A Generative Model for Multi-Dialect Representation", "comments": "19 pages, 3 figures, 2 tables, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the era of deep learning several unsupervised models have been developed\nto capture the key features in unlabeled handwritten data. Popular among them\nis the Restricted Boltzmann Machines RBM. However, due to the novelty in\nhandwritten multidialect data, the RBM may fail to generate an efficient\nrepresentation. In this paper we propose a generative model, the Mode\nSynthesizing Machine MSM for on-line representation of real life handwritten\nmultidialect language data. The MSM takes advantage of the hierarchical\nrepresentation of the modes of a data distribution using a two-point error\nupdate to learn a sequence of representative multidialects in a generative way.\nExperiments were performed to evaluate the performance of the MSM over the RBM\nwith the former attaining much lower error values than the latter on both\nindependent and mixed data set.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 14:05:44 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Osegi", "Emmanuel N.", ""]]}, {"id": "1508.04065", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Ankit B. Patel, Richard G. Baraniuk", "title": "A Deep Learning Approach to Structured Signal Recovery", "comments": null, "journal-ref": "In Proceeding of 2015 53rd Annual Allerton Conference on\n  Communication, Control, and Computing (Allerton)", "doi": "10.1109/ALLERTON.2015.7447163", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new framework for sensing and recovering\nstructured signals. In contrast to compressive sensing (CS) systems that employ\nlinear measurements, sparse representations, and computationally complex\nconvex/greedy algorithms, we introduce a deep learning framework that supports\nboth linear and mildly nonlinear measurements, that learns a structured\nrepresentation from training data, and that efficiently computes a signal\nestimate. In particular, we apply a stacked denoising autoencoder (SDA), as an\nunsupervised feature learner. SDA enables us to capture statistical\ndependencies between the different elements of certain signals and improve\nsignal recovery performance as compared to the CS approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 15:46:09 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Mousavi", "Ali", ""], ["Patel", "Ankit B.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1508.04123", "submitter": "Alex Mbaziira", "authors": "Alex V. Mbaziira, Ehab Abozinadah, and James H. Jones Jr", "title": "Evaluating Classifiers in Detecting 419 Scams in Bilingual Cybercriminal\n  Communities", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Incidents of organized cybercrime are rising because of criminals are reaping\nhigh financial rewards while incurring low costs to commit crime. As the\ndigital landscape broadens to accommodate more internet-enabled devices and\ntechnologies like social media, more cybercriminals who are not native English\nspeakers are invading cyberspace to cash in on quick exploits. In this paper we\nevaluate the performance of three machine learning classifiers in detecting 419\nscams in a bilingual Nigerian cybercriminal community. We use three popular\nclassifiers in text processing namely: Na\\\"ive Bayes, k-nearest neighbors (IBK)\nand Support Vector Machines (SVM). The preliminary results on a real world\ndataset reveal the SVM significantly outperforms Na\\\"ive Bayes and IBK at 95%\nconfidence level.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 19:38:50 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Mbaziira", "Alex V.", ""], ["Abozinadah", "Ehab", ""], ["Jones", "James H.", "Jr"]]}, {"id": "1508.04186", "submitter": "Hao Yi Ong", "authors": "Hao Yi Ong, Kevin Chavez, Augustus Hong", "title": "Distributed Deep Q-Learning", "comments": "Updated figure of distributed deep learning architecture, updated\n  content throughout paper including dealing with minor grammatical issues and\n  highlighting differences of our paper with respect to prior work. arXiv admin\n  note: text overlap with arXiv:1312.5602 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is based on the deep Q-network, a convolutional neural\nnetwork trained with a variant of Q-learning. Its input is raw pixels and its\noutput is a value function estimating future rewards from taking an action\ngiven a system state. To distribute the deep Q-network training, we adapt the\nDistBelief software framework to the context of efficiently training\nreinforcement learning agents. As a result, the method is completely\nasynchronous and scales well with the number of machines. We demonstrate that\nthe deep Q-network agent, receiving only the pixels and the game score as\ninputs, was able to achieve reasonable success on a simple game with minimal\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 01:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 09:06:38 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ong", "Hao Yi", ""], ["Chavez", "Kevin", ""], ["Hong", "Augustus", ""]]}, {"id": "1508.04210", "submitter": "Changwei Hu", "authors": "Changwei Hu, Piyush Rai, Lawrence Carin", "title": "Zero-Truncated Poisson Tensor Factorization for Massive Binary Tensors", "comments": "UAI (Uncertainty in Artificial Intelligence) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable Bayesian model for low-rank factorization of massive\ntensors with binary observations. The proposed model has the following key\nproperties: (1) in contrast to the models based on the logistic or probit\nlikelihood, using a zero-truncated Poisson likelihood for binary data allows\nour model to scale up in the number of \\emph{ones} in the tensor, which is\nespecially appealing for massive but sparse binary tensors; (2)\nside-information in form of binary pairwise relationships (e.g., an adjacency\nnetwork) between objects in any tensor mode can also be leveraged, which can be\nespecially useful in \"cold-start\" settings; and (3) the model admits simple\nBayesian inference via batch, as well as \\emph{online} MCMC; the latter allows\nscaling up even for \\emph{dense} binary data (i.e., when the number of ones in\nthe tensor/network is also massive). In addition, non-negative factor matrices\nin our model provide easy interpretability, and the tensor rank can be inferred\nfrom the data. We evaluate our model on several large-scale real-world binary\ntensors, achieving excellent computational scalability, and also demonstrate\nits usefulness in leveraging side-information provided in form of\nmode-network(s).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 04:24:24 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hu", "Changwei", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "1508.04211", "submitter": "Changwei Hu", "authors": "Changwei Hu, Piyush Rai, Changyou Chen, Matthew Harding, Lawrence\n  Carin", "title": "Scalable Bayesian Non-Negative Tensor Factorization for Massive Count\n  Data", "comments": "ECML PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian non-negative tensor factorization model for\ncount-valued tensor data, and develop scalable inference algorithms (both batch\nand online) for dealing with massive tensors. Our generative model can handle\noverdispersed counts as well as infer the rank of the decomposition. Moreover,\nleveraging a reparameterization of the Poisson distribution as a multinomial\nfacilitates conjugacy in the model and enables simple and efficient Gibbs\nsampling and variational Bayes (VB) inference updates, with a computational\ncost that only depends on the number of nonzeros in the tensor. The model also\nprovides a nice interpretability for the factors; in our model, each factor\ncorresponds to a \"topic\". We develop a set of online inference algorithms that\nallow further scaling up the model to massive tensors, for which batch\ninference methods may be infeasible. We apply our framework on diverse\nreal-world applications, such as \\emph{multiway} topic modeling on a scientific\npublications database, analyzing a political science data set, and analyzing a\nmassive household transactions data set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 04:28:56 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hu", "Changwei", ""], ["Rai", "Piyush", ""], ["Chen", "Changyou", ""], ["Harding", "Matthew", ""], ["Carin", "Lawrence", ""]]}, {"id": "1508.04221", "submitter": "Jingbin Wang", "authors": "Xuejie Liu, Jingbin Wang, Ming Yin, Benjamin Edwards, Peijuan Xu", "title": "Supervised learning of sparse context reconstruction coefficients for\n  data representation and classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.00019", "journal-ref": null, "doi": "10.1007/s00521-015-2042-5", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context of data points, which is usually defined as the other data points in\na data set, has been found to play important roles in data representation and\nclassification. In this paper, we study the problem of using context of a data\npoint for its classification problem. Our work is inspired by the observation\nthat actually only very few data points are critical in the context of a data\npoint for its representation and classification. We propose to represent a data\npoint as the sparse linear combination of its context, and learn the sparse\ncontext in a supervised way to increase its discriminative ability. To this\nend, we proposed a novel formulation for context learning, by modeling the\nlearning of context parameter and classifier in a unified objective, and\noptimizing it with an alternative strategy in an iterative algorithm.\nExperiments on three benchmark data set show its advantage over\nstate-of-the-art context-based data representation and classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:59:17 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Liu", "Xuejie", ""], ["Wang", "Jingbin", ""], ["Yin", "Ming", ""], ["Edwards", "Benjamin", ""], ["Xu", "Peijuan", ""]]}, {"id": "1508.04306", "submitter": "John Hershey", "authors": "John R. Hershey, Zhuo Chen, Jonathan Le Roux, Shinji Watanabe", "title": "Deep clustering: Discriminative embeddings for segmentation and\n  separation", "comments": "Originally submitted on June 5, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of acoustic source separation in a deep learning\nframework we call \"deep clustering.\" Rather than directly estimating signals or\nmasking functions, we train a deep network to produce spectrogram embeddings\nthat are discriminative for partition labels given in training data. Previous\ndeep network approaches provide great advantages in terms of learning power and\nspeed, but previously it has been unclear how to use them to separate signals\nin a class-independent way. In contrast, spectral clustering approaches are\nflexible with respect to the classes and number of items to be segmented, but\nit has been unclear how to leverage the learning power and speed of deep\nnetworks. To obtain the best of both worlds, we use an objective function that\nto train embeddings that yield a low-rank approximation to an ideal pairwise\naffinity matrix, in a class-independent way. This avoids the high cost of\nspectral factorization and instead produces compact clusters that are amenable\nto simple clustering methods. The segmentations are therefore implicitly\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\nexperiments show that the proposed method can separate speech: when trained on\nspectrogram features containing mixtures of two speakers, and tested on\nmixtures of a held-out set of speakers, it can infer masking functions that\nimprove signal quality by around 6dB. We show that the model can generalize to\nthree-speaker mixtures despite training only on two-speaker mixtures. The\nframework can be used without class labels, and therefore has the potential to\nbe trained on a diverse set of sound types, and to generalize to novel sources.\nWe hope that future work will lead to segmentation of arbitrary sounds, with\nextensions to microphone array methods as well as image segmentation and other\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 13:12:34 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Hershey", "John R.", ""], ["Chen", "Zhuo", ""], ["Roux", "Jonathan Le", ""], ["Watanabe", "Shinji", ""]]}, {"id": "1508.04326", "submitter": "Jiale Cao", "authors": "Yanwei Pang, Jiale Cao, and Xuelong Li", "title": "Cascade Learning by Optimally Partitioning", "comments": "17 pages, 20 figures", "journal-ref": null, "doi": "10.1109/TCYB.2016.2601438", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded AdaBoost classifier is a well-known efficient object detection\nalgorithm. The cascade structure has many parameters to be determined. Most of\nexisting cascade learning algorithms are designed by assigning detection rate\nand false positive rate to each stage either dynamically or statically. Their\nobjective functions are not directly related to minimum computation cost. These\nalgorithms are not guaranteed to have optimal solution in the sense of\nminimizing computation cost. On the assumption that a strong classifier is\ngiven, in this paper we propose an optimal cascade learning algorithm (we call\nit iCascade) which iteratively partitions the strong classifiers into two parts\nuntil predefined number of stages are generated. iCascade searches the optimal\nnumber ri of weak classifiers of each stage i by directly minimizing the\ncomputation cost of the cascade. Theorems are provided to guarantee the\nexistence of the unique optimal solution. Theorems are also given for the\nproposed efficient algorithm of searching optimal parameters ri. Once a new\nstage is added, the parameter ri for each stage decreases gradually as\niteration proceeds, which we call decreasing phenomenon. Moreover, with the\ngoal of minimizing computation cost, we develop an effective algorithm for\nsetting the optimal threshold of each stage classifier. In addition, we prove\nin theory why more new weak classifiers are required compared to the last\nstage. Experimental results on face detection demonstrate the effectiveness and\nefficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 14:12:45 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pang", "Yanwei", ""], ["Cao", "Jiale", ""], ["Li", "Xuelong", ""]]}, {"id": "1508.04333", "submitter": "Shouvick Mondal", "authors": "Shouvick Mondal and Arko Banerjee", "title": "ESDF: Ensemble Selection using Diversity and Frequency", "comments": "Conference: National Conference on Research Trends in Computer\n  Science and Application (NCRTCSA-2014) Date: 8th February 2014 Organized by:\n  Dept. of Computer Application, Siliguri Institute of Technology, India In\n  Association With: Computer Society of India, Siliguri Chapter Technically\n  Sponsored By: IEEE, Kolkata Section Paper Id: NCRTCSA118. Shouvick Mondal et\n  al.; ESDF: Ensemble Selection using Diversity and Frequency; Proceedings of\n  NCRTCSA 2014; pp. 28-33, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently ensemble selection for consensus clustering has emerged as a\nresearch problem in Machine Intelligence. Normally consensus clustering\nalgorithms take into account the entire ensemble of clustering, where there is\na tendency of generating a very large size ensemble before computing its\nconsensus. One can avoid considering the entire ensemble and can judiciously\nselect few partitions in the ensemble without compromising on the quality of\nthe consensus. This may result in an efficient consensus computation technique\nand may save unnecessary computational overheads. The ensemble selection\nproblem addresses this issue of consensus clustering. In this paper, we propose\nan efficient method of ensemble selection for a large ensemble. We prioritize\nthe partitions in the ensemble based on diversity and frequency. Our method\nselects top K of the partitions in order of priority, where K is decided by the\nuser. We observe that considering jointly the diversity and frequency helps in\nidentifying few representative partitions whose consensus is qualitatively\nbetter than the consensus of the entire ensemble. Experimental analysis on a\nlarge number of datasets shows our method gives better results than earlier\nensemble selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 14:43:57 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Mondal", "Shouvick", ""], ["Banerjee", "Arko", ""]]}, {"id": "1508.04395", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel,\n  Yoshua Bengio", "title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 17:40:00 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 23:07:20 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Chorowski", "Jan", ""], ["Serdyuk", "Dmitriy", ""], ["Brakel", "Philemon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.04422", "submitter": "Vince Lyzinski", "authors": "Aren Jansen, Gregory Sell, Vince Lyzinski", "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\n  Networks", "comments": "10 pages, 2 figures, 1 table, this paper is under consideration for\n  publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 19:47:31 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 16:07:53 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:50:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Jansen", "Aren", ""], ["Sell", "Gregory", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1508.04467", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Smoothed Rank Approximation", "comments": "Journal, code is available", "journal-ref": "IEEE Signal Processing Letters, 22(2015)2088-2092", "doi": "10.1109/LSP.2015.2460737", "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimizing subject to affine constraints arises in many\napplication areas, ranging from signal processing to machine learning. Nuclear\nnorm is a convex relaxation for this problem which can recover the rank exactly\nunder some restricted and theoretically interesting conditions. However, for\nmany real-world applications, nuclear norm approximation to the rank function\ncan only produce a result far from the optimum. To seek a solution of higher\naccuracy than the nuclear norm, in this paper, we propose a rank approximation\nbased on Logarithm-Determinant. We consider using this rank approximation for\nsubspace clustering application. Our framework can model different kinds of\nerrors and noise. Effective optimization strategy is developed with theoretical\nguarantee to converge to a stationary point. The proposed method gives\npromising results on face clustering and motion segmentation tasks compared to\nthe state-of-the-art subspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 21:54:03 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1508.04486", "submitter": "Cem Subakan", "authors": "Y. Cem Subakan, Johannes Traa, Paris Smaragdis, Noah Stein", "title": "A Dictionary Learning Approach for Factorial Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a parameter estimation method for factorially\nparametrized models such as Factorial Gaussian Mixture Model and Factorial\nHidden Markov Model. Our contributions are two-fold. First, we show that the\nemission matrix of the standard Factorial Model is unidentifiable even if the\ntrue assignment matrix is known. Secondly, we address the issue of\nidentifiability by making a one component sharing assumption and derive a\nparameter learning algorithm for this case. Our approach is based on a\ndictionary learning problem of the form $X = O R$, where the goal is to learn\nthe dictionary $O$ given the data matrix $X$. We argue that due to the specific\nstructure of the activation matrix $R$ in the shared component factorial\nmixture model, and an incoherence assumption on the shared component, it is\npossible to extract the columns of the $O$ matrix without the need for\nalternating between the estimation of $O$ and $R$.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 23:47:28 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Subakan", "Y. Cem", ""], ["Traa", "Johannes", ""], ["Smaragdis", "Paris", ""], ["Stein", "Noah", ""]]}, {"id": "1508.04525", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yang Yu, Osho Gupta, Judith Gelernter", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained\n  Average Perceptron Ensembles", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise geocoding and time normalization for text requires that location and\ntime phrases be identified. Many state-of-the-art geoparsers and temporal\nparsers suffer from low recall. Categories commonly missed by parsers are:\nnouns used in a non- spatiotemporal sense, adjectival and adverbial phrases,\nprepositional phrases, and numerical phrases. We collected and annotated data\nset by querying commercial web searches API with such spatiotemporal\nexpressions as were missed by state-of-the- art parsers. Due to the high cost\nof sentence annotation, active learning was used to label training data, and a\nnew strategy was designed to better select training examples to reduce labeling\ncost. For the learning algorithm, we applied an average perceptron trained\nFeaturized Hidden Markov Model (FHMM). Five FHMM instances were used to create\nan ensemble, with the output phrase selected by voting. Our ensemble model was\ntested on a range of sequential labeling tasks, and has shown competitive\nperformance. Our contributions include (1) an new dataset annotated with named\nentities and expanded spatiotemporal expressions; (2) a comparison of inference\nalgorithms for ensemble models showing the superior accuracy of Belief\nPropagation over Viterbi Decoding; (3) a new example re-weighting method for\nactive ensemble learning that 'memorizes' the latest examples trained; (4) a\nspatiotemporal parser that jointly recognizes expanded spatiotemporal\nexpressions as well as named entities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 04:17:47 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Zhang", "Wei", ""], ["Yu", "Yang", ""], ["Gupta", "Osho", ""], ["Gelernter", "Judith", ""]]}, {"id": "1508.04554", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu and Ann B.\n  Ragin", "title": "Mining Brain Networks using Multiple Side Views for Neurological\n  Disorder Identification", "comments": "in Proceedings of IEEE International Conference on Data Mining (ICDM)\n  2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.50", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining discriminative subgraph patterns from graph data has attracted great\ninterest in recent years. It has a wide variety of applications in disease\ndiagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the\ngraph representation alone. However, in many real-world applications, the side\ninformation is available along with the graph data. For example, for\nneurological disorder identification, in addition to the brain networks derived\nfrom neuroimaging data, hundreds of clinical, immunologic, serologic and\ncognitive measures may also be documented for each subject. These measures\ncompose multiple side views encoding a tremendous amount of supplemental\ninformation for diagnostic purposes, yet are often ignored. In this paper, we\nstudy the problem of discriminative subgraph selection using multiple side\nviews and propose a novel solution to find an optimal set of subgraph features\nfor graph classification by exploring a plurality of side views. We derive a\nfeature evaluation criterion, named gSide, to estimate the usefulness of\nsubgraph patterns based upon side views. Then we develop a branch-and-bound\nalgorithm, called gMSV, to efficiently search for optimal subgraph features by\nintegrating the subgraph mining process and the procedure of discriminative\nfeature selection. Empirical studies on graph classification tasks for\nneurological disorders using brain networks demonstrate that subgraph patterns\nselected by the multi-side-view guided subgraph selection approach can\neffectively boost graph classification performances and are relevant to disease\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 07:51:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Zhang", "Jingyuan", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""]]}, {"id": "1508.04559", "submitter": "Przemys{\\l}aw Spurek", "authors": "Jacek Tabor, Przemys{\\l}aw Spurek, Konrad Kamieniecki, Marek \\'Smieja,\n  Krzysztof Misztal", "title": "Introduction to Cross-Entropy Clustering The R Package CEC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R Package CEC performs clustering based on the cross-entropy clustering\n(CEC) method, which was recently developed with the use of information theory.\nThe main advantage of CEC is that it combines the speed and simplicity of\n$k$-means with the ability to use various Gaussian mixture models and reduce\nunnecessary clusters. In this work we present a practical tutorial to CEC based\non the R Package CEC. Functions are provided to encompass the whole process of\nclustering.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:13:56 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Tabor", "Jacek", ""], ["Spurek", "Przemys\u0142aw", ""], ["Kamieniecki", "Konrad", ""], ["\u015amieja", "Marek", ""], ["Misztal", "Krzysztof", ""]]}, {"id": "1508.04582", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt, Richard S. Sutton", "title": "Learning to Predict Independent of Span", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how to learn multi-step predictions efficiently. Conventional\nalgorithms wait until observing actual outcomes before performing the\ncomputations to update their predictions. If predictions are made at a high\nrate or span over a large amount of time, substantial computation can be\nrequired to store all relevant observations and to update all predictions when\nthe outcome is finally observed. We show that the exact same predictions can be\nlearned in a much more computationally congenial way, with uniform per-step\ncomputation that does not depend on the span of the predictions. We apply this\nidea to various settings of increasing generality, repeatedly adding desired\nproperties and each time deriving an equivalent span-independent algorithm for\nthe conventional algorithm that satisfies these desiderata. Interestingly,\nalong the way several known algorithmic constructs emerge spontaneously from\nour derivations, including dutch eligibility traces, temporal difference\nerrors, and averaging. This allows us to link these constructs one-to-one to\nthe corresponding desiderata, unambiguously connecting the `how' to the `why'.\nEach step, we make sure that the derived algorithm subsumes the previous\nalgorithms, thereby retaining their properties. Ultimately we arrive at a\nsingle general temporal-difference algorithm that is applicable to the full\nsetting of reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 09:37:25 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["van Hasselt", "Hado", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1508.04734", "submitter": "Arunav Sanyal", "authors": "M. Amarnath, S. Arunav, Hemantha Kumar, V. Sugumaran, and G.S\n  Raghvendra", "title": "Fault Diagnosis of Helical Gear Box using Large Margin K-Nearest\n  Neighbors Classifier using Sound Signals", "comments": "Accepted for publication", "journal-ref": "JVET V4 n2 2016", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gear drives are one of the most widely used transmission system in many\nmachinery. Sound signals of a rotating machine contain the dynamic information\nabout its health conditions. Not much information available in the literature\nreporting suitability of sound signals for fault diagnosis applications.\nMaximum numbers of literature are based on FFT (Fast Fourier Transform)\nanalysis and have its own limitations with non-stationary signals like the ones\nfrom gears. In this paper, attempt has been made in using sound signals\nacquired from gears in good and simulated faulty conditions for the purpose of\nfault diagnosis through a machine learning approach. The descriptive\nstatistical features were extracted from the acquired sound signals and the\npredominant features were selected using J48 decision tree technique. The\nselected features were then used for classification using Large Margin\nK-nearest neighbor approach. The paper also discusses the effect of various\nparameters on classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 18:33:07 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Amarnath", "M.", ""], ["Arunav", "S.", ""], ["Kumar", "Hemantha", ""], ["Sugumaran", "V.", ""], ["Raghvendra", "G. S", ""]]}, {"id": "1508.04757", "submitter": "Leonardo Ferreira", "authors": "Leonardo N. Ferreira and Liang Zhao", "title": "Time Series Clustering via Community Detection in Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2015.07.046", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a technique for time series clustering using\ncommunity detection in complex networks. Firstly, we present a method to\ntransform a set of time series into a network using different distance\nfunctions, where each time series is represented by a vertex and the most\nsimilar ones are connected. Then, we apply community detection algorithms to\nidentify groups of strongly connected vertices (called a community) and,\nconsequently, identify time series clusters. Still in this paper, we make a\ncomprehensive analysis on the influence of various combinations of time series\ndistance functions, network generation methods and community detection\ntechniques on clustering results. Experimental study shows that the proposed\nnetwork-based approach achieves better results than various classic or\nup-to-date clustering techniques under consideration. Statistical tests confirm\nthat the proposed method outperforms some classic clustering algorithms, such\nas $k$-medoids, diana, median-linkage and centroid-linkage in various data\nsets. Interestingly, the proposed method can effectively detect shape patterns\npresented in time series due to the topological structure of the underlying\nnetwork constructed in the clustering process. At the same time, other\ntechniques fail to identify such patterns. Moreover, the proposed method is\nrobust enough to group time series presenting similar pattern but with time\nshifts and/or amplitude variations. In summary, the main point of the proposed\nmethod is the transformation of time series from time-space domain to\ntopological domain. Therefore, we hope that our approach contributes not only\nfor time series clustering, but also for general time series analysis tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 19:55:08 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ferreira", "Leonardo N.", ""], ["Zhao", "Liang", ""]]}, {"id": "1508.04826", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularisation of deep neural networks (DNN) during training is critical to\nperformance. By far the most popular method is known as dropout. Here, cast\nthrough the prism of signal processing theory, we compare and contrast the\nregularisation effects of dropout with those of dither. We illustrate some\nserious inherent limitations of dropout and demonstrate that dither provides a\nmore effective regulariser.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 23:02:37 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 12:59:38 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1508.04887", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder and Alfred O. Hero III", "title": "Multi-criteria Similarity-based Anomaly Detection using Pareto Depth\n  Analysis", "comments": "The work is submitted to IEEE TNNLS Special Issue on Learning in\n  Non-(geo)metric Spaces for review on October 28, 2013, revised on July 26,\n  2015 and accepted on July 30, 2015. A preliminary version of this work is\n  reported in the conference Advances in Neural Information Processing Systems\n  (NIPS) 2012", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 27\n  (2016) 1307-1321", "doi": "10.1109/TNNLS.2015.2466686", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. Similarity-based\nanomaly detection algorithms detect abnormally large amounts of similarity or\ndissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between\na test sample and the training samples. In many application domains there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such cases, multiple dissimilarity measures can be defined,\nincluding non-metric measures, and one can test for anomalies by scalarizing\nusing a non-negative linear combination of them. If the relative importance of\nthe different dissimilarity measures are not known in advance, as in many\nanomaly detection applications, the anomaly detection algorithm may need to be\nexecuted multiple times with different choices of weights in the linear\ncombination. In this paper, we propose a method for similarity-based anomaly\ndetection using a novel multi-criteria dissimilarity measure, the Pareto depth.\nThe proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the\nconcept of Pareto optimality to detect anomalies under multiple criteria\nwithout having to run an algorithm multiple times with different choices of\nweights. The proposed PDA approach is provably better than using linear\ncombinations of the criteria and shows superior performance on experiments with\nsynthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 06:25:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1508.04904", "submitter": "Brendan Guillouet", "authors": "Philippe Besse (INSA Toulouse, IMT), Brendan Guillouet (IMT),\n  Jean-Michel Loubes, Royer Fran\\c{c}ois", "title": "Review and Perspective for Distance Based Trajectory Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the issue of clustering trajectories of geolocalized\nobservations. Using clustering technics based on the choice of a distance\nbetween the observations, we first provide a comprehensive review of the\ndifferent distances used in the literature to compare trajectories. Then based\non the limitations of these methods, we introduce a new distance : Symmetrized\nSegment-Path Distance (SSPD). We finally compare this new distance to the\nothers according to their corresponding clustering results obtained using both\nhierarchical clustering and affinity propagation methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 07:46:15 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Besse", "Philippe", "", "INSA Toulouse, IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Loubes", "Jean-Michel", ""], ["Fran\u00e7ois", "Royer", ""]]}, {"id": "1508.04906", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov (MAESTRO), Pavel Chebotarev, Alexey Mishenin", "title": "Semi-supervised Learning with Regularized Laplacian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a semi-supervised learning method based on the similarity graph and\nRegularizedLaplacian. We give convenient optimization formulation of the\nRegularized Laplacian method and establishits various properties. In\nparticular, we show that the kernel of the methodcan be interpreted in terms of\ndiscrete and continuous time random walks and possesses several\nimportantproperties of proximity measures. Both optimization and linear algebra\nmethods can be used for efficientcomputation of the classification functions.\nWe demonstrate on numerical examples that theRegularized Laplacian method is\ncompetitive with respect to the other state of the art semi-supervisedlearning\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:01:42 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Avrachenkov", "Konstantin", "", "MAESTRO"], ["Chebotarev", "Pavel", ""], ["Mishenin", "Alexey", ""]]}, {"id": "1508.04909", "submitter": "Alain Rakotomamonjy", "authors": "Alain Rakotomamonjy (LITIS), Gilles Gasso (LITIS)", "title": "Histogram of gradients of Time-Frequency Representations for Audio scene\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of audio scenes classification and\ncontributes to the state of the art by proposing a novel feature. We build this\nfeature by considering histogram of gradients (HOG) of time-frequency\nrepresentation of an audio scene. Contrarily to classical audio features like\nMFCC, we make the hypothesis that histogram of gradients are able to encode\nsome relevant informations in a time-frequency {representation:} namely, the\nlocal direction of variation (in time and frequency) of the signal spectral\npower. In addition, in order to gain more invariance and robustness, histogram\nof gradients are locally pooled. We have evaluated the relevance of {the novel\nfeature} by comparing its performances with state-of-the-art competitors, on\nseveral datasets, including a novel one that we provide, as part of our\ncontribution. This dataset, that we make publicly available, involves $19$\nclasses and contains about $900$ minutes of audio scene recording. We thus\nbelieve that it may be the next standard dataset for evaluating audio scene\nclassification algorithms. Our comparison results clearly show that our\nHOG-based features outperform its competitors\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:07:10 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Rakotomamonjy", "Alain", "", "LITIS"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1508.04912", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Francesco Orabona, Nicol\\`o Cesa-Bianchi", "title": "The ABACOC Algorithm: a Novel Approach for Nonparametric Classification\n  of Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream mining poses unique challenges to machine learning: predictive models\nare required to be scalable, incrementally trainable, must remain bounded in\nsize (even when the data stream is arbitrarily long), and be nonparametric in\norder to achieve high accuracy even in complex and dynamic environments.\nMoreover, the learning system must be parameterless ---traditional tuning\nmethods are problematic in streaming settings--- and avoid requiring prior\nknowledge of the number of distinct class labels occurring in the stream. In\nthis paper, we introduce a new algorithmic approach for nonparametric learning\nin data streams. Our approach addresses all above mentioned challenges by\nlearning a model that covers the input space using simple local classifiers.\nThe distribution of these classifiers dynamically adapts to the local (unknown)\ncomplexity of the classification problem, thus achieving a good balance between\nmodel complexity and predictive accuracy. We design four variants of our\napproach of increasing adaptivity. By means of an extensive empirical\nevaluation against standard nonparametric baselines, we show state-of-the-art\nresults in terms of accuracy versus model size. For the variant that imposes a\nstrict bound on the model size, we show better performance against all other\nmethods measured at the same model size value. Our empirical analysis is\ncomplemented by a theoretical performance guarantee which does not rely on any\nstochastic assumption on the source generating the stream.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:15:08 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["De Rosa", "Rocco", ""], ["Orabona", "Francesco", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1508.04924", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Rabab Ward, Li Deng", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Volume: 64, Issue: 17, pp.\n  4504-4518, 2016", "doi": "10.1109/TSP.2016.2557301", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various studies that address the compressed sensing problem with Multiple\nMeasurement Vectors (MMVs) have been recently carried. These studies assume the\nvectors of the different channels to be jointly sparse. In this paper, we relax\nthis condition. Instead we assume that these sparse vectors depend on each\nother but that this dependency is unknown. We capture this dependency by\ncomputing the conditional probability of each entry in each vector being\nnon-zero, given the \"residuals\" of all previous vectors. To estimate these\nprobabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], a\ndata driven model for sequence modelling that is deep in time. To calculate the\nmodel parameters, we minimize a cross entropy cost function. To reconstruct the\nsparse vectors at the decoder, we propose a greedy solver that uses the above\nmodel to estimate the conditional probabilities. By performing extensive\nexperiments on two real world datasets, we show that the proposed method\nsignificantly outperforms the general MMV solver (the Simultaneous Orthogonal\nMatching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The\nproposed method does not add any complexity to the general compressive sensing\nencoder. The trained model is used just at the decoder. As the proposed method\nis a data driven method, it is only applicable when training data is available.\nIn many applications however, training data is indeed available, e.g. in\nrecorded images and videos.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:57:29 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 01:15:11 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 22:18:13 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Palangi", "Hamid", ""], ["Ward", "Rabab", ""], ["Deng", "Li", ""]]}, {"id": "1508.04945", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Manfei Liu", "title": "DeepWriterID: An End-to-end Online Text-independent Writer\n  Identification System", "comments": "7 pages5 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the rapid growth of touchscreen mobile terminals and pen-based\ninterfaces, handwriting-based writer identification systems are attracting\nincreasing attention for personal authentication, digital forensics, and other\napplications. However, most studies on writer identification have not been\nsatisfying because of the insufficiency of data and difficulty of designing\ngood features under various conditions of handwritings. Hence, we introduce an\nend-to-end system, namely DeepWriterID, employed a deep convolutional neural\nnetwork (CNN) to address these problems. A key feature of DeepWriterID is a new\nmethod we are proposing, called DropSegment. It designs to achieve data\naugmentation and improve the generalized applicability of CNN. For sufficient\nfeature representation, we further introduce path signature feature maps to\nimprove performance. Experiments were conducted on the NLPR handwriting\ndatabase. Even though we only use pen-position information in the pen-down\nstate of the given handwriting samples, we achieved new state-of-the-art\nidentification rates of 95.72% for Chinese text and 98.51% for English text.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:39:19 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 14:05:48 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Liu", "Manfei", ""]]}, {"id": "1508.04999", "submitter": "Juhan Nam", "authors": "Juhan Nam, Jorge Herrera, Kyogu Lee", "title": "A Deep Bag-of-Features Model for Music Auto-Tagging", "comments": "We resubmit a new version to revive the paper and record it as a\n  technical report. We did not add any incremental work to the previous work\n  but removed out some sections (criticized by a review process) and polished\n  sentences accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning and deep learning have drawn great attention in recent years\nas a way of transforming input data into more effective representations using\nlearning algorithms. Such interest has grown in the area of music information\nretrieval (MIR) as well, particularly in music audio classification tasks such\nas auto-tagging. In this paper, we present a two-stage learning model to\neffectively predict multiple labels from music audio. The first stage learns to\nproject local spectral patterns of an audio track onto a high-dimensional\nsparse space in an unsupervised manner and summarizes the audio track as a\nbag-of-features. The second stage successively performs the unsupervised\nlearning on the bag-of-features in a layer-by-layer manner to initialize a deep\nneural network and finally fine-tunes it with the tag labels. Through the\nexperiment, we rigorously examine training choices and tuning parameters, and\nshow that the model achieves high performance on Magnatagatune, a popularly\nused dataset in music auto-tagging.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 14:38:56 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 02:45:04 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 13:03:20 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Nam", "Juhan", ""], ["Herrera", "Jorge", ""], ["Lee", "Kyogu", ""]]}, {"id": "1508.05003", "submitter": "Suvrit Sra", "authors": "Suvrit Sra, Adams Wei Yu, Mu Li, Alexander J. Smola", "title": "AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed stochastic convex optimization under the delayed\ngradient model where the server nodes perform parameter updates, while the\nworker nodes compute stochastic gradients. We discuss, analyze, and experiment\nwith a setup motivated by the behavior of real-world distributed computation\nnetworks, where the machines are differently slow at different time. Therefore,\nwe allow the parameter updates to be sensitive to the actual delays\nexperienced, rather than to worst-case bounds on the maximum delay. This\nsensitivity leads to larger stepsizes, that can help gain rapid initial\nconvergence without having to wait too long for slower machines, while\nmaintaining the same asymptotic complexity. We obtain encouraging improvements\nto overall convergence for distributed experiments on real datasets with up to\nbillions of examples and features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 15:11:11 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Sra", "Suvrit", ""], ["Yu", "Adams Wei", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1508.05128", "submitter": "Ondrej Kuzelka", "authors": "Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka", "title": "Lifted Relational Neural Networks", "comments": "Expanded section on weight learning, added explanation of\n  relationship to convolutional neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method combining relational-logic representations with neural\nnetwork learning. A general lifted architecture, possibly reflecting some\nbackground domain knowledge, is described through relational rules which may be\nhandcrafted or learned. The relational rule-set serves as a template for\nunfolding possibly deep neural networks whose structures also reflect the\nstructures of given training or testing relational examples. Different networks\ncorresponding to different examples share their weights, which co-evolve during\ntraining by stochastic gradient descent algorithm. The framework allows for\nhierarchical relational modeling constructs and learning of latent relational\nconcepts through shared hidden layers weights corresponding to the rules.\nDiscovery of notable relational concepts and experiments on 78 relational\nlearning benchmarks demonstrate favorable performance of the method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:18:25 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 12:55:45 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Sourek", "Gustav", ""], ["Aschenbrenner", "Vojtech", ""], ["Zelezny", "Filip", ""], ["Kuzelka", "Ondrej", ""]]}, {"id": "1508.05133", "submitter": "Tamir Hazan", "authors": "Tamir Hazan and Tommi Jaakkola", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep neural networks exhibit impressive results on practical\nproblems. These networks generalize well although their inherent capacity may\nextend significantly beyond the number of training examples. We analyze this\nbehavior in the context of deep, infinite neural networks. We show that deep\ninfinite layers are naturally aligned with Gaussian processes and kernel\nmethods, and devise stochastic kernels that encode the information of these\nnetworks. We show that stability results apply despite the size, offering an\nexplanation for their empirical success.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 21:35:52 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 18:27:36 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Hazan", "Tamir", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.05170", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Alexander Rakhlin, Karthik Sridharan", "title": "Adaptive Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for studying adaptive regret bounds in the\nonline learning framework, including model selection bounds and data-dependent\nbounds. Given a data- or model-dependent bound we ask, \"Does there exist some\nalgorithm achieving this bound?\" We show that modifications to recently\nintroduced sequential complexity measures can be used to answer this question\nby providing sufficient conditions under which adaptive rates can be achieved.\nIn particular each adaptive rate induces a set of so-called offset complexity\nmeasures, and obtaining small upper bounds on these quantities is sufficient to\ndemonstrate achievability. A cornerstone of our analysis technique is the use\nof one-sided tail inequalities to bound suprema of offset random processes.\n  Our framework recovers and improves a wide variety of adaptive bounds\nincluding quantile bounds, second-order data-dependent bounds, and small loss\nbounds. In addition we derive a new type of adaptive bound for online linear\noptimization based on the spectral norm, as well as a new online PAC-Bayes\ntheorem that holds for countably infinite sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 03:44:43 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 02:49:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1508.05243", "submitter": "Mario Lucic", "authors": "Mario Lucic, Olivier Bachem, Andreas Krause", "title": "Strong Coresets for Hard and Soft Bregman Clustering with Applications\n  to Exponential Family Mixtures", "comments": "14 pages, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are efficient representations of data sets such that models trained\non the coreset are provably competitive with models trained on the original\ndata set. As such, they have been successfully used to scale up clustering\nmodels such as K-Means and Gaussian mixture models to massive data sets.\nHowever, until now, the algorithms and the corresponding theory were usually\nspecific to each clustering problem.\n  We propose a single, practical algorithm to construct strong coresets for a\nlarge class of hard and soft clustering problems based on Bregman divergences.\nThis class includes hard clustering with popular distortion measures such as\nthe Squared Euclidean distance, the Mahalanobis distance, KL-divergence and\nItakura-Saito distance. The corresponding soft clustering problems are directly\nrelated to popular mixture models due to a dual relationship between Bregman\ndivergences and Exponential family distributions. Our theoretical results\nfurther imply a randomized polynomial-time approximation scheme for hard\nclustering. We demonstrate the practicality of the proposed algorithm in an\nempirical evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 11:31:04 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 15:11:23 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Lucic", "Mario", ""], ["Bachem", "Olivier", ""], ["Krause", "Andreas", ""]]}, {"id": "1508.05288", "submitter": "Martin Spanknebel", "authors": "Martin Spanknebel and Klaus Pawelzik", "title": "Dynamics of Human Cooperation in Economic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.GT cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human decision behaviour is quite diverse. In many games humans on average do\nnot achieve maximal payoff and the behaviour of individual players remains\ninhomogeneous even after playing many rounds. For instance, in repeated\nprisoner dilemma games humans do not always optimize their mean reward and\nfrequently exhibit broad distributions of cooperativity. The reasons for these\nfailures of maximization are not known. Here we show that the dynamics\nresulting from the tendency to shift choice probabilities towards previously\nrewarding choices in closed loop interaction with the strategy of the opponent\ncan not only explain systematic deviations from 'rationality', but also\nreproduce the diversity of choice behaviours. As a representative example we\ninvestigate the dynamics of choice probabilities in prisoner dilemma games with\nopponents using strategies with different degrees of extortion and generosity.\nWe find that already a simple model for human learning can account for a\nsurprisingly wide range of human decision behaviours. It reproduces suppression\nof cooperation against extortionists and increasing cooperation when playing\nwith generous opponents, explains the broad distributions of individual choices\nin ensembles of players, and predicts the evolution of individual subjects'\ncooperation rates over the course of the games. We conclude that important\naspects of human decision behaviours are rooted in elementary learning\nmechanisms realised in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 14:49:44 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 10:32:13 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 11:48:57 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Spanknebel", "Martin", ""], ["Pawelzik", "Klaus", ""]]}, {"id": "1508.05367", "submitter": "Sebasti\\'an Basterrech", "authors": "Andrea Mesa, Sebasti\\'an Basterrech, Gustavo Guerberoff, Fernando\n  Alvarez-Valin", "title": "Hidden Markov Models for Gene Sequence Classification: Classifying the\n  VSG genes in the Trypanosoma brucei Genome", "comments": "Accepted article in July, 2015 in Pattern Analysis and Applications,\n  Springer. The article contains 23 pages, 4 figures, 8 tables and 51\n  references", "journal-ref": null, "doi": "10.1007/s10044-015-0508-9", "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents an application of Hidden Markov Models (HMMs) for\npattern recognition on genome sequences. We apply HMM for identifying genes\nencoding the Variant Surface Glycoprotein (VSG) in the genomes of Trypanosoma\nbrucei (T. brucei) and other African trypanosomes. These are parasitic protozoa\ncausative agents of sleeping sickness and several diseases in domestic and wild\nanimals. These parasites have a peculiar strategy to evade the host's immune\nsystem that consists in periodically changing their predominant cellular\nsurface protein (VSG). The motivation for using patterns recognition methods to\nidentify these genes, instead of traditional homology based ones, is that the\nlevels of sequence identity (amino acid and DNA sequence) amongst these genes\nis often below of what is considered reliable in these methods. Among pattern\nrecognition approaches, HMM are particularly suitable to tackle this problem\nbecause they can handle more naturally the determination of gene edges. We\nevaluate the performance of the model using different number of states in the\nMarkov model, as well as several performance metrics. The model is applied\nusing public genomic data. Our empirical results show that the VSG genes on T.\nbrucei can be safely identified (high sensitivity and low rate of false\npositives) using HMM.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 14:57:09 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 19:39:43 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Mesa", "Andrea", ""], ["Basterrech", "Sebasti\u00e1n", ""], ["Guerberoff", "Gustavo", ""], ["Alvarez-Valin", "Fernando", ""]]}, {"id": "1508.05463", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, and Alexander Wong", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks is a branch in machine learning that has seen a meteoric\nrise in popularity due to its powerful abilities to represent and model\nhigh-level abstractions in highly complex data. One area in deep neural\nnetworks that is ripe for exploration is neural connectivity formation. A\npivotal study on the brain tissue of rats found that synaptic formation for\nspecific functional connectivity in neocortical neural microcircuits can be\nsurprisingly well modeled and predicted as a random formation. Motivated by\nthis intriguing finding, we introduce the concept of StochasticNet, where deep\nneural networks are formed via stochastic connectivity between neurons. As a\nresult, any type of deep neural networks can be formed as a StochasticNet by\nallowing the neuron connectivity to be stochastic. Stochastic synaptic\nformations, in a deep neural network architecture, can allow for efficient\nutilization of neurons for performing specific tasks. To evaluate the\nfeasibility of such a deep neural network architecture, we train a\nStochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and\nSTL-10). Experimental results show that a StochasticNet, using less than half\nthe number of neural connections as a conventional deep neural network,\nachieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and\nSVHN dataset. Interestingly, StochasticNet with less than half the number of\nneural connections, achieved a higher accuracy (relative improvement in test\nerror rate of ~6% compared to ConvNet) on the STL-10 dataset than a\nconventional deep neural network. Finally, StochasticNets have faster\noperational speeds while achieving better or similar accuracy performances.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 03:36:43 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 19:05:03 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 01:34:17 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 20:30:05 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wong", "Alexander", ""]]}, {"id": "1508.05508", "submitter": "Baolin Peng", "authors": "Baolin Peng, Zhengdong Lu, Hang Li and Kam-Fai Wong", "title": "Towards Neural Network-based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:15:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Peng", "Baolin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1508.05514", "submitter": "Tohid Ardeshiri", "authors": "Tohid Ardeshiri, Umut Orguner, Emre \\\"Ozkan", "title": "Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a greedy mixture reduction algorithm which is capable of pruning\nmixture components as well as merging them based on the Kullback-Leibler\ndivergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD\nbased method since it is not restricted to merging operations. The capability\nof pruning (in addition to merging) gives the algorithm the ability of\npreserving the peaks of the original mixture during the reduction. Analytical\napproximations are derived to circumvent the computational intractability of\nthe KLD which results in a computationally efficient method. The proposed\nalgorithm is compared with Runnalls' and Williams' methods in two numerical\nexamples, using both simulated and real world data. The results indicate that\nthe performance and computational complexity of the proposed approach make it\nan efficient alternative to existing mixture reduction methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:41:17 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ardeshiri", "Tohid", ""], ["Orguner", "Umut", ""], ["\u00d6zkan", "Emre", ""]]}, {"id": "1508.05550", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum, Arie Yeredor, Moshe Salhov, Amir Averbuch", "title": "MultiView Diffusion Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we address the challenging task of achieving multi-view\ndimensionality reduction. The goal is to effectively use the availability of\nmultiple views for extracting a coherent low-dimensional representation of the\ndata. The proposed method exploits the intrinsic relation within each view, as\nwell as the mutual relations between views. The multi-view dimensionality\nreduction is achieved by defining a cross-view model in which an implied random\nwalk process is restrained to hop between objects in the different views. The\nmethod is robust to scaling and insensitive to small structural changes in the\ndata. We define new diffusion distances and analyze the spectra of the proposed\nkernel. We show that the proposed framework is useful for various machine\nlearning applications such as clustering, classification, and manifold\nlearning. Finally, by fusing multi-sensor seismic data we present a method for\nautomatic identification of seismic events.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 00:20:17 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 16:48:36 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 08:41:28 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2016 09:01:14 GMT"}, {"version": "v5", "created": "Mon, 11 Jul 2016 16:20:04 GMT"}, {"version": "v6", "created": "Mon, 6 Mar 2017 23:28:34 GMT"}, {"version": "v7", "created": "Wed, 5 Jun 2019 02:19:15 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Yeredor", "Arie", ""], ["Salhov", "Moshe", ""], ["Averbuch", "Amir", ""]]}, {"id": "1508.05565", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm\n  for Separable Topic Discovery", "comments": "Typo corrected; Revised argument in Lemma 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 03:44:26 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 18:26:33 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1508.05581", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Jiale Cao, and Xuelong Li", "title": "Learning Sampling Distributions for Efficient Object Detection", "comments": "14 pages, 13 figures", "journal-ref": "IEEE Transactions on Cybernetics (2016)", "doi": "10.1109/TCYB.2015.2508603", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important task in computer vision and learning\nsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is an\nalgorithm of fast and accurate object detection. By sampling particle windows\nfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.\nDespite its success, it is unknown how to determine the number of stages and\nthe number of particle windows in each stage. Moreover, it has to generate too\nmany particle windows in the initialization step and it redraws unnecessary too\nmany particle windows around object-like regions. In this paper, we attempt to\nsolve the problems of MPW. An important fact we used is that there is large\nprobability for a randomly generated particle window not to contain the object\nbecause the object is a sparse event relevant to the huge number of candidate\nwindows. Therefore, we design the proposal distribution so as to efficiently\nreject the huge number of non-object windows. Specifically, we propose the\nconcepts of rejection, acceptance, and ambiguity windows and regions. This\ncontrasts to MPW which utilizes only on region of support. The PD of MPW is\nacceptance-oriented whereas the PD of our method (called iPW) is\nrejection-oriented. Experimental results on human and face detection\ndemonstrate the efficiency and effectiveness of the iPW algorithm. The source\ncode is publicly accessible.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 09:17:49 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 12:52:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Pang", "Yanwei", ""], ["Cao", "Jiale", ""], ["Li", "Xuelong", ""]]}, {"id": "1508.05608", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several sources (arms) of items (rewards), and interested in finding the\nbest item overall. At each time step the agent chooses an arm, and obtains a\nrandom real valued reward. The rewards of each arm are assumed to be i.i.d.,\nwith an unknown probability distribution that generally differs among the arms.\nUnder the PAC framework, we provide lower bounds on the sample complexity of\nany $(\\epsilon,\\delta)$-correct algorithm, and propose algorithms that attain\nthis bound up to logarithmic factors. We compare the performance of this\nmulti-arm algorithms to the variant in which the arms are not distinguishable\nby the agent and are chosen randomly at each stage. Interestingly, when the\nmaximal rewards of the arms happen to be similar, the latter approach may\nprovide better performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 13:38:15 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1508.05711", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao and Wu-Jun Li", "title": "Fast Asynchronous Parallel Stochastic Gradient Decent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent~(SGD) and its variants have become more and more\npopular in machine learning due to their efficiency and effectiveness. To\nhandle large-scale problems, researchers have recently proposed several\nparallel SGD methods for multicore systems. However, existing parallel SGD\nmethods cannot achieve satisfactory performance in real applications. In this\npaper, we propose a fast asynchronous parallel SGD method, called AsySVRG, by\ndesigning an asynchronous strategy to parallelize the recently proposed SGD\nvariant called stochastic variance reduced gradient~(SVRG). Both theoretical\nand empirical results show that AsySVRG can outperform existing\nstate-of-the-art parallel SGD methods like Hogwild! in terms of convergence\nrate and computation cost.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 07:51:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1508.05803", "submitter": "Felipe Llinares", "authors": "Felipe Llinares-Lopez, Laetitia Papaxanthos, Dean Bodenham, Karsten\n  Borgwardt", "title": "Searching for significant patterns in stratified data", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant pattern mining, the problem of finding itemsets that are\nsignificantly enriched in one class of objects, is statistically challenging,\nas the large space of candidate patterns leads to an enormous multiple testing\nproblem. Recently, the concept of testability was proposed as one approach to\ncorrect for multiple testing in pattern mining while retaining statistical\npower. Still, these strategies based on testability do not allow one to\ncondition the test of significance on the observed covariates, which severely\nlimits its utility in biomedical applications. Here we propose a strategy and\nan efficient algorithm to perform significant pattern mining in the presence of\ncategorical covariates with K states.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 13:53:06 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Llinares-Lopez", "Felipe", ""], ["Papaxanthos", "Laetitia", ""], ["Bodenham", "Dean", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1508.05873", "submitter": "Jie Chen", "authors": "Jingen Ni, Jian Yang, Jie Chen, C\\'edric Richard, Jos\\'e Carlos M.\n  Bermudez", "title": "Stochastic Behavior of the Nonnegative Least Mean Fourth Algorithm for\n  Stationary Gaussian Inputs and Slow Learning", "comments": "11 pages, 8 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some system identification problems impose nonnegativity constraints on the\nparameters to estimate due to inherent physical characteristics of the unknown\nsystem. The nonnegative least-mean-square (NNLMS) algorithm and its variants\nallow to address this problem in an online manner. A nonnegative least mean\nfourth (NNLMF) algorithm has been recently proposed to improve the performance\nof these algorithms in cases where the measurement noise is not Gaussian. This\npaper provides a first theoretical analysis of the stochastic behavior of the\nNNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation\nresults illustrate the accuracy of the proposed analysis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 16:26:38 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ni", "Jingen", ""], ["Yang", "Jian", ""], ["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""], ["Bermudez", "Jos\u00e9 Carlos M.", ""]]}, {"id": "1508.06013", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani, Leopoldo Bertossi and Nikolaos Vasiloglou", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution", "comments": "To appear in Proc. SUM, 2015", "journal-ref": "Proc. SUM'15, 2015, Springer LNAI 9310, pp. 399-414", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called matching dependencies (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating three\ncomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairs\nbuilt using machine learning (ML) techniques, (b) MDs for supporting both the\nblocking phase of ML and the merge itself; and (c) The use of the declarative\nlanguage LogiQL -an extended form of Datalog supported by the LogicBlox\nplatform- for data processing, and the specification and enforcement of MDs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 02:35:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1508.06091", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), Romaric Gaudel (SEQUEL), Stephan Clemencon\n  (LTCI)", "title": "AUC Optimisation and Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recommendation systems, one is interested in the ranking of the predicted\nitems as opposed to other losses such as the mean squared error. Although a\nvariety of ways to evaluate rankings exist in the literature, here we focus on\nthe Area Under the ROC Curve (AUC) as it widely used and has a strong\ntheoretical underpinning. In practical recommendation, only items at the top of\nthe ranked list are presented to the users. With this in mind, we propose a\nclass of objective functions over matrix factorisations which primarily\nrepresent a smooth surrogate for the real AUC, and in a special case we show\nhow to prioritise the top of the list. The objectives are differentiable and\noptimised through a carefully designed stochastic gradient-descent-based\nalgorithm which scales linearly with the size of the data. In the special case\nof square loss we show how to improve computational complexity by leveraging\npreviously computed measures. To understand theoretically the underlying matrix\nfactorisation approaches we study both the consistency of the loss functions\nwith respect to AUC, and generalisation using Rademacher theory. The resulting\ngeneralisation analysis gives strong motivation for the optimisation under\nstudy. Finally, we provide computation results as to the efficacy of the\nproposed method using synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 09:46:09 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Gaudel", "Romaric", "", "SEQUEL"], ["Clemencon", "Stephan", "", "LTCI"]]}, {"id": "1508.06092", "submitter": "Rossella Cancelliere", "authors": "R. Cancelliere and R. Deluca and M. Gai and P. Gallinari and L. Rubini", "title": "An analysis of numerical issues in neural training by pseudoinversion", "comments": "11 pages, submitted to: Comp. Appl. Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some novel strategies have recently been proposed for single hidden layer\nneural network training that set randomly the weights from input to hidden\nlayer, while weights from hidden to output layer are analytically determined by\npseudoinversion. These techniques are gaining popularity in spite of their\nknown numerical issues when singular and/or almost singular matrices are\ninvolved. In this paper we discuss a critical use of Singular Value Analysis\nfor identification of these drawbacks and we propose an original use of\nregularisation to determine the output weights, based on the concept of\ncritical hidden layer size. This approach also allows to limit the training\ncomputational effort. Besides, we introduce a novel technique which relies an\neffective determination of input weights to the hidden layer dimension. This\napproach is tested for both regression and classification tasks, resulting in a\nsignificant performance improvement with respect to alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 09:51:35 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cancelliere", "R.", ""], ["Deluca", "R.", ""], ["Gai", "M.", ""], ["Gallinari", "P.", ""], ["Rubini", "L.", ""]]}, {"id": "1508.06095", "submitter": "Rossella Cancelliere", "authors": "Rossella Cancelliere, Mario Gai, Patrick Gallinari, Luca Rubini", "title": "OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based\n  Neural Training", "comments": "Published on Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.015", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the training of single hidden layer neural networks\nby pseudoinversion, which, in spite of its popularity, is sometimes affected by\nnumerical instability issues. Regularization is known to be effective in such\ncases, so that we introduce, in the framework of Tikhonov regularization, a\nmatricial reformulation of the problem which allows us to use the condition\nnumber as a diagnostic tool for identification of instability. By imposing\nwell-conditioning requirements on the relevant matrices, our theoretical\nanalysis allows the identification of an optimal value for the regularization\nparameter from the standpoint of stability. We compare with the value derived\nby cross-validation for overfitting control and optimisation of the\ngeneralization performance. We test our method for both regression and\nclassification tasks. The proposed method is quite effective in terms of\npredictivity, often with some improvement on performance with respect to the\nreference cases considered. This approach, due to analytical determination of\nthe regularization parameter, dramatically reduces the computational load\nrequired by many other techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 10:09:31 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Cancelliere", "Rossella", ""], ["Gai", "Mario", ""], ["Gallinari", "Patrick", ""], ["Rubini", "Luca", ""]]}, {"id": "1508.06161", "submitter": "Scott Bronikowski", "authors": "Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, and Jeffrey\n  Mark Siskind", "title": "Robot Language Learning, Generation, and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework which supports grounding natural-language\nsemantics in robotic driving. This framework supports acquisition (learning\ngrounded meanings of nouns and prepositions from human annotation of robotic\ndriving paths), generation (using such acquired meanings to generate sentential\ndescription of new robotic driving paths), and comprehension (using such\nacquired meanings to support automated driving to accomplish navigational goals\nspecified in natural language). We evaluate the performance of these three\ntasks by having independent human judges rate the semantic fidelity of the\nsentences associated with paths, achieving overall average correctness of 94.6%\nand overall average completeness of 85.6%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:10:21 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Bronikowski", "Scott Alan", ""], ["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1508.06235", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang", "title": "Clustering With Side Information: From a Probabilistic Model to a\n  Deterministic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model-based clustering method (TVClust) that\nrobustly incorporates noisy side information as soft-constraints and aims to\nseek a consensus between side information and the observed data. Our method is\nbased on a nonparametric Bayesian hierarchical model that combines the\nprobabilistic model for the data instance and the one for the side-information.\nAn efficient Gibbs sampling algorithm is proposed for posterior inference.\nUsing the small-variance asymptotics of our probabilistic model, we then derive\na new deterministic clustering algorithm (RDP-means). It can be viewed as an\nextension of K-means that allows for the inclusion of side information and has\nthe additional property that the number of clusters does not need to be\nspecified a priori. Empirical studies have been carried out to compare our work\nwith many constrained clustering algorithms from the literature on both a\nvariety of data sets and under a variety of conditions such as using noisy side\ninformation and erroneous k values. The results of our experiments show strong\nresults for our probabilistic and deterministic approaches under these\nconditions when compared to other algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 18:13:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 17:46:36 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 17:48:54 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 05:38:15 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Khashabi", "Daniel", ""], ["Wieting", "John", ""], ["Liu", "Jeffrey Yufei", ""], ["Liang", "Feng", ""]]}, {"id": "1508.06264", "submitter": "Jingbin Wang", "authors": "Jingbin Wang, Haoxiang Wang, Yihua Zhou, Nancy McDonald", "title": "Multiple kernel multivariate performance learning using cutting plane\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-kernel classifier learning algorithm to\noptimize a given nonlinear and nonsmoonth multivariate classifier performance\nmeasure. Moreover, to solve the problem of kernel function selection and kernel\nparameter tuning, we proposed to construct an optimal kernel by weighted linear\ncombination of some candidate kernels. The learning of the classifier parameter\nand the kernel weight are unified in a single objective function considering to\nminimize the upper boundary of the given multivariate performance measure. The\nobjective function is optimized with regard to classifier parameter and kernel\nweight alternately in an iterative algorithm by using cutting plane algorithm.\nThe developed algorithm is evaluated on two different pattern classification\nmethods with regard to various multivariate performance measure optimization\nproblems. The experiment results show the proposed algorithm outperforms the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 19:45:06 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Wang", "Jingbin", ""], ["Wang", "Haoxiang", ""], ["Zhou", "Yihua", ""], ["McDonald", "Nancy", ""]]}, {"id": "1508.06336", "submitter": "Xiao Li", "authors": "Xiao Li, Joseph K. Bradley, Sameer Pawar, Kannan Ramchandran", "title": "SPRIGHT: A Fast and Robust Framework for Sparse Walsh-Hadamard Transform", "comments": "Part of our results was reported in ISIT 2014, titled \"The SPRIGHT\n  algorithm for robust sparse Hadamard Transforms.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the Walsh-Hadamard Transform (WHT) of\nsome $N$-length input vector in the presence of noise, where the $N$-point\nWalsh spectrum is $K$-sparse with $K = {O}(N^{\\delta})$ scaling sub-linearly in\nthe input dimension $N$ for some $0<\\delta<1$. Over the past decade, there has\nbeen a resurgence in research related to the computation of Discrete Fourier\nTransform (DFT) for some length-$N$ input signal that has a $K$-sparse Fourier\nspectrum. In particular, through a sparse-graph code design, our earlier work\non the Fast Fourier Aliasing-based Sparse Transform (FFAST) algorithm computes\nthe $K$-sparse DFT in time ${O}(K\\log K)$ by taking ${O}(K)$ noiseless samples.\nInspired by the coding-theoretic design framework, Scheibler et al. proposed\nthe Sparse Fast Hadamard Transform (SparseFHT) algorithm that elegantly\ncomputes the $K$-sparse WHT in the absence of noise using ${O}(K\\log N)$\nsamples in time ${O}(K\\log^2 N)$. However, the SparseFHT algorithm explicitly\nexploits the noiseless nature of the problem, and is not equipped to deal with\nscenarios where the observations are corrupted by noise. Therefore, a question\nof critical interest is whether this coding-theoretic framework can be made\nrobust to noise. Further, if the answer is yes, what is the extra price that\nneeds to be paid for being robust to noise? In this paper, we show, quite\ninterestingly, that there is {\\it no extra price} that needs to be paid for\nbeing robust to noise other than a constant factor. In other words, we can\nmaintain the same sample complexity ${O}(K\\log N)$ and the computational\ncomplexity ${O}(K\\log^2 N)$ as those of the noiseless case, using our SParse\nRobust Iterative Graph-based Hadamard Transform (SPRIGHT) algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 00:34:30 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Li", "Xiao", ""], ["Bradley", "Joseph K.", ""], ["Pawar", "Sameer", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1508.06388", "submitter": "Mu Qiao", "authors": "Mu Qiao and Jia Li", "title": "Gaussian Mixture Models with Component Means Constrained in Pre-selected\n  Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a Gaussian mixture model (GMM) with component means\nconstrained in a pre-selected subspace. Applications to classification and\nclustering are explored. An EM-type estimation algorithm is derived. We prove\nthat the subspace containing the component means of a GMM with a common\ncovariance matrix also contains the modes of the density and the class means.\nThis motivates us to find a subspace by applying weighted principal component\nanalysis to the modes of a kernel density and the class means. To circumvent\nthe difficulty of deciding the kernel bandwidth, we acquire multiple subspaces\nfrom the kernel densities based on a sequence of bandwidths. The GMM\nconstrained by each subspace is estimated; and the model yielding the maximum\nlikelihood is chosen. A dimension reduction property is proved in the sense of\nbeing informative for classification or clustering. Experiments on real and\nsimulated data sets are conducted to examine several ways of determining the\nsubspace and to compare with the reduced rank mixture discriminant analysis\n(MDA). Our new method with the simple technique of spanning the subspace only\nby class means often outperforms the reduced rank MDA when the subspace\ndimension is very low, making it particularly appealing for visualization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 07:25:22 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Qiao", "Mu", ""], ["Li", "Jia", ""]]}, {"id": "1508.06446", "submitter": "Lavanya Sita Tekumalla", "authors": "Lavanya Sita Tekumalla, Priyanka Agrawal, Indrajit Bhattacharya", "title": "Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric\n  Admixture Modeling", "comments": "Proceedings of European Conference of Machine Learning (ECML) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixture\nmodeling, where the number of mixture components grows with the number of data\nitems. The Hierarchical Dirichlet Process (HDP), is an extension of DP for\ngrouped data, often used for non-parametric topic modeling, where each group is\na mixture over shared mixture densities. The Nested Dirichlet Process (nDP), on\nthe other hand, is an extension of the DP for learning group level\ndistributions from data, simultaneously clustering the groups. It allows group\nlevel distributions to be shared across groups in a non-parametric setting,\nleading to a non-parametric mixture of mixtures. The nCRF extends the nDP for\nmultilevel non-parametric mixture modeling, enabling modeling topic\nhierarchies. However, the nDP and nCRF do not allow sharing of distributions as\nrequired in many applications, motivating the need for multi-level\nnon-parametric admixture modeling. We address this gap by proposing multi-level\nnested HDPs (nHDP) where the base distribution of the HDP is itself a HDP at\neach level thereby leading to admixtures of admixtures at each level. Because\nof couplings between various HDP levels, scaling up is naturally a challenge\nduring inference. We propose a multi-level nested Chinese Restaurant Franchise\n(nCRF) representation for the nested HDP, with which we outline an inference\nalgorithm based on Gibbs Sampling. We evaluate our model with the two level\nnHDP for non-parametric entity topic modeling where an inner HDP creates a\ncountably infinite set of topic mixtures and associates them with author\nentities, while an outer HDP associates documents with these author entities.\nIn our experiments on two real world research corpora, the nHDP is able to\ngeneralize significantly better than existing models and detect missing author\nentities with a reasonable level of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 11:24:36 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 08:14:45 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Tekumalla", "Lavanya Sita", ""], ["Agrawal", "Priyanka", ""], ["Bhattacharya", "Indrajit", ""]]}, {"id": "1508.06477", "submitter": "Alain Rakotomamonjy", "authors": "A Rakotomamonjy (LITIS), S Ko\\c{c}o (QARMA), Liva Ralaivola (QARMA)", "title": "Greedy methods, randomization approaches and multi-arm bandit algorithms\n  for efficient sparsity-constrained optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several sparsity-constrained algorithms such as Orthogonal Matching Pursuit\nor the Frank-Wolfe algorithm with sparsity constraints work by iteratively\nselecting a novel atom to add to the current non-zero set of variables. This\nselection step is usually performed by computing the gradient and then by\nlooking for the gradient component with maximal absolute entry. This step can\nbe computationally expensive especially for large-scale and high-dimensional\ndata. In this work, we aim at accelerating these sparsity-constrained\noptimization algorithms by exploiting the key observation that, for these\nalgorithms to work, one only needs the coordinate of the gradient's top entry.\nHence, we introduce algorithms based on greedy methods and randomization\napproaches that aim at cheaply estimating the gradient and its top entry.\nAnother of our contribution is to cast the problem of finding the best gradient\nentry as a best arm identification in a multi-armed bandit problem. Owing to\nthis novel insight, we are able to provide a bandit-based algorithm that\ndirectly estimates the top entry in a very efficient way. Theoretical\nobservations stating that the resulting inexact Frank-Wolfe or Orthogonal\nMatching Pursuit algorithms act, with high probability, similarly to their\nexact versions are also given. We have carried out several experiments showing\nthat the greedy deterministic and the bandit approaches we propose can achieve\nan acceleration of an order of magnitude while being as efficient as the exact\ngradient when used in algorithms such as OMP, Frank-Wolfe or CoSaMP.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 13:01:36 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 07:50:48 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Rakotomamonjy", "A", "", "LITIS"], ["Ko\u00e7o", "S", "", "QARMA"], ["Ralaivola", "Liva", "", "QARMA"]]}, {"id": "1508.06535", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Deep Convolutional Neural Networks for Smile Recognition", "comments": "MSc thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes the design and implementation of a smile detector based\non deep convolutional neural networks. It starts with a summary of neural\nnetworks, the difficulties of training them and new training methods, such as\nRestricted Boltzmann Machines or autoencoders. It then provides a literature\nreview of convolutional neural networks and recurrent neural networks. In order\nto select databases for smile recognition, comprehensive statistics of\ndatabases popular in the field of facial expression recognition were generated\nand are summarized in this thesis. It then proposes a model for smile\ndetection, of which the main part is implemented. The experimental results are\ndiscussed in this thesis and justified based on a comprehensive model selection\nperformed. All experiments were run on a Tesla K40c GPU benefiting from a\nspeedup of up to factor 10 over the computations on a CPU. A smile detection\ntest accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous\nFacial Action (DISFA) database, significantly outperforming existing approaches\nwith accuracies ranging from 65.55% to 79.67%. This experiment is re-run under\nvarious variations, such as retaining less neutral images or only the low or\nhigh intensities, of which the results are extensively compared.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:39:09 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1508.06574", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett, Pedro M. Esperan\\c{c}a, Chris C. Holmes", "title": "A review of homomorphic encryption and software tools for encrypted\n  statistical machine learning", "comments": "21 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cryptography promise to enable secure statistical\ncomputation on encrypted data, whereby a limited set of operations can be\ncarried out without the need to first decrypt. We review these homomorphic\nencryption schemes in a manner accessible to statisticians and machine\nlearners, focusing on pertinent limitations inherent in the current state of\nthe art. These limitations restrict the kind of statistics and machine learning\nalgorithms which can be implemented and we review those which have been\nsuccessfully applied in the literature. Finally, we document a high performance\nR package implementing a recent homomorphic scheme in a general framework.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:11:12 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1508.06585", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Towards universal neural nets: Gibbs machines and ACE", "comments": "v5: added thermodynamic identities and variational error estimation;\n  expanded references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study from a physics viewpoint a class of generative neural nets, Gibbs\nmachines, designed for gradual learning. While including variational\nauto-encoders, they offer a broader universal platform for incrementally adding\nnewly learned features, including physical symmetries. Their direct connection\nto statistical physics and information geometry is established. A variational\nPythagorean theorem justifies invoking the exponential/Gibbs class of\nprobabilities for creating brand new objects. Combining these nets with\nclassifiers, gives rise to a brand of universal generative neural nets -\nstochastic auto-classifier-encoders (ACE). ACE have state-of-the-art\nperformance in their class, both for classification and density estimation for\nthe MNIST data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:43:08 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 21:49:06 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 03:35:59 GMT"}, {"version": "v4", "created": "Fri, 8 Apr 2016 22:11:23 GMT"}, {"version": "v5", "created": "Thu, 30 Jun 2016 06:26:34 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1508.06717", "submitter": "Chandresh Maurya", "authors": "Chandresh Kumar Maurya, Durga Toshniwal, Gopalan Vijendran Venkoparao", "title": "Online Anomaly Detection via Class-Imbalance Learning", "comments": "This paper is accepted for publication in IC3 2015, Jaypee Noida", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is an important task in many real world applications such\nas fraud detection, suspicious activity detection, health care monitoring etc.\nIn this paper, we tackle this problem from supervised learning perspective in\nonline learning setting. We maximize well known \\emph{Gmean} metric for\nclass-imbalance learning in online learning framework. Specifically, we show\nthat maximizing \\emph{Gmean} is equivalent to minimizing a convex surrogate\nloss function and based on that we propose novel online learning algorithm for\nanomaly detection. We then show, by extensive experiments, that the performance\nof the proposed algorithm with respect to $sum$ metric is as good as a recently\nproposed Cost-Sensitive Online Classification(CSOC) algorithm for\nclass-imbalance learning over various benchmarked data sets while keeping\nrunning time close to the perception algorithm. Our another conclusion is that\nother competitive online algorithms do not perform consistently over data sets\nof varying size. This shows the potential applicability of our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 03:39:39 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Maurya", "Chandresh Kumar", ""], ["Toshniwal", "Durga", ""], ["Venkoparao", "Gopalan Vijendran", ""]]}, {"id": "1508.06845", "submitter": "Louis Aslett", "authors": "Louis J. M. Aslett, Pedro M. Esperan\\c{c}a, Chris C. Holmes", "title": "Encrypted statistical machine learning: new privacy preserving methods", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new statistical machine learning methods designed to learn on\nfully homomorphic encrypted (FHE) data. The introduction of FHE schemes\nfollowing Gentry (2009) opens up the prospect of privacy preserving statistical\nmachine learning analysis and modelling of encrypted data without compromising\nsecurity constraints. We propose tailored algorithms for applying extremely\nrandom forests, involving a new cryptographic stochastic fraction estimator,\nand na\\\"{i}ve Bayes, involving a semi-parametric model for the class decision\nboundary, and show how they can be used to learn and predict from encrypted\ndata. We demonstrate that these techniques perform competitively on a variety\nof classification data sets and provide detailed information about the\ncomputational practicalities of these and other FHE methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 13:06:55 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Esperan\u00e7a", "Pedro M.", ""], ["Holmes", "Chris C.", ""]]}, {"id": "1508.06901", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford", "title": "Compressive Sensing via Low-Rank Gaussian Mixture Models", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new compressive sensing (CS) inversion algorithm by utilizing\nthe Gaussian mixture model (GMM). While the compressive sensing is performed\nglobally on the entire image as implemented in our lensless camera, a low-rank\nGMM is imposed on the local image patches. This low-rank GMM is derived via\neigenvalue thresholding of the GMM trained on the projection of the measurement\ndata, thus learned {\\em in situ}. The GMM and the projection of the measurement\ndata are updated iteratively during the reconstruction. Our GMM algorithm\ndegrades to the piecewise linear estimator (PLE) if each patch is represented\nby a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also\ndeveloped for CS inversion, constituting an additional contribution of this\npaper. Extensive results on both simulation data and real data captured by the\nlensless camera demonstrate the efficacy of the proposed algorithm.\nFurthermore, we compare the CS reconstruction results using our algorithm with\nthe JPEG compression. Simulation results demonstrate that when limited\nbandwidth is available (a small number of measurements), our algorithm can\nachieve comparable results as JPEG.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:35:11 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul A.", ""]]}, {"id": "1508.06904", "submitter": "Markus Thom", "authors": "Markus Thom and Franz Gritschneder", "title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks", "comments": "Pages 1-16 only: Copyright (c) 2016 IEEE. Personal use is permitted,\n  but republication/redistribution requires IEEE permission", "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 5, pp.\n  1235-1250 (2017)", "doi": "10.1109/TSP.2016.2631454", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:50:26 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:49:52 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 12:18:13 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 17:38:00 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 13:23:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Thom", "Markus", ""], ["Gritschneder", "Franz", ""]]}, {"id": "1508.07091", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf and Rapha\\\"el Feraud", "title": "Multi-armed Bandit Problem with Known Trend", "comments": "Neurocomputing 2016. arXiv admin note: text overlap with\n  arXiv:0805.3415 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the multi-armed bandit model, which we call\nmulti-armed bandit problem with known trend, where the gambler knows the shape\nof the reward function of each arm but not its distribution. This new problem\nis motivated by different online problems like active learning, music and\ninterface recommendation applications, where when an arm is sampled by the\nmodel the received reward change according to a known trend. By adapting the\nstandard multi-armed bandit algorithm UCB1 to take advantage of this setting,\nwe propose the new algorithm named A-UCB that assumes a stochastic model. We\nprovide upper bounds of the regret which compare favourably with the ones of\nUCB1. We also confirm that experimentally with different simulations\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 04:35:28 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 00:24:58 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 18:05:59 GMT"}, {"version": "v4", "created": "Wed, 10 May 2017 19:06:56 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Feraud", "Rapha\u00ebl", ""]]}, {"id": "1508.07096", "submitter": "Yanping Huang", "authors": "Yanping Huang, Sai Zhang", "title": "Partitioning Large Scale Deep Belief Networks Using Dropout", "comments": "arXiv admin note: text overlap with arXiv:1207.0580 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 05:24:06 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Huang", "Yanping", ""], ["Zhang", "Sai", ""]]}, {"id": "1508.07103", "submitter": "Songlin Zhao", "authors": "Songlin Zhao", "title": "Regularized Kernel Recursive Least Square Algoirthm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most adaptive signal processing applications, system linearity is assumed\nand adaptive linear filters are thus used. The traditional class of supervised\nadaptive filters rely on error-correction learning for their adaptive\ncapability. The kernel method is a powerful nonparametric modeling tool for\npattern analysis and statistical signal processing. Through a nonlinear\nmapping, kernel methods transform the data into a set of points in a\nReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast\nconvergence rate in stationary scenario. However the good performance is\nobtained at a cost of high computation complexity. Sparsification in kernel\nmethods is know to related to less computational complexity and memory\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 06:04:37 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Zhao", "Songlin", ""]]}, {"id": "1508.07130", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Parallel Dither and Dropout for Regularising Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective regularisation during training can mean the difference between\nsuccess and failure for deep neural networks. Recently, dither has been\nsuggested as alternative to dropout for regularisation during batch-averaged\nstochastic gradient descent (SGD). In this article, we show that these methods\nfail without batch averaging and we introduce a new, parallel regularisation\nmethod that may be used without batch averaging. Our results for\nparallel-regularised non-batch-SGD are substantially better than what is\npossible with batch-SGD. Furthermore, our results demonstrate that dither and\ndropout are complimentary.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 08:50:18 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1508.07175", "submitter": "Zihao Wang", "authors": "Zihao Wang, Yiuming Cheung", "title": "Competitive and Penalized Clustering Auto-encoder", "comments": "The paper has been withdrawn since more effective experiments should\n  be completed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper has been withdrawn since more effective experiments should be\ncompleted.\n  Auto-encoders (AE) has been widely applied in different fields of machine\nlearning. However, as a deep model, there are a large amount of learnable\nparameters in the AE, which would cause over-fitting and slow learning speed in\npractice. Many researchers have been study the intrinsic structure of AE and\nshowed different useful methods to regularize those parameters. In this paper,\nwe present a novel regularization method based on a clustering algorithm which\nis able to classify the parameters into different groups. With this\nregularization, parameters in a given group have approximate equivalent values\nand over-fitting problem could be alleviated. Moreover, due to the competitive\nbehavior of clustering algorithm, this model also overcomes some intrinsic\nproblems of clustering algorithms like the determination of number of clusters.\nExperiments on handwritten digits recognition verify the effectiveness of our\nnovel model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 12:03:16 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 06:27:53 GMT"}, {"version": "v3", "created": "Sun, 12 Mar 2017 06:41:13 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wang", "Zihao", ""], ["Cheung", "Yiuming", ""]]}, {"id": "1508.07192", "submitter": "Niels Landwehr", "authors": "Matthias Bussas, Christoph Sawade, Tobias Scheffer and Niels Landwehr", "title": "Varying-coefficient models with isotropic Gaussian process priors", "comments": "17 pages, 4 Figures, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning problems in which the conditional distribution of the\noutput given the input varies as a function of additional task variables. In\nvarying-coefficient models with Gaussian process priors, a Gaussian process\ngenerates the functional relationship between the task variables and the\nparameters of this conditional. Varying-coefficient models subsume hierarchical\nBayesian multitask models, but also generalizations in which the conditional\nvaries continuously, for instance, in time or space. However, Bayesian\ninference in varying-coefficient models is generally intractable. We show that\ninference for varying-coefficient models with isotropic Gaussian process priors\nresolves to standard inference for a Gaussian process that can be solved\nefficiently. MAP inference in this model resolves to multitask learning using\ntask and instance kernels, and inference for hierarchical Bayesian multitask\nmodels can be carried out efficiently using graph-Laplacian kernels. We report\non experiments for geospatial prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 13:13:49 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 07:44:56 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Bussas", "Matthias", ""], ["Sawade", "Christoph", ""], ["Scheffer", "Tobias", ""], ["Landwehr", "Niels", ""]]}, {"id": "1508.07416", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou, Qibin Zhao, Yu Zhang, T\\\"ulay Adal{\\i}, Shengli Xie,\n  Andrzej Cichocki", "title": "Linked Component Analysis from Matrices to High Order Tensors:\n  Applications to Biomedical Data", "comments": "20 pages, 11 figures, Proceedings of the IEEE, 2015", "journal-ref": null, "doi": "10.1109/JPROC.2015.2474704", "report-no": null, "categories": "cs.CE cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of various sensor technologies, we now have\naccess to large amounts of multi-block (also called multi-set,\nmulti-relational, or multi-view) data that need to be jointly analyzed to\nexplore their latent connections. Various component analysis methods have\nplayed an increasingly important role for the analysis of such coupled data. In\nthis paper, we first provide a brief review of existing matrix-based (two-way)\ncomponent analysis methods for the joint analysis of such data with a focus on\nbiomedical applications. Then, we discuss their important extensions and\ngeneralization to multi-block multiway (tensor) data. We show how constrained\nmulti-block tensor decomposition methods are able to extract similar or\nstatistically dependent common features that are shared by all blocks, by\nincorporating the multiway nature of data. Special emphasis is given to the\nflexible common and individual feature analysis of multi-block data with the\naim to simultaneously extract common and individual latent components with\ndesired properties and types of diversity. Illustrative examples are given to\ndemonstrate their effectiveness for biomedical data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 08:18:14 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Zhou", "Guoxu", ""], ["Zhao", "Qibin", ""], ["Zhang", "Yu", ""], ["Adal\u0131", "T\u00fclay", ""], ["Xie", "Shengli", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1508.07551", "submitter": "Karim Awudu", "authors": "Awudu Karim and Shangbo Zhou", "title": "X-TREPAN: a multi class regression and adapted extraction of\n  comprehensible decision tree in artificial neural networks", "comments": "17 Pages, 8 Tables, 8 Figures, 6 Equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the TREPAN algorithm is enhanced and extended for extracting\ndecision trees from neural networks. We empirically evaluated the performance\nof the algorithm on a set of databases from real world events. This benchmark\nenhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree\ninduction algorithms to analyze the datasets. The models are then compared with\nX-TREPAN for comprehensibility and classification accuracy. Furthermore, we\nvalidate the experimentations by applying statistical methods. Finally, the\nmodified algorithm is extended to work with multi-class regression problems and\nthe ability to comprehend generalized feed forward networks is achieved.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 10:14:48 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Karim", "Awudu", ""], ["Zhou", "Shangbo", ""]]}, {"id": "1508.07630", "submitter": "Vural Aksakalli", "authors": "Vural Aksakalli and Milad Malekipirbazari", "title": "Feature Selection via Binary Simultaneous Perturbation Stochastic\n  Approximation", "comments": "This is the Istanbul Sehir University Technical Report\n  #SHR-ISE-2016.01. A short version of this report has been accepted for\n  publication at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": "SHR-ISE-2016.01", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection (FS) has become an indispensable task in dealing with\ntoday's highly complex pattern recognition problems with massive number of\nfeatures. In this study, we propose a new wrapper approach for FS based on\nbinary simultaneous perturbation stochastic approximation (BSPSA). This\npseudo-gradient descent stochastic algorithm starts with an initial feature\nvector and moves toward the optimal feature vector via successive iterations.\nIn each iteration, the current feature vector's individual components are\nperturbed simultaneously by random offsets from a qualified probability\ndistribution. We present computational experiments on datasets with numbers of\nfeatures ranging from a few dozens to thousands using three widely-used\nclassifiers as wrappers: nearest neighbor, decision tree, and linear support\nvector machine. We compare our methodology against the full set of features as\nwell as a binary genetic algorithm and sequential FS methods using\ncross-validated classification error rate and AUC as the performance criteria.\nOur results indicate that features selected by BSPSA compare favorably to\nalternative methods in general and BSPSA can yield superior feature sets for\ndatasets with tens of thousands of features by examining an extremely small\nfraction of the solution space. We are not aware of any other wrapper FS\nmethods that are computationally feasible with good convergence properties for\nsuch large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 20:03:53 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 08:02:42 GMT"}, {"version": "v3", "created": "Sat, 5 Mar 2016 19:42:14 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Aksakalli", "Vural", ""], ["Malekipirbazari", "Milad", ""]]}, {"id": "1508.07643", "submitter": "Marc Goessling", "authors": "Marc Goessling and Shan Kang", "title": "Directional Decision Lists", "comments": "IEEE Big Data for Advanced Manufacturing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel family of decision lists consisting of\nhighly interpretable models which can be learned efficiently in a greedy\nmanner. The defining property is that all rules are oriented in the same\ndirection. Particular examples of this family are decision lists with\nmonotonically decreasing (or increasing) probabilities. On simulated data we\nempirically confirm that the proposed model family is easier to train than\ngeneral decision lists. We exemplify the practical usability of our approach by\nidentifying problem symptoms in a manufacturing process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 23:00:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 16:41:16 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 19:38:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Goessling", "Marc", ""], ["Kang", "Shan", ""]]}, {"id": "1508.07680", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi", "title": "Domain Generalization for Object Recognition with Multi-task\n  Autoencoders", "comments": "accepted in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain generalization is to take knowledge acquired from a\nnumber of related domains where training data is available, and to then\nsuccessfully apply it to previously unseen domains. We propose a new feature\nlearning algorithm, Multi-Task Autoencoder (MTAE), that provides good\ngeneralization performance for cross-domain object recognition.\n  Our algorithm extends the standard denoising autoencoder framework by\nsubstituting artificially induced corruption with naturally occurring\ninter-domain variability in the appearance of objects. Instead of\nreconstructing images from noisy versions, MTAE learns to transform the\noriginal image into analogs in multiple related domains. It thereby learns\nfeatures that are robust to variations across domains. The learnt features are\nthen used as inputs to a classifier.\n  We evaluated the performance of the algorithm on benchmark image recognition\ndatasets, where the task is to learn features from multiple datasets and to\nthen predict the image label from unseen datasets. We found that (denoising)\nMTAE outperforms alternative autoencoder-based models as well as the current\nstate-of-the-art algorithms for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:15:31 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""]]}, {"id": "1508.07709", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Gertjan van Noord and Ivan Titov", "title": "Word Representations, Tree Models and Syntactic Functions", "comments": "Add github code repository link. Fix equation 4.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word representations induced from models with discrete latent variables\n(e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this\nwork, we exploit labeled syntactic dependency trees and formalize the induction\nproblem as unsupervised learning of tree-structured hidden Markov models.\nSyntactic functions are used as additional observed variables in the model,\ninfluencing both transition and emission components. Such syntactic information\ncan potentially lead to capturing more fine-grain and functional distinctions\nbetween words, which, in turn, may be desirable in many NLP applications. We\nevaluate the word representations on two tasks -- named entity recognition and\nsemantic frame identification. We observe improvements from exploiting\nsyntactic function information in both cases, and the results rivaling those of\nstate-of-the-art representation learning methods. Additionally, we revisit the\nrelationship between sequential and unlabeled-tree models and find that the\nadvantage of the latter is not self-evident.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 07:52:50 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 13:26:56 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["\u0160uster", "Simon", ""], ["van Noord", "Gertjan", ""], ["Titov", "Ivan", ""]]}, {"id": "1508.07933", "submitter": "Maxim Raginsky", "authors": "Soomin Lee, Angelia Nedi\\'c, Maxim Raginsky", "title": "Coordinate Dual Averaging for Decentralized Online Optimization with\n  Nonseparable Global Objectives", "comments": "10 pages; accepted for publication in IEEE Transactions on Control of\n  Network Systems", "journal-ref": null, "doi": "10.1109/TCNS.2016.2573639", "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized online convex optimization problem in a network\nof agents, where each agent controls only a coordinate (or a part) of the\nglobal decision vector. For such a problem, we propose two decentralized\nvariants (ODA-C and ODA-PS) of Nesterov's primal-dual algorithm with dual\naveraging. In ODA-C, to mitigate the disagreements on the primal-vector\nupdates, the agents implement a generalization of the local\ninformation-exchange dynamics recently proposed by Li and Marden over a static\nundirected graph. In ODA-PS, the agents implement the broadcast-based push-sum\ndynamics over a time-varying sequence of uniformly connected digraphs. We show\nthat the regret bounds in both cases have sublinear growth of $O(\\sqrt{T})$,\nwith the time horizon $T$, when the stepsize is of the form $1/\\sqrt{t}$ and\nthe objective functions are Lipschitz-continuous convex functions with\nLipschitz gradients. We also implement the proposed algorithms on a sensor\nnetwork to complement our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 17:51:38 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 20:31:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Lee", "Soomin", ""], ["Nedi\u0107", "Angelia", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1508.07964", "submitter": "Diyan Teng", "authors": "Diyan Teng and Emre Ertin", "title": "Wald-Kernel: Learning to Aggregate Information for Sequential Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential hypothesis testing is a desirable decision making strategy in any\ntime sensitive scenario. Compared with fixed sample-size testing, sequential\ntesting is capable of achieving identical probability of error requirements\nusing less samples in average. For a binary detection problem, it is well known\nthat for known density functions accumulating the likelihood ratio statistics\nis time optimal under a fixed error rate constraint. This paper considers the\nproblem of learning a binary sequential detector from training samples when\ndensity functions are unavailable. We formulate the problem as a constrained\nlikelihood ratio estimation which can be solved efficiently through convex\noptimization by imposing Reproducing Kernel Hilbert Space (RKHS) structure on\nthe log-likelihood ratio function. In addition, we provide a computationally\nefficient approximated solution for large scale data set. The proposed\nalgorithm, namely Wald-Kernel, is tested on a synthetic data set and two real\nworld data sets, together with previous approaches for likelihood ratio\nestimation. Our empirical results show that the classifier trained through the\nproposed technique achieves smaller average sampling cost than previous\napproaches proposed in the literature for the same error rate.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 19:06:52 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 11:39:23 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 02:03:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Teng", "Diyan", ""], ["Ertin", "Emre", ""]]}]