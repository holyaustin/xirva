[{"id": "1209.0001", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi, Tianbao Yang, Rong Jin", "title": "An Improved Bound for the Nystrom Method for Large Eigengap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an improved bound for the approximation error of the Nystr\\\"{o}m\nmethod under the assumption that there is a large eigengap in the spectrum of\nkernel matrix. This is based on the empirical observation that the eigengap has\na significant impact on the approximation error of the Nystr\\\"{o}m method. Our\napproach is based on the concentration inequality of integral operator and the\ntheory of matrix perturbation. Our analysis shows that when there is a large\neigengap, we can improve the approximation error of the Nystr\\\"{o}m method from\n$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is\nthe size of the kernel matrix, and $m$ is the number of sampled columns.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 20:40:06 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""]]}, {"id": "1209.0029", "submitter": "Stephen Purpura", "authors": "Stephen Purpura, Dustin Hillard, Mark Hubenthal, Jim Walsh, Scott\n  Golder, Scott Smith", "title": "Statistically adaptive learning for a general class of cost functions\n  (SA L-BFGS)", "comments": "7 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": "version 0.05", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that enables rapid model experimentation for tera-scale\nmachine learning with trillions of non-zero features, billions of training\nexamples, and millions of parameters. Our contribution to the literature is a\nnew method (SA L-BFGS) for changing batch L-BFGS to perform in near real-time\nby using statistical tools to balance the contributions of previous weights,\nold training examples, and new training examples to achieve fast convergence\nwith few iterations. The result is, to our knowledge, the most scalable and\nflexible linear learning system reported in the literature, beating standard\npractice with the current best system (Vowpal Wabbit and AllReduce). Using the\nKDD Cup 2012 data set from Tencent, Inc. we provide experimental results to\nverify the performance of this method.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 22:50:00 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 02:15:07 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2012 20:21:29 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Purpura", "Stephen", ""], ["Hillard", "Dustin", ""], ["Hubenthal", "Mark", ""], ["Walsh", "Jim", ""], ["Golder", "Scott", ""], ["Smith", "Scott", ""]]}, {"id": "1209.0056", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Learning implicitly in reasoning in PAC-Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering queries about formulas of propositional\nlogic based on background knowledge partially represented explicitly as other\nformulas, and partially represented as partially obscured examples\nindependently drawn from a fixed probability distribution, where the queries\nare answered with respect to a weaker semantics than usual -- PAC-Semantics,\nintroduced by Valiant (2000) -- that is defined using the distribution of\nexamples. We describe a fairly general, efficient reduction to limited versions\nof the decision problem for a proof system (e.g., bounded space treelike\nresolution, bounded degree polynomial calculus, etc.) from corresponding\nversions of the reasoning problem where some of the background knowledge is not\nexplicitly given as formulas, only learnable from the examples. Crucially, we\ndo not generate an explicit representation of the knowledge extracted from the\nexamples, and so the \"learning\" of the background knowledge is only done\nimplicitly. As a consequence, this approach can utilize formulas as background\nknowledge that are not perfectly valid over the distribution---essentially the\nanalogue of agnostic learning here.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 05:13:00 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1209.0089", "submitter": "Aaron Clauset", "authors": "Aaron Clauset, Ryan Woodard", "title": "Estimating the historical and future probabilities of large terrorist\n  events", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS614 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 1838-1865", "doi": "10.1214/12-AOAS614", "report-no": "IMS-AOAS-AOAS614", "categories": "physics.data-an cs.LG physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantities with right-skewed distributions are ubiquitous in complex social\nsystems, including political conflict, economics and social networks, and these\nsystems sometimes produce extremely large events. For instance, the 9/11\nterrorist events produced nearly 3000 fatalities, nearly six times more than\nthe next largest event. But, was this enormous loss of life statistically\nunlikely given modern terrorism's historical record? Accurately estimating the\nprobability of such an event is complicated by the large fluctuations in the\nempirical distribution's upper tail. We present a generic statistical algorithm\nfor making such estimates, which combines semi-parametric models of tail\nbehavior and a nonparametric bootstrap. Applied to a global database of\nterrorist events, we estimate the worldwide historical probability of observing\nat least one 9/11-sized or larger event since 1968 to be 11-35%. These results\nare robust to conditioning on global variations in economic development,\ndomestic versus international events, the type of weapon used and a truncated\nhistory that stops at 1998. We then use this procedure to make a data-driven\nstatistical forecast of at least one similar event over the next decade.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 12:58:35 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 05:52:57 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2014 08:38:09 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Clauset", "Aaron", ""], ["Woodard", "Ryan", ""]]}, {"id": "1209.0125", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Michael J. Kurtz", "title": "A History of Cluster Analysis Using the Classification Society's\n  Bibliography Over Four Decades", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Classification Literature Automated Search Service, an annual\nbibliography based on citation of one or more of a set of around 80 book or\njournal publications, ran from 1972 to 2012. We analyze here the years 1994 to\n2011. The Classification Society's Service, as it was termed, has been produced\nby the Classification Society. In earlier decades it was distributed as a\ndiskette or CD with the Journal of Classification. Among our findings are the\nfollowing: an enormous increase in scholarly production post approximately\n2000; a very major increase in quantity, coupled with work in different\ndisciplines, from approximately 2004; and a major shift also from cluster\nanalysis in earlier times having mathematics and psychology as disciplines of\nthe journals published in, and affiliations of authors, contrasted with, in\nmore recent times, a \"centre of gravity\" in management and engineering.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 19:27:19 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 23:52:49 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Murtagh", "Fionn", ""], ["Kurtz", "Michael J.", ""]]}, {"id": "1209.0127", "submitter": "Alexandra Faynburd Mrs", "authors": "Ran El-Yaniv, Alexandra Faynburd", "title": "Autoregressive short-term prediction of turning points using support\n  vector regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with autoregressive prediction of turning points in\nfinancial price sequences. Such turning points are critical local extrema\npoints along a series, which mark the start of new swings. Predicting the\nfuture time of such turning points or even their early or late identification\nslightly before or after the fact has useful applications in economics and\nfinance. Building on recently proposed neural network model for turning point\nprediction, we propose and study a new autoregressive model for predicting\nturning points of small swings. Our method relies on a known turning point\nindicator, a Fourier enriched representation of price histories, and support\nvector regression. We empirically examine the performance of the proposed\nmethod over a long history of the Dow Jones Industrial average. Our study shows\nthat the proposed method is superior to the previous neural network model, in\nterms of trading performance of a simple trading application and also exhibits\na quantifiable advantage over the buy-and-hold benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 19:53:23 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 19:28:24 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Faynburd", "Alexandra", ""]]}, {"id": "1209.0368", "submitter": "Silvia Villa", "authors": "Silvia Villa, Lorenzo Rosasco, Sofia Mosci, Alessandro Verri", "title": "Proximal methods for the latent group lasso penalty", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a regularized least squares problem, with regularization by\nstructured sparsity-inducing norms, which extend the usual $\\ell_1$ and the\ngroup lasso penalty, by allowing the subsets to overlap. Such regularizations\nlead to nonsmooth problems that are difficult to optimize, and we propose in\nthis paper a suitable version of an accelerated proximal method to solve them.\nWe prove convergence of a nested procedure, obtained composing an accelerated\nproximal method with an inner algorithm for computing the proximity operator.\nBy exploiting the geometrical properties of the penalty, we devise a new active\nset strategy, thanks to which the inner iteration is relatively fast, thus\nguaranteeing good computational performances of the overall algorithm. Our\napproach allows to deal with high dimensional problems without pre-processing\nfor dimensionality reduction, leading to better computational and prediction\nperformances with respect to the state-of-the art methods, as shown empirically\nboth on toy and real data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 14:46:14 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Villa", "Silvia", ""], ["Rosasco", "Lorenzo", ""], ["Mosci", "Sofia", ""], ["Verri", "Alessandro", ""]]}, {"id": "1209.0430", "submitter": "Bamdev Mishra", "authors": "B. Mishra, G. Meyer, S. Bonnabel and R. Sepulchre", "title": "Fixed-rank matrix factorizations and Riemannian low-rank optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of learning a linear regression model whose\nparameter is a large fixed-rank non-symmetric matrix, we consider the\noptimization of a smooth cost function defined on the set of fixed-rank\nmatrices. We adopt the geometric framework of optimization on Riemannian\nquotient manifolds. We study the underlying geometries of several well-known\nfixed-rank matrix factorizations and then exploit the Riemannian quotient\ngeometry of the search space in the design of a class of gradient descent and\ntrust-region algorithms. The proposed algorithms generalize our previous\nresults on fixed-rank symmetric positive semidefinite matrices, apply to a\nbroad range of applications, scale to high-dimensional problems and confer a\ngeometric basis to recent contributions on the learning of fixed-rank\nnon-symmetric matrices. We make connections with existing algorithms in the\ncontext of low-rank matrix completion and discuss relative usefulness of the\nproposed framework. Numerical experiments suggest that the proposed algorithms\ncompete with the state-of-the-art and that manifold optimization offers an\neffective and versatile framework for the design of machine learning algorithms\nthat learn a fixed-rank matrix.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 18:48:37 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2013 12:19:54 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Mishra", "B.", ""], ["Meyer", "G.", ""], ["Bonnabel", "S.", ""], ["Sepulchre", "R.", ""]]}, {"id": "1209.0521", "submitter": "Olivier Delalleau", "authors": "Olivier Delalleau and Aaron Courville and Yoshua Bengio", "title": "Efficient EM Training of Gaussian Mixtures with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data-mining applications, we are frequently faced with a large fraction of\nmissing entries in the data matrix, which is problematic for most discriminant\nmachine learning algorithms. A solution that we explore in this paper is the\nuse of a generative model (a mixture of Gaussians) to compute the conditional\nexpectation of the missing variables given the observed variables. Since\ntraining a Gaussian mixture with many different patterns of missing values can\nbe computationally very expensive, we introduce a spanning-tree based algorithm\nthat significantly speeds up training in these conditions. We also observe that\ngood results can be obtained by using the generative model to fill-in the\nmissing values for a separate discriminant learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 03:15:53 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 15:50:42 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Delalleau", "Olivier", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1209.0738", "submitter": "Bernardino Romera Paredes", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "Sparse coding for multitask and transfer learning", "comments": "International Conference on Machine Learning 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of sparse coding and dictionary learning in the\ncontext of multitask and transfer learning. The central assumption of our\nlearning method is that the tasks parameters are well approximated by sparse\nlinear combinations of the atoms of a dictionary on a high or infinite\ndimensional space. This assumption, together with the large quantity of\navailable data in the multitask and transfer learning settings, allows a\nprincipled choice of the dictionary. We provide bounds on the generalization\nerror of this approach, for both settings. Numerical experiments on one\nsynthetic and two real datasets show the advantage of our method over single\ntask learning, a previous method based on orthogonal and dense representation\nof the tasks and a related method learning task grouping.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 19:06:51 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 19:35:27 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 15:06:48 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1209.0853", "submitter": "Ehsan Saboori Mr.", "authors": "Ehsan Saboori, Shafigh Parsazad, Anoosheh Sadeghi", "title": "Improving the K-means algorithm using improved downhill simplex search", "comments": "4 Pages", "journal-ref": "2010 2nd International Conference on Software Technology and\n  Engineering (ICSTE), vol.2, no., pp.V2-350-V2-354, 3-5 Oct. 2010", "doi": "10.1109/ICSTE.2010.5608792", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means algorithm is one of the well-known and most popular clustering\nalgorithms. K-means seeks an optimal partition of the data by minimizing the\nsum of squared error with an iterative optimization procedure, which belongs to\nthe category of hill climbing algorithms. As we know hill climbing searches are\nfamous for converging to local optimums. Since k-means can converge to a local\noptimum, different initial points generally lead to different convergence\ncancroids, which makes it important to start with a reasonable initial\npartition in order to achieve high quality clustering solutions. However, in\ntheory, there exist no efficient and universal methods for determining such\ninitial partitions. In this paper we tried to find an optimum initial\npartitioning for k-means algorithm. To achieve this goal we proposed a new\nimproved version of downhill simplex search, and then we used it in order to\nfind an optimal result for clustering approach and then compare this algorithm\nwith Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved Genetic\nK-Means (IGKM) and k-means algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 03:02:26 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Saboori", "Ehsan", ""], ["Parsazad", "Shafigh", ""], ["Sadeghi", "Anoosheh", ""]]}, {"id": "1209.0913", "submitter": "Jun Wang", "authors": "Jun Wang and Alexandros Kalousis", "title": "Structuring Relevant Feature Sets with Multiple Model Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is one of the most prominent learning tasks, especially in\nhigh-dimensional datasets in which the goal is to understand the mechanisms\nthat underly the learning dataset. However most of them typically deliver just\na flat set of relevant features and provide no further information on what kind\nof structures, e.g. feature groupings, might underly the set of relevant\nfeatures. In this paper we propose a new learning paradigm in which our goal is\nto uncover the structures that underly the set of relevant features for a given\nlearning problem. We uncover two types of features sets, non-replaceable\nfeatures that contain important information about the target variable and\ncannot be replaced by other features, and functionally similar features sets\nthat can be used interchangeably in learned models, given the presence of the\nnon-replaceable features, with no change in the predictive performance. To do\nso we propose a new learning algorithm that learns a number of disjoint models\nusing a model disjointness regularization constraint together with a constraint\non the predictive agreement of the disjoint models. We explore the behavior of\nour approach on a number of high-dimensional datasets, and show that, as\nexpected by their construction, these satisfy a number of properties. Namely,\nmodel disjointness, a high predictive agreement, and a similar predictive\nperformance to models learned on the full set of relevant features. The ability\nto structure the set of relevant features in such a manner can become a\nvaluable tool in different applications of scientific knowledge discovery.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 10:08:02 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Wang", "Jun", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1209.1033", "submitter": "Benyuan Liu", "authors": "Benyuan Liu and Hongqi Fan and Zaiqi Lu and Qiang Fu", "title": "The Annealing Sparse Bayesian Learning Algorithm", "comments": "The update equation in the annealing process was too empirical for\n  practical usage. This paper need to be revised in order to be printed on the\n  arxiv.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a two-level hierarchical Bayesian model and an\nannealing schedule to re-enable the noise variance learning capability of the\nfast marginalized Sparse Bayesian Learning Algorithms. The performance such as\nNMSE and F-measure can be greatly improved due to the annealing technique. This\nalgorithm tends to produce the most sparse solution under moderate SNR\nscenarios and can outperform most concurrent SBL algorithms while pertains\nsmall computational load.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 16:29:17 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2012 08:17:24 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 00:46:16 GMT"}, {"version": "v4", "created": "Wed, 1 May 2013 09:28:29 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Liu", "Benyuan", ""], ["Fan", "Hongqi", ""], ["Lu", "Zaiqi", ""], ["Fu", "Qiang", ""]]}, {"id": "1209.1077", "submitter": "Guillermo Diez-Canas", "authors": "Guillermo D. Canas and Lorenzo Rosasco", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "comments": "13 pages, 2 figures. Advances in Neural Information Processing\n  Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating, in the sense of optimal transport\nmetrics, a measure which is assumed supported on a manifold embedded in a\nHilbert space. By establishing a precise connection between optimal transport\nmetrics, optimal quantization, and learning theory, we derive new probabilistic\nbounds for the performance of a classic algorithm in unsupervised learning\n(k-means), when used to produce a probability measure derived from the data. In\nthe course of the analysis, we arrive at new lower bounds, as well as\nprobabilistic upper bounds on the convergence rate of the empirical law of\nlarge numbers, which, unlike existing bounds, are applicable to a wide class of\nmeasures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 19:10:09 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Canas", "Guillermo D.", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1209.1086", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet and Amaury Habrard", "title": "Robustness and Generalization for Metric Learning", "comments": "16 pages, to appear in Neurocomputing", "journal-ref": "Neurocomputing,151(1):259-267, 2015", "doi": "10.1016/j.neucom.2014.09.044", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has attracted a lot of interest over the last decade, but the\ngeneralization ability of such methods has not been thoroughly studied. In this\npaper, we introduce an adaptation of the notion of algorithmic robustness\n(previously introduced by Xu and Mannor) that can be used to derive\ngeneralization bounds for metric learning. We further show that a weak notion\nof robustness is in fact a necessary and sufficient condition for a metric\nlearning algorithm to generalize. To illustrate the applicability of the\nproposed framework, we derive generalization results for a large family of\nexisting metric learning algorithms, including some sparse formulations that\nare not covered by previous results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 19:48:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 20:25:54 GMT"}, {"version": "v3", "created": "Mon, 29 Sep 2014 09:27:31 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""]]}, {"id": "1209.1121", "submitter": "Guillermo Diez-Canas", "authors": "Guillermo D. Canas and Tomaso Poggio and Lorenzo Rosasco", "title": "Learning Manifolds with K-Means and K-Flats", "comments": "19 pages, 2 figures; Advances in Neural Information Processing\n  Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a manifold from random samples. In\nparticular, we consider piecewise constant and piecewise linear estimators\ninduced by k-means and k-flats, and analyze their performance. We extend\nprevious results for k-means in two separate directions. First, we provide new\nresults for k-means reconstruction on manifolds and, secondly, we prove\nreconstruction bounds for higher-order approximation (k-flats), for which no\nknown results were previously available. While the results for k-means are\nnovel, some of the technical tools are well-established in the literature. In\nthe case of k-flats, both the results and the mathematical tools are new.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:18:03 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 17:11:23 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2012 16:00:03 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2013 17:53:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Canas", "Guillermo D.", ""], ["Poggio", "Tomaso", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1209.1360", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-Jacques Slotine", "title": "Multiclass Learning with Simplex Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss a novel framework for multiclass learning, defined\nby a suitable coding/decoding strategy, namely the simplex coding, that allows\nto generalize to multiple classes a relaxation approach commonly used in binary\nclassification. In this framework, a relaxation error analysis can be developed\navoiding constraints on the considered hypotheses class. Moreover, we show that\nin this setting it is possible to derive the first provably consistent\nregularized method with training/tuning complexity which is independent to the\nnumber of classes. Tools from convex analysis are introduced that can be used\nbeyond the scope of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 18:22:25 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 14:14:53 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Mroueh", "Youssef", ""], ["Poggio", "Tomaso", ""], ["Rosasco", "Lorenzo", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "1209.1450", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, LNAO), Ga\\\"el\n  Varoquaux (INRIA Saclay - Ile de France, LNAO), Bertrand Thirion (INRIA\n  Saclay - Ile de France, LNAO)", "title": "On spatial selectivity and prediction across conditions with fMRI", "comments": "PRNI 2012 : 2nd International Workshop on Pattern Recognition in\n  NeuroImaging, London : United Kingdom (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in functional neuroimaging mostly use activation coordinates to\nformulate their hypotheses. Instead, we propose to use the full statistical\nimages to define regions of interest (ROIs). This paper presents two machine\nlearning approaches, transfer learning and selection transfer, that are\ncompared upon their ability to identify the common patterns between brain\nactivation maps related to two functional tasks. We provide some preliminary\nquantification of these similarities, and show that selection transfer makes it\npossible to set a spatial scale yielding ROIs that are more specific to the\ncontext of interest than with transfer learning. In particular, selection\ntransfer outlines well known regions such as the Visual Word Form Area when\ndiscriminating between different visual tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 06:28:42 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Thirion", "Bertrand", "", "INRIA\n  Saclay - Ile de France, LNAO"]]}, {"id": "1209.1557", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani, Petros T. Boufounos, and Bhiksha Raj", "title": "Learning Model-Based Sparsity via Projected Gradient Descent", "comments": null, "journal-ref": "IEEE Transactions on Information Theory 62(4):2092--2099, 2016", "doi": "10.1109/TIT.2016.2515078", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several convex formulation methods have been proposed previously for\nstatistical estimation with structured sparsity as the prior. These methods\noften require a carefully tuned regularization parameter, often a cumbersome or\nheuristic exercise. Furthermore, the estimate that these methods produce might\nnot belong to the desired sparsity model, albeit accurately approximating the\ntrue parameter. Therefore, greedy-type algorithms could often be more desirable\nin estimating structured-sparse parameters. So far, these greedy methods have\nmostly focused on linear statistical models. In this paper we study the\nprojected gradient descent with non-convex structured-sparse parameter model as\nthe constraint set. Should the cost function have a Stable Model-Restricted\nHessian the algorithm produces an approximation for the desired minimizer. As\nan example we elaborate on application of the main results to estimation in\nGeneralized Linear Model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 14:46:49 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2012 22:10:01 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 20:08:10 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 13:14:52 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Bahmani", "Sohail", ""], ["Boufounos", "Petros T.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1209.1688", "submitter": "Sewoong Oh", "authors": "Sahand Negahban, Sewoong Oh, Devavrat Shah", "title": "Rank Centrality: Ranking from Pair-wise Comparisons", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 04:42:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 06:28:50 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 21:42:49 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2015 17:51:33 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Negahban", "Sahand", ""], ["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1209.1727", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck, Nicol\\`o Cesa-Bianchi and G\\'abor Lugosi", "title": "Bandits with heavy tail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic multi-armed bandit problem is well understood when the reward\ndistributions are sub-Gaussian. In this paper we examine the bandit problem\nunder the weaker assumption that the distributions have moments of order\n1+\\epsilon, for some $\\epsilon \\in (0,1]$. Surprisingly, moments of order 2\n(i.e., finite variance) are sufficient to obtain regret bounds of the same\norder as under sub-Gaussian reward distributions. In order to achieve such\nregret, we define sampling strategies based on refined estimators of the mean\nsuch as the truncated empirical mean, Catoni's M-estimator, and the\nmedian-of-means estimator. We also derive matching lower bounds that also show\nthat the best achievable regret deteriorates when \\epsilon <1.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 15:22:07 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1209.1739", "submitter": "Jan Oksanen", "authors": "Jan Oksanen, Jarmo Lund\\'en and Visa Koivunen", "title": "Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive\n  Radio Network", "comments": "In Proceedings of CISS 2012 Conference, Princeton, NJ, USA, March\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an optimal sensing policy for a particular access policy and sensing\nscheme is a laborious combinatorial problem that requires the system model\nparameters to be known. In practise the parameters or the model itself may not\nbe completely known making reinforcement learning methods appealing. In this\npaper a non-parametric reinforcement learning-based method is developed for\nsensing and accessing multi-band radio spectrum in multi-user cognitive radio\nnetworks. A suboptimal sensing policy search algorithm is proposed for a\nparticular multi-user multi-band access policy and the randomized\nChair-Varshney rule. The randomized Chair-Varshney rule is used to reduce the\nprobability of false alarms under a constraint on the probability of detection\nthat protects the primary user. The simulation results show that the proposed\nmethod achieves a sum profit (e.g. data rate) close to the optimal sensing\npolicy while achieving the desired probability of detection.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 18:34:01 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Oksanen", "Jan", ""], ["Lund\u00e9n", "Jarmo", ""], ["Koivunen", "Visa", ""]]}, {"id": "1209.1797", "submitter": "Eitan Menahem", "authors": "Eitan Menahem, Alon Schclar, Lior Rokach, Yuval Elovici", "title": "Securing Your Transactions: Detecting Anomalous Patterns In XML\n  Documents", "comments": "Journal version (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML transactions are used in many information systems to store data and\ninteract with other systems. Abnormal transactions, the result of either an\non-going cyber attack or the actions of a benign user, can potentially harm the\ninteracting systems and therefore they are regarded as a threat. In this paper\nwe address the problem of anomaly detection and localization in XML\ntransactions using machine learning techniques. We present a new XML anomaly\ndetection framework, XML-AD. Within this framework, an automatic method for\nextracting features from XML transactions was developed as well as a practical\nmethod for transforming XML features into vectors of fixed dimensionality. With\nthese two methods in place, the XML-AD framework makes it possible to utilize\ngeneral learning algorithms for anomaly detection. Central to the functioning\nof the framework is a novel multi-univariate anomaly detection algorithm,\nADIFA. The framework was evaluated on four XML transactions datasets, captured\nfrom real information systems, in which it achieved over 89% true positive\ndetection rate with less than a 0.2% false positive rate.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 13:02:49 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2012 05:48:34 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2013 13:19:42 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Menahem", "Eitan", ""], ["Schclar", "Alon", ""], ["Rokach", "Lior", ""], ["Elovici", "Yuval", ""]]}, {"id": "1209.1800", "submitter": "Rui Wang", "authors": "Rui Wang, Ke Tang", "title": "An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost\n  Matrices", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-sensitive learning relies on the availability of a known and fixed cost\nmatrix. However, in some scenarios, the cost matrix is uncertain during\ntraining, and re-train a classifier after the cost matrix is specified would\nnot be an option. For binary classification, this issue can be successfully\naddressed by methods maximizing the Area Under the ROC Curve (AUC) metric.\nSince the AUC can measure performance of base classifiers independent of cost\nduring training, and a larger AUC is more likely to lead to a smaller total\ncost in testing using the threshold moving method. As an extension of AUC to\nmulti-class problems, MAUC has attracted lots of attentions and been widely\nused. Although MAUC also measures performance of base classifiers independent\nof cost, it is unclear whether a larger MAUC of classifiers is more likely to\nlead to a smaller total cost. In fact, it is also unclear what kinds of\npost-processing methods should be used in multi-class problems to convert base\nclassifiers into discrete classifiers such that the total cost is as small as\npossible. In the paper, we empirically explore the relationship between MAUC\nand the total cost of classifiers by applying two categories of post-processing\nmethods. Our results suggest that a larger MAUC is also beneficial.\nInterestingly, simple calibration methods that convert the output matrix into\nposterior probabilities perform better than existing sophisticated post\nre-optimization methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 14:11:04 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Wang", "Rui", ""], ["Tang", "Ke", ""]]}, {"id": "1209.1873", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 03:25:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 15:30:25 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1209.1960", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela", "title": "A Comparative Study of Efficient Initialization Methods for the K-Means\n  Clustering Algorithm", "comments": "17 pages, 1 figure, 7 tables", "journal-ref": "Expert Systems with Applications 40 (2013) 200-210", "doi": "10.1016/j.eswa.2012.07.021", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is undoubtedly the most widely used partitional clustering algorithm.\nUnfortunately, due to its gradient descent nature, this algorithm is highly\nsensitive to the initial placement of the cluster centers. Numerous\ninitialization methods have been proposed to address this problem. In this\npaper, we first present an overview of these methods with an emphasis on their\ncomputational efficiency. We then compare eight commonly used linear time\ncomplexity initialization methods on a large and diverse collection of data\nsets using various performance criteria. Finally, we analyze the experimental\nresults using non-parametric statistical tests and provide recommendations for\npractitioners. We demonstrate that popular initialization methods often perform\npoorly and that there are in fact strong alternatives to these methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 12:22:06 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1209.2139", "submitter": "Sen Yang", "authors": "Sen Yang, Zhaosong Lu, Xiaotong Shen, Peter Wonka, Jieping Ye", "title": "Fused Multiple Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating multiple graphical\nmodels simultaneously using the fused lasso penalty, which encourages adjacent\ngraphs to share similar structures. A motivating example is the analysis of\nbrain networks of Alzheimer's disease using neuroimaging data. Specifically, we\nmay wish to estimate a brain network for the normal controls (NC), a brain\nnetwork for the patients with mild cognitive impairment (MCI), and a brain\nnetwork for Alzheimer's patients (AD). We expect the two brain networks for NC\nand MCI to share common structures but not to be identical to each other;\nsimilarly for the two brain networks for MCI and AD. The proposed formulation\ncan be solved using a second-order method. Our key technical contribution is to\nestablish the necessary and sufficient condition for the graphs to be\ndecomposable. Based on this key property, a simple screening rule is presented,\nwhich decomposes the large graphs into small subgraphs and allows an efficient\nestimation of multiple independent (small) subgraphs, dramatically reducing the\ncomputational cost. We perform experiments on both synthetic and real data; our\nresults demonstrate the effectiveness and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 20:13:42 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 03:19:45 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Yang", "Sen", ""], ["Lu", "Zhaosong", ""], ["Shen", "Xiaotong", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1209.2194", "submitter": "Alexander Olshevsky", "authors": "Naomi Ehrich Leonard, Alex Olshevsky", "title": "Cooperative learning in multi-agent systems from intermittent\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of tracking a direction in a decentralized way, we\nconsider the general problem of cooperative learning in multi-agent systems\nwith time-varying connectivity and intermittent measurements. We propose a\ndistributed learning protocol capable of learning an unknown vector $\\mu$ from\nnoisy measurements made independently by autonomous nodes. Our protocol is\ncompletely distributed and able to cope with the time-varying, unpredictable,\nand noisy nature of inter-agent communication, and intermittent noisy\nmeasurements of $\\mu$. Our main result bounds the learning speed of our\nprotocol in terms of the size and combinatorial features of the (time-varying)\nnetworks connecting the nodes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 01:33:58 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 23:54:30 GMT"}, {"version": "v3", "created": "Fri, 27 Dec 2013 20:54:59 GMT"}, {"version": "v4", "created": "Thu, 16 Oct 2014 18:07:19 GMT"}, {"version": "v5", "created": "Mon, 15 Dec 2014 21:07:19 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Leonard", "Naomi Ehrich", ""], ["Olshevsky", "Alex", ""]]}, {"id": "1209.2355", "submitter": "L\\'eon Bottou", "authors": "L\\'eon Bottou, Jonas Peters, Joaquin Qui\\~nonero-Candela, Denis X.\n  Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed\n  Snelson", "title": "Counterfactual Reasoning and Learning Systems", "comments": "revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows how to leverage causal inference to understand the behavior\nof complex learning systems interacting with their environment and predict the\nconsequences of changes to the system. Such predictions allow both humans and\nalgorithms to select changes that improve both the short-term and long-term\nperformance of such systems. This work is illustrated by experiments carried\nout on the ad placement system associated with the Bing search engine.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:47:43 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 16:47:55 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 21:36:18 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2013 03:09:16 GMT"}, {"version": "v5", "created": "Sat, 27 Jul 2013 18:02:46 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Bottou", "L\u00e9on", ""], ["Peters", "Jonas", ""], ["Qui\u00f1onero-Candela", "Joaquin", ""], ["Charles", "Denis X.", ""], ["Chickering", "D. Max", ""], ["Portugaly", "Elon", ""], ["Ray", "Dipankar", ""], ["Simard", "Patrice", ""], ["Snelson", "Ed", ""]]}, {"id": "1209.2388", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "On the Complexity of Bandit and Derivative-Free Stochastic Convex\n  Optimization", "comments": "Version appearing in COLT (Conference on Learning Theory) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of stochastic convex optimization with bandit feedback (in the\nlearning community) or without knowledge of gradients (in the optimization\ncommunity) has received much attention in recent years, in the form of\nalgorithms and performance upper bounds. However, much less is known about the\ninherent complexity of these problems, and there are few lower bounds in the\nliterature, especially for nonlinear functions. In this paper, we investigate\nthe attainable error/regret in the bandit and derivative-free settings, as a\nfunction of the dimension d and the available number of queries T. We provide a\nprecise characterization of the attainable performance for strongly-convex and\nsmooth functions, which also imply a non-trivial lower bound for more general\nproblems. Moreover, we prove that in both the bandit and derivative-free\nsetting, the required number of queries must scale at least quadratically with\nthe dimension. Finally, we show that on the natural class of quadratic\nfunctions, it is possible to obtain a \"fast\" O(1/T) error rate in terms of T,\nunder mild assumptions, even without having access to gradients. To the best of\nour knowledge, this is the first such rate in a derivative-free stochastic\nsetting, and holds despite previous results which seem to imply the contrary.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 18:16:56 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 20:03:52 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2013 18:48:17 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1209.2434", "submitter": "Kevin Jamieson", "authors": "Kevin G. Jamieson, Robert D. Nowak, Benjamin Recht", "title": "Query Complexity of Derivative-Free Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides lower bounds on the convergence rate of Derivative Free\nOptimization (DFO) with noisy function evaluations, exposing a fundamental and\nunavoidable gap between the performance of algorithms with access to gradients\nand those with access to only function evaluations. However, there are\nsituations in which DFO is unavoidable, and for such situations we propose a\nnew DFO algorithm that is proved to be near optimal for the class of strongly\nconvex objective functions. A distinctive feature of the algorithm is that it\nuses only Boolean-valued function comparisons, rather than function\nevaluations. This makes the algorithm useful in an even wider range of\napplications, such as optimization based on paired comparisons from human\nsubjects, for example. We also show that regardless of whether DFO is based on\nnoisy function evaluations or Boolean-valued function comparisons, the\nconvergence rate is the same.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 20:37:02 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Jamieson", "Kevin G.", ""], ["Nowak", "Robert D.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1209.2501", "submitter": "Hemanth K S", "authors": "Hemanth K. S Doreswamy", "title": "Performance Evaluation of Predictive Classifiers For Knowledge Discovery\n  From Engineering Materials Data Sets", "comments": "Volume 3,No 3,March 2011 7 pages, 6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are\nsuccessively applied on materials informatics to classify the engineering\nmaterials into different classes for the selection of materials that suit the\ninput design specifications. Here, the classifiers are analyzed individually\nand their performance evaluation is analyzed with confusion matrix predictive\nparameters and standard measures, the classification results are analyzed on\ndifferent class of materials. Comparison of classifiers has found that naive\nBayesian classifier is more accurate and better than the C4.5 DTC. The\nknowledge discovered by the naive bayesian classifier can be employed for\ndecision making in materials selection in manufacturing industries.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 05:28:32 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Doreswamy", "Hemanth K. S", ""]]}, {"id": "1209.2620", "submitter": "Marcus Hutter", "authors": "Marcus Hutter and John W. Lloyd and Kee Siong Ng and William T. B.\n  Uther", "title": "Probabilities on Sentences in an Expressive Logic", "comments": "52 LaTeX pages, 64 definiton/theorems/etc, presented at conference\n  Progic 2011 in New York", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG math.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated reasoning about uncertain knowledge has many applications. One\ndifficulty when developing such systems is the lack of a completely\nsatisfactory integration of logic and probability. We address this problem\ndirectly. Expressive languages like higher-order logic are ideally suited for\nrepresenting and reasoning about structured knowledge. Uncertain knowledge can\nbe modeled by using graded probabilities rather than binary truth-values. The\nmain technical problem studied in this paper is the following: Given a set of\nsentences, each having some probability of being true, what probability should\nbe ascribed to other (query) sentences? A natural wish-list, among others, is\nthat the probability distribution (i) is consistent with the knowledge base,\n(ii) allows for a consistent inference procedure and in particular (iii)\nreduces to deductive logic in the limit of probabilities being 0 and 1, (iv)\nallows (Bayesian) inductive reasoning and (v) learning in the limit and in\nparticular (vi) allows confirmation of universally quantified\nhypotheses/sentences. We translate this wish-list into technical requirements\nfor a prior probability and show that probabilities satisfying all our criteria\nexist. We also give explicit constructions and several general\ncharacterizations of probabilities that satisfy some or all of the criteria and\nvarious (counter) examples. We also derive necessary and sufficient conditions\nfor extending beliefs about finitely many sentences to suitable probabilities\nover all sentences, and in particular least dogmatic or least biased ones. We\nconclude with a brief outlook on how the developed theory might be used and\napproximated in autonomous reasoning agents. Our theory is a step towards a\nglobally consistent and empirically satisfactory unification of probability and\nlogic.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 14:17:09 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Hutter", "Marcus", ""], ["Lloyd", "John W.", ""], ["Ng", "Kee Siong", ""], ["Uther", "William T. B.", ""]]}, {"id": "1209.2673", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Conditional validity of inductive conformal predictors", "comments": "23 pages, 9 figures, 2 tables; to appear in the ACML 2012 Proceedings", "journal-ref": null, "doi": null, "report-no": "OCMNS05", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal predictors are set predictors that are automatically valid in the\nsense of having coverage probability equal to or exceeding a given confidence\nlevel. Inductive conformal predictors are a computationally efficient version\nof conformal predictors satisfying the same property of validity. However,\ninductive conformal predictors have been only known to control unconditional\ncoverage probability. This paper explores various versions of conditional\nvalidity and various ways to achieve them using inductive conformal predictors\nand their modifications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 17:39:37 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 18:28:44 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1209.2693", "submitter": "Daniil Ryabko", "authors": "Ronald Ortner, Daniil Ryabko, Peter Auer, R\\'emi Munos", "title": "Regret Bounds for Restless Markov Bandits", "comments": "In proceedings of The 23rd International Conference on Algorithmic\n  Learning Theory (ALT 2012)", "journal-ref": "Proceedings of ALT, Lyon, France, LNCS 7568, pp.214-228, 2012", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the restless Markov bandit problem, in which the state of each\narm evolves according to a Markov process independently of the learner's\nactions. We suggest an algorithm that after $T$ steps achieves\n$\\tilde{O}(\\sqrt{T})$ regret with respect to the best policy that knows the\ndistributions of all arms. No assumptions on the Markov chains are made except\nthat they are irreducible. In addition, we show that index-based policies are\nnecessarily suboptimal for the considered problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 19:14:21 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Ortner", "Ronald", ""], ["Ryabko", "Daniil", ""], ["Auer", "Peter", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1209.2759", "submitter": "Adel Javanmard", "authors": "Adel Javanmard, Maya Haridasan and Li Zhang", "title": "Multi-track Map Matching", "comments": "11 pages, 8 figures, short version appears in 20th International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS\n  2012). Extended Abstract in Proceedings of the 10th international conference\n  on Mobile systems, applications, and services (MobiSys 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for matching user tracks, consisting of time-ordered\nlocation points, to paths in the road network. Previous work has focused on the\nscenario where the location data is linearly ordered and consists of fairly\ndense and regular samples. In this work, we consider the \\emph{multi-track map\nmatching}, where the location data comes from different trips on the same\nroute, each with very sparse samples. This captures the realistic scenario\nwhere users repeatedly travel on regular routes and samples are sparsely\ncollected, either due to energy consumption constraints or because samples are\nonly collected when the user actively uses a service. In the multi-track\nproblem, the total set of combined locations is only partially ordered, rather\nthan globally ordered as required by previous map-matching algorithms. We\npropose two methods, the iterative projection scheme and the graph Laplacian\nscheme, to solve the multi-track problem by using a single-track map-matching\nsubroutine. We also propose a boosting technique which may be applied to either\napproach to improve the accuracy of the estimated paths. In addition, in order\nto deal with variable sampling rates in single-track map matching, we propose a\nmethod based on a particular regularized cost function that can be adapted for\ndifferent sampling rates and measurement errors. We evaluate the effectiveness\nof our techniques for reconstructing tracks under several different\nconfigurations of sampling error and sampling rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 01:44:12 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Javanmard", "Adel", ""], ["Haridasan", "Maya", ""], ["Zhang", "Li", ""]]}, {"id": "1209.2784", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta, Dongryeol Lee, Alexander G. Gray", "title": "Minimax Multi-Task Learning and a Generalized Loss-Compositional\n  Paradigm for MTL", "comments": "appearing at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, the modus operandi of multi-task learning (MTL) has been\nto minimize the task-wise mean of the empirical risks. We introduce a\ngeneralized loss-compositional paradigm for MTL that includes a spectrum of\nformulations as a subfamily. One endpoint of this spectrum is minimax MTL: a\nnew MTL formulation that minimizes the maximum of the tasks' empirical risks.\nVia a certain relaxation of minimax MTL, we obtain a continuum of MTL\nformulations spanning minimax MTL and classical MTL. The full paradigm itself\nis loss-compositional, operating on the vector of empirical risks. It\nincorporates minimax MTL, its relaxations, and many new MTL formulations as\nspecial cases. We show theoretically that minimax MTL tends to avoid worst case\noutcomes on newly drawn test tasks in the learning to learn (LTL) test setting.\nThe results of several MTL formulations on synthetic and real problems in the\nMTL and LTL test settings are encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 06:14:31 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1209.2790", "submitter": "Xianfu Chen", "authors": "Xianfu Chen, Honggang Zhang, Tao Chen, and Mika Lasanen", "title": "Improving Energy Efficiency in Femtocell Networks: A Hierarchical\n  Reinforcement Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates energy efficiency for two-tier femtocell networks\nthrough combining game theory and stochastic learning. With the Stackelberg\ngame formulation, a hierarchical reinforcement learning framework is applied to\nstudy the joint average utility maximization of macrocells and femtocells\nsubject to the minimum signal-to-interference-plus-noise-ratio requirements.\nThe macrocells behave as the leaders and the femtocells are followers during\nthe learning procedure. At each time step, the leaders commit to dynamic\nstrategies based on the best responses of the followers, while the followers\ncompete against each other with no further information but the leaders'\nstrategy information. In this paper, we propose two learning algorithms to\nschedule each cell's stochastic power levels, leading by the macrocells.\nNumerical experiments are presented to validate the proposed studies and show\nthat the two learning algorithms substantially improve the energy efficiency of\nthe femtocell networks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 06:47:26 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Chen", "Xianfu", ""], ["Zhang", "Honggang", ""], ["Chen", "Tao", ""], ["Lasanen", "Mika", ""]]}, {"id": "1209.2910", "submitter": "Marc Lelarge", "authors": "Simon Heimlicher, Marc Lelarge, Laurent Massouli\\'e", "title": "Community Detection in the Labelled Stochastic Block Model", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG math.PR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of community detection from observed interactions\nbetween individuals, in the context where multiple types of interaction are\npossible. We use labelled stochastic block models to represent the observed\ndata, where labels correspond to interaction types. Focusing on a two-community\nscenario, we conjecture a threshold for the problem of reconstructing the\nhidden communities in a way that is correlated with the true partition. To\nsubstantiate the conjecture, we prove that the given threshold correctly\nidentifies a transition on the behaviour of belief propagation from insensitive\nto sensitive. We further prove that the same threshold corresponds to the\ntransition in a related inference problem on a tree model from infeasible to\nfeasible. Finally, numerical results using belief propagation for community\ndetection give further support to the conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 14:35:58 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Heimlicher", "Simon", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1209.3056", "submitter": "Jun Wang", "authors": "Jun Wang, Adam Woznica, Alexandros Kalousis", "title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning local metrics for nearest neighbor\nclassification. Most previous works on local metric learning learn a number of\nlocal unrelated metrics. While this \"independence\" approach delivers an\nincreased flexibility its downside is the considerable risk of overfitting. We\npresent a new parametric local metric learning method in which we learn a\nsmooth metric matrix function over the data manifold. Using an approximation\nerror bound of the metric matrix function we learn local metrics as linear\ncombinations of basis metrics defined on anchor points over different regions\nof the instance space. We constrain the metric matrix function by imposing on\nthe linear combinations manifold regularization which makes the learned metric\nmatrix function vary smoothly along the geodesics of the data manifold. Our\nmetric learning method has excellent performance both in terms of predictive\npower and scalability. We experimented with several large-scale classification\nproblems, tens of thousands of instances, and compared it with several state of\nthe art metric learning methods, both global and local, as well as to SVM with\nautomatic kernel selection, all of which it outperforms in a significant\nmanner.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 22:47:07 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Wang", "Jun", ""], ["Woznica", "Adam", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1209.3129", "submitter": "Anteo Smerieri", "authors": "Anteo Smerieri, Fran\\c{c}ois Duport, Yvan Paquot, Benjamin Schrauwen,\n  Marc Haelterman, Serge Massar", "title": "Analog readout for optical reservoir computers", "comments": "to appear in NIPS 2012", "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 25,\n  953-961 (2012)", "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a new, powerful and flexible machine learning\ntechnique that is easily implemented in hardware. Recently, by using a\ntime-multiplexed architecture, hardware reservoir computers have reached\nperformance comparable to digital implementations. Operating speeds allowing\nfor real time information operation have been reached using optoelectronic\nsystems. At present the main performance bottleneck is the readout layer which\nuses slow, digital postprocessing. We have designed an analog readout suitable\nfor time-multiplexed optoelectronic reservoir computers, capable of working in\nreal time. The readout has been built and tested experimentally on a standard\nbenchmark task. Its performance is better than non-reservoir methods, with\nample room for further improvement. The present work thereby overcomes one of\nthe major limitations for the future development of hardware reservoir\ncomputers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 08:56:19 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Smerieri", "Anteo", ""], ["Duport", "Fran\u00e7ois", ""], ["Paquot", "Yvan", ""], ["Schrauwen", "Benjamin", ""], ["Haelterman", "Marc", ""], ["Massar", "Serge", ""]]}, {"id": "1209.3352", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "comments": "Improvements from previous version: (1) dependence on d improved from\n  d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in\n  terms of log(N) added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state-of-the-art methods.\nHowever, many questions regarding its theoretical performance remained open. In\nthis paper, we design and analyze a generalization of Thompson Sampling\nalgorithm for the stochastic contextual multi-armed bandit problem with linear\npayoff functions, when the contexts are provided by an adaptive adversary. This\nis among the most important and widely studied versions of the contextual\nbandits problem. We provide the first theoretical guarantees for the contextual\nversion of Thompson Sampling. We prove a high probability regret bound of\n$\\tilde{O}(d^{3/2}\\sqrt{T})$ (or $\\tilde{O}(d\\sqrt{T \\log(N)})$), which is the\nbest regret bound achieved by any computationally efficient algorithm available\nfor this problem in the current literature, and is within a factor of\n$\\sqrt{d}$ (or $\\sqrt{\\log(N)}$) of the information-theoretic lower bound for\nthis problem.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:27:11 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 18:35:56 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2014 07:00:54 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2014 07:09:03 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3353", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Further Optimal Regret Bounds for Thompson Sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1111.1797", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state of the art methods.\nIn this paper, we provide a novel regret analysis for Thompson Sampling that\nsimultaneously proves both the optimal problem-dependent bound of\n$(1+\\epsilon)\\sum_i \\frac{\\ln T}{\\Delta_i}+O(\\frac{N}{\\epsilon^2})$ and the\nfirst near-optimal problem-independent bound of $O(\\sqrt{NT\\ln T})$ on the\nexpected regret of this algorithm. Our near-optimal problem-independent bound\nsolves a COLT 2012 open problem of Chapelle and Li. The optimal\nproblem-dependent regret bound for this problem was first proven recently by\nKaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are\nconceptually simple, easily extend to distributions other than the Beta\ndistribution, and also extend to the more general contextual bandits setting\n[Manuscript, Agrawal and Goyal, 2012].\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:41:18 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3433", "submitter": "Salah A. Aly", "authors": "Hossam M. Zawbaa, Salah A. Aly, Adnan A. Gutub", "title": "A Hajj And Umrah Location Classification System For Video Crowded Scenes", "comments": "9 pages, 10 figures, 2 tables, 3 algirthms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new automatic system for classifying ritual locations in\ndiverse Hajj and Umrah video scenes is investigated. This challenging subject\nhas mostly been ignored in the past due to several problems one of which is the\nlack of realistic annotated video datasets. HUER Dataset is defined to model\nsix different Hajj and Umrah ritual locations[26].\n  The proposed Hajj and Umrah ritual location classifying system consists of\nfour main phases: Preprocessing, segmentation, feature extraction, and location\nclassification phases. The shot boundary detection and background/foregroud\nsegmentation algorithms are applied to prepare the input video scenes into the\nKNN, ANN, and SVM classifiers. The system improves the state of art results on\nHajj and Umrah location classifications, and successfully recognizes the six\nHajj rituals with more than 90% accuracy. The various demonstrated experiments\nshow the promising results.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 20:57:51 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Zawbaa", "Hossam M.", ""], ["Aly", "Salah A.", ""], ["Gutub", "Adnan A.", ""]]}, {"id": "1209.3686", "submitter": "Barzan Mozafari", "authors": "Barzan Mozafari, Purnamrita Sarkar, Michael J. Franklin, Michael I.\n  Jordan, Samuel Madden", "title": "Active Learning for Crowd-Sourced Databases", "comments": "A shorter version of this manuscript has been published in\n  Proceedings of Very Large Data Bases 2015, entitled \"Scaling Up\n  Crowd-Sourcing to Very Large Datasets: A Case for Active Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-sourcing has become a popular means of acquiring labeled data for a\nwide variety of tasks where humans are more accurate than computers, e.g.,\nlabeling images, matching objects, or analyzing sentiment. However, relying\nsolely on the crowd is often impractical even for data sets with thousands of\nitems, due to time and cost constraints of acquiring human input (which cost\npennies and minutes per label). In this paper, we propose algorithms for\nintegrating machine learning into crowd-sourced databases, with the goal of\nallowing crowd-sourcing applications to scale, i.e., to handle larger datasets\nat lower costs. The key observation is that, in many of the above tasks, humans\nand machine learning algorithms can be complementary, as humans are often more\naccurate but slow and expensive, while algorithms are usually less accurate,\nbut faster and cheaper.\n  Based on this observation, we present two new active learning algorithms to\ncombine humans and algorithms together in a crowd-sourced database. Our\nalgorithms are based on the theory of non-parametric bootstrap, which makes our\nresults applicable to a broad class of machine learning models. Our results, on\nthree real-life datasets collected with Amazon's Mechanical Turk, and on 15\nwell-known UCI data sets, show that our methods on average ask humans to label\none to two orders of magnitude fewer items to achieve the same accuracy as a\nbaseline that labels random images, and two to eight times fewer questions than\nprevious active learning schemes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 15:21:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2012 15:45:55 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2012 18:20:04 GMT"}, {"version": "v4", "created": "Sat, 20 Dec 2014 08:56:15 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Mozafari", "Barzan", ""], ["Sarkar", "Purnamrita", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Madden", "Samuel", ""]]}, {"id": "1209.3694", "submitter": "Yifei Ma", "authors": "Yifei Ma, Roman Garnett, Jeff Schneider", "title": "Submodularity in Batch Active Learning and Survey Problems on Gaussian\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world datasets can be represented in the form of a graph whose edge\nweights designate similarities between instances. A discrete Gaussian random\nfield (GRF) model is a finite-dimensional Gaussian process (GP) whose prior\ncovariance is the inverse of a graph Laplacian. Minimizing the trace of the\npredictive covariance Sigma (V-optimality) on GRFs has proven successful in\nbatch active learning classification problems with budget constraints. However,\nits worst-case bound has been missing. We show that the V-optimality on GRFs as\na function of the batch query set is submodular and hence its greedy selection\nalgorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have\nthe absence-of-suppressor (AofS) condition. For active survey problems, we\npropose a similar survey criterion which minimizes 1'(Sigma)1. In practice,\nV-optimality criterion performs better than GPs with mutual information gain\ncriteria and allows nonuniform costs for different nodes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 15:43:11 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Ma", "Yifei", ""], ["Garnett", "Roman", ""], ["Schneider", "Jeff", ""]]}, {"id": "1209.3761", "submitter": "Ming Sun", "authors": "Ming Sun, Carey E. Priebe, Minh Tang", "title": "Generalized Canonical Correlation Analysis for Disparate Data Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold matching works to identify embeddings of multiple disparate data\nspaces into the same low-dimensional space, where joint inference can be\npursued. It is an enabling methodology for fusion and inference from multiple\nand massive disparate data sources. In this paper we focus on a method called\nCanonical Correlation Analysis (CCA) and its generalization Generalized\nCanonical Correlation Analysis (GCCA), which belong to the more general Reduced\nRank Regression (RRR) framework. We present an efficiency investigation of CCA\nand GCCA under different training conditions for a particular text document\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 19:52:38 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Sun", "Ming", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "1209.3818", "submitter": "Alok Raj", "authors": "Alok Raj", "title": "Evolution and the structure of learning agents", "comments": "total 4 pages. Submitted to IEEE Congress on Evolutionary Computation\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the thesis that all learning agents of finite information\nsize are limited by their informational structure in what goals they can\nefficiently learn to achieve in a complex environment. Evolutionary change is\ncritical for creating the required structure for all learning agents in any\ncomplex environment. The thesis implies that there is no efficient universal\nlearning algorithm. An agent can go past the learning limits imposed by its\nstructure only by slow evolutionary change or blind search which in a very\ncomplex environment can only give an agent an inefficient universal learning\ncapability that can work only in evolutionary timescales or improbable luck.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 00:13:53 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2013 01:51:50 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2013 21:32:11 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2013 23:58:47 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Raj", "Alok", ""]]}, {"id": "1209.4115", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Frank C. Meinecke, Klaus-Robert M\\\"uller", "title": "Transferring Subspaces Between Subjects in Brain-Computer Interfacing", "comments": null, "journal-ref": "W. Samek, F. C. Meinecke and K-R. M\\\"uller, Transferring Subspaces\n  Between Subjects in Brain-Computer Interfacing, IEEE Transactions on\n  Biomedical Engineering, 2013", "doi": "10.1109/TBME.2013.2253608", "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compensating changes between a subjects' training and testing session in\nBrain Computer Interfacing (BCI) is challenging but of great importance for a\nrobust BCI operation. We show that such changes are very similar between\nsubjects, thus can be reliably estimated using data from other users and\nutilized to construct an invariant feature space. This novel approach to\nlearning from other subjects aims to reduce the adverse effects of common\nnon-stationarities, but does not transfer discriminative information. This is\nan important conceptual difference to standard multi-subject methods that e.g.\nimprove the covariance matrix estimation by shrinking it towards the average of\nother users or construct a global feature space. These methods do not reduces\nthe shift between training and test data and may produce poor results when\nsubjects have very different signal characteristics. In this paper we compare\nour approach to two state-of-the-art multi-subject methods on toy data and two\ndata sets of EEG recordings from subjects performing motor imagery. We show\nthat it can not only achieve a significant increase in performance, but also\nthat the extracted change patterns allow for a neurophysiologically meaningful\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 22:37:10 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 17:26:03 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Samek", "Wojciech", ""], ["Meinecke", "Frank C.", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1209.4129", "submitter": "John Duchi", "authors": "Yuchen Zhang and John C. Duchi and Martin Wainwright", "title": "Comunication-Efficient Algorithms for Statistical Optimization", "comments": "44 pages, to appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze two communication-efficient algorithms for distributed statistical\noptimization on large-scale data sets. The first algorithm is a standard\naveraging method that distributes the $N$ data samples evenly to $\\nummac$\nmachines, performs separate minimization on each subset, and then averages the\nestimates. We provide a sharp analysis of this average mixture algorithm,\nshowing that under a reasonable set of conditions, the combined parameter\nachieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$.\nWhenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate\nachievable by a centralized algorithm having access to all $\\totalnumobs$\nsamples. The second algorithm is a novel method, based on an appropriate form\nof bootstrap subsampling. Requiring only a single round of communication, it\nhas mean-squared error that decays as $\\order(N^{-1} + (N/m)^{-3})$, and so is\nmore robust to the amount of parallelization. In addition, we show that a\nstochastic gradient-based method attains mean-squared error decaying as\n$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in\nthe rate of convergence. We also provide experimental evaluation of our\nmethods, investigating their performance both on simulated data and on a\nlarge-scale regression problem from the internet search domain. In particular,\nwe show that our methods can be used to efficiently solve an advertisement\nprediction problem from the Chinese SoSo Search Engine, which involves logistic\nregression with $N \\approx 2.4 \\times 10^8$ samples and $d \\approx 740,000$\ncovariates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 01:27:40 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 03:12:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 19:23:38 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Zhang", "Yuchen", ""], ["Duchi", "John C.", ""], ["Wainwright", "Martin", ""]]}, {"id": "1209.4825", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala, Antti Airola, Michiel Stock, Bernard De Baets, Willem\n  Waegeman", "title": "Efficient Regularized Least-Squares Algorithms for Conditional Ranking\n  on Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domains like bioinformatics, information retrieval and social network\nanalysis, one can find learning tasks where the goal consists of inferring a\nranking of objects, conditioned on a particular target object. We present a\ngeneral kernel framework for learning conditional rankings from various types\nof relational data, where rankings can be conditioned on unseen data objects.\nWe propose efficient algorithms for conditional ranking by optimizing squared\nregression and ranking loss functions. We show theoretically, that learning\nwith the ranking loss is likely to generalize better than with the regression\nloss. Further, we prove that symmetry or reciprocity properties of relations\ncan be efficiently enforced in the learned models. Experiments on synthetic and\nreal-world data illustrate that the proposed methods deliver state-of-the-art\nperformance in terms of predictive power and computational efficiency.\nMoreover, we also show empirically that incorporating symmetry or reciprocity\nproperties can improve the generalization performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 14:14:35 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2013 14:23:04 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["Stock", "Michiel", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1209.4893", "submitter": "Xin Xiao", "authors": "Kasturi Varadarajan and Xin Xiao", "title": "On the Sensitivity of Shape Fitting Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this article, we study shape fitting problems, $\\epsilon$-coresets, and\ntotal sensitivity. We focus on the $(j,k)$-projective clustering problems,\nincluding $k$-median/$k$-means, $k$-line clustering, $j$-subspace\napproximation, and the integer $(j,k)$-projective clustering problem. We derive\nupper bounds of total sensitivities for these problems, and obtain\n$\\epsilon$-coresets using these upper bounds. Using a dimension-reduction type\nargument, we are able to greatly simplify earlier results on total sensitivity\nfor the $k$-median/$k$-means clustering problems, and obtain\npositively-weighted $\\epsilon$-coresets for several variants of the\n$(j,k)$-projective clustering problem. We also extend an earlier result on\n$\\epsilon$-coresets for the integer $(j,k)$-projective clustering problem in\nfixed dimension to the case of high dimension.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 19:55:53 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 22:22:46 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Varadarajan", "Kasturi", ""], ["Xiao", "Xin", ""]]}, {"id": "1209.4951", "submitter": "Tu Xu", "authors": "Tu Xu and Junhui Wang", "title": "An efficient model-free estimation of multiclass conditional probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multiclass conditional probability estimation methods, such as\nFisher's discriminate analysis and logistic regression, often require\nrestrictive distributional model assumption. In this paper, a model-free\nestimation method is proposed to estimate multiclass conditional probability\nthrough a series of conditional quantile regression functions. Specifically,\nthe conditional class probability is formulated as difference of corresponding\ncumulative distribution functions, where the cumulative distribution functions\ncan be converted from the estimated conditional quantile regression functions.\nThe proposed estimation method is also efficient as its computation cost does\nnot increase exponentially with the number of classes. The theoretical and\nnumerical studies demonstrate that the proposed estimation method is highly\ncompetitive against the existing competitors, especially when the number of\nclasses is relatively large.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 01:50:43 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 15:08:40 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2013 17:44:53 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Xu", "Tu", ""], ["Wang", "Junhui", ""]]}, {"id": "1209.5019", "submitter": "Gungor Polatkan", "authors": "Gungor Polatkan and Mingyuan Zhou and Lawrence Carin and David Blei\n  and Ingrid Daubechies", "title": "A Bayesian Nonparametric Approach to Image Super-resolution", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution methods form high-resolution images from low-resolution\nimages. In this paper, we develop a new Bayesian nonparametric model for\nsuper-resolution. Our method uses a beta-Bernoulli process to learn a set of\nrecurring visual patterns, called dictionary elements, from the data. Because\nit is nonparametric, the number of elements found is also determined from the\ndata. We test the results on both benchmark and natural images, comparing with\nseveral other models from the research literature. We perform large-scale human\nevaluation experiments to assess the visual quality of the results. In a first\nimplementation, we use Gibbs sampling to approximate the posterior. However,\nthis algorithm is not feasible for large-scale data. To circumvent this, we\nthen develop an online variational Bayes (VB) algorithm. This algorithm finds\nhigh quality dictionaries in a fraction of the time needed by the Gibbs\nsampler.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 21:01:06 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Polatkan", "Gungor", ""], ["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""], ["Blei", "David", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1209.5038", "submitter": "Daniel Gordon", "authors": "Daniel Gordon, Danny Hendler, Lior Rokach", "title": "Fast Randomized Model Generation for Shapelet-Based Time Series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification is a field which has drawn much attention over the\npast decade. A new approach for classification of time series uses\nclassification trees based on shapelets. A shapelet is a subsequence extracted\nfrom one of the time series in the dataset. A disadvantage of this approach is\nthe time required for building the shapelet-based classification tree. The\nsearch for the best shapelet requires examining all subsequences of all lengths\nfrom all time series in the training set.\n  A key goal of this work was to find an evaluation order of the shapelets\nspace which enables fast convergence to an accurate model. The comparative\nanalysis we conducted clearly indicates that a random evaluation order yields\nthe best results. Our empirical analysis of the distribution of high-quality\nshapelets within the shapelets space provides insights into why randomized\nshapelets sampling is superior to alternative evaluation orders.\n  We present an algorithm for randomized model generation for shapelet-based\nclassification that converges extremely quickly to a model with surprisingly\nhigh accuracy after evaluating only an exceedingly small fraction of the\nshapelets space.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 07:50:42 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Gordon", "Daniel", ""], ["Hendler", "Danny", ""], ["Rokach", "Lior", ""]]}, {"id": "1209.5251", "submitter": "Josef Moudrik", "authors": "Petr Baudi\\v{s}, Josef Moud\\v{r}\\'ik", "title": "On Move Pattern Trends in a Large Go Games Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We process a large corpus of game records of the board game of Go and propose\na way of extracting summary information on played moves. We then apply several\nbasic data-mining methods on the summary information to identify the most\ndifferentiating features within the summary information, and discuss their\ncorrespondence with traditional Go knowledge. We show statistically significant\nmappings of the features to player attributes such as playing strength or\ninformally perceived \"playing style\" (e.g. territoriality or aggressivity),\ndescribe accurate classifiers for these attributes, and propose applications\nincluding seeding real-work ranks of internet players, aiding in Go study and\ntuning of Go-playing programs, or contribution to Go-theoretical discussion on\nthe scope of \"playing style\".\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 12:54:18 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Baudi\u0161", "Petr", ""], ["Moud\u0159\u00edk", "Josef", ""]]}, {"id": "1209.5260", "submitter": "Mingkui Tan", "authors": "Mingkui Tan and Ivor W. Tsang and Li Wang", "title": "Towards Ultrahigh Dimensional Feature Selection for Big Data", "comments": "61 pages", "journal-ref": "JMLR, 15(1): 1371 - 1429, 2014", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new adaptive feature scaling scheme for\nultrahigh-dimensional feature selection on Big Data. To solve this problem\neffectively, we first reformulate it as a convex semi-infinite programming\n(SIP) problem and then propose an efficient \\emph{feature generating paradigm}.\nIn contrast with traditional gradient-based approaches that conduct\noptimization on all input features, the proposed method iteratively activates a\ngroup of features and solves a sequence of multiple kernel learning (MKL)\nsubproblems of much reduced scale. To further speed up the training, we propose\nto solve the MKL subproblems in their primal forms through a modified\naccelerated proximal gradient approach. Due to such an optimization scheme,\nsome efficient cache techniques are also developed. The feature generating\nparadigm can guarantee that the solution converges globally under mild\nconditions and achieve lower feature selection bias. Moreover, the proposed\nmethod can tackle two challenging tasks in feature selection: 1) group-based\nfeature selection with complex structures and 2) nonlinear feature selection\nwith explicit feature mappings. Comprehensive experiments on a wide range of\nsynthetic and real-world datasets containing tens of million data points with\n$O(10^{14})$ features demonstrate the competitive performance of the proposed\nmethod over state-of-the-art feature selection methods in terms of\ngeneralization performance and training efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 13:23:39 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 15:57:22 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Tan", "Mingkui", ""], ["Tsang", "Ivor W.", ""], ["Wang", "Li", ""]]}, {"id": "1209.5335", "submitter": "Arash Einolghozati", "authors": "Erman Ayday, Arash Einolghozati, Faramarz Fekri", "title": "BPRS: Belief Propagation Based Iterative Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the first application of the Belief Propagation\n(BP) algorithm in the design of recommender systems. We formulate the\nrecommendation problem as an inference problem and aim to compute the marginal\nprobability distributions of the variables which represent the ratings to be\npredicted. However, computing these marginal probability functions is\ncomputationally prohibitive for large-scale systems. Therefore, we utilize the\nBP algorithm to efficiently compute these functions. Recommendations for each\nactive user are then iteratively computed by probabilistic message passing. As\nopposed to the previous recommender algorithms, BPRS does not require solving\nthe recommendation problem for all the users if it wishes to update the\nrecommendations for only a single active. Further, BPRS computes the\nrecommendations for each user with linear complexity and without requiring a\ntraining period. Via computer simulations (using the 100K MovieLens dataset),\nwe verify that BPRS iteratively reduces the error in the predicted ratings of\nthe users until it converges. Finally, we confirm that BPRS is comparable to\nthe state of art methods such as Correlation-based neighborhood model (CorNgbr)\nand Singular Value Decomposition (SVD) in terms of rating and precision\naccuracy. Therefore, we believe that the BP-based recommendation algorithm is a\nnew promising approach which offers a significant advantage on scalability\nwhile providing competitive accuracy for the recommender systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 16:59:12 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Ayday", "Erman", ""], ["Einolghozati", "Arash", ""], ["Fekri", "Faramarz", ""]]}, {"id": "1209.5350", "submitter": "Adel Javanmard", "authors": "Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade", "title": "Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints", "comments": "38 pages, 6 figures, 2 tables, applications in topic models and\n  Bayesian networks are studied. Simulation section is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:11:02 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2012 06:34:45 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 18:25:32 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Javanmard", "Adel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1209.5467", "submitter": "Berdakh Abibullaev", "authors": "Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Sang-Hyeon Jin, Jeon-Il\n  Moon", "title": "Minimizing inter-subject variability in fNIRS based Brain Computer\n  Interfaces via multiple-kernel support vector learning", "comments": "This paper has been withdrawn by the author due to an error in\n  equation 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain signal variability in the measurements obtained from different subjects\nduring different sessions significantly deteriorates the accuracy of most\nbrain-computer interface (BCI) systems. Moreover these variabilities, also\nknown as inter-subject or inter-session variabilities, require lengthy\ncalibration sessions before the BCI system can be used. Furthermore, the\ncalibration session has to be repeated for each subject independently and\nbefore use of the BCI due to the inter-session variability. In this study, we\npresent an algorithm in order to minimize the above-mentioned variabilities and\nto overcome the time-consuming and usually error-prone calibration time. Our\nalgorithm is based on linear programming support-vector machines and their\nextensions to a multiple kernel learning framework. We tackle the inter-subject\nor -session variability in the feature spaces of the classifiers. This is done\nby incorporating each subject- or session-specific feature spaces into much\nricher feature spaces with a set of optimal decision boundaries. Each decision\nboundary represents the subject- or a session specific spatio-temporal\nvariabilities of neural signals. Consequently, a single classifier with\nmultiple feature spaces will generalize well to new unseen test patterns even\nwithout the calibration steps. We demonstrate that classifiers maintain good\nperformances even under the presence of a large degree of BCI variability. The\npresent study analyzes BCI variability related to oxy-hemoglobin neural signals\nmeasured using a functional near-infrared spectroscopy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 01:33:01 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2012 08:05:03 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2013 01:48:28 GMT"}, {"version": "v4", "created": "Wed, 8 May 2013 00:54:19 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Abibullaev", "Berdakh", ""], ["An", "Jinung", ""], ["Lee", "Seung-Hyun", ""], ["Jin", "Sang-Hyeon", ""], ["Moon", "Jeon-Il", ""]]}, {"id": "1209.5477", "submitter": "Yichao Lu", "authors": "Yichao Lu and Dean P. Foster", "title": "Optimal Weighting of Multi-View Data with Low Dimensional Hidden States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Processing (NLP) tasks, data often has the following two\nproperties: First, data can be chopped into multi-views which has been\nsuccessfully used for dimension reduction purposes. For example, in topic\nclassification, every paper can be chopped into the title, the main text and\nthe references. However, it is common that some of the views are less noisier\nthan other views for supervised learning problems. Second, unlabeled data are\neasy to obtain while labeled data are relatively rare. For example, articles\noccurred on New York Times in recent 10 years are easy to grab but having them\nclassified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less\nnoisy features are preferred before running supervised learning methods. In\nthis paper we propose an unsupervised algorithm which optimally weights\nfeatures from different views when these views are generated from a low\ndimensional hidden state, which occurs in widely used models like Mixture\nGaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation\n(LDA).\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 02:54:49 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 05:15:07 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Lu", "Yichao", ""], ["Foster", "Dean P.", ""]]}, {"id": "1209.5549", "submitter": "David Balduzzi", "authors": "David Balduzzi and Michel Besserve", "title": "Towards a learning-theoretic analysis of spike-timing dependent\n  plasticity", "comments": "To appear in Adv. Neural Inf. Proc. Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a learning-theoretic perspective on how synaptic\nplasticity benefits global brain functioning. We introduce a model, the\nselectron, that (i) arises as the fast time constant limit of leaky\nintegrate-and-fire neurons equipped with spiking timing dependent plasticity\n(STDP) and (ii) is amenable to theoretical analysis. We show that the selectron\nencodes reward estimates into spikes and that an error bound on spikes is\ncontrolled by a spiking margin and the sum of synaptic weights. Moreover, the\nefficacy of spikes (their usefulness to other reward maximizing selectrons)\nalso depends on total synaptic strength. Finally, based on our analysis, we\npropose a regularized version of STDP, and show the regularization improves the\nrobustness of neuronal learning when faced with multiple stimuli.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 09:23:41 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Balduzzi", "David", ""], ["Besserve", "Michel", ""]]}, {"id": "1209.5561", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Supervised Blockmodelling", "comments": "Workshop on Collective Learning and Inference on Structured Data 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective classification models attempt to improve classification\nperformance by taking into account the class labels of related instances.\nHowever, they tend not to learn patterns of interactions between classes and/or\nmake the assumption that instances of the same class link to each other\n(assortativity assumption). Blockmodels provide a solution to these issues,\nbeing capable of modelling assortative and disassortative interactions, and\nlearning the pattern of interactions in the form of a summary network. The\nSupervised Blockmodel provides good classification performance using link\nstructure alone, whilst simultaneously providing an interpretable summary of\nnetwork interactions to allow a better understanding of the data. This work\nexplores three variants of supervised blockmodels of varying complexity and\ntests them on four structurally different real world networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 09:59:56 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1209.5601", "submitter": "Fan Min", "authors": "Fan Min, Qinghua Hu, William Zhu", "title": "Feature selection with test cost constraint", "comments": "23 pages", "journal-ref": null, "doi": "10.1016/j.ijar.2013.04.003", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important preprocessing step in machine learning and\ndata mining. In real-world applications, costs, including money, time and other\nresources, are required to acquire the features. In some cases, there is a test\ncost constraint due to limited resources. We shall deliberately select an\ninformative and cheap feature subset for classification. This paper proposes\nthe feature selection with test cost constraint problem for this issue. The new\nproblem has a simple form while described as a constraint satisfaction problem\n(CSP). Backtracking is a general algorithm for CSP, and it is efficient in\nsolving the new problem on medium-sized data. As the backtracking algorithm is\nnot scalable to large datasets, a heuristic algorithm is also developed.\nExperimental results show that the heuristic algorithm can find the optimal\nsolution in most cases. We also redefine some existing feature selection\nproblems in rough sets, especially in decision-theoretic rough sets, from the\nviewpoint of CSP. These new definitions provide insight to some new research\ndirections.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 13:21:40 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Min", "Fan", ""], ["Hu", "Qinghua", ""], ["Zhu", "William", ""]]}, {"id": "1209.5833", "submitter": "Makiko Konoshima", "authors": "Makiko Konoshima and Yui Noma", "title": "Locality-Sensitive Hashing with Margin Based Feature Selection", "comments": "9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning method with feature selection for Locality-Sensitive\nHashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.\nThese bit arrays can be used to perform similarity searches and personal\nauthentication. The proposed method uses bit arrays longer than those used in\nthe end for similarity and other searches and by learning selects the bits that\nwill be used. We demonstrated this method can effectively perform optimization\nfor cases such as fingerprint images with a large number of labels and\nextremely few data that share the same labels, as well as verifying that it is\nalso effective for natural images, handwritten digits, and speech features.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 05:26:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 06:21:09 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Konoshima", "Makiko", ""], ["Noma", "Yui", ""]]}, {"id": "1209.5991", "submitter": "Satyaki Mahalanabis", "authors": "Satyaki Mahalanabis, Daniel Stefankovic", "title": "Subset Selection for Gaussian Markov Random Fields", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a Gaussian Markov random field, we consider the problem of selecting a\nsubset of variables to observe which minimizes the total expected squared\nprediction error of the unobserved variables. We first show that finding an\nexact solution is NP-hard even for a restricted class of Gaussian Markov random\nfields, called Gaussian free fields, which arise in semi-supervised learning\nand computer vision. We then give a simple greedy approximation algorithm for\nGaussian free fields on arbitrary graphs. Finally, we give a message passing\nalgorithm for general Gaussian Markov random fields on bounded tree-width\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 16:31:32 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Mahalanabis", "Satyaki", ""], ["Stefankovic", "Daniel", ""]]}, {"id": "1209.6001", "submitter": "Jonathan Shapiro", "authors": "Ruefei He and Jonathan Shapiro", "title": "Bayesian Mixture Models for Frequent Itemset Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary-transaction data-mining, traditional frequent itemset mining often\nproduces results which are not straightforward to interpret. To overcome this\nproblem, probability models are often used to produce more compact and\nconclusive results, albeit with some loss of accuracy. Bayesian statistics have\nbeen widely used in the development of probability models in machine learning\nin recent years and these methods have many advantages, including their\nabilities to avoid overfitting. In this paper, we develop two Bayesian mixture\nmodels with the Dirichlet distribution prior and the Dirichlet process (DP)\nprior to improve the previous non-Bayesian mixture model developed for\ntransaction dataset mining. We implement the inference of both mixture models\nusing two methods: a collapsed Gibbs sampling scheme and a variational\napproximation algorithm. Experiments in several benchmark problems have shown\nthat both mixture models achieve better performance than a non-Bayesian mixture\nmodel. The variational algorithm is the faster of the two approaches while the\nGibbs sampling method achieves a more accurate results. The Dirichlet process\nmixture model can automatically grow to a proper complexity for a better\napproximation. Once the model is built, it can be very fast to query and run\nanalysis on (typically 10 times faster than Eclat, as we will show in the\nexperiment section). However, these approaches also show that mixture models\nunderestimate the probabilities of frequent itemsets. Consequently, these\nmodels have a higher sensitivity but a lower specificity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 16:41:59 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["He", "Ruefei", ""], ["Shapiro", "Jonathan", ""]]}, {"id": "1209.6004", "submitter": "Sean Gerrish", "authors": "Sean M. Gerrish and David M. Blei", "title": "The Issue-Adjusted Ideal Point Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model of issue-specific voting behavior. This model can be used\nto explore lawmakers' personal voting patterns of voting by issue area,\nproviding an exploratory window into how the language of the law is correlated\nwith political support. We derive approximate posterior inference algorithms\nbased on variational methods. Across 12 years of legislative data, we\ndemonstrate both improvement in heldout prediction performance and the model's\nutility in interpreting an inherently multi-dimensional space.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 17:00:21 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Gerrish", "Sean M.", ""], ["Blei", "David M.", ""]]}, {"id": "1209.6070", "submitter": "Tanvir Ahmed", "authors": "Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman", "title": "Movie Popularity Classification based on Inherent Movie Attributes using\n  C4.5,PART and Correlation Coefficient", "comments": "6 pages", "journal-ref": "IEEE/OSA/IAPR International Conference on Informatics, Electronics\n  & Vision (ICIEV2012), pp. 747-752, May 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundance of movie data across the internet makes it an obvious candidate for\nmachine learning and knowledge discovery. But most researches are directed\ntowards bi-polar classification of movie or generation of a movie\nrecommendation system based on reviews given by viewers on various internet\nsites. Classification of movie popularity based solely on attributes of a movie\ni.e. actor, actress, director rating, language, country and budget etc. has\nbeen less highlighted due to large number of attributes that are associated\nwith each movie and their differences in dimensions. In this paper, we propose\nclassification scheme of pre-release movie popularity based on inherent\nattributes using C4.5 and PART classifier algorithm and define the relation\nbetween attributes of post release movies using correlation coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 20:30:02 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Asad", "Khalid Ibnal", ""], ["Ahmed", "Tanvir", ""], ["Rahman", "Md. Saiedur", ""]]}, {"id": "1209.6329", "submitter": "Yoav Haimovitch", "authors": "Yoav Haimovitch, Koby Crammer, Shie Mannor", "title": "More Is Better: Large Scale Partially-supervised Sentiment\n  Classification - Appendix", "comments": "This is the appendix to the paper \"More Is Better: Large Scale\n  Partially-supervised Sentiment Classification\" accepted to ACML 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a bootstrapping algorithm to learn from partially labeled data,\nand the results of an empirical study for using it to improve performance of\nsentiment classification using up to 15 million unlabeled Amazon product\nreviews. Our experiments cover semi-supervised learning, domain adaptation and\nweakly supervised learning. In some cases our methods were able to reduce test\nerror by more than half using such large amount of data.\n  NOTICE: This is only the supplementary material.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 18:57:26 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Haimovitch", "Yoav", ""], ["Crammer", "Koby", ""], ["Mannor", "Shie", ""]]}, {"id": "1209.6342", "submitter": "Jie Cheng MS", "authors": "Jie Cheng, Elizaveta Levina, Pei Wang and Ji Zhu", "title": "Sparse Ising Models with Covariates", "comments": "32 pages (including 5 pages of appendix), 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of work fitting Ising models to multivariate binary data\nin order to understand the conditional dependency relationships between the\nvariables. However, additional covariates are frequently recorded together with\nthe binary data, and may influence the dependence relationships. Motivated by\nsuch a dataset on genomic instability collected from tumor samples of several\ntypes, we propose a sparse covariate dependent Ising model to study both the\nconditional dependency within the binary data and its relationship with the\nadditional covariates. This results in subject-specific Ising models, where the\nsubject's covariates influence the strength of association between the genes.\nAs in all exploratory data analysis, interpretability of results is important,\nand we use L1 penalties to induce sparsity in the fitted graphs and in the\nnumber of selected covariates. Two algorithms to fit the model are proposed and\ncompared on a set of simulated data, and asymptotic results are established.\nThe results on the tumor dataset and their biological significance are\ndiscussed in detail.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 19:43:44 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Cheng", "Jie", ""], ["Levina", "Elizaveta", ""], ["Wang", "Pei", ""], ["Zhu", "Ji", ""]]}, {"id": "1209.6393", "submitter": "Alex Bronstein", "authors": "Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro", "title": "Learning Robust Low-Rank Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a comprehensive framework for learning robust\nlow-rank representations by combining and extending recent ideas for learning\nfast sparse coding regressors with structured non-convex optimization\ntechniques. This approach connects robust principal component analysis (RPCA)\nwith dictionary learning techniques and allows its approximation via trainable\nencoders. We propose an efficient feed-forward architecture derived from an\noptimization algorithm designed to exactly solve robust low dimensional\nprojections. This architecture, in combination with different training\nobjective functions, allows the regressors to be used as online approximants of\nthe exact offline RPCA problem or as RPCA-based neural networks. Simple\nmodifications of these encoders can handle challenging extensions, such as the\ninclusion of geometric data transformations. We present several examples with\nreal data from image, audio, and video processing. When used to approximate\nRPCA, our basic implementation shows several orders of magnitude speedup\ncompared to the exact solvers with almost no performance degradation. We show\nthe strength of the inclusion of learning to the RPCA approach on a music\nsource separation application, where the encoders outperform the exact RPCA\nalgorithms, which are already reported to produce state-of-the-art results on a\nbenchmark database. Our preliminary implementation on an iPad shows\nfaster-than-real-time performance with minimal latency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 23:03:53 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Sprechmann", "Pablo", ""], ["Bronstein", "Alex M.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1209.6409", "submitter": "Mehmet Donmez", "authors": "Mehmet A. Donmez, Sait Tunc, Suleyman S. Kozat", "title": "A Deterministic Analysis of an Online Convex Mixture of Expert\n  Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1203.4209", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze an online learning algorithm that adaptively combines outputs of\ntwo constituent algorithms (or the experts) running in parallel to model an\nunknown desired signal. This online learning algorithm is shown to achieve (and\nin some cases outperform) the mean-square error (MSE) performance of the best\nconstituent algorithm in the mixture in the steady-state. However, the MSE\nanalysis of this algorithm in the literature uses approximations and relies on\nstatistical models on the underlying signals and systems. Hence, such an\nanalysis may not be useful or valid for signals generated by various real life\nsystems that show high degrees of nonstationarity, limit cycles and, in many\ncases, that are even chaotic. In this paper, we produce results in an\nindividual sequence manner. In particular, we relate the time-accumulated\nsquared estimation error of this online algorithm at any time over any interval\nto the time accumulated squared estimation error of the optimal convex mixture\nof the constituent algorithms directly tuned to the underlying signal in a\ndeterministic sense without any statistical assumptions. In this sense, our\nanalysis provides the transient, steady-state and tracking behavior of this\nalgorithm in a strong sense without any approximations in the derivations or\nstatistical assumptions on the underlying signals such that our results are\nguaranteed to hold. We illustrate the introduced results through examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 01:46:47 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Donmez", "Mehmet A.", ""], ["Tunc", "Sait", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1209.6419", "submitter": "Xiaotong Yuan", "authors": "Xiao-Tong Yuan and Tong Zhang", "title": "Partial Gaussian Graphical Model Estimation", "comments": "32 pages, 5 figures, 4tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the partial estimation of Gaussian graphical models from\nhigh-dimensional empirical observations. We derive a convex formulation for\nthis problem using $\\ell_1$-regularized maximum-likelihood estimation, which\ncan be solved via a block coordinate descent algorithm. Statistical estimation\nperformance can be established for our method. The proposed approach has\ncompetitive empirical performance compared to existing methods, as demonstrated\nby various experiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 04:12:14 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Zhang", "Tong", ""]]}, {"id": "1209.6425", "submitter": "Houtao Deng", "authors": "Houtao Deng and George Runger", "title": "Gene selection with guided regularized random forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regularized random forest (RRF) was recently proposed for feature\nselection by building only one ensemble. In RRF the features are evaluated on a\npart of the training data at each tree node. We derive an upper bound for the\nnumber of distinct Gini information gain values in a node, and show that many\nfeatures can share the same information gain at a node with a small number of\ninstances and a large number of features. Therefore, in a node with a small\nnumber of instances, RRF is likely to select a feature not strongly relevant.\nHere an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In\nGRRF, the importance scores from an ordinary random forest (RF) are used to\nguide the feature selection process in RRF. Experiments on 10 gene data sets\nshow that the accuracy performance of GRRF is, in general, more robust than RRF\nwhen their parameters change. GRRF is computationally efficient, can select\ncompact feature subsets, and has competitive accuracy performance, compared to\nRRF, varSelRF and LASSO logistic regression (with evaluations from an RF\nclassifier). Also, RF applied to the features selected by RRF with the minimal\nregularization outperforms RF applied to all the features for most of the data\nsets considered here. Therefore, if accuracy is considered more important than\nthe size of the feature subset, RRF with the minimal regularization may be\nconsidered. We use the accuracy performance of RF, a strong classifier, to\nevaluate feature selection methods, and illustrate that weak classifiers are\nless capable of capturing the information contained in a feature subset. Both\nRRF and GRRF were implemented in the \"RRF\" R package available at CRAN, the\nofficial R package archive.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 04:59:33 GMT"}, {"version": "v2", "created": "Sat, 25 May 2013 03:50:59 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2013 05:41:39 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""]]}, {"id": "1209.6525", "submitter": "Marcelo Fiori", "authors": "Marcelo Fiori, Pablo Mus\\'e, Guillermo Sapiro", "title": "A Complete System for Candidate Polyps Detection in Virtual Colonoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer tomographic colonography, combined with computer-aided detection, is\na promising emerging technique for colonic polyp analysis. We present a\ncomplete pipeline for polyp detection, starting with a simple colon\nsegmentation technique that enhances polyps, followed by an adaptive-scale\ncandidate polyp delineation and classification based on new texture and\ngeometric features that consider both the information in the candidate polyp\nlocation and its immediate surrounding area. The proposed system is tested with\nground truth data, including flat and small polyps which are hard to detect\neven with optical colonoscopy. For polyps larger than 6mm in size we achieve\n100% sensitivity with just 0.9 false positives per case, and for polyps larger\nthan 3mm in size we achieve 93% sensitivity with 2.8 false positives per case.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 14:09:30 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Fiori", "Marcelo", ""], ["Mus\u00e9", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1209.6561", "submitter": "Giorgos Borboudakis", "authors": "Giorgos Borboudakis and Ioannis Tsamardinos", "title": "Scoring and Searching over Bayesian Networks with Causal and Associative\n  Priors", "comments": "Accepted for publication to the 29th Conference on Uncertainty in\n  Artificial Intelligence (UAI-2013). The content of the paper is identical to\n  the published one, but the compiler at arXiv produces a 11 page long paper,\n  whereas the compiler we used produces a 10 page long paper (page limit for\n  the conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant theoretical advantage of search-and-score methods for learning\nBayesian Networks is that they can accept informative prior beliefs for each\npossible network, thus complementing the data. In this paper, a method is\npresented for assigning priors based on beliefs on the presence or absence of\ncertain paths in the true network. Such beliefs correspond to knowledge about\nthe possible causal and associative relations between pairs of variables. This\ntype of knowledge naturally arises from prior experimental and observational\ndata, among others. In addition, a novel search-operator is proposed to take\nadvantage of such prior knowledge. Experiments show that, using path beliefs\nimproves the learning of the skeleton, as well as the edge directions in the\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 16:06:09 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 19:57:02 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Borboudakis", "Giorgos", ""], ["Tsamardinos", "Ioannis", ""]]}]