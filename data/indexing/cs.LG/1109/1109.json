[{"id": "1109.0093", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux (INRIA Paris - Rocquencourt, LIENS), Francis Bach\n  (INRIA Paris - Rocquencourt, LIENS)", "title": "Local Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation, a.k.a. Parzen windows, is a popular density\nestimation method, which can be used for outlier detection or clustering. With\nmultivariate data, its performance is heavily reliant on the metric used within\nthe kernel. Most earlier work has focused on learning only the bandwidth of the\nkernel (i.e., a scalar multiplicative factor). In this paper, we propose to\nlearn a full Euclidean metric through an expectation-minimization (EM)\nprocedure, which can be seen as an unsupervised counterpart to neighbourhood\ncomponent analysis (NCA). In order to avoid overfitting with a fully\nnonparametric density estimator in high dimensions, we also consider a\nsemi-parametric Gaussian-Parzen density model, where some of the variables are\nmodelled through a jointly Gaussian density, while others are modelled through\nParzen windows. For these two models, EM leads to simple closed-form updates\nbased on matrix inversions and eigenvalue decompositions. We show empirically\nthat our method leads to density estimators with higher test-likelihoods than\nnatural competing methods, and that the metrics may be used within most\nunsupervised learning techniques that rely on such metrics, such as spectral\nclustering or manifold learning methods. Finally, we present a stochastic\napproximation scheme which allows for the use of this method in a large-scale\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 05:28:55 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 07:03:58 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2011 04:16:06 GMT"}, {"version": "v4", "created": "Mon, 10 Dec 2012 09:00:47 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Roux", "Nicolas Le", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1109.0105", "submitter": "Abhradeep Thakurta", "authors": "Prateek Jain, Pravesh Kothari, Abhradeep Thakurta", "title": "Differentially Private Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of preserving privacy in the online\nlearning setting. We study the problem in the online convex programming (OCP)\nframework---a popular online learning setting with several interesting\ntheoretical and practical implications---while using differential privacy as\nthe formal privacy measure. For this problem, we distill two critical\nattributes that a private OCP algorithm should have in order to provide\nreasonable privacy as well as utility guarantees: 1) linearly decreasing\nsensitivity, i.e., as new data points arrive their effect on the learning model\ndecreases, 2) sub-linear regret bound---regret bound is a popular\ngoodness/utility measure of an online learning algorithm.\n  Given an OCP algorithm that satisfies these two conditions, we provide a\ngeneral framework to convert the given algorithm into a privacy preserving OCP\nalgorithm with good (sub-linear) regret. We then illustrate our approach by\nconverting two popular online learning algorithms into their differentially\nprivate variants while guaranteeing sub-linear regret ($O(\\sqrt{T})$). Next, we\nconsider the special case of online linear regression problems, a practically\nimportant class of online learning problems, for which we generalize an\napproach by Dwork et al. to provide a differentially private algorithm with\njust $O(\\log^{1.5} T)$ regret. Finally, we show that our online learning\nframework can be used to provide differentially private algorithms for offline\nlearning as well. For the offline learning problem, our approach obtains better\nerror bounds as well as can handle larger class of problems than the existing\nstate-of-the-art methods Chaudhuri et al.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 06:43:23 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2011 17:10:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jain", "Prateek", ""], ["Kothari", "Pravesh", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1109.0325", "submitter": "Kristen Pudenz", "authors": "Kristen L. Pudenz, Daniel A. Lidar", "title": "Quantum adiabatic machine learning", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": "10.1007/s11128-012-0506-4", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach to machine learning and anomaly detection via quantum\nadiabatic evolution. In the training phase we identify an optimal set of weak\nclassifiers, to form a single strong classifier. In the testing phase we\nadiabatically evolve one or more strong classifiers on a superposition of\ninputs in order to find certain anomalous elements in the classification space.\nBoth the training and testing phases are executed via quantum adiabatic\nevolution. We apply and illustrate this approach in detail to the problem of\nsoftware verification and validation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 23:10:31 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Pudenz", "Kristen L.", ""], ["Lidar", "Daniel A.", ""]]}, {"id": "1109.0455", "submitter": "Kenji Fukumizu", "authors": "Kenji Fukumizu and Chenlei Leng", "title": "Gradient-based kernel dimension reduction for supervised learning", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel kernel approach to linear dimension reduction for\nsupervised learning. The purpose of the dimension reduction is to find\ndirections in the input space to explain the output as effectively as possible.\nThe proposed method uses an estimator for the gradient of regression function,\nbased on the covariance operators on reproducing kernel Hilbert spaces. In\ncomparison with other existing methods, the proposed one has wide applicability\nwithout strong assumptions on the distributions or the type of variables, and\nuses computationally simple eigendecomposition. Experimental results show that\nthe proposed method successfully finds the effective directions with efficient\ncomputation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 14:27:25 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Fukumizu", "Kenji", ""], ["Leng", "Chenlei", ""]]}, {"id": "1109.0486", "submitter": "Bert Kappen", "authors": "Hilbert J. Kappen, Vicen\\c{c} G\\'omez", "title": "The Variational Garrote", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new variational method for sparse regression\nusing $L_0$ regularization. The variational parameters appear in the\napproximate model in a way that is similar to Breiman's Garrote model. We refer\nto this method as the variational Garrote (VG). We show that the combination of\nthe variational approximation and $L_0$ regularization has the effect of making\nthe problem effectively of maximal rank even when the number of samples is\nsmall compared to the number of variables. The VG is compared numerically with\nthe Lasso method, ridge regression and the recently introduced paired mean\nfield method (PMF) (M. Titsias & M. L\\'azaro-Gredilla., NIPS 2012). Numerical\nresults show that the VG and PMF yield more accurate predictions and more\naccurately reconstruct the true model than the other methods. It is shown that\nthe VG finds correct solutions when the Lasso solution is inconsistent due to\nlarge input correlations. Globally, VG is significantly faster than PMF and\ntends to perform better as the problems become denser and in problems with\nstrongly correlated inputs. The naive implementation of the VG scales cubic\nwith the number of features. By introducing Lagrange multipliers we obtain a\ndual formulation of the problem that scales cubic in the number of samples, but\nclose to linear in the number of features.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 15:48:23 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 15:54:57 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2012 16:07:03 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Kappen", "Hilbert J.", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1109.0507", "submitter": "Benjamin Rubinstein", "authors": "Adam Barth, Saung Li, Benjamin I. P. Rubinstein, Dawn Song", "title": "How Open Should Open Source Be?", "comments": "19 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": "UCB/EECS-2011-98", "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many open-source projects land security fixes in public repositories before\nshipping these patches to users. This paper presents attacks on such projects -\ntaking Firefox as a case-study - that exploit patch metadata to efficiently\nsearch for security patches prior to shipping. Using access-restricted bug\nreports linked from patch descriptions, security patches can be immediately\nidentified for 260 out of 300 days of Firefox 3 development. In response to\nMozilla obfuscating descriptions, we show that machine learning can exploit\nmetadata such as patch author to search for security patches, extending the\ntotal window of vulnerability by 5 months in an 8 month period when examining\nup to two patches daily. Finally we present strong evidence that further\nmetadata obfuscation is unlikely to prevent information leaks, and we argue\nthat open-source projects instead ought to keep security patches secret until\nthey are ready to be released.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 17:35:50 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Barth", "Adam", ""], ["Li", "Saung", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Song", "Dawn", ""]]}, {"id": "1109.0820", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Yonatan Wexler and Amnon Shashua", "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiclass prediction is the problem of classifying an object into a relevant\ntarget class. We consider the problem of learning a multiclass predictor that\nuses only few features, and in particular, the number of used features should\nincrease sub-linearly with the number of possible classes. This implies that\nfeatures should be shared by several classes. We describe and analyze the\nShareBoost algorithm for learning a multiclass predictor that uses few shared\nfeatures. We prove that ShareBoost efficiently finds a predictor that uses few\nshared features (if such a predictor exists) and that it has a small\ngeneralization error. We also describe how to use ShareBoost for learning a\nnon-linear predictor that has a fast evaluation time. In a series of\nexperiments with natural data sets we demonstrate the benefits of ShareBoost\nand evaluate its success relatively to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 07:52:17 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Wexler", "Yonatan", ""], ["Shashua", "Amnon", ""]]}, {"id": "1109.0895", "submitter": "Anis Charrada", "authors": "Anis Charrada, Abdelaziz Samet", "title": "Nonlinear Channel Estimation for OFDM System by Complex LS-SVM under\n  High Mobility Conditions", "comments": "11 pages", "journal-ref": "International Journal of Wireless & Mobile Networks (IJWMN) Vol.\n  3, No. 4, August 2011", "doi": "10.5121/ijwmn.2011.3412", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A nonlinear channel estimator using complex Least Square Support Vector\nMachines (LS-SVM) is proposed for pilot-aided OFDM system and applied to Long\nTerm Evolution (LTE) downlink under high mobility conditions. The estimation\nalgorithm makes use of the reference signals to estimate the total frequency\nresponse of the highly selective multipath channel in the presence of\nnon-Gaussian impulse noise interfering with pilot signals. Thus, the algorithm\nmaps trained data into a high dimensional feature space and uses the structural\nrisk minimization (SRM) principle to carry out the regression estimation for\nthe frequency response function of the highly selective channel. The\nsimulations show the effectiveness of the proposed method which has good\nperformance and high precision to track the variations of the fading channels\ncompared to the conventional LS method and it is robust at high speed mobility.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 21:40:00 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Charrada", "Anis", ""], ["Samet", "Abdelaziz", ""]]}, {"id": "1109.1062", "submitter": "Victo Sudha george", "authors": "G. Victo Sudha George and V.Cyril Raj", "title": "Review on Feature Selection Techniques and the Impact of SVM for Cancer\n  Classification using Gene Expression Profile", "comments": "12 pages", "journal-ref": "International Journal of Computer Science & Engineering Survey\n  (IJCSES) Vol.2, No.3, International Journal of Computer Science & Engineering\n  Survey (IJCSES) Vol.2, No.3, August 2011", "doi": "10.5121/ijcses.2011.2302", "report-no": null, "categories": "cs.CE cs.ET cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DNA microarray technology has modernized the approach of biology research\nin such a way that scientists can now measure the expression levels of\nthousands of genes simultaneously in a single experiment. Gene expression\nprofiles, which represent the state of a cell at a molecular level, have great\npotential as a medical diagnosis tool. But compared to the number of genes\ninvolved, available training data sets generally have a fairly small sample\nsize for classification. These training data limitations constitute a challenge\nto certain classification methodologies. Feature selection techniques can be\nused to extract the marker genes which influence the classification accuracy\neffectively by eliminating the un wanted noisy and redundant genes This paper\npresents a review of feature selection techniques that have been employed in\nmicro array data based cancer classification and also the predominant role of\nSVM for cancer classification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 04:42:55 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["George", "G. Victo Sudha", ""], ["Raj", "V. Cyril", ""]]}, {"id": "1109.1355", "submitter": "Michael Mahoney", "authors": "Mihai Cucuringu and Michael W. Mahoney", "title": "Localization on low-order eigenvectors of data matrices", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenvector localization refers to the situation when most of the components\nof an eigenvector are zero or near-zero. This phenomenon has been observed on\neigenvectors associated with extremal eigenvalues, and in many of those cases\nit can be meaningfully interpreted in terms of \"structural heterogeneities\" in\nthe data. For example, the largest eigenvectors of adjacency matrices of large\ncomplex networks often have most of their mass localized on high-degree nodes;\nand the smallest eigenvectors of the Laplacians of such networks are often\nlocalized on small but meaningful community-like sets of nodes. Here, we\ndescribe localization associated with low-order eigenvectors, i.e.,\neigenvectors corresponding to eigenvalues that are not extremal but that are\n\"buried\" further down in the spectrum. Although we have observed it in several\nunrelated applications, this phenomenon of low-order eigenvector localization\ndefies common intuitions and simple explanations, and it creates serious\ndifficulties for the applicability of popular eigenvector-based machine\nlearning and data analysis tools. After describing two examples where low-order\neigenvector localization arises, we present a very simple model that\nqualitatively reproduces several of the empirically-observed results. This\nmodel suggests certain coarse structural similarities among the\nseemingly-unrelated applications where we have observed low-order eigenvector\nlocalization, and it may be used as a diagnostic tool to help extract insight\nfrom data graphs when such low-order eigenvector localization is present.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 05:10:58 GMT"}], "update_date": "2011-09-08", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1109.1396", "submitter": "R\\'obert Orm\\'andi", "authors": "R\\'obert Orm\\'andi, Istv\\'an Heged\\\"us, M\\'ark Jelasity", "title": "Gossip Learning with Linear Models on Fully Distributed Data", "comments": "The paper was published in the journal Concurrency and Computation:\n  Practice and Experience\n  http://onlinelibrary.wiley.com/journal/10.1002/%28ISSN%291532-0634 (DOI:\n  http://dx.doi.org/10.1002/cpe.2858). The modifications are based on the\n  suggestions from the reviewers", "journal-ref": null, "doi": "10.1002/cpe.2858", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning over fully distributed data poses an important problem in\npeer-to-peer (P2P) applications. In this model we have one data record at each\nnetwork node, but without the possibility to move raw data due to privacy\nconsiderations. For example, user profiles, ratings, history, or sensor\nreadings can represent this case. This problem is difficult, because there is\nno possibility to learn local models, the system model offers almost no\nguarantees for reliability, yet the communication cost needs to be kept low.\nHere we propose gossip learning, a generic approach that is based on multiple\nmodels taking random walks over the network in parallel, while applying an\nonline learning algorithm to improve themselves, and getting combined via\nensemble learning methods. We present an instantiation of this approach for the\ncase of classification with linear models. Our main contribution is an ensemble\nlearning method which---through the continuous combination of the models in the\nnetwork---implements a virtual weighted voting mechanism over an exponential\nnumber of models at practically no extra cost as compared to independent random\nwalks. We prove the convergence of the method theoretically, and perform\nextensive experiments on benchmark datasets. Our experimental analysis\ndemonstrates the performance and robustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 09:16:37 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2012 09:55:07 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2012 09:26:30 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Orm\u00e1ndi", "R\u00f3bert", ""], ["Heged\u00fcs", "Istv\u00e1n", ""], ["Jelasity", "M\u00e1rk", ""]]}, {"id": "1109.1528", "submitter": "Ardeshir Kianercy", "authors": "Ardeshir Kianercy, Aram Galstyan", "title": "Dynamics of Boltzmann Q-Learning in Two-Player Two-Action Games", "comments": "10 pages, 12 figures. Version 2: added more extensive discussion of\n  asymmetric equilibria; clarified conditions for continuous/discontinuous\n  bifurcations in coordination/anti-coordination games", "journal-ref": "Physical Review E, vol.85, 4, 041145, 2012", "doi": "10.1103/PhysRevE.85.041145", "report-no": null, "categories": "cs.GT cs.LG cs.MA nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dynamics of Q-learning in two-player two-action games with a\nBoltzmann exploration mechanism. For any non-zero exploration rate the dynamics\nis dissipative, which guarantees that agent strategies converge to rest points\nthat are generally different from the game's Nash Equlibria (NE). We provide a\ncomprehensive characterization of the rest point structure for different games,\nand examine the sensitivity of this structure with respect to the noise due to\nexploration. Our results indicate that for a class of games with multiple NE\nthe asymptotic behavior of learning dynamics can undergo drastic changes at\ncritical exploration rates. Furthermore, we demonstrate that for certain games\nwith a single NE, it is possible to have additional rest points (not\ncorresponding to any NE) that persist for a finite range of the exploration\nrates and disappear when the exploration rates of both players tend to zero.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 18:21:39 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2011 02:11:08 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2012 22:51:48 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Kianercy", "Ardeshir", ""], ["Galstyan", "Aram", ""]]}, {"id": "1109.1533", "submitter": "Yi Gai", "authors": "Wenhan Dai, Yi Gai, Bhaskar Krishnamachari, Qing Zhao", "title": "The Non-Bayesian Restless Multi-Armed Bandit: A Case of Near-Logarithmic\n  Strict Regret", "comments": "arXiv admin note: significant text overlap with arXiv:1011.4752", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NI cs.SY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic Bayesian restless multi-armed bandit (RMAB) problem, there are\n$N$ arms, with rewards on all arms evolving at each time as Markov chains with\nknown parameters. A player seeks to activate $K \\geq 1$ arms at each time in\norder to maximize the expected total reward obtained over multiple plays. RMAB\nis a challenging problem that is known to be PSPACE-hard in general. We\nconsider in this work the even harder non-Bayesian RMAB, in which the\nparameters of the Markov chain are assumed to be unknown \\emph{a priori}. We\ndevelop an original approach to this problem that is applicable when the\ncorresponding Bayesian problem has the structure that, depending on the known\nparameter values, the optimal solution is one of a prescribed finite set of\npolicies. In such settings, we propose to learn the optimal policy for the\nnon-Bayesian RMAB by employing a suitable meta-policy which treats each policy\nfrom this finite set as an arm in a different non-Bayesian multi-armed bandit\nproblem for which a single-arm selection policy is optimal. We demonstrate this\napproach by developing a novel sensing policy for opportunistic spectrum access\nover unknown dynamic channels. We prove that our policy achieves\nnear-logarithmic regret (the difference in expected reward compared to a\nmodel-aware genie), which leads to the same average reward that can be achieved\nby the optimal policy under a known model. This is the first such result in the\nliterature for a non-Bayesian RMAB. For our proof, we also develop a novel\ngeneralization of the Chernoff-Hoeffding bound.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 18:33:59 GMT"}], "update_date": "2011-12-25", "authors_parsed": [["Dai", "Wenhan", ""], ["Gai", "Yi", ""], ["Krishnamachari", "Bhaskar", ""], ["Zhao", "Qing", ""]]}, {"id": "1109.1552", "submitter": "Yi Gai", "authors": "Wenhan Dai, Yi Gai, Bhaskar Krishnamachari", "title": "Efficient Online Learning for Opportunistic Spectrum Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.SY math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of opportunistic spectrum access in cognitive radio networks has\nbeen recently formulated as a non-Bayesian restless multi-armed bandit problem.\nIn this problem, there are N arms (corresponding to channels) and one player\n(corresponding to a secondary user). The state of each arm evolves as a\nfinite-state Markov chain with unknown parameters. At each time slot, the\nplayer can select K < N arms to play and receives state-dependent rewards\n(corresponding to the throughput obtained given the activity of primary users).\nThe objective is to maximize the expected total rewards (i.e., total\nthroughput) obtained over multiple plays. The performance of an algorithm for\nsuch a multi-armed bandit problem is measured in terms of regret, defined as\nthe difference in expected reward compared to a model-aware genie who always\nplays the best K arms. In this paper, we propose a new continuous exploration\nand exploitation (CEE) algorithm for this problem. When no information is\navailable about the dynamics of the arms, CEE is the first algorithm to\nguarantee near-logarithmic regret uniformly over time. When some bounds\ncorresponding to the stationary state distributions and the state-dependent\nrewards are known, we show that CEE can be easily modified to achieve\nlogarithmic regret over time. In contrast, prior algorithms require additional\ninformation concerning bounds on the second eigenvalues of the transition\nmatrices in order to guarantee logarithmic regret. Finally, we show through\nnumerical simulations that CEE is more efficient than prior algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 19:54:30 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Dai", "Wenhan", ""], ["Gai", "Yi", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1109.1605", "submitter": "Ali Pinar", "authors": "Matthew Rocklin and Ali Pinar", "title": "On Clustering on Graphs with Multiple Edge Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study clustering on graphs with multiple edge types. Our main motivation\nis that similarities between objects can be measured in many different metrics.\nFor instance similarity between two papers can be based on common authors,\nwhere they are published, keyword similarity, citations, etc. As such, graphs\nwith multiple edges is a more accurate model to describe similarities between\nobjects. Each edge/metric provides only partial information about the data;\nrecovering full information requires aggregation of all the similarity metrics.\nClustering becomes much more challenging in this context, since in addition to\nthe difficulties of the traditional clustering problem, we have to deal with a\nspace of clusterings. We generalize the concept of clustering in single-edge\ngraphs to multi-edged graphs and investigate problems such as: Can we find a\nclustering that remains good, even if we change the relative weights of\nmetrics? How can we describe the space of clusterings efficiently? Can we find\nunexpected clusterings (a good clustering that is distant from all given\nclusterings)? If given the ground-truth clustering, can we recover how the\nweights for edge types were aggregated? %In this paper, we discuss these\nproblems and the underlying algorithmic challenges and propose some solutions.\nWe also present two case studies: one based on papers on Arxiv and one based on\nCIA World Factbook.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 00:00:16 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Rocklin", "Matthew", ""], ["Pinar", "Ali", ""]]}, {"id": "1109.1606", "submitter": "Yi Gai", "authors": "Yi Gai, Bhaskar Krishnamachari, Mingyan Liu", "title": "Online Learning for Combinatorial Network Optimization with Restless\n  Markovian Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial network optimization algorithms that compute optimal structures\ntaking into account edge weights form the foundation for many network\nprotocols. Examples include shortest path routing, minimal spanning tree\ncomputation, maximum weighted matching on bipartite graphs, etc. We present\nCLRMR, the first online learning algorithm that efficiently solves the\nstochastic version of these problems where the underlying edge weights vary as\nindependent Markov chains with unknown dynamics.\n  The performance of an online learning algorithm is characterized in terms of\nregret, defined as the cumulative difference in rewards between a\nsuitably-defined genie, and that obtained by the given algorithm. We prove\nthat, compared to a genie that knows the Markov transition matrices and uses\nthe single-best structure at all times, CLRMR yields regret that is polynomial\nin the number of edges and nearly-logarithmic in time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 00:43:42 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Gai", "Yi", ""], ["Krishnamachari", "Bhaskar", ""], ["Liu", "Mingyan", ""]]}, {"id": "1109.1729", "submitter": "Nan Wang", "authors": "Nan Wang and Jizhong Han and Jinyun Fang", "title": "Anomaly Sequences Detection from Logs Based on Compression", "comments": "7 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining information from logs is an old and still active research topic. In\nrecent years, with the rapid emerging of cloud computing, log mining becomes\nincreasingly important to industry. This paper focus on one major mission of\nlog mining: anomaly detection, and proposes a novel method for mining abnormal\nsequences from large logs. Different from previous anomaly detection systems\nwhich based on statistics, probabilities and Markov assumption, our approach\nmeasures the strangeness of a sequence using compression. It first trains a\ngrammar about normal behaviors using grammar-based compression, then measures\nthe information quantities and densities of questionable sequences according to\nincrementation of grammar length. We have applied our approach on mining some\nreal bugs from fine grained execution logs. We have also tested its ability on\nintrusion detection using some publicity available system call traces. The\nexperiments show that our method successfully selects the strange sequences\nwhich related to bugs or attacking.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 14:34:57 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Wang", "Nan", ""], ["Han", "Jizhong", ""], ["Fang", "Jinyun", ""]]}, {"id": "1109.1844", "submitter": "Margareta Ackerman", "authors": "Margareta Ackerman, Shai Ben-David, Simina Br\\^anzei, and David Loker", "title": "Weighted Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most prominent challenges in clustering is \"the user's dilemma,\"\nwhich is the problem of selecting an appropriate clustering algorithm for a\nspecific task. A formal approach for addressing this problem relies on the\nidentification of succinct, user-friendly properties that formally capture when\ncertain clustering methods are preferred over others.\n  Until now these properties focused on advantages of classical Linkage-Based\nalgorithms, failing to identify when other clustering paradigms, such as\npopular center-based methods, are preferable. We present surprisingly simple\nnew properties that delineate the differences between common clustering\nparadigms, which clearly and formally demonstrates advantages of center-based\napproaches for some applications. These properties address how sensitive\nalgorithms are to changes in element frequencies, which we capture in a\ngeneralized setting where every element is associated with a real-valued\nweight.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 20:53:54 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 08:33:09 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Ackerman", "Margareta", ""], ["Ben-David", "Shai", ""], ["Br\u00e2nzei", "Simina", ""], ["Loker", "David", ""]]}, {"id": "1109.1990", "submitter": "Francis Bach", "authors": "Edouard Grave (LIENS, INRIA Paris - Rocquencourt), Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt), Francis Bach (LIENS, INRIA Paris -\n  Rocquencourt)", "title": "Trace Lasso: a trace norm regularization for correlated designs", "comments": null, "journal-ref": "Neural Information Processing Systems, Spain (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the $\\ell_1$-norm to regularize the estimation of the parameter vector\nof a linear model leads to an unstable estimator when covariates are highly\ncorrelated. In this paper, we introduce a new penalty function which takes into\naccount the correlation of the design matrix to stabilize the estimation. This\nnorm, called the trace Lasso, uses the trace norm, which is a convex surrogate\nof the rank, of the selected covariates as the criterion of model complexity.\nWe analyze the properties of our norm, describe an optimization algorithm based\non reweighted least-squares, and illustrate the behavior of this norm on\nsynthetic data, showing that it is more adapted to strong correlations than\ncompeting methods such as the elastic net.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 13:01:41 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Grave", "Edouard", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris -\n  Rocquencourt"]]}, {"id": "1109.2034", "submitter": "Justin Bayer", "authors": "Justin Bayer and Christian Osendorfer and Patrick van der Smagt", "title": "Learning Sequence Neighbourhood Metrics", "comments": "Artificial Neural Networks and Machine Learning ICANN 2012 Springer\n  Berlin Heidelberg 2012. 531-538", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) in combination with a pooling operator and\nthe neighbourhood components analysis (NCA) objective function are able to\ndetect the characterizing dynamics of sequences and embed them into a\nfixed-length vector space of arbitrary dimensionality. Subsequently, the\nresulting features are meaningful and can be used for visualization or nearest\nneighbour classification in linear time. This kind of metric learning for\nsequential data enables the use of algorithms tailored towards fixed length\nvector spaces such as R^n.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 14:59:59 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 12:21:24 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1109.2047", "submitter": "N. V. Chawla", "authors": "N. V. Chawla, Grigoris Karakoulas", "title": "Learning From Labeled And Unlabeled Data: An Empirical Study Across\n  Techniques And Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  331-366, 2005", "doi": "10.1613/jair.1509", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increased interest in devising learning techniques that\ncombine unlabeled data with labeled data ? i.e. semi-supervised learning.\nHowever, to the best of our knowledge, no study has been performed across\nvarious techniques and different types and amounts of labeled and unlabeled\ndata. Moreover, most of the published work on semi-supervised learning\ntechniques assumes that the labeled and unlabeled data come from the same\ndistribution. It is possible for the labeling process to be associated with a\nselection bias such that the distributions of data points in the labeled and\nunlabeled sets are different. Not correcting for such bias can result in biased\nfunction approximation with potentially poor performance. In this paper, we\npresent an empirical study of various semi-supervised learning techniques on a\nvariety of datasets. We attempt to answer various questions such as the effect\nof independence or relevance amongst features, the effect of the size of the\nlabeled and unlabeled sets and the effect of noise. We also investigate the\nimpact of sample-selection bias on the semi-supervised learning techniques\nunder study and implement a bivariate probit technique particularly designed to\ncorrect for such bias.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 15:56:58 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Chawla", "N. V.", ""], ["Karakoulas", "Grigoris", ""]]}, {"id": "1109.2088", "submitter": "Yi Gai", "authors": "Yi Gai, Bhaskar Krishnamachari", "title": "Online Learning Algorithms for Stochastic Water-Filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.SY math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water-filling is the term for the classic solution to the problem of\nallocating constrained power to a set of parallel channels to maximize the\ntotal data-rate. It is used widely in practice, for example, for power\nallocation to sub-carriers in multi-user OFDM systems such as WiMax. The\nclassic water-filling algorithm is deterministic and requires perfect knowledge\nof the channel gain to noise ratios. In this paper we consider how to do power\nallocation over stochastically time-varying (i.i.d.) channels with unknown gain\nto noise ratio distributions. We adopt an online learning framework based on\nstochastic multi-armed bandits. We consider two variations of the problem, one\nin which the goal is to find a power allocation to maximize $\\sum\\limits_i\n\\mathbb{E}[\\log(1 + SNR_i)]$, and another in which the goal is to find a power\nallocation to maximize $\\sum\\limits_i \\log(1 + \\mathbb{E}[SNR_i])$. For the\nfirst problem, we propose a \\emph{cognitive water-filling} algorithm that we\ncall CWF1. We show that CWF1 obtains a regret (defined as the cumulative gap\nover time between the sum-rate obtained by a distribution-aware genie and this\npolicy) that grows polynomially in the number of channels and logarithmically\nin time, implying that it asymptotically achieves the optimal time-averaged\nrate that can be obtained when the gain distributions are known. For the second\nproblem, we present an algorithm called CWF2, which is, to our knowledge, the\nfirst algorithm in the literature on stochastic multi-armed bandits to exploit\nnon-linear dependencies between the arms. We prove that the number of times\nCWF2 picks the incorrect power allocation is bounded by a function that is\npolynomial in the number of channels and logarithmic in time, implying that its\nfrequency of incorrect allocation tends to zero.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 18:42:42 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Gai", "Yi", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1109.2141", "submitter": "R. Khardon", "authors": "R. Khardon, D. Roth, R. A. Servedio", "title": "Efficiency versus Convergence of Boolean Kernels for On-Line Learning\n  Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  341-356, 2005", "doi": "10.1613/jair.1655", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies machine learning problems where each example is described\nusing a set of Boolean features and where hypotheses are represented by linear\nthreshold elements. One method of increasing the expressiveness of learned\nhypotheses in this context is to expand the feature set to include conjunctions\nof basic features. This can be done explicitly or where possible by using a\nkernel function. Focusing on the well known Perceptron and Winnow algorithms,\nthe paper demonstrates a tradeoff between the computational efficiency with\nwhich the algorithm can be run over the expanded feature space and the\ngeneralization ability of the corresponding learning algorithm. We first\ndescribe several kernel functions which capture either limited forms of\nconjunctions or all conjunctions. We show that these kernels can be used to\nefficiently run the Perceptron algorithm over a feature space of exponentially\nmany conjunctions; however we also show that using such kernels, the Perceptron\nalgorithm can provably make an exponential number of mistakes even when\nlearning simple functions. We then consider the question of whether kernel\nfunctions can analogously be used to run the multiplicative-update Winnow\nalgorithm over an expanded feature space of exponentially many conjunctions.\nKnown upper bounds imply that the Winnow algorithm can learn Disjunctive Normal\nForm (DNF) formulae with a polynomial mistake bound in this setting. However,\nwe prove that it is computationally hard to simulate Winnows behavior for\nlearning DNF over such a feature set. This implies that the kernel functions\nwhich correspond to running Winnow for this problem are not efficiently\ncomputable, and that there is no general construction that can run Winnow with\nkernels.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:31:05 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Khardon", "R.", ""], ["Roth", "D.", ""], ["Servedio", "R. A.", ""]]}, {"id": "1109.2147", "submitter": "P. Geibel", "authors": "P. Geibel, F. Wysotzki", "title": "Risk-Sensitive Reinforcement Learning Applied to Control under\n  Constraints", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  81-108, 2005", "doi": "10.1613/jair.1666", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Markov Decision Processes (MDPs) with error\nstates. Error states are those states entering which is undesirable or\ndangerous. We define the risk with respect to a policy as the probability of\nentering such a state when the policy is pursued. We consider the problem of\nfinding good policies whose risk is smaller than some user-specified threshold,\nand formalize it as a constrained MDP with two criteria. The first criterion\ncorresponds to the value function originally given. We will show that the risk\ncan be formulated as a second criterion function based on a cumulative return,\nwhose definition is independent of the original value function. We present a\nmodel free, heuristic reinforcement learning algorithm that aims at finding\ngood deterministic policies. It is based on weighting the original value\nfunction and the risk. The weight parameter is adapted in order to find a\nfeasible solution for the constrained problem that has a good performance with\nrespect to the value function. The algorithm was successfully applied to the\ncontrol of a feed tank with stochastic inflows that lies upstream of a\ndistillation column. This control task was originally formulated as an optimal\ncontrol problem with chance constraints, and it was solved under certain\nassumptions on the model to obtain an optimal solution. The power of our\nlearning algorithm is that it can be used even when some of these restrictive\nassumptions are relaxed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:32:41 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Geibel", "P.", ""], ["Wysotzki", "F.", ""]]}, {"id": "1109.2229", "submitter": "Aaron Roth", "authors": "Avrim Blum, Katrina Ligett, Aaron Roth", "title": "A Learning Theory Approach to Non-Interactive Database Privacy", "comments": "Full Version. Extended Abstract appeared in STOC 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate that, ignoring computational constraints, it is\npossible to privately release synthetic databases that are useful for large\nclasses of queries -- much larger in size than the database itself.\nSpecifically, we give a mechanism that privately releases synthetic data for a\nclass of queries over a discrete domain with error that grows as a function of\nthe size of the smallest net approximately representing the answers to that\nclass of queries. We show that this in particular implies a mechanism for\ncounting queries that gives error guarantees that grow only with the\nVC-dimension of the class of queries, which itself grows only logarithmically\nwith the size of the query class.\n  We also show that it is not possible to privately release even simple classes\nof queries (such as intervals and their generalizations) over continuous\ndomains. Despite this, we give a privacy-preserving polynomial time algorithm\nthat releases information useful for all halfspace queries, given a slight\nrelaxation of the utility guarantee. This algorithm does not release synthetic\ndata, but instead another data structure capable of representing an answer for\neach query. We also give an efficient algorithm for releasing synthetic data\nfor the class of interval queries and axis-aligned rectangles of constant\ndimension.\n  Finally, inspired by learning theory, we introduce a new notion of data\nprivacy, which we call distributional privacy, and show that it is strictly\nstronger than the prevailing privacy notion, differential privacy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2011 15:23:14 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Blum", "Avrim", ""], ["Ligett", "Katrina", ""], ["Roth", "Aaron", ""]]}, {"id": "1109.2296", "submitter": "Dotan Di Castro", "authors": "Dotan Di Castro, Claudio Gentile, Shie Mannor", "title": "Bandits with an Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a bandit problem over a graph where the rewards are not directly\nobserved. Instead, the decision maker can compare two nodes and receive\n(stochastic) information pertaining to the difference in their value. The graph\nstructure describes the set of possible comparisons. Consequently, comparing\nbetween two nodes that are relatively far requires estimating the difference\nbetween every pair of nodes on the path between them. We analyze this problem\nfrom the perspective of sample complexity: How many queries are needed to find\nan approximately optimal node with probability more than $1-\\delta$ in the PAC\nsetup? We show that the topology of the graph plays a crucial in defining the\nsample complexity: graphs with a low diameter have a much better sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 09:00:53 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Di Castro", "Dotan", ""], ["Gentile", "Claudio", ""], ["Mannor", "Shie", ""]]}, {"id": "1109.2388", "submitter": "Emre Akbas", "authors": "Emre Akbas, Bernard Ghanem, Narendra Ahuja", "title": "MIS-Boost: Multiple Instance Selection Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new multiple instance learning (MIL) method,\ncalled MIS-Boost, which learns discriminative instance prototypes by explicit\ninstance selection in a boosting framework. Unlike previous instance selection\nbased MIL methods, we do not restrict the prototypes to a discrete set of\ntraining instances but allow them to take arbitrary values in the instance\nfeature space. We also do not restrict the total number of prototypes and the\nnumber of selected-instances per bag; these quantities are completely\ndata-driven. We show that MIS-Boost outperforms state-of-the-art MIL methods on\na number of benchmark datasets. We also apply MIS-Boost to large-scale image\nclassification, where we show that the automatically selected prototypes map to\nvisually meaningful image regions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 07:31:34 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Akbas", "Emre", ""], ["Ghanem", "Bernard", ""], ["Ahuja", "Narendra", ""]]}, {"id": "1109.2389", "submitter": "Bernard Ghanem", "authors": "Bernard Ghanem and Narendra Ahuja", "title": "A Probabilistic Framework for Discriminative Dictionary Learning", "comments": "10 pages, 4 figures, conference, dictionary learning, sparse coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of discriminative dictionary learning\n(DDL), where sparse linear representation and classification are combined in a\nprobabilistic framework. As such, a single discriminative dictionary and linear\nbinary classifiers are learned jointly. By encoding sparse representation and\ndiscriminative classification models in a MAP setting, we propose a general\noptimization framework that allows for a data-driven tradeoff between faithful\nrepresentation and accurate classification. As opposed to previous work, our\nlearning methodology is capable of incorporating a diverse family of\nclassification cost functions (including those used in popular boosting\nmethods), while avoiding the need for involved optimization techniques. We show\nthat DDL can be solved by a sequence of updates that make use of well-known and\nwell-studied sparse coding and dictionary learning algorithms from the\nliterature. To validate our DDL framework, we apply it to digit classification\nand face recognition and test it on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 07:45:03 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Ghanem", "Bernard", ""], ["Ahuja", "Narendra", ""]]}, {"id": "1109.2397", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt), Rodolphe Jenatton\n  (LIENS, INRIA Paris - Rocquencourt), Julien Mairal, Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Structured sparsity through convex optimization", "comments": "Statistical Science (2012) To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods are aimed at using or obtaining parsimonious\nrepresentations of data or models. While naturally cast as a combinatorial\noptimization problem, variable or feature selection admits a convex relaxation\nthrough the regularization by the $\\ell_1$-norm. In this paper, we consider\nsituations where we are not only interested in sparsity, but where some\nstructural prior knowledge is available as well. We show that the $\\ell_1$-norm\ncan then be extended to structured norms built on either disjoint or\noverlapping groups of variables, leading to a flexible framework that can deal\nwith various structures. We present applications to unsupervised learning, for\nstructured sparse principal component analysis and hierarchical dictionary\nlearning, and to supervised learning in the context of non-linear variable\nselection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 08:23:02 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2012 13:20:27 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1109.2415", "submitter": "Nicolas Le Roux", "authors": "Mark Schmidt (INRIA Paris - Rocquencourt, LIENS), Nicolas Le Roux\n  (INRIA Paris - Rocquencourt, LIENS), Francis Bach (INRIA Paris -\n  Rocquencourt, LIENS)", "title": "Convergence Rates of Inexact Proximal-Gradient Methods for Convex\n  Optimization", "comments": "Neural Information Processing Systems (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimizing the sum of a smooth convex function and\na non-smooth convex function using proximal-gradient methods, where an error is\npresent in the calculation of the gradient of the smooth term or in the\nproximity operator with respect to the non-smooth term. We show that both the\nbasic proximal-gradient method and the accelerated proximal-gradient method\nachieve the same convergence rate as in the error-free case, provided that the\nerrors decrease at appropriate rates.Using these rates, we perform as well as\nor better than a carefully chosen fixed error level on a set of structured\nsparsity problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 09:45:02 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 16:06:06 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Schmidt", "Mark", "", "INRIA Paris - Rocquencourt, LIENS"], ["Roux", "Nicolas Le", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris -\n  Rocquencourt, LIENS"]]}, {"id": "1109.3240", "submitter": "Yaojia Zhu", "authors": "Cristopher Moore, Xiaoran Yan, Yaojia Zhu, Jean-Baptiste Rouquier,\n  Terran Lane", "title": "Active Learning for Node Classification in Assortative and\n  Disassortative Networks", "comments": "9 pages, 7 figures, KDD 2011: The 17th ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world networks, nodes have class labels, attributes, or\nvariables that affect the network's topology. If the topology of the network is\nknown but the labels of the nodes are hidden, we would like to select a small\nsubset of nodes such that, if we knew their labels, we could accurately predict\nthe labels of all the other nodes. We develop an active learning algorithm for\nthis problem which uses information-theoretic techniques to choose which nodes\nto explore. We test our algorithm on networks from three different domains: a\nsocial network, a network of English words that appear adjacently in a novel,\nand a marine food web. Our algorithm makes no initial assumptions about how the\ngroups connect, and performs well even when faced with quite general types of\nnetwork structure. In particular, we do not assume that nodes of the same class\nare more likely to be connected to each other---only that they connect to the\nrest of the network in similar ways.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 02:10:26 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Moore", "Cristopher", ""], ["Yan", "Xiaoran", ""], ["Zhu", "Yaojia", ""], ["Rouquier", "Jean-Baptiste", ""], ["Lane", "Terran", ""]]}, {"id": "1109.3248", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Reconstruction of sequential data with density models", "comments": "30 pages, 9 figures. Original manuscript dated January 27, 2004 and\n  not updated since. Current author's email address:\n  mcarreira-perpinan@ucmerced.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of reconstructing a sequence of multidimensional\nreal vectors where some of the data are missing. This problem contains\nregression and mapping inversion as particular cases where the pattern of\nmissing data is independent of the sequence index. The problem is hard because\nit involves possibly multivalued mappings at each vector in the sequence, where\nthe missing variables can take more than one value given the present variables;\nand the set of missing variables can vary from one vector to the next. To solve\nthis problem, we propose an algorithm based on two redundancy assumptions:\nvector redundancy (the data live in a low-dimensional manifold), so that the\npresent variables constrain the missing ones; and sequence redundancy (e.g.\ncontinuity), so that consecutive vectors constrain each other. We capture the\nlow-dimensional nature of the data in a probabilistic way with a joint density\nmodel, here the generative topographic mapping, which results in a Gaussian\nmixture. Candidate reconstructions at each vector are obtained as all the modes\nof the conditional distribution of missing variables given present variables.\nThe reconstructed sequence is obtained by minimising a global constraint, here\nthe sequence length, by dynamic programming. We present experimental results\nfor a toy problem and for inverse kinematics of a robot arm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 03:12:36 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1109.3318", "submitter": "Dan-Cristian Tomozei", "authors": "Dan-Cristian Tomozei, Laurent Massouli\\'e", "title": "Distributed User Profiling via Spectral Methods", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User profiling is a useful primitive for constructing personalised services,\nsuch as content recommendation. In the present paper we investigate the\nfeasibility of user profiling in a distributed setting, with no central\nauthority and only local information exchanges between users. We compute a\nprofile vector for each user (i.e., a low-dimensional vector that characterises\nher taste) via spectral transformation of observed user-produced ratings for\nitems. Our two main contributions follow: i) We consider a low-rank\nprobabilistic model of user taste. More specifically, we consider that users\nand items are partitioned in a constant number of classes, such that users and\nitems within the same class are statistically identical. We prove that without\nprior knowledge of the compositions of the classes, based solely on few random\nobserved ratings (namely $O(N\\log N)$ such ratings for $N$ users), we can\npredict user preference with high probability for unrated items by running a\nlocal vote among users with similar profile vectors. In addition, we provide\nempirical evaluations characterising the way in which spectral profiling\nperformance depends on the dimension of the profile space. Such evaluations are\nperformed on a data set of real user ratings provided by Netflix. ii) We\ndevelop distributed algorithms which provably achieve an embedding of users\ninto a low-dimensional space, based on spectral transformation. These involve\nsimple message passing among users, and provably converge to the desired\nembedding. Our method essentially relies on a novel combination of gossiping\nand the algorithm proposed by Oja and Karhunen.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 11:31:31 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 09:11:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tomozei", "Dan-Cristian", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1109.3437", "submitter": "Jia Zeng", "authors": "Jia Zeng and William K. Cheung and Jiming Liu", "title": "Learning Topic Models by Belief Propagation", "comments": "14 pages, 17 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Volume 33, Number 5, Pages 1121-1134, 2013", "doi": "10.1109/TPAMI.2012.185", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model\nfor probabilistic topic modeling, which attracts worldwide interests and\ntouches on many important applications in text mining, computer vision and\ncomputational biology. This paper represents LDA as a factor graph within the\nMarkov random field (MRF) framework, which enables the classic loopy belief\npropagation (BP) algorithm for approximate inference and parameter estimation.\nAlthough two commonly-used approximate inference methods, such as variational\nBayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in\nlearning LDA, the proposed BP is competitive in both speed and accuracy as\nvalidated by encouraging experimental results on four large-scale document data\nsets. Furthermore, the BP algorithm has the potential to become a generic\nlearning scheme for variants of LDA-based topic models. To this end, we show\nhow to learn two typical variants of LDA-based topic models, such as\nauthor-topic models (ATM) and relational topic models (RTM), using BP based on\nthe factor graph representation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 19:20:48 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 21:17:41 GMT"}, {"version": "v3", "created": "Mon, 3 Oct 2011 03:17:44 GMT"}, {"version": "v4", "created": "Sat, 24 Mar 2012 12:47:02 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zeng", "Jia", ""], ["Cheung", "William K.", ""], ["Liu", "Jiming", ""]]}, {"id": "1109.3701", "submitter": "Kevin Jamieson", "authors": "Kevin G. Jamieson and Robert D. Nowak", "title": "Active Ranking using Pairwise Comparisons", "comments": "17 pages, an extended version of our NIPS 2011 paper. The new version\n  revises the argument of the robust section and slightly modifies the result\n  there to give it more impact", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of ranking a collection of objects using\npairwise comparisons (rankings of two objects). In general, the ranking of $n$\nobjects can be identified by standard sorting methods using $n log_2 n$\npairwise comparisons. We are interested in natural situations in which\nrelationships among the objects may allow for ranking using far fewer pairwise\ncomparisons. Specifically, we assume that the objects can be embedded into a\n$d$-dimensional Euclidean space and that the rankings reflect their relative\ndistances from a common reference point in $R^d$. We show that under this\nassumption the number of possible rankings grows like $n^{2d}$ and demonstrate\nan algorithm that can identify a randomly selected ranking using just slightly\nmore than $d log n$ adaptively selected pairwise comparisons, on average. If\ninstead the comparisons are chosen at random, then almost all pairwise\ncomparisons must be made in order to identify any ranking. In addition, we\npropose a robust, error-tolerant algorithm that only requires that the pairwise\ncomparisons are probably correct. Experimental studies with synthetic and real\ndatasets support the conclusions of our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 19:35:13 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2011 01:02:14 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Jamieson", "Kevin G.", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1109.3843", "submitter": "Michael Mahoney", "authors": "Petros Drineas and Malik Magdon-Ismail and Michael W. Mahoney and\n  David P. Woodruff", "title": "Fast approximation of matrix coherence and statistical leverage", "comments": "29 pages; conference version is in ICML; journal version is in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical leverage scores of a matrix $A$ are the squared row-norms of\nthe matrix containing its (top) left singular vectors and the coherence is the\nlargest leverage score. These quantities are of interest in recently-popular\nproblems such as matrix completion and Nystr\\\"{o}m-based low-rank matrix\napproximation as well as in large-scale statistical data analysis applications\nmore generally; moreover, they are of interest since they define the key\nstructural nonuniformity that must be dealt with in developing fast randomized\nmatrix algorithms. Our main result is a randomized algorithm that takes as\ninput an arbitrary $n \\times d$ matrix $A$, with $n \\gg d$, and that returns as\noutput relative-error approximations to all $n$ of the statistical leverage\nscores. The proposed algorithm runs (under assumptions on the precise values of\n$n$ and $d$) in $O(n d \\log n)$ time, as opposed to the $O(nd^2)$ time required\nby the na\\\"{i}ve algorithm that involves computing an orthogonal basis for the\nrange of $A$. Our analysis may be viewed in terms of computing a relative-error\napproximation to an underconstrained least-squares approximation problem, or,\nrelatedly, it may be viewed as an application of Johnson-Lindenstrauss type\nideas. Several practically-important extensions of our basic result are also\ndescribed, including the approximation of so-called cross-leverage scores, the\nextension of these ideas to matrices with $n \\approx d$, and the extension to\nstreaming environments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2011 04:38:12 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2012 00:13:53 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""], ["Mahoney", "Michael W.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1109.3940", "submitter": "Yuan Shi", "authors": "Yuan Shi, Yung-Kyun Noh, Fei Sha, Daniel D. Lee", "title": "Learning Discriminative Metrics via Generative Models and Kernel\n  Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics specifying distances between data points can be learned in a\ndiscriminative manner or from generative models. In this paper, we show how to\nunify generative and discriminative learning of metrics via a kernel learning\nframework. Specifically, we learn local metrics optimized from parametric\ngenerative models. These are then used as base kernels to construct a global\nkernel that minimizes a discriminative training criterion. We consider both\nlinear and nonlinear combinations of local metric kernels. Our empirical\nresults show that these combinations significantly improve performance on\nclassification tasks. The proposed learning algorithm is also very efficient,\nachieving order of magnitude speedup in training time compared to previous\ndiscriminative baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 04:19:30 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Shi", "Yuan", ""], ["Noh", "Yung-Kyun", ""], ["Sha", "Fei", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1109.4347", "submitter": "Yohji Akama", "authors": "Yohji Akama and Kei Irie", "title": "VC dimension of ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will establish that the VC dimension of the class of d-dimensional\nellipsoids is (d^2+3d)/2, and that maximum likelihood estimate with N-component\nd-dimensional Gaussian mixture models induces a geometric class having VC\ndimension at least N(d^2+3d)/2.\n  Keywords: VC dimension; finite dimensional ellipsoid; Gaussian mixture model\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 16:53:29 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Akama", "Yohji", ""], ["Irie", "Kei", ""]]}, {"id": "1109.4540", "submitter": "Christopher R. Genovese", "authors": "Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli,\n  Larry Wasserman", "title": "Manifold estimation and singular deconvolution under Hausdorff loss", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS994 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 941-963", "doi": "10.1214/12-AOS994", "report-no": "IMS-AOS-AOS994", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find lower and upper bounds for the risk of estimating a manifold in\nHausdorff distance under several models. We also show that there are close\nconnections between manifold estimation and the problem of deconvolving a\nsingular measure.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 14:29:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2012 13:37:56 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Genovese", "Christopher R.", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1109.4609", "submitter": "Farnood Merrikh-Bayat", "authors": "Farnood Merrikh-Bayat and Saeed Bagheri Shouraki", "title": "Memristive fuzzy edge detector", "comments": "21 pages, 6 figures, submitted to IET Image Processing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy inference systems always suffer from the lack of efficient structures\nor platforms for their hardware implementation. In this paper, we tried to\novercome this problem by proposing new method for the implementation of those\nfuzzy inference systems which use fuzzy rule base to make inference. To achieve\nthis goal, we have designed a multi-layer neuro-fuzzy computing system based on\nthe memristor crossbar structure by introducing some new concepts like fuzzy\nminterms. Although many applications can be realized through the use of our\nproposed system, in this study we show how the fuzzy XOR function can be\nconstructed and how it can be used to extract edges from grayscale images. Our\nmemristive fuzzy edge detector (implemented in analog form) compared with other\ncommon edge detectors has this advantage that it can extract edges of any given\nimage all at once in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 18:45:03 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Merrikh-Bayat", "Farnood", ""], ["Shouraki", "Saeed Bagheri", ""]]}, {"id": "1109.4668", "submitter": "Sebastian Roch", "authors": "Elchanan Mossel, Sebastien Roch, Allan Sly", "title": "Robust estimation of latent tree graphical models: Inferring hidden\n  states with inexact parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent tree graphical models are widely used in computational biology, signal\nand image processing, and network tomography. Here we design a new efficient,\nestimation procedure for latent tree models, including Gaussian and discrete,\nreversible models, that significantly improves on previous sample requirement\nbounds. Our techniques are based on a new hidden state estimator which is\nrobust to inaccuracies in estimated parameters. More precisely, we prove that\nlatent tree models can be estimated with high probability in the so-called\nKesten-Stigum regime with $O(log^2 n)$ samples where $n$ is the number of\nnodes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 22:34:12 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Mossel", "Elchanan", ""], ["Roch", "Sebastien", ""], ["Sly", "Allan", ""]]}, {"id": "1109.4684", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Horace H.S. Ip, Yuxin Peng", "title": "Exhaustive and Efficient Constraint Propagation: A Semi-Supervised\n  Learning Perspective and Its Applications", "comments": "The short version of this paper appears as oral paper in ECCV 2010", "journal-ref": "International Journal of Computer Vision (IJCV), 2012", "doi": "10.1007/s11263-012-0602-z", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel pairwise constraint propagation approach by\ndecomposing the challenging constraint propagation problem into a set of\nindependent semi-supervised learning subproblems which can be solved in\nquadratic time using label propagation based on k-nearest neighbor graphs.\nConsidering that this time cost is proportional to the number of all possible\npairwise constraints, our approach actually provides an efficient solution for\nexhaustively propagating pairwise constraints throughout the entire dataset.\nThe resulting exhaustive set of propagated pairwise constraints are further\nused to adjust the similarity matrix for constrained spectral clustering. Other\nthan the traditional constraint propagation on single-source data, our approach\nis also extended to more challenging constraint propagation on multi-source\ndata where each pairwise constraint is defined over a pair of data points from\ndifferent sources. This multi-source constraint propagation has an important\napplication to cross-modal multimedia retrieval. Extensive results have shown\nthe superior performance of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 00:56:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lu", "Zhiwu", ""], ["Ip", "Horace H. S.", ""], ["Peng", "Yuxin", ""]]}, {"id": "1109.4979", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Yuxin Peng", "title": "Latent Semantic Learning with Structured Sparse Representation for Human\n  Action Recognition", "comments": "The short version of this paper appears in ICCV 2011", "journal-ref": null, "doi": "10.1016/j.patcog.2012.09.027", "report-no": null, "categories": "cs.MM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel latent semantic learning method for extracting\nhigh-level features (i.e. latent semantics) from a large vocabulary of abundant\nmid-level features (i.e. visual keywords) with structured sparse\nrepresentation, which can help to bridge the semantic gap in the challenging\ntask of human action recognition. To discover the manifold structure of\nmidlevel features, we develop a spectral embedding approach to latent semantic\nlearning based on L1-graph, without the need to tune any parameter for graph\nconstruction as a key step of manifold learning. More importantly, we construct\nthe L1-graph with structured sparse representation, which can be obtained by\nstructured sparse coding with its structured sparsity ensured by novel L1-norm\nhypergraph regularization over mid-level features. In the new embedding space,\nwe learn latent semantics automatically from abundant mid-level features\nthrough spectral clustering. The learnt latent semantics can be readily used\nfor human action recognition with SVM by defining a histogram intersection\nkernel. Different from the traditional latent semantic analysis based on topic\nmodels, our latent semantic learning method can explore the manifold structure\nof mid-level features in both L1-graph construction and spectral embedding,\nwhich results in compact but discriminative high-level features. The\nexperimental results on the commonly used KTH action dataset and unconstrained\nYouTube action dataset show the superior performance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 00:39:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lu", "Zhiwu", ""], ["Peng", "Yuxin", ""]]}, {"id": "1109.5078", "submitter": "Jose Hernandez-Orallo", "authors": "Jorge-Alonso Bedoya-Puerta and Jose Hernandez-Orallo", "title": "Application of distances between terms for flat and hierarchical data", "comments": "in Spanish, Master Thesis, 101 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, distance-based algorithms, and other approaches, use\ninformation that is represented by propositional data. However, this kind of\nrepresentation can be quite restrictive and, in many cases, it requires more\ncomplex structures in order to represent data in a more natural way. Terms are\nthe basis for functional and logic programming representation. Distances\nbetween terms are a useful tool not only to compare terms, but also to\ndetermine the search space in many of these applications. This dissertation\napplies distances between terms, exploiting the features of each distance and\nthe possibility to compare from propositional data types to hierarchical\nrepresentations. The distances between terms are applied through the k-NN\n(k-nearest neighbor) classification algorithm using XML as a common language\nrepresentation. To be able to represent these data in an XML structure and to\ntake advantage of the benefits of distance between terms, it is necessary to\napply some transformations. These transformations allow the conversion of flat\ndata into hierarchical data represented in XML, using some techniques based on\nintuitive associations between the names and values of variables and\nassociations based on attribute similarity.\n  Several experiments with the distances between terms of Nienhuys-Cheng and\nEstruch et al. were performed. In the case of originally propositional data,\nthese distances are compared to the Euclidean distance. In all cases, the\nexperiments were performed with the distance-weighted k-nearest neighbor\nalgorithm, using several exponents for the attraction function (weighted\ndistance). It can be seen that in some cases, the term distances can\nsignificantly improve the results on approaches applied to flat\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 13:51:31 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Bedoya-Puerta", "Jorge-Alonso", ""], ["Hernandez-Orallo", "Jose", ""]]}, {"id": "1109.5231", "submitter": "Naresh Manwani", "authors": "Naresh Manwani, P. S. Sastry", "title": "Noise Tolerance under Risk Minimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSMCB.2012.2223460", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore noise tolerant learning of classifiers. We formulate\nthe problem as follows. We assume that there is an ${\\bf unobservable}$\ntraining set which is noise-free. The actual training set given to the learning\nalgorithm is obtained from this ideal data set by corrupting the class label of\neach example. The probability that the class label of an example is corrupted\nis a function of the feature vector of the example. This would account for most\nkinds of noisy data one encounters in practice. We say that a learning method\nis noise tolerant if the classifiers learnt with the ideal noise-free data and\nwith noisy data, both have the same classification accuracy on the noise-free\ndata. In this paper we analyze the noise tolerance properties of risk\nminimization (under different loss functions), which is a generic method for\nlearning classifiers. We show that risk minimization under 0-1 loss function\nhas impressive noise tolerance properties and that under squared error loss is\ntolerant only to uniform noise; risk minimization under other loss functions is\nnot noise tolerant. We conclude the paper with some discussion on implications\nof these theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2011 04:50:55 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2012 16:17:35 GMT"}, {"version": "v3", "created": "Mon, 21 May 2012 12:56:04 GMT"}, {"version": "v4", "created": "Sat, 13 Oct 2012 11:14:22 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Manwani", "Naresh", ""], ["Sastry", "P. S.", ""]]}, {"id": "1109.5302", "submitter": "Wei Dai", "authors": "Wei Dai, Tao Xu, Wenwu Wang", "title": "Simultaneous Codeword Optimization (SimCO) for Dictionary Update and\n  Learning", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2012.2215026", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the data-driven dictionary learning problem. The goal is to seek\nan over-complete dictionary from which every training signal can be best\napproximated by a linear combination of only a few codewords. This task is\noften achieved by iteratively executing two operations: sparse coding and\ndictionary update. In the literature, there are two benchmark mechanisms to\nupdate a dictionary. The first approach, such as the MOD algorithm, is\ncharacterized by searching for the optimal codewords while fixing the sparse\ncoefficients. In the second approach, represented by the K-SVD method, one\ncodeword and the related sparse coefficients are simultaneously updated while\nall other codewords and coefficients remain unchanged. We propose a novel\nframework that generalizes the aforementioned two methods. The unique feature\nof our approach is that one can update an arbitrary set of codewords and the\ncorresponding sparse coefficients simultaneously: when sparse coefficients are\nfixed, the underlying optimization problem is similar to that in the MOD\nalgorithm; when only one codeword is selected for update, it can be proved that\nthe proposed algorithm is equivalent to the K-SVD method; and more importantly,\nour method allows us to update all codewords and all sparse coefficients\nsimultaneously, hence the term simultaneous codeword optimization (SimCO).\nUnder the proposed framework, we design two algorithms, namely, primitive and\nregularized SimCO. We implement these two algorithms based on a simple gradient\ndescent mechanism. Simulations are provided to demonstrate the performance of\nthe proposed algorithms, as compared with two baseline algorithms MOD and\nK-SVD. Results show that regularized SimCO is particularly appealing in terms\nof both learning performance and running speed.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2011 20:32:42 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 16:37:39 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2012 21:58:48 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Dai", "Wei", ""], ["Xu", "Tao", ""], ["Wang", "Wenwu", ""]]}, {"id": "1109.5311", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Bias Plus Variance Decomposition for Survival Analysis Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias - variance decomposition of the expected error defined for regression\nand classification problems is an important tool to study and compare different\nalgorithms, to find the best areas for their application. Here the\ndecomposition is introduced for the survival analysis problem. In our\nexperiments, we study bias -variance parts of the expected error for two\nalgorithms: original Cox proportional hazard regression and CoxPath, path\nalgorithm for L1-regularized Cox regression, on the series of increased\ntraining sets. The experiments demonstrate that, contrary expectations, CoxPath\ndoes not necessarily have an advantage over Cox regression.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2011 22:14:46 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1109.5370", "submitter": "Jia Zeng", "authors": "Jia Zeng, Wei Feng, William K. Cheung, Chun-Hung Li", "title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the topic modeling problem of tagged documents and images.\nHigher-order relations among tagged documents and images are major and\nubiquitous characteristics, and play positive roles in extracting reliable and\ninterpretable topics. In this paper, we propose the tag-topic models (TTM) to\ndepict such higher-order topic structural dependencies within the Markov random\nfield (MRF) framework. First, we use the novel factor graph representation of\nlatent Dirichlet allocation (LDA)-based topic models from the MRF perspective,\nand present an efficient loopy belief propagation (BP) algorithm for\napproximate inference and parameter estimation. Second, we propose the factor\nhypergraph representation of TTM, and focus on both pairwise and higher-order\nrelation modeling among tagged documents and images. Efficient loopy BP\nalgorithm is developed to learn TTM, which encourages the topic labeling\nsmoothness among tagged documents and images. Extensive experimental results\nconfirm the incorporation of higher-order relations to be effective in\nenhancing the overall topic modeling performance, when compared with current\nstate-of-the-art topic models, in many text and image mining tasks of broad\ninterests such as word and link prediction, document classification, and tag\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 16:58:06 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Zeng", "Jia", ""], ["Feng", "Wei", ""], ["Cheung", "William K.", ""], ["Li", "Chun-Hung", ""]]}, {"id": "1109.5647", "submitter": "Ohad Shamir", "authors": "Alexander Rakhlin, Ohad Shamir, Karthik Sridharan", "title": "Making Gradient Descent Optimal for Strongly Convex Stochastic\n  Optimization", "comments": "Updated version which fixes a bug in the proof of lemma 1 and\n  modifies the step size choice. As a result, constants are changed throughout\n  the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a simple and popular method to solve\nstochastic optimization problems which arise in machine learning. For strongly\nconvex problems, its convergence rate was known to be O(\\log(T)/T), by running\nSGD for T iterations and returning the average point. However, recent results\nshowed that using a different algorithm, one can get an optimal O(1/T) rate.\nThis might lead one to believe that standard SGD is suboptimal, and maybe\nshould even be replaced as a method of choice. In this paper, we investigate\nthe optimality of SGD in a stochastic setting. We show that for smooth\nproblems, the algorithm attains the optimal O(1/T) rate. However, for\nnon-smooth problems, the convergence rate with averaging might really be\n\\Omega(\\log(T)/T), and this is not just an artifact of the analysis. On the\nflip side, we show that a simple modification of the averaging step suffices to\nrecover the O(1/T) rate, and no other change of the algorithm is necessary. We\nalso present experimental results which support our findings, and point out\nopen problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 17:24:52 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 18:43:20 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 23:14:46 GMT"}, {"version": "v4", "created": "Mon, 14 May 2012 19:06:53 GMT"}, {"version": "v5", "created": "Tue, 3 Jul 2012 22:06:45 GMT"}, {"version": "v6", "created": "Thu, 5 Jul 2012 14:08:50 GMT"}, {"version": "v7", "created": "Sun, 9 Dec 2012 21:19:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1109.5664", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis, Malik Magdon-Ismail", "title": "Deterministic Feature Selection for $k$-means Clustering", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2255021", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study feature selection for $k$-means clustering. Although the literature\ncontains many methods with good empirical performance, algorithms with provable\ntheoretical behavior have only recently been developed. Unfortunately, these\nalgorithms are randomized and fail with, say, a constant probability. We\naddress this issue by presenting a deterministic feature selection algorithm\nfor k-means with theoretical guarantees. At the heart of our algorithm lies a\ndeterministic method for decompositions of the identity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 18:44:00 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2012 19:03:07 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2013 02:30:59 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2013 20:52:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Boutsidis", "Christos", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1109.5894", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Yee Whye Teh", "title": "Learning Item Trees for Probabilistic Modelling of Implicit Feedback", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preferences for items can be inferred from either explicit feedback,\nsuch as item ratings, or implicit feedback, such as rental histories. Research\nin collaborative filtering has concentrated on explicit feedback, resulting in\nthe development of accurate and scalable models. However, since explicit\nfeedback is often difficult to collect it is important to develop effective\nmodels that take advantage of the more widely available implicit feedback. We\nintroduce a probabilistic approach to collaborative filtering with implicit\nfeedback based on modelling the user's item selection process. In the interests\nof scalability, we restrict our attention to tree-structured distributions over\nitems and develop a principled and efficient algorithm for learning item trees\nfrom data. We also identify a problem with a widely used protocol for\nevaluating implicit feedback models and propose a way of addressing it using a\nsmall quantity of explicit feedback data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 13:58:39 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mnih", "Andriy", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1109.6341", "submitter": "H. Daume III", "authors": "H. Daume III, D. Marcu", "title": "Domain Adaptation for Statistical Classifiers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  101-126, 2006", "doi": "10.1613/jair.1872", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most basic assumption used in statistical learning theory is that\ntraining data and test data are drawn from the same underlying distribution.\nUnfortunately, in many applications, the \"in-domain\" test data is drawn from a\ndistribution that is related, but not identical, to the \"out-of-domain\"\ndistribution of the training data. We consider the common case in which labeled\nout-of-domain data is plentiful, but labeled in-domain data is scarce. We\nintroduce a statistical formulation of this problem in terms of a simple\nmixture model and present an instantiation of this framework to maximum entropy\nclassifiers and their linear chain counterparts. We present efficient inference\nalgorithms for this special case based on the technique of conditional\nexpectation maximization. Our experimental results show that our approach leads\nto improved performance on three real world tasks on four different data sets\nfrom the natural language processing domain.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:18:30 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Daume", "H.", "III"], ["Marcu", "D.", ""]]}]