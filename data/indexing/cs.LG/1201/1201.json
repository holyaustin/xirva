[{"id": "1201.0292", "submitter": "Vincent Graziano", "authors": "Vincent Graziano, Faustino Gomez, Mark Ring, Juergen Schmidhuber", "title": "T-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Reinforcement Learning (RL) has focused on problems involving\nmany states and few actions, such as simple grid worlds. Most real world\nproblems, however, are of the opposite type, Involving Few relevant states and\nmany actions. For example, to return home from a conference, humans identify\nonly few subgoal states such as lobby, taxi, airport etc. Each valid behavior\nconnecting two such states can be viewed as an action, and there are trillions\nof them. Assuming the subgoal identification problem is already solved, the\nquality of any RL method---in real-world settings---depends less on how well it\nscales with the number of states than on how well it scales with the number of\nactions. This is where our new method T-Learning excels, by evaluating the\nrelatively few possible transits from one state to another in a\npolicy-independent way, rather than a huge number of state-action pairs, or\nstates in traditional policy-dependent ways. Illustrative experiments\ndemonstrate that performance improvements of T-Learning over Q-learning can be\narbitrarily large.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 17:29:08 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Graziano", "Vincent", ""], ["Gomez", "Faustino", ""], ["Ring", "Mark", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1201.0341", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Barnabas Poczos, Andras Lorincz", "title": "Collaborative Filtering via Group-Structured Dictionary Learning", "comments": "A compressed version of the paper has been accepted for publication\n  at the 10th International Conference on Latent Variable Analysis and Source\n  Separation (LVA/ICA 2012)", "journal-ref": "International Conference on Latent Variable Analysis and Source\n  Separation (LVA/ICA), vol. 7191 of LNCS, pp. 247-254, 2012", "doi": "10.1007/978-3-642-28551-6_31", "report-no": null, "categories": "math.OC cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured sparse coding and the related structured dictionary learning\nproblems are novel research areas in machine learning. In this paper we present\na new application of structured dictionary learning for collaborative filtering\nbased recommender systems. Our extensive numerical experiments demonstrate that\nthe presented technique outperforms its state-of-the-art competitors and has\nseveral advantages over approaches that do not put structured constraints on\nthe dictionary elements.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 09:05:33 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["Szabo", "Zoltan", ""], ["Poczos", "Barnabas", ""], ["Lorincz", "Andras", ""]]}, {"id": "1201.0490", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa, Ga\\\"el Varoquaux, Alexandre Gramfort, Vincent\n  Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\\\"uller,\n  Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg,\n  Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher,\n  Matthieu Perrot, \\'Edouard Duchesnay", "title": "Scikit-learn: Machine Learning in Python", "comments": "Update authors list and URLs", "journal-ref": "Journal of Machine Learning Research (2011)", "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scikit-learn is a Python module integrating a wide range of state-of-the-art\nmachine learning algorithms for medium-scale supervised and unsupervised\nproblems. This package focuses on bringing machine learning to non-specialists\nusing a general-purpose high-level language. Emphasis is put on ease of use,\nperformance, documentation, and API consistency. It has minimal dependencies\nand is distributed under the simplified BSD license, encouraging its use in\nboth academic and commercial settings. Source code, binaries, and documentation\ncan be downloaded from http://scikit-learn.org.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2012 16:42:40 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2013 10:24:49 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 14:37:34 GMT"}, {"version": "v4", "created": "Tue, 5 Jun 2018 13:41:07 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Pedregosa", "Fabian", ""], ["Varoquaux", "Ga\u00ebl", ""], ["Gramfort", "Alexandre", ""], ["Michel", "Vincent", ""], ["Thirion", "Bertrand", ""], ["Grisel", "Olivier", ""], ["Blondel", "Mathieu", ""], ["M\u00fcller", "Andreas", ""], ["Nothman", "Joel", ""], ["Louppe", "Gilles", ""], ["Prettenhofer", "Peter", ""], ["Weiss", "Ron", ""], ["Dubourg", "Vincent", ""], ["Vanderplas", "Jake", ""], ["Passos", "Alexandre", ""], ["Cournapeau", "David", ""], ["Brucher", "Matthieu", ""], ["Perrot", "Matthieu", ""], ["Duchesnay", "\u00c9douard", ""]]}, {"id": "1201.0610", "submitter": "Jason J Corso", "authors": "Caiming Xiong, David Johnson, Ran Xu and Jason J. Corso", "title": "Random Forests for Metric Learning with Implicit Pairwise Position\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning makes it plausible to learn distances for complex\ndistributions of data from labeled data. However, to date, most metric learning\nmethods are based on a single Mahalanobis metric, which cannot handle\nheterogeneous data well. Those that learn multiple metrics throughout the space\nhave demonstrated superior accuracy, but at the cost of computational\nefficiency. Here, we take a new angle to the metric learning problem and learn\na single metric that is able to implicitly adapt its distance function\nthroughout the feature space. This metric adaptation is accomplished by using a\nrandom forest-based classifier to underpin the distance function and\nincorporate both absolute pairwise position and standard relative position into\nthe representation. We have implemented and tested our method against state of\nthe art global and multi-metric methods on a variety of data sets. Overall, the\nproposed method outperforms both types of methods in terms of accuracy\n(consistently ranked first) and is an order of magnitude faster than state of\nthe art multi-metric methods (16x faster in the worst case).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 11:29:17 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Xiong", "Caiming", ""], ["Johnson", "David", ""], ["Xu", "Ran", ""], ["Corso", "Jason J.", ""]]}, {"id": "1201.0794", "submitter": "John Lafferty", "authors": "John Lafferty, Han Liu, Larry Wasserman", "title": "Sparse Nonparametric Graphical Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS391 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 519-537", "doi": "10.1214/12-STS391", "report-no": "IMS-STS-STS391", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some nonparametric methods for graphical modeling. In the discrete\ncase, where the data are binary or drawn from a finite alphabet, Markov random\nfields are already essentially nonparametric, since the cliques can take only a\nfinite number of values. Continuous data are different. The Gaussian graphical\nmodel is the standard parametric model for continuous data, but it makes\ndistributional assumptions that are often unrealistic. We discuss two\napproaches to building more flexible graphical models. One allows arbitrary\ngraphs and a nonparametric extension of the Gaussian; the other uses kernel\ndensity estimation and restricts the graphs to trees and forests. Examples of\nboth methods are presented. We also discuss possible future research directions\nfor nonparametric graphical modeling.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 00:43:53 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 13:43:13 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Lafferty", "John", ""], ["Liu", "Han", ""], ["Wasserman", "Larry", ""]]}, {"id": "1201.0838", "submitter": "Jia Zeng", "authors": "Jia Zeng", "title": "A Topic Modeling Toolbox Using Belief Propagation", "comments": "4 pages", "journal-ref": "Journal of Machine Learning Research (13) 2233-2236, 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model\nfor probabilistic topic modeling, which attracts worldwide interests and\ntouches on many important applications in text mining, computer vision and\ncomputational biology. This paper introduces a topic modeling toolbox (TMBP)\nbased on the belief propagation (BP) algorithms. TMBP toolbox is implemented by\nMEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing\ntopic modeling packages, the novelty of this toolbox lies in the BP algorithms\nfor learning LDA-based topic models. The current version includes BP algorithms\nfor latent Dirichlet allocation (LDA), author-topic models (ATM), relational\ntopic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project\nand more BP-based algorithms for various topic models will be added in the near\nfuture. Interested users may also extend BP algorithms for learning more\ncomplicated topic models. The source codes are freely available under the GNU\nGeneral Public Licence, Version 1.0 at https://mloss.org/software/view/399/.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 07:07:06 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2012 06:48:35 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Zeng", "Jia", ""]]}, {"id": "1201.0959", "submitter": "Fabrice Rossi", "authors": "Fabrice Rossi (LTCI), Yves Lechevallier (INRIA Rocquencourt / INRIA\n  Sophia Antipolis)", "title": "Constrained variable clustering and the best basis problem in functional\n  data analysis", "comments": null, "journal-ref": "Classification and Multivariate Analysis for Complex Data\n  Structures 435-444 (2011)", "doi": "10.1007/978-3-642-13312-1_46", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis involves data described by regular functions rather\nthan by a finite number of real valued variables. While some robust data\nanalysis methods can be applied directly to the very high dimensional vectors\nobtained from a fine grid sampling of functional data, all methods benefit from\na prior simplification of the functions that reduces the redundancy induced by\nthe regularity. In this paper we propose to use a clustering approach that\ntargets variables rather than individual to design a piecewise constant\nrepresentation of a set of functions. The contiguity constraint induced by the\nfunctional nature of the variables allows a polynomial complexity algorithm to\ngive the optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 18:39:37 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Rossi", "Fabrice", "", "LTCI"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"]]}, {"id": "1201.0963", "submitter": "Fabrice Rossi", "authors": "Alzennyr Da Silva (INRIA Rocquencourt / INRIA Sophia Antipolis), Yves\n  Lechevallier (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis), Francisco De A. T. De Carvahlo\n  (CIn)", "title": "Clustering Dynamic Web Usage Data", "comments": null, "journal-ref": "Innovative Applications in Data Mining (2009) 71-82", "doi": "10.1007/978-3-540-88045-5", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification methods are based on the assumption that data conforms to\na stationary distribution. The machine learning domain currently suffers from a\nlack of classification techniques that are able to detect the occurrence of a\nchange in the underlying data distribution. Ignoring possible changes in the\nunderlying concept, also known as concept drift, may degrade the performance of\nthe classification model. Often these changes make the model inconsistent and\nregular updatings become necessary. Taking the temporal dimension into account\nduring the analysis of Web usage data is a necessity, since the way a site is\nvisited may indeed evolve due to modifications in the structure and content of\nthe site, or even due to changes in the behavior of certain user groups. One\nsolution to this problem, proposed in this article, is to update models using\nsummaries obtained by means of an evolutionary approach based on an intelligent\nclustering approach. We carry out various clustering strategies that are\napplied on time sub-periods. To validate our approach we apply two external\nevaluation criteria which compare different partitions from the same data set.\nOur experiments show that the proposed approach is efficient to detect the\noccurrence of changes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 18:45:23 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Da Silva", "Alzennyr", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Lechevallier", "Yves", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["Rossi", "Fabrice", "", "INRIA Rocquencourt / INRIA Sophia Antipolis"], ["De Carvahlo", "Francisco De A. T.", "", "CIn"]]}, {"id": "1201.1384", "submitter": "Takashi Isozaki Dr.", "authors": "Takashi Isozaki", "title": "A Thermodynamical Approach for Probability Estimation", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of discrete probability estimation for samples of small size is\naddressed in this study. The maximum likelihood method often suffers\nover-fitting when insufficient data is available. Although the Bayesian\napproach can avoid over-fitting by using prior distributions, it still has\nproblems with objective analysis. In response to these drawbacks, a new\ntheoretical framework based on thermodynamics, where energy and temperature are\nintroduced, was developed. Entropy and likelihood are placed at the center of\nthis method. The key principle of inference for probability mass functions is\nthe minimum free energy, which is shown to unify the two principles of maximum\nlikelihood and maximum entropy. Our method can robustly estimate probability\nfunctions from small size data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 10:15:37 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 06:27:55 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Isozaki", "Takashi", ""]]}, {"id": "1201.1450", "submitter": "Casey Bennett", "authors": "Casey Bennett", "title": "The Interaction of Entropy-Based Discretization and Sample Size: An\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical investigation of the interaction of sample size and\ndiscretization - in this case the entropy-based method CAIM (Class-Attribute\nInterdependence Maximization) - was undertaken to evaluate the impact and\npotential bias introduced into data mining performance metrics due to variation\nin sample size as it impacts the discretization process. Of particular interest\nwas the effect of discretizing within cross-validation folds averse to outside\ndiscretization folds. Previous publications have suggested that discretizing\nexternally can bias performance results; however, a thorough review of the\nliterature found no empirical evidence to support such an assertion. This\ninvestigation involved construction of over 117,000 models on seven distinct\ndatasets from the UCI (University of California-Irvine) Machine Learning\nLibrary and multiple modeling methods across a variety of configurations of\nsample size and discretization, with each unique \"setup\" being independently\nreplicated ten times. The analysis revealed a significant optimistic bias as\nsample sizes decreased and discretization was employed. The study also revealed\nthat there may be a relationship between the interaction that produces such\nbias and the numbers and types of predictor attributes, extending the \"curse of\ndimensionality\" concept from feature selection into the discretization realm.\nDirections for further exploration are laid out, as well some general\nguidelines about the proper application of discretization in light of these\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 16:45:57 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Bennett", "Casey", ""]]}, {"id": "1201.1587", "submitter": "Houtao Deng", "authors": "Houtao Deng and George Runger", "title": "Feature Selection via Regularized Trees", "comments": "8 pages; The 2012 International Joint Conference on Neural Networks\n  (IJCNN), IEEE, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tree regularization framework, which enables many tree models to\nperform feature selection efficiently. The key idea of the regularization\nframework is to penalize selecting a new feature for splitting when its gain\n(e.g. information gain) is similar to the features used in previous splits. The\nregularization framework is applied on random forest and boosted trees here,\nand can be easily applied to other tree models. Experimental studies show that\nthe regularized trees can select high-quality feature subsets with regard to\nboth strong and weak classifiers. Because tree models can naturally deal with\ncategorical and numerical variables, missing values, different scales between\nvariables, interactions and nonlinearities etc., the tree regularization\nframework provides an effective and efficient feature selection solution for\nmany practical problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2012 21:15:32 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2012 01:12:49 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2012 06:31:53 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""]]}, {"id": "1201.1670", "submitter": "Reza Keyvan", "authors": "Siavash Emtiyaz, MohammadReza Keyvanpour", "title": "Customers Behavior Modeling by Semi-Supervised Learning in Customer\n  Relationship Management", "comments": null, "journal-ref": "Advances in information Sciences and Service Sciences(AISS),Volume\n  3, Number 9, October 2011, 229-236", "doi": "10.4156/AISS.vol3.issue9.31", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging the power of increasing amounts of data to analyze customer base\nfor attracting and retaining the most valuable customers is a major problem\nfacing companies in this information age. Data mining technologies extract\nhidden information and knowledge from large data stored in databases or data\nwarehouses, thereby supporting the corporate decision making process. CRM uses\ndata mining (one of the elements of CRM) techniques to interact with customers.\nThis study investigates the use of a technique, semi-supervised learning, for\nthe management and analysis of customer-related data warehouse and information.\nThe idea of semi-supervised learning is to learn not only from the labeled\ntraining data, but to exploit also the structural information in additionally\navailable unlabeled data. The proposed semi-supervised method is a model by\nmeans of a feed-forward neural network trained by a back propagation algorithm\n(multi-layer perceptron) in order to predict the category of an unknown\ncustomer (potential customers). In addition, this technique can be used with\nRapid Miner tools for both labeled and unlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2012 23:59:27 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Emtiyaz", "Siavash", ""], ["Keyvanpour", "MohammadReza", ""]]}, {"id": "1201.2056", "submitter": "Marcus Hutter", "authors": "Alexander O'Neill and Marcus Hutter and Wen Shao and Peter Sunehag", "title": "Adaptive Context Tree Weighting", "comments": "11 LaTeX pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an adaptive context tree weighting (ACTW) algorithm, as an\nextension to the standard context tree weighting (CTW) algorithm. Unlike the\nstandard CTW algorithm, which weights all observations equally regardless of\nthe depth, ACTW gives increasing weight to more recent observations, aiming to\nimprove performance in cases where the input sequence is from a non-stationary\ndistribution. Data compression results show ACTW variants improving over CTW on\nmerged files from standard compression benchmark tests while never being\nsignificantly worse on any individual file.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 14:10:30 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["O'Neill", "Alexander", ""], ["Hutter", "Marcus", ""], ["Shao", "Wen", ""], ["Sunehag", "Peter", ""]]}, {"id": "1201.2173", "submitter": "Gholamreza Bahmanyar", "authors": "Davar Giveki, Hamid Salimi, GholamReza Bahmanyar, Younes Khademian", "title": "Automatic Detection of Diabetes Diagnosis using Feature Weighted Support\n  Vector Machines based on Mutual Information and Modified Cuckoo Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes is a major health problem in both developing and developed countries\nand its incidence is rising dramatically. In this study, we investigate a novel\nautomatic approach to diagnose Diabetes disease based on Feature Weighted\nSupport Vector Machines (FW-SVMs) and Modified Cuckoo Search (MCS). The\nproposed model consists of three stages: Firstly, PCA is applied to select an\noptimal subset of features out of set of all the features. Secondly, Mutual\nInformation is employed to construct the FWSVM by weighting different features\nbased on their degree of importance. Finally, since parameter selection plays a\nvital role in classification accuracy of SVMs, MCS is applied to select the\nbest parameter values. The proposed MI-MCS-FWSVM method obtains 93.58% accuracy\non UCI dataset. The experimental results demonstrate that our method\noutperforms the previous methods by not only giving more accurate results but\nalso significantly speeding up the classification procedure.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 11:03:42 GMT"}], "update_date": "2012-01-12", "authors_parsed": [["Giveki", "Davar", ""], ["Salimi", "Hamid", ""], ["Bahmanyar", "GholamReza", ""], ["Khademian", "Younes", ""]]}, {"id": "1201.2416", "submitter": "Pierre Machart", "authors": "Pierre Machart (LIF), Thomas Peel (LIF, LATP), Liva Ralaivola (LIF),\n  Sandrine Anthoine (LATP), Herv\\'e Glotin (LSIS)", "title": "Stochastic Low-Rank Kernel Learning for Regression", "comments": "International Conference on Machine Learning (ICML'11), Bellevue\n  (Washington) : United States (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to learn a kernel-based regression function. It\nis based on the useof conical combinations of data-based parameterized kernels\nand on a new stochastic convex optimization procedure of which we establish\nconvergence guarantees. The overall learning procedure has the nice properties\nthat a) the learned conical combination is automatically designed to perform\nthe regression task at hand and b) the updates implicated by the optimization\nprocedure are quite inexpensive. In order to shed light on the appositeness of\nour learning strategy, we present empirical results from experiments conducted\non various benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 21:03:55 GMT"}], "update_date": "2012-01-13", "authors_parsed": [["Machart", "Pierre", "", "LIF"], ["Peel", "Thomas", "", "LIF, LATP"], ["Ralaivola", "Liva", "", "LIF"], ["Anthoine", "Sandrine", "", "LATP"], ["Glotin", "Herv\u00e9", "", "LSIS"]]}, {"id": "1201.2555", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Sparse Reward Processes", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of learning problems where the agent is presented with a\nseries of tasks. Intuitively, if there is relation among those tasks, then the\ninformation gained during execution of one task has value for the execution of\nanother task. Consequently, the agent is intrinsically motivated to explore its\nenvironment beyond the degree necessary to solve the current task it has at\nhand. We develop a decision theoretic setting that generalises standard\nreinforcement learning tasks and captures this intuition. More precisely, we\nconsider a multi-stage stochastic game between a learning agent and an\nopponent. We posit that the setting is a good model for the problem of\nlife-long learning in uncertain environments, where while resources must be\nspent learning about currently important tasks, there is also the need to\nallocate effort towards learning about aspects of the world which are not\nrelevant at the moment. This is due to the fact that unpredictable future\nevents may lead to a change of priorities for the decision maker. Thus, in some\nsense, the model \"explains\" the necessity of curiosity. Apart from introducing\nthe general formalism, the paper provides algorithms. These are evaluated\nexperimentally in some exemplary domains. In addition, performance bounds are\nproven for some cases of this problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 13:08:27 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2012 12:23:38 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1201.2575", "submitter": "Sung-eok Jeon", "authors": "Sung-eok Jeon, and Chuanyi Ji", "title": "Joint Approximation of Information and Distributed Link-Scheduling\n  Decisions in Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a large multi-hop wireless network, nodes are preferable to make\ndistributed and localized link-scheduling decisions with only interactions\namong a small number of neighbors. However, for a slowly decaying channel and\ndensely populated interferers, a small size neighborhood often results in\nnontrivial link outages and is thus insufficient for making optimal scheduling\ndecisions. A question arises how to deal with the information outside a\nneighborhood in distributed link-scheduling. In this work, we develop joint\napproximation of information and distributed link scheduling. We first apply\nmachine learning approaches to model distributed link-scheduling with complete\ninformation. We then characterize the information outside a neighborhood in\nform of residual interference as a random loss variable. The loss variable is\nfurther characterized by either a Mean Field approximation or a normal\ndistribution based on the Lyapunov central limit theorem. The approximated\ninformation outside a neighborhood is incorporated in a factor graph. This\nresults in joint approximation and distributed link-scheduling in an iterative\nfashion. Link-scheduling decisions are first made at each individual node based\non the approximated loss variables. Loss variables are then updated and used\nfor next link-scheduling decisions. The algorithm repeats between these two\nphases until convergence. Interactive iterations among these variables are\nimplemented with a message-passing algorithm over a factor graph. Simulation\nresults show that using learned information outside a neighborhood jointly with\ndistributed link-scheduling reduces the outage probability close to zero even\nfor a small neighborhood.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 14:28:23 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2012 23:06:41 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Jeon", "Sung-eok", ""], ["Ji", "Chuanyi", ""]]}, {"id": "1201.2605", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai and J\\\"org L\\\"ucke", "title": "Autonomous Cleaning of Corrupted Scanned Documents - A Generative\n  Modeling Approach", "comments": "oral presentation and Google Student Travel Award; IEEE conference on\n  Computer Vision and Pattern Recognition 2012", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2313126", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of cleaning scanned text documents that are strongly\ncorrupted by dirt such as manual line strokes, spilled ink etc. We aim at\nautonomously removing dirt from a single letter-size page based only on the\ninformation the page contains. Our approach, therefore, has to learn character\nrepresentations without supervision and requires a mechanism to distinguish\nlearned representations from irregular patterns. To learn character\nrepresentations, we use a probabilistic generative model parameterizing pattern\nfeatures, feature variances, the features' planar arrangements, and pattern\nfrequencies. The latent variables of the model describe pattern class, pattern\nposition, and the presence or absence of individual pattern features. The model\nparameters are optimized using a novel variational EM approximation. After\nlearning, the parameters represent, independently of their absolute position,\nplanar feature arrangements and their variances. A quality measure defined\nbased on the learned representation then allows for an autonomous\ndiscrimination between regular character patterns and the irregular patterns\nmaking up the dirt. The irregular patterns can thus be removed to clean the\ndocument. For a full Latin alphabet we found that a single page does not\ncontain sufficiently many character examples. However, even if heavily\ncorrupted by dirt, we show that a page containing a lower number of character\ntypes can efficiently and autonomously be cleaned solely based on the\nstructural regularity of the characters it contains. In different examples\nusing characters from different alphabets, we demonstrate generality of the\napproach and discuss its implications for future developments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 16:09:10 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 12:42:01 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Dai", "Zhenwen", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1201.2902", "submitter": "Marian George", "authors": "Marian George, Moustafa Youssef", "title": "Acoustical Quality Assessment of the Classroom Environment", "comments": "7 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching is one of the most important factors affecting any education system.\nMany research efforts have been conducted to facilitate the presentation modes\nused by instructors in classrooms as well as provide means for students to\nreview lectures through web browsers. Other studies have been made to provide\nacoustical design recommendations for classrooms like room size and\nreverberation times. However, using acoustical features of classrooms as a way\nto provide education systems with feedback about the learning process was not\nthoroughly investigated in any of these studies. We propose a system that\nextracts different sound features of students and instructors, and then uses\nmachine learning techniques to evaluate the acoustical quality of any learning\nenvironment. We infer conclusions about the students' satisfaction with the\nquality of lectures. Using classifiers instead of surveys and other subjective\nways of measures can facilitate and speed such experiments which enables us to\nperform them continuously. We believe our system enables education systems to\ncontinuously review and improve their teaching strategies and acoustical\nquality of classrooms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 17:46:17 GMT"}], "update_date": "2012-01-16", "authors_parsed": [["George", "Marian", ""], ["Youssef", "Moustafa", ""]]}, {"id": "1201.2925", "submitter": "Geetha  Manjunath", "authors": "Geetha Manjunatha, M Narasimha Murty, Dinkar Sitaram", "title": "Combining Heterogeneous Classifiers for Relational Databases", "comments": "Withdrawn - as that was a trial upload only. Non public information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Most enterprise data is distributed in multiple relational databases with\nexpert-designed schema. Using traditional single-table machine learning\ntechniques over such data not only incur a computational penalty for converting\nto a 'flat' form (mega-join), even the human-specified semantic information\npresent in the relations is lost. In this paper, we present a practical,\ntwo-phase hierarchical meta-classification algorithm for relational databases\nwith a semantic divide and conquer approach. We propose a recursive, prediction\naggregation technique over heterogeneous classifiers applied on individual\ndatabase tables. The proposed algorithm was evaluated on three diverse\ndatasets, namely TPCH, PKDD and UCI benchmarks and showed considerable\nreduction in classification time without any loss of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 19:54:27 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 20:23:24 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Manjunatha", "Geetha", ""], ["Murty", "M Narasimha", ""], ["Sitaram", "Dinkar", ""]]}, {"id": "1201.3249", "submitter": "Gerard Howard", "authors": "Gerard Howard and Larry Bull and Pier-Luca Lanzi", "title": "A Spiking Neural Learning Classifier System", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Classifier Systems (LCS) are population-based reinforcement learners\nused in a wide variety of applications. This paper presents a LCS where each\ntraditional rule is represented by a spiking neural network, a type of network\nwith dynamic internal state. We employ a constructivist model of growth of both\nneurons and dendrites that realise flexible learning by evolving structures of\nsufficient complexity to solve a well-known problem involving continuous,\nreal-valued inputs. Additionally, we extend the system to enable temporal state\ndecomposition. By allowing our LCS to chain together sequences of heterogeneous\nactions into macro-actions, it is shown to perform optimally in a problem where\ntraditional methods can fail to find a solution in a reasonable amount of time.\nOur final system is tested on a simulated robotics platform.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 13:19:55 GMT"}], "update_date": "2012-01-17", "authors_parsed": [["Howard", "Gerard", ""], ["Bull", "Larry", ""], ["Lanzi", "Pier-Luca", ""]]}, {"id": "1201.3382", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow and Aaron Courville and Yoshua Bengio", "title": "Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using a factor model we call {\\em spike-and-slab\nsparse coding} (S3C) to learn features for a classification task. The S3C model\nresembles both the spike-and-slab RBM and sparse coding. Since exact inference\nin this model is intractable, we derive a structured variational inference\nprocedure and employ a variational EM training algorithm. Prior work on\napproximate inference for this model has not prioritized the ability to exploit\nparallel architectures and scale to enormous problem sizes. We present an\ninference procedure appropriate for use with GPUs which allows us to\ndramatically increase both the training set size and the amount of latent\nfactors.\n  We demonstrate that this approach improves upon the supervised learning\ncapabilities of both sparse coding and the ssRBM on the CIFAR-10 dataset. We\nevaluate our approach's potential for semi-supervised learning on subsets of\nCIFAR-10. We demonstrate state-of-the art self-taught learning performance on\nthe STL-10 dataset and use our method to win the NIPS 2011 Workshop on\nChallenges In Learning Hierarchical Models' Transfer Learning Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 22:00:07 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 22:48:52 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1201.3674", "submitter": "Allen Yang", "authors": "Dheeraj Singaraju, Ehsan Elhamifar, Roberto Tron, Allen Y. Yang, S.\n  Shankar Sastry", "title": "On the Lagrangian Biduality of Sparsity Minimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in Compressive Sensing have shown that, under certain\nconditions, the solution to an underdetermined system of linear equations with\nsparsity-based regularization can be accurately recovered by solving convex\nrelaxations of the original problem. In this work, we present a novel\nprimal-dual analysis on a class of sparsity minimization problems. We show that\nthe Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the\nsparsity minimization problems can be used to derive interesting convex\nrelaxations: the bidual of the $\\ell_0$-minimization problem is the\n$\\ell_1$-minimization problem; and the bidual of the $\\ell_{0,1}$-minimization\nproblem for enforcing group sparsity on structured data is the\n$\\ell_{1,\\infty}$-minimization problem. The analysis provides a means to\ncompute per-instance non-trivial lower bounds on the (group) sparsity of the\ndesired solutions. In a real-world application, the bidual relaxation improves\nthe performance of a sparsity-based classification framework applied to robust\nface recognition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 00:46:12 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Singaraju", "Dheeraj", ""], ["Elhamifar", "Ehsan", ""], ["Tron", "Roberto", ""], ["Yang", "Allen Y.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1201.4002", "submitter": "Apostolos Burnetas", "authors": "Apostolos Burnetas and Odysseas Kanavetas", "title": "Adaptive Policies for Sequential Sampling under Incomplete Information\n  and a Cost Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential sampling from a finite number of\nindependent statistical populations to maximize the expected infinite horizon\naverage outcome per period, under a constraint that the expected average\nsampling cost does not exceed an upper bound. The outcome distributions are not\nknown. We construct a class of consistent adaptive policies, under which the\naverage outcome converges with probability 1 to the true value under complete\ninformation for all distributions with finite means. We also compare the rate\nof convergence for various policies in this class using simulation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 10:06:29 GMT"}], "update_date": "2012-01-20", "authors_parsed": [["Burnetas", "Apostolos", ""], ["Kanavetas", "Odysseas", ""]]}, {"id": "1201.4714", "submitter": "Huyen  Do", "authors": "Huyen Do, Alexandros Kalousis, Jun Wang and Adam Woznica", "title": "A metric learning perspective of SVM: on the relation of SVM and LMNN", "comments": "To appear in AISTATS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor\nalgorithm, LMNN, are two very popular learning algorithms with quite different\nlearning biases. In this paper we bring them into a unified view and show that\nthey have a much stronger relation than what is commonly thought. We analyze\nSVMs from a metric learning perspective and cast them as a metric learning\nproblem, a view which helps us uncover the relations of the two algorithms. We\nshow that LMNN can be seen as learning a set of local SVM-like models in a\nquadratic space. Along the way and inspired by the metric-based interpretation\nof SVM s we derive a novel variant of SVMs, epsilon-SVM, to which LMNN is even\nmore similar. We give a unified view of LMNN and the different SVM variants.\nFinally we provide some preliminary experiments on a number of benchmark\ndatasets in which show that epsilon-SVM compares favorably both with respect to\nLMNN and SVM.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 13:48:33 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Do", "Huyen", ""], ["Kalousis", "Alexandros", ""], ["Wang", "Jun", ""], ["Woznica", "Adam", ""]]}, {"id": "1201.4777", "submitter": "Alfonso E. Romero PhD", "authors": "Alfonso E. Romero, Luis M. de Campos", "title": "A probabilistic methodology for multilabel classification", "comments": "14 pages, 1 figure, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilabel classification is a relatively recent subfield of machine\nlearning. Unlike to the classical approach, where instances are labeled with\nonly one category, in multilabel classification, an arbitrary number of\ncategories is chosen to label an instance. Due to the problem complexity (the\nsolution is one among an exponential number of alternatives), a very common\nsolution (the binary method) is frequently used, learning a binary classifier\nfor every category, and combining them all afterwards. The assumption taken in\nthis solution is not realistic, and in this work we give examples where the\ndecisions for all the labels are not taken independently, and thus, a\nsupervised approach should learn those existing relationships among categories\nto make a better classification. Therefore, we show here a generic methodology\nthat can improve the results obtained by a set of independent probabilistic\nbinary classifiers, by using a combination procedure with a classifier trained\non the co-occurrences of the labels. We show an exhaustive experimentation in\nthree different standard corpora of labeled documents (Reuters-21578,\nOhsumed-23 and RCV1), which present noticeable improvements in all of them,\nwhen using our methodology, in three probabilistic base classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 17:25:34 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2013 20:22:47 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Romero", "Alfonso E.", ""], ["de Campos", "Luis M.", ""]]}, {"id": "1201.4906", "submitter": "Keqin Liu", "authors": "Keqin Liu, Qing Zhao", "title": "Adaptive Shortest-Path Routing under Unknown and Stochastically Varying\n  Link States", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adaptive shortest-path routing problem in wireless networks\nunder unknown and stochastically varying link states. In this problem, we aim\nto optimize the quality of communication between a source and a destination\nthrough adaptive path selection. Due to the randomness and uncertainties in the\nnetwork dynamics, the quality of each link varies over time according to a\nstochastic process with unknown distributions. After a path is selected for\ncommunication, the aggregated quality of all links on this path (e.g., total\npath delay) is observed. The quality of each individual link is not observable.\nWe formulate this problem as a multi-armed bandit with dependent arms. We show\nthat by exploiting arm dependencies, a regret polynomial with network size can\nbe achieved while maintaining the optimal logarithmic order with time. This is\nin sharp contrast with the exponential regret order with network size offered\nby a direct application of the classic MAB policies that ignore arm\ndependencies. Furthermore, our results are obtained under a general model of\nlink-quality distributions (including heavy-tailed distributions) and find\napplications in cognitive radio and ad hoc networks with unknown and dynamic\ncommunication environments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 02:45:58 GMT"}], "update_date": "2012-01-25", "authors_parsed": [["Liu", "Keqin", ""], ["Zhao", "Qing", ""]]}, {"id": "1201.5217", "submitter": "Mohammad Tarek Al-Muallim M.Sc.", "authors": "M. T. Al-Muallim, R. El-Kouatly", "title": "Unsupervised Classification Using Immune Algorithm", "comments": null, "journal-ref": null, "doi": "10.5120/677-952", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised classification algorithm based on clonal selection principle\nnamed Unsupervised Clonal Selection Classification (UCSC) is proposed in this\npaper. The new proposed algorithm is data driven and self-adaptive, it adjusts\nits parameters to the data to make the classification operation as fast as\npossible. The performance of UCSC is evaluated by comparing it with the well\nknown K-means algorithm using several artificial and real-life data sets. The\nexperiments show that the proposed UCSC algorithm is more reliable and has high\nclassification precision comparing to traditional classification methods such\nas K-means.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 09:44:06 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["Al-Muallim", "M. T.", ""], ["El-Kouatly", "R.", ""]]}, {"id": "1201.5283", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu", "title": "An Efficient Primal-Dual Prox Method for Non-Smooth Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-smooth optimization problems in machine learning, where both\nthe loss function and the regularizer are non-smooth functions. Previous\nstudies on efficient empirical loss minimization assume either a smooth loss\nfunction or a strongly convex regularizer, making them unsuitable for\nnon-smooth optimization. We develop a simple yet efficient method for a family\nof non-smooth optimization problems where the dual form of the loss function is\nbilinear in primal and dual variables. We cast a non-smooth optimization\nproblem into a minimax optimization problem, and develop a primal dual prox\nmethod that solves the minimax optimization problem at a rate of $O(1/T)$\n{assuming that the proximal step can be efficiently solved}, significantly\nfaster than a standard subgradient descent method that has an $O(1/\\sqrt{T})$\nconvergence rate. Our empirical study verifies the efficiency of the proposed\nmethod for various non-smooth optimization problems that arise ubiquitously in\nmachine learning by comparing it to the state-of-the-art first order methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 04:09:54 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2012 17:50:21 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2012 16:11:15 GMT"}, {"version": "v4", "created": "Mon, 2 Apr 2012 15:50:38 GMT"}, {"version": "v5", "created": "Fri, 26 Jul 2013 05:03:51 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Yang", "Tianbao", ""], ["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1201.5338", "submitter": "Xiang Wang", "authors": "Xiang Wang, Buyue Qian, Ian Davidson", "title": "On Constrained Spectral Clustering and Its Applications", "comments": "Data Mining and Knowledge Discovery, 2012", "journal-ref": null, "doi": "10.1007/s10618-012-0291-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained clustering has been well-studied for algorithms such as $K$-means\nand hierarchical clustering. However, how to satisfy many constraints in these\nalgorithmic settings has been shown to be intractable. One alternative to\nencode many constraints is to use spectral clustering, which remains a\ndeveloping area. In this paper, we propose a flexible framework for constrained\nspectral clustering. In contrast to some previous efforts that implicitly\nencode Must-Link and Cannot-Link constraints by modifying the graph Laplacian\nor constraining the underlying eigenspace, we present a more natural and\nprincipled formulation, which explicitly encodes the constraints as part of a\nconstrained optimization problem. Our method offers several practical\nadvantages: it can encode the degree of belief in Must-Link and Cannot-Link\nconstraints; it guarantees to lower-bound how well the given constraints are\nsatisfied using a user-specified threshold; it can be solved deterministically\nin polynomial time through generalized eigendecomposition. Furthermore, by\ninheriting the objective function from spectral clustering and encoding the\nconstraints explicitly, much of the existing analysis of unconstrained spectral\nclustering techniques remains valid for our formulation. We validate the\neffectiveness of our approach by empirical results on both artificial and real\ndatasets. We also demonstrate an innovative use of encoding large number of\nconstraints: transfer learning via constraints.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 18:36:11 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 06:04:35 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Wang", "Xiang", ""], ["Qian", "Buyue", ""], ["Davidson", "Ian", ""]]}, {"id": "1201.5604", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Discrete and fuzzy dynamical genetic programming in the XCSF learning\n  classifier system", "comments": null, "journal-ref": "Soft Computing (2014), 18(1):153-167", "doi": "10.1007/s00500-013-1044-4", "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of representation schemes have been presented for use within\nlearning classifier systems, ranging from binary encodings to neural networks.\nThis paper presents results from an investigation into using discrete and fuzzy\ndynamical system representations within the XCSF learning classifier system. In\nparticular, asynchronous random Boolean networks are used to represent the\ntraditional condition-action production system rules in the discrete case and\nasynchronous fuzzy logic networks in the continuous-valued case. It is shown\npossible to use self-adaptive, open-ended evolution to design an ensemble of\nsuch dynamical systems within XCSF to solve a number of well-known test\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2012 18:54:42 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 15:34:57 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1201.6053", "submitter": "Golriz Amooee", "authors": "Golriz Amooee, Behrouz Minaei-Bidgoli, Malihe Bagheri-Dehnavi", "title": "A Comparison Between Data Mining Prediction Algorithms for Fault\n  Detection(Case study: Ahanpishegan co.)", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 3, November 2011 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current competitive world, industrial companies seek to manufacture\nproducts of higher quality which can be achieved by increasing reliability,\nmaintainability and thus the availability of products. On the other hand,\nimprovement in products lifecycle is necessary for achieving high reliability.\nTypically, maintenance activities are aimed to reduce failures of industrial\nmachinery and minimize the consequences of such failures. So the industrial\ncompanies try to improve their efficiency by using different fault detection\ntechniques. One strategy is to process and analyze previous generated data to\npredict future failures. The purpose of this paper is to detect wasted parts\nusing different data mining algorithms and compare the accuracy of these\nalgorithms. A combination of thermal and physical characteristics has been used\nand the algorithms were implemented on Ahanpishegan's current data to estimate\nthe availability of its produced parts.\n  Keywords: Data Mining, Fault Detection, Availability, Prediction Algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2012 16:23:54 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Amooee", "Golriz", ""], ["Minaei-Bidgoli", "Behrouz", ""], ["Bagheri-Dehnavi", "Malihe", ""]]}, {"id": "1201.6251", "submitter": "Panagiotis Tigkas", "authors": "Panagiotis Tigas", "title": "Real-time jam-session support system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a method for the problem of real time chord accompaniment of\nimprovised music. Our implementation can learn an underlying structure of the\nmusical performance and predict next chord. The system uses Hidden Markov Model\nto find the most probable chord sequence for the played melody and then a\nVariable Order Markov Model is used to a) learn the structure (if any) and b)\npredict next chord. We implemented our system in Java and MAX/Msp and compared\nand evaluated using objective (prediction accuracy) and subjective\n(questionnaire) evaluation methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 18:30:11 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Tigas", "Panagiotis", ""]]}, {"id": "1201.6462", "submitter": "Ron Begleiter", "authors": "Nir Ailon and Ron Begleiter", "title": "Active Learning of Custering with Side Information Using $\\eps$-Smooth\n  Relative Regret Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is considered a non-supervised learning setting, in which the goal\nis to partition a collection of data points into disjoint clusters. Often a\nbound $k$ on the number of clusters is given or assumed by the practitioner.\nMany versions of this problem have been defined, most notably $k$-means and\n$k$-median.\n  An underlying problem with the unsupervised nature of clustering it that of\ndetermining a similarity function. One approach for alleviating this difficulty\nis known as clustering with side information, alternatively, semi-supervised\nclustering. Here, the practitioner incorporates side information in the form of\n\"must be clustered\" or \"must be separated\" labels for data point pairs. Each\nsuch piece of information comes at a \"query cost\" (often involving human\nresponse solicitation). The collection of labels is then incorporated in the\nusual clustering algorithm as either strict or as soft constraints, possibly\nadding a pairwise constraint penalty function to the chosen clustering\nobjective.\n  Our work is mostly related to clustering with side information. We ask how to\nchoose the pairs of data points. Our analysis gives rise to a method provably\nbetter than simply choosing them uniformly at random. Roughly speaking, we show\nthat the distribution must be biased so as more weight is placed on pairs\nincident to elements in smaller clusters in some optimal solution. Of course we\ndo not know the optimal solution, hence we don't know the bias. Using the\nrecently introduced method of $\\eps$-smooth relative regret approximations of\nAilon, Begleiter and Ezra, we can show an iterative process that improves both\nthe clustering and the bias in tandem. The process provably converges to the\noptimal solution faster (in terms of query cost) than an algorithm selecting\npairs uniformly.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 07:46:08 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Ailon", "Nir", ""], ["Begleiter", "Ron", ""]]}, {"id": "1201.6530", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harish Karnick", "title": "Random Feature Maps for Dot Product Kernels", "comments": "To appear in the proceedings of the 15th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2012). This version corrects\n  a minor error with Lemma 10. Acknowledgements : Devanshu Bhimwal", "journal-ref": "Journal of Machine Learning Research, W&CP 22 (2012) 583-591", "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating non-linear kernels using feature maps has gained a lot of\ninterest in recent years due to applications in reducing training and testing\ntimes of SVM classifiers and other kernel based learning algorithms. We extend\nthis line of work and present low distortion embeddings for dot product kernels\ninto linear Euclidean spaces. We base our results on a classical result in\nharmonic analysis characterizing all dot product kernels and use it to define\nrandomized feature maps into explicit low dimensional Euclidean spaces in which\nthe native dot product provides an approximation to the dot product kernel with\nhigh confidence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 12:59:50 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 13:57:55 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2012 10:56:00 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Kar", "Purushottam", ""], ["Karnick", "Harish", ""]]}, {"id": "1201.6583", "submitter": "Tobias Jung", "authors": "Tobias Jung and Daniel Polani and Peter Stone", "title": "Empowerment for Continuous Agent-Environment Systems", "comments": null, "journal-ref": "Adaptive Behavior 19(1),2011", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops generalizations of empowerment to continuous states.\nEmpowerment is a recently introduced information-theoretic quantity motivated\nby hypotheses about the efficiency of the sensorimotor loop in biological\norganisms, but also from considerations stemming from curiosity-driven\nlearning. Empowemerment measures, for agent-environment systems with stochastic\ntransitions, how much influence an agent has on its environment, but only that\ninfluence that can be sensed by the agent sensors. It is an\ninformation-theoretic generalization of joint controllability (influence on\nenvironment) and observability (measurement by sensors) of the environment by\nthe agent, both controllability and observability being usually defined in\ncontrol theory as the dimensionality of the control/observation spaces. Earlier\nwork has shown that empowerment has various interesting and relevant\nproperties, e.g., it allows us to identify salient states using only the\ndynamics, and it can act as intrinsic reward without requiring an external\nreward. However, in this previous work empowerment was limited to the case of\nsmall-scale and discrete domains and furthermore state transition probabilities\nwere assumed to be known. The goal of this paper is to extend empowerment to\nthe significantly more important and relevant case of continuous vector-valued\nstate spaces and initially unknown state transition probabilities. The\ncontinuous state space is addressed by Monte-Carlo approximation; the unknown\ntransitions are addressed by model learning and prediction for which we apply\nGaussian processes regression with iterated forecasting. In a number of\nwell-known continuous control tasks we examine the dynamics induced by\nempowerment and include an application to exploration and online model\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 15:46:27 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Jung", "Tobias", ""], ["Polani", "Daniel", ""], ["Stone", "Peter", ""]]}, {"id": "1201.6604", "submitter": "Tobias Jung", "authors": "Tobias Jung and Peter Stone", "title": "Gaussian Processes for Sample Efficient Reinforcement Learning with\n  RMAX-like Exploration", "comments": "European Conference on Machine Learning (ECML'2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of model-based online reinforcement learning\n(RL) for continuous domains with deterministic transitions that is specifically\ndesigned to achieve low sample complexity. To achieve low sample complexity,\nsince the environment is unknown, an agent must intelligently balance\nexploration and exploitation, and must be able to rapidly generalize from\nobservations. While in the past a number of related sample efficient RL\nalgorithms have been proposed, to allow theoretical analysis, mainly\nmodel-learners with weak generalization capabilities were considered. Here, we\nseparate function approximation in the model learner (which does require\nsamples) from the interpolation in the planner (which does not require\nsamples). For model-learning we apply Gaussian processes regression (GP) which\nis able to automatically adjust itself to the complexity of the problem (via\nBayesian hyperparameter selection) and, in practice, often able to learn a\nhighly accurate model from very little data. In addition, a GP provides a\nnatural way to determine the uncertainty of its predictions, which allows us to\nimplement the \"optimism in the face of uncertainty\" principle used to\nefficiently control exploration. Our method is evaluated on four common\nbenchmark domains.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 16:36:51 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Jung", "Tobias", ""], ["Stone", "Peter", ""]]}, {"id": "1201.6615", "submitter": "Tobias Jung", "authors": "Tobias Jung and Peter Stone", "title": "Feature Selection for Value Function Approximation Using Bayesian Model\n  Selection", "comments": "European Conference on Machine Learning (ECML'09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection in reinforcement learning (RL), i.e. choosing basis\nfunctions such that useful approximations of the unkown value function can be\nobtained, is one of the main challenges in scaling RL to real-world\napplications. Here we consider the Gaussian process based framework GPTD for\napproximate policy evaluation, and propose feature selection through marginal\nlikelihood optimization of the associated hyperparameters. Our approach has two\nappealing benefits: (1) given just sample transitions, we can solve the policy\nevaluation problem fully automatically (without looking at the learning task,\nand, in theory, independent of the dimensionality of the state space), and (2)\nmodel selection allows us to consider more sophisticated kernels, which in turn\nenable us to identify relevant subspaces and eliminate irrelevant state\nvariables such that we can achieve substantial computational savings and\nimproved prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 16:57:55 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Jung", "Tobias", ""], ["Stone", "Peter", ""]]}, {"id": "1201.6626", "submitter": "Tobias Jung", "authors": "Tobias Jung and Daniel Polani", "title": "Learning RoboCup-Keepaway with Kernels", "comments": null, "journal-ref": "JMLR Workshop and Conference Proceedings (1st Gaussian Processes\n  in Practice Workshop, 2006)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply kernel-based methods to solve the difficult reinforcement learning\nproblem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in\nkeepaway are the high-dimensionality of the state space (rendering conventional\ndiscretization-based function approximation like tilecoding infeasible), the\nstochasticity due to noise and multiple learning agents needing to cooperate\n(meaning that the exact dynamics of the environment are unknown) and real-time\nlearning (meaning that an efficient online implementation is required). We\nemploy the general framework of approximate policy iteration with\nleast-squares-based policy evaluation. As underlying function approximator we\nconsider the family of regularization networks with subset of regressors\napproximation. The core of our proposed solution is an efficient recursive\nimplementation with automatic supervised selection of relevant basis functions.\nSimulation results indicate that the behavior learned through our approach\nclearly outperforms the best results obtained earlier with tilecoding by Stone\net al. (2005).\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 17:26:17 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Jung", "Tobias", ""], ["Polani", "Daniel", ""]]}]