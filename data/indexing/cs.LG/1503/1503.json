[{"id": "1503.00024", "submitter": "Sharan Vaswani", "authors": "Sharan Vaswani, Laks.V.S. Lakshmanan and Mark Schmidt", "title": "Influence Maximization with Bandits", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of \\emph{influence maximization}, the problem of\nmaximizing the number of people that become aware of a product by finding the\n`best' set of `seed' users to expose the product to. Most prior work on this\ntopic assumes that we know the probability of each user influencing each other\nuser, or we have data that lets us estimate these influences. However, this\ninformation is typically not initially available or is difficult to obtain. To\navoid this assumption, we adopt a combinatorial multi-armed bandit paradigm\nthat estimates the influence probabilities as we sequentially try different\nseed sets. We establish bounds on the performance of this procedure under the\nexisting edge-level feedback as well as a novel and more realistic node-level\nfeedback. Beyond our theoretical results, we describe a practical\nimplementation and experimentally demonstrate its efficiency and effectiveness\non four real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:59:08 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 20:42:52 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 19:53:49 GMT"}, {"version": "v4", "created": "Wed, 27 Apr 2016 18:27:20 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Vaswani", "Sharan", ""], ["Lakshmanan", "Laks. V. S.", ""], ["Schmidt", "Mark", ""]]}, {"id": "1503.00036", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "Norm-Based Capacity Control in Neural Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capacity, convexity and characterization of a general\nfamily of norm-constrained feed-forward networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:50:22 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 22:55:08 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1503.00038", "submitter": "Md Amran Siddiqui", "authors": "Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich and Weng-Keen Wong", "title": "Sequential Feature Explanations for Anomaly Detection", "comments": "9 pages, 4 figures and submitted to KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 00:04:11 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Siddiqui", "Md Amran", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.00075", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Richard Socher, Christopher D. Manning", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:31:50 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 20:13:25 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 06:51:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Socher", "Richard", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1503.00164", "submitter": "Yuan Yao", "authors": "Braxton Osting and Jiechao Xiong and Qianqian Xu and Yuan Yao", "title": "Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse\n  Random Graphs", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis, 2016", "doi": "10.1016/j.acha.2016.03.007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms are now extensively used for conducting subjective\npairwise comparison studies. In this setting, a pairwise comparison dataset is\ntypically gathered via random sampling, either \\emph{with} or \\emph{without}\nreplacement. In this paper, we use tools from random graph theory to analyze\nthese two random sampling methods for the HodgeRank estimator. Using the\nFiedler value of the graph as a measurement for estimator stability\n(informativeness), we provide a new estimate of the Fiedler value for these two\nrandom graph models. In the asymptotic limit as the number of vertices tends to\ninfinity, we prove the validity of the estimate. Based on our findings, for a\nsmall number of items to be compared, we recommend a two-stage sampling\nstrategy where a greedy sampling method is used initially and random sampling\n\\emph{without} replacement is used in the second stage. When a large number of\nitems is to be compared, we recommend random sampling with replacement as this\nis computationally inexpensive and trivially parallelizable. Experiments on\nsynthetic and real-world datasets support our analysis.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 18:32:45 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 11:47:10 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Osting", "Braxton", ""], ["Xiong", "Jiechao", ""], ["Xu", "Qianqian", ""], ["Yao", "Yuan", ""]]}, {"id": "1503.00244", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and\n  Management", "comments": "IEEE Data Science and Advanced Analytics (DSAA'2014)", "journal-ref": null, "doi": "10.1109/DSAA.2014.7058121", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global influence of Big Data is not only growing but seemingly endless.\nThe trend is leaning towards knowledge that is attained easily and quickly from\nmassive pools of Big Data. Today we are living in the technological world that\nDr. Usama Fayyad and his distinguished research fellows discussed in the\nintroductory explanations of Knowledge Discovery in Databases (KDD) predicted\nnearly two decades ago. Indeed, they were precise in their outlook on Big Data\nanalytics. In fact, the continued improvement of the interoperability of\nmachine learning, statistics, database building and querying fused to create\nthis increasingly popular science- Data Mining and Knowledge Discovery. The\nnext generation computational theories are geared towards helping to extract\ninsightful knowledge from even larger volumes of data at higher rates of speed.\nAs the trend increases in popularity, the need for a highly adaptive solution\nfor knowledge discovery will be necessary. In this research paper, we are\nintroducing the investigation and development of 23 bit-questions for a\nMetaknowledge template for Big Data Processing and clustering purposes. This\nresearch aims to demonstrate the construction of this methodology and proves\nthe validity and the beneficial utilization that brings Knowledge Discovery\nfrom Big Data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:41:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00255", "submitter": "Nahum Shimkin", "authors": "Nahum Shimkin", "title": "An Online Convex Optimization Approach to Blackwell's Approachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of approachability in repeated games with vector payoffs was\nintroduced by Blackwell in the 1950s, along with geometric conditions for\napproachability and corresponding strategies that rely on computing {\\em\nsteering directions} as projections from the current average payoff vector to\nthe (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposed\na class of approachability algorithms that rely on the no-regret properties of\nOnline Linear Programming for computing a suitable sequence of steering\ndirections. This is first carried out for target sets that are convex cones,\nand then generalized to any convex set by embedding it in a higher-dimensional\nconvex cone. In this paper we present a more direct formulation that relies on\nthe support function of the set, along with suitable Online Convex Optimization\nalgorithms, which leads to a general class of approachability algorithms. We\nfurther show that Blackwell's original algorithm and its convergence follow as\na special case.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 11:46:35 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Shimkin", "Nahum", ""]]}, {"id": "1503.00269", "submitter": "Marco Loog", "authors": "Marco Loog", "title": "Contrastive Pessimistic Likelihood Estimation for Semi-Supervised\n  Classification", "comments": "32 pages, minor revision submitted to TPAMI, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvement guarantees for semi-supervised classifiers can currently only be\ngiven under restrictive conditions on the data. We propose a general way to\nperform semi-supervised parameter estimation for likelihood-based classifiers\nfor which, on the full training set, the estimates are never worse than the\nsupervised solution in terms of the log-likelihood. We argue, moreover, that we\nmay expect these solutions to really improve upon the supervised classifier in\nparticular cases. In a worked-out example for LDA, we take it one step further\nand essentially prove that its semi-supervised version is strictly better than\nits supervised counterpart. The two new concepts that form the core of our\nestimation principle are contrast and pessimism. The former refers to the fact\nthat our objective function takes the supervised estimates into account,\nenabling the semi-supervised solution to explicitly control the potential\nimprovements over this estimate. The latter refers to the fact that our\nestimates are conservative and therefore resilient to whatever form the true\nlabeling of the unlabeled data takes on. Experiments demonstrate the\nimprovements in terms of both the log-likelihood and the classification error\nrate on independent test sets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 13:16:43 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 21:36:53 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Loog", "Marco", ""]]}, {"id": "1503.00323", "submitter": "Efr\\'en Cruz Cort\\'es", "authors": "E. Cruz Cort\\'es, C. Scott", "title": "Sparse Approximation of a Kernel Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel means are frequently used to represent probability distributions in\nmachine learning problems. In particular, the well known kernel density\nestimator and the kernel mean embedding both have the form of a kernel mean.\nUnfortunately, kernel means are faced with scalability issues. A single point\nevaluation of the kernel density estimator, for example, requires a computation\ntime linear in the training sample size. To address this challenge, we present\na method to efficiently construct a sparse approximation of a kernel mean. We\ndo so by first establishing an incoherence-based bound on the approximation\nerror, and then noticing that, for the case of radial kernels, the bound can be\nminimized by solving the $k$-center problem. The outcome is a linear time\nconstruction of a sparse kernel mean, which also lends itself naturally to an\nautomatic sparsity selection scheme. We show the computational gains of our\nmethod by looking at three problems involving kernel means: Euclidean embedding\nof distributions, class proportion estimation, and clustering using the\nmean-shift algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 18:30:07 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Cort\u00e9s", "E. Cruz", ""], ["Scott", "C.", ""]]}, {"id": "1503.00332", "submitter": "Ardavan Saeedi", "authors": "Jonathan H. Huggins, Karthik Narasimhan, Ardavan Saeedi, Vikash K.\n  Mansinghka", "title": "JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes", "comments": "In Proceedings of the 32nd International Conference on Machine\n  Learning (ICML 2015)", "journal-ref": "JMLR: W&CP Volume 37, 2015 pp. 693-701", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov jump processes (MJPs) are used to model a wide range of phenomena from\ndisease progression to RNA path folding. However, maximum likelihood estimation\nof parametric models leads to degenerate trajectories and inferential\nperformance is poor in nonparametric models. We take a small-variance\nasymptotics (SVA) approach to overcome these limitations. We derive the\nsmall-variance asymptotics for parametric and nonparametric MJPs for both\ndirectly observed and hidden state models. In the parametric case we obtain a\nnovel objective function which leads to non-degenerate trajectories. To derive\nthe nonparametric version we introduce the gamma-gamma process, a novel\nextension to the gamma-exponential process. We propose algorithms for each of\nthese formulations, which we call \\emph{JUMP-means}. Our experiments\ndemonstrate that JUMP-means is competitive with or outperforms widely used MJP\ninference approaches in terms of both speed and reconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 18:59:12 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 23:26:53 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 16:11:10 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Narasimhan", "Karthik", ""], ["Saeedi", "Ardavan", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1503.00424", "submitter": "Qingqing Huang", "authors": "Rong Ge, Qingqing Huang, Sham M. Kakade", "title": "Learning Mixtures of Gaussians in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently learning mixture of Gaussians is a fundamental problem in\nstatistics and learning theory. Given samples coming from a random one out of k\nGaussian distributions in Rn, the learning problem asks to estimate the means\nand the covariance matrices of these Gaussians. This learning problem arises in\nmany areas ranging from the natural sciences to the social sciences, and has\nalso found many machine learning applications. Unfortunately, learning mixture\nof Gaussians is an information theoretically hard problem: in order to learn\nthe parameters up to a reasonable accuracy, the number of samples required is\nexponential in the number of Gaussian components in the worst case. In this\nwork, we show that provided we are in high enough dimensions, the class of\nGaussian mixtures is learnable in its most general form under a smoothed\nanalysis framework, where the parameters are randomly perturbed from an\nadversarial starting point. In particular, given samples from a mixture of\nGaussians with randomly perturbed parameters, when n > {\\Omega}(k^2), we give\nan algorithm that learns the parameters with polynomial running time and using\npolynomial number of samples. The central algorithmic ideas consist of new ways\nto decompose the moment tensor of the Gaussian mixture by exploiting its\nstructural properties. The symmetries of this tensor are derived from the\ncombinatorial structure of higher order moments of Gaussian distributions\n(sometimes referred to as Isserlis' theorem or Wick's theorem). We also develop\nnew tools for bounding smallest singular values of structured random matrices,\nwhich could be useful in other smoothed analysis settings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 06:59:06 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 02:59:10 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Ge", "Rong", ""], ["Huang", "Qingqing", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1503.00491", "submitter": "Fabrizio Sebastiani", "authors": "Giacomo Berardi, Andrea Esuli, Fabrizio Sebastiani", "title": "Utility-Theoretic Ranking for Semi-Automated Text Classification", "comments": "Forthcoming on ACM Transactions on Knowledge Discovery from Data", "journal-ref": null, "doi": "10.1145/2742548", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  \\emph{Semi-Automated Text Classification} (SATC) may be defined as the task\nof ranking a set $\\mathcal{D}$ of automatically labelled textual documents in\nsuch a way that, if a human annotator validates (i.e., inspects and corrects\nwhere appropriate) the documents in a top-ranked portion of $\\mathcal{D}$ with\nthe goal of increasing the overall labelling accuracy of $\\mathcal{D}$, the\nexpected increase is maximized. An obvious SATC strategy is to rank\n$\\mathcal{D}$ so that the documents that the classifier has labelled with the\nlowest confidence are top-ranked. In this work we show that this strategy is\nsuboptimal. We develop new utility-theoretic ranking methods based on the\nnotion of \\emph{validation gain}, defined as the improvement in classification\neffectiveness that would derive by validating a given automatically labelled\ndocument. We also propose a new effectiveness measure for SATC-oriented ranking\nmethods, based on the expected reduction in classification error brought about\nby partially validating a list generated by a given ranking method. We report\nthe results of experiments showing that, with respect to the baseline method\nabove, and according to the proposed measure, our utility-theoretic ranking\nmethods can achieve substantially higher expected reductions in classification\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 12:09:23 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Berardi", "Giacomo", ""], ["Esuli", "Andrea", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1503.00516", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Ho N. Phien, Hoang D. Tuan and Minh N. Do", "title": "Matrix Product State for Feature Extraction of Higher-Order Tensors", "comments": "10 pages, 3 figures, updated introduction, submitted to IEEE\n  Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces matrix product state (MPS) decomposition as a\ncomputational tool for extracting features of multidimensional data represented\nby higher-order tensors. Regardless of tensor order, MPS extracts its relevant\nfeatures to the so-called core tensor of maximum order three which can be used\nfor classification. Mainly based on a successive sequence of singular value\ndecompositions (SVD), MPS is quite simple to implement without any recursive\nprocedure needed for optimizing local tensors. Thus, it leads to substantial\ncomputational savings compared to other tensor feature extraction methods such\nas higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition\n(TD). Benchmark results show that MPS can reduce significantly the feature\nspace of data while achieving better classification performance compared to\nHOOI.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 13:20:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 22:45:24 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 21:29:47 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 22:11:39 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1503.00547", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail", "title": "Recovering PCA from Hybrid-$(\\ell_1,\\ell_2)$ Sparse Sampling of Data\n  Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how well we can recover a data matrix when only given a\nfew of its elements. We present a randomized algorithm that element-wise\nsparsifies the data, retaining only a few its elements. Our new algorithm\nindependently samples the data using sampling probabilities that depend on both\nthe squares ($\\ell_2$ sampling) and absolute values ($\\ell_1$ sampling) of the\nentries. We prove that the hybrid algorithm recovers a near-PCA reconstruction\nof the data from a sublinear sample-size: hybrid-($\\ell_1,\\ell_2$) inherits the\n$\\ell_2$-ability to sample the important elements as well as the regularization\nproperties of $\\ell_1$ sampling, and gives strictly better performance than\neither $\\ell_1$ or $\\ell_2$ on their own. We also give a one-pass version of\nour algorithm and show experiments to corroborate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 14:34:48 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kundu", "Abhisek", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1503.00587", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Uwe Aickelin, Jonathan Garibaldi, Chris Damski", "title": "Personalising Mobile Advertising Based on Users Installed Apps", "comments": "IEEE International Conference of Data Mining: The 4th International\n  Workshop on Data Mining for Service (DMS), 2014", "journal-ref": null, "doi": "10.1109/ICDMW.2014.90", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile advertising is a billion pound industry that is rapidly expanding. The\nsuccess of an advert is measured based on how users interact with it. In this\npaper we investigate whether the application of unsupervised learning and\nassociation rule mining could be used to enable personalised targeting of\nmobile adverts with the aim of increasing the interaction rate. Over May and\nJune 2014 we recorded advert interactions such as tapping the advert or\nwatching the whole advert video along with the set of apps a user has installed\nat the time of the interaction. Based on the apps that the users have installed\nwe applied k-means clustering to profile the users into one of ten classes. Due\nto the large number of apps considered we implemented dimension reduction to\nreduced the app feature space by mapping the apps to their iTunes category and\nclustered users based on the percentage of their apps that correspond to each\niTunes app category. The clustering was externally validated by investigating\ndifferences between the way the ten profiles interact with the various adverts\ngenres (lifestyle, finance and entertainment adverts). In addition association\nrule mining was performed to find whether the time of the day that the advert\nis served and the number of apps a user has installed makes certain profiles\nmore likely to interact with the advert genres. The results showed there were\nclear differences in the way the profiles interact with the different advert\ngenres and the results of this paper suggest that mobile advert targeting would\nimprove the frequency that users interact with an advert.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 17:48:34 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Reps", "Jenna", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan", ""], ["Damski", "Chris", ""]]}, {"id": "1503.00600", "submitter": "Weiran Wang", "authors": "Weiran Wang", "title": "An $\\mathcal{O}(n\\log n)$ projection operator for weighted $\\ell_1$-norm\n  regularization with sum constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple and efficient algorithm for the projection operator for\nweighted $\\ell_1$-norm regularization subject to a sum constraint, together\nwith an elementary proof. The implementation of the proposed algorithm can be\ndownloaded from the author's homepage.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 16:35:02 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Wang", "Weiran", ""]]}, {"id": "1503.00623", "submitter": "Yiming Ying", "authors": "Yiming Ying and Ding-Xuan Zhou", "title": "Unregularized Online Learning Algorithms with General Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider unregularized online learning algorithms in a\nReproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit\nconvergence rates of the unregularized online learning algorithms for\nclassification associated with a general gamma-activating loss (see Definition\n1 in the paper). Our results extend and refine the results in Ying and Pontil\n(2008) for the least-square loss and the recent result in Bach and Moulines\n(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we\nestablish a very general condition on the step sizes which guarantees the\nconvergence of the last iterate of such algorithms. Secondly, we establish, for\nthe first time, the convergence of the unregularized pairwise learning\nalgorithm with a general loss function and derive explicit rates under the\nassumption of polynomially decaying step sizes. Concrete examples are used to\nillustrate our main results. The main techniques are tools from convex\nanalysis, refined inequalities of Gaussian averages, and an induction approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:21:23 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 17:58:51 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Ying", "Yiming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1503.00687", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "A review of mean-shift algorithms for clustering", "comments": "28 pages, 9 figures. Invited book chapter to appear in the CRC\n  Handbook of Cluster Analysis (eds. Roberto Rocci, Fionn Murtagh, Marina Meila\n  and Christian Hennig)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to characterize the cluster structure of a dataset is by\nfinding regions containing a high density of data. This can be done in a\nnonparametric way with a kernel density estimate, whose modes and hence\nclusters can be found using mean-shift algorithms. We describe the theory and\npractice behind clustering based on kernel density estimates and mean-shift\nalgorithms. We discuss the blurring and non-blurring versions of mean-shift;\ntheoretical results about mean-shift algorithms and Gaussian mixtures;\nrelations with scale-space theory, spectral clustering and other algorithms;\nextensions to tracking, to manifold and graph data, and to manifold denoising;\nK-modes and Laplacian K-modes algorithms; acceleration strategies for large\ndatasets; and applications to image segmentation, manifold denoising and\nmultivalued regression.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:09:14 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1503.00693", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Noah A. Smith", "title": "Bayesian Optimization of Text Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying machine learning to problems in NLP, there are many choices to\nmake about how to represent input texts. These choices can have a big effect on\nperformance, but they are often uninteresting to researchers or practitioners\nwho simply need a module that performs well. We propose an approach to\noptimizing over this space of choices, formulating the problem as global\noptimization. We apply a sequential model-based optimization technique and show\nthat our method makes standard linear models competitive with more\nsophisticated, expensive state-of-the-art methods based on latent variable\nmodels or neural networks on various topic classification and sentiment\nanalysis problems. Our approach is a first step towards black-box NLP systems\nthat work with raw text and do not require manual tuning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:23:18 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Yogatama", "Dani", ""], ["Smith", "Noah A.", ""]]}, {"id": "1503.00759", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich", "title": "A Review of Relational Machine Learning for Knowledge Graphs", "comments": "To appear in Proceedings of the IEEE", "journal-ref": null, "doi": "10.1109/JPROC.2015.2483592", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational machine learning studies methods for the statistical analysis of\nrelational, or graph-structured, data. In this paper, we provide a review of\nhow such statistical models can be \"trained\" on large knowledge graphs, and\nthen used to predict new facts about the world (which is equivalent to\npredicting new edges in the graph). In particular, we discuss two fundamentally\ndifferent kinds of statistical relational models, both of which can scale to\nmassive datasets. The first is based on latent feature models such as tensor\nfactorization and multiway neural networks. The second is based on mining\nobservable patterns in the graph. We also show how to combine these latent and\nobservable models to get improved modeling power at decreased computational\ncost. Finally, we discuss how such statistical models of graphs can be combined\nwith text-based information extraction methods for automatically constructing\nknowledge graphs from the Web. To this end, we also discuss Google's Knowledge\nVault project as an example of such combination.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:35:41 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 16:35:31 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 17:40:35 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nickel", "Maximilian", ""], ["Murphy", "Kevin", ""], ["Tresp", "Volker", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1503.00778", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra", "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a basic task in many fields including signal processing,\nneuroscience and machine learning where the goal is to learn a basis that\nenables a sparse representation of a given set of data, if one exists. Its\nstandard formulation is as a non-convex optimization problem which is solved in\npractice by heuristics based on alternating minimization. Re- cent work has\nresulted in several algorithms for sparse coding with provable guarantees, but\nsomewhat surprisingly these are outperformed by the simple alternating\nminimization heuristics. Here we give a general framework for understanding\nalternating minimization which we leverage to analyze existing heuristics and\nto design new ones also with provable guarantees. Some of these algorithms seem\nimplementable on simple neural architectures, which was the original motivation\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\nfirst efficient algorithm for sparse coding that works almost up to the\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\nprevious algorithms that approached or surpassed this limit run in time\nexponential in some natural parameter. Finally, our algorithms improve upon the\nsample complexity of existing approaches. We believe that our analysis\nframework will have applications in other settings where simple iterative\nalgorithms are used.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 23:02:56 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1503.00841", "submitter": "Biao Liu", "authors": "Biao Liu, Minlie Huang", "title": "Robustly Leveraging Prior Knowledge in Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Prior knowledge has been shown very useful to address many natural language\nprocessing tasks. Many approaches have been proposed to formalise a variety of\nknowledge, however, whether the proposed approach is robust or sensitive to the\nknowledge supplied to the model has rarely been discussed. In this paper, we\npropose three regularization terms on top of generalized expectation criteria,\nand conduct extensive experiments to justify the robustness of the proposed\nmethods. Experimental results demonstrate that our proposed methods obtain\nremarkable improvements and are much more robust than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 06:59:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Liu", "Biao", ""], ["Huang", "Minlie", ""]]}, {"id": "1503.00900", "submitter": "Dr. Deepali Virmani", "authors": "Deepali Virmani, Shweta Taneja, Geetika Malhotra", "title": "Normalization based K means Clustering Algorithm", "comments": "5 pages, 4 figures in International Journal of Advanced Engineering\n  Research and Science (IJAERS)-Feb 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is an effective clustering technique used to separate similar data\ninto groups based on initial centroids of clusters. In this paper,\nNormalization based K-means clustering algorithm(N-K means) is proposed.\nProposed N-K means clustering algorithm applies normalization prior to\nclustering on the available data as well as the proposed approach calculates\ninitial centroids based on weights. Experimental results prove the betterment\nof proposed N-K means clustering algorithm over existing K-means clustering\nalgorithm in terms of complexity and overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 11:26:27 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Virmani", "Deepali", ""], ["Taneja", "Shweta", ""], ["Malhotra", "Geetika", ""]]}, {"id": "1503.01002", "submitter": "Weiran Wang", "authors": "Weiran Wang, Canyi Lu", "title": "Projection onto the capped simplex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple and efficient algorithm for computing the Euclidean\nprojection of a point onto the capped simplex---a simplex with an additional\nuniform bound on each coordinate---together with an elementary proof. Both the\nMATLAB and C++ implementations of the proposed algorithm can be downloaded at\nhttps://eng.ucmerced.edu/people/wwang5.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:40:17 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Wang", "Weiran", ""], ["Lu", "Canyi", ""]]}, {"id": "1503.01007", "submitter": "Armand Joulin", "authors": "Armand Joulin, Tomas Mikolov", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent achievements in machine learning, we are still very far\nfrom achieving real artificial intelligence. In this paper, we discuss the\nlimitations of standard deep learning approaches and show that some of these\nlimitations can be overcome by learning how to grow the complexity of a model\nin a structured way. Specifically, we study the simplest sequence prediction\nproblems that are beyond the scope of what is learnable with standard recurrent\nnetworks, algorithmically generated sequences which can only be learned by\nmodels which have the capacity to count and to memorize sequences. We show that\nsome basic algorithms can be learned from sequential data using a recurrent\nnetwork associated with a trainable memory.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:50:28 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 22:41:39 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 19:23:44 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 20:37:55 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1503.01057", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Hannes Nickisch", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes\n  (KISS-GP)", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new structured kernel interpolation (SKI) framework, which\ngeneralises and unifies inducing point methods for scalable Gaussian processes\n(GPs). SKI methods produce kernel approximations for fast computations through\nkernel interpolation. The SKI framework clarifies how the quality of an\ninducing point approach depends on the number of inducing (aka interpolation)\npoints, interpolation strategy, and GP covariance kernel. SKI also provides a\nmechanism to create new scalable kernel methods, through choosing different\nkernel interpolation strategies. Using SKI, with local cubic kernel\ninterpolation, we introduce KISS-GP, which is 1) more scalable than inducing\npoint alternatives, 2) naturally enables Kronecker and Toeplitz algebra for\nsubstantial additional gains in scalability, without requiring any grid data,\nand 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)\ntime and storage for GP inference. We evaluate KISS-GP for kernel matrix\napproximation, kernel learning, and natural sound modelling.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:06:17 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1503.01158", "submitter": "Andrew Emmott", "authors": "Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern and\n  Weng-Keen Wong", "title": "A Meta-Analysis of the Anomaly Detection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a thorough meta-analysis of the anomaly detection\nproblem. To accomplish this we first identify approaches to benchmarking\nanomaly detection algorithms across the literature and produce a large corpus\nof anomaly detection benchmarks that vary in their construction across several\ndimensions we deem important to real-world applications: (a) point difficulty,\n(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)\nrelevance of features. We apply a representative set of anomaly detection\nalgorithms to this corpus, yielding a very large collection of experimental\nresults. We analyze these results to understand many phenomena observed in\nprevious work. First we observe the effects of experimental design on\nexperimental results. Second, results are evaluated with two metrics, ROC Area\nUnder the Curve and Average Precision. We employ statistical hypothesis testing\nto demonstrate the value (or lack thereof) of our benchmarks. We then offer\nseveral approaches to summarizing our experimental results, drawing several\nconclusions about the impact of our methodology as well as the strengths and\nweaknesses of some algorithms. Last, we compare results against a trivial\nsolution as an alternate means of normalizing the reported performance of\nalgorithms. The intended contributions of this article are many; in addition to\nproviding a large publicly-available corpus of anomaly detection benchmarks, we\nprovide an ontology for describing anomaly detection contexts, a methodology\nfor controlling various aspects of benchmark creation, guidelines for future\nexperimental design and a discussion of the many potential pitfalls of trying\nto measure success in this field.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 23:07:37 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:26:36 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Emmott", "Andrew", ""], ["Das", "Shubhomoy", ""], ["Dietterich", "Thomas", ""], ["Fern", "Alan", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.01161", "submitter": "Been Kim", "authors": "Been Kim, Cynthia Rudin and Julie Shah", "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning\n  and Prototype Classification", "comments": "Published in Neural Information Processing Systems (NIPS) 2014,\n  Neural Information Processing Systems (NIPS) 2014", "journal-ref": "NIPS 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bayesian Case Model (BCM), a general framework for Bayesian\ncase-based reasoning (CBR) and prototype classification and clustering. BCM\nbrings the intuitive power of CBR to a Bayesian generative framework. The BCM\nlearns prototypes, the \"quintessential\" observations that best represent\nclusters in a dataset, by performing joint inference on cluster labels,\nprototypes and important features. Simultaneously, BCM pursues sparsity by\nlearning subspaces, the sets of features that play important roles in the\ncharacterization of the prototypes. The prototype and subspace representation\nprovides quantitative benefits in interpretability while preserving\nclassification accuracy. Human subject experiments verify statistically\nsignificant improvements to participants' understanding when using explanations\nproduced by BCM, compared to those given by prior art.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 23:25:55 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kim", "Been", ""], ["Rudin", "Cynthia", ""], ["Shah", "Julie", ""]]}, {"id": "1503.01183", "submitter": "Saeid Amiri", "authors": "Saeid Amiri, Bertrand Clarke, Jennifer Clarke and Hoyt A. Koepke", "title": "A General Hybrid Clustering Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we propose a clustering technique for general clustering problems\nincluding those that have non-convex clusters. For a given desired number of\nclusters $K$, we use three stages to find a clustering. The first stage uses a\nhybrid clustering technique to produce a series of clusterings of various sizes\n(randomly selected). They key steps are to find a $K$-means clustering using\n$K_\\ell$ clusters where $K_\\ell \\gg K$ and then joins these small clusters by\nusing single linkage clustering. The second stage stabilizes the result of\nstage one by reclustering via the `membership matrix' under Hamming distance to\ngenerate a dendrogram. The third stage is to cut the dendrogram to get $K^*$\nclusters where $K^* \\geq K$ and then prune back to $K$ to give a final\nclustering. A variant on our technique also gives a reasonable estimate for\n$K_T$, the true number of clusters.\n  We provide a series of arguments to justify the steps in the stages of our\nmethods and we provide numerous examples involving real and simulated data to\ncompare our technique with other related techniques.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:08:17 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 22:50:24 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Amiri", "Saeid", ""], ["Clarke", "Bertrand", ""], ["Clarke", "Jennifer", ""], ["Koepke", "Hoyt A.", ""]]}, {"id": "1503.01190", "submitter": "Michael Bloodgood", "authors": "Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr,\n  Lori Levin, Christine D. Piatko, Owen Rambow and Benjamin Van Durme", "title": "Statistical modality tagging from rule-based annotations and\n  crowdsourcing", "comments": "8 pages, 6 tables; appeared in Proceedings of the Workshop on\n  Extra-Propositional Aspects of Meaning in Computational Linguistics, July\n  2012; In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "journal-ref": "In Proceedings of the Workshop on Extra-Propositional Aspects of\n  Meaning in Computational Linguistics, pages 57-64, Jeju, Republic of Korea,\n  July 2012. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore training an automatic modality tagger. Modality is the attitude\nthat a speaker might have toward an event or state. One of the main hurdles for\ntraining a linguistic tagger is gathering training data. This is particularly\nproblematic for training a tagger for modality because modality triggers are\nsparse for the overwhelming majority of sentences. We investigate an approach\nto automatically training a modality tagger where we first gathered sentences\nbased on a high-recall simple rule-based modality tagger and then provided\nthese sentences to Mechanical Turk annotators for further annotation. We used\nthe resulting set of training data to train a precise modality tagger using a\nmulti-class SVM that delivers good performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 01:34:36 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Prabhakaran", "Vinodkumar", ""], ["Bloodgood", "Michael", ""], ["Diab", "Mona", ""], ["Dorr", "Bonnie", ""], ["Levin", "Lori", ""], ["Piatko", "Christine D.", ""], ["Rambow", "Owen", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1503.01212", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan", "title": "Hierarchies of Relaxations for Online Prediction Problems with Evolving\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online prediction where regret of the algorithm is measured against\na benchmark defined via evolving constraints. This framework captures online\nprediction on graphs, as well as other prediction problems with combinatorial\nstructure. A key aspect here is that finding the optimal benchmark predictor\n(even in hindsight, given all the data) might be computationally hard due to\nthe combinatorial nature of the constraints. Despite this, we provide\npolynomial-time \\emph{prediction} algorithms that achieve low regret against\ncombinatorial benchmark sets. We do so by building improper learning algorithms\nbased on two ideas that work together. The first is to alleviate part of the\ncomputational burden through random playout, and the second is to employ\nLasserre semidefinite hierarchies to approximate the resulting integer program.\nInterestingly, for our prediction algorithms, we only need to compute the\nvalues of the semidefinite programs and not the rounded solutions. However, the\nintegrality gap for Lasserre hierarchy \\emph{does} enter the generic regret\nbound in terms of Rademacher complexity of the benchmark set. This establishes\na trade-off between the computation time and the regret bound of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 04:05:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 00:04:05 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1503.01228", "submitter": "Kui Tang", "authors": "Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara", "title": "Bethe Learning of Conditional Random Fields via MAP Decoding", "comments": "19 pages (9 supplementary), 10 figures (3 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be formulated in terms of predicting\nstructured outputs. In frameworks such as the structured support vector machine\n(SVM-Struct) and the structured perceptron, discriminative functions are\nlearned by iteratively applying efficient maximum a posteriori (MAP) decoding.\nHowever, maximum likelihood estimation (MLE) of probabilistic models over these\nsame structured spaces requires computing partition functions, which is\ngenerally intractable. This paper presents a method for learning discrete\nexponential family models using the Bethe approximation to the MLE. Remarkably,\nthis problem also reduces to iterative (MAP) decoding. This connection emerges\nby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a\nconvex dual objective which circumvents the intractable partition function. The\nresult is a new single loop algorithm MLE-Struct, which is substantially more\nefficient than previous double-loop methods for approximate maximum likelihood\nestimation. Our algorithm outperforms existing methods in experiments involving\nimage segmentation, matching problems from vision, and a new dataset of\nuniversity roommate assignments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 05:41:29 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Tang", "Kui", ""], ["Ruozzi", "Nicholas", ""], ["Belanger", "David", ""], ["Jebara", "Tony", ""]]}, {"id": "1503.01239", "submitter": "Changsheng Li", "authors": "Changsheng Li and Xiangfeng Wang and Weishan Dong and Junchi Yan and\n  Qingshan Liu and Hongyuan Zha", "title": "Joint Active Learning with Feature Selection via CUR Matrix\n  Decomposition", "comments": "Accepted by T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised learning approach for simultaneous sample\nand feature selection, which is in contrast to existing works which mainly\ntackle these two problems separately. In fact the two tasks are often\ninterleaved with each other: noisy and high-dimensional features will bring\nadverse effect on sample selection, while informative or representative samples\nwill be beneficial to feature selection. Specifically, we propose a framework\nto jointly conduct active learning and feature selection based on the CUR\nmatrix decomposition. From the data reconstruction perspective, both the\nselected samples and features can best approximate the original dataset\nrespectively, such that the selected samples characterized by the features are\nhighly representative. In particular, our method runs in one-shot without the\nprocedure of iterative sample selection for progressive labeling. Thus, our\nmodel is especially suitable when there are few labeled samples or even in the\nabsence of supervision, which is a particular challenge for existing methods.\nAs the joint learning problem is NP-hard, the proposed formulation involves a\nconvex but non-smooth optimization problem. We solve it efficiently by an\niterative algorithm, and prove its global convergence. Experimental results on\npublicly available datasets corroborate the efficacy of our method compared\nwith the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 06:47:16 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 08:58:34 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 02:20:27 GMT"}, {"version": "v4", "created": "Sun, 9 Sep 2018 14:13:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Li", "Changsheng", ""], ["Wang", "Xiangfeng", ""], ["Dong", "Weishan", ""], ["Yan", "Junchi", ""], ["Liu", "Qingshan", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1503.01397", "submitter": "David Belanger", "authors": "Luke Vilnis and David Belanger and Daniel Sheldon and Andrew McCallum", "title": "Bethe Projections for Non-Local Inference", "comments": "minor bug fix to appendix. appeared in UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems in structured prediction are naturally solved by\naugmenting a tractable dependency structure with complex, non-local auxiliary\nobjectives. This includes the mean field family of variational inference\nalgorithms, soft- or hard-constrained inference using Lagrangian relaxation or\nlinear programming, collective graphical models, and forms of semi-supervised\nlearning such as posterior regularization. We present a method to\ndiscriminatively learn broad families of inference objectives, capturing\npowerful non-local statistics of the latent variables, while maintaining\ntractable and provably fast inference using non-Euclidean projected gradient\ndescent with a distance-generating function given by the Bethe entropy. We\ndemonstrate the performance and flexibility of our method by (1) extracting\nstructured citations from research papers by learning soft global constraints,\n(2) achieving state-of-the-art results on a widely-used handwriting recognition\ntask using a novel learned non-convex inference procedure, and (3) providing a\nfast and highly scalable algorithm for the challenging problem of inference in\na collective graphical model applied to bird migration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 07:32:25 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 18:44:53 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Vilnis", "Luke", ""], ["Belanger", "David", ""], ["Sheldon", "Daniel", ""], ["McCallum", "Andrew", ""]]}, {"id": "1503.01428", "submitter": "Nan Ding", "authors": "Nan Ding and Jia Deng and Kevin Murphy and Hartmut Neven", "title": "Probabilistic Label Relation Graphs with Ising Models", "comments": "International Conference on Computer Vision (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider classification problems in which the label space has structure. A\ncommon example is hierarchical label spaces, corresponding to the case where\none label subsumes another (e.g., animal subsumes dog). But labels can also be\nmutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To\njointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy\nand exclusion) graph was introduced in [7]. This combined a conditional random\nfield (CRF) with a deep neural network (DNN), resulting in state of the art\nresults when applied to visual object classification problems where the\ntraining labels were drawn from different levels of the ImageNet hierarchy\n(e.g., an image might be labeled with the basic level category \"dog\", rather\nthan the more specific label \"husky\"). In this paper, we extend the HEX model\nto allow for soft or probabilistic relations between labels, which is useful\nwhen there is uncertainty about the relationship between two labels (e.g., an\nantelope is \"sort of\" furry, but not to the same degree as a grizzly bear). We\ncall our new model pHEX, for probabilistic HEX. We show that the pHEX graph can\nbe converted to an Ising model, which allows us to use existing off-the-shelf\ninference methods (in contrast to the HEX method, which needed specialized\ninference algorithms). Experimental results show significant improvements in a\nnumber of large-scale visual object classification tasks, outperforming the\nprevious HEX model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 19:23:55 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 04:12:19 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 17:18:32 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Ding", "Nan", ""], ["Deng", "Jia", ""], ["Murphy", "Kevin", ""], ["Neven", "Hartmut", ""]]}, {"id": "1503.01436", "submitter": "Qinxun Bai", "authors": "Qinxun Bai, Steven Rosenberg, Zheng Wu, Stan Sclaroff", "title": "Class Probability Estimation via Differential Geometric Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of supervised learning for both binary and multiclass\nclassification from a unified geometric perspective. In particular, we propose\na geometric regularization technique to find the submanifold corresponding to a\nrobust estimator of the class probability $P(y|\\pmb{x})$. The regularization\nterm measures the volume of this submanifold, based on the intuition that\noverfitting produces rapid local oscillations and hence large volume of the\nestimator. This technique can be applied to regularize any classification\nfunction that satisfies two requirements: firstly, an estimator of the class\nprobability can be obtained; secondly, first and second derivatives of the\nclass probability estimator can be calculated. In experiments, we apply our\nregularization technique to standard loss functions for classification, our\nRBF-based implementation compares favorably to widely used regularization\nmethods for both binary and multiclass classification.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 19:51:19 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 10:50:07 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 15:16:32 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 01:54:17 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2015 03:08:39 GMT"}, {"version": "v6", "created": "Thu, 2 Jul 2015 04:52:13 GMT"}, {"version": "v7", "created": "Thu, 11 Feb 2016 05:35:31 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Bai", "Qinxun", ""], ["Rosenberg", "Steven", ""], ["Wu", "Zheng", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1503.01444", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So\n  Kweon", "title": "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and\n  Applications", "comments": "Accepted in Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Principal Component Analysis (RPCA) via rank minimization is a\npowerful tool for recovering underlying low-rank structure of clean data\ncorrupted with sparse noise/outliers. In many low-level vision problems, not\nonly it is known that the underlying structure of clean data is low-rank, but\nthe exact rank of clean data is also known. Yet, when applying conventional\nrank minimization for those problems, the objective function is formulated in a\nway that does not fully utilize a priori target rank information about the\nproblems. This observation motivates us to investigate whether there is a\nbetter alternative solution when using rank minimization. In this paper,\ninstead of minimizing the nuclear norm, we propose to minimize the partial sum\nof singular values, which implicitly encourages the target rank constraint. Our\nexperimental analyses show that, when the number of samples is deficient, our\napproach leads to a higher success rate than conventional rank minimization,\nwhile the solutions obtained by the two approaches are almost identical when\nthe number of samples is more than sufficient. We apply our approach to various\nlow-level vision problems, e.g. high dynamic range imaging, motion edge\ndetection, photometric stereo, image alignment and recovery, and show that our\nresults outperform those obtained by the conventional nuclear norm rank\nminimization method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:14:35 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:51:08 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Tai", "Yu-Wing", ""], ["Bazin", "Jean-Charles", ""], ["Kim", "Hyeongwoo", ""], ["Kweon", "In So", ""]]}, {"id": "1503.01445", "submitter": "Thomas Unterthiner", "authors": "Thomas Unterthiner, Andreas Mayr, G\\\"unter Klambauer, Sepp Hochreiter", "title": "Toxicity Prediction using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday we are exposed to various chemicals via food additives, cleaning and\ncosmetic products and medicines -- and some of them might be toxic. However\ntesting the toxicity of all existing compounds by biological experiments is\nneither financially nor logistically feasible. Therefore the government\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\nwas to assess the performance of computational methods in predicting the\ntoxicity of chemical compounds. State of the art toxicity prediction methods\nbuild upon specifically-designed chemical descriptors developed over decades.\nThough Deep Learning is new to the field and was never applied to toxicity\nprediction before, it clearly outperformed all other participating methods. In\nthis application paper we show that deep nets automatically learn features\nresembling well-established toxicophores. In total, our Deep Learning approach\nwon both of the panel-challenges (nuclear receptors and stress response) as\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:18:55 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Unterthiner", "Thomas", ""], ["Mayr", "Andreas", ""], ["Klambauer", "G\u00fcnter", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1503.01521", "submitter": "Liwen Zhang", "authors": "Liwen Zhang, Subhransu Maji, Ryota Tomioka", "title": "Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 02:57:19 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:09:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 21:42:55 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhang", "Liwen", ""], ["Maji", "Subhransu", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.01578", "submitter": "Sanghyuk Chun", "authors": "Sanghyuk Chun, Yung-Kyun Noh, Jinwoo Shin", "title": "Scalable Iterative Algorithm for Robust Subspace Clustering", "comments": "This paper has been withdrawn by the author due to an error in the\n  initialization section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering (SC) is a popular method for dimensionality reduction of\nhigh-dimensional data, where it generalizes Principal Component Analysis (PCA).\nRecently, several methods have been proposed to enhance the robustness of PCA\nand SC, while most of them are computationally very expensive, in particular,\nfor high dimensional large-scale data. In this paper, we develop much faster\niterative algorithms for SC, incorporating robustness using a {\\em non-squared}\n$\\ell_2$-norm objective. The known implementations for optimizing the objective\nwould be costly due to the alternative optimization of two separate objectives:\noptimal cluster-membership assignment and robust subspace selection, while the\nsubstitution of one process to a faster surrogate can cause failure in\nconvergence. To address the issue, we use a simplified procedure requiring\nefficient matrix-vector multiplications for subspace update instead of solving\nan expensive eigenvector problem at each iteration, in addition to release\nnested robust PCA loops. We prove that the proposed algorithm monotonically\nconverges to a local minimum with approximation guarantees, e.g., it achieves\n2-approximation for the robust PCA objective. In our experiments, the proposed\nalgorithm is shown to converge at an order of magnitude faster than known\nalgorithms optimizing the same objective, and have outperforms prior subspace\nclustering methods in accuracy and running time for MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 08:54:51 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 20:47:35 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chun", "Sanghyuk", ""], ["Noh", "Yung-Kyun", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1503.01596", "submitter": "Sungjin Ahn", "authors": "Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, Max Welling", "title": "Large-Scale Distributed Bayesian Matrix Factorization using Stochastic\n  Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having various attractive qualities such as high prediction accuracy\nand the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix\nFactorization has not been widely adopted because of the prohibitive cost of\ninference. In this paper, we propose a scalable distributed Bayesian matrix\nfactorization algorithm using stochastic gradient MCMC. Our algorithm, based on\nDistributed Stochastic Gradient Langevin Dynamics, can not only match the\nprediction accuracy of standard MCMC methods like Gibbs sampling, but at the\nsame time is as fast and simple as stochastic gradient descent. In our\nexperiments, we show that our algorithm can achieve the same level of\nprediction accuracy as Gibbs sampling an order of magnitude faster. We also\nshow that our method reduces the prediction error as fast as distributed\nstochastic gradient descent, achieving a 4.1% improvement in RMSE for the\nNetflix dataset and an 1.8% for the Yahoo music dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 10:17:16 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 02:28:41 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Ahn", "Sungjin", ""], ["Korattikara", "Anoop", ""], ["Liu", "Nathan", ""], ["Rajan", "Suju", ""], ["Welling", "Max", ""]]}, {"id": "1503.01673", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Jeff Schneider, Barnabas Poczos", "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models", "comments": "Proceedings of The 32nd International Conference on Machine Learning\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimisation (BO) is a technique used in optimising a\n$D$-dimensional function which is typically expensive to evaluate. While there\nhave been many successes for BO in low dimensions, scaling it to high\ndimensions has been notoriously difficult. Existing literature on the topic are\nunder very restrictive settings. In this paper, we identify two key challenges\nin this endeavour. We tackle these challenges by assuming an additive structure\nfor the function. This setting is substantially more expressive and contains a\nricher class of functions than previous work. We prove that, for additive\nfunctions the regret has only linear dependence on $D$ even though the function\ndepends on all $D$ dimensions. We also demonstrate several other statistical\nand computational benefits in our framework. Via synthetic examples, a\nscientific simulation and a face detection problem we demonstrate that our\nmethod outperforms naive BO on additive functions and on several examples where\nthe function is not additive.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 15:56:08 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 21:59:15 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 15:31:03 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1503.01737", "submitter": "Ping Li", "authors": "Ping Li", "title": "Min-Max Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The min-max kernel is a generalization of the popular resemblance kernel\n(which is designed for binary data). In this paper, we demonstrate, through an\nextensive classification study using kernel machines, that the min-max kernel\noften provides an effective measure of similarity for nonnegative data. As the\nmin-max kernel is nonlinear and might be difficult to be used for industrial\napplications with massive data, we show that the min-max kernel can be\nlinearized via hashing techniques. This allows practitioners to apply min-max\nkernel to large-scale applications using well matured linear algorithms such as\nlinear SVM or logistic regression.\n  The previous remarkable work on consistent weighted sampling (CWS) produces\nsamples in the form of ($i^*, t^*$) where the $i^*$ records the location (and\nin fact also the weights) information analogous to the samples produced by\nclassical minwise hashing on binary data. Because the $t^*$ is theoretically\nunbounded, it was not immediately clear how to effectively implement CWS for\nbuilding large-scale linear classifiers. In this paper, we provide a simple\nsolution by discarding $t^*$ (which we refer to as the \"0-bit\" scheme). Via an\nextensive empirical study, we show that this 0-bit scheme does not lose\nessential information. We then apply the \"0-bit\" CWS for building linear\nclassifiers to approximate min-max kernel classifiers, as extensively validated\non a wide range of publicly available classification datasets. We expect this\nwork will generate interests among data mining practitioners who would like to\nefficiently utilize the nonlinear information of non-binary and nonnegative\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 19:29:03 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.01793", "submitter": "Min  Wen", "authors": "Min Wen, Ruediger Ehlers, Ufuk Topcu", "title": "Correct-by-synthesis reinforcement learning with temporal logic\n  constraints", "comments": "8 pages, 3 figures, 2 tables, submitted to IROS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.GT cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem on the synthesis of reactive controllers that optimize\nsome a priori unknown performance criterion while interacting with an\nuncontrolled environment such that the system satisfies a given temporal logic\nspecification. We decouple the problem into two subproblems. First, we extract\na (maximally) permissive strategy for the system, which encodes multiple\n(possibly all) ways in which the system can react to the adversarial\nenvironment and satisfy the specifications. Then, we quantify the a priori\nunknown performance criterion as a (still unknown) reward function and compute\nan optimal strategy for the system within the operating envelope allowed by the\npermissive strategy by using the so-called maximin-Q learning algorithm. We\nestablish both correctness (with respect to the temporal logic specifications)\nand optimality (with respect to the a priori unknown performance criterion) of\nthis two-step technique for a fragment of temporal logic specifications. For\nspecifications beyond this fragment, correctness can still be preserved, but\nthe learned strategy may be sub-optimal. We present an algorithm to the overall\nproblem, and demonstrate its use and computational requirements on a set of\nrobot motion planning examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 21:23:45 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Wen", "Min", ""], ["Ehlers", "Ruediger", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1503.01800", "submitter": "Samira Ebrahimi Kahou", "authors": "Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar\n  Gulcehre, Vincent Michalski, Kishore Konda, S\\'ebastien Jean, Pierre\n  Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias\n  Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent,\n  Roland Memisevic, Christopher Pal, Yoshua Bengio", "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in\n  video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of the emotion recognition in the wild (EmotiW) Challenge is to\nassign one of seven emotions to short video clips extracted from Hollywood\nstyle movies. The videos depict acted-out emotions under realistic conditions\nwith a large degree of variation in attributes such as pose and illumination,\nmaking it worthwhile to explore approaches which consider combinations of\nfeatures from multiple modalities for label assignment. In this paper we\npresent our approach to learning several specialist models using deep learning\ntechniques, each focusing on one modality. Among these are a convolutional\nneural network, focusing on capturing visual information in detected faces, a\ndeep belief net focusing on the representation of the audio stream, a K-Means\nbased \"bag-of-mouths\" model, which extracts visual features around the mouth\nregion and a relational autoencoder, which addresses spatio-temporal aspects of\nvideos. We explore multiple methods for the combination of cues from these\nmodalities into one common classifier. This achieves a considerably greater\naccuracy than predictions from our strongest single-modality classifier. Our\nmethod was the winning submission in the 2013 EmotiW challenge and achieved a\ntest set accuracy of 47.67% on the 2014 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:03:26 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 00:55:02 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Kahou", "Samira Ebrahimi", ""], ["Bouthillier", "Xavier", ""], ["Lamblin", "Pascal", ""], ["Gulcehre", "Caglar", ""], ["Michalski", "Vincent", ""], ["Konda", "Kishore", ""], ["Jean", "S\u00e9bastien", ""], ["Froumenty", "Pierre", ""], ["Dauphin", "Yann", ""], ["Boulanger-Lewandowski", "Nicolas", ""], ["Ferrari", "Raul Chandias", ""], ["Mirza", "Mehdi", ""], ["Warde-Farley", "David", ""], ["Courville", "Aaron", ""], ["Vincent", "Pascal", ""], ["Memisevic", "Roland", ""], ["Pal", "Christopher", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1503.01811", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "Optimally Combining Classifiers Using Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a worst-case analysis of aggregation of classifier ensembles for\nbinary classification. The task of predicting to minimize error is formulated\nas a game played over a given set of unlabeled data (a transductive setting),\nwhere prior label information is encoded as constraints on the game. The\nminimax solution of this game identifies cases where a weighted combination of\nthe classifiers can perform significantly better than any single classifier.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:56:07 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 07:55:42 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 20:26:54 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1503.01820", "submitter": "Ninghang Hu", "authors": "Ninghang Hu, Gwenn Englebienne, Zhongyu Lou, and Ben Kr\\\"ose", "title": "Latent Hierarchical Model for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical model for human activity recognition. In\ncontrast to approaches that successively recognize actions and activities, our\napproach jointly models actions and activities in a unified framework, and\ntheir labels are simultaneously predicted. The model is embedded with a latent\nlayer that is able to capture a richer class of contextual information in both\nstate-state and observation-state pairs. Although loops are present in the\nmodel, the model has an overall linear-chain structure, where the exact\ninference is tractable. Therefore, the model is very efficient in both\ninference and learning. The parameters of the graphical model are learned with\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\nused to initialize the latent variables; therefore, no manual labeling for the\nlatent states is required. The experimental results from using two benchmark\ndatasets show that our model outperforms the state-of-the-art approach, and our\nmodel is computationally more efficient.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 00:05:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Hu", "Ninghang", ""], ["Englebienne", "Gwenn", ""], ["Lou", "Zhongyu", ""], ["Kr\u00f6se", "Ben", ""]]}, {"id": "1503.01824", "submitter": "MinYoung Kim", "authors": "Minyoung Kim, Luca Rigazio", "title": "Deep Clustered Convolutional Kernels", "comments": "draft", "journal-ref": "JMLR: Workshop and Conference Proceedings 44 (2015) 160-172", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved state of the art performance\nthanks to new training algorithms for rapid parameter estimation and new\nregularization methods to reduce overfitting. However, in practice the network\narchitecture has to be manually set by domain experts, generally by a costly\ntrial and error procedure, which often accounts for a large portion of the\nfinal system performance. We view this as a limitation and propose a novel\ntraining algorithm that automatically optimizes network architecture, by\nprogressively increasing model complexity and then eliminating model redundancy\nby selectively removing parameters at training time. For convolutional neural\nnetworks, our method relies on iterative split/merge clustering of\nconvolutional kernels interleaved by stochastic gradient descent. We present a\ntraining algorithm and experimental results on three different vision tasks,\nshowing improved performance compared to similarly sized hand-crafted\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 00:53:40 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Kim", "Minyoung", ""], ["Rigazio", "Luca", ""]]}, {"id": "1503.01838", "submitter": "Fandong Meng", "authors": "Fandong Meng and Zhengdong Lu and Mingxuan Wang and Hang Li and Wenbin\n  Jiang and Qun Liu", "title": "Encoding Source Language with Convolutional Neural Network for Machine\n  Translation", "comments": "Accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)\naugments the n-gram target language model with a heuristically chosen source\ncontext window, achieving state-of-the-art performance in SMT. In this paper,\nwe give a more systematic treatment by summarizing the relevant source\ninformation through a convolutional architecture guided by the target\ninformation. With different guiding signals during decoding, our specifically\ndesigned convolution+gating architectures can pinpoint the parts of a source\nsentence that are relevant to predicting a target word, and fuse them with the\ncontext of entire source sentence to form a unified representation. This\nrepresentation, together with target language words, are fed to a deep neural\nnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English\ntranslation tasks show that the proposed model can achieve significant\nimprovements over the previous NNJM by up to +1.08 BLEU points on average\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 03:04:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 08:28:32 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 01:34:58 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 10:07:40 GMT"}, {"version": "v5", "created": "Mon, 8 Jun 2015 09:04:14 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Wang", "Mingxuan", ""], ["Li", "Hang", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""]]}, {"id": "1503.01883", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Isabel Serra, \\'Alvaro Corral and Josep Lluis Arcos", "title": "Ranking and significance of variable-length similarity-based time series\n  motifs", "comments": "20 pages, 10 figures", "journal-ref": "Expert Systems with Applications 55: 452-460. Aug 2016", "doi": "10.1016/j.eswa.2016.02.026", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of very similar patterns in a time series, commonly called\nmotifs, has received continuous and increasing attention from diverse\nscientific communities. In particular, recent approaches for discovering\nsimilar motifs of different lengths have been proposed. In this work, we show\nthat such variable-length similarity-based motifs cannot be directly compared,\nand hence ranked, by their normalized dissimilarities. Specifically, we find\nthat length-normalized motif dissimilarities still have intrinsic dependencies\non the motif length, and that lowest dissimilarities are particularly affected\nby this dependency. Moreover, we find that such dependencies are generally\nnon-linear and change with the considered data set and dissimilarity measure.\nBased on these findings, we propose a solution to rank those motifs and measure\ntheir significance. This solution relies on a compact but accurate model of the\ndissimilarity space, using a beta distribution with three parameters that\ndepend on the motif length in a non-linear way. We believe the incomparability\nof variable-length dissimilarities could go beyond the field of time series,\nand that similar modeling strategies as the one used here could be of help in a\nmore broad context.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 09:10:34 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Serra", "Isabel", ""], ["Corral", "\u00c1lvaro", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1503.01910", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, Nadia Fawaz, Fernando Silveira", "title": "Sequential Relevance Maximization with Binary Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online settings where users can provide explicit feedback about\nthe relevance of products that are sequentially presented to them, we look at\nthe recommendation process as a problem of dynamically optimizing this\nrelevance feedback. Such an algorithm optimizes the fine tradeoff between\npresenting the products that are most likely to be relevant, and learning the\npreferences of the user so that more relevant recommendations can be made in\nthe future.\n  We assume a standard predictive model inspired by collaborative filtering, in\nwhich a user is sampled from a distribution over a set of possible types. For\nevery product category, each type has an associated relevance feedback that is\nassumed to be binary: the category is either relevant or irrelevant. Assuming\nthat the user stays for each additional recommendation opportunity with\nprobability $\\beta$ independent of the past, the problem is to find a policy\nthat maximizes the expected number of recommendations that are deemed relevant\nin a session.\n  We analyze this problem and prove key structural properties of the optimal\npolicy. Based on these properties, we first present an algorithm that strikes a\nbalance between recursion and dynamic programming to compute this policy. We\nfurther propose and analyze two heuristic policies: a `farsighted' greedy\npolicy that attains at least $1-\\beta$ factor of the optimal payoff, and a\nnaive greedy policy that attains at least $\\frac{1-\\beta}{1+\\beta}$ factor of\nthe optimal payoff in the worst case. Extensive simulations show that these\nheuristics are very close to optimal in practice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:02:41 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Kamble", "Vijay", ""], ["Fawaz", "Nadia", ""], ["Silveira", "Fernando", ""]]}, {"id": "1503.01916", "submitter": "Edward Meeds", "authors": "Edward Meeds, Robert Leenders, and Max Welling", "title": "Hamiltonian ABC", "comments": "Submission to UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a powerful and elegant framework\nfor performing inference in simulation-based models. However, due to the\ndifficulty in scaling likelihood estimates, ABC remains useful for relatively\nlow-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of\nlikelihood-free algorithms that apply recent advances in scaling Bayesian\nlearning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find\nthat a small number forward simulations can effectively approximate the ABC\ngradient, allowing Hamiltonian dynamics to efficiently traverse parameter\nspaces. We also describe a new simple yet general approach of incorporating\nrandom seeds into the state of the Markov chain, further reducing the random\nwalk behavior of HABC. We demonstrate HABC on several typical ABC problems, and\nshow that HABC samples comparably to regular Bayesian inference using true\ngradients on a high-dimensional problem from machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:16:58 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Meeds", "Edward", ""], ["Leenders", "Robert", ""], ["Welling", "Max", ""]]}, {"id": "1503.02031", "submitter": "Vivek Kulkarni", "authors": "Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams", "title": "To Drop or Not to Drop: Robustness, Consistency and Differential Privacy\n  Properties of Dropout", "comments": "Currently under review for ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep belief networks (DBNs) requires optimizing a non-convex\nfunction with an extremely large number of parameters. Naturally, existing\ngradient descent (GD) based methods are prone to arbitrarily poor local minima.\nIn this paper, we rigorously show that such local minima can be avoided (upto\nan approximation error) by using the dropout technique, a widely used heuristic\nin this domain. In particular, we show that by randomly dropping a few nodes of\na one-hidden layer neural network, the training objective function, up to a\ncertain approximation error, decreases by a multiplicative factor.\n  On the flip side, we show that for training convex empirical risk minimizers\n(ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple\ndropout based GD method for convex ERMs is stable in the face of arbitrary\nchanges to any one of the training points. Using the above assertion, we show\nthat dropout provides fast rates for generalization error in learning (convex)\ngeneralized linear models (GLM). Moreover, using the above mentioned stability\nproperties of dropout, we design dropout based differentially private\nalgorithms for solving ERMs. The learned GLM thus, preserves privacy of each of\nthe individual training points while providing accurate predictions for new\ntest points. Finally, we empirically validate our stability assertions for\ndropout in the context of convex ERMs and show that surprisingly, dropout\nsignificantly outperforms (in terms of prediction accuracy) the L2\nregularization based methods for several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 18:39:53 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Jain", "Prateek", ""], ["Kulkarni", "Vivek", ""], ["Thakurta", "Abhradeep", ""], ["Williams", "Oliver", ""]]}, {"id": "1503.02101", "submitter": "Chi Jin", "authors": "Rong Ge, Furong Huang, Chi Jin, Yang Yuan", "title": "Escaping From Saddle Points --- Online Stochastic Gradient for Tensor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic gradient descent for optimizing non-convex functions.\nIn many cases for non-convex functions the goal is to find a reasonable local\nminimum, and the main concern is that gradient updates are trapped in saddle\npoints. In this paper we identify strict saddle property for non-convex problem\nthat allows for efficient optimization. Using this property we show that\nstochastic gradient descent converges to a local minimum in a polynomial number\nof iterations. To the best of our knowledge this is the first work that gives\nglobal convergence guarantees for stochastic gradient descent on non-convex\nfunctions with exponentially many local minima and saddle points. Our analysis\ncan be applied to orthogonal tensor decomposition, which is widely used in\nlearning a rich class of latent variable models. We propose a new optimization\nformulation for the tensor decomposition problem that has strict saddle\nproperty. As a result we get the first online algorithm for orthogonal tensor\ndecomposition with global convergence guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:07:05 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Ge", "Rong", ""], ["Huang", "Furong", ""], ["Jin", "Chi", ""], ["Yuan", "Yang", ""]]}, {"id": "1503.02108", "submitter": "Zhen Huang", "authors": "Zhen Huang, Sabato Marco Siniscalchi, I-Fan Chen, Jiadong Wu, and\n  Chin-Hui Lee", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach to adapting parameters of a well-trained\ncontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to\nimprove automatic speech recognition performance. Given an abundance of DNN\nparameters but with only a limited amount of data, the effectiveness of the\nadapted DNN model can often be compromised. We formulate maximum a posteriori\n(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an\naugmented linear hidden networks connected to the output tied states, or\nsenones, and compare it to feature space MAP linear regression previously\nproposed. Experimental evidences on the 20,000-word open vocabulary Wall Street\nJournal task demonstrate the feasibility of the proposed framework. In\nsupervised adaptation, the proposed MAP adaptation approach provides more than\n10% relative error reduction and consistently outperforms the conventional\ntransformation based methods. Furthermore, we present an initial attempt to\ngenerate hierarchical priors to improve adaptation efficiency and effectiveness\nwith limited adaptation data by exploiting similarities among senones.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:48:29 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 04:53:53 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Huang", "Zhen", ""], ["Siniscalchi", "Sabato Marco", ""], ["Chen", "I-Fan", ""], ["Wu", "Jiadong", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "1503.02128", "submitter": "Qingming Tang", "authors": "Qingming Tang, Chao Yang, Jian Peng and Jinbo Xu", "title": "Exact Hybrid Covariance Thresholding for Joint Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating multiple related Gaussian\ngraphical models from a $p$-dimensional dataset consisting of different\nclasses. Our work is based upon the formulation of this problem as group\ngraphical lasso. This paper proposes a novel hybrid covariance thresholding\nalgorithm that can effectively identify zero entries in the precision matrices\nand split a large joint graphical lasso problem into small subproblems. Our\nhybrid covariance thresholding method is superior to existing uniform\nthresholding methods in that our method can split the precision matrix of each\nindividual class using different partition schemes and thus split group\ngraphical lasso into much smaller subproblems, each of which can be solved very\nfast. In addition, this paper establishes necessary and sufficient conditions\nfor our hybrid covariance thresholding algorithm. The superior performance of\nour thresholding method is thoroughly analyzed and illustrated by a few\nexperiments on simulated data and real gene expression data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:34:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 02:52:51 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Yang", "Chao", ""], ["Peng", "Jian", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02129", "submitter": "Qingming Tang", "authors": "Qingming Tang, Siqi Sun, and Jinbo Xu", "title": "Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the network structure underlying data is an important problem in\nmachine learning. This paper introduces a novel prior to study the inference of\nscale-free networks, which are widely used to model social and biological\nnetworks. The prior not only favors a desirable global node degree\ndistribution, but also takes into consideration the relative strength of all\nthe possible edges adjacent to the same node and the estimated degree of each\nindividual node.\n  To fulfill this, ranking is incorporated into the prior, which makes the\nproblem challenging to solve. We employ an ADMM (alternating direction method\nof multipliers) framework to solve the Gaussian Graphical model regularized by\nthis prior. Our experiments on both synthetic and real data show that our prior\nnot only yields a scale-free network, but also produces many more correctly\npredicted edges than the others such as the scale-free inducing prior, the\nhub-inducing prior and the $l_1$ norm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:35:26 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 04:13:31 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 03:37:44 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Sun", "Siqi", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02143", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Xingping Sun, Zongben Xu, Jinshan Zeng", "title": "Model selection of polynomial kernel regression", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial kernel regression is one of the standard and state-of-the-art\nlearning strategies. However, as is well known, the choices of the degree of\npolynomial kernel and the regularization parameter are still open in the realm\nof model selection. The first aim of this paper is to develop a strategy to\nselect these parameters. On one hand, based on the worst-case learning rate\nanalysis, we show that the regularization term in polynomial kernel regression\nis not necessary. In other words, the regularization parameter can decrease\narbitrarily fast when the degree of the polynomial kernel is suitable tuned. On\nthe other hand,taking account of the implementation of the algorithm, the\nregularization term is required. Summarily, the effect of the regularization\nterm in polynomial kernel regression is only to circumvent the \" ill-condition\"\nof the kernel matrix. Based on this, the second purpose of this paper is to\npropose a new model selection strategy, and then design an efficient learning\nalgorithm. Both theoretical and experimental analysis show that the new\nstrategy outperforms the previous one. Theoretically, we prove that the new\nlearning strategy is almost optimal if the regression function is smooth.\nExperimentally, it is shown that the new strategy can significantly reduce the\ncomputational burden without loss of generalization capability.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 08:39:15 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Lin", "Shaobo", ""], ["Sun", "Xingping", ""], ["Xu", "Zongben", ""], ["Zeng", "Jinshan", ""]]}, {"id": "1503.02144", "submitter": "Jun Fang", "authors": "Linxiao Yang, Jun Fang, Hong Cheng, and Hongbin Li", "title": "Sparse Bayesian Dictionary Learning with a Gaussian Hierarchical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dictionary learning problem whose objective is to design a\ndictionary such that the signals admits a sparse or an approximate sparse\nrepresentation over the learned dictionary. Such a problem finds a variety of\napplications such as image denoising, feature extraction, etc. In this paper,\nwe propose a new hierarchical Bayesian model for dictionary learning, in which\na Gaussian-inverse Gamma hierarchical prior is used to promote the sparsity of\nthe representation. Suitable priors are also placed on the dictionary and the\nnoise variance such that they can be reasonably inferred from the data. Based\non the hierarchical model, a variational Bayesian method and a Gibbs sampling\nmethod are developed for Bayesian inference. The proposed methods have the\nadvantage that they do not require the knowledge of the noise variance \\emph{a\npriori}. Numerical results show that the proposed methods are able to learn the\ndictionary with an accuracy better than existing methods, particularly for the\ncase where there is a limited number of training signals.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 09:03:37 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Yang", "Linxiao", ""], ["Fang", "Jun", ""], ["Cheng", "Hong", ""], ["Li", "Hongbin", ""]]}, {"id": "1503.02164", "submitter": "Shubao Zhang", "authors": "Shubao Zhang and Hui Qian and Zhihua Zhang", "title": "A Nonconvex Approach for Structured Sparse Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1409.4575", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning is an important topic in many areas such as machine learning,\nstatistical estimation, signal processing, etc. Recently, there emerges a\ngrowing interest on structured sparse learning. In this paper we focus on the\n$\\ell_q$-analysis optimization problem for structured sparse learning ($0< q\n\\leq 1$). Compared to previous work, we establish weaker conditions for exact\nrecovery in noiseless case and a tighter non-asymptotic upper bound of estimate\nerror in noisy case. We further prove that the nonconvex $\\ell_q$-analysis\noptimization can do recovery with a lower sample complexity and in a wider\nrange of cosparsity than its convex counterpart. In addition, we develop an\niteratively reweighted method to solve the optimization problem under the\nvariational framework. Theoretical analysis shows that our method is capable of\npursuing a local minima close to the global minima. Also, empirical results of\npreliminary computational experiments illustrate that our nonconvex method\noutperforms both its convex counterpart and other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 12:06:48 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Zhang", "Shubao", ""], ["Qian", "Hui", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1503.02193", "submitter": "Andrej Risteski", "authors": "Pranjal Awasthi, Moses Charikar, Kevin A. Lai, Andrej Risteski", "title": "Label optimal regret bounds for online local learning", "comments": "13 pages; Changes from previous version: small changes to proofs of\n  Theorems 1 & 2, a small rewrite of introduction as well (this version is the\n  same as camera-ready copy in COLT '15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve an open question from (Christiano, 2014b) posed in COLT'14\nregarding the optimal dependency of the regret achievable for online local\nlearning on the size of the label set. In this framework the algorithm is shown\na pair of items at each step, chosen from a set of $n$ items. The learner then\npredicts a label for each item, from a label set of size $L$ and receives a\nreal valued payoff. This is a natural framework which captures many interesting\nscenarios such as collaborative filtering, online gambling, and online max cut\namong others. (Christiano, 2014a) designed an efficient online learning\nalgorithm for this problem achieving a regret of $O(\\sqrt{nL^3T})$, where $T$\nis the number of rounds. Information theoretically, one can achieve a regret of\n$O(\\sqrt{n \\log L T})$. One of the main open questions left in this framework\nconcerns closing the above gap.\n  In this work, we provide a complete answer to the question above via two main\nresults. We show, via a tighter analysis, that the semi-definite programming\nbased algorithm of (Christiano, 2014a), in fact achieves a regret of\n$O(\\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely,\nwe show that a polynomial time algorithm for online local learning with lower\nregret would imply a polynomial time algorithm for the planted clique problem\nwhich is widely believed to be hard. We prove a similar hardness result under a\nrelated conjecture concerning planted dense subgraphs that we put forth. Unlike\nplanted clique, the planted dense subgraph problem does not have any known\nquasi-polynomial time algorithms.\n  Computational lower bounds for online learning are relatively rare, and we\nhope that the ideas developed in this work will lead to lower bounds for other\nonline learning scenarios as well.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 17:36:08 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 19:56:12 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Charikar", "Moses", ""], ["Lai", "Kevin A.", ""], ["Risteski", "Andrej", ""]]}, {"id": "1503.02216", "submitter": "Yuning Yang", "authors": "Yuning Yang, Siamak Mehrkanoon and Johan A.K. Suykens", "title": "Higher order Matching Pursuit for Low Rank Tensor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor learning, such as tensor completion and multilinear multitask\nlearning, has received much attention in recent years. In this paper, we\npropose higher order matching pursuit for low rank tensor learning problems\nwith a convex or a nonconvex cost function, which is a generalization of the\nmatching pursuit type methods. At each iteration, the main cost of the proposed\nmethods is only to compute a rank-one tensor, which can be done efficiently,\nmaking the proposed methods scalable to large scale problems. Moreover, storing\nthe resulting rank-one tensors is of low storage requirement, which can help to\nbreak the curse of dimensionality. The linear convergence rate of the proposed\nmethods is established in various circumstances. Along with the main methods,\nwe also provide a method of low computational complexity for approximately\ncomputing the rank-one tensors, with provable approximation ratio, which helps\nto improve the efficiency of the main methods and to analyze the convergence\nrate. Experimental results on synthetic as well as real datasets verify the\nefficiency and effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 21:38:07 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Yang", "Yuning", ""], ["Mehrkanoon", "Siamak", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1503.02328", "submitter": "Mike Wu", "authors": "Mike Wu", "title": "Financial Market Prediction", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given financial data from popular sites like Yahoo and the London Exchange,\nthe presented paper attempts to model and predict stocks that can be considered\n\"good investments\". Stocks are characterized by 125 features ranging from gross\ndomestic product to EDIBTA, and are labeled by discrepancies between stock and\nmarket price returns. An artificial neural network (Self-Organizing Map) is\nfitted to train on more than a million data points to predict \"good\ninvestments\" given testing stocks from 2013 and after.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 21:45:07 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Wu", "Mike", ""]]}, {"id": "1503.02346", "submitter": "Ping Li", "authors": "Ping Li", "title": "One Scan 1-Bit Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on $\\alpha$-stable random projections with small $\\alpha$, we develop a\nsimple algorithm for compressed sensing (sparse signal recovery) by utilizing\nonly the signs (i.e., 1-bit) of the measurements. Using only 1-bit information\nof the measurements results in substantial cost reduction in collection,\nstorage, communication, and decoding for compressed sensing. The proposed\nalgorithm is efficient in that the decoding procedure requires only one scan of\nthe coordinates. Our analysis can precisely show that, for a $K$-sparse signal\nof length $N$, $12.3K\\log N/\\delta$ measurements (where $\\delta$ is the\nconfidence) would be sufficient for recovering the support and the signs of the\nsignal. While the method is very robust against typical measurement noises, we\nalso provide the analysis of the scheme under random flipping of the signs of\nthe measurements.\n  \\noindent Compared to the well-known work on 1-bit marginal regression (which\ncan also be viewed as a one-scan method), the proposed algorithm requires\norders of magnitude fewer measurements. Compared to 1-bit Iterative Hard\nThresholding (IHT) (which is not a one-scan algorithm), our method is still\nsignificantly more accurate. Furthermore, the proposed method is reasonably\nrobust against random sign flipping while IHT is known to be very sensitive to\nthis type of noise.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 23:53:04 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:11:29 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.02351", "submitter": "Alexander Schwing", "authors": "Alexander G. Schwing and Raquel Urtasun", "title": "Fully Connected Deep Structured Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks with many layers have recently been shown to\nachieve excellent results on many high-level tasks such as image\nclassification, object detection and more recently also semantic segmentation.\nParticularly for semantic segmentation, a two-stage procedure is often\nemployed. Hereby, convolutional networks are trained to provide good local\npixel-wise features for the second step being traditionally a more global\ngraphical model. In this work we unify this two-stage process into a single\njoint training algorithm. We demonstrate our method on the semantic image\nsegmentation task and show encouraging results on the challenging PASCAL VOC\n2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 01:08:00 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Schwing", "Alexander G.", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1503.02357", "submitter": "Zhaopeng Tu", "authors": "Zhaopeng Tu, Baotian Hu, Zhengdong Lu, and Hang Li", "title": "Context-Dependent Translation Selection Using Convolutional Neural\n  Network", "comments": "Short version is accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for translation selection in statistical machine\ntranslation, in which a convolutional neural network is employed to judge the\nsimilarity between a phrase pair in two languages. The specifically designed\nconvolutional architecture encodes not only the semantic similarity of the\ntranslation pair, but also the context containing the phrase in the source\nlanguage. Therefore, our approach is able to capture context-dependent semantic\nsimilarities of translation pairs. We adopt a curriculum learning strategy to\ntrain the model: we classify the training examples into easy, medium, and\ndifficult categories, and gradually build the ability of representing phrase\nand sentence level context by using training examples from easy to difficult.\nExperimental results show that our approach significantly outperforms the\nbaseline system by up to 1.4 BLEU points.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:16:19 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 01:07:40 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Tu", "Zhaopeng", ""], ["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02398", "submitter": "Matthias Seibert", "authors": "Matthias Seibert, Julian W\\\"ormann, R\\'emi Gribonval, Martin\n  Kleinsteuber", "title": "Learning Co-Sparse Analysis Operators with Separable Structures", "comments": "11 pages double column, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TSP.2015.2481875", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the co-sparse analysis model a set of filters is applied to a signal out\nof the signal class of interest yielding sparse filter responses. As such, it\nmay serve as a prior in inverse problems, or for structural analysis of signals\nthat are known to belong to the signal class. The more the model is adapted to\nthe class, the more reliable it is for these purposes. The task of learning\nsuch operators for a given class is therefore a crucial problem. In many\napplications, it is also required that the filter responses are obtained in a\ntimely manner, which can be achieved by filters with a separable structure. Not\nonly can operators of this sort be efficiently used for computing the filter\nresponses, but they also have the advantage that less training samples are\nrequired to obtain a reliable estimate of the operator. The first contribution\nof this work is to give theoretical evidence for this claim by providing an\nupper bound for the sample complexity of the learning process. The second is a\nstochastic gradient descent (SGD) method designed to learn an analysis operator\nwith separable structures, which includes a novel and efficient step size\nselection rule. Numerical experiments are provided that link the sample\ncomplexity to the convergence speed of the SGD algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 08:53:33 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 10:36:58 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 13:02:02 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2015 14:08:50 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2015 19:04:19 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Seibert", "Matthias", ""], ["W\u00f6rmann", "Julian", ""], ["Gribonval", "R\u00e9mi", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1503.02406", "submitter": "Naftali Tishby", "authors": "Naftali Tishby and Noga Zaslavsky", "title": "Deep Learning and the Information Bottleneck Principle", "comments": "5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information\n  Theory Workshop (ITW) (IEEE ITW 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the\ninformation bottleneck (IB) principle. We first show that any DNN can be\nquantified by the mutual information between the layers and the input and\noutput variables. Using this representation we can calculate the optimal\ninformation theoretic limits of the DNN and obtain finite sample generalization\nbounds. The advantage of getting closer to the theoretical limit is\nquantifiable both by the generalization bound and by the network's simplicity.\nWe argue that both the optimal architecture, number of layers and\nfeatures/connections at each layer, are related to the bifurcation points of\nthe information bottleneck tradeoff, namely, relevant compression of the input\nlayer with respect to the output layer. The hierarchical representations at the\nlayered network naturally correspond to the structural phase transitions along\nthe information curve. We believe that this new insight can lead to new\noptimality bounds and deep learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 09:39:41 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Tishby", "Naftali", ""], ["Zaslavsky", "Noga", ""]]}, {"id": "1503.02417", "submitter": "Ehsan Shareghi", "authors": "Ehsan Shareghi, Gholamreza Haffari, Trevor Cohn, Ann Nicholson", "title": "Structured Prediction of Sequences and Trees using Infinite Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic structures exhibit a rich array of global phenomena, however\ncommonly used Markov models are unable to adequately describe these phenomena\ndue to their strong locality assumptions. We propose a novel hierarchical model\nfor structured prediction over sequences and trees which exploits global\ncontext by conditioning each generation decision on an unbounded context of\nprior decisions. This builds on the success of Markov models but without\nimposing a fixed bound in order to better represent global phenomena. To\nfacilitate learning of this large and unbounded model, we use a hierarchical\nPitman-Yor process prior which provides a recursive form of smoothing. We\npropose prediction algorithms based on A* and Markov Chain Monte Carlo\nsampling. Empirical results demonstrate the potential of our model compared to\nbaseline finite-context Markov models on part-of-speech tagging and syntactic\nparsing.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 10:35:10 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Shareghi", "Ehsan", ""], ["Haffari", "Gholamreza", ""], ["Cohn", "Trevor", ""], ["Nicholson", "Ann", ""]]}, {"id": "1503.02427", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang and Zhengdong Lu and Hang Li and Qun Liu", "title": "Syntax-based Deep Matching of Short Texts", "comments": "Accepted by IJCAI-2015 as full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 11:11:15 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 03:24:58 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 08:31:01 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2015 04:48:25 GMT"}, {"version": "v5", "created": "Mon, 18 May 2015 13:26:28 GMT"}, {"version": "v6", "created": "Fri, 12 Jun 2015 08:26:01 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1503.02510", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Compositional Distributional Semantics with Long Short Term Memory", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing an extension of the recursive neural network that makes use\nof a variant of the long short-term memory architecture. The extension allows\ninformation low in parse trees to be stored in a memory register (the `memory\ncell') and used much later higher up in the parse tree. This provides a\nsolution to the vanishing gradient problem and allows the network to capture\nlong range dependencies. Experimental results show that our composition\noutperformed the traditional neural-network composition on the Stanford\nSentiment Treebank.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:13:38 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 23:54:37 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1503.02531", "submitter": "Oriol Vinyals", "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean", "title": "Distilling the Knowledge in a Neural Network", "comments": "NIPS 2014 Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:44:49 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Hinton", "Geoffrey", ""], ["Vinyals", "Oriol", ""], ["Dean", "Jeff", ""]]}, {"id": "1503.02551", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami,\n  Balaji Lakshminarayanan, Dino Sejdinovic, Zolt\\'an Szab\\'o", "title": "Kernel-Based Just-In-Time Learning for Passing Expectation Propagation\n  Messages", "comments": "accepted to UAI 2015. Correct typos. Add more content to the\n  appendix. Main results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient nonparametric strategy for learning a message\noperator in expectation propagation (EP), which takes as input the set of\nincoming messages to a factor node, and produces an outgoing message as output.\nThis learned operator replaces the multivariate integral required in classical\nEP, which may not have an analytic expression. We use kernel-based regression,\nwhich is trained on a set of probability distributions representing the\nincoming messages, and the associated outgoing messages. The kernel approach\nhas two main advantages: first, it is fast, as it is implemented using a novel\ntwo-layer random feature representation of the input message distributions;\nsecond, it has principled uncertainty estimates, and can be cheaply updated\nonline, meaning it can request and incorporate new training data when it\nencounters inputs on which it is uncertain. In experiments, our approach is\nable to solve learning problems where a single message operator is required for\nmultiple, substantially different data sets (logistic regression for a variety\nof classification problems), where it is essential to accurately assess\nuncertainty and to efficiently and robustly update the message operator.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 16:30:17 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 09:17:38 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Gretton", "Arthur", ""], ["Heess", "Nicolas", ""], ["Eslami", "S. M. Ali", ""], ["Lakshminarayanan", "Balaji", ""], ["Sejdinovic", "Dino", ""], ["Szab\u00f3", "Zolt\u00e1n", ""]]}, {"id": "1503.02578", "submitter": "Mahdi Khademian", "authors": "Mahdi Khademian, Mohammad Mehdi Homayounpour", "title": "Modeling State-Conditional Observation Distribution using Weighted\n  Stereo Samples for Factorial Speech Processing Models", "comments": "Updated version of the first submission. Several clarifications are\n  added to previous version. One experiment is added to the experiments,\n  Circuits Syst Signal Process, Apr. 2016", "journal-ref": null, "doi": "10.1007/s00034-016-0310-y", "report-no": null, "categories": "cs.LG cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the effectiveness of factorial speech processing\nmodels in noise-robust automatic speech recognition tasks. For this purpose,\nthe paper proposes an idealistic approach for modeling state-conditional\nobservation distribution of factorial models based on weighted stereo samples.\nThis approach is an extension to previous single pass retraining for ideal\nmodel compensation which is extended here to support multiple audio sources.\nNon-stationary noises can be considered as one of these audio sources with\nmultiple states. Experiments of this paper over the set A of the Aurora 2\ndataset show that recognition performance can be improved by this\nconsideration. The improvement is significant in low signal to noise energy\nconditions, up to 4% absolute word recognition accuracy. In addition to the\npower of the proposed method in accurate representation of state-conditional\nobservation distribution, it has an important advantage over previous methods\nby providing the opportunity to independently select feature spaces for both\nsource and corrupted features. This opens a new window for seeking better\nfeature spaces appropriate for noisy speech, independent from clean speech\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 17:40:08 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 12:05:10 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Khademian", "Mahdi", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1503.02596", "submitter": "Daniel L. Pimentel-Alarc\\'on", "authors": "Daniel L. Pimentel-Alarc\\'on, Nigel Boston, Robert D. Nowak", "title": "A Characterization of Deterministic Sampling Patterns for Low-Rank\n  Matrix Completion", "comments": "This update corrects an error in version 2 of this paper, where we\n  erroneously assumed that columns with more than r+1 observed entries would\n  yield multiple independent constraints", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 10, no.\n  4, pp. 623-636, June, 2016", "doi": "10.1109/JSTSP.2016.2537145", "report-no": null, "categories": "stat.ML cs.LG math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix completion (LRMC) problems arise in a wide variety of\napplications. Previous theory mainly provides conditions for completion under\nmissing-at-random samplings. This paper studies deterministic conditions for\ncompletion. An incomplete $d \\times N$ matrix is finitely rank-$r$ completable\nif there are at most finitely many rank-$r$ matrices that agree with all its\nobserved entries. Finite completability is the tipping point in LRMC, as a few\nadditional samples of a finitely completable matrix guarantee its unique\ncompletability. The main contribution of this paper is a deterministic sampling\ncondition for finite completability. We use this to also derive deterministic\nsampling conditions for unique completability that can be efficiently verified.\nWe also show that under uniform random sampling schemes, these conditions are\nsatisfied with high probability if $O(\\max\\{r,\\log d\\})$ entries per column are\nobserved. These findings have several implications on LRMC regarding lower\nbounds, sample and computational complexity, the role of coherence, adaptive\nsettings and the validation of any completion algorithm. We complement our\ntheoretical results with experiments that support our findings and motivate\nfuture analysis of uncharted sampling regimes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 18:12:58 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 17:13:34 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 13:54:25 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Pimentel-Alarc\u00f3n", "Daniel L.", ""], ["Boston", "Nigel", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1503.02761", "submitter": "Ava Bargi", "authors": "Ava Bargi, Richard Yi Da Xu, Massimo Piccardi", "title": "An Adaptive Online HDP-HMM for Segmentation and Classification of\n  Sequential Data", "comments": "23 pages, 9 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, the desire and need to understand sequential data has\nbeen increasing, with particular interest in sequential contexts such as\npatient monitoring, understanding daily activities, video surveillance, stock\nmarket and the like. Along with the constant flow of data, it is critical to\nclassify and segment the observations on-the-fly, without being limited to a\nrigid number of classes. In addition, the model needs to be capable of updating\nits parameters to comply with possible evolutions. This interesting problem,\nhowever, is not adequately addressed in the literature since many studies focus\non offline classification over a pre-defined class set. In this paper, we\npropose a principled solution to this gap by introducing an adaptive online\nsystem based on Markov switching models with hierarchical Dirichlet process\npriors. This infinite adaptive online approach is capable of segmenting and\nclassifying the sequential data over unlimited number of classes, while meeting\nthe memory and delay constraints of streaming contexts. The model is further\nenhanced by introducing a learning rate, responsible for balancing the extent\nto which the model sustains its previous learning (parameters) or adapts to the\nnew streaming observations. Experimental results on several variants of\nstationary and evolving synthetic data and two video datasets, TUM Assistive\nKitchen and collatedWeizmann, show remarkable performance in segmentation and\nclassification, particularly for evolutionary sequences with changing\ndistributions and/or containing new, unseen classes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 03:27:34 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 01:36:18 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Bargi", "Ava", ""], ["Da Xu", "Richard Yi", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1503.02828", "submitter": "Mingkui Tan", "authors": "Mingkui Tan and Shijie Xiao and Junbin Gao and Dong Xu and Anton Van\n  Den Hengel and Qinfeng Shi", "title": "Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal\n  Riemannian Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear-norm regularization plays a vital role in many learning tasks, such\nas low-rank matrix recovery (MR), and low-rank representation (LRR). Solving\nthis problem directly can be computationally expensive due to the unknown rank\nof variables or large-rank singular value decompositions (SVDs). To address\nthis, we propose a proximal Riemannian gradient (PRG) scheme which can\nefficiently solve trace-norm regularized problems defined on real-algebraic\nvariety $\\mMLr$ of real matrices of rank at most $r$. Based on PRG, we further\npresent a simple and novel subspace pursuit (SP) paradigm for general\ntrace-norm regularized problems without the explicit rank constraint $\\mMLr$.\nThe proposed paradigm is very scalable by avoiding large-rank SVDs. Empirical\nstudies on several tasks, such as matrix completion and LRR based subspace\nclustering, demonstrate the superiority of the proposed paradigms over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:42:17 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 02:20:44 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tan", "Mingkui", ""], ["Xiao", "Shijie", ""], ["Gao", "Junbin", ""], ["Xu", "Dong", ""], ["Hengel", "Anton Van Den", ""], ["Shi", "Qinfeng", ""]]}, {"id": "1503.02852", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang and Wonyong Sung", "title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "comments": "Accepted by the 40th IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP) 2015", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178129", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown outstanding performance on\nprocessing sequence data. However, they suffer from long training time, which\ndemands parallel implementations of the training procedure. Parallelization of\nthe training algorithms for RNNs are very challenging because internal\nrecurrent paths form dependencies between two different time frames. In this\npaper, we first propose a generalized graph-based RNN structure that covers the\nmost popular long short-term memory (LSTM) network. Then, we present a\nparallelization approach that automatically explores parallelisms of arbitrary\nRNNs by analyzing the graph structure. The experimental results show that the\nproposed approach shows great speed-up even with a single training stream, and\nfurther accelerates the training when combined with multiple parallel training\nstreams.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 10:27:55 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1503.02946", "submitter": "Frederik Diehl", "authors": "Frederik Diehl, Andreas Jauch", "title": "apsis - Framework for Automated Optimization of Machine Learning Hyper\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apsis toolkit presented in this paper provides a flexible framework for\nhyperparameter optimization and includes both random search and a bayesian\noptimizer. It is implemented in Python and its architecture features\nadaptability to any desired machine learning code. It can easily be used with\ncommon Python ML frameworks such as scikit-learn. Published under the MIT\nLicense other researchers are heavily encouraged to check out the code,\ncontribute or raise any suggestions. The code can be found at\ngithub.com/FrederikDiehl/apsis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 15:09:25 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 15:38:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Diehl", "Frederik", ""], ["Jauch", "Andreas", ""]]}, {"id": "1503.03132", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "L_1-regularized Boltzmann machine learning using majorizer minimization", "comments": "16pages, 6 figures", "journal-ref": null, "doi": "10.7566/JPSJ.84.054801", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inference method to estimate sparse interactions and biases\naccording to Boltzmann machine learning. The basis of this method is $L_1$\nregularization, which is often used in compressed sensing, a technique for\nreconstructing sparse input signals from undersampled outputs. $L_1$\nregularization impedes the simple application of the gradient method, which\noptimizes the cost function that leads to accurate estimations, owing to the\ncost function's lack of smoothness. In this study, we utilize the majorizer\nminimization method, which is a well-known technique implemented in\noptimization problems, to avoid the non-smoothness of the cost function. By\nusing the majorizer minimization method, we elucidate essentially relevant\nbiases and interactions from given data with seemingly strongly-correlated\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 00:21:51 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1503.03148", "submitter": "Jayadeva", "authors": "Jayadeva, Sumit Soman, Amit Bhaya", "title": "A Neurodynamical System for finding a Minimal VC Dimension Classifier", "comments": "15 pages, 3 figures", "journal-ref": "Neural Networks, Volume 132, 2020, Pages 405-415", "doi": "10.1016/j.neunet.2020.08.013", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane\nclassifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)\ndimension. The VC dimension measures the capacity of a learning machine, and a\nsmaller VC dimension leads to improved generalization. On many benchmark\ndatasets, the MCM generalizes better than SVMs and uses far fewer support\nvectors than the number used by SVMs. In this paper, we describe a neural\nnetwork based on a linear dynamical system, that converges to the MCM solution.\nThe proposed MCM dynamical system is conducive to an analogue circuit\nimplementation on a chip or simulation using Ordinary Differential Equation\n(ODE) solvers. Numerical experiments on benchmark datasets from the UCI\nrepository show that the proposed approach is scalable and accurate, as we\nobtain improved accuracies and fewer number of support vectors (upto 74.3%\nreduction) with the MCM dynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 02:10:26 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jayadeva", "", ""], ["Soman", "Sumit", ""], ["Bhaya", "Amit", ""]]}, {"id": "1503.03163", "submitter": "Yanwei  Fu", "authors": "Xi Zhang, Yanwei Fu, Andi Zang, Leonid Sigal, Gady Agam", "title": "Learning Classifiers from Synthetic Data Using a Multichannel\n  Autoencoder", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for using synthetic data to help learning classifiers.\nSynthetic data, even is generated based on real data, normally results in a\nshift from the distribution of real data in feature space. To bridge the gap\nbetween the real and synthetic data, and jointly learn from synthetic and real\ndata, this paper proposes a Multichannel Autoencoder(MCAE). We show that by\nsuing MCAE, it is possible to learn a better feature representation for\nclassification. To evaluate the proposed approach, we conduct experiments on\ntwo types of datasets. Experimental results on two datasets validate the\nefficiency of our MCAE model and our methodology of generating synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 03:31:53 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Zhang", "Xi", ""], ["Fu", "Yanwei", ""], ["Zang", "Andi", ""], ["Sigal", "Leonid", ""], ["Agam", "Gady", ""]]}, {"id": "1503.03167", "submitter": "Tejas Kulkarni", "authors": "Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum", "title": "Deep Convolutional Inverse Graphics Network", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a\nmodel that learns an interpretable representation of images. This\nrepresentation is disentangled with respect to transformations such as\nout-of-plane rotations and lighting variations. The DC-IGN model is composed of\nmultiple layers of convolution and de-convolution operators and is trained\nusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a\ntraining procedure to encourage neurons in the graphics code layer to represent\na specific transformation (e.g. pose or light). Given a single input image, our\nmodel can generate new images of the same object with variations in pose and\nlighting. We present qualitative and quantitative results of the model's\nefficacy at learning a 3D rendering engine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 04:08:42 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 04:57:24 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 02:22:07 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 02:10:00 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Whitney", "Will", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1503.03238", "submitter": "Josif Grabocka", "authors": "Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme", "title": "Scalable Discovery of Time-Series Shapelets", "comments": "Under review in the journal \"Knowledge and Information Systems\"\n  (KAIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series classification is an important problem for the data mining\ncommunity due to the wide range of application domains involving time-series\ndata. A recent paradigm, called shapelets, represents patterns that are highly\npredictive for the target variable. Shapelets are discovered by measuring the\nprediction accuracy of a set of potential (shapelet) candidates. The candidates\ntypically consist of all the segments of a dataset, therefore, the discovery of\nshapelets is computationally expensive. This paper proposes a novel method that\navoids measuring the prediction accuracy of similar candidates in Euclidean\ndistance space, through an online clustering pruning technique. In addition,\nour algorithm incorporates a supervised shapelet selection that filters out\nonly those candidates that improve classification accuracy. Empirical evidence\non 45 datasets from the UCR collection demonstrate that our method is 3-4\norders of magnitudes faster than the fastest existing shapelet-discovery\nmethod, while providing better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:38:49 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grabocka", "Josif", ""], ["Wistuba", "Martin", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1503.03244", "submitter": "Baotian Hu", "authors": "Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen", "title": "Convolutional Neural Network Architectures for Matching Natural Language\n  Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic matching is of central importance to many natural language tasks\n\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to\nadequately model the internal structures of language objects and the\ninteraction between them. As a step toward this goal, we propose convolutional\nneural network models for matching two sentences, by adapting the convolutional\nstrategy in vision and speech. The proposed models not only nicely represent\nthe hierarchical structures of sentences with their layer-by-layer composition\nand pooling, but also capture the rich matching patterns at different levels.\nOur models are rather generic, requiring no prior knowledge on language, and\ncan hence be applied to matching tasks of different nature and in different\nlanguages. The empirical study on a variety of matching tasks demonstrates the\nefficacy of the proposed model on a variety of matching tasks and its\nsuperiority to competitor models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:46:36 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Hu", "Baotian", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Chen", "Qingcai", ""]]}, {"id": "1503.03355", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis", "title": "Automatic Unsupervised Tensor Mining with Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular tool for unsupervised modelling and mining multi-aspect data is\ntensor decomposition. In an exploratory setting, where and no labels or ground\ntruth are available how can we automatically decide how many components to\nextract? How can we assess the quality of our results, so that a domain expert\ncan factor this quality measure in the interpretation of our results? In this\npaper, we introduce AutoTen, a novel automatic unsupervised tensor mining\nalgorithm with minimal user intervention, which leverages and improves upon\nheuristics that assess the result quality. We extensively evaluate AutoTen's\nperformance on synthetic data, outperforming existing baselines on this very\nhard problem. Finally, we apply AutoTen on a variety of real datasets,\nproviding insights and discoveries. We view this work as a step towards a fully\nautomated, unsupervised tensor mining tool that can be easily adopted by\npractitioners in academia and industry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 14:34:46 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Papalexakis", "Evangelos E.", ""]]}, {"id": "1503.03438", "submitter": "Mark Tygert", "authors": "Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur\n  Szlam, and Mark Tygert", "title": "A mathematical motivation for complex-valued convolutional networks", "comments": "11 pages, 3 figures; this is the retitled version submitted to the\n  journal, \"Neural Computation\"", "journal-ref": "Neural Computation, 28 (5): 815-825, May 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex-valued convolutional network (convnet) implements the repeated\napplication of the following composition of three operations, recursively\napplying the composition to an input vector of nonnegative real numbers: (1)\nconvolution with complex-valued vectors followed by (2) taking the absolute\nvalue of every entry of the resulting vectors followed by (3) local averaging.\nFor processing real-valued random vectors, complex-valued convnets can be\nviewed as \"data-driven multiscale windowed power spectra,\" \"data-driven\nmultiscale windowed absolute spectra,\" \"data-driven multiwavelet absolute\nvalues,\" or (in their most general configuration) \"data-driven nonlinear\nmultiwavelet packets.\" Indeed, complex-valued convnets can calculate multiscale\nwindowed spectra when the convnet filters are windowed complex-valued\nexponentials. Standard real-valued convnets, using rectified linear units\n(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.\npooling, etc., do not obviously exhibit the same exact correspondence with\ndata-driven wavelets (whereas for complex-valued convnets, the correspondence\nis much more than just a vague analogy). Courtesy of the exact correspondence,\nthe remarkably rich and rigorous body of mathematical analysis for wavelets\napplies directly to (complex-valued) convnets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 18:24:13 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 00:42:09 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2015 19:04:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Bruna", "Joan", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""], ["Piantino", "Serkan", ""], ["Szlam", "Arthur", ""], ["Tygert", "Mark", ""]]}, {"id": "1503.03488", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "Estimating the Mean Number of K-Means Clusters to Form", "comments": "These writings are part of a longer writing which has been submitted\n  for publication. I plan to replace this writing (and the other 2 writings)\n  with the single writing that has been submitted for publication. The other\n  writings to be withdraw are 1501.07227 and 1412.4178", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing the sample size of a dataset, the random cluster model is employed\nin order to derive an estimate of the mean number of K-Means clusters to form\nduring classification of a dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 22:45:54 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 22:28:16 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1503.03506", "submitter": "Christian Wachinger", "authors": "Christian Wachinger and Polina Golland", "title": "Diverse Landmark Sampling from Determinantal Point Processes for\n  Scalable Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High computational costs of manifold learning prohibit its application for\nlarge point sets. A common strategy to overcome this problem is to perform\ndimensionality reduction on selected landmarks and to successively embed the\nentire dataset with the Nystr\\\"om method. The two main challenges that arise\nare: (i) the landmarks selected in non-Euclidean geometries must result in a\nlow reconstruction error, (ii) the graph constructed from sparsely sampled\nlandmarks must approximate the manifold well. We propose the sampling of\nlandmarks from determinantal distributions on non-Euclidean spaces. Since\ncurrent determinantal sampling algorithms have the same complexity as those for\nmanifold learning, we present an efficient approximation running in linear\ntime. Further, we recover the local geometry after the sparsification by\nassigning each landmark a local covariance matrix, estimated from the original\npoint set. The resulting neighborhood selection based on the Bhattacharyya\ndistance improves the embedding of sparsely sampled manifolds. Our experiments\nshow a significant performance improvement compared to state-of-the-art\nlandmark selection techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:09:28 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Wachinger", "Christian", ""], ["Golland", "Polina", ""]]}, {"id": "1503.03517", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Mohammad Amin Rahimian, Ali Jadbabaie", "title": "Switching to Learn", "comments": "6 pages, To appear in American Control Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network of agents attempt to learn some unknown state of the world drawn by\nnature from a finite set. Agents observe private signals conditioned on the\ntrue state, and form beliefs about the unknown state accordingly. Each agent\nmay face an identification problem in the sense that she cannot distinguish the\ntruth in isolation. However, by communicating with each other, agents are able\nto benefit from side observations to learn the truth collectively. Unlike many\ndistributed algorithms which rely on all-time communication protocols, we\npropose an efficient method by switching between Bayesian and non-Bayesian\nregimes. In this model, agents exchange information only when their private\nsignals are not informative enough; thence, by switching between the two\nregimes, agents efficiently learn the truth using only a few rounds of\ncommunications. The proposed algorithm preserves learnability while incurring a\nlower communication cost. We also verify our theoretical findings by simulation\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 22:05:50 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rahimian", "Mohammad Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1503.03562", "submitter": "Zhiyong Cheng", "authors": "Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan", "title": "Training Binary Multilayer Neural Networks for Image Classification\n  using Expectation Backpropagation", "comments": "8 pages with 1 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to Multilayer Neural Networks with real weights, Binary Multilayer\nNeural Networks (BMNNs) can be implemented more efficiently on dedicated\nhardware. BMNNs have been demonstrated to be effective on binary classification\ntasks with Expectation BackPropagation (EBP) algorithm on high dimensional text\ndatasets. In this paper, we investigate the capability of BMNNs using the EBP\nalgorithm on multiclass image classification tasks. The performances of binary\nneural networks with multiple hidden layers and different numbers of hidden\nunits are examined on MNIST. We also explore the effectiveness of image spatial\nfilters and the dropout technique in BMNNs. Experimental results on MNIST\ndataset show that EBP can obtain 2.12% test error with binary weights and 1.66%\ntest error with real weights, which is comparable to the results of standard\nBackPropagation algorithm on fully connected MNNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 02:24:31 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 01:32:15 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2015 21:47:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Cheng", "Zhiyong", ""], ["Soudry", "Daniel", ""], ["Mao", "Zexi", ""], ["Lan", "Zhenzhong", ""]]}, {"id": "1503.03578", "submitter": "Jian Tang", "authors": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei", "title": "LINE: Large-scale Information Network Embedding", "comments": "WWW 2015", "journal-ref": null, "doi": "10.1145/2736277.2741093", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper studies the problem of embedding very large information networks\ninto low-dimensional vector spaces, which is useful in many tasks such as\nvisualization, node classification, and link prediction. Most existing graph\nembedding methods do not scale for real world information networks which\nusually contain millions of nodes. In this paper, we propose a novel network\nembedding method called the \"LINE,\" which is suitable for arbitrary types of\ninformation networks: undirected, directed, and/or weighted. The method\noptimizes a carefully designed objective function that preserves both the local\nand global network structures. An edge-sampling algorithm is proposed that\naddresses the limitation of the classical stochastic gradient descent and\nimproves both the effectiveness and the efficiency of the inference. Empirical\nexperiments prove the effectiveness of the LINE on a variety of real-world\ninformation networks, including language networks, social networks, and\ncitation networks. The algorithm is very efficient, which is able to learn the\nembedding of a network with millions of vertices and billions of edges in a few\nhours on a typical single machine. The source code of the LINE is available\nonline.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 04:07:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Tang", "Jian", ""], ["Qu", "Meng", ""], ["Wang", "Mingzhe", ""], ["Zhang", "Ming", ""], ["Yan", "Jun", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1503.03585", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya\n  Ganguli", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 04:51:37 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 06:48:02 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 06:00:20 GMT"}, {"version": "v4", "created": "Wed, 13 May 2015 01:57:49 GMT"}, {"version": "v5", "created": "Wed, 20 May 2015 03:19:10 GMT"}, {"version": "v6", "created": "Thu, 9 Jul 2015 16:16:33 GMT"}, {"version": "v7", "created": "Tue, 21 Jul 2015 19:44:20 GMT"}, {"version": "v8", "created": "Wed, 18 Nov 2015 21:50:51 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Weiss", "Eric A.", ""], ["Maheswaranathan", "Niru", ""], ["Ganguli", "Surya", ""]]}, {"id": "1503.03594", "submitter": "Nika Haghtalab", "authors": "Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Ruth Urner", "title": "Efficient Learning of Linear Separators under Bounded Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the learnability of linear separators in $\\Re^d$ in the presence of\nbounded (a.k.a Massart) noise. This is a realistic generalization of the random\nclassification noise model, where the adversary can flip each example $x$ with\nprobability $\\eta(x) \\leq \\eta$. We provide the first polynomial time algorithm\nthat can learn linear separators to arbitrarily small excess error in this\nnoise model under the uniform distribution over the unit ball in $\\Re^d$, for\nsome constant value of $\\eta$. While widely studied in the statistical learning\ntheory community in the context of getting faster convergence rates,\ncomputationally efficient algorithms in this model had remained elusive. Our\nwork provides the first evidence that one can indeed design algorithms\nachieving arbitrarily small excess error in polynomial time under this\nrealistic noise model and thus opens up a new and exciting line of research.\n  We additionally provide lower bounds showing that popular algorithms such as\nhinge loss minimization and averaging cannot lead to arbitrarily small excess\nerror under Massart noise, even under the uniform distribution. Our work\ninstead, makes use of a margin based technique developed in the context of\nactive learning. As a result, our algorithm is also an active learning\nalgorithm with label complexity that is only a logarithmic the desired excess\nerror $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 05:38:19 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria-Florina", ""], ["Haghtalab", "Nika", ""], ["Urner", "Ruth", ""]]}, {"id": "1503.03613", "submitter": "Mesrob Ohannessian", "authors": "Elchanan Mossel and Mesrob I. Ohannessian", "title": "On the Impossibility of Learning the Missing Mass", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\".\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 07:27:24 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mossel", "Elchanan", ""], ["Ohannessian", "Mesrob I.", ""]]}, {"id": "1503.03701", "submitter": "Alessandro Perina", "authors": "Nebojsa Jojic and Alessandro Perina and Dongwoo Kim", "title": "Hierarchical learning of grids of microtopics", "comments": "To Appear in Uncertainty in Artificial Intelligence - UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The counting grid is a grid of microtopics, sparse word/feature\ndistributions. The generative model associated with the grid does not use these\nmicrotopics individually. Rather, it groups them in overlapping rectangular\nwindows and uses these grouped microtopics as either mixture or admixture\ncomponents. This paper builds upon the basic counting grid model and it shows\nthat hierarchical reasoning helps avoid bad local minima, produces better\nclassification accuracy and, most interestingly, allows for extraction of large\nnumbers of coherent microtopics even from small datasets. We evaluate this in\nterms of consistency, diversity and clarity of the indexed content, as well as\nin a user study on word intrusion tasks. We demonstrate that these models work\nwell as a technique for embedding raw images and discuss interesting parallels\nbetween hierarchical CG models and other deep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 12:59:25 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 16:38:24 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 16:46:07 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 15:05:38 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jojic", "Nebojsa", ""], ["Perina", "Alessandro", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1503.03712", "submitter": "Kfir Levy", "authors": "Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz", "title": "On Graduated Optimization for Stochastic Non-Convex Problems", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graduated optimization approach, also known as the continuation method,\nis a popular heuristic to solving non-convex problems that has received renewed\ninterest over the last decade. Despite its popularity, very little is known in\nterms of theoretical convergence analysis. In this paper we describe a new\nfirst-order algorithm based on graduated optimiza- tion and analyze its\nperformance. We characterize a parameterized family of non- convex functions\nfor which this algorithm provably converges to a global optimum. In particular,\nwe prove that the algorithm converges to an {\\epsilon}-approximate solution\nwithin O(1/\\epsilon^2) gradient-based steps. We extend our algorithm and\nanalysis to the setting of stochastic non-convex optimization with noisy\ngradient feedback, attaining the same convergence rate. Additionally, we\ndiscuss the setting of zero-order optimization, and devise a a variant of our\nalgorithm which converges at rate of O(d^2/\\epsilon^4).\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 13:39:28 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 05:14:22 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Hazan", "Elad", ""], ["Levy", "Kfir Y.", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1503.03893", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Sanjiv Kumar, Henry Rowley, Shih-Fu Chang", "title": "Compact Nonlinear Maps and Circulant Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel approximation via nonlinear random feature maps is widely used in\nspeeding up kernel machines. There are two main challenges for the conventional\nkernel approximation methods. First, before performing kernel approximation, a\ngood kernel has to be chosen. Picking a good kernel is a very challenging\nproblem in itself. Second, high-dimensional maps are often required in order to\nachieve good performance. This leads to high computational cost in both\ngenerating the nonlinear maps, and in the subsequent learning and prediction\nprocess. In this work, we propose to optimize the nonlinear maps directly with\nrespect to the classification objective in a data-dependent fashion. The\nproposed approach achieves kernel approximation and kernel learning in a joint\nframework. This leads to much more compact maps without hurting the\nperformance. As a by-product, the same framework can also be used to achieve\nmore compact kernel maps to approximate a known kernel. We also introduce\nCirculant Nonlinear Maps, which uses a circulant-structured projection matrix\nto speed up the nonlinear maps for high-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 21:19:13 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Yu", "Felix X.", ""], ["Kumar", "Sanjiv", ""], ["Rowley", "Henry", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1503.03903", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail", "title": "Approximating Sparse PCA from Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how well one can recover sparse principal components of a data\nmatrix using a sketch formed from a few of its elements. We show that for a\nwide class of optimization problems, if the sketch is close (in the spectral\nnorm) to the original data matrix, then one can recover a near optimal solution\nto the optimization problem by using the sketch. In particular, we use this\napproach to obtain sparse principal components and show that for \\math{m} data\npoints in \\math{n} dimensions, \\math{O(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})}\nelements gives an \\math{\\epsilon}-additive approximation to the sparse PCA\nproblem (\\math{\\tilde k} is the stable rank of the data matrix). We demonstrate\nour algorithms extensively on image, text, biological and financial data. The\nresults show that not only are we able to recover the sparse PCAs from the\nincomplete data, but by using our sparse sketch, the running time drops by a\nfactor of five or more.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 22:16:55 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Kundu", "Abhisek", ""], ["Drineas", "Petros", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1503.03964", "submitter": "Shintaro Mori", "authors": "Shunsuke Yoshida, Masato Hisakado and Shintaro Mori", "title": "Interactive Restless Multi-armed Bandit Game and Swarm Intelligence\n  Effect", "comments": "18 pages, 4 figures", "journal-ref": "New generation computing, vol.34, No. 3, 291-306, 2016", "doi": "10.1007/s00354-016-0306-y", "report-no": null, "categories": "cs.AI cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the conditions for the emergence of the swarm intelligence effect\nin an interactive game of restless multi-armed bandit (rMAB). A player competes\nwith multiple agents. Each bandit has a payoff that changes with a probability\n$p_{c}$ per round. The agents and player choose one of three options: (1)\nExploit (a good bandit), (2) Innovate (asocial learning for a good bandit among\n$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good\nbandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:\n(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability\nfor Observe in learning. The parameters $(c,p_{obs})$ are uniformly\ndistributed. We determine the optimal strategies for the player using complete\nknowledge about the rMAB. We show whether or not social or asocial learning is\nmore optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence\neffect. We conduct a laboratory experiment (67 subjects) and observe the swarm\nintelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning\nis far more optimal than asocial learning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 06:53:01 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Yoshida", "Shunsuke", ""], ["Hisakado", "Masato", ""], ["Mori", "Shintaro", ""]]}, {"id": "1503.04069", "submitter": "Klaus Greff", "authors": "Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\\'ik, Bas R.\n  Steunebrink, J\\\"urgen Schmidhuber", "title": "LSTM: A Search Space Odyssey", "comments": "12 pages, 6 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (\n  Volume: 28, Issue: 10, Oct. 2017 ) Pages: 2222 - 2232", "doi": "10.1109/TNNLS.2016.2582924", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several variants of the Long Short-Term Memory (LSTM) architecture for\nrecurrent neural networks have been proposed since its inception in 1995. In\nrecent years, these networks have become the state-of-the-art models for a\nvariety of machine learning problems. This has led to a renewed interest in\nunderstanding the role and utility of various computational components of\ntypical LSTM variants. In this paper, we present the first large-scale analysis\nof eight LSTM variants on three representative tasks: speech recognition,\nhandwriting recognition, and polyphonic music modeling. The hyperparameters of\nall LSTM variants for each task were optimized separately using random search,\nand their importance was assessed using the powerful fANOVA framework. In\ntotal, we summarize the results of 5400 experimental runs ($\\approx 15$ years\nof CPU time), which makes our study the largest of its kind on LSTM networks.\nOur results show that none of the variants can improve upon the standard LSTM\narchitecture significantly, and demonstrate the forget gate and the output\nactivation function to be its most critical components. We further observe that\nthe studied hyperparameters are virtually independent and derive guidelines for\ntheir efficient adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 14:01:38 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 11:40:31 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Greff", "Klaus", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koutn\u00edk", "Jan", ""], ["Steunebrink", "Bas R.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1503.04269", "submitter": "Richard Sutton", "authors": "Richard S. Sutton, A. Rupam Mahmood, Martha White", "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference\n  Learning", "comments": "29 pages This is a significant revision based on the first set of\n  reviews. The most important change was to signal early that the main result\n  is about stability, not convergence", "journal-ref": "Journal of Machine Learning Research 17(73): 1-29, 2016", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the idea of improving the performance of\nparametric temporal-difference (TD) learning algorithms by selectively\nemphasizing or de-emphasizing their updates on different time steps. In\nparticular, we show that varying the emphasis of linear TD($\\lambda$)'s updates\nin a particular way causes its expected update to become stable under\noff-policy training. The only prior model-free TD methods to achieve this with\nper-step computation linear in the number of function approximation parameters\nare the gradient-TD family of methods including TDC, GTD($\\lambda$), and\nGQ($\\lambda$). Compared to these methods, our _emphatic TD($\\lambda$)_ is\nsimpler and easier to use; it has only one learned parameter vector and one\nstep-size parameter. Our treatment includes general state-dependent discounting\nand bootstrapping functions, and a way of specifying varying degrees of\ninterest in accurately valuing different states.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 04:44:20 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 02:21:57 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Sutton", "Richard S.", ""], ["Mahmood", "A. Rupam", ""], ["White", "Martha", ""]]}, {"id": "1503.04337", "submitter": "Yuekai Sun", "authors": "Jason D. Lee, Yuekai Sun, Qiang Liu, Jonathan E. Taylor", "title": "Communication-efficient sparse regression: a one-shot approach", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a one-shot approach to distributed sparse regression in the\nhigh-dimensional setting. The key idea is to average \"debiased\" or\n\"desparsified\" lasso estimators. We show the approach converges at the same\nrate as the lasso as long as the dataset is not split across too many machines.\nWe also extend the approach to generalized linear models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 19:43:30 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 13:57:12 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2015 17:16:01 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Liu", "Qiang", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1503.04400", "submitter": "Jaros{\\l}aw Miszczak", "authors": "Jaros{\\l}aw Adam Miszczak", "title": "Separable and non-separable data representation for pattern\n  discrimination", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete work-flow, based on the language of quantum information\ntheory, suitable for processing data for the purpose of pattern recognition.\nThe main advantage of the introduced scheme is that it can be easily\nimplemented and applied to process real-world data using modest computation\nresources. At the same time it can be used to investigate the difference in the\npattern recognition resulting from the utilization of the tensor product\nstructure of the space of quantum states. We illustrate this difference by\nproviding a simple example based on the classification of 2D data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 09:04:22 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Miszczak", "Jaros\u0142aw Adam", ""]]}, {"id": "1503.04567", "submitter": "Hanie Sedghi", "authors": "Anima Anandkumar and Hanie Sedghi", "title": "Learning Mixed Membership Community Models in Social Tagging Networks\n  through Tensor Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in graphs has been extensively studied both in theory and\nin applications. However, detecting communities in hypergraphs is more\nchallenging. In this paper, we propose a tensor decomposition approach for\nguaranteed learning of communities in a special class of hypergraphs modeling\nsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform\nhypergraph consisting of (user, tag, resource) hyperedges. We posit a\nprobabilistic mixed membership community model, and prove that the tensor\nmethod consistently learns the communities under efficient sample complexity\nand separation requirements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 08:27:54 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 21:29:55 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Anandkumar", "Anima", ""], ["Sedghi", "Hanie", ""]]}, {"id": "1503.04596", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell and Tony Vladusich", "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional\n  Neural Network", "comments": "7 pages, 2 figures, Paper at IJCNN 2015 (International Joint\n  Conference on Neural Networks, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network architecture and training method designed to\nenable very rapid training and low implementation complexity. Due to its\ntraining speed and very few tunable parameters, the method has strong potential\nfor applications requiring frequent retraining or online training. The approach\nis characterized by (a) convolutional filters based on biologically inspired\nvisual processing filters, (b) randomly-valued classifier-stage input weights,\n(c) use of least squares regression to train the classifier output weights in a\nsingle batch, and (d) linear classifier-stage output units. We demonstrate the\nefficacy of the method by applying it to image classification. Our results\nmatch existing state-of-the-art results on the MNIST (0.37% error) and\nNORB-small (2.2% error) image classification databases, but with very fast\ntraining times compared to standard deep network approaches. The network's\nperformance on the Google Street View House Number (SVHN) (4% error) database\nis also competitive with state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:41:30 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:26:40 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2015 13:02:08 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Vladusich", "Tony", ""]]}, {"id": "1503.04843", "submitter": "Thomas Steinke", "authors": "Raef Bassily and Adam Smith and Thomas Steinke and Jonathan Ullman", "title": "More General Queries and Less Generalization Error in Adaptive Data\n  Analysis", "comments": "This paper was merged with another manuscript and is now subsumed by\n  arXiv:1511.02513", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptivity is an important feature of data analysis---typically the choice of\nquestions asked about a dataset depends on previous interactions with the same\ndataset. However, generalization error is typically bounded in a non-adaptive\nmodel, where all questions are specified before the dataset is drawn. Recent\nwork by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated the\nformal study of this problem, and gave the first upper and lower bounds on the\nachievable generalization error for adaptive data analysis.\n  Specifically, suppose there is an unknown distribution $\\mathcal{P}$ and a\nset of $n$ independent samples $x$ is drawn from $\\mathcal{P}$. We seek an\nalgorithm that, given $x$ as input, \"accurately\" answers a sequence of\nadaptively chosen \"queries\" about the unknown distribution $\\mathcal{P}$. How\nmany samples $n$ must we draw from the distribution, as a function of the type\nof queries, the number of queries, and the desired level of accuracy?\n  In this work we make two new contributions towards resolving this question:\n  *We give upper bounds on the number of samples $n$ that are needed to answer\nstatistical queries that improve over the bounds of Dwork et al.\n  *We prove the first upper bounds on the number of samples required to answer\nmore general families of queries. These include arbitrary low-sensitivity\nqueries and the important class of convex risk minimization queries.\n  As in Dwork et al., our algorithms are based on a connection between\ndifferential privacy and generalization error, but we feel that our analysis is\nsimpler and more modular, which may be useful for studying these questions in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 20:48:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 02:01:05 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""], ["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1503.04881", "submitter": "Xiaodan Zhu", "authors": "Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo", "title": "Long Short-Term Memory Over Tree Structures", "comments": "On February 6th, 2015, this work was submitted to the International\n  Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chain-structured long short-term memory (LSTM) has showed to be effective\nin a wide range of problems such as speech recognition and machine translation.\nIn this paper, we propose to extend it to tree structures, in which a memory\ncell can reflect the history memories of multiple child cells or multiple\ndescendant cells in a recursive process. We call the model S-LSTM, which\nprovides a principled way of considering long-distance interaction over\nhierarchies, e.g., language or image parse structures. We leverage the models\nfor semantic composition to understand the meaning of text, a fundamental\nproblem in natural language understanding, and show that it outperforms a\nstate-of-the-art recursive model by replacing its composition layers with the\nS-LSTM memory blocks. We also show that utilizing the given structures is\nhelpful in achieving a performance better than that without considering the\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 23:59:02 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Zhu", "Xiaodan", ""], ["Sobhani", "Parinaz", ""], ["Guo", "Hongyu", ""]]}, {"id": "1503.04964", "submitter": "Sindhu Padakandla", "authors": "Sindhu Padakandla, Prabuchandran K.J and Shalabh Bhatnagar", "title": "Energy Sharing for Multiple Sensor Nodes with Finite Buffers", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TCOMM.2015.2415777", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding optimal energy sharing policies that\nmaximize the network performance of a system comprising of multiple sensor\nnodes and a single energy harvesting (EH) source. Sensor nodes periodically\nsense the random field and generate data, which is stored in the corresponding\ndata queues. The EH source harnesses energy from ambient energy sources and the\ngenerated energy is stored in an energy buffer. Sensor nodes receive energy for\ndata transmission from the EH source. The EH source has to efficiently share\nthe stored energy among the nodes in order to minimize the long-run average\ndelay in data transmission. We formulate the problem of energy sharing between\nthe nodes in the framework of average cost infinite-horizon Markov decision\nprocesses (MDPs). We develop efficient energy sharing algorithms, namely\nQ-learning algorithm with exploration mechanisms based on the $\\epsilon$-greedy\nmethod as well as upper confidence bound (UCB). We extend these algorithms by\nincorporating state and action space aggregation to tackle state-action space\nexplosion in the MDP. We also develop a cross entropy based method that\nincorporates policy parameterization in order to find near optimal energy\nsharing policies. Through simulations, we show that our algorithms yield energy\nsharing policies that outperform the heuristic greedy method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 09:32:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Padakandla", "Sindhu", ""], ["J", "Prabuchandran K.", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1503.04996", "submitter": "Khaled Fawagreh", "authors": "Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan", "title": "On Extreme Pruning of Random Forest Ensembles for Real-time Predictive\n  Applications", "comments": "10 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest (RF) is an ensemble supervised machine learning technique that\nwas developed by Breiman over a decade ago. Compared with other ensemble\ntechniques, it has proved its accuracy and superiority. Many researchers,\nhowever, believe that there is still room for enhancing and improving its\nperformance accuracy. This explains why, over the past decade, there have been\nmany extensions of RF where each extension employed a variety of techniques and\nstrategies to improve certain aspect(s) of RF. Since it has been proven\nempiricallthat ensembles tend to yield better results when there is a\nsignificant diversity among the constituent models, the objective of this paper\nis twofold. First, it investigates how data clustering (a well known diversity\ntechnique) can be applied to identify groups of similar decision trees in an RF\nin order to eliminate redundant trees by selecting a representative from each\ngroup (cluster). Second, these likely diverse representatives are then used to\nproduce an extension of RF termed CLUB-DRF that is much smaller in size than\nRF, and yet performs at least as good as RF, and mostly exhibits higher\nperformance in terms of accuracy. The latter refers to a known technique called\nensemble pruning. Experimental results on 15 real datasets from the UCI\nrepository prove the superiority of our proposed extension over the traditional\nRF. Most of our experiments achieved at least 95% or above pruning level while\nretaining or outperforming the RF accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 11:01:37 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Fawagreh", "Khaled", ""], ["Gaber", "Mohamad Medhat", ""], ["Elyan", "Eyad", ""]]}, {"id": "1503.05018", "submitter": "Martin Wistuba", "authors": "Martin Wistuba, Josif Grabocka, Lars Schmidt-Thieme", "title": "Ultra-Fast Shapelets for Time Series Classification", "comments": "Preprint submitted to Journal of Data & Knowledge Engineering January\n  24, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series shapelets are discriminative subsequences and their similarity to\na time series can be used for time series classification. Since the discovery\nof time series shapelets is costly in terms of time, the applicability on long\nor multivariate time series is difficult. In this work we propose Ultra-Fast\nShapelets that uses a number of random shapelets. It is shown that Ultra-Fast\nShapelets yield the same prediction quality as current state-of-the-art\nshapelet-based time series classifiers that carefully select the shapelets by\nbeing by up to three orders of magnitudes. Since this method allows a\nultra-fast shapelet discovery, using shapelets for long multivariate time\nseries classification becomes feasible.\n  A method for using shapelets for multivariate time series is proposed and\nUltra-Fast Shapelets is proven to be successful in comparison to\nstate-of-the-art multivariate time series classifiers on 15 multivariate time\nseries datasets from various domains. Finally, time series derivatives that\nhave proven to be useful for other time series classifiers are investigated for\nthe shapelet-based classifiers. It is shown that they have a positive impact\nand that they are easy to integrate with a simple preprocessing step, without\nthe need of adapting the shapelet discovery algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 12:41:30 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Wistuba", "Martin", ""], ["Grabocka", "Josif", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1503.05087", "submitter": "Gergely Neu", "authors": "Gergely Neu and G\\'abor Bart\\'ok", "title": "Importance weighting without importance weights: An efficient algorithm\n  for combinatorial semi-bandits", "comments": "To appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sample-efficient alternative for importance weighting for\nsituations where one only has sample access to the probability distribution\nthat generates the observations. Our new method, called Geometric Resampling\n(GR), is described and analyzed in the context of online combinatorial\noptimization under semi-bandit feedback, where a learner sequentially selects\nits actions from a combinatorial decision set so as to minimize its cumulative\nloss. In particular, we show that the well-known Follow-the-Perturbed-Leader\n(FPL) prediction method coupled with Geometric Resampling yields the first\ncomputationally efficient reduction from offline to online optimization in this\nsetting. We provide a thorough theoretical analysis for the resulting\nalgorithm, showing that its performance is on par with previous, inefficient\nsolutions. Our main contribution is showing that, despite the relatively large\nvariance induced by the GR procedure, our performance guarantees hold with high\nprobability rather than only in expectation. As a side result, we also improve\nthe best known regret bounds for FPL in online combinatorial optimization with\nfull feedback, closing the perceived performance gap between FPL and\nexponential weights in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 15:26:15 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 22:00:25 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Neu", "Gergely", ""], ["Bart\u00f3k", "G\u00e1bor", ""]]}, {"id": "1503.05140", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Mohammad R.K. Mofrad", "title": "ProtVec: A Continuous Distributed Representation of Biological Sequences", "comments": null, "journal-ref": "PLoS ONE 10(11): e0141287, 2015", "doi": "10.1371/journal.pone.0141287", "report-no": null, "categories": "q-bio.QM cs.AI cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation and feature extraction method for\nbiological sequences. Named bio-vectors (BioVec) to refer to biological\nsequences in general with protein-vectors (ProtVec) for proteins (amino-acid\nsequences) and gene-vectors (GeneVec) for gene sequences, this representation\ncan be widely used in applications of deep learning in proteomics and genomics.\nIn the present paper, we focus on protein-vectors that can be utilized in a\nwide array of bioinformatics investigations such as family classification,\nprotein visualization, structure prediction, disordered protein identification,\nand protein-protein interaction prediction. In this method, we adopt artificial\nneural network approaches and represent a protein sequence with a single dense\nn-dimensional vector. To evaluate this method, we apply it in classification of\n324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein\nfamilies, where an average family classification accuracy of 93%+-0.06% is\nobtained, outperforming existing family classification methods. In addition, we\nuse ProtVec representation to predict disordered proteins from structured\nproteins. Two databases of disordered sequences are used: the DisProt database\nas well as a database featuring the disordered regions of nucleoporins rich\nwith phenylalanine-glycine repeats (FG-Nups). Using support vector machine\nclassifiers, FG-Nup sequences are distinguished from structured protein\nsequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and\nunstructured DisProt sequences are differentiated from structured DisProt\nsequences with 100.0% accuracy. These results indicate that by only providing\nsequence data for various proteins into this model, accurate information about\nprotein structure can be determined.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 17:55:22 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 20:17:51 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Mofrad", "Mohammad R. K.", ""]]}, {"id": "1503.05187", "submitter": "Khaled Fawagreh", "authors": "Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan", "title": "An Outlier Detection-based Tree Selection Approach to Extreme Pruning of\n  Random Forests", "comments": "21 pages, 4 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1503.04996", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest (RF) is an ensemble classification technique that was developed\nby Breiman over a decade ago. Compared with other ensemble techniques, it has\nproved its accuracy and superiority. Many researchers, however, believe that\nthere is still room for enhancing and improving its performance in terms of\npredictive accuracy. This explains why, over the past decade, there have been\nmany extensions of RF where each extension employed a variety of techniques and\nstrategies to improve certain aspect(s) of RF. Since it has been proven\nempirically that ensembles tend to yield better results when there is a\nsignificant diversity among the constituent models, the objective of this paper\nis twofolds. First, it investigates how an unsupervised learning technique,\nnamely, Local Outlier Factor (LOF) can be used to identify diverse trees in the\nRF. Second, trees with the highest LOF scores are then used to produce an\nextension of RF termed LOFB-DRF that is much smaller in size than RF, and yet\nperforms at least as good as RF, but mostly exhibits higher performance in\nterms of accuracy. The latter refers to a known technique called ensemble\npruning. Experimental results on 10 real datasets prove the superiority of our\nproposed extension over the traditional RF. Unprecedented pruning levels\nreaching 99% have been achieved at the time of boosting the predictive accuracy\nof the ensemble. The notably high pruning level makes the technique a good\ncandidate for real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 11:05:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fawagreh", "Khaled", ""], ["Gaber", "Mohamad Medhat", ""], ["Elyan", "Eyad", ""]]}, {"id": "1503.05214", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Mohamed Hefeeda", "title": "Analysis of PCA Algorithms in Distributed Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical machine learning algorithms often face scalability bottlenecks when\nthey are applied to large-scale data. Such algorithms were designed to work\nwith small data that is assumed to fit in the memory of one machine. In this\nreport, we analyze different methods for computing an important machine learing\nalgorithm, namely Principal Component Analysis (PCA), and we comment on its\nlimitations in supporting large datasets. The methods are analyzed and compared\nacross two important metrics: time complexity and communication complexity. We\nconsider the worst-case scenarios for both metrics, and we identify the\nsoftware libraries that implement each method. The analysis in this report\nhelps researchers and engineers in (i) understanding the main bottlenecks for\nscalability in different PCA algorithms, (ii) choosing the most appropriate\nmethod and software library for a given application and data set\ncharacteristics, and (iii) designing new scalable PCA algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 20:38:15 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 12:05:02 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Elgamal", "Tarek", ""], ["Hefeeda", "Mohamed", ""]]}, {"id": "1503.05296", "submitter": "Omar Al-Jarrah", "authors": "O. Y. Al-Jarrah, P. D. Yoo, S Muhaidat, G. K. Karagiannidis, and K.\n  Taha", "title": "Efficient Machine Learning for Big Data: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emerging technologies and all associated devices, it is predicted\nthat massive amount of data will be created in the next few years, in fact, as\nmuch as 90% of current data were created in the last couple of years,a trend\nthat will continue for the foreseeable future. Sustainable computing studies\nthe process by which computer engineer/scientist designs computers and\nassociated subsystems efficiently and effectively with minimal impact on the\nenvironment. However, current intelligent machine-learning systems are\nperformance driven, the focus is on the predictive/classification accuracy,\nbased on known properties learned from the training samples. For instance, most\nmachine-learning-based nonparametric models are known to require high\ncomputational cost in order to find the global optima. With the learning task\nin a large dataset, the number of hidden nodes within the network will\ntherefore increase significantly, which eventually leads to an exponential rise\nin computational complexity. This paper thus reviews the theoretical and\nexperimental data-modeling literature, in large-scale data-intensive fields,\nrelating to: (1) model efficiency, including computational requirements in\nlearning, and data-intensive areas structure and design, and introduces (2) new\nalgorithmic approaches with the least memory requirements and processing to\nminimize computational cost, while maintaining/improving its\npredictive/classification accuracy and stability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 07:56:12 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Al-Jarrah", "O. Y.", ""], ["Yoo", "P. D.", ""], ["Muhaidat", "S", ""], ["Karagiannidis", "G. K.", ""], ["Taha", "K.", ""]]}, {"id": "1503.05471", "submitter": "Danila Doroshin", "authors": "Danila Doroshin, Alexander Yamshinin, Nikolay Lubimov, Marina\n  Nastasenko, Mikhail Kotov, Maxim Tkachenko", "title": "Shared latent subspace modelling within Gaussian-Binary Restricted\n  Boltzmann Machines for NIST i-Vector Challenge 2014", "comments": "5 pages, 3 figures, submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to speaker subspace modelling based on\nGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is\nbased on the idea of shared factors as in the Probabilistic Linear Discriminant\nAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,\nherein the speaker factor is shared over all vectors of the speaker. Then\nMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.\nVarious new scoring techniques for speaker verification using GRBM are\nproposed. The results for NIST i-vector Challenge 2014 dataset are presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:28:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Doroshin", "Danila", ""], ["Yamshinin", "Alexander", ""], ["Lubimov", "Nikolay", ""], ["Nastasenko", "Marina", ""], ["Kotov", "Mikhail", ""], ["Tkachenko", "Maxim", ""]]}, {"id": "1503.05479", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, Ryota Tomioka", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the\n  Subspace Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a low-rank tensor from its noisy\nobservation. Previous work has shown a recovery guarantee with signal to noise\nratio $O(n^{\\lceil K/2 \\rceil /2})$ for recovering a $K$th order rank one\ntensor of size $n\\times \\cdots \\times n$ by recursive unfolding. In this paper,\nwe first improve this bound to $O(n^{K/4})$ by a much simpler approach, but\nwith a more careful analysis. Then we propose a new norm called the subspace\nnorm, which is based on the Kronecker products of factors obtained by the\nproposed simple estimator. The imposed Kronecker structure allows us to show a\nnearly ideal $O(\\sqrt{n}+\\sqrt{H^{K-1}})$ bound, in which the parameter $H$\ncontrols the blend from the non-convex estimator to mode-wise nuclear norm\nminimization. Furthermore, we empirically demonstrate that the subspace norm\nachieves the nearly ideal denoising performance even with $H=O(1)$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:45:04 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 01:44:23 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Zheng", "Qinqing", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.05526", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Interpretable Aircraft Engine Diagnostic via Expert Indicator\n  Aggregation", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.6214,\n  arXiv:1409.4747, arXiv:1407.0880", "journal-ref": "Transactions on Machine Learning and Data Mining, 2014, 7 (2),\n  pp.39-64", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting early signs of failures (anomalies) in complex systems is one of\nthe main goal of preventive maintenance. It allows in particular to avoid\nactual failures by (re)scheduling maintenance operations in a way that\noptimizes maintenance costs. Aircraft engine health monitoring is one\nrepresentative example of a field in which anomaly detection is crucial.\nManufacturers collect large amount of engine related data during flights which\nare used, among other applications, to detect anomalies. This article\nintroduces and studies a generic methodology that allows one to build automatic\nearly signs of anomaly detection in a way that builds upon human expertise and\nthat remains understandable by human operators who make the final maintenance\ndecision. The main idea of the method is to generate a very large number of\nbinary indicators based on parametric anomaly scores designed by experts,\ncomplemented by simple aggregations of those scores. A feature selection method\nis used to keep only the most discriminant indicators which are used as inputs\nof a Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:30:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1503.05571", "submitter": "Guillaume Alain", "authors": "Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric\n  Thibodeau-Laufer, Saizheng Zhang, Pascal Vincent", "title": "GSNs : Generative Stochastic Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.1091", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel training principle for probabilistic models that is an\nalternative to maximum likelihood. The proposed Generative Stochastic Networks\n(GSN) framework is based on learning the transition operator of a Markov chain\nwhose stationary distribution estimates the data distribution. Because the\ntransition distribution is a conditional distribution generally involving a\nsmall move, it has fewer dominant modes, being unimodal in the limit of small\nmoves. Thus, it is easier to learn, more like learning to perform supervised\nfunction approximation, with gradients that can be obtained by\nback-propagation. The theorems provided here generalize recent work on the\nprobabilistic interpretation of denoising auto-encoders and provide an\ninteresting justification for dependency networks and generalized\npseudolikelihood (along with defining an appropriate joint distribution and\nsampling mechanism, even when the conditionals are not consistent). We study\nhow GSNs can be used with missing inputs and can be used to sample subsets of\nvariables given the rest. Successful experiments are conducted, validating\nthese theoretical results, on two image datasets and with a particular\narchitecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows\ntraining to proceed with backprop, without the need for layerwise pretraining.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 20:06:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 16:44:52 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Alain", "Guillaume", ""], ["Bengio", "Yoshua", ""], ["Yao", "Li", ""], ["Yosinski", "Jason", ""], ["Thibodeau-Laufer", "Eric", ""], ["Zhang", "Saizheng", ""], ["Vincent", "Pascal", ""]]}, {"id": "1503.05615", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang, He He, Hal Daum\\'e III, John Langford", "title": "Learning to Search for Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a dependency parser can be built using a credit\nassignment compiler which removes the burden of worrying about low-level\nmachine learning details from the parser implementation. The result is a simple\nparser which robustly applies to many languages that provides similar\nstatistical and computational performance with best-to-date transition-based\nparsing approaches, while avoiding various downsides including randomization,\nextra feature requirements, and custom learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 23:33:17 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 22:12:11 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Chang", "Kai-Wei", ""], ["He", "He", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""]]}, {"id": "1503.05671", "submitter": "James Martens", "authors": "James Martens, Roger Grosse", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "comments": "Reduction ratio formula corrected. Removed incorrect claim about\n  geodesics in footnote", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method for approximating natural gradient descent in\nneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).\nK-FAC is based on an efficiently invertible approximation of a neural network's\nFisher information matrix which is neither diagonal nor low-rank, and in some\ncases is completely non-sparse. It is derived by approximating various large\nblocks of the Fisher (corresponding to entire layers) as being the Kronecker\nproduct of two much smaller matrices. While only several times more expensive\nto compute than the plain stochastic gradient, the updates produced by K-FAC\nmake much more progress optimizing the objective, which results in an algorithm\nthat can be much faster than stochastic gradient descent with momentum in\npractice. And unlike some previously proposed approximate\nnatural-gradient/Newton methods which use high-quality non-diagonal curvature\nmatrices (such as Hessian-free optimization), K-FAC works very well in highly\nstochastic optimization regimes. This is because the cost of storing and\ninverting K-FAC's approximation to the curvature matrix does not depend on the\namount of data used to estimate it, which is a feature typically associated\nonly with diagonal or low-rank approximations to the curvature matrix.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 08:30:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 20:19:14 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 05:48:59 GMT"}, {"version": "v4", "created": "Thu, 21 May 2015 00:25:06 GMT"}, {"version": "v5", "created": "Fri, 24 Jul 2015 02:30:35 GMT"}, {"version": "v6", "created": "Wed, 4 May 2016 00:29:33 GMT"}, {"version": "v7", "created": "Mon, 8 Jun 2020 01:28:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Martens", "James", ""], ["Grosse", "Roger", ""]]}, {"id": "1503.05724", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Patrick van der Smagt", "title": "A Neural Transfer Function for a Smooth and Differentiable Transition\n  Between Additive and Multiplicative Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. This leads either to an\ninefficient distribution of computational resources or an extensive increase in\nthe computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 11:48:14 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 18:01:52 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 14:45:03 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1503.05743", "submitter": "Ken Miura", "authors": "Ken Miura and Tatsuya Harada", "title": "Implementation of a Practical Distributed Calculation System with\n  Browsers and JavaScript, and Application to Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning can achieve outstanding results in various fields. However, it\nrequires so significant computational power that graphics processing units\n(GPUs) and/or numerous computers are often required for the practical\napplication. We have developed a new distributed calculation framework called\n\"Sashimi\" that allows any computer to be used as a distribution node only by\naccessing a website. We have also developed a new JavaScript neural network\nframework called \"Sukiyaki\" that uses general purpose GPUs with web browsers.\nSukiyaki performs 30 times faster than a conventional JavaScript library for\ndeep convolutional neural networks (deep CNNs) learning. The combination of\nSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the\ndistributed deep learning of deep CNNs only with web browsers on various\ndevices. The libraries that comprise the proposed methods are available under\nMIT license at http://mil-tokyo.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 12:41:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Miura", "Ken", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1503.05782", "submitter": "Sheng Huang", "authors": "Sheng Huang and Mohamed Elhoseiny and Ahmed Elgammal and Dan Yang", "title": "Learning Hypergraph-regularized Attribute Predictors", "comments": "This is an attribute learning paper accepted by CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a novel attribute learning framework named Hypergraph-based\nAttribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the\nattribute relations in the data. Then the attribute prediction problem is\ncasted as a regularized hypergraph cut problem in which HAP jointly learns a\ncollection of attribute projections from the feature space to a hypergraph\nembedding space aligned with the attribute space. The learned projections\ndirectly act as attribute classifiers (linear and kernelized). This formulation\nleads to a very efficient approach. By considering our model as a multi-graph\ncut task, our framework can flexibly incorporate other available information,\nin particular class label. We apply our approach to attribute prediction,\nZero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUB\ndatabases demonstrate the value of our methods in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 14:31:56 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Huang", "Sheng", ""], ["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Yang", "Dan", ""]]}, {"id": "1503.05831", "submitter": "Abhisek Ukil", "authors": "A. Ukil, J. Bernasconi", "title": "Neural Network-Based Active Learning in Multivariate Calibration", "comments": "9 pages in final printed version", "journal-ref": "IEEE Transactions on Systems, Man, Cybernetics-Part C, vol. 42,\n  issue 6, pp. 1763-1771, 2012", "doi": "10.1109/TSMCC.2012.2220963", "report-no": null, "categories": "cs.NE cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In chemometrics, data from infrared or near-infrared (NIR) spectroscopy are\noften used to identify a compound or to analyze the composition of amaterial.\nThis involves the calibration of models that predict the concentration\nofmaterial constituents from the measured NIR spectrum. An interesting aspect\nof multivariate calibration is to achieve a particular accuracy level with a\nminimum number of training samples, as this reduces the number of laboratory\ntests and thus the cost of model building. In these chemometric models, the\ninput refers to a proper representation of the spectra and the output to the\nconcentrations of the sample constituents. The search for a most informative\nnew calibration sample thus has to be performed in the output space of the\nmodel, rather than in the input space as in conventionalmodeling problems. In\nthis paper, we propose to solve the corresponding inversion problem by\nutilizing the disagreements of an ensemble of neural networks to represent the\nprediction error in the unexplored component space. The next calibration sample\nis then chosen at a composition where the individual models of the ensemble\ndisagree most. The results obtained for a realistic chemometric calibration\nexample show that the proposed active learning can achieve a given calibration\naccuracy with less training samples than random sampling.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 16:30:21 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ukil", "A.", ""], ["Bernasconi", "J.", ""]]}, {"id": "1503.05849", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic\n  Re-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of recording, storage and transmission of time-domain audio\nsignals, errors may be introduced that are difficult to correct in an\nunsupervised way. Here, we train a convolutional deep neural network to\nre-synthesize input time-domain speech signals at its output layer. We then use\nthis abstract transformation, which we call a deep transform (DT), to perform\nprobabilistic re-synthesis on further speech (of the same speaker) which has\nbeen degraded. Using the convolutive DT, we demonstrate the recovery of speech\naudio that has been subject to extreme degradation. This approach may be useful\nfor correction of errors in communications devices.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:24:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.05938", "submitter": "Fabio Anselmi", "authors": "Fabio Anselmi, Lorenzo Rosasco, Tomaso Poggio", "title": "On Invariance and Selectivity in Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss data representation which can be learned automatically from data,\nare invariant to transformations, and at the same time selective, in the sense\nthat two points have the same representation only if they are one the\ntransformation of the other. The mathematical results here sharpen some of the\nkey claims of i-theory -- a recent theory of feedforward processing in sensory\ncortex.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 20:30:46 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Anselmi", "Fabio", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1503.05951", "submitter": "Kai Li", "authors": "Kai Li, Guojun Qi, Jun Ye, Kien A. Hua", "title": "Rank Subspace Learning for Compact Hash Codes", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of Big Data has spawned unprecedented interests in developing hashing\nalgorithms for efficient storage and fast nearest neighbor search. Most\nexisting work learn hash functions that are numeric quantizations of feature\nvalues in projected feature space. In this work, we propose a novel hash\nlearning framework that encodes feature's rank orders instead of numeric values\nin a number of optimal low-dimensional ranking subspaces. We formulate the\nranking subspace learning problem as the optimization of a piece-wise linear\nconvex-concave function and present two versions of our algorithm: one with\nindependent optimization of each hash bit and the other exploiting a sequential\nlearning framework. Our work is a generalization of the Winner-Take-All (WTA)\nhash family and naturally enjoys all the numeric stability benefits of rank\ncorrelation measures while being optimized to achieve high precision at very\nshort code length. We compare with several state-of-the-art hashing algorithms\nin both supervised and unsupervised domain, showing superior performance in a\nnumber of data sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 21:34:33 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Li", "Kai", ""], ["Qi", "Guojun", ""], ["Ye", "Jun", ""], ["Hua", "Kien A.", ""]]}, {"id": "1503.06046", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Cocktail Party Source Separation via Probabilistic\n  Re-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cocktail party listening scenarios, the human brain is able to separate\ncompeting speech signals. However, the signal processing implemented by the\nbrain to perform cocktail party listening is not well understood. Here, we\ntrained two separate convolutive autoencoder deep neural networks (DNN) to\nseparate monaural and binaural mixtures of two concurrent speech streams. We\nthen used these DNNs as convolutive deep transform (CDT) devices to perform\nprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Our\nsimulations demonstrate that very simple neural networks are capable of\nexploiting monaural and binaural information available in a cocktail party\nlistening scenario.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 12:00:44 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.06169", "submitter": "Yaqin Zhou", "authors": "Shaojie Tang, Yaqin Zhou", "title": "Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a largely extended version of classical MAB\nproblem, called networked combinatorial bandit problems. In particular, we\nconsider the setting of a decision maker over a networked bandits as follows:\neach time a combinatorial strategy, e.g., a group of arms, is chosen, and the\ndecision maker receives a reward resulting from her strategy and also receives\na side bonus resulting from that strategy for each arm's neighbor. This is\nmotivated by many real applications such as on-line social networks where\nfriends can provide their feedback on shared content, therefore if we promote a\nproduct to a user, we can also collect feedback from her friends on that\nproduct. To this end, we consider two types of side bonus in this study: side\nobservation and side reward. Upon the number of arms pulled at each time slot,\nwe study two cases: single-play and combinatorial-play. Consequently, this\nleaves us four scenarios to investigate in the presence of side bonus:\nSingle-play with Side Observation, Combinatorial-play with Side Observation,\nSingle-play with Side Reward, and Combinatorial-play with Side Reward. For each\ncase, we present and analyze a series of \\emph{zero regret} polices where the\nexpect of regret over time approaches zero as time goes to infinity. Extensive\nsimulations validate the effectiveness of our results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 17:21:12 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Tang", "Shaojie", ""], ["Zhou", "Yaqin", ""]]}, {"id": "1503.06239", "submitter": "Jinye Zhang", "authors": "Jinye Zhang, Zhijian Ou", "title": "Block-Wise MAP Inference for Determinantal Point Processes with\n  Application to Change-Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Existing MAP inference algorithms for determinantal point processes (DPPs)\nneed to calculate determinants or conduct eigenvalue decomposition generally at\nthe scale of the full kernel, which presents a great challenge for real-world\napplications. In this paper, we introduce a class of DPPs, called BwDPPs, that\nare characterized by an almost block diagonal kernel matrix and thus can allow\nefficient block-wise MAP inference. Furthermore, BwDPPs are successfully\napplied to address the difficulty of selecting change-points in the problem of\nchange-point detection (CPD), which results in a new BwDPP-based CPD method,\nnamed BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is\nfirst created based on existing well-studied metrics. Then, these change-point\ncandidates are treated as DPP items, and DPP-based subset selection is\nconducted to give the final estimate of the change-points that favours both\nquality and diversity. The effectiveness of BwDppCpd is demonstrated through\nextensive experiments on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 22:01:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Jinye", ""], ["Ou", "Zhijian", ""]]}, {"id": "1503.06250", "submitter": "Ilya Safro", "authors": "Talayeh Razzaghi and Oleg Roderick and Ilya Safro and Nick Marko", "title": "Fast Imbalanced Classification of Healthcare Data with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical domain, data features often contain missing values. This can\ncreate serious bias in the predictive modeling. Typical standard data mining\nmethods often produce poor performance measures. In this paper, we propose a\nnew method to simultaneously classify large datasets and reduce the effects of\nmissing values. The proposed method is based on a multilevel framework of the\ncost-sensitive SVM and the expected maximization imputation method for missing\nvalues, which relies on iterated regression analyses. We compare classification\nresults of multilevel SVM-based algorithms on public benchmark datasets with\nimbalanced classes and missing values as well as real data in health\napplications, and show that our multilevel SVM-based method produces fast, and\nmore accurate and robust classification results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 00:13:54 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Roderick", "Oleg", ""], ["Safro", "Ilya", ""], ["Marko", "Nick", ""]]}, {"id": "1503.06350", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Thomas J. Fuchs and Stefano Soatto", "title": "Boosting Convolutional Features for Robust Object Proposals", "comments": "9 pages, 4 figures, 2 tables, 42 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have demonstrated excellent\nperformance in image classification, but still show room for improvement in\nobject-detection tasks with many categories, in particular for cluttered scenes\nand occlusion. Modern detection algorithms like Regions with CNNs (Girshick et\nal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions\nwhich with high probability represent objects, where in turn CNNs are deployed\nfor classification. Selective Search represents a family of sophisticated\nalgorithms that are engineered with multiple segmentation, appearance and\nsaliency cues, typically coming with a significant run-time overhead.\nFurthermore, (Hosang et al., 2014) have shown that most methods suffer from low\nreproducibility due to unstable superpixels, even for slight image\nperturbations. Although CNNs are subsequently used for classification in\ntop-performing object-detection pipelines, current proposal methods are\nagnostic to how these models parse objects and their rich learned\nrepresentations. As a result they may propose regions which may not resemble\nhigh-level objects or totally miss some of them. To overcome these drawbacks we\npropose a boosting approach which directly takes advantage of hierarchical CNN\nfeatures for detecting regions of interest fast. We demonstrate its performance\non ImageNet 2013 detection benchmark and compare it with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 20:54:39 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Fuchs", "Thomas J.", ""], ["Soatto", "Stefano", ""]]}, {"id": "1503.06379", "submitter": "Abhisek Kundu", "authors": "Abhisek Kundu", "title": "Relaxed Leverage Sampling for Low-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exact recovery of any $m\\times n$ matrix of rank\n$\\varrho$ from a small number of observed entries via the standard nuclear norm\nminimization framework. Such low-rank matrices have degrees of freedom\n$(m+n)\\varrho - \\varrho^2$. We show that any arbitrary low-rank matrices can be\nrecovered exactly from a $\\Theta\\left(((m+n)\\varrho -\n\\varrho^2)\\log^2(m+n)\\right)$ randomly sampled entries, thus matching the lower\nbound on the required number of entries (in terms of degrees of freedom), with\nan additional factor of $O(\\log^2(m+n))$. To achieve this bound on sample size\nwe observe each entry with probabilities proportional to the sum of\ncorresponding row and column leverage scores, minus their product. We show that\nthis relaxation in sampling probabilities (as opposed to sum of leverage scores\nin Chen et al, 2014) can give us an $O(\\varrho^2\\log^2(m+n))$ additive\nimprovement on the (best known) sample size obtained by Chen et al, 2014, for\nthe nuclear norm minimization. Experiments on real data corroborate the\ntheoretical improvement on sample size. Further, exact recovery of $(a)$\nincoherent matrices (with restricted leverage scores), and $(b)$ matrices with\nonly one of the row or column spaces to be incoherent, can be performed using\nour relaxed leverage score sampling, via nuclear norm minimization, without\nknowing the leverage scores a priori. In such settings also we can achieve\nimprovement on sample size.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 03:27:15 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 16:20:52 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 08:13:52 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Kundu", "Abhisek", ""]]}, {"id": "1503.06384", "submitter": "Matthias Boehm", "authors": "Matthias Boehm", "title": "Costing Generated Runtime Execution Plans for Large-Scale Machine\n  Learning Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative large-scale machine learning (ML) aims at the specification of ML\nalgorithms in a high-level language and automatic generation of hybrid runtime\nexecution plans ranging from single node, in-memory computations to distributed\ncomputations on MapReduce (MR) or similar frameworks like Spark. The\ncompilation of large-scale ML programs exhibits many opportunities for\nautomatic optimization. Advanced cost-based optimization techniques\nrequire---as a fundamental precondition---an accurate cost model for evaluating\nthe impact of optimization decisions. In this paper, we share insights into a\nsimple and robust yet accurate technique for costing alternative runtime\nexecution plans of ML programs. Our cost model relies on generating and costing\nruntime plans in order to automatically reflect all successive optimization\nphases. Costing runtime plans also captures control flow structures such as\nloops and branches, and a variety of cost factors like IO, latency, and\ncomputation costs. Finally, we linearize all these cost factors into a single\nmeasure of expected execution time. Within SystemML, this cost model is\nleveraged by several advanced optimizers like resource optimization and global\ndata flow optimization. We share our lessons learned in order to provide\nfoundations for the optimization of ML programs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 05:00:08 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Boehm", "Matthias", ""]]}, {"id": "1503.06394", "submitter": "Insu Han", "authors": "Insu Han, Dmitry Malioutov, Jinwoo Shin", "title": "Large-scale Log-determinant Computation through Stochastic Chebyshev\n  Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Logarithms of determinants of large positive definite matrices appear\nubiquitously in machine learning applications including Gaussian graphical and\nGaussian process models, partition functions of discrete graphical models,\nminimum-volume ellipsoids, metric learning and kernel learning. Log-determinant\ncomputation involves the Cholesky decomposition at the cost cubic in the number\nof variables, i.e., the matrix dimension, which makes it prohibitive for\nlarge-scale applications. We propose a linear-time randomized algorithm to\napproximate log-determinants for very large-scale positive definite and general\nnon-singular matrices using a stochastic trace approximation, called the\nHutchinson method, coupled with Chebyshev polynomial expansions that both rely\non efficient matrix-vector multiplications. We establish rigorous additive and\nmultiplicative approximation error bounds depending on the condition number of\nthe input matrix. In our experiments, the proposed algorithm can provide very\nhigh accuracy solutions at orders of magnitude faster time than the Cholesky\ndecomposition and Schur completion, and enables us to compute log-determinants\nof matrices involving tens of millions of variables.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 06:55:12 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Han", "Insu", ""], ["Malioutov", "Dmitry", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06429", "submitter": "Conrado Miranda", "authors": "Conrado S. Miranda and Fernando J. Von Zuben", "title": "Asymmetric Distributions from Constrained Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces constrained mixtures for continuous distributions,\ncharacterized by a mixture of distributions where each distribution has a shape\nsimilar to the base distribution and disjoint domains. This new concept is used\nto create generalized asymmetric versions of the Laplace and normal\ndistributions, which are shown to define exponential families, with known\nconjugate priors, and to have maximum likelihood estimates for the original\nparameters, with known closed-form expressions. The asymmetric and symmetric\nnormal distributions are compared in a linear regression example, showing that\nthe asymmetric version performs at least as well as the symmetric one, and in a\nreal world time-series problem, where a hidden Markov model is used to fit a\nstock index, indicating that the asymmetric version provides higher likelihood\nand may learn distribution models over states and transition distributions with\nconsiderably less entropy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 13:55:10 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Miranda", "Conrado S.", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1503.06452", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Unsupervised model compression for multilayer bootstrap networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multilayer bootstrap network (MBN) has demonstrated promising\nperformance in unsupervised dimensionality reduction. It can learn compact\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\npropose an unsupervised model compression framework for this general problem of\nunsupervised bootstrap methods. The framework compresses a large unsupervised\nbootstrap model into a small model by taking the bootstrap model and its\napplication together as a black box and learning a mapping function from the\ninput of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\nneural network (DNN) as the supervised learner. Our initial result on MNIST\nshowed that compressive MBN not only maintains the high prediction accuracy of\nMBN but also is over thousands of times faster than MBN at the prediction\nstage. Our result suggests that the new technique integrates the effectiveness\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\nsupervised learning together for the effectiveness and efficiency of\ncompressive MBN on unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 18:22:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1503.06468", "submitter": "Mete Ozay", "authors": "Mete Ozay, Inaki Esnaola, Fatos T. Yarman Vural, Sanjeev R. Kulkarni,\n  H. Vincent Poor", "title": "Machine Learning Methods for Attack Detection in the Smart Grid", "comments": "14 pages, 11 Figures", "journal-ref": "A version of the manuscript was published in IEEE Transactions on\n  Neural Networks and Learning Systems, 19 March 2015", "doi": "10.1109/TNNLS.2015.2404803", "report-no": null, "categories": "cs.LG cs.CR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attack detection problems in the smart grid are posed as statistical learning\nproblems for different attack scenarios in which the measurements are observed\nin batch or online settings. In this approach, machine learning algorithms are\nused to classify measurements as being either secure or attacked. An attack\ndetection framework is provided to exploit any available prior knowledge about\nthe system and surmount constraints arising from the sparse structure of the\nproblem in the proposed approach. Well-known batch and online learning\nalgorithms (supervised and semi-supervised) are employed with decision and\nfeature level fusion to model the attack detection problem. The relationships\nbetween statistical and geometric properties of attack vectors employed in the\nattack scenarios and learning algorithms are analyzed to detect unobservable\nattacks using statistical learning methods. The proposed algorithms are\nexamined on various IEEE test systems. Experimental analyses show that machine\nlearning algorithms can detect attacks with performances higher than the attack\ndetection algorithms which employ state vector estimation methods in the\nproposed attack detection framework.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 19:38:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Ozay", "Mete", ""], ["Esnaola", "Inaki", ""], ["Vural", "Fatos T. Yarman", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1503.06483", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby,\n  Simon Y. Berkovich", "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation\n  for Searching Applications", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060313", "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching through a large volume of data is very critical for companies,\nscientists, and searching engines applications due to time complexity and\nmemory complexity. In this paper, a new technique of generating FuzzyFind\nDictionary for text mining was introduced. We simply mapped the 23 bits of the\nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more\nFuzzyFind Dictionary, and reflecting the presence or absence of particular\nletters. This representation preserves closeness of word distortions in terms\nof closeness of the created binary vectors within Hamming distance of 2\ndeviations. This paper talks about the Golay Coding Transformation Hash Table\nand how it can be used on a FuzzyFind Dictionary as a new technology for using\nin searching through big data. This method is introduced by linear time\ncomplexity for generating the dictionary and constant time complexity to access\nthe data and update by new data sets, also updating for new data sets is linear\ntime depends on new data points. This technique is based on searching only for\nletters of English that each segment has 23 bits, and also we have more than\n23-bit and also it could work with more segments as reference table.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:46:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Yammahi", "Maryam", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Alsaby", "Faisal", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.06549", "submitter": "Lydia Fischer", "authors": "Lydia Fischer, Barbara Hammer and Heiko Wersing", "title": "Optimum Reject Options for Prototype-based Classification", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse optimum reject strategies for prototype-based classifiers and\nreal-valued rejection measures, using the distance of a data point to the\nclosest prototype or probabilistic counterparts. We compare reject schemes with\nglobal thresholds, and local thresholds for the Voronoi cells of the\nclassifier. For the latter, we develop a polynomial-time algorithm to compute\noptimum thresholds based on a dynamic programming scheme, and we propose an\nintuitive linear time, memory efficient approximation thereof with competitive\naccuracy. Evaluating the performance in various benchmarks, we conclude that\nlocal reject options are beneficial in particular for simple prototype-based\nclassifiers, while the improvement is less pronounced for advanced models. For\nthe latter, an accuracy-reject curve which is comparable to support vector\nmachine classifiers with state of the art reject options can be reached.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 08:19:17 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Fischer", "Lydia", ""], ["Hammer", "Barbara", ""], ["Wersing", "Heiko", ""]]}, {"id": "1503.06567", "submitter": "Andrej Risteski", "authors": "Pranjal Awasthi and Andrej Risteski", "title": "On some provably correct cases of variational inference for topic models", "comments": "46 pages, Compared to previous version: clarified notation, a number\n  of typos fixed throughout paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a very efficient and popular heuristic used in\nvarious forms in the context of latent variable models. It's closely related to\nExpectation Maximization (EM), and is applied when exact EM is computationally\ninfeasible. Despite being immensely popular, current theoretical understanding\nof the effectiveness of variaitonal inference based algorithms is very limited.\nIn this work we provide the first analysis of instances where variational\ninference algorithms converge to the global optimum, in the setting of topic\nmodels.\n  More specifically, we show that variational inference provably learns the\noptimal parameters of a topic model under natural assumptions on the topic-word\nmatrix and the topic priors. The properties that the topic word matrix must\nsatisfy in our setting are related to the topic expansion assumption introduced\nin (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora\net al., 2012c). The assumptions on the topic priors are related to the well\nknown Dirichlet prior, introduced to the area of topic modeling by (Blei et\nal., 2003).\n  It is well known that initialization plays a crucial role in how well\nvariational based algorithms perform in practice. The initializations that we\nuse are fairly natural. One of them is similar to what is currently used in\nLDA-c, the most popular implementation of variational inference for topic\nmodels. The other one is an overlapping clustering algorithm, inspired by a\nwork by (Arora et al., 2014) on dictionary learning, which is very simple and\nefficient.\n  While our primary goal is to provide insights into when variational inference\nmight work in practice, the multiplicative, rather than the additive nature of\nthe variational inference updates forces us to use fairly non-standard proof\narguments, which we believe will be of general interest.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:20:39 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 11:24:43 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Risteski", "Andrej", ""]]}, {"id": "1503.06572", "submitter": "Georgiana Ifrim", "authors": "Bichen Shi, Michel Schellekens, Georgiana Ifrim", "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of\n  Sorting Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a framework for analyzing the complexity of an\nalgorithm, acting as a bridge between average and worst-case behaviour. For\nexample, Quicksort and the Simplex algorithm are widely used in practical\napplications, despite their heavy worst-case complexity. Smoothed complexity\naims to better characterize such algorithms. Existing theoretical bounds for\nthe smoothed complexity of sorting algorithms are still quite weak.\nFurthermore, empirically computing the smoothed complexity via its original\ndefinition is computationally infeasible, even for modest input sizes. In this\npaper, we focus on accurately predicting the smoothed complexity of sorting\nalgorithms, using machine learning techniques. We propose two regression models\nthat take into account various properties of sorting algorithms and some of the\nknown theoretical results in smoothed analysis to improve prediction quality.\nWe show experimental results for predicting the smoothed complexity of\nQuicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore\nfilling the gap between known theoretical and empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:37:33 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Shi", "Bichen", ""], ["Schellekens", "Michel", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1503.06608", "submitter": "Lakshmi Devasena C", "authors": "Lakshmi Devasena C", "title": "Proficiency Comparison of LADTree and REPTree Classifiers for Credit\n  Risk Forecast", "comments": "arXiv admin note: text overlap with arXiv:1310.5963 by other authors", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.5, No.1, February 2015, pp. 39 - 50", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the Credit Defaulter is a perilous task of Financial Industries\nlike Banks. Ascertaining non-payer before giving loan is a significant and\nconflict-ridden task of the Banker. Classification techniques are the better\nchoice for predictive analysis like finding the claimant, whether he/she is an\nunpretentious customer or a cheat. Defining the outstanding classifier is a\nrisky assignment for any industrialist like a banker. This allow computer\nscience researchers to drill down efficient research works through evaluating\ndifferent classifiers and finding out the best classifier for such predictive\nproblems. This research work investigates the productivity of LADTree\nClassifier and REPTree Classifier for the credit risk prediction and compares\ntheir fitness through various measures. German credit dataset has been taken\nand used to predict the credit risk with a help of open source machine learning\ntool.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 11:47:05 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["C", "Lakshmi Devasena", ""]]}, {"id": "1503.06619", "submitter": "Tingting Zhu", "authors": "Tingting Zhu, Nic Dunkley, Joachim Behar, David A. Clifton, Gari D.\n  Clifford", "title": "Fusing Continuous-valued Medical Labels using a Bayesian Model", "comments": null, "journal-ref": null, "doi": "10.1007/s10439-015-1344-1", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  With the rapid increase in volume of time series medical data available\nthrough wearable devices, there is a need to employ automated algorithms to\nlabel data. Examples of labels include interventions, changes in activity (e.g.\nsleep) and changes in physiology (e.g. arrhythmias). However, automated\nalgorithms tend to be unreliable resulting in lower quality care. Expert\nannotations are scarce, expensive, and prone to significant inter- and\nintra-observer variance. To address these problems, a Bayesian\nContinuous-valued Label Aggregator(BCLA) is proposed to provide a reliable\nestimation of label aggregation while accurately infer the precision and bias\nof each algorithm. The BCLA was applied to QT interval (pro-arrhythmic\nindicator) estimation from the electrocardiogram using labels from the 2006\nPhysioNet/Computing in Cardiology Challenge database. It was compared to the\nmean, median, and a previously proposed Expectation Maximization (EM) label\naggregation approaches. While accurately predicting each labelling algorithm's\nbias and precision, the root-mean-square error of the BCLA was\n11.78$\\pm$0.63ms, significantly outperforming the best Challenge entry\n(15.37$\\pm$2.13ms) as well as the EM, mean, and median voting strategies\n(14.76$\\pm$0.52ms, 17.61$\\pm$0.55ms, and 14.43$\\pm$0.57ms respectively with\n$p<0.0001$).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 12:31:18 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 13:06:29 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Zhu", "Tingting", ""], ["Dunkley", "Nic", ""], ["Behar", "Joachim", ""], ["Clifton", "David A.", ""], ["Clifford", "Gari D.", ""]]}, {"id": "1503.06629", "submitter": "Akshay Gadde", "authors": "Akshay Gadde and Antonio Ortega", "title": "A Probabilistic Interpretation of Sampling Theory of Graph Signals", "comments": "5 pages, 2 figures, To appear in International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a probabilistic interpretation of sampling theory of graph signals.\nTo do this, we first define a generative model for the data using a pairwise\nGaussian random field (GRF) which depends on the graph. We show that, under\ncertain conditions, reconstructing a graph signal from a subset of its samples\nby least squares is equivalent to performing MAP inference on an approximation\nof this GRF which has a low rank covariance matrix. We then show that a\nsampling set of given size with the largest associated cut-off frequency, which\nis optimal from a sampling theoretic point of view, minimizes the worst case\npredictive covariance of the MAP estimate on the GRF. This interpretation also\ngives an intuitive explanation for the superior performance of the sampling\ntheoretic approach to active semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 13:20:22 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Gadde", "Akshay", ""], ["Ortega", "Antonio", ""]]}, {"id": "1503.06666", "submitter": "David Martins de Matos", "authors": "Francisco Raposo, Ricardo Ribeiro, David Martins de Matos", "title": "Using Generic Summarization to Improve Music Information Retrieval Tasks", "comments": "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing,\n  vol. 24, n. 6, March 2016", "doi": "10.1109/TASLP.2016.2541299", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to satisfy processing time constraints, many MIR tasks process only\na segment of the whole music signal. This practice may lead to decreasing\nperformance, since the most important information for the tasks may not be in\nthose processed segments. In this paper, we leverage generic summarization\nalgorithms, previously applied to text and speech summarization, to summarize\nitems in music datasets. These algorithms build summaries, that are both\nconcise and diverse, by selecting appropriate segments from the input signal\nwhich makes them good candidates to summarize music as well. We evaluate the\nsummarization process on binary and multiclass music genre classification\ntasks, by comparing the performance obtained using summarized datasets against\nthe performances obtained using continuous segments (which is the traditional\nmethod used for addressing the previously mentioned time constraints) and full\nsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,\nMMR, and a Support Sets-based Centrality model improve classification\nperformance when compared to selected 30-second baselines. We also show that\nsummarized datasets lead to a classification performance whose difference is\nnot statistically significant from using full songs. Furthermore, we make an\nargument stating the advantages of sharing summarized datasets for future MIR\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 14:48:24 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 18:38:22 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 16:24:42 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Raposo", "Francisco", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1503.06745", "submitter": "Junlin Zhang", "authors": "Junlin Zhang, Jose Garcia", "title": "Online classifier adaptation for cost-sensitive learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the problem of online cost-sensitive clas- sifier\nadaptation and the first algorithm to solve it. We assume we have a base\nclassifier for a cost-sensitive classification problem, but it is trained with\nrespect to a cost setting different to the desired one. Moreover, we also have\nsome training data samples streaming to the algorithm one by one. The prob- lem\nis to adapt the given base classifier to the desired cost setting using the\nsteaming training samples online. To solve this problem, we propose to learn a\nnew classifier by adding an adaptation function to the base classifier, and\nupdate the adaptation function parameter according to the streaming data\nsamples. Given a input data sample and the cost of misclassifying it, we up-\ndate the adaptation function parameter by minimizing cost weighted hinge loss\nand respecting previous learned parameter simultaneously. The proposed\nalgorithm is compared to both online and off-line cost-sensitive algorithms on\ntwo cost-sensitive classification problems, and the experiments show that it\nnot only outperforms them one classification performances, but also requires\nsignificantly less running time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 17:47:00 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Junlin", ""], ["Garcia", "Jose", ""]]}, {"id": "1503.06833", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani, Shai Shalev-Shwartz, Ohad Shamir", "title": "On Lower and Upper Bounds for Smooth and Strongly Convex Optimization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel framework to study smooth and strongly convex optimization\nalgorithms, both deterministic and stochastic. Focusing on quadratic functions\nwe are able to examine optimization algorithms as a recursive application of\nlinear operators. This, in turn, reveals a powerful connection between a class\nof optimization algorithms and the analytic theory of polynomials whereby new\nlower and upper bounds are derived. Whereas existing lower bounds for this\nsetting are only valid when the dimensionality scales with the number of\niterations, our lower bound holds in the natural regime where the\ndimensionality is fixed. Lastly, expressing it as an optimal solution for the\ncorresponding optimization problem over polynomials, as formulated by our\nframework, we present a novel systematic derivation of Nesterov's well-known\nAccelerated Gradient Descent method. This rather natural interpretation of AGD\ncontrasts with earlier ones which lacked a simple, yet solid, motivation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 21:00:18 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1503.06858", "submitter": "Yingyu Liang", "authors": "Maria-Florina Balcan, Yingyu Liang, Le Song, David Woodruff, Bo Xie", "title": "Communication Efficient Distributed Kernel Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Principal Component Analysis (KPCA) is a key machine learning\nalgorithm for extracting nonlinear features from data. In the presence of a\nlarge volume of high dimensional data collected in a distributed fashion, it\nbecomes very costly to communicate all of this data to a single data center and\nthen perform kernel PCA. Can we perform kernel PCA on the entire dataset in a\ndistributed and communication efficient fashion while maintaining provable and\nstrong guarantees in solution quality?\n  In this paper, we give an affirmative answer to the question by developing a\ncommunication efficient algorithm to perform kernel PCA in the distributed\nsetting. The algorithm is a clever combination of subspace embedding and\nadaptive sampling techniques, and we show that the algorithm can take as input\nan arbitrary configuration of distributed datasets, and compute a set of global\nkernel principal components with relative error guarantees independent of the\ndimension of the feature space or the total number of data points. In\nparticular, computing $k$ principal components with relative error $\\epsilon$\nover $s$ workers has communication cost $\\tilde{O}(s \\rho k/\\epsilon+s\nk^2/\\epsilon^3)$ words, where $\\rho$ is the average number of nonzero entries\nin each data point. Furthermore, we experimented the algorithm with large-scale\nreal world datasets and showed that the algorithm produces a high quality\nkernel PCA solution while using significantly less communication than\nalternative approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 22:00:51 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 03:19:53 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 17:23:53 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2016 23:40:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Liang", "Yingyu", ""], ["Song", "Le", ""], ["Woodruff", "David", ""], ["Xie", "Bo", ""]]}, {"id": "1503.06902", "submitter": "Li Zhou", "authors": "Li Zhou", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduce three Bayesian style Multi-armed bandit algorithms:\nInformation-directed sampling, Thompson Sampling and Generalized Thompson\nSampling. The goal is to give an intuitive explanation for these three\nalgorithms and their regret bounds, and provide some derivations that are\nomitted in the original papers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 03:26:28 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Zhou", "Li", ""]]}, {"id": "1503.06944", "submitter": "Emilie Morvant", "authors": "Pascal Germain (SIERRA), Amaury Habrard (LHC), Fran\\c{c}ois\n  Laviolette, Emilie Morvant (LHC)", "title": "PAC-Bayesian Theorems for Domain Adaptation with Specialization to\n  Linear Classifiers", "comments": "This report is a long version of our paper entitled A PAC-Bayesian\n  Approach for Domain Adaptation with Specialization to Linear Classifiers\n  published in the proceedings of the International Conference on Machine\n  Learning (ICML) 2013. We improved our main results, extended our experiments,\n  and proposed an extension to multisource domain adaptation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide two main contributions in PAC-Bayesian theory for\ndomain adaptation where the objective is to learn, from a source distribution,\na well-performing majority vote on a different target distribution. On the one\nhand, we propose an improvement of the previous approach proposed by Germain et\nal. (2013), that relies on a novel distribution pseudodistance based on a\ndisagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain\nadaptation bound for the stochastic Gibbs classifier. We specialize it to\nlinear classifiers, and design a learning algorithm which shows interesting\nresults on a synthetic problem and on a popular sentiment annotation task. On\nthe other hand, we generalize these results to multisource domain adaptation\nallowing us to take into account different source domains. This study opens the\ndoor to tackle domain adaptation tasks by making use of all the PAC-Bayesian\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 08:17:44 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 12:10:09 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 08:34:04 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Germain", "Pascal", "", "SIERRA"], ["Habrard", "Amaury", "", "LHC"], ["Laviolette", "Fran\u00e7ois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "1503.06952", "submitter": "Maria-Carolina Monard MC", "authors": "Jean Metz and Newton Spola\\^or and Everton A. Cherman and Maria C.\n  Monard", "title": "Comparing published multi-label classifier performance measures to the\n  ones obtained by a simple multi-label baseline classifier", "comments": "19 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning, simple baseline classifiers can be constructed by\nonly looking at the class, i.e., ignoring any other information from the\ndataset. The single-label learning community frequently uses as a reference the\none which always predicts the majority class. Although a classifier might\nperform worse than this simple baseline classifier, this behaviour requires a\nspecial explanation. Aiming to motivate the community to compare experimental\nresults with the ones provided by a multi-label baseline classifier, calling\nthe attention about the need of special explanations related to classifiers\nwhich perform worse than the baseline, in this work we propose the use of\nGeneral_B, a multi-label baseline classifier. General_B was evaluated in\ncontrast to results published in the literature which were carefully selected\nusing a systematic review process. It was found that a considerable number of\npublished results on 10 frequently used datasets are worse than or equal to the\nones obtained by General_B, and for one dataset it reaches up to 43% of the\ndataset published results. Moreover, although a simple baseline classifier was\nnot considered in these publications, it was observed that even for very poor\nresults no special explanations were provided in most of them. We hope that the\nfindings of this work would encourage the multi-label community to consider the\nidea of using a simple baseline classifier, such that further explanations are\nprovided when a classifiers performs worse than a baseline.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 08:57:25 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Metz", "Jean", ""], ["Spola\u00f4r", "Newton", ""], ["Cherman", "Everton A.", ""], ["Monard", "Maria C.", ""]]}, {"id": "1503.06960", "submitter": "Shay Moran", "authors": "Shay Moran, Amir Yehudayoff", "title": "Sample compression schemes for VC classes", "comments": "14 pages. The previous version of this text contained an error;\n  Theorem 2.1 in it is false. This error only affects the statement for\n  multi-labeled classes, and the construction for binary-labeled classes still\n  holds. In the new version of the text, we added a relevant discussion in\n  Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample compression schemes were defined by Littlestone and Warmuth (1986) as\nan abstraction of the structure underlying many learning algorithms. Roughly\nspeaking, a sample compression scheme of size $k$ means that given an arbitrary\nlist of labeled examples, one can retain only $k$ of them in a way that allows\nto recover the labels of all other examples in the list. They showed that\ncompression implies PAC learnability for binary-labeled classes, and asked\nwhether the other direction holds. We answer their question and show that every\nconcept class $C$ with VC dimension $d$ has a sample compression scheme of size\nexponential in $d$. The proof uses an approximate minimax phenomenon for binary\nmatrices of low VC dimension, which may be of interest in the context of game\ntheory.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 09:30:33 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 12:18:14 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Moran", "Shay", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1503.06962", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Probabilistic Binary-Mask Cocktail-Party Source Separation in a\n  Convolutional Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separation of competing speech is a key challenge in signal processing and a\nfeat routinely performed by the human auditory brain. A long standing benchmark\nof the spectrogram approach to source separation is known as the ideal binary\nmask. Here, we train a convolutional deep neural network, on a two-speaker\ncocktail party problem, to make probabilistic predictions about binary masks.\nOur results approach ideal binary mask performance, illustrating that\nrelatively simple deep neural networks are capable of robust binary mask\nprediction. We also illustrate the trade-off between prediction statistics and\nseparation quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 09:34:51 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1503.07027", "submitter": "Karin Schnass", "authors": "Karin Schnass", "title": "Convergence radius and sample complexity of ITKM algorithms for\n  dictionary learning", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that iterative thresholding and K-means (ITKM)\nalgorithms can recover a generating dictionary with K atoms from noisy $S$\nsparse signals up to an error $\\tilde \\varepsilon$ as long as the\ninitialisation is within a convergence radius, that is up to a $\\log K$ factor\ninversely proportional to the dynamic range of the signals, and the sample size\nis proportional to $K \\log K \\tilde \\varepsilon^{-2}$. The results are valid\nfor arbitrary target errors if the sparsity level is of the order of the square\nroot of the signal dimension $d$ and for target errors down to $K^{-\\ell}$ if\n$S$ scales as $S \\leq d/(\\ell \\log K)$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 13:29:12 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 10:27:29 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 10:43:21 GMT"}, {"version": "v4", "created": "Mon, 8 Aug 2016 08:07:39 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Schnass", "Karin", ""]]}, {"id": "1503.07077", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Kyle W. Willett, Joni Dambre", "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction", "comments": "Accepted for publication in MNRAS. 20 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/stv632", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 15:34:06 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Dieleman", "Sander", ""], ["Willett", "Kyle W.", ""], ["Dambre", "Joni", ""]]}, {"id": "1503.07104", "submitter": "Freeha  Azmat", "authors": "Freeha Azmat, Yunfei Chen (Senior Member, IEEE) and Nigel Stocks", "title": "Analysis of Spectrum Occupancy Using Machine Learning Algorithms", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the spectrum occupancy using different machine\nlearning techniques. Both supervised techniques (naive Bayesian classifier\n(NBC), decision trees (DT), support vector machine (SVM), linear regression\n(LR)) and unsupervised algorithm (hidden markov model (HMM)) are studied to\nfind the best technique with the highest classification accuracy (CA). A\ndetailed comparison of the supervised and unsupervised algorithms in terms of\nthe computational time and classification accuracy is performed. The classified\noccupancy status is further utilized to evaluate the probability of secondary\nuser outage for the future time slots, which can be used by system designers to\ndefine spectrum allocation and spectrum sharing policies. Numerical results\nshow that SVM is the best algorithm among all the supervised and unsupervised\nclassifiers. Based on this, we proposed a new SVM algorithm by combining it\nwith fire fly algorithm (FFA), which is shown to outperform all other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 16:38:32 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Azmat", "Freeha", "", "Senior Member, IEEE"], ["Chen", "Yunfei", "", "Senior Member, IEEE"], ["Stocks", "Nigel", ""]]}, {"id": "1503.07211", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Universal Approximation of Markov Kernels by Shallow Stochastic\n  Feedforward Networks", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish upper bounds for the minimal number of hidden units for which a\nbinary stochastic feedforward network with sigmoid activation probabilities and\na single hidden layer is a universal approximator of Markov kernels. We show\nthat each possible probabilistic assignment of the states of $n$ output units,\ngiven the states of $k\\geq1$ input units, can be approximated arbitrarily well\nby a network with $2^{k-1}(2^{n-1}-1)$ hidden units.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 21:38:59 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1503.07240", "submitter": "Dengyong  Zhou", "authors": "Dengyong Zhou, Qiang Liu, John C. Platt, Christopher Meek, Nihar B.\n  Shah", "title": "Regularized Minimax Conditional Entropy for Crowdsourcing", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rapidly increasing interest in crowdsourcing for data labeling. By\ncrowdsourcing, a large number of labels can be often quickly gathered at low\ncost. However, the labels provided by the crowdsourcing workers are usually not\nof high quality. In this paper, we propose a minimax conditional entropy\nprinciple to infer ground truth from noisy crowdsourced labels. Under this\nprinciple, we derive a unique probabilistic labeling model jointly\nparameterized by worker ability and item difficulty. We also propose an\nobjective measurement principle, and show that our method is the only method\nwhich satisfies this objective measurement principle. We validate our method\nthrough a variety of real crowdsourcing datasets with binary, multiclass or\nordinal labels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 00:10:11 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Zhou", "Dengyong", ""], ["Liu", "Qiang", ""], ["Platt", "John C.", ""], ["Meek", "Christopher", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1503.07274", "submitter": "Elman Mansimov", "authors": "Elman Mansimov, Nitish Srivastava, Ruslan Salakhutdinov", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural\n  Networks", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new way of incorporating temporal information present in videos\ninto Spatial Convolutional Neural Networks (ConvNets) trained on images, that\navoids training Spatio-Temporal ConvNets from scratch. We describe several\ninitializations of weights in 3D Convolutional Layers of Spatio-Temporal\nConvNet using 2D Convolutional Weights learned from ImageNet. We show that it\nis important to initialize 3D Convolutional Weights judiciously in order to\nlearn temporal representations of videos. We evaluate our methods on the\nUCF-101 dataset and demonstrate improvement over Spatial ConvNets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 03:41:47 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Mansimov", "Elman", ""], ["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1503.07477", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Praful Koturwar, Sheetal Girase, Debajyoti Mukhopadhyay", "title": "A Survey of Classification Techniques in the Area of Big Data", "comments": "7 pages, 3 figures, 2 tables in IJAFRC, Vol.1, Issue 11, November\n  2014, ISSN: 2348-4853", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data concern large-volume, growing data sets that are complex and have\nmultiple autonomous sources. Earlier technologies were not able to handle\nstorage and processing of huge data thus Big Data concept comes into existence.\nThis is a tedious job for users unstructured data. So, there should be some\nmechanism which classify unstructured data into organized form which helps user\nto easily access required data. Classification techniques over big\ntransactional database provide required data to the users from large datasets\nmore simple way. There are two main classification techniques, supervised and\nunsupervised. In this paper we focused on to study of different supervised\nclassification techniques. Further this paper shows a advantages and\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 17:56:19 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Koturwar", "Praful", ""], ["Girase", "Sheetal", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.07508", "submitter": "Bo Xin", "authors": "Bo Xin, Lingjing Hu, Yizhou Wang and Wen Gao", "title": "Stable Feature Selection from Brain sMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimage analysis usually involves learning thousands or even millions of\nvariables using only a limited number of samples. In this regard, sparse\nmodels, e.g. the lasso, are applied to select the optimal features and achieve\nhigh diagnosis accuracy. The lasso, however, usually results in independent\nunstable features. Stability, a manifest of reproducibility of statistical\nresults subject to reasonable perturbations to data and the model, is an\nimportant focus in statistics, especially in the analysis of high dimensional\ndata. In this paper, we explore a nonnegative generalized fused lasso model for\nstable feature selection in the diagnosis of Alzheimer's disease. In addition\nto sparsity, our model incorporates two important pathological priors: the\nspatial cohesion of lesion voxels and the positive correlation between the\nfeatures and the disease labels. To optimize the model, we propose an efficient\nalgorithm by proving a novel link between total variation and fast network flow\nalgorithms via conic duality. Experiments show that the proposed nonnegative\nmodel performs much better in exploring the intrinsic structure of data via\nselecting stable features compared with other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 19:30:14 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Xin", "Bo", ""], ["Hu", "Lingjing", ""], ["Wang", "Yizhou", ""], ["Gao", "Wen", ""]]}, {"id": "1503.07790", "submitter": "Yongxin Yang", "authors": "Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang and Shaogang Gong", "title": "Transductive Multi-label Zero-shot Learning", "comments": "12 pages, 6 figures, Accepted to BMVC 2014 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning has received increasing interest as a means to alleviate\nthe often prohibitive expense of annotating training data for large scale\nrecognition problems. These methods have achieved great success via learning\nintermediate semantic representations in the form of attributes and more\nrecently, semantic word vectors. However, they have thus far been constrained\nto the single-label case, in contrast to the growing popularity and importance\nof more realistic multi-label data. In this paper, for the first time, we\ninvestigate and formalise a general framework for multi-label zero-shot\nlearning, addressing the unique challenge therein: how to exploit multi-label\ncorrelation at test time with no training data for those classes? In\nparticular, we propose (1) a multi-output deep regression model to project an\nimage into a semantic word space, which explicitly exploits the correlations in\nthe intermediate semantic layer of word vectors; (2) a novel zero-shot learning\nalgorithm for multi-label data that exploits the unique compositionality\nproperty of semantic word vector representations; and (3) a transductive\nlearning strategy to enable the regression model learned from seen classes to\ngeneralise well to unseen classes. Our zero-shot learning experiments on a\nnumber of standard multi-label datasets demonstrate that our method outperforms\na variety of baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:12:34 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Fu", "Yanwei", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Tim", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1503.07795", "submitter": "Marina Sokolova", "authors": "Naveen Kumar Parachur Cotha and Marina Sokolova", "title": "Multi-Labeled Classification of Demographic Attributes of Patients: a\n  case study of diabetics patients", "comments": "16 pages, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated learning of patients demographics can be seen as multi-label\nproblem where a patient model is based on different race and gender groups. The\nresulting model can be further integrated into Privacy-Preserving Data Mining,\nwhere it can be used to assess risk of identification of different patient\ngroups. Our project considers relations between diabetes and demographics of\npatients as a multi-labelled problem. Most research in this area has been done\nas binary classification, where the target class is finding if a person has\ndiabetes or not. But very few, and maybe no work has been done in multi-labeled\nanalysis of the demographics of patients who are likely to be diagnosed with\ndiabetes. To identify such groups, we applied ensembles of several multi-label\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:22:26 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Cotha", "Naveen Kumar Parachur", ""], ["Sokolova", "Marina", ""]]}, {"id": "1503.07884", "submitter": "Yongxin Yang", "authors": "Yanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang and Shaogang\n  Gong", "title": "Transductive Multi-class and Multi-label Zero-shot Learning", "comments": "4 pages, 4 figures, ECCV 2014 Workshop on Parts and Attributes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, zero-shot learning (ZSL) has received increasing interest. The key\nidea underpinning existing ZSL approaches is to exploit knowledge transfer via\nan intermediate-level semantic representation which is assumed to be shared\nbetween the auxiliary and target datasets, and is used to bridge between these\ndomains for knowledge transfer. The semantic representation used in existing\napproaches varies from visual attributes to semantic word vectors and semantic\nrelatedness. However, the overall pipeline is similar: a projection mapping\nlow-level features to the semantic representation is learned from the auxiliary\ndataset by either classification or regression models and applied directly to\nmap each instance into the same semantic representation space where a zero-shot\nclassifier is used to recognise the unseen target class instances with a single\nknown 'prototype' of each target class. In this paper we discuss two related\nlines of work improving the conventional approach: exploiting transductive\nlearning ZSL, and generalising ZSL to the multi-label case.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 20:07:37 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1503.07906", "submitter": "Gang Chen", "authors": "Gang Chen and Sargur N. Srihari", "title": "Generalized K-fan Multimodal Deep Model with Shared Representations", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Multimodal learning with deep Boltzmann machines (DBMs) is an generative\napproach to fuse multimodal inputs, and can learn the shared representation via\nContrastive Divergence (CD) for classification and information retrieval tasks.\nHowever, it is a 2-fan DBM model, and cannot effectively handle multiple\nprediction tasks. Moreover, this model cannot recover the hidden\nrepresentations well by sampling from the conditional distribution when more\nthan one modalities are missing. In this paper, we propose a K-fan deep\nstructure model, which can handle the multi-input and muti-output learning\nproblems effectively. In particular, the deep structure has K-branch for\ndifferent inputs where each branch can be composed of a multi-layer deep model,\nand a shared representation is learned in an discriminative manner to tackle\nmultimodal tasks. Given the deep structure, we propose two objective functions\nto handle two multi-input and multi-output tasks: joint visual restoration and\nlabeling, and the multi-view multi-calss object recognition tasks. To estimate\nthe model parameters, we initialize the deep model parameters with CD to\nmaximize the joint distribution, and then we use backpropagation to update the\nmodel according to specific objective function. The experimental results\ndemonstrate that the model can effectively leverages multi-source information\nand predict multiple tasks well over competitive baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 21:17:46 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Chen", "Gang", ""], ["Srihari", "Sargur N.", ""]]}, {"id": "1503.07940", "submitter": "Ananda Theertha Suresh", "authors": "Alon Orlitsky and Ananda Theertha Suresh", "title": "Competitive Distribution Estimation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating an unknown distribution from its samples is a fundamental problem\nin statistics. The common, min-max, formulation of this goal considers the\nperformance of the best estimator over all distributions in a class. It shows\nthat with $n$ samples, distributions over $k$ symbols can be learned to a KL\ndivergence that decreases to zero with the sample size $n$, but grows\nunboundedly with the alphabet size $k$.\n  Min-max performance can be viewed as regret relative to an oracle that knows\nthe underlying distribution. We consider two natural and modest limits on the\noracle's power. One where it knows the underlying distribution only up to\nsymbol permutations, and the other where it knows the exact distribution but is\nrestricted to use natural estimators that assign the same probability to\nsymbols that appeared equally many times in the sample.\n  We show that in both cases the competitive regret reduces to\n$\\min(k/n,\\tilde{\\mathcal{O}}(1/\\sqrt n))$, a quantity upper bounded uniformly\nfor every alphabet size. This shows that distributions can be estimated nearly\nas well as when they are essentially known in advance, and nearly as well as\nwhen they are completely known in advance but need to be estimated via a\nnatural estimator. We also provide an estimator that runs in linear time and\nincurs competitive regret of $\\tilde{\\mathcal{O}}(\\min(k/n,1/\\sqrt n))$, and\nshow that for natural estimators this competitive regret is inevitable. We also\ndemonstrate the effectiveness of competitive estimators using simulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 01:41:48 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1503.07970", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Bayesian Cross Validation and WAIC for Predictive Prior Design in\n  Regular Asymptotic Theory", "comments": "33 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior design is one of the most important problems in both statistics and\nmachine learning. The cross validation (CV) and the widely applicable\ninformation criterion (WAIC) are predictive measures of the Bayesian\nestimation, however, it has been difficult to apply them to find the optimal\nprior because their mathematical properties in prior evaluation have been\nunknown and the region of the hyperparameters is too wide to be examined. In\nthis paper, we derive a new formula by which the theoretical relation among CV,\nWAIC, and the generalization loss is clarified and the optimal hyperparameter\ncan be directly found.\n  By the formula, three facts are clarified about predictive prior design.\nFirstly, CV and WAIC have the same second order asymptotic expansion, hence\nthey are asymptotically equivalent to each other as the optimizer of the\nhyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes\nthe average generalization loss to be minimized asymptotically but does not the\nrandom generalization loss. And lastly, by using the mathematical relation\nbetween priors, the variances of the optimized hyperparameters by CV and WAIC\nare made smaller with small computational costs. Also we show that the\noptimized hyperparameter by DIC or the marginal likelihood does not minimize\nthe average or random generalization loss in general.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 06:21:06 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "1503.07989", "submitter": "Naveed Akhtar Mr.", "authors": "Naveed Akhtar, Faisal Shafait, Ajmal Mian", "title": "Discriminative Bayesian Dictionary Learning for Classification", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach to learn discriminative dictionaries for\nsparse representation of data. The proposed approach infers probability\ndistributions over the atoms of a discriminative dictionary using a Beta\nProcess. It also computes sets of Bernoulli distributions that associate class\nlabels to the learned dictionary atoms. This association signifies the\nselection probabilities of the dictionary atoms in the expansion of\nclass-specific data. Furthermore, the non-parametric character of the proposed\napproach allows it to infer the correct size of the dictionary. We exploit the\naforementioned Bernoulli distributions in separately learning a linear\nclassifier. The classifier uses the same hierarchical Bayesian model as the\ndictionary, which we present along the analytical inference solution for Gibbs\nsampling. For classification, a test instance is first sparsely encoded over\nthe learned dictionary and the codes are fed to the classifier. We performed\nexperiments for face and action recognition; and object and scene-category\nclassification using five public datasets and compared the results with\nstate-of-the-art discriminative sparse representation approaches. Experiments\nshow that the proposed Bayesian approach consistently outperforms the existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 08:36:15 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Akhtar", "Naveed", ""], ["Shafait", "Faisal", ""], ["Mian", "Ajmal", ""]]}, {"id": "1503.08169", "submitter": "Azalia Mirhoseini", "authors": "Azalia Mirhoseini, Eva L. Dyer, Ebrahim.M. Songhori, Richard G.\n  Baraniuk, Farinaz Koushanfar", "title": "RankMap: A Platform-Aware Framework for Distributed Learning from Dense\n  Datasets", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces RankMap, a platform-aware end-to-end framework for\nefficient execution of a broad class of iterative learning algorithms for\nmassive and dense datasets. Our framework exploits data structure to factorize\nit into an ensemble of lower rank subspaces. The factorization creates sparse\nlow-dimensional representations of the data, a property which is leveraged to\ndevise effective mapping and scheduling of iterative learning algorithms on the\ndistributed computing machines. We provide two APIs, one matrix-based and one\ngraph-based, which facilitate automated adoption of the framework for\nperforming several contemporary learning applications. To demonstrate the\nutility of RankMap, we solve sparse recovery and power iteration problems on\nvarious real-world datasets with up to 1.8 billion non-zeros. Our evaluations\nare performed on Amazon EC2 and IBM iDataPlex servers using up to 244 cores.\nThe results demonstrate up to two orders of magnitude improvements in memory\nusage, execution speed, and bandwidth compared with the best reported prior\nwork, while achieving the same level of learning accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 18:02:51 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 14:29:44 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Mirhoseini", "Azalia", ""], ["Dyer", "Eva L.", ""], ["Songhori", "Ebrahim. M.", ""], ["Baraniuk", "Richard G.", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1503.08316", "submitter": "Aurelien Lucchi", "authors": "Aurelien Lucchi, Brian McWilliams, Thomas Hofmann", "title": "A Variance Reduced Stochastic Newton Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Newton methods are widely used in practise for convex loss minimization\nproblems. These methods exhibit good empirical performance on a wide variety of\ntasks and enjoy super-linear convergence to the optimal solution. For\nlarge-scale learning problems, stochastic Quasi-Newton methods have been\nrecently proposed. However, these typically only achieve sub-linear convergence\nrates and have not been shown to consistently perform well in practice since\nnoisy Hessian approximations can exacerbate the effect of high-variance\nstochastic gradient estimates. In this work we propose Vite, a novel stochastic\nQuasi-Newton algorithm that uses an existing first-order technique to reduce\nthis variance. Without exploiting the specific form of the approximate Hessian,\nwe show that Vite reaches the optimum at a geometric rate with a constant\nstep-size when dealing with smooth strongly convex functions. Empirically, we\ndemonstrate improvements over existing stochastic Quasi-Newton and variance\nreduced stochastic gradient methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 15:51:48 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 06:57:26 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 11:34:58 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2015 19:24:03 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Lucchi", "Aurelien", ""], ["McWilliams", "Brian", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1503.08329", "submitter": "Pascal Germain", "authors": "Pascal Germain, Alexandre Lacasse, Fran\\c{c}ois Laviolette, Mario\n  Marchand, Jean-Francis Roy", "title": "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a\n  Learning Algorithm", "comments": "Published in JMLR http://jmlr.org/papers/v16/germain15a.html", "journal-ref": "Journal of Machine Learning Research 2015, vol. 16, p. 787-860", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extensive analysis of the behavior of majority votes in binary\nclassification. In particular, we introduce a risk bound for majority votes,\ncalled the C-bound, that takes into account the average quality of the voters\nand their average disagreement. We also propose an extensive PAC-Bayesian\nanalysis that shows how the C-bound can be estimated from various observations\ncontained in the training data. The analysis intends to be self-contained and\ncan be used as introductory material to PAC-Bayesian statistical learning\ntheory. It starts from a general PAC-Bayesian perspective and ends with\nuncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler\ndivergence and others allow kernel functions to be used as voters (via the\nsample compression setting). Finally, out of the analysis, we propose the MinCq\nlearning algorithm that basically minimizes the C-bound. MinCq reduces to a\nsimple quadratic program. Aside from being theoretically grounded, MinCq\nachieves state-of-the-art performance, as shown in our extensive empirical\ncomparison with both AdaBoost and the Support Vector Machine.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 17:19:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 20:08:16 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Germain", "Pascal", ""], ["Lacasse", "Alexandre", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1503.08348", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Rebecca M. Willett", "title": "Sparse Linear Regression With Missing Data", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fast and accurate method for sparse regression in the\npresence of missing data. The underlying statistical model encapsulates the\nlow-dimensional structure of the incomplete data matrix and the sparsity of the\nregression coefficients, and the proposed algorithm jointly learns the\nlow-dimensional structure of the data and a linear regressor with sparse\ncoefficients. The proposed stochastic optimization method, Sparse Linear\nRegression with Missing Data (SLRM), performs an alternating minimization\nprocedure and scales well with the problem size. Large deviation inequalities\nshed light on the impact of the various problem-dependent parameters on the\nexpected squared loss of the learned regressor. Extensive simulations on both\nsynthetic and real datasets show that SLRM performs better than competing\nalgorithms in a variety of contexts.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 21:03:32 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1503.08363", "submitter": "Ravi Ganti", "authors": "Ravi Ganti", "title": "Active Model Aggregation via Stochastic Mirror Descent", "comments": "12 pages, 20 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning convex aggregation of models, that is as\ngood as the best convex aggregation, for the binary classification problem.\nWorking in the stream based active learning setting, where the active learner\nhas to make a decision on-the-fly, if it wants to query for the label of the\npoint currently seen in the stream, we propose a stochastic-mirror descent\nalgorithm, called SMD-AMA, with entropy regularization. We establish an excess\nrisk bounds for the loss of the convex aggregate returned by SMD-AMA to be of\nthe order of $O\\left(\\sqrt{\\frac{\\log(M)}{{T^{1-\\mu}}}}\\right)$, where $\\mu\\in\n[0,1)$ is an algorithm dependent parameter, that trades-off the number of\nlabels queried, and excess risk.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:54:12 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""]]}, {"id": "1503.08370", "submitter": "Onur Atan", "authors": "Onur Atan, Cem Tekin, Mihaela van der Schaar", "title": "Global Bandits", "comments": "arXiv admin note: substantial text overlap with arXiv:1410.7890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits (MAB) model sequential decision making problems, in which\na learner sequentially chooses arms with unknown reward distributions in order\nto maximize its cumulative reward. Most of the prior work on MAB assumes that\nthe reward distributions of each arm are independent. But in a wide variety of\ndecision problems -- from drug dosage to dynamic pricing -- the expected\nrewards of different arms are correlated, so that selecting one arm provides\ninformation about the expected rewards of other arms as well. We propose and\nanalyze a class of models of such decision problems, which we call {\\em global\nbandits}. In the case in which rewards of all arms are deterministic functions\nof a single unknown parameter, we construct a greedy policy that achieves {\\em\nbounded regret}, with a bound that depends on the single true parameter of the\nproblem. Hence, this policy selects suboptimal arms only finitely many times\nwith probability one. For this case we also obtain a bound on regret that is\n{\\em independent of the true parameter}; this bound is sub-linear, with an\nexponent that depends on the informativeness of the arms. We also propose a\nvariant of the greedy policy that achieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$\nworst-case and $\\mathcal{O}(1)$ parameter dependent regret. Finally, we perform\nexperiments on dynamic pricing and show that the proposed algorithms achieve\nsignificant gains with respect to the well-known benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 00:16:58 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:42:42 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 07:43:48 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Atan", "Onur", ""], ["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1503.08381", "submitter": "Xu Sun", "authors": "Xu Sun, Shuming Ma, Yi Zhang, Xuancheng Ren", "title": "Towards Easier and Faster Sequence Labeling for Natural Language\n  Processing: A Search-based Probabilistic Online Learning Framework (SAPO)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major approaches for sequence labeling. One is the\nprobabilistic gradient-based methods such as conditional random fields (CRF)\nand neural networks (e.g., RNN), which have high accuracy but drawbacks: slow\ntraining, and no support of search-based optimization (which is important in\nmany cases). The other is the search-based learning methods such as structured\nperceptron and margin infused relaxed algorithm (MIRA), which have fast\ntraining but also drawbacks: low accuracy, no probabilistic information, and\nnon-convergence in real-world tasks. We propose a novel and \"easy\" solution, a\nsearch-based probabilistic online learning method, to address most of those\nissues. The method is \"easy\", because the optimization algorithm at the\ntraining stage is as simple as the decoding algorithm at the test stage. This\nmethod searches the output candidates, derives probabilities, and conducts\nefficient online learning. We show that this method with fast training and\ntheoretical guarantee of convergence, which is easy to implement, can support\nsearch-based optimization and obtain top accuracy. Experiments on well-known\ntasks show that our method has better accuracy than CRF and BiLSTM\\footnote{The\nSAPO code is released at \\url{https://github.com/lancopku/SAPO}.}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 03:41:03 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 02:20:57 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 01:30:13 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2018 11:11:36 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Sun", "Xu", ""], ["Ma", "Shuming", ""], ["Zhang", "Yi", ""], ["Ren", "Xuancheng", ""]]}, {"id": "1503.08395", "submitter": "Shusen Wang", "authors": "Shusen Wang and Zhihua Zhang and Tong Zhang", "title": "Towards More Efficient SPSD Matrix Approximation and CUR Matrix\n  Decomposition", "comments": "Journal of Machine Learning Research 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive semi-definite (SPSD) matrix approximation methods have\nbeen extensively used to speed up large-scale eigenvalue computation and kernel\nlearning methods. The standard sketch based method, which we call the prototype\nmodel, produces relatively accurate approximations, but is inefficient on large\nsquare matrices. The Nystr\\\"om method is highly efficient, but can only achieve\nlow accuracy. In this paper we propose a novel model that we call the {\\it fast\nSPSD matrix approximation model}. The fast model is nearly as efficient as the\nNystr\\\"om method and as accurate as the prototype model. We show that the fast\nmodel can potentially solve eigenvalue problems and kernel learning problems in\nlinear time with respect to the matrix size $n$ to achieve $1+\\epsilon$\nrelative-error, whereas both the prototype model and the Nystr\\\"om method cost\nat least quadratic time to attain comparable error bound. Empirical comparisons\namong the prototype model, the Nystr\\\"om method, and our fast model demonstrate\nthe superiority of the fast model. We also contribute new understandings of the\nNystr\\\"om method. The Nystr\\\"om method is a special instance of our fast model\nand is approximation to the prototype model. Our technique can be\nstraightforwardly applied to make the CUR matrix decomposition more efficiently\ncomputed without much affecting the accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 07:25:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 10:34:26 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 05:58:28 GMT"}, {"version": "v4", "created": "Fri, 15 May 2015 06:59:42 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 06:00:23 GMT"}, {"version": "v6", "created": "Sat, 10 Dec 2016 04:20:46 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""], ["Zhang", "Tong", ""]]}, {"id": "1503.08471", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "Cross-validation of matching correlation analysis by resampling matching\n  weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength of association between a pair of data vectors is represented by\na nonnegative real number, called matching weight. For dimensionality\nreduction, we consider a linear transformation of data vectors, and define a\nmatching error as the weighted sum of squared distances between transformed\nvectors with respect to the matching weights. Given data vectors and matching\nweights, the optimal linear transformation minimizing the matching error is\nsolved by the spectral graph embedding of Yan et al. (2007). This method is a\ngeneralization of the canonical correlation analysis, and will be called as\nmatching correlation analysis (MCA). In this paper, we consider a novel\nsampling scheme where the observed matching weights are randomly sampled from\nunderlying true matching weights with small probability, whereas the data\nvectors are treated as constants. We then investigate a cross-validation by\nresampling the matching weights. Our asymptotic theory shows that the\ncross-validation, if rescaled properly, computes an unbiased estimate of the\nmatching error with respect to the true matching weights. Existing ideas of\ncross-validation for resampling data vectors, instead of resampling matching\nweights, are not applicable here. MCA can be used for data vectors from\nmultiple domains with different dimensions via an embarrassingly simple idea of\ncoding the data vectors. This method will be called as cross-domain matching\ncorrelation analysis (CDMCA), and an interesting connection to the classical\nassociative memory model of neural networks is also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 18:21:22 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 14:50:44 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2015 02:03:47 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1503.08528", "submitter": "Edith Cohen", "authors": "Shiri Chechik and Edith Cohen and Haim Kaplan", "title": "Average Distance Queries through Weighted Samples in Graphs and Metric\n  Spaces: High Scalability with Tight Statistical Guarantees", "comments": "21 pages, will appear in the Proceedings of RANDOM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average distance from a node to all other nodes in a graph, or from a\nquery point in a metric space to a set of points, is a fundamental quantity in\ndata analysis. The inverse of the average distance, known as the (classic)\ncloseness centrality of a node, is a popular importance measure in the study of\nsocial networks. We develop novel structural insights on the sparsifiability of\nthe distance relation via weighted sampling. Based on that, we present highly\npractical algorithms with strong statistical guarantees for fundamental\nproblems. We show that the average distance (and hence the centrality) for all\nnodes in a graph can be estimated using $O(\\epsilon^{-2})$ single-source\ndistance computations. For a set $V$ of $n$ points in a metric space, we show\nthat after preprocessing which uses $O(n)$ distance computations we can compute\na weighted sample $S\\subset V$ of size $O(\\epsilon^{-2})$ such that the average\ndistance from any query point $v$ to $V$ can be estimated from the distances\nfrom $v$ to $S$. Finally, we show that for a set of points $V$ in a metric\nspace, we can estimate the average pairwise distance using $O(n+\\epsilon^{-2})$\ndistance computations. The estimate is based on a weighted sample of\n$O(\\epsilon^{-2})$ pairs of points, which is computed using $O(n)$ distance\ncomputations. Our estimates are unbiased with normalized mean square error\n(NRMSE) of at most $\\epsilon$. Increasing the sample size by a $O(\\log n)$\nfactor ensures that the probability that the relative error exceeds $\\epsilon$\nis polynomially small.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 03:50:27 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2015 04:32:42 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 17:20:33 GMT"}, {"version": "v4", "created": "Mon, 4 May 2015 17:12:36 GMT"}, {"version": "v5", "created": "Sat, 9 May 2015 23:18:01 GMT"}, {"version": "v6", "created": "Fri, 26 Jun 2015 09:39:52 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Chechik", "Shiri", ""], ["Cohen", "Edith", ""], ["Kaplan", "Haim", ""]]}, {"id": "1503.08535", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Infinite Author Topic Model based on Mixed Gamma-Negative Binomial\n  Process", "comments": "10 pages, 5 figures, submitted to KDD conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating the side information of text corpus, i.e., authors, time\nstamps, and emotional tags, into the traditional text mining models has gained\nsignificant interests in the area of information retrieval, statistical natural\nlanguage processing, and machine learning. One branch of these works is the\nso-called Author Topic Model (ATM), which incorporates the authors's interests\nas side information into the classical topic model. However, the existing ATM\nneeds to predefine the number of topics, which is difficult and inappropriate\nin many real-world settings. In this paper, we propose an Infinite Author Topic\n(IAT) model to resolve this issue. Instead of assigning a discrete probability\non fixed number of topics, we use a stochastic process to determine the number\nof topics from the data itself. To be specific, we extend a gamma-negative\nbinomial process to three levels in order to capture the\nauthor-document-keyword hierarchical structure. Furthermore, each document is\nassigned a mixed gamma process that accounts for the multi-author's\ncontribution towards this document. An efficient Gibbs sampling inference\nalgorithm with each conditional distribution being closed-form is developed for\nthe IAT model. Experiments on several real-world datasets show the capabilities\nof our IAT model to learn the hidden topics, authors' interests on these topics\nand the number of topics simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:03:37 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08542", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Nonparametric Relational Topic Models through Dependent Gamma Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:40:41 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08581", "submitter": "Ioannis Partalas", "authors": "Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry\n  Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza\n  Amini, Patrick Galinari", "title": "LSHTC: A Benchmark for Large-Scale Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSHTC is a series of challenges which aims to assess the performance of\nclassification systems in large-scale classification in a a large number of\nclasses (up to hundreds of thousands). This paper describes the dataset that\nhave been released along the LSHTC series. The paper details the construction\nof the datsets and the design of the tracks as well as the evaluation measures\nthat we implemented and a quick overview of the results. All of these datasets\nare available online and runs may still be submitted on the online server of\nthe challenges.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 08:03:47 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Partalas", "Ioannis", ""], ["Kosmopoulos", "Aris", ""], ["Baskiotis", "Nicolas", ""], ["Artieres", "Thierry", ""], ["Paliouras", "George", ""], ["Gaussier", "Eric", ""], ["Androutsopoulos", "Ion", ""], ["Amini", "Massih-Reza", ""], ["Galinari", "Patrick", ""]]}, {"id": "1503.08639", "submitter": "Rapha\\\"el Li\\'egeois", "authors": "Rapha\\\"el Li\\'egeois, Bamdev Mishra, Mattia Zorzi, Rodolphe Sepulchre", "title": "Sparse plus low-rank autoregressive identification in neuroimaging time\n  series", "comments": "6 pages paper submitted to CDC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of identifying multivariate autoregressive\n(AR) sparse plus low-rank graphical models. Based on the corresponding problem\nformulation recently presented, we use the alternating direction method of\nmultipliers (ADMM) to efficiently solve it and scale it to sizes encountered in\nneuroimaging applications. We apply this decomposition on synthetic and real\nneuroimaging datasets with a specific focus on the information encoded in the\nlow-rank structure of our model. In particular, we illustrate that this\ninformation captures the spatio-temporal structure of the original data,\ngeneralizing classical component analysis approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 11:11:57 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Li\u00e9geois", "Rapha\u00ebl", ""], ["Mishra", "Bamdev", ""], ["Zorzi", "Mattia", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1503.08650", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "Comparison of Bayesian predictive methods for model selection", "comments": "A few minor changes; added a few sentences, corrected some\n  grammatical errors and modified Figure 7", "journal-ref": "Statistics and Computing, 2017, Volume 27, Issue 3, 711-735", "doi": "10.1007/s11222-016-9649-y", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to compare several widely used Bayesian model\nselection methods in practical model selection problems, highlight their\ndifferences and give recommendations about the preferred approaches. We focus\non the variable subset selection for regression and classification and perform\nseveral numerical experiments using both simulated and real world data. The\nresults show that the optimization of a utility estimate such as the\ncross-validation (CV) score is liable to finding overfitted models due to\nrelatively high variance in the utility estimates when the data is scarce. This\ncan also lead to substantial selection induced bias and optimism in the\nperformance evaluation for the selected model. From a predictive viewpoint,\nbest results are obtained by accounting for model uncertainty by forming the\nfull encompassing model, such as the Bayesian model averaging solution over the\ncandidate models. If the encompassing model is too complex, it can be robustly\nsimplified by the projection method, in which the information of the full model\nis projected onto the submodels. This approach is substantially less prone to\noverfitting than selection based on CV-score. Overall, the projection method\nappears to outperform also the maximum a posteriori model and the selection of\nthe most probable variables. The study also demonstrates that the model\nselection can greatly benefit from using cross-validation outside the searching\nprocess both for guiding the model size selection and assessing the predictive\nperformance of the finally selected model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 11:58:20 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:34:51 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 13:27:27 GMT"}, {"version": "v4", "created": "Wed, 23 Mar 2016 11:28:15 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1503.08818", "submitter": "Zimu Yuan", "authors": "Zimu Yuan, Zhiwei Xu", "title": "Founding Digital Currency on Imprecise Commodity", "comments": "arXiv admin note: text overlap with arXiv:1503.08407", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current digital currency schemes provide instantaneous exchange on precise\ncommodity, in which \"precise\" means a buyer can possibly verify the function of\nthe commodity without error. However, imprecise commodities, e.g. statistical\ndata, with error existing are abundant in digital world. Existing digital\ncurrency schemes do not offer a mechanism to help the buyer for payment\ndecision on precision of commodity, which may lead the buyer to a dilemma\nbetween having to buy and being unconfident. In this paper, we design a\ncurrency schemes IDCS for imprecise digital commodity. IDCS completes a trade\nin three stages of handshake between a buyer and providers. We present an IDCS\nprototype implementation that assigns weights on the trustworthy of the\nproviders, and calculates a confidence level for the buyer to decide the\nquality of a imprecise commodity. In experiment, we characterize the\nperformance of IDCS prototype under varying impact factors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 09:35:17 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Yuan", "Zimu", ""], ["Xu", "Zhiwei", ""]]}, {"id": "1503.08855", "submitter": "Gonzalo Mateos", "authors": "Georgios B. Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D. Schizas,\n  and Hao Zhu", "title": "Decentralized learning for wireless communications and networking", "comments": "Contributed chapter to appear in Splitting Methods in Communication\n  and Imaging, Science and Engineering, R. Glowinski, S. Osher, and W. Yin,\n  Editors, New York, Springer, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 21:18:38 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Giannakis", "Georgios B.", ""], ["Ling", "Qing", ""], ["Mateos", "Gonzalo", ""], ["Schizas", "Ioannis D.", ""], ["Zhu", "Hao", ""]]}, {"id": "1503.08873", "submitter": "Paul Mineiro", "authors": "Paul Mineiro and Nikos Karampatziakis", "title": "Fast Label Embeddings for Extremely Large Output Spaces", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern multiclass and multilabel problems are characterized by\nincreasingly large output spaces. For these problems, label embeddings have\nbeen shown to be a useful primitive that can improve computational and\nstatistical efficiency. In this work we utilize a correspondence between rank\nconstrained estimation and low dimensional label embeddings that uncovers a\nfast label embedding algorithm which works in both the multiclass and\nmultilabel settings. The result is a randomized algorithm for partial least\nsquares, whose running time is exponentially faster than naive algorithms. We\ndemonstrate our techniques on two large-scale public datasets, from the Large\nScale Hierarchical Text Challenge and the Open Directory Project, where we\nobtain state of the art results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 23:29:46 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1503.09022", "submitter": "Jesse Read", "authors": "Jesse Read and Jaakko Hollm\\'en", "title": "Multi-label Classification using Labels as Hidden Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive methods for multi-label classification typically invest in\nlearning labels together. To do so in a beneficial way, analysis of label\ndependence is often seen as a fundamental step, separate and prior to\nconstructing a classifier. Some methods invest up to hundreds of times more\ncomputational effort in building dependency models, than training the final\nclassifier itself. We extend some recent discussion in the literature and\nprovide a deeper analysis, namely, developing the view that label dependence is\noften introduced by an inadequate base classifier, rather than being inherent\nto the data or underlying concept; showing how even an exhaustive analysis of\nlabel dependence may not lead to an optimal classification structure. Viewing\nlabels as additional features (a transformation of the input), we create\nneural-network inspired novel methods that remove the emphasis of a prior\ndependency structure. Our methods have an important advantage particular to\nmulti-label data: they leverage labels to create effective units in middle\nlayers, rather than learning these units from scratch in an unsupervised\nfashion with gradient-based methods. Results are promising. The methods we\npropose perform competitively, and also have very important qualities of\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 12:29:13 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 10:54:26 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 12:12:49 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Read", "Jesse", ""], ["Hollm\u00e9n", "Jaakko", ""]]}, {"id": "1503.09025", "submitter": "Jos\\'e L Balc\\'azar", "authors": "Marta Arias, Jos\\'e L. Balc\\'azar, Cristina T\\^irn\\u{a}uc\\u{a}", "title": "Learning Definite Horn Formulas from Closure Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A definite Horn theory is a set of n-dimensional Boolean vectors whose\ncharacteristic function is expressible as a definite Horn formula, that is, as\nconjunction of definite Horn clauses. The class of definite Horn theories is\nknown to be learnable under different query learning settings, such as learning\nfrom membership and equivalence queries or learning from entailment. We propose\nyet a different type of query: the closure query. Closure queries are a natural\nextension of membership queries and also a variant, appropriate in the context\nof definite Horn formulas, of the so-called correction queries. We present an\nalgorithm that learns conjunctions of definite Horn clauses in polynomial time,\nusing closure and equivalence queries, and show how it relates to the canonical\nGuigues-Duquenne basis for implicational systems. We also show how the\ndifferent query models mentioned relate to each other by either showing\nfull-fledged reductions by means of query simulation (where possible), or by\nshowing their connections in the context of particular algorithms that use them\nfor learning definite Horn formulas.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 12:41:01 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 09:27:43 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 12:02:25 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Arias", "Marta", ""], ["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["T\u00eern\u0103uc\u0103", "Cristina", ""]]}, {"id": "1503.09082", "submitter": "Jian Yu", "authors": "Jian Yu", "title": "Generalized Categorization Axioms", "comments": "16 pages. Dimensionality reduction, density estimation, regression,\n  clustering and classification are represented in a unified way, where\n  unsupervised dimensionality reduction, density estimation, regression are\n  considered as one category problem, clustering and classification are\n  considered as multiple category problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorization axioms have been proposed to axiomatizing clustering results,\nwhich offers a hint of bridging the difference between human recognition system\nand machine learning through an intuitive observation: an object should be\nassigned to its most similar category. However, categorization axioms cannot be\ngeneralized into a general machine learning system as categorization axioms\nbecome trivial when the number of categories becomes one. In order to\ngeneralize categorization axioms into general cases, categorization input and\ncategorization output are reinterpreted by inner and outer category\nrepresentation. According to the categorization reinterpretation, two category\nrepresentation axioms are presented. Category representation axioms and\ncategorization axioms can be combined into a generalized categorization\naxiomatic framework, which accurately delimit the theoretical categorization\nconstraints and overcome the shortcoming of categorization axioms. The proposed\naxiomatic framework not only discuses categorization test issue but also\nreinterprets many results in machine learning in a unified way, such as\ndimensionality reduction,density estimation, regression, clustering and\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 15:17:57 GMT"}, {"version": "v10", "created": "Sun, 1 Nov 2015 00:55:56 GMT"}, {"version": "v11", "created": "Fri, 15 Jan 2016 01:26:03 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 07:59:01 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2015 10:07:40 GMT"}, {"version": "v4", "created": "Sat, 11 Apr 2015 16:07:08 GMT"}, {"version": "v5", "created": "Tue, 21 Apr 2015 11:05:32 GMT"}, {"version": "v6", "created": "Thu, 28 May 2015 04:12:36 GMT"}, {"version": "v7", "created": "Sun, 31 May 2015 04:48:55 GMT"}, {"version": "v8", "created": "Sat, 25 Jul 2015 04:03:57 GMT"}, {"version": "v9", "created": "Wed, 28 Oct 2015 00:54:26 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Yu", "Jian", ""]]}]